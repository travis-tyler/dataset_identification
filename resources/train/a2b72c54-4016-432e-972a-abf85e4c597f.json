[{"section_title": "Abstract", "text": "A Bayesian IRT-model approach was used to investigate the validity and reliability of student perceptions of teaching quality. Furthermore, the student perceptions were compared with ratings of teaching quality by external observers. Grade 4 students (n = 675) filled out a questionnaire that was used to measure their opinions about the lessons of their teachers. Three lessons of 39 teachers were recorded and rated by 4 raters. The analyses showed that student perception and lesson observation scales fit best in an 11-dimensional model, which was an indication of construct validity and discriminant validity. Student perception scales were reliable, although not all items contributed to the scales to the same extent. Student ratings and lesson observations scores generally correlated moderately (ranging from r = .18 to r = .50). Higher correlations were found for scales with a similar content; however, no clear pattern was apparent. Suggestions for future research are presented.\nKEYWORDS\nCONTACT Emmelien A. van der Scheer e.a.van.der.scheer@rug.nl GION education/research,"}, {"section_title": "Introduction", "text": "Determining teaching quality reliably and validly serves important improvement and accountability purposes (Timperley, Wilson, Barrar, & Fung, 2007) . In primary education, teaching quality is predominantly measured using classroom observations (Goe, Bell, & Little, 2008) . However, classroom observations are time consuming as obtaining reliable measurements requires multiple lesson observations carried out by multiple trained observers (e.g., Hill, Charalambous, & Kraft, 2012) . As this is generally not feasible for school leaders, school inspectors, and researchers, it is common for only one lesson to be observed, which causes problems regarding the reliability (and validity) of the measures.\nA less time-consuming method is the use of student perceptions of teaching quality (Goe et al., 2008) . Students experience their teacher daily and thus may be an important source of information regarding the teaching qualities of their teachers (Ferguson & Danielson, 2014; Gaertner, 2014; Peterson, Wahlquist, & Bone, 2000) . Furthermore, students have experienced various teachers, and as such can base their judgements on comparative observations (Goe et al., 2008) .\nHowever, student perceptions are rarely used in primary education for measuring teaching quality (Hamre & Pianta, 2010; Kyriakides, 2005) . Although recent studies have shown that student perceptions of teaching quality can provide reliable and valid information for both formative evaluation and research purposes (Burniske & Meibaum, 2012; Ferguson & Danielson, 2014; Kane, McCaffrey, Miller, & Staiger, 2013; Kyriakides, 2005; Peterson et al., 2000) , there are concerns about the validity and reliability of the perceptions of younger children (De Jong & Westerhof, 2001; Ferguson, 2012 , Kunter & Baumert, 2006 .\nThese concerns generally involve the discriminant validity of student perceptions, that is, the extent to which students are able to discriminate between the different facets of teaching (Fauth, Decristan, Rieser, Klieme, & B\u00fcttner, 2014) . Because studies into student perceptions of teaching quality in primary education are limited, additional research is required to identify the extent to which such perceptions could be used for evaluating teaching quality. More insight into the validity and reliability of student perceptions of teaching quality in primary education, and how these perceptions relate to external observer ratings, is especially important (Maulana & Helms-Lorenz, 2016; Van der Lans, 2017) . Therefore, the following research questions are addressed in this study:\n(1) To what extent do fourth-grade students' perceptions of teaching quality show construct and discriminant validity? (2) To what extent are fourth-grade students' perceptions of teaching quality reliable at the scale and item levels? (3) How consistent are student perceptions of teaching quality with observer ratings of lessons?"}, {"section_title": "Theoretical framework", "text": "First, some concerns about the validity and reliability of student perceptions of teaching quality in primary education are described. This is followed by a discussion of what is known about the extent to which the results of student perceptions and observations match. To indicate whether student perceptions of teaching quality can be used as measures of teaching quality, it is important to know which aspects of teaching are relevant to focus on. Therefore, the characteristics of effective lessons that are generally measured using observation schemes are also presented here."}, {"section_title": "Student perceptions of teaching quality in primary education", "text": "As was already mentioned in the introduction of this article, students may be a valuable source of information regarding teachers' teaching qualities because their views are based on many lessons (De Jong & Westerhof, 2001) . Moreover, student perceptions are cost efficient as multiple raters (e.g., 20-30 students) all provide their opinion at one moment in time, which also reduces rater bias (De Jong & Westerhof, 2001; Goe et al., 2008; Kyriakides, 2005) . Peterson et al. (2000) found that student perception questionnaires used at various levels of the education system (primary school, middle school, and high school) were reliable and valid teacher evaluation measures. Although several studies into student perceptions have also provided evidence that students are generally capable of discriminating between teaching quality constructs (Fauth et al., 2014; Greenwald, 1997; Kyriakides, 2005; Wagner, G\u00f6llner, Helmke, Trautwein, & L\u00fcdtke, 2013) , two aspects of discriminant validity are still debated with respect to student perceptions in primary education. First, whether students are able to distinguish between different teaching quality constructs and, second, whether student perceptions are confounded with teacher popularity (Fauth et al., 2014) . Related to the first concern, younger students are untrained at rating teaching behavior, thus they may have a less sophisticated understanding of different teaching aspects compared to a trained observer (Van der Lans, Van de Grift, & Van Veen, 2015) . Therefore, the extent to which they can discriminate between different teaching constructs may be limited (Fauth et al., 2014) . The concern regarding teacher popularity is that popular teachers may receive higher student perception scores regardless of their teaching quality (Ben-Chaim & Zoller, 2001) . Combined with the potentially limited ability to discriminate between different teaching quality constructs, student perceptions may simply be a popularity contest rather than a measure of teaching quality (Fauth et al., 2014) ."}, {"section_title": "Combining multiple measurements for establishing validity and reliability", "text": "It is to be expected that students and external observers have differing views concerning a teacher's teaching quality because of differences in background, knowledge, and aims (Patton, 1980) . External observers generally do not have a(n) (emotional) connection with the observed teacher and may have been trained and familiarized with the use of a predetermined observation scheme. Furthermore, their aim is generally to assess teaching quality as objectively as possible during one or more lessons (Kane & Staiger, 2012) . If trained well, observers have knowledge regarding what effective teaching looks like, and thus can assess teaching behavior from an evidence-based perspective.\nOn the other hand, student perceptions of teaching quality reflect the perception of the target group, resulting in data based on subjective frames of references (Kane & Staiger, 2012) . Students have less knowledge of effective teaching than a trained observer does, and they therefore provide information about their experience of the teaching activities of their teachers. Moreover, students normally have an emotional connection with their teachers, which might lead to socially desirable answers from students (Maulana & Helms-Lorenz, 2016) .\nHowever, research into the extent to which both measurements relate is limited, and results are often subject to critique. A study by De Jong and Westerhof (2001) showed that student ratings of teaching quality were as reliable as ratings from external observers. They however found that both measurements correlated slightly (ranging from r = \u2212.17 to r = .38 with only 4 out of 30 correlations at or above r = .30). The authors explained this finding by indicating that the content of the scales differed substantially, thus the scales used across the instruments were incomparable. Van der Lans (2017) also found a low correlation between classroom observations and student questionnaires (r = .26 overall; r = .34 after removal of misfitting items), consistent with other studies in which student perceptions and observer ratings of teaching quality were statistically related to each other (Howard, Conway, & Maxwell, as cited in Van der Lans, 2017; Maulana & Helmz-Lorenz, 2016) . However, in these studies the researchers observed only one lesson and/or worked with only one rater (instead of several observations and raters, which are required for the reliable measurement of teaching quality; Hill et al., 2012) . Therefore, the extent to which observer ratings and student perceptions in primary education are consistent with each other is still unclear, especially if similar teaching dimensions are used and if several lesson observations per teacher are rated by multiple observers. In the current study, the results of multiple lesson observations per teacher (assessed by multiple raters) were statistically related to student perceptions of teaching qualities in primary education. To clarify whether student perceptions of teaching quality can be used as a measure for teacher evaluation, similar teaching dimensions were measured by means of student perceptions and classroom observations. To indicate which teaching constructs are relevant to focus on in evaluations of teaching quality, the characteristics of effective lessons are described below."}, {"section_title": "Characteristics of effective lessons", "text": "The study of effective classroom practices and \"what works\" in the classroom has been central to the measurement of teaching quality (Reynolds et al., 2014) . Several studies have been conducted to identify effective teaching practices (e.g., Creemers, 1994; Fauth et al., 2014; Hattie, 2008; Muijs et al., 2014; Pianta & Hamre, 2009; Reynolds et al., 2014; Sammons, Hillman, & Mortimore, 1995; Van de Grift, 2007) , and a variety of teacher behaviors which promote student learning were identified. These can be categorized into three basic teaching dimensions (Day et al., 2008) .\nFirst, providing a supportive, positive, and inclusive classroom climate is important (Fraser, 1998) and promoting positive teacher-pupil relationships through praise and feedback (Den Brok, Brekelmans, Levy, & Wubbels, 2002) .\nSecond, classroom management quality can be used to identify an effective teacher ( Van de Grift, 2007) . Classroom management is about having clear classroom rules and routines, preventing disruptive behavior, and about having well-organized and structured lessons (Day et al., 2008) .\nThird, the teacher's instructional approach is important. This involves connecting the lesson to what students already know, explaining the subject matter in such a way that students understand it, engaging students by means of assignments and activities, and providing feedback (Hattie & Timperley, 2007; Hollingsworth & Ybarra, 2009; Rosenshine, 1995) . Van de Grift (2007) adds two teaching dimensions to these three features of effective lessons. According to Van de Grift, effective teachers also teach learning strategies to their students. They support the higher level thinking and metacognitive learning of their students (Arends, 2009 ). Teaching then is broken down into small steps wherein teachers provide simplified problems, model problem-solving strategies, and think aloud when solving a problem (Rosenshine, 1995) . Moreover, Van de Grift (2007) and Maulana, Helms-Lorenz, and Van de Grift (2014) stress the importance of adaptive instruction in the classroom: teachers adapting their teaching activities to the (varying) needs of their students. Another feature of effective lessons is ensuring that lessons are meaningful to students (Keuning & Van Geel, 2016; Kyriakides, Campbell, & Gagatsis, 2000) . This means that teachers set clear lesson goals and clarify them to the students at the beginning of the lesson (what they are learning and why), that they make sure that all lesson activities relate to these goals, and that the goals are evaluated (Locke & Latham, 2002) .\nIn summary, several important dimensions of teaching quality were discussed: providing a positive and inclusive classroom climate, quality of classroom management, clear and activating instructional approach (the three basic dimensions), adaptive instruction, teaching learning strategies, and goal orientation."}, {"section_title": "Method", "text": ""}, {"section_title": "Design", "text": "The data used in this study were collected as part of an intervention study. The teachers participated in a data-based decision-making intervention during the school year 2013-2014 (e.g., Van der Scheer & Visscher, 2018) . The lessons of participating teachers were recorded prior to the intervention, either at the end of school year 2012-2013 or at the start of school year 2013-2014. The students completed a student perception questionnaire on the teaching quality of their teachers after the first few weeks of the school year 2013-2014."}, {"section_title": "Participants", "text": "In this study, 31 teachers (64.5% women) from 27 primary schools participated. Eight teachers were part of a teacher pair (two teachers teaching the same class). The teachers had, on average, 13.29 years (SD = 10.06) of teaching experience. Each teacher taught a fourth-grade class or a multigrade class including fourth-grade students.\nTheir 675 students (52.4% boy) filled out the student perception questionnaire. The majority of students were fourth-grade students (83.1%), the other students were either in Grade 3 or in Grade 5. The average class size was 19.35 students (SD = 4.98)."}, {"section_title": "Instruments", "text": ""}, {"section_title": "Observation scheme", "text": "The validated ICALT (International Comparative Analysis of Learning and Instruction) lesson observation instrument was used by the observers to rate the teachers' teaching quality (Van de Grift, 2007) . In line with the study of Van der Scheer, Glas, and Visscher (2017), the following six scales were used: safe and stimulating learning climate, efficient classroom management, clear and activating instruction, adaption of instruction, teaching learning strategies, and engagement of students. The first five constructs are discussed in the theoretical framework of this study as effective teaching dimensions. The latter is about how lesson observers rated students' behavior (their engagement in the lesson) instead of teacher behavior, as student engagement is an important prerequisite for learning (Van de Grift, 2007) . All 35 items were scored on a 4-point Likert scale (ranging from predominantly weak to predominantly strong). The content of the ICALT instrument is discussed extensively in other studies (see, e.g., Maulana et al., 2014; Van de Grift, 2007 Van de Grift, Van der Wal, & Torenbeek, 2011) .\nThe student perception questionnaire The student perception questionnaire used in this study was originally developed to evaluate the effects of a data-based decision-making (DBDM) intervention (Keuning & Van Geel, 2016; Van der Scheer, 2016) . The development of the questionnaire was inspired by the Tripod 7Cs instrument, used in the large-scale Measures of Effective Teaching project (Ferguson & Danielson, 2014) . The translated Tripod 7Cs questionnaire was then piloted with 59 primary school teachers, and the results were discussed with an expert group of primary education teachers. Based on the gathered feedback, the instrument was further adapted to the Dutch context (in terms of the items used and the wording of the items). The resulting questionnaire consists of 36 items, scored on a 5-point Likert scale (ranging from no never to yes, always) and includes the following five scales: classroom climate (6 items), classroom management (9 items), instruction (10 items), goal orientation (6 items), and challenging students (5 items). The first four are discussed in the theoretical framework of this study as effective teaching dimensions. The fifth scale measures the extent to which students feel challenged by their teacher as a result of the expectations teachers have of them. Teacher expectations are identified as one of the most important factors for student learning in educational effectiveness research (Muijs et al., 2014) .\nAll 11 scales in the lesson observation instrument and the student perception questionnaire were included in this study. An overview of the observer scales and the student perception scales is provided in Table 1 . It shows that the content of the first three scales is comparable for both instruments. As discussed in the theoretical framework, these three scales represent the three general dimensions of effective teaching. The content of the remaining teaching quality scales differs between the two instruments due to the different backgrounds of the questionnaires. However, the observers' scale regarding the adaptation of instruction also included items that are related to goal orientation."}, {"section_title": "Data collection", "text": "Each teacher was recorded during three mathematics lessons, and the recordings were rated by four observers. Although both multiple raters and multiple lessons are necessary to obtain reliable estimates of teachers' teaching skills (e.g., Hill et al., 2012) , these requirements are seldom met. The lessons were recorded using the IRIS Connect video system. This system consists of two iPods; one iPod recorded what the teacher did, while the other iPod simultaneously recorded the students. The recordings were uploaded to an online environment, where both observers and teachers could view the recordings. Adaption of instruction Challenging students 5\nLearning strategies Goal orientation 6\nEngagement of students\nTo avoid order bias, each observer was provided with a list of the lessons to be rated in a specific sequence; the sequence of the recordings had been ordered randomly (Shadish, Cook, & Campbell, 2002) . Each lesson was rated by each observer; however, one observer could not assess two recordings. The three observers were trained for 3 days in the use of the ICALT observation instrument. As part of the training, observers viewed six recorded lessons independently. The results were discussed to achieve consensus on the use of the scale. A fourth experienced observer was added later. Observer variance accounted for a low percentage of the total variance, which indicates a high reliability of the estimates of the teachers' teaching quality (Van der Scheer et al., 2017) .\nStudents filled out the student perception questionnaire about the teaching quality of their teacher. To avoid test scores being influenced by the presence of their teacher, it was recommended that another teacher substituted the teacher during the administration of the questionnaire. Afterwards, teachers received a report including their average scores given by their students and a comparison of their scores with the scores of other teachers."}, {"section_title": "Analyses", "text": "Nowadays, latent variable models in general and item response theory (IRT; see, e.g., Lord, 1980) in particular have become the standard statistical tools in educational measurement. For instance, all large-scale international educational surveys, such as the Programme for International Student Assessment (PISA), the Trends in International Mathematics and Science Study (TIMSS), the Progress in International Reading Literacy Study (PIRLS), the International Computer and Information Literacy Study (ICILS), and the Programme for the International Assessment of Adult Competencies (PIAAC), use this methodology (see, e.g., Rutkowski, von Davier, & Rutkowski, 2014) . This methodology will also be used for the analyses in the current research.\nIRT is used to model the responses of the students to the questionnaire and observers to the observation scheme. The advantage of using an IRT measurement model over a simple sum-score model is that it explicitly accounts for the discrete nature of the item responses. Furthermore, the parameter estimates give insight into how the individual items contribute to the overall reliability. Each of the 11 scales (see Table 1 ) was associated with a unidimensional latent proficiency variable (the estimate of a teacher's teaching quality per scale), that is, with an IRT model. Below, the designations, proficiency, and teaching quality will be used interchangeably.\nTo assess the extent to which student perceptions of teaching quality show construct and discriminant validity and answer the first research question, the five latent variables related to the five subscales used by the students were correlated. This is completely analogous to performing a confirmatory factor analysis on categorical data. The correlations between the scales should be lower than r = .85 to indicate discriminant validity (Brown, 2015) . To answer the second research question, about the reliability of student perceptions of teaching quality, the global (on scale level) and local (on item level) reliability of the student perception scales was estimated. For the third research question, the correlations between the teacher proficiencies for each scale as rated by external observers and the student perceptions of teaching quality were estimated.\nSince the structure of the data is different for the student perceptions (a multilevel structure, with multiple students providing their opinions about the teachers' teaching quality) and the observer ratings (three lessons per teacher, rated by four raters), two different underlying models of teachers' proficiency had to be defined. Below, first a description of the general model is provided, followed by a description of the student perceptions model and the observer ratings model.\nFinally, the complete model, that is, the IRT models for the observation scales and their covariance structure, was estimated in a Bayesian framework using OpenBugs. The motive for this choice rather than traditional standard software for latent variable modeling is that OpenBugs allows the users to completely specify their own model, taking into account all dependencies."}, {"section_title": "The general model", "text": "The relation between the ratings by the external observers and the ratings by the students was modeled as follows. The observation instrument used by the students consisted of K (s) = 36 items distributed across five subscales, while the observation instrument used by the observers consisted of K (o) = 35 items distributed across six subscales (see Table 1 ). The items (indexed by the subscript i) were scored polytomously (more than two response categories). The response categories were indexed as j = 0, 1, 2, . . . , M, where M was the highest category. For the student questionnaire, M = M (s) = 4; for the observers, M = M (o) = 3.\nThe item responses were assumed to load on latent variables, one for every subscale, with a total of Q = 11 subscales, 5 for the students and 6 for the observers. So, the proficiency of every teacher (indexed n) is modeled by a Q-dimensional vector \u03b8n = \u03b8n1, ... , \u03b8nq, ... ,\u03b8nQ. These proficiency vectors have a normal distribution with expectation zero and a covariance matrix AE, that is, \u03b8 n e N\u00f00; AE\u00de\n(1)\nThe covariance matrix reflects the relation between various subscales and is the primary concern of the analyses. In the Results section, the covariance matrix is converted to a correlation matrix. It is an indicator for the discriminant validity of the student perceptions (correlation between the students' dimensions in the matrix) and the consistency between the observer ratings and student perceptions (correlation between the students' and observers' dimensions). An IRT model was used to transform the discrete item responses X i (X i = j, j = 0, . . . , M) to continuous latent values \u03b8. The specific IRT model used was the generalized partial credit model (GPCM; Muraki, 1992) . In the GPCM, the probability of observing a score X i in a response category j (j = 0, . . . , M) is defined as:\nThe item parameters b ij (j = 0, . . . , M, b i0 = 0 to identify the model) are related to the location of the items on the latent \u03b8 scale. The parameter a i is called a discrimination parameter and reflects the extent to which the item depends on the latent variable \u03b8.\nThe multidimensional IRT model used here is closely related to a confirmatory factor analysis model (Takane & De Leeuw, 1987) . Therefore, translated to factor-analytic terminology, the discrimination parameter is a factor loading.\nThe model for the student perceptions The model for the student perceptions is a multilevel model with two levels. The first level is an IRT model for a response regarding teacher n, assessed by student s (s = 1, . . . , N n where N n is the number of students in the class of teacher n) on scale q, say the response X nsqi . The model is a special version of the general model defined by Formula (2). Therefore, the probability of a response on item i in category j is defined by\nwhere the function \u03a8 ij \u00f0:\u00de is defined as in Formula (2). The second-level model entails that the variables \u03b8 nsq have a normal distribution, that is,\nSo, the mean is equal to the teacher's proficiency \u03b8 nq on scale q, and the variance \u03c3 2 0q indicates the degree to which students differ in their assessment on this scale. Notice that this is the definition of a multilevel model: Students are nested under a teacher and are assumed random effects."}, {"section_title": "The model for the observers' ratings", "text": "The model for the observer was built up along the same lines, but here three different time points (t = 1, . . . , T, T = 3), and four observers (r = 1, . . . , R, R = 4) had to be accounted for. As such, the responses were modeled through a combination of an IRT model and a generalizability theory (GT) model (Brennan, 1992; Brennan & Johnson, 1995) , defined on a latent variable (Glas, 2012) . Let X nqrti be the response pertaining to teacher n by observer r on time point t, on item i of scale q. Then the response model is defined by P\u00f0X nqrti \u00bc jj\u03b8 nqrt \u00de \u00bc \u03a8 ij \u00f0\u03b8 nqrt \u00de, where, again, the function \u03a8 ij \u00f0:\u00de is defined as in Formula (2). The GT model is defined as\nwhere all components with their interpretation and distribution are summarized in Table 2 . For each of the scales, the parameters of the combined IRT model, the multilevel model, and the GT model were concurrently estimated in a Bayesian framework using OpenBugs (Version 3.2.3., rev 2012). "}, {"section_title": "Model comparisons", "text": "In this section, the fit of the model is evaluated. A more complex model fits data better than a simpler model; however, at some point adding more complexity destroys the interpretability and reliability of the analyses. Therefore, we apply a methodology for the evaluation of model fit that penalizes overly complex models. The model used above for the analyses was a between-items 11-dimensional model, as six observer scales and five student perception scales were defined. It is betweenitem multidimensional because the response probabilities as defined by Formula (2) depend on only one latent variable. For instance, \u03a8 ij \u00f0\u03b8 nqrt \u00de depends on one specific value \u03b8 nqrt for a unique teacher n, on one scale indexed q, by observer r at time point t. To obtain an impression of the fit of the model, the model was compared to three alternative models. The first two were simpler than the between-items 11-dimensional model and had fewer parameters. In these models, the student and observer subscales (as presented in Table 1 ) were not taken into account to investigate both construct and discriminant validity. The first one was a unidimensional model where all 11 dimensions were collapsed into a single dimension. The hypothesis here was that all items measured the same dimension for both the students and the observers. The second model assumed two correlated dimensions, wherein it was hypothesized that two dimensions could be distinguished: one for the student perceptions and one for the external observer ratings. The models were compared using the so-called deviance information criterion (DIC). The DIC is the sum of two components, the expected deviance denoted by \" D and a penalty for the number of parameters in the model, denoted by p D . As is done in the more common likelihood-based approach, a lower deviance is an indication of better model fit. Further, p D favors models with a smaller number of parameters. The third model, with which the between-items 11-dimensional model was compared, was a slightly more general model, which was called the 11-dimensional bi-factor model. It still has 11 dimensions. However, every item now loads on two dimensions, one general teaching quality dimension, say \u03b8 0 , and one scale-specific dimension, say \u03b8 q . The model is defined as:\nNote that compared to the between-items 11-dimensional model, there are more discrimination parameters in the model. However, both for the students and observers scales, the items pertaining to one of the subscales must be unidimensional to identify the model. The results are given in Table 3 . Although the DIC values, as presented in Table 3 , are quite close, the between-items 11-dimensional model has the lowest DIC value; so, this is the preferred model. Thus, the confirmatory factor analyses showed that the model in which the six observer scales and five student perception scales were distinguished best fitted the data, and this model was therefore used for the analyses.\nIt will now be described how the scale reliability (of both the observer and students scales) and the local reliability (of the items of students scales) were estimated to answer the second research question regarding the reliability of student perceptions."}, {"section_title": "Reliability of the observer scale", "text": "The reliability of the external observer scales was high (ranging from 0.82, with a posterior variance of 0.05, to 0.95, with a posterior variance of 0.01). For the external observer ratings, the reliability is equal to the ratio of the systematic variance, that is, the variance of the teachers and the systematic variance plus all error variance components (for more information on reliability indices in generalizability theory, see Brennan, 1992) ."}, {"section_title": "Reliability of the student scale", "text": "For the student ratings, the reliability is equal to:\nwhere AE qq is the variance of the teachers on scale q, that is, the q-th diagonal element of the covariance matrix AE defined in Formula (2), and \u03c3 2 0q is the variance of the student ratings as defined in Formula (3). Further, \" N s stands for the average number of students in a class.\nNot only the reliability for the entire scale (global reliability) was estimated but also the local reliability. The local reliability is indicated by the contributions of individual items to the reliability, which depends on both the discrimination and item location parameters. High discrimination and a location close to a specific \u03b8 lead to high reliability at that point. Local reliability is expressed in \"Information\". The information that an item i attributes to the reliability with which \u03b8 is estimated, is equal to:\nAs will be discussed in the Results section, Table 6 presents the expected item information computed over the posterior distribution of \u03b8."}, {"section_title": "Results", "text": "The discriminant validity of student perceptions of teaching quality is described first to answer the first research question. Next, the global and local reliabilities (second research question) of the student perception scales are presented. To answer the third research question, the results of the analyses of the consistency (correlations) between the observers' ratings of teaching quality and the student perceptions of teaching quality are presented. Table 4 gives the correlations between the student perceptions scales (see Formula [1] ). The table shows that the scales correlate moderately to strongly (ranging from r = .42 to r = .74). The correlation between the classroom climate scale and the instruction scale is highest (r = .74). Apparently, classes in which students reported that they experienced a safe classroom climate also felt that their teacher's instruction was clear. A moderate correlation was found between goal orientation and instruction (r = .65), two aspects of teaching that relate to the same lesson phase. The lowest correlation was found between the classroom management scale and the goal-orientation scale (r = .42). This indicates that students' experience of classroom management correlated limitedly to the extent to which their teachers communicated the lesson goals with them."}, {"section_title": "Discriminant validity of student perceptions", "text": "Global reliability student perceptionsscale level Table 5 presents the estimates of the reliabilities of the student perception scales and shows that the global reliability of each of the student perception scales is sufficient, as the reliabilities ranged from .80 to .91.\nLocal reliability student perceptionsitem level Table 6 shows the estimates of the student scales' item parameters. The columns labeled d and a respectively refer to the item location and discrimination parameters (as described in the general model description). For items with a higher location parameter (d), it is harder to receive a high score. For Item 5 (\"My teacher seems to know if something is bothering me\"), it is relatively hard for teachers to receive a positive judgement from their students. On the other hand, students generally report that they know when they can ask their teacher questions during work time (Item 11) as this item has a low location parameter. Items with high discriminations (a) load high on their relevant scale. So, Item 2 (\"I like the way my teacher treats me when I need help\") has a high loading on Student scale 1 (classroom climate), whereas Item 6 (\"I like this class\") has a low loading on that scale. Items with a high Information value contribute much to the reliability of that scale, which means that the value added of the items for the scale is high. Items with a low Information value do not contribute much to the reliability of the scale. As shown in Table 6 , each item contributes to its scale; however, some more than others. This means that all items are related to their scale to some extent, but that some items are either very simple or do not discriminate between teachers, or both.\nFor each scale, items can be identified that contribute much to the reliability of the scale. This means that for Scale 1 (classroom climate) Item 1 (\"My teacher is nice to me when I ask questions\") is most informative, whereas for Scale 2 (classroom management) Item 9 (\"When we are working individually, it is quiet in the classroom\") is most informative. \"My teacher knows when I understand something, and when I do not\" is most indicative for the instruction scale (Scale 3). For the challenging students scale (Scale 4), the item \"My teacher thinks I can learn everything if I do my best\" is most informative, whereas Item 34 (\"My teacher asks at the end of the lesson what we have learned\") is most informative for the goal-orientation scale (Scale 5)."}, {"section_title": "Comparing observers and students", "text": "To answer the third research question, the correlations between the estimates of teachers' teaching quality from the observer scales and the estimates from the student perception scales are presented in Table 7 (see Formula [1] ). The approach used here is somewhat related to the multitrait-multimethod approach by Campbell and Fiske (1959) for assessing convergent and discriminant validity by inspecting patterns of correlations within and between scales. The relation between constructs and measures is less clearcut in the present case, but the idea of looking for patterns remains the same.\nAs is presented in Table 7 , the correlations between the student scales and the observer scales range from very low to moderate (r = .18 to r = .50). The three scales with comparable contents (Scales 1 to 3) all show low correlations (ranging from r = .42 to r = .48) but are higher than most of the other correlations. These subscales all measure teachers' behavior, which can be operationalized quite well for both external observers and students. However, the highest correlation was found for two unrelated scales: the classroom students' classroom management scale and the Table 7 . Correlation matrix for the observer scales (O1-O6) and the student perception scales (S1-S5). observers' classroom climate scale (r = .50). A clear pattern could therefore not be identified. Nevertheless, correlations between, on the one hand, the student scales for instruction and, on the other hand, the instruction, adaption of instruction, and the learning strategies scales for the external observers are relatively high (r = .47 to r = .48). These lesson aspects mainly take place during the instruction phase of the lesson.\nThe student goal-orientation scale (S5) hardly correlates with any of the observer scales. Only the correlation with the adaption of instruction scale used by the observers is reasonable (r = .39), wherein two items from the adaption of instruction scale relate to discussing lesson goals.\nThe correlations that were found between the \"Engagement of student\" scale (O6) and the student perception scales were moderate to low (ranging from r = .18 to r = .42). A moderate correlation was found for the classroom management scale (r = .41) and the instruction scale (r = .42), indicating that when students are more engaged according to the observers, students report better instruction and classroom management."}, {"section_title": "Conclusion and discussion", "text": "In this study, we investigated the validity and reliability of student perceptions of teaching quality in primary education and their consistency with the ratings from by external observers. We will first elaborate on the findings and strengths of this study. Thereafter, the limitations of this study and suggestions for further research are presented.\nThe first research question was answered by studying two related types of validity: construct validity and discriminant validity. The confirmatory factor analysis, which can be used as an indicator of construct validity (Shadish et al., 2002) , showed that the assumed scales for both the observer ratings and the student perceptions were represented in the data. In other words, students are able to discriminate between different teaching quality constructs. If not, the unidimensional or dimensional model (in which the observer and student perception scales were not taken into account) would have resulted in a better fit. Furthermore, the correlations between the various student perceptions scales also indicated that Grade 4 students were able to distinguish between different teaching quality constructs. It is, of course, difficult to define exactly how low the latent correlations should be to assume discriminant validity. However, they are so low that the hypothesis that the five latent dimensions form a unidimensional scale can clearly be rejected. In this respect, Brown (2015) gives a boundary correlation of .85. Since the correlations range between .42 and .74, unidimensionality is clearly rejected and the latent dimensions are distinguishable. This finding is very valuable for the use of student perceptions in primary education and is in line with the findings from other recent studies, for example, the studies by Kane, McCaffrey, and Staiger (2010) , Kunter and Baumert (2006) , and Fauth et al. (2014) .\nAs far as the second research question is concerned, the global (scale-level) reliability and the local (item-level) reliability of the student perceptions measures were investigated. The scales showed a high global reliability (even at the same level as the reliabilities of the observer scales), and the local reliability showed that individual items belonged to their scale. Furthermore, when considering the content of the most informative items per scale, each of these items was clearly related to the scale's content. For example, in the theoretical framework of this study we described that classroom management is about having clear class rules and routines, preventing disruptive behavior, and having a well-organized and clear lesson structure (Day et al., 2008) . It makes sense that, from a student perspective, the statement \"Everybody pays attention when my teacher explains something\" reflects the extent to which teachers have good classroom management skills. Also for the instruction scale, which for lesson observers includes items like \"The teacher checks during the instruction whether students understood the subject matter or not\", the most indicative student perception item was \"My teacher knows when I understand something, and when I do not\". These findings suggest that student perceptions may be used for evaluating teachers' distinct teaching qualities.\nWhen the student perceptions results were compared with the ratings by external observers (Research question 3), we found that the teaching quality scales of both measurements correlated to a certain extent with each other, ranging from r = .18 to r = .50, with 23 out of 30 correlations at or above r = .30. Considering the correlations found in previous studies, as described in the theoretical framework by De Jong and Westerhof (2001) and Maulana and Helmz-Lorenz (2016) , it can be concluded that relatively high correlations were found in this study. However, the results in the other studies are not entirely comparable with the results found in the current study for several reasons. First, the studies were executed in different contexts (in secondary schools instead of in primary schools). Second, in the study of De Jong and Westerhof (2001) , three lessons per teacher were observed (similar to the current study), but only one observer rated the lessons. Maulana and Helms-Lorenz (2016) observed only one lesson per teacher (rated by one observer). The current study incorporated three lessons and four raters, which is important for obtaining a reliable estimate of a teacher's teaching quality (Hill et al., 2012; Murray, 1983) . Third, when student and observers' ratings were compared by De Jong and Westerhof (2001) , they compared completely different teaching constructs, whereas in the current study it was intended to measure the three basic constructs of teaching quality in a similar way using both the student perception questionnaire and the observation scheme.\nDespite the methodological strengths of this study, the correlations between students and observers still range from moderate to low. Although the correlations between the three scales with a comparable content (Scales 1 to 3) are higher than most of the other correlations, the highest correlation was found for two constructs that were not comparable. Therefore, a clear pattern could not be identified. This could be explained by the differences at the item level between both instruments (Maulana & Helms-Lorenz, 2016) , where it was intended to measure the same construct. The items were not phrased identically across the observer and the student perceptions instruments, and the number of items per construct also differed across the measures.\nAnother explanation for the moderate to low correlations could be that, due to the differences in background and knowledge between external observers and students (as mentioned in our theoretical framework), students and observers perceive the same constructs in a different way. This was, for example, found for the student perception instruction scale, on the one hand, and for the observers teaching learning strategies scale (r = .48) and the observers adaption of instruction scale (r = .47), on the other hand. This indicates that external observers and students observe similar aspects of teaching during the lesson. Again, this assumes that student perceptions might have the potential to be used for evaluating teaching quality (for professional development or research purposes) instead of using expensive and time-consuming classroom observations. However, as the relation between student perceptions and observations did not show a clear pattern, further in-depth research is required to gain more insight into this relationship."}, {"section_title": "Limitations of the study and recommendations for further research", "text": "A first remark related to the limitations of the study is that, although the same teacher was assessed by both observers and students, both measurements were conducted at different times during the school year. The majority of the lesson recordings were made in the year preceding the school year in which the student perception questionnaires were assessed. This may have affected the comparability of both measurements, and could have impacted the extent to which student perceptions and lesson observations were comparable.\nFurthermore, in this study students provided their opinion about their teacher once, while three lessons were used to estimate teaching quality by means of observations. For future research, it is suggested to investigate the comparability of both measurements in a more strict way: by comparing teaching quality based on student perceptions and ratings by external observers for the same lessons, using the same wording and rating scales for the items.\nAnother suggestion for future research is to gain more insight into how students interpret the statements in the items of a student perception questionnaire, such that differences between what raters observe and students perceive can be indicated. This information could be used to adapt the questionnaires, such that the content is more aligned to the observation scheme. In addition, some items of the student perception questionnaire in this study did not provide much information and might be excluded in future research.\nA final remark pertains to the statistical methodology used for this study for both student perception research and lesson observation research in general. The main objective of statistical modeling is to identify the different sources of uncertainty in order to develop reliable unbiased estimates of teaching proficiency. It is now generally accepted that the nested structure of the data should be addressed using multilevel models. Furthermore, in analyses of educational surveys, the use of an IRT model to model item responses is generally accepted. The reason is that just summing up item responses ignores the unreliability of the responses and does not differentiate between the items by weighting the responses, as is done in IRT. In this article, this approach is generalized by applying IRT modeling to the item responses of the student perception questionnaire and the observation instrument. We argue that this approach should be the standard in student perception and lesson observations research.\nIn summary, the results of this study are valuable for how we can use student perceptions of teaching quality for evaluating teaching quality and for feeding back the results of the evaluations to teachers. In response to this feedback, teachers can reflect on their lessons, and they may use the feedback to improve their teaching quality. Our research also points to new directions for future research which can help us gain an in-depth and precise understanding of what we measure when we study student perceptions of teaching quality and how we can utilize how students perceive the teaching quality of their teachers."}]