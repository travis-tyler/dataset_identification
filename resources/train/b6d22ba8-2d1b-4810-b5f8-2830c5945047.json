[{"section_title": "Introduction", "text": "Alzheimer's Disease (AD) and its early stage, Mild Cognitive Impairment (MCI), are becoming the most prevalent neurodegenerative brain diseases in elderly people worldwide. According to [1] , the prevalence of AD will rise dramatically during the next 20 years, and 1 in 85 people will be affected by 2050. To this end, there have been a lot of efforts on investigating the underlying biological or neurological mechanisms and also discovering biomarkers for early diagnosis or prognosis of AD and MCI. Neuroimaging tool- s such as Magnetic Resonance Imaging (MRI) [2] , Positron Emission Tomography (PET) [9] , and functional MRI (fM-RI) [4] have played the key roles in those works, and different neuroimaging tools can convey different information for diagnosis. Recent studies have shown that information fusion from multiple modalities can thus help enhance the diagnostic performance [5, 8, 16, 19, 22, 26, 25] ."}, {"section_title": "Previous Methods", "text": "Regarding the multi-modal fusion, most of the previous methods first extracted features from each modality (e.g., gray matter tissue volume from MRI, mean signal intensities from PET), trained a typical classifier to model the training examples for each modality independently, and then combined the outputs from classifiers in an ensemble way for a final decision. Here, we should note that, to our best knowledge, those methods assumed the conditional independence among the features. However, since we extract features in a homogeneous way, e.g., statistical information from particular Region Of Interests (ROIs) in a brain, they are naturally related to each other in certain ways. Furthermore, it's important to combine multi-modal information in a systematic manner.\nTo this end, in this paper, we design a new framework, in which we consider the feature-level coupled-interaction analysis and modality-level coupled-interaction analysis. Specifically, for the feature-level coupled-interaction, we devise a coupled-feature representation using intra-coupled interaction (correlations between features and their own powers) and inter-coupled interaction (correlations between features and the powers of other features) [19] . For the modality-level coupled-interaction, we propose a novel coupled boosting method that analyzes the pairwise coupled-diversity correlation between modalities. We illustrate the major difference between the previous methods and our new method in Fig.1 . Fig.2 schematizes the proposed framework, where we adopt two neuroimaging modalities of MRI and PET. Without loss of generality, we denote the MRI as modality A, and the PET as modality B. After the image preprocessing and low-level feature extraction, we find a coupled feature representation [19] by mapping the original feature vectors into the expanded feature vectors via both linear and nonlinear fashion. We then select the label-related features from the expanded feature vectors by means of Least Absolute Shrinkage and Selection Operator (LASSO), which is one of the most widely used feature selection methods in the literature. Finally, the proposed coupled boosting algorithm trains the base learners that analyze the pairwise coupleddiversity correlation between modalities at multiple rounds. Our major contributions can be two folds:\n\u2022 We propose a novel coupled boosting algorithm that makes a full use of the pairwise coupled diversity between multi-modal data (i.e., MRI and PET) to improve the generalization power. Unlike the previous boosting algorithms [10, 11] that usually focused on single-modal data classification, the proposed coupled boosting algorithm introduces the large diversity theory in ensemble learning and thus deals with multi-modal data classification problems, but still maintaining the general steps of AdaBoost.\n\u2022 A coupled feature representation method is employed to analyze the intra-coupled and inter-coupled interaction among features for AD/MCI diagnosis, which can successfully capture the intrinsic linear and nonlinear information.\nThe rest of this paper is organized as follows. In Section 2, we briefly introduce the related work for AD/MCI diagnosis with multi-modal data. The MRI and PET image processing and feature extraction are described in Section 3. In Section 4, we propose our new coupled feature representation and coupled boosting algorithm. Experimental results and performance comparisons with competing methods are presented in Section 5. We conclude this paper by summarizing the proposed method and also discussing the obtained results in Section 6."}, {"section_title": "Related Work", "text": "Recent studies have shown that fusing the complementary information from multiple modalities helps enhance the AD/MCI diagnostic accuracy [5, 8, 16, 18, 19, 22, 26] . In general, we can divide the previous methods into two categories: feature-concatenation approach and kernel-based approach. The first approach simply concatenates features from different modalities into a long feature vector and then build a classifier to find the relations implicitly. For example, Kohannim et al. concatenated features from modalities into a vector and trained a Support Vector Machine (SVM) classifier using the concatenated feature vectors [8] . Walhovd et al. applied multi-method stepwise logistic regression analyses [18] , and Westman et al. exploited a hierarchical modeling of orthogonal partial least squares to latent structures [22] . Meanwhile, the latter approach fuses multi-modal information by means of a kernel technique, in which the original feature vectors are mapped into a higher dimensional space via different types of kernels. For example, Hinrichs et al. [5] and Zhang et al. [26] , independently, utilized a multi-kernel SVM to combine information from different modalities.\nHowever, none of these methods considered relational information inherent in the original feature vectors. Since a human brain is a highly complicated system and multiple brain regions interact consistently, we believe that there exist relations among brain regions, and therefore the features extracted from multiple ROIs are also highly correlated to each other. Recently, Suk and Shen used a deep learning method to find the latent high-level features that capture the relations inherent in the low-level features and achieved prominent results [16] .\nIn this paper, we focus on the problems of feature representation and multi-modal data fusion for AD/MCI diagnosis. Specifically, we devise a feature-coupled representation by considering intra-coupled and inter-coupled interactions among features within a modality, and then propose a novel coupled boosting algorithm that systematically combine multi-modal information by building base learners at multiple rounds and finally combine them in an ensemble manner for a final decision."}, {"section_title": "Preprocessing and Feature Extraction", "text": "The MRI and PET images are preprocessed to extract ROI-based features. For MR images, we first perform anterior commissure-posterior commissure correction using MIPAV software 1 , and then re-sample the images to 256\u00d7256\u00d7256 resolution. The intensity inhomogeneity correction and skull stripping are performed by [14] and [20] , respectively. The MR images are then segmented into three different types of tissues, i.e., Gray Matter (GM), White Matter (WM) and CerebroSpinal Fluid (CSF), using FAST [27] in the FMRIB Software Library (FSL) package [23] . After registration using HAMMER [12] , we obtain the subject-labeled images based on a template with 93 manually labeled ROIs [7] . For each subject, we use the volumes of GM tissue of the 93 ROIs, which are normalized by the total intracranial volume (which is estimated by the summation of GM, WM and CSF volumes from all ROIs), as features. For PET images, we first align them to their respective M-R images through affine transformation, and then compute the average intensity of each ROI as feature. In summary, we have 93 features from MRI and 93 features from PET, and, hereafter, we regard these features as original low-level features."}, {"section_title": "Proposed Method", "text": ""}, {"section_title": "Notations", "text": "In this paper, we denote matrices as boldface uppercase letters, vectors as boldface lowercase letters, and scalars as normal italic letters, respectively. For a matrix Z = [Z ji ], its j-th row is denoted as Z j,\u00b7 . For the k-th element in a vector z i , we denote it as z i,k . We further denote a transpose operator of a vector or a matrix with a superscript .\nSuppose that we have a set of samples {z\n, where m and n denote, respectively, the number of training samples and the number of testing samples, and without loss of generality we assume that the samples are sorted in the order of training and testing samples. Here, z 2 . Then, we can represent the whole original feature samples with matrices"}, {"section_title": "Coupled Feature Representation", "text": "To our best knowledge, the previous work that addressed the AD/MCI diagnosis as a classification problem and used original low-level features without considering their relationships. However, it's well known that a human brain is a complex system that multiple brain regions are anatomically connected and functionally interact with each other for tasks. That is, it is natural to hypothesize that the original features extracted from MRI and PET in multiple ROIs are related to each other in a way. To this end, in this work, we propose to find such latent relations among features with a coupled feature representation method [19] and use the high-level information for the AD/MCI diagnosis.\nIn particular, we consider two types of relational information inherent in the original low-level features: 'intracoupled interaction' with correlations between features and their own powers, and 'inter-coupled interaction' with correlations between features and the powers of other features. Here, we should note that we find the coupled feature representation for each modality individually. That is, although we describe the feature representation for a modality A, it's equally applicable to a modality B.\nFor an original feature vector z A i of the modality A in the i-th sample, we map it to an expanded feature space with the incorporation of linear and nonlinear information by means 2 In this work, p A = 93 and p B = 93. of a matrix expansion as follows:\ne indicates the e-th power of the numerical value z A i,j and in this case e \u2208 {1, 2}. Utilizing the matrix expansion described above, we first define an intra-coupled interaction, which considers the correlations between the j-th feature and its own powers as follows:\n, and E is a maximal power.\nBesides the intra-coupled interaction, we also define an inter-coupled interaction that captures the correlations between the j-th feature and the powers of other features as follows: . Note that we use both the training and testing samples in inter-coupled interaction estimation for robustness by taking advantage of the information from testing samples, but the testing samples are not further involved in the following steps, i.e., feature selection and classifier learning.\nWe integrate the intra-coupled interaction R A a (j) and inter-coupled interaction R A b (j) to obtain the coupled feature representation of the j-th feature for the i-th sample as follows:\nwhere and \u2297 denote, respectively, a Hadamard product and a matrix multiplication. Therefore, the final coupled \nfeature representation for the i-th sample can be represented as follows:\nWe then apply a feature selection method to focus on only the label-related features. Specifically, we use a LAS-SO method [17] that selects features with a sparsity constraint in an objective function. In the following, we denote the dimension-reduced coupled-feature representation of the modality A for the i-th sample as x A i ."}, {"section_title": "Coupled Boosting", "text": "In a nutshell, our coupled boosting algorithm follows the general steps of AdaBoost [3] : iteration of (1) drawing training samples and (2) learning a base learner and determining the respective weight function.\nLet T denotes the total number of iterations, and h , where m is the number of training samples in our dataset.\nAt the t-th iteration, we draw samples according to the weight distributions and use them to train our base learners. After training base learners of h A t and h B t , we then measure the errors over the total m training samples as follows:\nwhere h Therefore, the core problem in our coupled boosting is the way of updating the weight distribution functions. For a binary classification problem with two modalities, there exist four different cases (see Table. Here, it is clear that we need to increase the weights of the samples belonging to Case 2, Case 3, and Case 4 compared to the samples belonging to Case 1 for the next round.\nFirst, for the incorrectly classified samples, we apply the following rule (similar to AdaBoost):\nwhere \u03b1 , we impose the pairwise coupleddiversity to strengthen the generalization power, which has been mathematically validated in [6, 13] . Formally, we denote the pairwise coupled-diversity function G(\nwhere \u03b4(\u00b7) is a Dirac Delta function. By combining the functions of Eq.(4), Eq. (5) and Eq.(6), we define the following weight factors:\n). Then, our new weight distributions can be estimated as follows: (11) After T iterations, we can finally get an ensemble classifier H(x A , x B ) that makes a final decision via weighted majority voting on all the base learners as follows:\n(11) where sgn(\u00b7) is a sign function. We present the pseudo-code of the proposed coupled boosting method in Algorithm.1."}, {"section_title": "Experimental Results and Analysis", "text": ""}, {"section_title": "Experimental Setup", "text": "We conducted experiments on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset 4 , which has been considered as the benchmark database for performance evaluation of various methods for AD/MCI diagnosis. In our experiments, we used baseline MRI and PET data obtained from 202 subjects of 51 AD patients, 99 MCI patients 5 , and 52 healthy Normal Controls (NC).\nFollowing the related works [21, 26] , we considered two binary classification tasks: AD vs. NC and MCI vs. NC. We employed four different metrics, namely, classification ACCuracy (ACC), SPEcificity (SPE), SENsitivity (SEN), and Area Under Receiver Operating Characteristic (ROC) Curve (AUC) to compare the proposed method with the previous methods. Particularly, ACC is calculated as the number of correctly classified testing samples divided by the total number of testing samples. SPE means the proportion of correctly classified NC samples. SEN means the proportion of correctly classified patient samples. Regarding the four metrics under consideration, the higher the values are, the better the respective method is. Due to a limited number of samples, we used a 10-fold Cross Validation (CV) technique in evaluating the performance, and repeated the 10-fold CV 100 times to reduce the possible bias that could be raised during data partition. For the base learner in our coupled boosting, we used a linear SVM or a Sparse Representation-based Classifier (SRC), which are the most widely used classification methods in many real applications [15, 24, 26] . The model parameters of the base learners were determined by a nested CV on the training samples. Instead of considering the full combination of parameters which is computationally very expensive, we chose the parameters for each base learning separately that helps reduce the computational cost greatly. The sparsity control parameter in LASSO, and \u03bb A t and \u03bb B t in Eq. (7) and Eq. (8) were also chosen by a nested CV on the training samples. Hereafter, we denote our Coupled Boosting (CB) with SVM or SRC as CB-SVM or CB-SRC, respectively."}, {"section_title": "Coupled Feature Representation Evaluation", "text": "To validate the advantage of the coupled feature representation, we compared the performances of CB-SRC trained on the Original Feature Representations (CBSRCwOFR) with those of CB-SRC trained on the Coupled Feature Representations (CB-SRCwCFR). We also compared results of CB-SVMwOFR with those of CB-SVMwCFR. In Fig.3 , we presented the performance changes of the competing methods by varying the power expansion value from 1 to 10. For all the methods, we applied LASSO on their respective feature representations.\nFor both AD vs. NC and MCI vs. NC classifications, the proposed CB-SRCwCFR and CB-SVMwCFR outperformed CB-SRCwOFR and CB-SVMwOFR, respectively, in terms of ACC, SEN, SPE, and AUC. Specifically, the proposed method of CB-SRC/CB-SVM improved by 6%/4% (ACC), 3.30%/1.2% (SEN), 11%/9.5% (SPE), and 6%/3.50% (AUC) for AD diagnosis, and by 4.5%/0.69% (ACC), 2.45%/0.77% (SEN), 9.53%/0.6% (SPE), and 4.43%/0.6% (AUC) for MCI diagnosis. That is, the coupled feature representation is helpful to improve the diagnostic accuracy for both AD vs. NC and MCI vs. NC classifications with the base learners of SRC and SVM.\nRegarding the expansion parameter E, a small E deteriorates the classification performance while a large E increases the unnecessary computation burden. In the following experiments, we fixed E to 5."}, {"section_title": "Feature Selection Evaluation", "text": "In order to show the efficacy of feature selection, we compared the performances of CB-SRC or CB-SVM without feature selection (called as CB-SRCwoFS or CBSVMwoFS) with those of the same methods with feature selection (called as CB-SRCwFS or CB-SVMwFS) in Fig.4 .\nIt is obvious that feature selection helped improve the classification results both for CB-SRC and CB-SVM (see Table. 2). "}, {"section_title": "Coupled Boosting Evaluation", "text": "If we set the parameters \u03bb A t and \u03bb B t to 0, then the proposed coupled boosting algorithm becomes the conventional AdaBoost by degenerating the weights for the inconsistently classified samples. In order to show the validity of the newly devised formulation for weight distributions, we compared the proposed method with AdaBoost, for which we also used SRC or SVM as a base learner (called as AdaBoost-SRC or AdaBoost-SVM) (see Fig.5 ). Although there exist cases that the proposed method showed lower performance, i.e., -0.7% (SEN) with CB-SVM in AD vs. NC, and -1.3% (SEN) with CB-SRC and -2.6% (SPE) with CB-SVM in MCI vs. NC, overall, our method was statistically superior to AdaBoost with p-values of 1.1e-17 (CB-SRC), 3.7e-05 (CB-SVM) in AD vs. NC, and 0.0061 (CB-SRC), 2.7e-12 (CB-SVM) in MCI vs. NC. Thanks to the pairwise coupled-diversity, the proposed methods of CBSRCwFS and CB-SVMwFS outperformed both AdaBoost-SRC and AdaBoost-SVM."}, {"section_title": "Comparison with Previous Methods", "text": "We also compared the classification performances of the proposed method with those of the state-of-the-art methods, namely, Wang et al.'s method [21] and Zhang et al.'s method [26] . For a comparison purpose, we also present the performance of four single-modality based methods: (1) SRC with MRI features (SRC-MRI), SRC with PET features (SRC-PET), SVM with MRI features (SVM-MRI), and SVM with PET features (SVM-PET). For SRC-MRI, SRC-PET, SVM-MRI, and SVM-PET, the parameters (e.g., the sparsity regularization parameter in SRC) in the respective models were determined by a nested CV on the training samples. [26] , we presented the best performances reported in the respective papers. Our method achieved the best performance for both AD vs. NC (by CB-SVM) and MCI vs. NC (by CB-SRC) classifications. Regarding AUCs, we repeated the 10-fold CV 100 times to reduce the possible bias caused by different data partition, and then obtained the mean AUC value. Our best AUCs were 0.975 (AD vs. NC) and 0.833 (MCI vs. NC), better than results in [21] . It is noteworthy that (1) coupled features assigned the lower weight for the higher power features, thus preventing the possible over-fitting caused by complex high power representations and (2) boosting-like algorithm is a wellknown classifier which often does not overfit data by increasing the ensemble margin according to several empirical and theoretical studies. In this regard, we can say that our experimental results were not suffered from overfitting."}, {"section_title": "Conclusion", "text": "Recently, by addressing the AD/MCI diagnosis process as a classification problem, most of the previous methods assume the conditional independence among the low-level features extracted from neuroimaging data. However, since a human brain is a complex system in which different regions interact for cognitive tasks, it is obvious that the features are naturally correlated to each other. Furthermore, there also exists relational information between different imaging modalities such as MRI and PET.\nIn this paper, we devised a coupled feature representation with intra-coupled and inter-coupled interaction relationship by means of a matrix expansion. Regarding multimodal data fusion, we proposed a novel coupled boosting algorithm that analyzes the pairwise coupled-diversity correlation between modalities. Specifically, we formulated a method of updating weight distribution functions, which jointly considered both incorrectly and inconsistently classified samples. From our experiments on the publicly available ADNI dataset, we validated the effectiveness of the proposed method both on the AD vs. NC and the MCI vs. NC diagnosis by comparing with the competing methods and the state-of-the-art methods. "}]