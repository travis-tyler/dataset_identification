[{"section_title": "Abstract", "text": "Abstract-This paper presents a novel dimensionality reduction method for classification in medical imaging. The goal is to transform very high-dimensional input (typically, millions of voxels) to a low-dimensional representation (small number of constructed features) that preserves discriminative signal and is clinically interpretable. We formulate the task as a constrained optimization problem that combines generative and discriminative objectives and show how to extend it to the semi-supervised learning (SSL) setting. We propose a novel large-scale algorithm to solve the resulting optimization problem. In the fully supervised case, we demonstrate accuracy rates that are better than or comparable to state-of-the-art algorithms on several datasets while producing a representation of the group difference that is consistent with prior clinical reports. Effectiveness of the proposed algorithm for SSL is evaluated with both benchmark and medical imaging datasets. In the benchmark datasets, the results are better than or comparable to the state-of-the-art methods for SSL. For evaluation of the SSL setting in medical datasets, we use images of subjects with mild cognitive impairment (MCI), which is believed to be a precursor to Alzheimer's disease (AD), as unlabeled data. AD subjects and normal control (NC) subjects are used as labeled data, and we try to predict conversion from MCI to AD on follow-up. The semi-supervised extension of this method not only improves the generalization accuracy for the labeled data (AD/NC) slightly but is also able to predict subjects which are likely to converge to AD."}, {"section_title": "", "text": "activation map [6] . VBA therefore identifies regions in which two groups differ (e.g., patients and controls [7] ) or regions in which other variables (e.g., disease severity [8] ) correlate with imaging measurements. However, VBA has limited ability to identify complex population differences because it does not take into account multivariate relationships in data [9] [10] [11] [12] . In other words, values of voxels or regions of interest (ROIs) showing significant group difference are not necessarily good discriminatory factors at the patient-level.\nIn order to overcome these limitations, high-dimensional pattern classification methods have been proposed in recent literature for morphological analysis [13] [14] [15] [16] and fMRI [9] , [17] , [18] , which aim to capture multivariate nonlinear relationships in the data and seek to achieve high classification accuracy at the individual level. A fundamental limitation in these methods, however, is the lack of sufficient training samples relative to the high dimensionality of the data. Therefore, a critical step underlying the success of such methods is effective feature extraction and selection, i.e., dimensionality reduction. Our main objective in this paper is to propose a dimensionality reduction method that finds a parsimonious set of image features for the sake of a better representation of group difference, best differentiates between two or more groups, and generalizes well to new samples.\nDimensionality reduction methods can be categorized into two groups: generative (typically unsupervised) and discriminative (typically supervised) methods. One of the most wellknown unsupervised dimensionality reduction methods is principal component analysis (PCA). PCA results are often hard to interpret since PCA does not specifically attempt to identify localized brain regions, instead capturing global correlations. More generally, unsupervised methods often focus on irrelevant variations in the data and do not yield the best performance if the main objective is discrimination. On the other hand, supervised methods like Fisher discriminant analysis (FDA) and feature selection methods have been recently applied for medical image analysis [14] , [15] , [19] . Similar to PCA, FDA may not be able to identify localized abnormal brain regions; in the medical imaging context, the ability of a method to provide an interpretable model is important. Feature selection methods, on the other hand, produce regions that are potentially interpretable. However, reducing the dimensionality to a small number of features comparable to the typical number of labeled samples can diminish discriminative ability since individual features are very noisy.\nTo address these issues, we propose a method that combines generative and discriminative approaches and bridges between feature selection and feature construction. Recently, there has been much interest in the machine learning community in fusing generative and discriminative perspectives of learning [20] . The computer vision community has adopted this approach for var-0278-0062/$26.00 \u00a9 2011 IEEE ious purposes ranging from object recognition [21] to image scene classification [22] . For the hybrid generative-discriminative method proposed here, we have adopted a constrained matrix factorization framework. The proposed method jointly finds a matrix decomposition and a classifier that uses the decomposition for feature extraction. The data matrix is factored into a basis and coefficient matrix, and the classifier uses projection coefficients of the samples on the basis as new features for prediction. The basis matrix is encouraged to possess two properties. 1) The basis vectors should be anatomically meaningful. That is, they should correspond to anatomical regions preferably in areas which are related to a pathology of interest.\n2) The basis vectors must be discriminative: we are interested in finding features, i.e., projections onto the basis vectors, that construct spatial patterns that best differentiate between groups. We formulate this decomposition as an optimization problem that seeks to satisfy the two criteria above. The discriminative property of the decomposition is enforced by the joint learning of the classifier and interpretability is encouraged through sparsity and nonnegativity. The contributions of the paper are the following.\n\u2022 We propose a novel generative-discriminative approach well-suited to medical imaging applications (Section II-B and II-C). In addition to the nonnegativity and sparsity constraints used in previous work [23] , [24] , we introduce a new type of constraint (group-sparsity) that allows further anatomical coupling between voxels defined by a segmentation (II-D).\n\u2022 In order to solve our large-scale optimization problem, we propose an efficient, scalable algorithm using a novel closed-form projection onto the constraints.\n\u2022 We extend our approach to the semi-supervised learning setting applicable for group analysis in medical imaging, particularly when images do not have class labels either because the labels are not provided or are hard to define. A large numbers of experiments were conducted to evaluate the practical merit of the proposed method on real and simulated datasets and also to clarify effects of various terms on the accuracy and clinical interpretability of the proposed method.\nThe remainder of this paper is organized as follows. In Section II, we detail three important components of the optimization problem, namely the generative term, discriminative term, and constraints. We will also describe the proposed algorithm for efficient optimization in Section II. In Section III, experimental results on some clinical datasets are provided. Discussions and conclusion are left to Section IV."}, {"section_title": "II. METHOD A. General Framework", "text": "We adopt a regularized matrix factorization framework for our purposes. In regularized matrix factorization, the objective is to decompose a matrix into two or more matrices subjected to some constraints or priors such that the decomposition describes the matrix as accurately as possible. Assuming that each column of represents an observation (i.e., a sample image that is vectorized), the columns of matrix can be viewed as basis vectors and the th column of contains corresponding loading coefficients of the basis vectors for the th observation (1)   TABLE I  THIS TABLE SHOWS EXAMPLES OF WELL-KNOWN METHODS THAT CAN  BE VIEWED AS MATRIX FACTORIZATION: SVD, -MEANS/MEDIANS,  PROBABILISTIC LATENT SEMANTIC INDEXING (PLSI), NMF. IN THE TABLE,  DENOTES FROBENIUS NORM AND IS A DIAGONAL MATRIX in which is decomposed into two matrices and , each of which has its own feasible set, and , respectively. This framework will be elaborated in the sequel, but it is important to note that regularized matrix decomposition is a rich framework and many well-established methods can be viewed as its variants. Table I represents some examples of well-known methods that can be described by (1) (for more examples, see [25] ). In Table I , represents the divergence term between the reconstruction and the data which will be explained in Section II-B and denotes Kullback-Leibler divergence [26] .\nIn order to define the feasible sets in (1), we need to elaborate the requirements that our model should satisfy. 1) The basis vectors must be anatomically meaningful. This means that a constructed basis vector should correspond to contiguous anatomical regions preferably in areas which are biologically related to a pathology of interest. Having local spatial support can be viewed mathematically as sparsity of a basis vector, i.e., a relatively small number of nonzero voxel values.\n2) The basis must be discriminative: we are interested in finding features, i.e., projections onto the basis vectors, that construct spatial patterns which best differentiate between groups, e.g., patients and controls or activation and baseline.\n3) The basis vectors must be representative of the data as much as possible, while maintaining their discriminatory ability. In order to represent the data, we derive a basis matrix, the columns of which satisfy aforementioned properties, and loadings of the samples on those basis vectors . In subsequent sections, we will introduce appropriate priors that encourage the aforementioned properties, but we first lay out our framework. This framework is represented in Fig. 1 as a graphical model. Let us assume that we collect an image into a column of matrix , therefore a column represents one sample image whose label (class) is represented by . For example, can be the determinant of Jacobian of a deformation field that warps a subject to a common template (see Section III), a tissue density map representing region volume (see [28] and [2] ), or fMRI of an activation task. Assuming that each image consists of voxels that concatenated together in lexicographical order, each column of is a -dimensional vector. If the dataset includes samples, matrix is a matrix. are assumed to reside in the positive quadrant (in most cases, images, or determinant of Jacobian of diffeomorphic transformation derived from them, are nonnegative). The goal is to decompose the data, , into a matrix , which is a matrix whose is the th sample (out of samples) and is the corresponding class label. is the th basis vector (out of basis vectors) and is the loading coefficient for the th sample; parametrizes the class-likelihood, i.e.,\n; in other words, it parametrizes the classifier. Since samples and corresponding labels are observed variables, they are shaded with gray while unobserved variables (i.e., , and ) are white. (b) shows the same idea as a matrix factorization;\n, and are columns of , and , respectively.\ncolumns are optimized basis vectors, and a loadings matrix , which holds corresponding loadings of the basis vectors, namely . At the same time the basis representation is used to predict the labels using as we describe below, thus trading off generative and discriminative criteria. Without additional constraints, the decomposition is ill-posed and has infinitely many solutions; hence regularization is necessary. Given conditional independence depicted in Fig. 1 , we formulate the problem as a maximum a posteriori (MAP) estimation problem as follows: (2) in which is a vector that parametrizes class-likelihood , or, in other words, it parametrizes a classifier that will be explained later (Section II-C). Instead of maximizing the logarithm of the posterior, we can minimize the negative of the logarithm of the posterior that yields (3) in which the first term is a divergence term that encourages good data approximation, which will be referred to as the generative term. The second term is a loss function that encourages good classification, which will be referred to as the discriminative term. The last term in the objective of (3) is a combination of prior terms on , and ; due to conditional independence assumed in our model (Fig. 1) , this term can be decomposed into addition of priors over each of them. Observe that in (3) feasible sets of and are added for future reference; this perspective is consistent with (2) because every constraint can be transformed to a prior by imposing an infinite cost for points outside the feasible set and zero for points inside the feasible set.\nWe will describe each term in detail in the subsequent sections, but before that we introduce some examples of well-known methods in Table I that can be viewed as regularized matrix decomposition and can be formulated as (3) . Note that the examples in Table I are all generative methods, hence , and consequently its feasible set, , is omitted."}, {"section_title": "B. Generative Term", "text": "In this section, we will explain (the generative term) that measures the divergence between the data and its decomposition in the basis vectors (columns of ). Various divergence choices can model different noise assumptions between the reconstruction by and and observation . Since we have adopted a matrix decomposition framework, the reconstruction is performed via matrix multiplication namely . We assume Gaussian noise between observation and reconstruction , i.e., , the divergence term becomes (4) Observe that the divergence term is a convex function with respect to if is fixed, and vice-versa, but it is not jointly convex with respect to both and . Other assumptions of noise between observation and reconstruction, e.g., Poisson, can be modeled by various choices for the divergence term, e.g., Kullback-Leibler (KL) divergence [26] ."}, {"section_title": "C. Discriminative Term", "text": "The idea behind the discriminative term is to encourage discriminative basis vectors; i.e., if an image, , is projected on basis vectors yielding new features, , the latter should be discriminative. In other words, for new features , there exists a classifier parametrized by, say , that minimizes a loss function, , for an optimal set of parameters . In this paper, we use a linear classifier, namely where represents inner product and entries of are new features after projection.\nIdeally, can be written as a projection operator acting on to project it on the subspace spanned by . However, in this paper we set or, in matrix notation, . It is not a proper projection unless the basis vectors are orthonormal; nevertheless, as it will become clear in the next section, due to the positivity constraint and the fact that basis vectors act like indicator functions, is proportional to the weighted sum of features in a nonzero area of a basis vector, which is the quantity we are interested in using as new features. Therefore, the classifier function is (5) in which is an image concatenated into a -dimensional vector and is a vector with same dimensionality as the number of basis vectors. In fact, reduces the dimensionality from to . is linearly related to the classifier, , because of computational reasons; more specifically, becomes convex with respect to when is fixed. The loss term penalizes misclassification of data by comparing estimated classification with class labels, . Many choices are possible for the loss function in SVM; in this paper, we choose the squared hinge loss function, namely . This loss function is chosen due to differentiability. Therefore, the loss function of all samples can be written as follows: (6) Other possibilities for the loss function (e.g., logistic, hinge, etc.) are not investigated in this paper. For more diverse choices of the loss function, please see [29] and references therein."}, {"section_title": "D. Priors", "text": "In this section, we discuss regularization terms for , and . We choose a simple for , namely similar to -SVM [30] . The rationale behind using this type of regularization for is similar to that of -SVM. It can be shown [30] that adding this regularization for SVM encourages a linear classifier in the feature space that maximizes the margin between two classes and the decision boundary while minimizing the loss function. Another common option for regularization of is -norm [29] that favors a sparser (or fewer features). However, given that the basis vectors, , have already reduced the dimensionality significantly from to , a sparse is not preferable in this paper.\nFor , we simply impose a nonnegativity constraint. Lee et al. [23] demonstrated that nonnegative matrix factorization (NMF) is able to learn parts of faces and semantic features of text. NMF is distinguished from the other factorization methods, e.g., PCA and vector quantization (VQ) which learn holistic but not parts-based representations, by its use of nonnegativity constraints that leads to a parts-based representation because it allows only additive, not subtractive, combinations (this idea is intuitively represented in Fig. 2 1 ) . Donoho et al. [32] showed that under certain conditions, basically requiring that some of the samples are spread across the faces of the positive orthant, result in a unique decomposition.\nFor , we define two types of regularizations: boxed-sparsity and group-sparsity.\nBoxed-Sparsity: We would like to encourage basis vectors that act like indicator functions. Mathematically speaking, we would like the elements of to be either 0 or 1, namely . In addition, we are interested in finding localized basis vectors for two reasons: it increases robustness and interpretability of basis vectors. The sparsity constraint promotes the indicator functions that select subsets of voxels. The -norm, which counts number of nonzero entities in a vector, can be used as a regularization or constraint in order to encourage or bound sparsity. In this paper, we prefer to use sparsity as a constraint. Hence, a basis vector should reside in the intersection of two sets: the set of indicator functions and the set of sparse vectors, which can be written mathematically as follows:\nwhere is a constant that defines the level of sparseness and is the number of basis vectors. However, this constraint is combinatorial in nature, hence difficult to optimize. In the context of machine learning [33] and optimization [34] , the integer and constraints are relaxed with their convex surrogates where denotes a relaxation and shows equivalence, is the -norm of a vector which is a convex relaxation of its -norm and is an element-wise inequality constraint. Geometrically, each basis vector, , dwells in the intersection of the -norm ball of radius with unit -norm ball (box) in the positive orthant, which is shown graphically in Fig. 3 for for sake of illustration. We call the feasible set the boxed-sparsity set, in contrast to a feasible set to be defined subsequently.\nGroup-Sparsity: Another interesting prior on arises when a partition is available and needs to be taken into account. We assume a common coordinate system by warping all images to a template and an image partitioning (image segmentation) is available for the template image (e.g., an anatomical parcellation in a template space). It is possible to consider sparsity constraint/regularization on the group-level rather than voxel level which promotes that a few groups (e.g., brain structures) are involved in group difference rather than a few voxels. In order to encourage this property, we can enforce an -norm on groups instead of voxels. Before defining the idea precisely, we need a few definitions. Assuming is a segmentation of an image into sets ( ), we can define two group-norms as follows (the idea is graphically shown in Fig. 4 ): (8) where is a -dimensional vector such that its voxels not belonging to the group are set to zero, is a positive constant that in this paper compensates for a group-size, namely where is cardinality of a set. Notice that in the definition of , the -norm is used instead of because the squared norm does not have the sparsifying properties. This kind of regularization is called group regularization or mixed-norm regularization and have received much attention in recent years in machine learning [35] , [36] .\nGiven the new norm definitions in (8), we can define the group-sparsity constraint mathematically as follows: (9) For the rest of the paper, we will refer to subject to the constraints as group-sparsity. Observe the correspondence between Boxed-and group-sparsity; in (9) replaced and exchanged for ."}, {"section_title": "E. Optimization", "text": "Given the generative term (4), the discriminative term (6), and the regularization on , on , and (7) or (9), we form an optimization problem as follows: (10) where and are given in (4) and (6) respectively and is either the boxed-sparsity constraint in (7) or the group-sparsity in (9) ; and are relative weights to control importance of the three terms in the objective function;\ndepends on the definition of sparsity, i.e., if the boxed-sparsity is chosen replaces in (7) or if the group-sparsity is selected it substitutes in (8) . The ratio controls the discriminative power versus the generative power of the model: the higher the ratio, the more discriminative the model. Throughout the experiments, and are normalized by the number of samples [i.e., ] and is normalized by the dimensionality of the images [i.e.,\n]. Therefore, we report as a percentage value that means is some percentage of voxels. Note that the objective in (10) , is comprised of three terms; thus, two regularization weights suffice to control the relative ratio of the terms.\nAlthough this optimization is not jointly convex with respect to all variables, it is a block-wise convex program; i.e., if any pair of blocks of variables is fixed, it is a convex optimization problem with respect to the other block. For example, if and are fixed, it is a convex optimization problem with respect to . Therefore, we propose a block-wise optimization scheme shown in Algorithm 1 that converges to a local minimum. Proof of the convergence to a local minimum follows from the fact that the optimization problem is convex with respect to each block of variables, the objective is lower-bounded and continuous on the domain, and nondifferentiable constraints can be added as separable terms to the objective ([37, Prop. 5.1] for more detail).\nThe optimization is straightforward with respect to two of the blocks ( and ) but challenging with respect to the others that will be discussed in detail subsequently."}, {"section_title": "Algorithm 1 Block-Wise Optimization", "text": "Require: Data , Labels , Regularization ( ) initialize repeat [ (13) or (14)] (12) ( 11) until some convergence criteria satisfied"}, {"section_title": "1) Optimization With Respect to :", "text": "We start with the most straightforward block. In the th iteration, fixing and , the optimization should find the global minimum of the following convex function: (11) in which is the loss function defined in (6) . Solving this optimization problem with respect to is not challenging because it is basically a linear SVM classifier with regularization applied on new features, namely . Any off-the-shelf solver for a linear SVM can solve (11) efficiently in a reasonable time because computational complexity of such a solver is bounded by the number of new features and number of samples , which are not large in our application. In this paper, we use LIBLINEAR [29] as the solver.\n2) Optimization With Respect to : Fixing and in the th iteration, we need to find the global optimum of the following objective with respect to (12) This problem can be easily formulated as a nonnegative least squared problem with variables. Given that is not typically large in medical imaging applications and is also not large, any off-the-shelf least squared solver can solve this problem. There is an abundant supply of options for nonnegative least squared solvers. We used MOSEK [38] to solve this problem.\n3) Optimization With Respect to : Fixing and in the th iteration, a constrained convex programming problem needs to be solved to find optimal . In the case of boxed-sparsity, the following problem needs to be solved: (13) In case of group-sparsity, the objective of the optimization problem is as follows: (14) where was defined earlier in (8) . While (13) is a constrained quadratic programming, (14) is a second-order cone programming (SOCP) [34] ; nevertheless, solving either case poses a challenge due two reasons: 1) highdimensionality: for both cases, the number of variables is at least (number of voxels by number of basis vectors) plus variables introduced by the nondifferentiability of the constraints or objective, and 2) constrained programming subject to a nonsmooth feasible set. In general, constrained optimization is more expensive to solve than unconstrained optimization problem.\nProjected gradient (PG) [39] is a first-order method that can be used for a constrained problem. However, PG can be slow particularly for nonsmooth feasible sets. The newton method is used to accelerate first-order solvers [39] . The interior point (IP) method is a variant of the Newton method for a constrained problem [34] . However, the IP method implemented naively fails to solve (13) or (14) because IP involves computation and inversion of a Hessian matrix which is prohibitive in term of computation and memory costs. In our experiments, more sophisticated implementations like MOSEK [38] fail to find a point in the feasible set in a reasonable time. Our chosen alternative is use to use spectral projected gradient (SPG) [40] that is a modification of the classical PG method which differs in two essential ways. 1) It uses a nonmonotone line search that measures descent with respect to a fixed number of previous iterations instead of just the last iteration. This may lead to a temporary increase in the objective while ensuring overall convergence.\n2) It uses spectral step length introduced by Barzilai-Borwein (BB) [41] that gives an initial step length. In the BB approach, the step length in th iteration is chosen such that mimics the Hessian of the objective over the most recent step. Similar approaches have been taken recently by Schmidt et al. [42] and Wright et al. [43] for large-scale nonsmooth problems. There are several choices for BB step length [44] , in this paper, we choose the following method to compute it [45] ( 15) where is an operator that reorders elements of a matrix into a vector. We omitted the detail of computation of the gradient of the objective here, for more detail, see Appendix A."}, {"section_title": "Algorithm 2 Spectral Projected Gradient Solver", "text": "Require: Initial point, step-length bounds repeat while do\nChoose with quadratic interpolation [46] end while compute BB step-length (15) until some convergence criteria satisfied\nOur proposed algorithm is shown in Algorithm (2). It is conceivable that the bottleneck of the algorithm is the projection because it should be performed in each iteration. One of the technical contributions of this paper is to suggest an efficient way to perform the projection; see Appendix B for more detail."}, {"section_title": "F. An Extension: Semi-Supervised Learning", "text": "Semi-supervised learning refers to a class of machine learning techniques that simultaneously use both labeled and unlabeled data for training in settings in which a small amount of labeled data and a large amount of unlabeled data are available. Semi-supervised learning combines elements of unsupervised and supervised learning.\nIn many medical imaging applications, such situations arise either due to the availability of abundant sample images with no labels, or more importantly due to uncertainty about the labels. For example, recent studies have shown that individuals with mild cognitive impairment (MCI 2 ) tend to progress to Alzheimer's disease (AD) [47] ; but not all MCI subjects converge to AD. Recently, several methods have been proposed to address this issue. Sabuncu et al. [48] and Blezek et al. [49] proposed different frameworks for joint image registration and clustering that can exploit unlabeled images. Ribbens et al. [50] suggested a probabilistic method that can incorporate prior clinical information. 2 MCI is viewed as an intermediate stage between normal aging and Alzheimer's disease (AD).\nIn case of semi-supervised learning in our method, some subjects have certain labels (denoted by ) and some subjects do not have labels (denoted by ). In other words, the data matrix can be partitioned into two submatrices, namely . Our generative-discriminative framework can easily handle such cases. Recall the objective function of the optimization problem in (10); it was decomposed into three terms: generative term , discriminative term , and regularization term (recall that the constraint can be written as regularization).\ncontributes in both generative and discriminative terms while only contributes in the generative term, namely (16) in which is introduced to simplify the notation by grouping all parameters into denotes the objective function, stands for the regularization terms. Equation (16) shows that unlabeled samples are not penalized in the discriminative term (the second term) because the true labels are not available for them. This setting will be investigated in Section III."}, {"section_title": "G. On Selection of the Regularization Parameters", "text": "To set values of the parameters (i.e., and ), two strategies are available. First, to embed searching for the best parameters as a part of the training of the algorithm. This strategy is chosen to show the results in this paper. Second, to set values of the parameters to predefined values which are presumed to perform well. Ideally, the first option is preferred because it potentially yields better performance than setting parameters to predefined values, however, the large optimization with respect to renders searching an expensive task. Although the latter strategy is not investigated in this paper, we will give intuition on how to select parameters to some fixed values.\nParameters of the proposed algorithm are as follows: number of basis vectors; , the weight for the generative term;\n, the weight for the discriminative term; , the sparsity ratio for the basis vectors. We propose to choose the parameters in the following order.\n1) : Given (11) and (6), it can be readily derived that defines the weight for the second term in (11) . One suggestion is to run the algorithm for a small-scale dataset for a few iterations and choose such that it produces a reasonable classification rate. One can even run the algorithm for a few iterations without the discriminative term and extracts feature (i.e., ) in order to have a sense of an appropriate range for . 2) and : Selection of can be inspired by our clinical hypothesis; approximately sets the nonzero ratio of each basis vector. Depending on our clinical expectations regarding portion of an anatomy (e.g., brain) affected by the disease of interest, we can choose a range for . However, if sparseness is set to a high value (low ), the generative term may not be able to represent the data well because it may not be able to cover the whole domain of images; hence, optimal basis vectors may stay away from the boundaries of the feasible set (where basis vectors achieve 0-1 values) while the model may try to compensate with to reconstruct the data. In fact, there is a limited budget to reconstruct the data. In order to increase the budget, one can increase the number of basis vectors . However, a very large value of increases the computational cost significantly, so one needs to trade off between excessive sparsity and computational cost. There are also other factors involved in choosing the sparsity ratio that will be discussed in Section III-B.\n3)\n: Once other parameters are set, we can set a value for . The ratio decides the balance between the generative and the discriminative terms; since is already set, one needs to choose the ratio of . As it will be shown in Section III-A, the algorithm is relatively robust with respect to ratio of as long as is in a reasonable range; hence the value of should be chosen such that the first and second terms in (13) have similar magnitude."}, {"section_title": "III. EXPERIMENTS", "text": "In this section, we conduct several experiments with the proposed method on various data sets and different settings. In the first set of experiments, we will investigate the effect of generative-discriminative trade-off on generalization power of features used for classification. We will also explore the sparsity effect with both definitions of sparsity. The methods will also be compared to other established methods in the literature. We also briefly examine the potentials of the proposed method for semi-supervised learning with both definitions of sparsity for medical imaging datasets. At the end, we investigate effect of the parameter selection on the accuracy rates on datasets that are held out from previous experiments."}, {"section_title": "A. Generative Versus Discriminative Trade-Off", "text": "The images used in this experiment are structural MR brain images ( image) obtained from Alzheimer's Disease Neuroimaging Initiative (ADNI 3 ). 63 normal control (NC) individuals and 54 AD patients were preprocessed via the same preprocessing pipeline. The preprocessing pipeline is designed according to previously validated and published techniques by Goldszal et al. [28] . It includes the following steps: 1) alignment of images to the AC-PC plane; 2) removal of extra-cranial material (skull-stripping); 3) tissue segmentation into gray matter (GM), white matter (WM), and cerebral fluid (CSF), using a brain tissue segmentation method proposed in Pham et al. [51] ; 4) nonrigid image warping using the method proposed by Shen et al. [52] to a standardized coordinate system, a brain atlas (template) that was aligned with MNI coordinate space [53] ; 5) formation of regional volumetric maps, named RAVENS maps (see [28] and [2] ), using tissue-preserving image warping [28] . RAVENS maps quantify the regional distribution of a GM, WM, and CSF, since one RAVENS map is formed for each tissue type. A RAVENS map quantifies an expansion (or contraction) of the tissue modeled by a transformation that warps the image from the original space to the template space. Consequently, voxel 3 http://www.loni.ucla.edu/ADNI values of a RAVENS map in a template space are directly proportional to the volume of the respective structures in the original brain scan. Although this map can be formed for CSF, WM, and GM, we only used maps corresponding to the GM tissue type. An example of GM, WM, and ventricle RAVENS map is shown in Fig. 5 .\nIn order to investigate the effect of the hybrid generative-discriminative model, we modified the ratio for various numbers of basis vectors . In this experiment, Boxed-Sparsity was used as the sparsity regularization and was set to 20% (i.e.,\n). The number of basis vectors was chosen from set of to examine robustness of the algorithm to different numbers of basis vectors. As mentioned earlier in the methods section, the proposed algorithm can be viewed as a dimensionality reduction from an original large dimension to smaller but more discriminative and representative dimensions ; hence so-called projection can be viewed as feature extraction. While the original dimension may be too large to apply a nonlinear classifier on, we can simply apply a classifier (in this experiment Logistic Model Trees [54] 4 ) on the extracted features ( -dimensional instead of -dimensional) to boost the performance. For each setting, i.e., a particular ratio of and number of basis vectors , data was split into ten-folds; training including learning and training a classifier on the extracted features , was conducted on nine-fold and the test was carried on the remaining fold. This process was repeated 10 times to compute an average classification accuracy; hence, each point in Fig. 7 is the ten-fold cross-validation accuracy. Results are shown in Fig. 7 . In order to avoid occlusion of the Fig. 7(a) , error-bars (i.e., standard deviations of the accuracy rates) are added as a separate figure  [Fig. 7(b) ].\nIn Fig. 7 , as number of basis vector increases, the accuracy rates also increase but they reach a plateau around . An excessively discriminative model (yellow and violet corresponding to and respectively) becomes more unstable as the number of basis vector increases while the blue graph, in which the generative term dominates, is quite stable. Increasing the number of basis vectors further, not only increases computational cost drastically but also degrades generalization of the model because of high dimensionality, since the number of samples is of the same order of magnitude (in this experiment ), so we set the maximum number of basis vectors to 50 which is in the same order magnitude. The best performance is shown by red line that maintains a balance between the generative and discriminative terms. This graph shows that having the generative term helps to create more stable classification rates. It also shows that unless the algorithm is pushed too much toward the discriminative side, it is fairly robust with respect to choice of parameters; for example for , perturbations in classification accuracy rates are about 6% for a reasonable range of (i.e., around 0.01 and 0.1 for this data). Notice that in this cross validation process, every fold contains few samples (between 11 to 13 samples) and 7%-9% missclassification is about one miss classification per fold. "}, {"section_title": "B. Sparsity Effect", "text": "In the previous section (Section III-A), the boxed-sparsity was used and the ratio of was set to 20%. Given that a large portion of images are dark background, it is a reasonable value. In this section, we investigate different sparsity types (Boxed-and Group-) for different values of while keeping number of the basis vectors to a constant that shows roughly the best performance in Fig. 7 . Fig. 8 shows a basis vector as in Fig. 6 (a) and (b) but with stronger sparsity constraint % to illustrate sparsity effect. It shows more localized areas than those of Fig. 6(a) and (b) . Decreasing which enforces stricter sparsity constraint (say %) may not be helpful for better representation because as decreases, the algorithm has a limited budget of voxels (i.e., few voxels can be selected) to satisfy the generative term ; therefore it prefers to push values of the voxels away from boundaries (i.e., ) to satisfy the generative term. Nevertheless, we changed in range of to examine its effect on the classification accuracy (Fig. 9) . The experiment elaborated in Section III-A is repeated but for different values of and . The settings of the experiment in term of number of samples and preprocessing is identical with those of the experiments in Section III-A. Fig. 9 shows comparison of different ratios of for the boxed-sparsity for different rates of . Since two types of behaviors are observed, they are shown in two separate graphs for a sake of illustration. Fig. 9(a) shows cases in which the generative term is dominant or moderate while Fig. 9(b) shows graphs in which the discriminative term is dominant.\nIn Fig. 9(a) , increasing (less sparse) slightly improves level of classification accuracy up to a certain point ( depending on the ratio ) because it yields better reconstruction. However, from that point on, it decreases because it means less regularization on the model. Nevertheless, if the generative term is dominant, the algorithm is relatively robust. Fig. 9(b) shows similar graph for the cases in which the discriminative term is dominant or has relatively higher weight than those of Fig. 9(a) . In this case, increasing (decreasing sparsity) deteriorates the classification accuracy. When the discriminative term is dominant, reducing sparsity can approximately be compared to -SVM with small regularization weight; excessive reduction of the regularization weight in -SVM can worsen generalization of the classifier. Fig. 10 shows an example of a basis vector when group-sparsity is used. The feasible set of the group-sparsity is smoother than that of the boxed-sparsity (Fig. 8) ; in other words, it has fewer sharp corners than the boxed-sparsity one. This encourages solutions that are smooth, i.e., voxel values are likely to be in (0,1) rather than 0 or 1. Nevertheless such behavior is also affected by -norm of the samples (i.e., normalization of samples) that are not discussed in this paper in interest of space. Fig. 11 , depicts the same graphs as Fig. 9 but for group-sparsity regularization. As in Fig. 9 , the graphs are divided into two (generative-or discriminative-dominant) sub-graphs for a sake of better illustration. In term of maximum accuracy, the group-sparsity is comparable with the boxed-sparsity (about 3% improvement) but it is more robust with respect to change of parameters; Fig. 10(a) shows perturbation is accuracy that is about 5% across different settings. In Fig. 11(b) , the group-sparsity shows significantly more robust behavior when the discriminative term is dominant comparing to Fig. 9(b) . Such robustness can be explained by definition of the group-sparsity regularization. Due to the nonlinear relationship within each group, group-sparsity imposes fewer degrees of freedom than those of boxed-sparsity, therefore it regularizes the objective further. Fig. 11 (b) also shows that a reasonable range for group-sparsity is around which is different that of the boxed-sparsity; the accuracy rates slightly degrade after this range."}, {"section_title": "C. Comparison With Other Methods", "text": "In this section, we compare performance of the proposed algorithm with other methods but first we need to clarify some points about parameter selection ( ). The dataset is divided into 20 splits, 18 splits are used to learn and the testing accuracy on one of the two left-out splits is used to search for the best and finally the classification accuracy is reported on the other left-out split.\nTable II compares the accuracy rates between five different methods (two of them are variants of the proposed method) on two dataset. and stand for the proposed for Boxedand group-sparsity constraints, respectively. SVD and nonnegative matrix factorization were added to the table in order to have baseline comparisons. In order to have a fair comparison, number of basis vectors for NMF, SVD, and both variants of the proposed method are set to the same number which is 30.\nis a method proposed by Fan et al. [14] and has shown to perform well on ADNI dataset [59] .\nWhile features extracted from NMF and SVD methods were fed to the same procedure as the proposed method to find the best classifier, COMPARE has it own routine to find an optimal classifier.\ndataset is already explained in the Section III-A.\ncontains 22 subjects performing a forced-choice deception and their brain activations were acquired using BOLD imaging (fMRI). SPM2 software [60] is used to calculate Parameter Estimate Images (PEIs), i.e., regression coefficients or , of the HRF regressors for each of the 50 conditions from the least mean square fit of the model to the time series. The 50 conditions include 48 regressors modeled \"lie\" and \"truth\" events individually while two additional regressors modeled the variant distracter and recurrent distracter conditions.\nIn the Table II , while the group-sparsity regularization outperforms , the boxed-sparsity performs almost as well as on the dataset. On the dataset, outperforms our method although the boxed-sparsity is in a reasonable range of the best performance. The group-sparsity result for fMRI dataset is shown as \"N/A\" because fMRI images which are preprocessed with SPM2 are registered to SPM2 atlas with affine transformation. Therefore, structural brain regions of the atlas do not match well with the corresponding regions on the individual subjects that makes the definition of the groups in the group-sparsity inaccurate.\nThe values reported in the Table II for the dataset are in the same range as the accuracy rates reported in [61] ; Nevertheless, the conditions of the experiments (including preprocessing, features extraction, samples in the training and testing lists, etc.) are different, which make the results not one-to-one comparable. "}, {"section_title": "D. Semi-Supervised Extension", "text": "In this section, we investigate an extension of our method to semi-supervised learning proposed in the Section II-F. In order to examine effectiveness of the proposed method for semi-supervised learning, we performed two sets of experiments. In the first set of experiments, the proposed method is compared with well-established semi-supervised methods on a benchmark data published earlier by Sch\u00f6lkopf et al. [62] : in the second sets of experiments, we apply the method on a real medical images acquired from the ADNI dataset. Table III compares accuracy rates of the proposed method with those of three well-established semi-supervised learning methods on three datasets of a publicly available benchmark [62] . Although the setting in [62] is not in favor of our method and the proposed method is designed to address semi-supervised learning for medical image data, the results can evaluate soundness of the method in a very general context. Full descriptions of the datasets and preprocessing steps are elaborated in [62] but briefly.\n\u2022 : It is a dataset consisting of 150 images of each of the ten digits randomly drawn from the USPS set of handwritten digits. The digits \"2\" and \"5\" were assigned to the class , and all the others formed class . The images were obscured by application of [62, Algor. 21.1] to prevent people from exploiting spatial relationship of features in the images [62] ; more specifically for this dataset: and .\n\u2022 : This is the groups from the dataset and the goal is to classify the category versus the rest (by Tong et al. [63] ); more specifically for this dataset:\nand .\n\u2022 : This dataset originates from research toward the development of a brain computer interface (BCI) (Lal et al. [64] ). In each trial, EEG (electroencephalography) was acquired from a single subject from 39 electrodes. An autoregressive model of order 3 was fitted to each of the resulting 39 time series. The trail was represented by the total of fitted parameters; more specifically for this dataset:\nand . In Table III , in the first four rows, number of label samples are set to 10 and in the second four rows, it is set to 100. The Table reports error rates for non/linear transductive support vector machine (TSVM) [65] , Laplacian SVM (lapSVM) [66] , which are chosen due to their good performance on the three datasets, in addition to the error rate for the proposed method. Entries of the table for lapSVM and non/linear-TSVM are adopted from [62] . According to [62] , hyper-parameters of each of the algorithms are chosen by minimizing the test error, which is not possible in real applications; however, the results of this procedure can be useful to judge the potential of a method. To be comparable, similar procedure was applied to find and for our algorithm. Table III shows that no method consistently outperforms other methods across datasets; however, the results are consistent on each dataset. It shows that although our method outperforms others only on the BCI dataset but it is within a reasonable range of the best performance. This result motivates us to employ semi-supervised extension of our method on a real medical image data.\nIn medical imaging applications, semi-supervised learning arises either due to availability of abundant of sample images with no labels, or more importantly in case that there is uncertainty about the labels. For example MCI is viewed as an intermediate stage between normal aging and dementia. It has diverse range of symptoms but when memory loss is the predominant one, it is considered as a risk factor for the Alzheimer's disease (AD) [47] . Recent studies have shown that individuals with MCI incline to progress to the Alzheimer's disease. Grundman et al. [47] estimated an approximate rate of 10%-15% per year; nevertheless not all MCI subjects converge to the AD. One interesting question would be to determine which MCI subjects have higher likelihood to become AD subject.\nIn this experiment 238 structural MRI images of MCI subjects were acquired from the ADNI dataset and used as unlabeled data. All 238 MCI subjects have at least two scans corresponding to 24-36 months follow-ups. Among 238 subjects, 99 patients have converted to AD at some point by their third year follow ups (MCI-C) and 139 did not convert after three years MCI-NC). AD and NC subjects explained in the Section III-A were used as labeled data and the MCI subjects (MCI-C/MCI-NC) were used an unlabeled data. RAVENS maps of the images were computed by the same preprocessing pipeline as those of AD and NC subjects explained in the Section III-A. Similar to the experiments in the Section III-A, labeled data (AD/NC) is divided to 20 folds; data from 19 folds plus unlabeled data (MCI subjects) is used to learn the basis vectors. One fold out of 20 folds of the labeled data plus the unlabeled data were used for testing. In order to avoid searching for the best parameters, the most frequently selected parameters in the Section III-C were used as the parameters.\nTo evaluate the performance of the algorithm, accuracy rates on the labeled data (AD/NC) and recall rates on the unlabeled data are reported in Table IV for both regularization types. Since unlabeled data is shared between 20 folds, the recall rates (true positive and true negative rates depending on the class label) are averaged among 20 folds. Table IV shows the results for the semi-supervised learning, represent semi-supervised learning for the Boxedand group-sparsity constraints respectively. The classification accuracy rates for the labeled data have been improved slightly for the boxed-sparsity compared to the Table II meaning that unlabeled data can help improving the classification accuracy for the labeled data. While the recall rates show high values for the MCI-C group, they demonstrate low recall rates for the MCI-NC group. Such low value can partly be described by the fact that the patients in the MCI-NC group have not converted to the AD group yet but they may convert in the future. In addition, the labeled data anchored the classifiers to produce valid results for the AD/NC groups and avoid a case in which all data are assigned to one class. Therefore, area under curve (AUC) of the classifiers should be investigated for further evaluation of the method. For MCI subjects, since a ground truth is not available for MCI-NC subjects, we will investigate this measure in the new experiment.\nObserve that for all values reported in Table IV , basis vectors (hence features) extracted in the semi-supervised way but the classifiers are supervised classifier (Logistic Model Trees [54] ). One question would be whether a semi-supervised classifier can improve the results. Therefore, we designed an experiment to answer multiple questions: 1) whether it is helpful to feed the features extracted using semi-supervised basis learning to a semi-supervised classifier instead of a supervised classifier; 2) whether our semi-supervised basis learning is useful when there are few labeled samples; 3) how the number of labeled samples and different configurations of (semi-)supervised basis learning and (semi-)supervised classifiers affect AUC for MCI subjects.\nFor computational efficiency, the basis vectors were learned only from 79 MCI subjects (as unlabeled data), and 20 AD and 20 NC subjects (as labeled data). The labeled subjects were divided into five folds for cross validation (4/5 for training and 1/5 for testing) and the 79 MCI subjects were shared as unlabeled data across folds. In order to investigate the effect of number of labeled data, we performed four basis learning experiments by increasing number of revealed labels from 4 to 32; each fold has AD/NC subjects and we revealed labels of AD/NC subjects as . Rest of MCI subjects (i.e., ) and AD/NC subjects that do not contribute in the basis learning are added to the testing lists for each fold.\nAfter basis learning, features are extracted by projecting all images on the learned basis vectors. These features were fed into a supervised-classifier (Logistic Model Trees [54] ) and a semi-supervised classifier (linear Laplacian SVM [67] ) to produces labels. To have a reference point for comparison, we also learned the basis without unlabeled data (supervised basis learning). Fig. 12 plots accuracy rates of AD/NC with respect to the number labeled data in different settings. The accuracy rates were computed on the left-out labeled data and the rest of the labeled data that was not introduced during the basis learning or training of the classifier. For brevity, in Fig. 12 indicates Supervised Features, i.e., using only labeled data to learn the basis vectors, and denotes semi-supervised features, i.e., using the labeled and the unlabeled data to learn the basis vectors. The figure shows different scenarios for classification: supervised features fed into a supervised classifier and a semi-supervised classifier and compares them with semi-supervised features fed into a supervised classifier and a semi-supervised classifier . Fig. 12(a) and (b) shows accuracy rates and AUC for the MCI respectively when the boxed-sparsity is used for regularization and Fig. 12(c) and (d) represent the same quantifies when the group-sparsity is applied as the sparsity regularization.\nThe results shown in Fig. 12 can be summarized as follows.\n\u2022 Semi-supervised classifier helps: in all scenarios in Fig. 12 semi-supervised classifiers (i.e., and ) outperform their corresponding supervised classifiers for both types of regularizations [boxed-sparsity: Fig. 12(a) and (b) , group-sparsity: Fig. 12 (c) and (d)] and both measures (i.e., accuracy and AUC).\n\u2022 Semi-supervised basis learning helps: in all scenarios semi-supervised features which are extracted by basis vectors learned in presence of unlabeled data outperform their corresponding supervised features . Significant difference can be seen when the semi-supervised features are fed into semi-supervised classifier (i.e., ) which achieves the best performance for both measures particularly for the boxed-sparsity. Note that semi-supervised features are more stable in terms of performance even if they are fed into a supervised classifier; for example, compare and in Fig. 12 (b) and (d). Also note that AUC measures are computed for MCI-NC/ MCI-C subjects because there is no real ground truth for them; hence AUC might be a better measure to show that the classifiers are not biased toward one of the classes although good performances on the labeled data (i.e., AD versus NC) already show this fact."}, {"section_title": "E. Sensitivity Analysis of the Parameters", "text": "In this section, we perform a few experiments to investigate the effect of parameter selection ( ) on the classification accuracy rates. In this section, instead of optimizing , we set to the most frequently chosen ones in the Section III-C. The MCI subjects were not involved in the experiments of the Section III-C. In addition, we held out 205 AD and NC subjects (89 AD and 114 NC) from the ADNI dataset. Therefore, optimizing in the Section III-C is oblivious with respect to the samples used in this section. In addition to the AD versus NC classification, we have included classification between converter and nonconverter MCI subjects to the Table V which is known to be a difficult classification problem [61] . In fact, this experiment shows conservative results for the proposed methods. As the Table V shows, the proposed method outperforms other methods on both datasets. The classification rates are relatively low on the dataset as reported in the literature [61] yet the proposed method shows slightly better performance comparing to other methods in the Table. This experiment shows that as long as the datasets are similar, one can reduce the computational cost of optimizing by removing the extra nested loop for parameter selection (i.e., searching for the best inside of training sets) without significant degradation in the performance of the classifiers."}, {"section_title": "IV. DISCUSSION AND CONCLUSION", "text": "The experiments in this paper show that the algorithm is robust with respect to choice of parameters as long as they are chosen within a reasonable range. It also shows that the generative term is helpful; indeed we have observed in our experiments that in the process of searching for the best , those settings biased toward the generative terms are selected quite frequently. The experiments shows that discriminative term is also essential because in its absence, the formulation becomes more or less similar to NMF [23] formulation which is shown to underperform in Table II . Nevertheless, for very large sample size experiments finding optimal parameters might be computationally expensive. Therefore, in Section II-G, we analyzed the role of each parameter in well possessedness of the objective function and introduced an intuitive sequence to pick within a reasonable range. In addition, we empirically showed in the Section III-E that as long as datasets are similar one can avoid parameter selection without significant degradation in the accuracy rate.\nIn Section III-C, we also compared the proposed method with PCA and NMF as baseline methods and COMPARE [14] as the state-of-the-art algorithm. Both variants of the proposed method outperformed the baseline methods (i.e., NMF and PCA) and performed better or almost as well as COMPARE. The groupsparsity achieved the best performance in but it was not applicable to because we defined the groups for the group-sparsity based on a segmentation of an atlas and all fMRI subjects are brought to the atlas space using only affine registration; it yields inaccurate brain segmentation for each subject and consequently inaccurate definition for the groups. It is also worth mentioning that COMPARE achieves such level of accuracy using 150-250 features while our algorithm uses only 30 basis vectors (i.e., number of features). There is no clear winner between the group-and the box-sparsity.\nCombination of the generative and the discriminative terms makes extension to a semi-supervised learning readily accessible. We showed in Section III-D that the features extracted in the semi-supervised way are more stable for classification of the labeled data than the supervised features in spite of scarce labeled data. Again, there is no clear winner when it comes to comparison between the box-sparsity and the group-sparsity regularization.\nThere are still several avenues for improvements and extensions that are left for the future work. For example, the framework can be extend to multichannel images (i.e., when each subject has multiple modalities). Another open field for future research can address approximate alignment. Groups can be defined approximately by associating probability or membership values of each voxel to groups. Such definition of groups changes the definition of unit-ball of the group-sparsity norm and makes the support of the groups to overlap. Defining overlapping groups imposes a challenge to the optimization problem which needs to be addressed. Projection on the unit-ball of the group sparsity for overlapping groups has been recently studied in [68] and [69] .\nThis framework can be easily extended to handle multiclass classification. Other regularization terms that enhance the performance of the semi-supervised basis learning (e.g., Laplacian regularization [66] ) can be incorporated into the framework. We currently use random initialization but perhaps a multi-scale strategy improves the convergence rate of the algorithm. A faster algorithm can possibly be achieved if the basis vectors are parameterize by other basis vectors from possibly an over-complete dictionary; it may lead to a convex formulation for the framework instead of the current nonconvex formulation. In summary, we proposed a novel dimensionality reduction that can extract discriminative yet interpretable features. The proposed framework is a hybrid generative and discriminative model that provides a flexible structure: it can incorporate prior knowledge through regularization terms (two variants are proposed in this paper); it can be readily extended to extract features in a semi-supervised way. We formulated the proposed framework as an optimization problem and proposed a novel projection-based algorithm to solve such large scale nonlinear problem efficiently. The method was applied on real data in different scenarios and attained superior or comparable results to the state-of-the-art algorithm; at the same time it delineated areas of the difference in the brain which are in agreement with previous clinical studies. The developed software will be available for download via the website of Section of Biomedical Image Analysis, University of Pennsylvania. 5 APPENDIX A COMPUTING THE GRADIENT OF The objective function consists of two terms: 1) the generative term ; 2) and the discriminative term . Derivative of the generative term with respect to is where is the second derivative of which is set to in this paper and is element-wise matrix multiplication. It is worth mentioning that if is replaced with other choices of a convex function (e.g., ) for yields other options for the divergence term (e.g., KL-divergence) to model other assumptions about noise (e.g., Poisson).\nDerivative of the discriminative term with respect to th column of is in which ."}, {"section_title": "APPENDIX B EFFICIENT PROJECTIONS ON THE BOXED-SPARSITY", "text": "AND GROUP-SPARSITY BALLS Euclidean projection operator on a feasible set can be viewed as an optimization problem 5 https://www.rad.upenn.edu/sbia/ or homepage of the first author. For boxed-sparsity, the problem is a constrained quadratic programming (17) Geometrically, the projection point lies either on the boundary of the box in Fig. 13 or inside of the box, on the inside boundary of the shaded area in Fig. 13 . To determine which one, we can simply project the point on the box where . If still lies outside of the feasible set, it means that the projection point is on the inside boundary of the shaded area. To find the projection in this case, this problem should be solved (18) Lagrangian of (18) is (19) where and are Lagrangian multipliers. Differentiating it with respect to and setting it to zero, yields optimality condition:\n. By complementary slackness of KKT condition, we know whenever then and whenever then . Hence, if then (20) In order to determine optimal solution, , we need to determine and indices for which are zero or one. If indices of ones and zeros of are given, complementary slackness of KKT condition and the optimality conditions of (18) suffices to find optimal (21) where and is cardinality of this set.\nFollowing lemmas help us to determine the indices. 6 Lemma 1: [71] Let be the optimal solution to the minimization in (18) . Let and be two indices such that . If then must be zero as well. Proof 1: We will propose a similar lemma for the upper bound.\nLemma 2: Let be the optimal solution to the minimization in (18) . Let and be two indices such that . If then must be 1 as well.\nProof 2: The proof is by contradiction, similar to Lemma 1. Assume that is optimal solution and there exist indices and such that and but . Now, let us assume that new vector that is equal to except in two indices and in which and . It can be readily checked that is also feasible. The difference in objective value for new vector is which contradicts with optimality of .\nGiven the lemmas, we can form an optimization problem similar to (18) . For a fixed , we solve the following optimization problem: (22) and then we search over such that the solution satisfies the equality constraint in (18) . Observe that the term with in (19) is absorbed into the quadratic term in (22) . However, (22) has a closed form solution (23) Since we do not know the appropriate , we need to search for it. So far, optimization problem has simplified from -dimensional to one dimensional problem. However, the two lemmas help us to find exact in finite number of iterations. The idea is to shrink with a bisection-type algorithm until 6 Similar approach was adopted by Duchi et al. [70] .\nnumber of zeros and ones stay unchanged, then can be found exactly with (21) . The details of the algorithm are shown in Algorithm 3. Given Algorithm (3), efficient projection on a group-sparsity ball is very simple because it uses Algorithm (3) as a submodule. An algorithm for efficient projection on a group-sparsity ball is shown in Algorithm (4). In this case, the following optimization problem should be solved: (24) where is a positive -dimensional vector and is th element of that and is a constant. Equation (24) is a second-order cone programming (SOCP) and may look significantly different from (17) but a careful inspection reveals that an efficient algorithm to solve (17) [Algorithm (3)] can help us to solve (24) by defining"}, {"section_title": "Algorithm 3 Efficient Projection on", "text": "The defined can be provided as input to Algorithm (3) to find a projection in space. Given the projected point, simple rescaling yields optimal . The procedure is explained in Algorithm (4). Recently there have been a few research papers about efficient projection on the group-sparsity ball for arbitrary definition of the groups. Although it has been shown that projection on group-sparsity ball for arbitrary group is possible [68] , it is an expensive operation unless some special structures are assumes for the groups [69] (e.g., tree structure)."}, {"section_title": "Algorithm 4 Efficient Projection on", "text": ""}]