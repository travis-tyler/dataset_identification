[{"section_title": "", "text": ". Number of students and mean math theta scores by sex, race/ethnicity, and SES quartile 4 Table 3. Percentage of missing values and average imputation error for math score 9 Table 4. Comparison of relative bias of variance for math theta score Table 5. Comparison of mean for multiple imputation for math theta score Table 6. Comparison of standard deviation for multiple imputation math theta score Table 7. Percentage of missing values and average imputation error for reading score Table 8. Comparison of relative bias of variance for reading theta score Table 9. Comparison of mean for multiple imputation for reading theta score Table 10. Comparison of standard deviation for multiple imputation reading theta score Table 11. Comparison of mean for math theta score before and after imputation Table 12. Comparison of standard deviation (SD) for math theta score before and after imputation Table 13. Comparison of mean for reading theta score before and after imputation Table 14. Comparison of standard deviation (SD) for reading theta score before and after imputation Table 15. Comparison of mean for science theta score before and after imputation 20 Table 16. Comparison of standard deviation (SD) for science theta score before and after imputation 20 Table 17. Comparison of mean for history/citizenship/geography theta score before and after imputation 21   Table 18. Comparison of standard deviation (SD) for history/citizenship/geography theta score before and after imputation 22 The National Education Longitudinal Study of 1988 (NELS:88) is the only current National Center for Education Statistics (NCES) dataset that contains scores from cognitive tests given to the same set of students across multiple points in time. The resulting longitudinal test data offer the possibility of researching cognitive gains from middle school through high schoolan attractive feature. However, as is inevitable in any survey, cognitive test data are missing for some individuals in each round; the problem is more severe in the second follow up (F2) than in the earlier rounds. Therefore, NCES decided to use imputation to reduce the bias caused by nonresponse."}, {"section_title": "List of Figures", "text": "This study involved a two-step process for implementing this imputation. The first step, as described in chapter 2, was to conduct a simulation study to evaluate two different imputation procedures currently used at NCES: a model-based random imputation method called PROC IMPUTE and a within-class random hot-deck imputation. In our simulation study, we first examined and selected a range of auxiliary variables that are conceptually and empirically related to the F2 test scores, and then we imputed the Item Response Theory (IRT) theta scores in math and reading. The findings of the simulation study confirmed that PROC IMPUTE performed better (Hu and Salvucci 1999). The second step, as described in chapter 3, involved using PROC IMPUTE to impute missing F2 cognitive test scores in four subject areas: math, science, reading, and history/citizenship/geography. The results provide end users with complete cognitive test data for both cross-sectional and longitudinal research with the F2 data or the base-year through the second follow-up (BYF2) panel data. As a future step, other measurement scales (proficiency scores, standardized scores, and the number right scores) may be subsequently converted using the theta scores."}, {"section_title": "BACKGROUND", "text": "In NELS :88, the respondents' cognitive ability and the growth (cognitive gains) from 8th through 12th grades at the group and individual levels were measured by a calibrated scale based on Item Response Theory (IRT). This calibration process requires that items are relatively unifactorial across grades in each subject area; that is, with the same dominant factor underlying all test forms in a given subject, say, math (Rock and Pollack 1995). There should be a common set of \"anchor\" items across adjacent forms, and most content areas should be represented in all grade forms. In NELS:88, the increasingly difficult levels from 8th through 12th grades were created by raising the problem-solving demands in the existing content areas and adding new content in the later forms, especially at 12th grade. IRT assumes that a test taker's probability of answering an item correctly is a function of his or her ability and one or more characteristics of the test item itself The three-parameter IRT logistic model uses the pattern of right, wrong, and omitted responses to the items administered in a test form, and the difficulty, discriminating ability, and \"guess-ability\" of each item, to place each test taker at a particular point, 0 (theta), on a continuous ability scale. The probability of a correct answer (called the theta score) on item i can be expressed as: (1 c,) 1+e \" where 0 is the ability of the test taker, a; is discrimination of item 1, or how well the item distinguishes between ability levels at a particular point, bi is the difficulty of item i, and ci is the \"guess-ability\" of item i. A computer program is used to calculate the marginal maximum-likelihood estimates of the IRT parameters that best fit test takers' responses (Muraki and Bock 1991). To assess the models' match with the test data, one compares the IRT-estimated parameters with the actual proportion of correct answers to a test item for test takers grouped by ability. If the IRT-estimated curves and the actual data points match closely, then the theoretical model represents the data accurately. After the parameters for a set of test items are calibrated on the same scale as the test takers' ability estimates, a test taker's probability of a correct answer to each item in the test battery can be estimated, even for items that were not administered to the test taker. Theta scores can be used to derive other test scores: the IRT-estimated number correct score in a subject area is the sum of the probabilities of correct answers for the items in the area. NELS:88 nonresponse issues: Nonresponse is always a concern in survey data, and some cases in the NELS:88 cognitive test data are missing in each round due to absence, nonparticipation, or results that were unscorable because of too many unattempted test items. This missingness problem is more severe for math theta scores in F2 (22.9 percent missing scores) than in the earlier two rounds of tests (3.7 percent and 6.0 percent missing scores for the base-year (BY) and the first follow up (F1), respectively), as shown in table 1. The sample weighting adjustment cannot fully solve the problem resulting from survey nonresponse, neither in theory nor in practice (Rubin 1996). Specifically, the bias generated by missing cognitive scores cannot be corrected by the NELS:88 sampling weights because the weights were constructed to remedy unit nonresponse, not item nonresponse angels et al. 1994, p. 70). In fact, the joint impact of item nonresponse to cognitive tests and unit nonresponse on NELS:88 tends to damage the data quality to a potentially dangerous extent. The weighted percentage of students who took all four cognitive tests in all three waves of the survey was 65 percent of the eligible core panel sample (see Rock and Pollack 1995, table 1.1, p. 2). In addition, Rock and Pollack (1995, pp. 53-56) demonstrated that the missingness pattern of F2 test scores across demographic subgroups was not completely at random. Our tabulation of the BYF2 panel data confirms this. Table 2 presents a comparison of the rate of missing F2 test scores for some basic demographic subgroups of students in the BYF2 panel who completed all three tests and those who missed the F2 test. It shows that minority students and students in the lowest socioeconomic (SES) quartile were more likely than others to miss the test. Thus, NELS:88 estimates of academic performance based on the available cases could be biased.  The gain measure, which is of critical utility in NELS:88 longitudinal research, is thus built upon test data with high levels of item nonresponse. To assure NELS:88 data quality, strategies other than weighting are needed to address the item nonresponse problem. Imputation of missing test scores is one viable strategy. It is feasible to impute F2 cognitive test scores because a great deal of information is available to reasonably predict the missing scores. This information includes student sociodemographic background, school experience (e.g., coursework, ability and curriculum program placements, and enrichment activity participation), self-reported achievement level, and available scores in other subjects. Furthermore, the general pattern in which such predictive variables relate to achievement is known in the educational research literature. We developed our imputation models based on such knowledge. (Our approach to NELS:88 cognitive test score imputations could be applicable to similar problems likely to arise in the Early Childhood Longitudinal Studies (ECLS), conducted by NCES, which will also include multiple rounds of cognitive tests.) SIMULATION STUDY COMPARING THE PROC IMPUTE AND HOT-DECK IMPUTATION METHODS"}, {"section_title": "APPROACH", "text": ""}, {"section_title": "Selection of Auxiliary Variables", "text": "Our simulation study compared PROC IMPUTE and the hot-deck imputation method by imputing the F2 IRT-estimated theta scores in math and reading. To impute missing test scores in a given subject, we used information from available tests in other subjects, student demographic and socioeconomic background, academic coursework, and self-reported grade point averages. We also compared the imputed F2 test scores with BY and Fl test scores in a given subject. We used three criteria to compare the accuracy of the two sets of imputations: the average imputing error, the variance, and the mean bias. We decided to impute the IRT-estimated theta scores since theta scores are the original estimates of the test takers' probability of correctly answering items in a given set of test items. As mentioned previously, the F2 missing test scores were not \"missing completely at random\" (MCAR) as defined by Little and Rubin (1987). That is, the cases that did not have scorable tests in F2 were systematically different from the cases that had completed the three tests in a variety of auxiliary variables, including background and schooling (see table 2 and Rock  and Pollack 1995, pp. 53-56). Such non-MCAR missingness patterns call for imputation based on information for a subsample that had completed test scores but shared attributes with the missing cases. Our first step, therefore, was to examine a range of candidate variables in order to select the best auxiliary variables; that is, those which were related to test missingness. The candidate variables were race/ethnicity, sex, SES, coursework in the target subject areas, advanced academic program placement, Fl and F2 dropout status, early graduation status, and BY and F1 cognitive test scores. To determine their utility in the imputation model, we examined bivariate correlations between these variables and the cognitive test scores in two subject areas, math and reading. We then selected variables that correlated highly with the theta scores. Next we identified important predictors of the cognitive test outcome by fitting regression models. The final regression model reflected test scores that were homogeneous within the imputation classes defined by the covariates. We studied two imputation techniques, namely, a model-based imputation method implemented by computer software called PROC IMPUTE and a within-class random hot-deck imputation method. The study included simulating a few levels and patterns of missingness (about 20 percent of the data were made missing) in the NELS:88 BYF2 panel cases where the BY, F 1 , and F2 test scores are all nonmissing. We compared statistics derived from the incomplete data with the data after imputing simulated missing cases. Three criteria were used to compare the accuracy of the two types of imputations: the average imputing error, the bias of the variance, and the mean bias."}, {"section_title": "Description of Imputation Methods", "text": "The relative bias of the variance estimate is defined as  where m is the number of missing values, yi is the true value which is intentionally set to missing, and y,* is the imputed value for the i-th missing case. That an imputation method has smaller average imputation errors only implies that the method provides imputations on average closer to the real values. This does not necessarily mean that it gives more accurate estimates for all types of statistics, although that is true in many situations. Within-class random hot-deck imputation Since we understand reasonably well the factors related to F2 test nonresponse and have data on such factors, we could assume model-based approaches would probably produce more accurate imputation than randomization-based approaches if the model assumptions were satisfied (Hu and Salvucci 1999). Thus, we imputed the 1RT-estimated number of the right score in each subject using F2 crosssectional data on student sociodemographic and socioeconomic background, academic coursework, self-reported grade average point, and available test scores on subjects other than the one to be imputed. For the implementation of the within-class random hot-deck imputation method, we first sorted the dataset by the auxiliary variables in order to obtain homogeneous cells called imputation classes. To impute a missing value in a given imputation class, we randomly selected an observed value of the target variable in that class to fill-in for the missing value. PROC IMPUTE: To overcome the underestimation of variance which is typical in a hot-deck imputation method or a regression-based imputation method, we also added disturbance by using the software package PROC IMPUTE (McLaughlin 1991). PROC IMPUTE combines the procedures of regression-based and data sampling (often called \"hot-deck\") methods. Regression involves generating a function, 5, = f (x, ,x2,...,xp) , that relates a \"target\" variable (cognitive test score) to auxiliary variables, then uses the function along with the existing values of the auxiliary variables to compute 5) whenever it is missing. Data sampling involves subsetting the data on the basis of relevant variables and randomly selecting a value for the target variable from an available target variable within the same subset. PROC IMPUTE considers each variable on the file in turn as a target variable whose missing values are to be filled in, and it uses information on other variables to minimize the error in imputing each target variable. Three steps are taken to impute each variable in PROC IMPUTE. First, stepwise regression analyses are performed \"simultaneously\" for each variable. During these analyses, an ordered list of the imputation variables is constructed. The regression analysis for each variable uses as predictors all the complete variables, including the previously imputed variables. The process terminates when there are no more permissible predictors that provide a significant improvement of fit in the prediction of any of the target variables. Second, homogeneous cells (imputation classes) are created for records that have close predicted regression values. Finally, two donors are drawn from the adjacent cells. Each missing record in a given cell is imputed with a weighted average of these two donors with probability proportional to the observed frequencies within the two cells. PROC IMPUTE mils all the imputation procedures automatically and generates a dataset in which all the records are complete. Imputed data flags are also automatically created by the software and set for each variable; a value of \"I\" corresponds to imputed values, \"R\" to reported values, and \"A\" to skip missing values. "}, {"section_title": "Math Theta Score", "text": "We used the F2 panel sample members that had nonmissing math theta scores and nonmissing information for the following auxiliary variables: sex, race/ethnicity, SES, units in foreign languages, units in physics, BY grade composites, and teacher's opinion about student attending college. We selected 1,996 cases, about 20 percent, from the F2 panel members and set their math theta scores as missing. To simulate the actual missingness pattern, the rate of missingness across sex, race/ethnicity, and SES quartiles mimicked that of the actual F2 test missing cases. We used PROC IMPUTE and random hot-deck to impute these simulated missing cases. The mean and variance for the math scores were calculated for the following four groups: 1. A group of 10,248 cases in the F2 panel that reported the math theta scores and auxiliary variables specified above; 2. A group that included the 8,252 cases with actual math theta scores and 1,996 cases with imputed scores using PROC IMPUTE; 3. A group that included the 8,252 cases with actual math theta scores and 1,996 cases with imputed scores using the hot-deck method; and 4. A group of 8,252 cases with actual math theta scores (the 1,996 cases were deleted as \"missing\"). This group simulates the current scenario in NELS:88 where there are missing test scores, but no imputation has been used. Group 1 estimates served as the \"true scores.\" Groups 2, 3, and 4 estimates were compared with the true Group 1 estimates to examine if Group 2 (with PROC IMPUTE imputation) did better than Group 3 (with hot-deck imputation) and Group 4 (non-imputed). Table 3 provides the results for average imputation error for the math theta score. Then figure 1 compares the results for the bias of the mean, while table 4 presents the relative bias of the variance for the math theta score. Tables 5 and 6 show, respectively, the mean and standard deviation for the multiple imputation using the PROC IMPUTE and within-class random hot-deck imputation methods. Note that in the race/ethnicity subgroup, whites and Asians were combined because preliminary results had shown that both whites and Asians have on average higher math scores than the other racial/ethnic groups. About 20 percent of the math scores were imputed using first PROC IMPUTE, and then the random hot-deck imputation method. The average imputation error is consistently lower for PROC IMPUTE than it is for hotdeck in each sociodemographic subgroup, and overall (see table 3).     4 shows that the relative bias of the variance is consistently smaller for PROC IMPUTE than it is for hot-deck and the non-imputed group, in each of the sociodemographic subgroups of study, and overall, with the exception of the highest quartile of the SES subgroup.  Table 5 presents the resulting mean for a set of five imputations on the math theta score using the PROC IMPUTE and within-class random hot-deck imputation method. As we can see, the multiple imputation means based on the PROC IMPUTE method are consistently closer to the true means than are the means based on the within-class random hot-deck imputation method. This observation is valid for each of the study's sociodemographic subgroups, and overall. With a set of five imputations on the math theta score using the PROC IMPUTE and within-class random hot-deck imputation methods, we calculated the resulting standard deviations (see table 6). From table 6, it is clear that the multiple imputation standard deviations based on the PROC IMPUTE method are consistently closer to the true standard deviations than are the standard deviations based on the within-class random hot-deck imputation method. This held true for all the sociodemographic subgroups of study. Reading Theta Score For the reading cognitive test score simulation study, we used the F2 panel sample members that had nonmissing reading theta scores and nonmissing auxiliary variables. The auxiliary variables considered here were sex, race/ethnicity, SES, units in foreign languages, units in reading, units in chemistry, grade composites from base-year, and teacher's opinion about student attending college. We selected 2,017 cases, about 20 percent, from the F2 panel members and set their reading theta scores as missing. We used PROC IMPUTE and random hot-deck to impute these simulated missing cases. The mean and variance for the reading scores were calculated for the following four groups: (1) a group of 10,249 cases in the F2 panel that reported the reading theta scores and auxiliary variables specified above; (2) a group of 8,232 cases with actual reading theta scores and 2,017 cases with imputed scores using PROC IMPUTE; (3) a group of 8,232 cases with actual reading theta scores and 2,017 cases with imputed scores using the hot-deck method; and (4) a group of 8,232 cases with actual reading theta scores. Imputation of Test Scores in the National Education Longitudinal Study of 1988 (NELS:88) Table 7 provides the calculated average imputation error for the reading theta score, figure 2 displays the calculated bias of the mean, and table 8 presents the calculated relative bias of the variance for the reading theta scores when non-imputed and when imputed using PROC IMPUTE and random hotdeck. Table 9 shows the mean for a set of five imputations using the PROC IMPUTE and within-class random hot-deck imputation methods, and table 10 shows the corresponding standard deviations. Note that, unlike the math test score, the race/ethnicity variable here is categorized by whites on one hand and the other racial/ethnic groups on the other hand. As in the simulation of math theta scores, around 20 percent of the reading scores were set to missing and imputed using first the PROC IMPUTE and then the random hot-deck imputation methods. The average imputation error is consistently lower for PROC IMPUTE than it is for hot-deck, in each sociodemographic subgroup, and overall (see table 7). In figure 2, note that the bias of the mean for female reading theta score is zero for PROC IMPUTE. Nevertheless, the bias of the mean does not show that any particular method is consistently better across all sociodemographic subgroups. 12 Imputation of Test Scores in the National Education Longitudinal Study of 1988 (NELS:88) tr-0 However, the relative bias of the variance is consistently smaller for PROC IMPUTE than it is for the hot-deck and the non-imputed groups, in each sociodemographic subgroup, and overall, with the exception of the third and fourth quartile of the socioeconomic status subgroup (see table 8).  Table 9 provides the mean for multiple imputation on the reading theta score using the PROC IMPUTE and within-class random hot-deck imputation method. In most of the sociodemographic subgroups of study, and overall, the multiple imputation means based on the PROC IMPUTE method are closer to the true means than are the means based on the within-class random hot-deck imputation method. With a set of five imputations on the math theta score using the PROC IMPUTE and within-class random hot-deck imputation methods, we calculated the resulting standard deviations (see table 10). It is clear that the multiple imputation standard deviations based on the PROC IMPUTE method are consistently closer to the true standard deviations than are the standard deviations based on the within-class random hot-deck imputation method. This held true for all the sociodemographic subgroups of study. "}, {"section_title": "CONCLUSION OF THE SIMULATION STUDY", "text": "Using PROC IMPUTE to impute the missing math and reading cognitive test scores produced better results than using the random hot-deck imputation method or no imputation in the simulation study that we conducted using NELS:88 second follow-up (F2) data. We therefore chose PROC IMPUTE as our method of imputing the NELS:88 theta scores in the second part of this project. Those results are discussed in the next chapter. The results of the simulation study described in the previous chapter showed that PROC IMPUTE was the appropriate choice of imputation techniques for imputing the missing test score for the second follow-up in the NELS:88. It generated the \"best\" scores based on the criteria used; that is, PROC IMPUTE was the method with the least average imputing error and mean bias and with the least distortion in variance. Hence, in this chapter, we used PROC IMPUTE to impute the missing test scores in the four tested F2 subject areas: math, science, reading, and history/ citizenship/geography. We used PROC IMPUTE to impute the 3,775 missing cases for the math theta score. We started by using the full BY-F2 panel sample members and the following auxiliary variables: from F2sex, race/ethnicity, SES, units in foreign languages, units in math, units in geometry, units in chemistry, and units in physics; from Flthe teacher's opinion about whether the student will go to college or not, number of course the student took in geometry, and math theta score; and from BYgrade composite variable and math theta score. We then computed the overall mean and standard deviation for the math theta, and also the mean and standard deviation for the math theta score across sex, race/ethnicity, and SES quartiles. Those were compared for the following two groups: 1. A group of 12,714 cases in the BY-F2 panel that reported the math theta scores; and 2. A group that included the 12,714 cases with actual math theta scores and 3,775 cases with imputed scores using PROC IMPUTE. The mean and standard deviation of the math theta score for both groups defined above are shown in tables 11 and 12, respectively.  Reading Theta Score We used PROC IMPUTE to impute the 3,771 missing cases for the reading theta score. We started by using the full BYF2 panel sample members and the following auxiliary variables: from F2sex, race/ethnicity, SES, units in foreign languages, and units in chemistry; from Flthe teacher's opinion about whether the student will go to college or not, number of course the student took in foreign languages, and reading theta score; from BYgrade composite variable and reading theta score. We then computed the overall mean and standard deviation for the reading theta, and also the mean and standard deviation for the reading theta score across sex, race/ethnicity, and SES quartiles. Those were compared for the following two groups: 1. A group of 12,718 cases in the BYF2 panel that reported the reading theta scores; and 2. A group that included the 12,718 cases with actual reading theta scores and 3,771 cases with imputed scores using PROC IMPUTE. The mean and standard deviation of the reading theta score for both groups defined above are shown in tables 13 and 14, respectively.  9.46 9.58 'There are 9 cases with missing data on race/ethnicity. 2 There is 1 case with missing data on SES. Source: U.S. Department of Education, National Center for Education Statistics, National Education Longitudinal Study of 1988 (NELS:88), original and imputed data. Science Theta Score We used PROC IMPUTE to impute the 3,858 missing cases for the science theta score. We started by using the full BYF2 panel sample members and the following auxiliary variables: from F2sex, race/ethnicity, SES, units in foreign languages, units in math, units in geometry, units in chemistry, and units in physics; from Flthe teacher's opinion about whether the student will go to college or not, number of course the student took in geometry, and science theta score; from BYgrade composite variable and science theta score. We then computed the overall mean and standard deviation for the science theta, and also the mean and standard deviation for the science theta score across sex, race/ethnicity, and SES quartiles. Those were compared for the following two groups: 1. A group of 12,631 cases in the BYF2 panel that reported the science theta scores; and 2. A group that included the 12,631 cases with actual science theta scores and 3,858 cases with imputed scores using PROC IMPUTE. The mean and standard deviation of the science theta score for both groups defined above are shown in tables 15 and 16, respectively.   from Flthe teacher's opinion about whether the student will go to college or not, number of course the student took in foreign languages, number of course the student took in geometry, and history/citizenship/geography theta score; from BYgrade composite variable and history/citizenship/ geography theta score. We then computed the overall mean and standard deviation for the history/citizenship/geography theta, and also the mean and standard deviation for the history/citizenship/geography theta score across sex, race/ethnicity, and SES quartiles. Those were compared for the following two groups: 1. A group of 12,572 cases in the BYF2 panel that reported the history/citizenship/geography theta scores; and 2. A group that included the 12,572 cases with actual history/citizenship/geography theta scores and 3,917 cases with imputed scores using PROC IMPUTE. The mean and standard deviation of the history/citizenship/geography theta score for both groups defined above are shown in tables 17 and 18, respectively.  "}, {"section_title": "CONCLUSION", "text": "The SES variable is associated with the race/ethnicity variable (with Pearson chi-squared p-value).0001). As seen in figure 3, as the SES quartile increases, the proportion of minorities in that SES quartile decreases. Also the proportion of minorities that have missing values for each subject theta score is higher than the corresponding proportion of minorities that have nonmissing values for that given subject, as shown in figure 4. Since the mean theta score increases for each subject as the socioeconomic status quartile increases, we would expect (as is the case in tables 11, 13, 15, and 17) the mean theta score to be slightly lower after imputation than before imputation. That is, the higher proportion of minority students with missing test scores have a slightly lower overall average test score after imputation.     1996(BPS:1996 "}]