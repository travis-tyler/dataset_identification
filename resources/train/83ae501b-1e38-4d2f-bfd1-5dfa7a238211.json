[{"section_title": "Abstract", "text": "A key step for Alzheimer's disease (AD) study is to identify associations between genetic variations and intermediate phenotypes (e.g., brain structures). At the same time, it is crucial to develop a noninvasive means for AD diagnosis. Although these two tasks-association discovery and disease diagnosis-have been treated separately by a variety of approaches, they are tightly coupled due to their common biological basis. We hypothesize that the two tasks can potentially benefit each other by a joint analysis, because (i) the association study discovers correlated biomarkers from different data sources, which may help improve diagnosis accuracy, and (ii) the disease status may help identify disease-sensitive associations between genetic variations and MRI features. Based on this hypothesis, we present a new sparse Bayesian approach for joint association study and disease diagnosis. In this approach, common latent features are extracted from different data sources based on sparse projection matrices and used to predict multiple disease severity levels based on Gaussian process ordinal regression; in return, the disease status is used to guide the discovery of relationships between the data sources. The sparse projection matrices not only reveal the associations but also select groups of biomarkers related to AD. To learn the model from data, we develop an efficient variational expectation maximization algorithm. Simulation results demonstrate that our approach achieves higher accuracy in both predicting ordinal labels and discovering associations between data sources than alternative methods. We apply our approach to an imaging genetics dataset of AD. Our joint analysis approach not only identifies meaningful and interesting associations between genetic variations, brain structures, and AD status, but also achieves significantly higher accuracy for predicting ordinal AD stages than the competing methods."}, {"section_title": "Introduction", "text": "Alzheimer's disease (AD) is a neurodegenerative disorder associated with aging. Although it accounts for 60-80% of age-related dementia cases, currently there is no cure for AD and its underlying mechanism remain elusive. To study AD mechanism, a crucial step is to identify associations between genetic variations and intermediate phenotypes (e.g., endophenotypical traits). In other words, we want to discover cross linkages between genetic risk factors based on genomic data-such as single nucleotide polymorphisms (SNPs)-and indicative intermediate phenotypes-such as cortical thickness of different brain regions (based on magnetic resonance imaging (MRI)). This identification can help us locate a subset of polymorphisms which may have functional consequences on brain structures. Although GWAS studies have been applied to AD studies, 1,2 the association study between genetic variations and multiple intermediate phenotypes is still relatively scarce for AD. A similar task arises for expression quantitative trait locus (eQTL) analysis, where canonical correlation analysis (CCA) and its extensions [3] [4] [5] [6] have been widely applied. Meanwhile, it has become increasingly important to develop a noninvasive means for AD diagnosis based on various biomarkers, including both genetic variations and MRI features. Because many of these biomarkers are irrelevant to the diagnosis, sparse models are needed to identify the relevant ones. For disease diagnosis, popular sparse models include lasso, 7 elastic net, 8 and automatic relevance determination. 9 Here we treat genotypes or intermediate phenotypes as biomarkers and the disease status as the response in a linear regression or classification setting. Non-zero regression or classification weights in our estimation indicate relevant biomarkers for the disease. 10, 11 Although these two tasks-association discovery and disease diagnosis-have been addressed separately in the previous works, they are closely related-due to the their common underlying biological basis-and can potentially benefit each other by a joint analysis. To harness the natural synergy between the two tasks, we propose a new Bayesian approach that integrates multiview learning for association discovery with sparse ordinal regression for disease diagnosis. In the new approach, genetic variations and phenotypical traits are generated from common latent features based on separate sparse projection matrices and the common latent features are used to predict the disease status based on Gaussian process ordinal regression (See Section 2). To enforce sparsity in projection matrices, we assign spike and slab priors 12 over them; these priors have been shown to be more effective than l 1 penalty to learn sparse projection matrices. 13, 14 The sparse projection matrices not only reveal critical interactions between the different data sources but also identify groups of biomarkers in data relevant to disease status. Finding groups of biomarkers can avoid over-sparsification (i.e., selecting one instead of multiple correlated features), thus boosting the accuracy for disease diagnosis. It can also help provide a better biological understanding because these groups may form biologically meaningful units (e.g., pathways). Meanwhile, via its direct connection to the latent features, the disease status influences the estimation of the projection matrices. Hence we name this new method Supervised Heterogeneous Multiview Learning (SHML). In addition to enjoying the benefit of integrating the related tasks, two features of our model distinguish it from previous approaches:\nTo learn the model from data, we develop a variational Bayesian expectation maximization (VB-EM) approach (See Section 3). Maximizing this estimate enables us to automatically choose a suitable dimension for the latent features in a principled Bayesian framework.\nIn Section 4, we test our approach SHML on both synthetic and real datasets. On synthetic data, SHML achieves both higher estimation accuracy in recovering true associations between different views and higher prediction accuracy than alternative state-of-the-art methods. We then apply SHML to an AD study. SHML achieved highest prediction accuracy among all competing methods and yielded biologically meaningful relationships between genetic variations, brain atrophy, and the disease status."}, {"section_title": "Model", "text": "First, let us describe the data. We assume there are two heterogeneous data sources: one contains continuous data -for example, MRI features -and one discrete ordinal data -for instance, SNPs. Given data from n subjects, p continuous features and q discrete features, we denote the continuous data by a p \u00d7 n matrix X = [x 1 , . . . , x n ], the discrete ordinal data by a q \u00d7 n matrix Z = [z 1 , . . . , z n ], and the labels (i.e., the disease status) by a n \u00d7 1 vector y = [y 1 , . . . , y n ] . For the AD study, we let y i = 0, 1, and 2 if the i-th subject is in the normal, MCI or AD condition, respectively.\nTo link two data sources X and Z together, we introduce common latent features U = [u 1 , . . . , u n ] and assume X and Z are generated from U by sparse projections. The common latent feature assumption is sensible for association studies because both SNPs and MRI features are biological measurements of the same subjects. Note that u i is the latent feature for the i-th subject and its dimension k is estimated by evidence maximization. In a Bayesian framework, we give a Gaussian prior over , I) , and specify the rest of the model (see Figure 1) as follows: Continuous data. Given U, X is generated from \nwhere ). Without any prior preference over the selecting probabilities, we assign uniform priors, p(\u03a0 g ) = 1. Similarly, H is sampled from\nwhere S h are binary selection variables and \u03c0 ij h in \u03a0 h is the probability of s ij h = 1. Again, we assign uninformative uniform priors over\nFinally, the joint distribution of our model, SHML, is simply the product of all the prior distributions and the conditional density distributions."}, {"section_title": "Algorithm", "text": ""}, {"section_title": "Estimating latent variables", "text": "Given the model specified in the previous section, now we present an efficient, principled method to estimate the latent features U, the projection matrices H and G, the selection indicators S g and S h , the selection probabilities \u03a0 g and \u03a0 h , the variance \u03b7, the auxiliary variables C for generating ordinal data Z, and the auxiliary variables f for generating the labels y. In a Bayesian framework, this estimation task amounts to computing their posterior distributions. However, computing the exact posteriors turns out to be infeasible since we cannot calculate the normalization constant of the exact posterior distribution. Thus, we resort to a variational Bayesian Expectation Maximization (VB-EM) approach. More specifically, in the E step, we approximate the posterior distributions of\nand in the M step, based on the approximate distributions, we optimize the latent features U.\nTo obtain the variational approximation, we minimize the Kullback-Leibler (KL) divergence between the approximate and the exact posteriors. To this end, we use coordinate descent; we update an approximate distribution, say, Q(H), while fixing the other approximate distributions, and iteratively refine all the approximate distributions. The detailed updates are given in the following paragraphs."}, {"section_title": "Updating variational distributions for continuous data", "text": "For the continuous data X, the approximate distributions of the projection matrix G, the noise variance \u03b7, the selection indicators S g and the selection probabilities \u03a0 g are\nThe mean and covariance of g i are calculated as \nThe moments required in the above distributions are calculated as \u03b7 =r"}, {"section_title": "Updating variational distributions for ordinal data", "text": "For the ordinal data Z, we update the approximate distributions of the projection matrix H, the auxiliary variables C, the sparse selection indicators S h and the selection probabilities \u03a0 h . Specifically, the variational distributions of C, H, S h and \u03a0 h are\nis the transpose of the i-th row of C,\nThe required moments for updating the above distributions can be calculated as log(\u03c0\n, where \u03a6(\u00b7) is the cumulative distribution function of a standard Gaussian distribution. Note that in Equation (3), Q(C) is the product of truncated Gaussian distributions and the truncation is controlled by the observed ordinal data Z."}, {"section_title": "Updating variational distributions for labels", "text": "We update the variational distribution of the auxiliary variables f as follows:\nis also the product of truncated Gaussian distributions and the truncated region is decided by the ordinal label y. In this way, the supervised information from y is incorporated into estimation of f and then estimation of the other quantities by the recursive updates."}, {"section_title": "Optimizing the latent representation U", "text": "After the expectations of the other variables are calculated, we optimize U by maximizing the following variational lower bound\nwhere\n, and the constant means a value independent of U so that it is irrelevant for optimizing U. Note that we can optimize the dimension k by maximizing the full variational lower bound of our model, which involves other quantities as well, such as H and G . To save space, we do not present the long equation for the full lower bound (which can be easily derived based on what we have presented). We use the L-BFGS algorithm to maximize the cost function F over U. The gradient of U is given by\nNote that \u2202K \u2202U depends on the form of the kernel function k(u i , u j ).\nComputational complexity. Based on the previous equations, we can show that the total computational complexity of our algorithm is O(max(n 3 , (p + q)nk 2 ))-it is either cubic in the number of samples n or linear in the number of the features."}, {"section_title": "Predicting disease status", "text": "Let us denote the training data as D train = {X train , Z train , y train } and the test data as D test = {X test , Z test }. To obtain the latent representation U train and U test for prediction, we carry out variational EM simultaneously on D train and D test . The benefit is that the variational EM learning procedure can utilize both the training and test data. Note that there are no updates for ordinal label part on D test and the terms regarding ordinal labels should also be removed from Equation (6) and (7). After both U test and U train are obtained from the M-step, we predict the labels for test data as follows:\nwhere y i test is the prediction for i-th test sample."}, {"section_title": "Experiments", "text": ""}, {"section_title": "Simulation Study", "text": "We first design a simulation study to examine SHML in terms of (i) estimation accuracy on finding associations between the two views and (ii) prediction accuracy on the ordinal labels. Simulation data. To generate the ground truth, we set n = 200 (200 instances), p = q = 40, and k = 5. We designed G, the 40 \u00d7 5 projection matrix for the continuous data X, to be a block diagonal matrix; each column of G had 8 elements being ones and the rest of them were zeros, ensuring each row with only one nonzero element. We designed H, the 40 \u00d7 5 projection matrix for the ordinal data Z, to be a block diagonal matrix; each of the first four columns of H had 10 elements being ones and the rest of them were zeros, and the fifth column contained only zeros. We randomly generated the latent representations U \u2208 R k\u00d7n with each column u i \u223c N (0, I). To generate Z, we first sampled the auxiliary variables C with each column c i \u223c N (Hu i , 1), and then decided the value of each element z ij by the region c ij fell in-in other words, z ij = 2 r=0 r\u03b4(b r \u2264 c ij < b r+1 ). Similarly, to generate y, we sampled the auxiliary variables f from N (0, U U + I) and then each y i was generated by\nComparative methods. We compared SHML with several state-of-the-art methods including (1) CCA, 4 which finds the projection directions that maximize the correlation between two views, (2) sparse CCA, 6, 18 where sparse priors are put on the CCA directions, and (3) multiple-response regression with lasso (MRLasso) 19 where each column of the second view (Z) is regarded as the output of the first view (X). We did not include results from the sparse probabilistic projection approach 20 because it performed unstably in our experiments. Regarding the software implementation, we used the built-in Matlab Matlab routine for CCA and the code by 18 for sparse CCA. We implemented MRLasso based on the Glmnet package (cran.r-project.org/web/packages/glmnet/index.html).\nTo test prediction accuracy, we compared our method with the following ordinal or multinomial regression methods: (1) lasso for multinomial regression, 7 (2) elastic net for multinomial regression, 8 (3) sparse ordinal regression with the splike and slab prior, (4) CCA + lasso, for which we first ran CCA to obtain the latent features H and then applied lasso to predict y, (5) CCA + elastic net, for which we first ran CCA to obtain the projection matrices and then applied elastic net on the projected data, (6) Gaussian Process Ordinal Regression (GPOR), 15 and (7) Laplacian Support Vector Machine (LapSVM), 21 a semi-supervised SVM classification method. We used the published code for lasso, elastic net, GPOR and LapSVM. For all the methods, we used 10-fold cross validation on the training data for each run to choose the kernel form (Gaussian or linear or Polynomials) and its parameters (the kernel width or polynomial orders) for SHML, GPOR, and LapSVM.\nBecause alternative methods cannot learn the dimension automatically for simple comparison, we provided the dimension of the latent representation to all the methods we tested in our simulations. We partitioned the data into 10 subsets and used 9 of them for training and 1 subset for testing; we repeated the procedure 10 times to generate the averaged test results.\nResults.To estimate linkage (i.e., interactions) between X and Z, we calculated the cross covariance matrix GH . We then computed the precision and the recall based on the ground truth. The precision-recall curves are shown in Figure 2 . Clearly, our method successfully recovered almost all the links and significantly outperformed all the competing methods. This improvement may come from i) the use of the spike and slab priors, which not only remove irrelevant elements in the projection matrices but also avoid over-penalizing the active association structures (the Laplace prior used in sparse CCA does over penalize the relevant ones) and ii) more importantly, the supervision from the labels y, which is probably the biggest difference between ours and the other methods for the association study. The prediction accuracies on unknown y and their standard errors are shown in Figure 3a and the AUC and their standard errors are shown in Figure 3b . Our proposed SHML model achieves significant improvement over all the other methods. It reduces the prediction error of elastic net (which ranks the second best) by 25%, and reduces the error of LapSVM by 48%."}, {"section_title": "AD Study", "text": "We conducted joint association analysis and AD diagnosis based on the Alzheimer's Disease Neuroimaging Initiative 1 (ADNI 1) dataset. The ADNI study is a longitudinal multisite observational study of elderly individuals with normal cognition, mild cognitive impairment, or AD. Specifically, we used SHML to study the associations of genotypes and brain atrophy measured by MRI and to predict the disease status (normal vs MCI vs AD). Note that the labels are ordinal since the three states represent increasing severity levels of AD. Genetic and phenotypic data used in this study were obtained from the ADNI database (http://www.loni.ucla.edu/ADNI). Genomic DNA samples of 818 ADNI 1 subjects were analyzed on the Human610-Quad BeadChip according to the manufacturer's protocols. After quality control, a list of 512,788 SNPs was used in an initial GWAS analysis associating them with the disease trait (AD vs. normal subjects). As a result, the top 1000 SNPs were preselected for analysis in this study. For structural MRI, we used image analysis results from UCSF based on the Freesurfer software package (http://surfer.nmr.mgh.harvard.edu); the resulting imaging data includes volumetric, cortical thickness and surface area measurements for a variety cortical and subcortical regions. After removing missing data, the final dataset consists of 618 subjects (183 normal, 308 MCI and 134 AD), and 924 SNPs and 328 MRI features measuring the brain atrophies for each subject at baseline.\nWe compared SHML with the alternative methods on accuracy of predicting whether a subject is in the normal or MCI or AD condition. We randomly split the dataset into 556 training and 62 test samples 10 times and ran all the competing methods on each partition. We used the 10-fold cross validation for each run to tune free parameters on the training data. In SHML, in order to determine k, the dimension of U, we computed the variational lower bound as an approximation to the model marginal likelihood with various k values {10, 20, 40, 60}. We chose the value with the largest approximate evidence, which led to k = 20 (see Figure 4) . Our experiments confirmed that, with k = 20, SHML achieved highest prediction accuracy, demonstrating the benefit of evidence maximization.\nThe accuracies for predicting unknown labels y and their standard errors are shown in Figure 3c . Our method achieved the highest prediction accuracy, higher than that of the second best method, GP ordinal Regression, by 10% and than that of the worst method, CCA+lasso, by 22%.\nWe also examined the strongest associations discovered by SHML based on the whole dataset. First of all, the ranking of MRI features in terms of their prediction power of different disease stages (normal, MCI and AD) demonstrates that most of the top ranked (c)\nSurf Area of R. CaudalAnteriorCingulate\nVol (CP) of R. CaudalAnteriorCingulate features are the cortical thickness measurements, followed by the volume of white matter, volume of gray matter in cortical regions, and the cortical surface area measurements. These results are consistent with the literature for demonstrating that the cortical thickness measurement is potentially a more sensitive measurement of the brain atrophy for Alzheimer's dementia. 22, 23 Particularly, thickness measurements of frontal lobe, middle temporal lobe, and precuneus were found to be most predictive compared with other brain regions. These findings are consistent with their atrophy pattern and prediction power of AD found in the literature [23] [24] [25] [26] [27] . We also found that measurements of the same structure on the left and right hemisphere have similar weights (See Table 1); this is again consistent with the related literature-no asymmetrical relationship has been found for the brain regions involved in AD. 28 Secondly, the analysis of associating genotypes to AD also generated interesting results. Similar to the MRI features, SNPs that are in the vicinity of each other are selected together due to the group-selection characteristics of our algorithm. The top ranked SNPs are associated with a few genes including PSMC1P12 (proteasome 26S subunit, ATPase), NCOA2 (The nuclear receptor coactivator 2), and WDR52 (WD repeat domain 52). These genes have been associated with diseases such as breast neoplasms, carcinoma, and endometrial neoplasms. 29 At last, biclustering of the genotype-MRI association, as shown in Figure 5 , revealed interesting patterns in terms of the relationship between genetic variations and brain atrophy in association with AD. For example, the highest ranked association was found between genes such as MAP3K1 (mitogen-activated protein kinase kinase kinase 1) and MIER3 (mesoderm induction early response 1, family member 3) with the caudate anterior cingulate cortex. MAP3K1 and MIER3 genes are associated with biological process such as apoptosis, cell cycle, chromatin binding and DNA binding (https://portal.genego.com/), and cingulate cortex has been shown to be severely affected by AD 30 . The strong association discovered in this work might indicate potential genetic effect in the atrophy pattern observed in this cingulate subregion. Additionally, SNPs in MAPT (microtubule-associated protein tau) gene were also found to have association with brain atrophy in a variety of cortical regions including frontal, cingulate and temperate lobes. The hyperphosphorylation of tau protein, which is a product of MAPT, can result in the self-assembly of tangles that are involved in the pathogenesis of AD. Therefore, the genetic variation of MAPT has been associated with increased risk of AD [31] [32] [33] [34] [35] . The association between MATP gene and brain atrophies found in this analysis is consistent with the gray matter loss observed in MATP genetic variant carrier in recent studies. 36 In summary, SHML discovered the synergistic predictive relationships between brain atrophy, genetic variations and the disease status, and achieved higher prediction accuracy than the alternative methods."}, {"section_title": "Conclusions", "text": "We have presented, SHML, a new Bayesian supervised multiview learning algorithm for AD study. By integrating association discovery with disease diagnosis, it improves performance for both tasks. Although we have focused on the AD study in this paper, we expect that SHML can be applied to a wide range of applications in biomedical research-for example, eQTL analysis supervised by additional labeling information. As to the future work, we plan to incorporate additional biological or side information into our model to improve its quality. In particular, linkage disequilibrium structures encode important correlation information between SNPs. Our current model uses independent, uniform priors over the selection probabilities of SNPs, which ignore the correlation between SNPs (note that the posterior distribution of the model does capture some correlation between genetic variations based on the data likelihood). To overcome this limitation, we plan to use graph Laplacian matrices to encode linkage disequilibrium structures and use these matrices in our prior distributions. We have explored a similar strategy to incorporate biological pathway constraints for biomarker selection and obtained improved performance over the models that do not use the pathway information. 37 We expect a similar improvement can be obtained by incorporating LD structures into SHML."}]