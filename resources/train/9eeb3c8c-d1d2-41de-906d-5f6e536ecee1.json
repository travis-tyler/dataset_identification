[{"section_title": "Abstract", "text": "We explore the main characteristics of big brain network data that offer unique statistical challenges. The brain networks are biologically expected to be both sparse and hierarchical. Such unique characterizations put specific topological constraints onto statistical approaches and models we can use effectively. We explore the limitations of the current models used in the field and offer alternative approaches and explains new challenges."}, {"section_title": "Introduction", "text": "Wikipedia defines big data as data sets that are so large or complex that traditional data processing application software is inadequate to deal with them (en.wikipedia.org/wiki/Big data). Big data is not just about the size of the data although that is the main obstacle of using traditional statistical approaches. Big data usually includes data sets with sizes beyond the ability of standard software tools to process and analyze within a reasonable time limit. Even 100MB of data can be big if existing computing resources can only handle 1MB of data at a time. Thus, the size of the data is a relative quantity respect to the available computing resources.\nIf we pick any article in big data literature these days, chances are that we often encounter hardware solutions to solving big data problems. They often suggest increasing more central processing units (CPU) or graphical processing units (GPU) and emphasize the need for cluster or parallel computing. For instance, [1] suggests to use parallel computing as a way to compute large-scale Pearson correlation coefficients for 390GB of data in the Human Connectome Project (HCP) but did not suggest any other simpler algorithmic approaches that can be implemented in a limited computing resource environment. Simply adding more hardware is not necessarily an effective but costly strategy for big data. Such hardware approaches often do not provide a venue for more interesting statistical problems. Further, the access to fast computational resources is not necessarily given to everyone. Many biological laboratories still do not have technical expertise of using cluster or parallel computing. Therefore, it is often necessary to develop more algorithmic and statistical approaches in addressing big data at least for biological sciences.\nIn this paper, we mainly focus on the statistical challenges of big data in brain imaging and networks that are somewhat different from more traditional big data problems."}, {"section_title": "Large-scale brain imaging data", "text": "Many big datasets introduce unique computational and statistical challenges that include scalability, storage bottleneck, data representation visualization, and computation mostly related to sample sizes [2] . However, the challenges in big brain imaging datasets such as HCP and Alzheimer's Disease Neuroimaging Initiative (ADNI ; adni.loni.usc.edu) are slightly different. There are substantially more number of voxels (p) per image than the number of images (n) in the datasets. Even at 3mm low resolution, functional magnetic resonance images (fMRI) has more than 25000 voxels [3] . Unless the dataset consists of more than 25000 images, brain imaging is often the problem of small-n large-p, which is different from the usual big data setting where n is often big. HCP and ADNI have n in the range of a thousands, far smaller than the number of voxels.\nTraditionally, numerical accuracy has been less of concerns in brain imaging particularly due to spatial and temporal smoothing often done in images to smooth out various image processing artifacts and physiological noises. Due to the increased sample size and the central limit theorem, which is further reinforced by smoothing, the statistical distribution of the data might become less of a concerns in big imaging data [4] .\nIn the traditional mass univariate approaches [8, 5] , where statistical inference is done at each voxel, the problem of small-n large-p is not critical. Further, spatial smoothing has the effect of reducing the number of resolution element (RESEL), so we have far less number of effective p [5] . However, the problem becomes critical in brain network modeling, where we need to correlate different voxels. In the small-n large-p setting, the computed sample covariance and correlation matrices are no longer positive definite. Subsequently, up to p \u2212 n nodes are statistically dependent although there might be no true dependency at all. Thus, there is need to regularize the covariance or correlation matrices [8] .\nThere begin to emerge large-scale brain networks with more than 25000 nodes, where each voxel is taken as a network node (Figure 1 ) [3, 6, 7] . The size of such large-scale brain networks can easily match publicly available network data with the ground-truth such as Stanford Large Network Dataset (snap.stanford.edu/data). In such large-scale networks, the small-n largep problem will be more amplified. [3] . The network is so dense, simply displaying all the nodes and edges of the network is not very informative. It is necessary to represent such dense network more sparsely. The sparse correlation model with sparse parameters \u03bb = 0.7 (middle) and \u03bb = 0.8 (right). It can be shown that they form a nested hierarchy called the network filtration."}, {"section_title": "Large-scale brain networks", "text": "Purely data-driven approaches for large-scale brain networks are not going to be computationally efficient or effective. It is often necessary to incorporate the first-order principles of the brain networks into models to possibly reduce the computational bottleneck."}, {"section_title": "Sparsity", "text": "At the microscopic level, the activation of cortical neurons in the brain show sparse and widely distributed patterns [9] . At the macroscopic level, diffusion tensor imaging (DTI) can produce up to a half million white matter fiber tracts per brain. Even then not every part of the brain is directly connected to other parts of the brain but sparsely connected [10] . In any fMRI study, not every part of brain is simultaneously activated [3] . Thus, it is reasonable to assume that brain networks are not densely connected but sparsely connected at the both microscopic and macroscopic levels. Thus, there is a strong biological motivation for modeling brain networks sparsely.\nThe small-n large-p problem in brain imaging often produces under-determined models with infinitely many possible solutions. Such problems are often remedied by regularizing the systems with additional sparse penalties. Sparse models used in brain imaging include compressed sensing (CS) [11] , sparse correlations [3] , least absolute shrinkage and selection operator (LASSO) [12, 11] , sparse canonical correlations [13] and graphical-LASSO [8, 12] . Most of these sparse models require optimizing L1-norm penalties, which has been the major computational bottleneck for solving large-scale problems in brain imaging. Thus, almost all sparse brain network models have been restricted to a few hundreds nodes or less. 2527 MRI features used in a LASSO model for Alzheimer's disease [14] is probably the largest number of features used in any sparse model in the brain imaging literature. Recently, a more scalable large-scale sparse brain network models, where each voxel is a network node, are begin to emerge [3] . For such large-scale network construction, faster scalable algorithms are needed. In [3] , the computational bottleneck of L1-optimization is overcame by simplifying the sparse network problem into an orthogonal design. Other promising methods include a constrained L 1 -minimization estimator (CLIME) [15] and faster computations for graphical-LASSO [16] although they were never applied to large-scale brain networks yet."}, {"section_title": "Hierarchy", "text": "Brain networks are fundamentally multiscale. Intuitive and palatable biological hypothesis is that brain networks are organized into hierarchies [17] . A brain network at any particular sale might be subdivided into subnetworks, which can be further subdivided into smaller subnetworks in an iterative fashion. There have been various attempts at modeling brain networks at multiple scales [17, 8, 3, 18] . Unfortunately, many multiscale models give raise to conflicting topological structures of the networks from one scale to the next. For instance, the estimated modular structure in the multiscale community detection problem usually do not have continuity over different resolution parameters [17] .\nAny sparse brain network model is usually parameterized by a tuning parameter that controls the sparsity of the solution. Increasing the sparse parameter makes the solution more sparse. Thus, sparse models are inherently multiscale, where the scale of the model is determined by the sparsity. Many existing sparse network models use a fixed parameter \u03bb that may not be optimal in other datasets or studies. Depending on the choice of the sparse parameter, the final network structure will be different [8, 18] . There is a need to develop a multiscale sparse network model that provide a consistent analysis results and interpretation regardless of the choice of parameter.\nPersistent homology may offer an effective framework in addressing the topological inconsistency in multiscale models. Instead of studying images and networks at a fixed scale, as usually done in traditional approaches, persistent homology summarizes the changes of topological features over different scales and identifies the most persistent topological features that are robust under different scales. This robust performance under different scales is needed for network models that are parameter and scale dependent. Instead of building networks at one fixed parameter that may not be optimal, persistent homological approaches exploit the topological structure of the data and models and topologically consistent nested hierarchical networks called the network filtration is obtained [18, 8] . Such a nested hierarchical structure can further speed up various computations for even for large-scale networks with a billions of connections [3] ."}, {"section_title": "Discussion", "text": "We have presented two main characterizations (sparsity and hierarchy) of brain networks that should be utilized even in big data environment. Then, we have explored various statistical challenges related to such characterizations.\nIn terms of computation, many existing brain image analysis software such as SPM (www.fil.ion.ucl.ac.uk/spm) and AFNI (afni.nimh.nih.gov) are not effective for big data. The general statistical premise of such mainstream tools is that all the image measurements are available in the computer memory and statistics are computed using all the data. However, in the big data setting, it may not be possible to fit all of the imaging data in a computer's memory, making it necessary to perform the analysis by adding one image at a time in a sequential manner. We need a way to incrementally update the statistical analysis results without repeatedly running the entire analysis whenever new images or parts of images are added.\nAn online algorithm is one that processes its inputted data in a sequential manner [19] . Instead of processing the entire set of imaging data from the start, an online algorithm processes one image at a time. That way, we can bypass the memory requirement, reduce numerical instability and increase computational efficiency. With the ever-increasing amount of large-scale brain imaging datasets such as ADNI and HCP, the development of various online statistical method is warranted [19] . Thus, here is an immediate need to develop the online version of sparse or hierarchical network models although there are no such available methods yet. Even large-scale Pearson correlation coefficients can be computed using an online algorithm."}]