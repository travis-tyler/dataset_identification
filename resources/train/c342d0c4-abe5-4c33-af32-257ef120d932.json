[{"section_title": "Abstract", "text": "The LONI Pipeline is a graphical environment for construction, validation and execution of advanced neuroimaging data analysis protocols (Rex et al., 2003) . It enables automated data format conversion, allows Grid utilization, facilitates data provenance, and provides a signifi cant library of computational tools. There are two main advantages of the LONI Pipeline over other graphical analysis workfl ow architectures. It is built as a distributed Grid computing environment and permits effi cient tool integration, protocol validation and broad resource distribution. To integrate existing data and computational tools within the LONI Pipeline environment, no modifi cation of the resources themselves is required. The LONI Pipeline provides several types of process submissions based on the underlying server hardware infrastructure. Only workfl ow instructions and references to data, executable scripts and binary instructions are stored within the LONI Pipeline environment. This makes it portable, computationally effi cient, distributed and independent of the individual binary processes involved in pipeline data-analysis workfl ows. We have expanded the LONI Pipeline (V.4.2) to include server-to-server (peer-to-peer) communication and a 3-tier failover infrastructure (Grid hardware, Sun Grid Engine/Distributed Resource Management Application API middleware, and the Pipeline server). Additionally, the LONI Pipeline provides three layers of background-server executions for all users/sites/systems. These new LONI Pipeline features facilitate resource-interoperability, decentralized computing, construction and validation of effi cient and robust neuroimaging data-analysis workfl ows. Using brain imaging data from the Alzheimer's Disease Neuroimaging Initiative (Mueller et al., 2005) , we demonstrate integration of disparate resources, graphical construction of complex neuroimaging analysis protocols and distributed parallel computing. The LONI Pipeline, its features, specifi cations, documentation and usage are available online (http://Pipeline.loni.ucla.edu)."}, {"section_title": "INTRODUCTION", "text": "Modern tools for image processing employ large amounts of heterogeneous data, diverse computational resources and distributed web-services . Effi cient analysis protocols combine diverse data, software tools and network infrastructure to obtain, analyze and disseminate results. Construction of such analysis protocols are signifi cantly enhanced by a graphical workfl ow interface that provides high-level manipulation of the analysis sequence while hiding many of its technical details. In this manuscript, we discuss the challenges of development, maintenance and dissemination of integrated resources including data, software tools and web-services, as platform-independent, agile and scalable frameworks. We demonstrate the development and utilization of the LONI Pipeline environment for combining of user computational and biological expertise with disparate resources and Grid infrastructures. Version 4 of the LONI Pipeline, extends the previous implementation of this environment (Rex et al., 2003) .\nTo provide an extensible framework for interoperability of diverse computational resources the LONI Pipeline employs a decentralized infrastructure, where tools, services and data are linked through an external resource-mediating-layer. This approach requires no modifi cations of existing tools to enable their interoperability with other computational counterparts. A new XML schema forms the backbone for the inter-resource-mediating-layer. Each XML resource (module) description includes important information about the resource location, the proper invocation protocol (i.e., I/O types, parameter specifi cations, etc.), run-time controls and data-types. Also included are auxiliary meta-data about the resource state, specifi cations, history, authorship and bibliography. This infrastructure 1 facilitates the integration of disparate resources and provides a natural and comprehensive data provenance (MacKenzie-Graham et al., 2008a) . The LONI Pipeline also enables the broad dissemination of resource meta-data descriptions via web-services and the constructive utilization of multidisciplinary expertise by experts, novice users and trainees.\nThere are a number of efforts to develop environments for tool integration, interoperability and meta-analysis (Rex et al., 2004) .\nEffi cient, distributed and interactive neuroimaging data analysis using the LONI Pipeline"}, {"section_title": "Ivo D. Dinov 1", "text": "There is a clear community need to establish effi cient tool interoperability, which enables new types of analyses and facilitates new applications . Taverna (Oinn et al., 2005) is an open-source, platform-independent graphical workfl ow environment, which enables linking tools after explicit rebuilding. It is mainly employed for bioinformatics applications via myGRID infrastructure. Kepler (Lud\u00e4scher et al., 2006 ) is another scientifi c workfl ow environment used for various applications, which also requires rebuilding each executable to link it with the core libraries. The Triana (Churches et al., 2006) workfl ow environment enables external data storage which signifi cantly improves the effi ciency and robustness of its user interface and optimizes the system requirements (e.g., low memory demands). VisTrails (Callahan et al., 2006) addresses the problem of visualization from a data management perspective where imaging data and meta-data are represented as a conjoint visualization product. Swift is another graphical workfl ow environment, which uses a scripting language, SwiftScript, to enable concise high-level specifi cation of workfl ows based on various applications using large quantities of data. The Swift engine provides effi cient execution of these workfl ows on sequential or parallel computers or distributed grids (Stef-Praun et al., 2007) . There are a number of other graphical workfl ow environments that are proposed, tested and validated for specifi c applications, types of users, scientifi c areas or hardware infrastructures (Bowers et al., 2008) .\nCompared to other graphical workfl ow environments, the LONI Pipeline offers several advantages. It facilitates the backend integration with distributed Grid-enabled and client-server infrastructures, and provides an effi cient and robust framework for deployment of new resources to the community (new tools need not be recompiled, migrated or altered in anyway to be made functionally available to the community). The choice of a particular analysis workfl ow infrastructure always depends on the application domain, the type of user, types of access to resources (e.g., computational framework, human or machine resource interface, database, etc.), as well as, the desired features and functionalities (Bitter et al., 2007) . There are inevitable similarities between the LONI Pipeline and other such environments. These include the graphical interface provided by most workfl ow environments, which facilitates the design of analysis protocols and improves the usability of these graphical protocols. Visual interfaces present complex analysis protocols in an intuitive manner and improve the management of technical details. Most of the graphical workfl ow environments provide the ability to save, load and distribute protocols through servers, SOAP/WSDL/XML or other means.\nThe LONI Pipeline addresses the specifi c needs of the neuroimaging and computational neuroscience community, but its general goals of providing portability, transparency, intuitiveness and abstraction from Grid mechanics make it appealing in other fi elds. The Pipeline is a dynamic resource manager, treating all resources as well-described external applications that may be invoked with standard remote execution protocols. The LONI Pipeline XML description protocol allows any command-line driven process, web-service or data-server to be accessed within the environment by reference, with dependencies validated (checked) dynamically on-demand. There is no need to reprogram, revise or recompile external resources to make them usable within the LONI Pipeline.\nOne side effect of this design choice is that to all external Pipeline server installations require complete installations of all software tools, services and data as currently available on the LONI Grid. This however, is only required, if a remote Pipleine server must mirror all tools and resources as available on the LONI Grid. In most situations, each site has specifi c suits of tools that they utilize to meet their computational needs. This design reduces the integration/utilization costs of including new resources within the LONI Pipeline environment. This approach provides the benefi t of quick and easy management of large and disparately located resources and data. In addition, this choice signifi cantly minimizes the user/ client machine hardware and software requirements (e.g., memory, storage, CPU). Finally, a key difference between the LONI Pipeline and some other environments is its management of distributed resources via its client-to-server infrastructure and its ability to export automated makefi les/scripts. These allow the LONI Pipeline to provide processing power independently of the available computational environment (e.g., SOLARIS, LINUX, Grid, mainframe, desktop, etc.). The LONI Pipeline servers communicate and interact with clients and facilitate secure transfer of processes, instructions, data and results via the Internet.\nVersion 4 of the LONI Pipeline introduces several important improvements and extensions of the previous version of the LONI Pipeline (Rex et al., 2003) . These include a 3-tier failover mechanism for Grid hardware, Sun Grid Engine (SGE)/Distributed Resource Management Application API (DRMAA) middleware, and the Pipeline server, as well as client-server communication, makefi le/ script export and data provenance model. LONI Pipeline v.4 also includes a new more functional and robust graphical user interface and a signifi cantly increased library of tools. V.4 also simplifi es the inclusion of external data display modules and facilitates remote database connectivity (e.g., LONI Imaging Data Archive "}, {"section_title": "MATERIALS AND METHODS", "text": "The main goal of developing the LONI Pipeline was to provide a robust and extensible infrastructure for computational neuroscience enabling effi cient data utilization, construction of reliable analysis workfl ows, and provide the means for wide dissemination and validation of research protocols and scientifi c fi ndings. The LONI Pipeline developments are subdivided into several complementary goals:\n\u2022 Effi cient Distributed Computing: Facilitate the integration of disparate, heterogeneous and multi-platform implementations of software tools, database protocols and remote web-services. The LONI Pipeline client-server communication protocol allows blending of resources that are built on remote server architectures to be accessed by the pipeline clients. This greatly lowers the usability requirements for the general user. In addition, we need a fl exible export of available pipeline workfl ows into makefi les and bash scripts that can be submitted virtually to any computational architecture.\n\u2022 Design a robust 3-tier failover mechanism for the LONI Grid: This included the three layers of the Grid submission protocolSun Grid Engine (SGE), Distributed Resource Management Application API 5 , and the Pipeline job handling server. These three layers of background-server executions enable various types of users and systems to utilize the Pipeline environment in any of these three execution modes: single machines, or main-frames with only one queue job submission protocol; Globus Grid infrastructure, and SGE Grid Infrastructure.\n\u2022 Provenance: LONI Pipeline includes a provenance manager, which enables tracking data, workfl ow and execution history of all processes. This functionality improves the communication, reproducibility and validation of newly proposed experimental designs, scientifi c analysis protocols and research fi ndings. This includes the ability to record, track, extract, replicate and evaluate the data and analysis provenance to enable rigorous validation and comparison of classical and novel design paradigms.\n\u2022 Tool Discovery: Enable expert researchers to quickly design, test and validate novel experimental designs and data analysis protocols. This is achieved via a dynamic, responsive and intelligent graphical user interface for tool exploration and construction of draft pipeline workfl ows.\n\u2022 Friendly Graphical User Interface: Create a robust environment for tool interoperability, Grid integration and low-cost interactive user interface. For maximum portability, scalability and effi ciency, this environment is built in Java and utilizes XML for storing and communication of meta-data, and descriptors for tools and services.\nThe LONI Pipeline execution environment controls the local and remote server connections, module communication, process management, data transfers and Grid mediation. The XML descriptions of individual modules, or networks of modules, may be constructed, edited and revised directly within the LONI Pipeline graphical user interface, as well as saved or loaded from disk or the LONI Pipeline server. These workfl ows completely describe new methodological developments and allow validation, reproducibility, provenance and tracking of data and results. The core six types of Pipeline specifi cations are summarized below."}, {"section_title": "TYPES OF TOOLS AND SERVICES THAT CAN BE INTEGRATED WITHIN THE LONI Pipeline", "text": "The development and utilization of the LONI Pipeline environment is focused on neuroimaging data and analysis protocols. However, by design, the LONI Pipeline software architecture is domain agnostic and has been adopted in other research and clinical fi elds, e.g., bioinformatics ). There are two major types of resources that may be integrated within the LONI Pipeline. The fi rst one is data, in terms of databases, data services and fi le systems. The second type of pipelineable resources includes stand-alone tools, comprising local or remote binary executables and services with well-defi ned command line syntax. This fl exibility permits effi cient resource integration, tool interoperability and wide dissemination."}, {"section_title": "GENERAL LONI Pipeline SPECIFICATIONS INCLUDING GRID INTEGRATION", "text": "The LONI Pipeline routinely executes thousands of simultaneous jobs on our symmetric multiprocessing systems (SMP) and on DRMAA 6 clusters. On SMP systems, the LONI Pipeline can detect the number of available processing units and scale the number of simultaneous jobs accordingly to maximize system utilization and prevent system crashes. For computer clusters, a grid engine implementing DRMAA, with Java bindings, may be used to submit jobs for processing, and a shared fi le system is used to store inputs and outputs from individual jobs. Later, we will extend the scope of the LONI Pipeline server to interact and submit jobs to other Grid infrastructures, e.g., Condor, Globus, etc.\nThe LONI Pipeline environment has been integrated with UNIX authentication using Pluggable Authentication Module (PAM), to enable a username and password challenge-response authentication method using existing credentials. A dependency on the underlying security and encryption system of the LONI Pipeline server's host machine offers maximum versatility in light of the diverse policies governing system authentication and access control.\nUsing Java binding to DRMAA interface, we have integrated the LONI Pipeline environment with the SUN Grid Engine (SGE), a free, well-engineered distributed resource manager (DRM) that simplifi es the processing and management of submitted jobs on the grid. It is important to note, however, that other DRMs such as Condor, LSF and PBS/Torque could be made compatible with the LONI Pipeline environment using the same interface. DRMAA's Java foundation allows jobs to be submitted from the LONI Pipeline to the compute grid without the use of external scripts and provides signifi cant job control functionality internally. We accomplished several key goals with the LONI Pipeline-DRMAA-SGE integration:\n\u2022 the parallel nature of the LONI Pipeline environment is enhanced by allowing for both horizontal (across compute nodes) and vertical (across CPUs on the same node) processing parallelization; \u2022 the LONI Pipeline's client-server functionality can directly control a large array of computational resources with DRMAA over the network, signifi cantly increasing its versatility and effi cacy; \u2022 facilitate the use of a heterogeneous set of neuroimaging software tools in pipelines involving large number of datasets and multiple processing tools; \u2022 the overall usability of grid resources is improved by the intuitive graphical interface offered by the LONI Pipeline environment, and \u2022 the ability to display interim results from user-specifi ed modules, which can be used for visual inspection of the outputs of various tools (interactive outcome checking)."}, {"section_title": "LONI Pipeline DATA PROVENANCE", "text": "In neuroimaging studies, data provenance, or the history of how the data were acquired and subsequently processed, is often discussed but seldom implemented (MacKenzie-Graham et al., 2008b) . Recently, several groups have proposed provenance challenges in order to evaluate the status of various provenance models (Miles et al., 2006) . For instance, collecting provenance information from a simple neuroimaging workfl ow and documenting each system's ability to respond to a set of predefi ned queries. Some of the existing provenance systems are designed as mechanisms for capturing provenance in neuroimaging (MacKenzie-Graham et al., 2008a; Zhao et al., 2007) . It is diffi cult to provide systematic, accurate and comprehensive capture of provenance information with minimal user intervention. The processes of data provenance and curation are signifi cantly automated via the LONI Pipeline. Each dataset has a provenance fi le (*.prov) that is automatically updated by the LONI Pipeline, based on the protocols used in the data analysis. This data processing history refl ects sequentially the steps that a dataset goes through and provides a detailed record of the types of tools, versions, platforms, parameters, control and compilation fl ags. The data provenance can be imported and exported by the LONI Pipeline, which enables utilization internally by other Pipeline workfl ows or by external resources (e.g., databases, workfl ow environments). Provenance can be used for determining data quality, for result interpretation, and for protocol interoperability (Simmhan et al., 2005; Zhao et al., 2007) . It is imperative that the provenance of neuroimaging data be easily captured and readily accessible (MacKenzieGraham et al., 2008b) . For instance, increasingly complex analysis workfl ows are being developed to extract information from large cross-sectional or longitudinal studies in multiple sclerosis (Liu et al., 2005 ), Alzheimer's disease (Fleisher et al., 2005) , autism (Langen et al., 2007) , depression (Drevets, 2001) , schizophrenia (Narr et al., 2007) , and studies of normal populations (Gogtay et al., 2006) . The implementation of the complex workfl ows associated with these studies requires provenance-based quality control to ensure the accuracy, reproducibility, and reusability of the data and analysis protocols.\nWe designed the provenance framework to take advantage of context information that can be retrieved and stored while data is being processed within the LONI Pipeline environment (MacKenzieGraham et al., 2008b) . Additionally, the LONI Provenance Editor is a self-contained, platform-independent application that automatically extracts provenance information from image headers (such as a DICOM images) and generates an XML data provenance fi le with that information. The Provenance Editor 7 allows the user to edit the meta-data prior to saving the provenance fi le, correcting inaccuracies or adding additional information. This provenance information is stored in.prov fi les, XML formatted fi les that contain the meta-data and processing provenance and follows the XSD defi nition 8 . Then the data provenance is expanded by the LONI Pipeline to include the analysis protocol, the specifi c binaries used for analysis, and the environment that they were run in. The LONI Pipeline dramatically improves compliance by minimizing the burden on the provenance curator. This frees the user to focus on performing neuroimaging research rather than on managing provenance information."}, {"section_title": "LONI Pipeline INTELLIGENCE", "text": "Construction of elaborate, functional and valid workfl ows within the LONI Pipeline environment requires deep understanding of the research goals, tool specifi cations and neuroscientifi c expertise. To enhance the usability of this environment, we developed an intelligent LONI Pipeline component. It has two complementary features -constructive and validating. The pipeline constructive intelligent feature uses the spectra of available module descriptors and pipeline workfl ows to automatically generate valid versions of new graphical protocols according to a set of user-specifi ed keywords. This intelligence feature uses a grammar on the set of XML module and pipeline descriptions to determine the most appropriate analysis protocol, and its corresponding module inputs and outputs, according to the keywords provided by the user. Then, it exports a.pipe fi le, which contains a draft of the desired analysis protocol, Figure 1 .\nThe pipeline validating intelligence feature offers interactive support for running or modifying existent pipeline workfl ows. This feature contextually monitors the consistency of the data types, parameter matches, validity of the analysis protocol, and ensures optimal job-submission (e.g., order of module execution). The LONI Pipeline intelligence component reduces the need to review in details of, and double check modifi cations of new or existing workfl ows. Still, users control the processes of saving workfl ows and module descriptions, data input and output, and the scientifi c design of their experiments. This functionality signifi cantly improves usability and facilitates scientifi c exploration."}, {"section_title": "LONI Pipeline GRAPHICAL AND SCRIPTING INTERFACES", "text": "Pipeline workfl ows (.pipe fi les) may be constructed in many different ways (e.g., using text editors) and these protocols may be executed in a batch mode without involving the LONI Pipeline graphical user interface (GUI). However, the LONI Pipeline GUI signifi cantly aids most users in designing and running analysis workfl ows. A library of available tools for usage is presented on the left hand side of the LONI Pipeline client window. Users may search for, drag and drop these tools onto the main canvas to create or revise a workfl ow. Connections between the nodes are used to represent the piping of output from one program to another. This is accomplished without requiring the user to specify fi le paths, server locations or command line syntax. Pipeline workfl ows may be constructed and executed with data dynamically fl owing (by reference) within the workfl ow. This enables trivial inclusion of pipeline protocols in external scripts and integration into other applications. Currently, the LONI Pipeline allows exporting of any workfl ow from XML (*.pipe) format to a makefi le or a bash script for direct or queuing execution."}, {"section_title": "FUNCTIONALITY AND USABILITY", "text": "In the past 3 years, we have gone through several cycles of design, implementation, analysis and re-design stages of the new LONI Pipeline. During this process a number of usability issues were addressed. These included the editing and usage modes of the graphical user interface, state specifi c menus, pop-up and information dialogs, the handling of local and global variables within the pipeline, the integration of data sources and executable module nodes, data type checking and workfl ow validation, client connect and disconnects, job management and client-server communications. All of these were critical in improving the usability of the LONI Pipeline and are necessary before the execution of any data analysis workfl ow. The core LONI Pipeline functionality is based on our prior experience (Rex et al., 2003) , user feedback and information technology advancements over the past several years. The current LONI Pipeline functionality includes -tool discovery engine, plug-in interface for meta algorithm design, grid interface, secure user authentication, data transfers and client-server communications, graphical and batch-mode execution, encapsulation of tools, resources and workfl ows, and data provenance."}, {"section_title": "RESULTS", "text": "LONI Pipeline can be used to construct a wide variety of processing and analysis workfl ows. Here we demonstrate the utilization of the LONI Pipeline to conduct and validate new (semi)automated, robust and user-friendly protocols for (1) regional parcellation and volume extraction, (2) population-based atlas construction, and (3) the analysis of multiple population cohorts. In the following sections, we discuss the graphical Pipeline workfl ows for each of these applications:"}, {"section_title": "BRAIN PARCELLATION", "text": "Regional parcellation of distinct brain regions is often needed to perform region-of-interest-based analyses between healthy as compared to diseased subjects. Manual region drawing can be labor intensive, prone to errors, and have poor reproducibility. Figure 2 illustrates a pipeline workfl ow constructed to automatically extract 3D masks of 56 regions of interest using Brain Parser (Tu et al., 2008) 9 . These regions can be then be used to examine regionally specifi c shape characteristics among other variables of interest to the neuroimaging community. The ability to automatically obtain robust 3D masks of various brain regions is a critical step in many brain mapping studies."}, {"section_title": "BRAIN ATLAS CONSTRUCTION", "text": "Brain atlasing is a major research effort in the fi eld and the development of effi cient workfl ows to take large numbers of T1-weighted anatomical images, spatially warp them into a common space, and then to pool them to result in a representative atlas is often a complex process. Development of effi cient workfl ows and utilizing a large-scale computational Grid, based at LONI, permits streamlined and rapid atlas creation in normal subjects as well as in disease, Figure 3 . Using Automated Image Registration (Woods et al., 1998) , we constructed a workfl ow to systematically create a whole brain atlas for use in describing the average brain anatomical structure in patients drawn from the ADNI series of Alzheimer's subject MRI data contained in the LONI Image Data Archive (IDA) (Mueller et al., 2005) . Such atlases characterize \"mean\" population features such as shape, regional area, sulcal anatomy, etc. "}, {"section_title": "STRUCTURAL ANALYSIS OF ALZHEIMER'S DISEASE (AD) NEUROIMAGING STUDY", "text": "We used brain imaging data from the Alzheimer's Disease Neuroimaging Initiative, ADNI (Mueller et al., 2005) , to demonstrate the processes of construction, validation and execution of integrated workfl ow analysis protocols. This AD pipeline workfl ow represents a complex neuroimaging analysis protocols based on disparate tools, data and distributed parallel-computing infrastructure. Figure 4 demonstrates this Alzheimer's disease Pipeline workfl ow. The left-panel in the Pipeline environment contains some predefi ned module defi nitions and complete workfl ows. The user may drag-and-drop these in the main workfl ow canvas to design new analysis protocols. The central workfl ow canvas shows that main six steps of the AD data analysis. These include data conversion, volumetric data pre-processing, automated extraction of regions of interest, shape processing, global shape analysis and automated cortical surface extraction. Each of these steps is itself a nested collection of groups of modules, a nested pipeline workfl ow, which contains a series of processing steps. The insert-fi gure illustrates the 3-level deep nested processing part of the Global Shape Analysis node (see the top-level tabs of the insert). This pipeline workfl ow demonstrates the entire data processing and analysis protocol, from retrieval of the data from the LONI Imaging Data Archive 10 , through the data manipulation, shape processing, generation of derived data (e.g., global shape measures like curvature, fractal dimension, surface area, etc.), to the fi nal statistical analysis. In this case, the study design included three age-matched populations -asymptomatic subjects (NC), minor cognitive impairment (MCI), and Alzheimer's disease (AD) patients. There were fi ve males and fi ve females for each group and each subject was scanned several times longitudinally. A total of 104 brain volumes were automatically processed in about 26 h. The time of workfl ow completion depends on the study and workfl ow designs, number of subjects, and general hardware infrastructure specifi cations (e.g., system characteristics and user demand). The results of this completely automated pipeline workfl ow included cortical surface representations (shapes) for each subject, parcellations of the raw MRI brain scans into 56 regions of interest (labels), surface models for each of the 56 regions for each subject and time-point, and global statistical mapping identifying the NC, MCI and AD group differences for each of the 56 regions. Figure 5 depicts the shape-curvature measure for fi ve regions of interest (ROI's) at two time-points -baseline (blue) and 12-month follow-up (green) for the cohort of normal subjects (NC). Figure 6 compares the shape measures for one region (right Superior Frontal Gyrus) across all three cohorts, at baseline (time = 0). Notice the consistent decrease of shape and volume measures, for both time "}, {"section_title": "DISCUSSION", "text": "Interactive workfl ow environments for automated data analysis are critical in many research studies involving complex computations and large datasets (Kawas et al., 2006; Myers et al., 2006; Oinn et al., 2005; Taylor et al., 2006) . There are three distinct necessities that underlie the importance of such graphical frameworks for management of novel analysis strategies -high data volume and complexity, sophisticated study protocols and demands for distributed computational resources. These three fundamental needs are evident in most modern neuroimaging, bioinformatics and multidisciplinary studies. The LONI Pipeline environment aims to provide distributed access to varieties of computational resources via its graphical interface. The ability of investigators to share, integrate, collaborate and expand resources will increase the statistical power in studies involving heterogeneous datasets and complex analysis protocols. New challenges that emerge from our increased abilities to utilize computational resources and hardware infrastructure include the need to assure reliability and reproducibility of identically analyzed data, and the desire to continually lower the costs of employing and sharing data, tools and services. The LONI Pipeline environment attempts to provide the means to address these diffi culties by providing secure integrated access to resource visualization, databases and intelligent agents.\nThe LONI Pipeline already has been used in a number of neuroimaging applications including health (Sowell et al., 2007) , disease (Thompson et al., 2003) , animal models (MacKenzie-Graham et al., 2006) , volumetric (Luders et al., 2006) , functional (Rasser et al., 2005) , shape (Narr et al., 2007) and tensor-based (Chiang et al., 2007) studies. The LONI Pipeline infrastructure improved consistency, reduced development and execution times, and enabled new functionality and usability of the analysis protocols designed by expert investigators in all of these studies. Perhaps the most powerful feature provided by the LONI Pipeline environment is the ability to quickly communicate new protocols, data, tools and service resources, fi ndings and challenges to the wider community.\nThe main LONI Pipeline page 11 provides links to the forum, support, video tutorials and usage. There are examples demonstrating how to describe individual modules and construct integrated workfl ows. Version information, download instructions and server/ forum account information is also available on this page. There are example pipeline workfl ows and the XSD schema defi nition 12 for the.pipe format used for module and workfl ow XML description. Users may either install Pipeline servers on their own hardware systems, or they may use some of the available Pipeline servers. The primary LONI Pipeline server is cranium.loni.ucla.edu. It utilizes a CentOS-based compute cluster comprised of approximately 800 Core 2, 2.4GHz, 8GB RAM, AMD Opteron processors. Each dualprocessor compute node has eight gigabytes of memory to accommodate memory-intensive neuroimaging applications. We selected SUN Grid Engine v6, bound by DRMAA, as the LONI Pipeline distributed resource manager. A highly-optimized non-blocking Cisco Gigabit network provides the connectivity infrastructure with sixteen terabytes of fault-tolerant, clustered storage from Isilon Systems acting as a cache fi le system for the LONI Pipeline environment. Users may obtain accounts on this Grid 13 . In general, some practical diffi culties in validating new LONI Pipeline workfl ows may be caused by unavailability of the initial raw data, differences of hardware infrastructures or variations in compiler settings and platform confi gurations. Such situations require analysis workfl ow validation by teams of experts capable of validating the input, output and state of each module within the pipeline workfl ow. Further LONI Pipeline validation would require comparison between synergistic workfl ows that are implemented using different executable modules or module specifi cations. For example, one may be interested in comparing similar analysis workfl ows by choosing different sets of imaging fi lters, reconfi guring computation parameters or manipulating the resulting outcomes, e.g., fi le format, (Bitter et al., 2007) . Such studies contrasting the benefi ts and limitations of each resource or processing workfl ow aid both application developers and general users in the decision of how to design and utilize module and pipeline defi nitions to improve resource usability.\nA signifi cant challenge in computational neuroimaging studies is the problem of reproducing fi ndings and validating analyses described by different investigators. Frequently, methodological details described in research publications may be insuffi cient to accurately reconstruct the analysis protocol used to study the data. Such methodological ambiguity or incompleteness may lead to misunderstanding, misinterpretation or reduction of usability of newly proposed techniques. The LONI Pipeline mediates these diffi culties by providing clear, functional and complete record of the methodological and technological protocols for the analysis.\nEven though the LONI Pipeline was designed and tested to solve neuroimaging problems, its generic architecture will permit applications in other fi elds, where computationally intense tasks are performed and there is a need of resource interoperability. Its light-weight and platform-independent design and its low memory requirements make the LONI Pipeline potentially useful in many research fi elds relying on the integration of large and heterogeneous processing protocols. For example, the LONI Pipeline was recently used in conjunction with a number of bioinformatics data processing and analysis protocols . We are also working on several new features of the LONI Pipeline including web-service-based client interface, direct integration with external resource archives (e.g., http://www.ncbcs.org/biositemaps, http:// NeuroGateway.org, etc.) and interface enhancements using intelligent plug-in components."}]