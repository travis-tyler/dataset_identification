[{"section_title": "", "text": "ii RECOMMENDATIONS 1. Score the 2009 ARMS III Core sample using criteria specified in the five models. Records meeting the criteria specified on one or more models will be flagged as likely respondents. 2. Conduct a similar study using ARMS III Core 2005training, 2006validation, and ultimately 2007 test data to flag 2010 mail nonrespondents. 3. Contact flagged and confirmed ARMS III Core 2007 mail nonrespondents for cognitive interviews in order to identify alternative incentives for use in 2010. 4. Randomly divide flagged 2010 mail nonrespondents into three groups: 1) a control group receiving no incentive, 2) a treatment group receiving a $20 ATM card incentive, and 3) a treatment group receiving an alternative incentive identified via cognitive interviews, to determine if the identified alternative incentive is more effective for the given mail nonresponse group. 1"}, {"section_title": "INTRODUCTION", "text": "On February 2, 2006, a panel was formed within the National Research Council (NRC) at the recommendation of the Committee on National Statistics, Division of Behavioral and Social Sciences and Education to review the United States Department of Agriculture's (USDA) Agricultural Resource Management Survey (ARMS). Two years later, the NRC released a report entitled Understanding American Agriculture; Challenges for the Agricultural Resource Management Survey (2008). Section 6 of the NRC ARMS review specifically addresses nonresponse, imputation, and estimation. The review recommends that the USDA's National Agricultural Statistics Service's (NASS) Research and Development Division (RDD) explore characteristics of nonrespondents, as well as the relationship between incentives and nonresponse bias: Recommendation 6.3: The nature of ARMS nonresponse bias should be a key focus of the research and development program the panel recommends. This research and development program should focus, initially, on understanding the characteristics of nonrespondents. Recommendation 6.4: The research and development program should continue NASS's work on both public relations and incentives, and it should do so with a focus on nonresponse bias, not simple nonresponse rates. The ARMS is conducted in three phases. Phase I screens for potential samples for Phases II and III. Phase II collects data on cropping practices and agricultural chemical usage, while Phase III collects detailed economic information about the agricultural operation, as well as the operator's household. ARMS data are used by farm organizations, commodity groups, agribusiness, Congress, State Departments of Agriculture, and the USDA. The USDA uses ARMS data to evaluate the financial performance of farms and ranches, which influence agricultural policy decisions. The Department also uses the ARMS Phase III (ARMS III) data for objective evaluation of critical issues related to agriculture and the rural economy; therefore, it is essential that measures be taken to minimize bias, especially for Phase III."}, {"section_title": "Problem", "text": "In an effort to increase response rates, the USDA's NASS began experimenting with monetary incentives in the 2004 ARMS III (Beckler, Ott, & Horvath, 2005). Follow-up assessments of the monetary incentive in the 2005 ARMS III, where operations were mailed a pre-survey letter with a pre-paid $20 ATM card prior to the survey, demonstrated that ATM cash cards are beneficial in increasing response rates and decreasing survey costs for ARMS questionnaires mailed to respondents (McCarthy, Beckler, & Ott, 2006); however, it is unknown how much the effectiveness of the monetary incentive varies across sampled entities. Are certain operations likely to respond regardless of incentives? Are certain operations more likely to respond via mail given a monetary incentive? Lastly, are there operations more likely to respond via mail given an alternative incentive? Without a basic understanding of operation characteristics, specifically those unique to ARMS III respondents versus nonrespondents, it is unclear whether incentives either vary in effectiveness or are distributed efficiently. Furthermore, offering incentives may increase response rates, but it does not necessarily decrease bias. There are four possible outcomes when giving incentives: 1) if persons are already more apt to respond and begin responding at a higher rate given incentives, we exacerbate response bias; 2) if persons previously responding stop responding, nonresponse bias may be increased; 3) if prior nonrespondents respond, we may reduce nonresponse bias; and 4) if prior nonrespondents continue not 2 responding bias may continue ( Figure 1). Note that only one of these outcomes results in a reduction of bias."}, {"section_title": "Figure 1. Incentive Outcomes", "text": ""}, {"section_title": "Purpose", "text": "This pilot study explores and identifies characteristics of 2003 through 2007 ARMS III respondents by testing whether certain operations are likely to respond regardless of incentivesthe rationale being that the ATM monetary incentive may vary in effectiveness based on operation characteristics, and thus incentives may be unnecessary when persons are already apt to respond. By flagging persons likely to respond given no incentive, NASS may be able to decrease response bias and survey costs, and better allocate incentive funds toward those least likely to respond. Ultimately, this study aims to demonstrate a method for identifying likely respondents."}, {"section_title": "Research Questions:", "text": "What are the characteristics of operations that are likely to respond to the ARMS III Core Version regardless of incentives?"}, {"section_title": "METHOD", "text": "In order to identify characteristics of ARMS III respondents, 2002 Census of Agriculture data were matched to sampled operations (both respondents and nonrespondents) in the 2003-2007 ARMS III Core Version. The research included data on various operation characteristics from the Census of Agriculture that were recommended by both NASS's Chief Cognitive Research Methodologist and Chief Research Statistician. These operation characteristics were used to predict respondents in the 2003-2007 ARMS III using classification trees."}, {"section_title": "Procedure", "text": "Classification or decision trees (these terms are used interchangeably) were used to identify characteristics of ARMS III Core Version respondents. Classification trees model relationships with a categorical outcome (respondent or nonrespondent) using a tree-like structure. In this type of analysis, the full data were comprised of the 2002 Census of Agriculture data for the 2003-2007 ARMS III Core form sample. The Core Version of the ARMS survey is the part of the sample that has included a $20 ATM Card mailing as an incentive and is the only part of the ARMS survey in which questionnaires are mailed to respondents. If a response is not received by mail, an enumerator will attempt to complete a face to face interview. The Core Version is only used in the 15 estimating states, which include the 15 leading cash receipts states (Arkansas, California, Florida, Georgia, Illinois, Indiana, Iowa, Kansas, Minnesota, Missouri, Nebraska, North Carolina, Texas, Washington, and Wisconsin). Maryland and Delaware were given special permission to use the Core Version in place of face-to-face interviews in 2003 and 2005 due to an avian influenza epidemic. Their data are also included. The data were broken into subsets by year to be used as the training (2003), validation 2004, and test (2005)(2006)(2007) sets. The training dataset was used to construct the initial tree model that identified subsets of records that responded at a higher rate than the overall sample. This model was applied to the validation dataset in order to prevent generating a model for the training data that would not fit other data or that would be unreliable (i.e. overfitted). The validation data were used when pruning the initial tree to generate the final model. Finally, the test data were used to evaluate the model's performance on independent data not used in the creation of the model. In this case, the initial tree was constructed using data from years immediately prior to the use of any incentives in the ARMS III survey. This model was applied to the test data, which consisted of the years following the introduction of incentives, to identify groups of records that responded consistently regardless of the use of incentives. A decision tree model is constructed by segmenting the data through the application of a series of simple rules. Each rule assigns an observation to a subsegment based on the value of one input variable. One rule is applied after another, resulting in a hierarchy of segments within segments. The rules are chosen to maximally separate the subsegments with respect to the target variable. Thus, the rule selects both the variable and the best breakpoint to maximally separate the resulting subgroups. Variables may appear multiple times throughout the tree for further segmentation. The resulting hierarchy is called a tree, and each segment is called a node. The original segment contains the entire data set and is called the root node of the tree. A node with all its successors is termed a branch of the node that created it. The final nodes are called leaves. In our analysis, we are ultimately interested in the leaves that contain a higher proportion of records with the target (response). Decision trees describe subsets of data and are constructed without any theoretical guidance. Variables are chosen to maximally separate the subsegments, so only one or a few similar correlated variables (which individually might be related to the target) may appear in the tree. There are several alternative methods for constructing decision trees. For the purposes of this report, trees were grown using the chi-square approach available in SAS Enterprise Miner, which is similar to the chi-square automatic interaction detection (CHAID) algorithm (deVille, 2006). There are multiple stopping criteria used to decide how large to grow a decision tree. Generally, trees were pruned so leaves represented at least 500 records or when adding additional leaves did not markedly improve the overall misclassification rates of the tree as a whole. All trees had similar misclassification rates for the training and validation datasets used to grow the trees and for the test data used to verify reliability of the trees after construction. For the purposes of this study, the target was ARMS III Core Version response. Operations responding to ARMS III were marked with a \"1\" and those not responding with a \"0\" in a new survey response target variable. A decision tree considers all input variables (independent variables) and grows branches using input variables that demonstrate significant relationships with the target, while also considering interaction effects between the various inputs. The classification trees described in this study explored the relationship between operation characteristics and survey response. Trees were grown using the 2003 sample to train the models and identify significant splits. Trees were pruned and validated by assessing the average squared error of the model using data from the 2004 sample. Reliability of the trees was tested and compared using the 2005, 2006, and 2007 samples. It is assumed that characteristics consistently associated with significantly higher response rates from 2003 through 2007 are invariant to the effect of incentives, since no incentives were given in 2003 and 2004, but were in 2005, 2006, and 2007. In a typical classification tree approach, the best initial splitting variable would be chosen and a single model built. Many models can be built using a single dataset, with increasing misclassification rates. Each model will identify different (but possibly overlapping) subgroups. For this project, five separate models were built using each of the top five best initial splitting variables. Each of these five models was grown by forcing the primary split on a different one of the five potential splitting variables. All variables were available for each of the models and subsequent splits were determined automatically by the software. The groups of records with highest response rates were selected from each model. Each model identified unique subsets of respondents based on varying initial splits; furthermore, significance levels used to evaluate the initial splits were based solely on the training data. By creating several complementary models, we identified more respondents than we could have using a single model, and we were able to reevaluate the strength of the models in comparison to one another using the training, validation, and test data. The significance of potential splitting variables was assessed using the LogWorth statistic, which measures how well a given input variable measures the target. All five decision trees were comparable, and thus, were explored for two reasons: 1) The LogWorth of initial split variables is calculated using only the training data 2003, so although it may be highly significant in the training phase, it may prove unreliable using the validation data (2004) or the test data (2005)(2006)(2007). Therefore, competing models may in fact produce better results when tested over time. 2) The characteristics identified in a given tree vary given the variable used in the initial split; therefore, each tree is capable of identifying unique subsets of respondents. Predicted response probabilities generated using the five models are available for scoring of future ARMS III samples."}, {"section_title": "2.2 Data", "text": "Data from the 2002 Census of Agriculture were matched to both respondents and nonrespondents in the ARMS III Core Version sample. Associated characteristics of respondents were identified prior to incentives being used in the ARMS III (2003III ( -2004. There were 28,372 records with available 2002 Census data in this set. These models were used to flag likely respondents after incentives were used in the ARMS III Core Version (2005)(2006)(2007). There were 40,487 records with matching census data in this second data set. In order to ensure reliability of results, data were partitioned into three groups: training, validation, and test. Training data were used to grow trees. Validation data were used to prune trees when classification became unreliable. Test data were used to compare trees (models) in terms of gain rates and reliability over time. The respondent characteristic models data for the ARMS III Core Version sample were identified using the available 2002 census data. The respondent characteristic models were trained using the matched 2003 sample (n = 14,193), validated using the matched 2004 sample (n = 14,179), and tested using the matched 2005 (n = 14,027), 2006 (n = 13,614), and 2007 (n = 12,846) samples. Census data were matched and available for most records. See Appendix A for a comparison of match rates by year, version, and respondents versus nonrespondents."}, {"section_title": "Variables", "text": "Eighty-four variables from the 2002 Census were selected and used to explore respondent characteristics. The variables included descriptive information about the operation such as its size, the type of commodities produced, its location, etc. as well as information about the principal operator, such as the operator's race, gender, number of days worked off the farm, etc. The full list of variables used is shown in Table 1. 6 1) Acres of Cropland Harvested 3 less than 211 acres versus Acres of Cropland Harvested equal to or greater than 211 acres ( Figure 2); 2) Acres of Cropland less than 354 acres versus Acres of Cropland equal to or greater than 354 acres ( Figure 3); 3) Total Sales Not Under Production Contract (NUPC) less than $43,551 versus greater than or equal to $43,551 ( Figure 4); 4) Sum of Cropland Harvested 4 less than 211 acres versus greater than or equal to 211 acres ( Figure 5); and 5) State: California, Delaware, Florida, Illinois, Indiana, Iowa, Kansas, Minnesota, Missouri, Nebraska, Washington, and Wisconsin versus Arkansas, Georgia, North Carolina, and Texas ( Figure 6)."}, {"section_title": "Models", "text": "In order to reduce the risk of future misclassification, only the subgroups that demonstrated substantial gains (response rates > 80 percent), and thus minimal misclassification rates in 2003 and 2004, were selected to design future scoring criteria. This approach resulted in one subgroup being selected from each model."}, {"section_title": "Model One: Acres of Cropland Harvested", "text": "The first model split used Acres of Cropland Harvested less than 211 acres versus Acres of Cropland Harvested equal to or greater than 211 acres (Figure 2). This model identified the 13,919 operations with less than 211 Acres of Cropland Harvested in Arkansas, Georgia, Illinois, North Carolina, and Texas, and Total Sales Not under Production Contract less than $522,250, exhibiting response rates between 78 percent and 83 percent (2003)(2004)(2005)(2006)(2007)."}, {"section_title": "Model Two: Acres of Cropland", "text": "The second model split using Acres of Cropland less than 354 acres versus Acres of Cropland greater than or equal to 354 acres (Figure 3). This model identified the 10,910 operations with less than 354 Acres of Cropland in Arkansas, Georgia, Illinois, North Carolina, and Texas with Total Sales Not under Production Contract less than $38,074, exhibiting response rates between 80 percent and 84 percent (2003 -2007)."}, {"section_title": "Model Three: Total Sales Not Under Production Contract (NUPC)", "text": "The third model split using Total Sales Not under Production Contract (NUPC) less than $43,551 versus greater than or equal to $43,551 (Figure 4). This model identified the 15,206 operations with Total Sales Not under Production Contract less than $43,551 in Arkansas, Florida, Georgia, Illinois, Indiana, North Carolina, and Texas, exhibiting response rates between 80 percent and 84 percent (2003 -2007). The fourth model split using Sum of Cropland Harvested less than 211 acres versus greater than or equal to 211 acres ( Figure 5). This model identified the 14,678 operations with Sum of Cropland Harvested less than 211 acres, in Arkansas, Georgia, Illinois, North Carolina, and Texas, exhibiting response rates between 77 percent and 82 percent (2003)(2004)(2005)(2006)(2007)."}, {"section_title": "Model Five: State", "text": "The fifth model split using two state groupings: 1) California, Delaware, Florida, Illinois, Indiana, Iowa, Kansas, Maryland, Minnesota, Missouri, Nebraska, Washington, and Wisconsin; and 2) Arkansas, Georgia, North Carolina, and Texas ( Figure 6). This model identified the 17,181 operations in Arkansas, Georgia, North Carolina, and Texas with Total Acres Operated less than 201 acres, exhibiting responses rates between 75 percent and 81 percent (2003)(2004)(2005)(2006)(2007). 3.1.6 All Models: Model One, Model Two, Model Three, Model Four, and Model Five The \"All Models\" indicator (applied to any record flagged as a potential respondent by Model One, Model Two, Model Three, Model Four, and Model Five) identified the 9,272 operations that appeared in each of the model nodes above. These operations had response rates between 81 percent and 85 percent (2003)(2004)(2005)(2006)(2007).      "}, {"section_title": "Accuracy Assessments", "text": "The identified subgroups of likely respondents maintained consistent rates of response over time (2003)(2004)(2005)(2006)(2007) even after incentives were introduced (Figure 7)."}, {"section_title": "Figure 7. Model Accuracy over Time", "text": "In other words, these subgroups of operations responded at a higher rate than the sample as a whole, both with and without incentives. Data from 2003-2007 were used to compare the five respondent characteristic subgroups and determine average response rate gain and variability over time. Model Using the \"All Models\" indicator to identify likely respondents will result in the greatest respondent classification accuracy; however, it also identifies the smallest group of respondents ( Figure 8). Although there is a 4.60 percent drop in prediction accuracy when using the \"Any Model\" indicator versus the \"All Models\" indicator, the \"Any Models\" indicator correctly identified over twice as many respondents; therefore, using the \"Any Models\" indicator provides the potential for saving over twice the resources, that may be better allocated toward converting likely mail nonrespondents not enticed by the $20 ATM Card currently offered.   130,157.67, saving between $119,370.08 (47.84 percent) and $6,290.08 (4.61 percent) annually."}, {"section_title": "Figure 8. Number of Respondents Correctly Identified by Model", "text": "Using \"Any Model\" to identify likely respondents and reallocate monetary incentive funds will result in the greatest annual savings between $6,290.08 and $119,370.08 and will provide NASS with the ability to reallocate funds earmarked for those sample units to entice likely mail nonrespondents currently not enticed by the monetary incentive. The models discussed in this report identify consistent respondent characteristics with or without the use of monetary incentives. In order to determine the operations for which incentives are not necessary (those already responding at higher rates than other operations), criteria identified by any of the five models should be used to score future ARMS III Core samples, starting with 2009. In fact, to minimize bias, once these operations are identified, their previously allocated incentive funds could be redirected towards exploring alternative incentives for consistent mail nonrespondents currently unresponsive to monetary incentives. The identification of those non-respondent subgroups will be the subject of a follow-up research report to this one. Redirecting these funds is not only cost effective, but also works toward reducing both response and nonresponse bias. Given that the above groups are already more apt to respond relative to the rest of the sample, efforts should focus on soliciting responses from underrepresented operations, not necessarily those already represented, if the goal is ultimately to reduce nonresponse bias. The follow-up report will identify mail nonrespondents using 2002 Census data for ARMS III Core 2005-2007 samples. Cognitive interviews should be conducted with a sample of operations identified as mail nonrespondents in 2007, in order to determine alternative incentives or data collection strategies, since the current $20 ATM monetary incentive appears ineffective. Based on cognitive interviews, alternative incentives will be identified for specific groups of mail nonrespondents and recommended for use with the 2010 ARMS III Core sample. Ultimately, it is expected that this two-phase study will result in more efficient incentive allocation methods, and reductions in survey costs, nonresponse, and bias."}, {"section_title": "LIMITATIONS", "text": "Although the above research aims to improve data quality and reduce the waste of taxpayer funds, implementation depends on approval from the Office of Management and Budget (OMB) which currently has only approved equitable distribution of incentives. The Guidance on Agency Survey and Statistical Information Collections report states, \"Agencies should treat all respondents equally with regard to incentives. OMB generally does not approve agency plans to give incentives solely to convert refusals, or treat specific subgroups differently, unless the plan is part of an experimental design for further investigation into the effects of incentives\" (2006, p.25). If such an incentive allocation is approved for an experimental design study, the question becomes whether differential allocation may continue beyond the experiment. If it is not approved, alternative uses of the research may include eliminating incentives and reallocating funds towards oversampling likely mail nonrespondent groups. However, although this will likely reduce bias, it will increase the overall likelihood of nonresponse within the sample, almost ensuring that response rates will remain well below the OMB's standard of 80 percent. Another potential alternative could be elimination of incentives and reallocation of funds towards rewarding enumerators for obtaining good responses from operations identified as likely nonrespondents. However, this too has a downside in that encouraging refusal conversions may not actually improve data quality. It is possible that enumerators will feel financially pressured to convert a refusal regardless of reporting capability or accuracy."}, {"section_title": "RECOMMENDATIONS", "text": "1. Score the 2009 ARMS III Core sample using criteria specified in the five models. Records meeting the criteria specified on one or more models will be flagged as likely respondents. 2. Conduct a similar study using ARMS III Core 2005training, 2006validation, and ultimately 2007 test data to flag 2010 mail nonrespondents. 3. Contact flagged and confirmed ARMS III Core 2007 mail nonrespondents for cognitive interviews in order to identify alternative incentives for use in 2010. 4. Randomly divide flagged 2010 mail nonrespondents into three groups: 1) A control group receiving no incentive, 2) a treatment group receiving a $20 ATM card incentive, and 3) a treatment group receiving an alternative incentive identified via cognitive interviews to determine if the identified alternative incentive is more effective for the given mail nonresponse group.   "}, {"section_title": "7.", "text": ""}, {"section_title": "APPENDIX A", "text": ""}]