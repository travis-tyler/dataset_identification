[{"section_title": "INTRODUCTION", "text": "In national and international testing programs, multiple forms of a single test are used to provide test security or to allow sampling a large of items without having each student answer all of the items. Alternative test forms which developed considering the same construct and blueprint almost differ somewhat in their difficulty. If one form is more difficult than other form, examinees would be expected to get higher scores from the easier form and get lower scores from the more difficult form. Test equating is required to remove effects on scores of these undesirable differences in test form difficulty (Dorans, Moses, & Eignor, 2010). As Kolen and Brennan (1995, p. 2) defined \"equating is a statistical process that is used to adjust scores on test forms so that scores on the forms can be used interchangeably.\" In testing programs like Trends in International Mathematics and Science Study (TIMSS), Programme for International Student Assessment (PISA) common-item nonequivalent groups (CINEG) design is used to equate test scores. In CINEG design, common items from different test forms are used to equate test forms. Like other statistical analysis methods, common item test equating is exposing to sampling error. One way to reduce sampling error is to conduct equating with large samples of examinees (Kurtz & Dwyer, 2013). Random equating error is directly related to sample size. The sample sizes required to conduct test equating accurately vary based on equating designs and equating methods. For example, \"a random-groups design requires a much larger sample than a common-item design, which requires a larger sample than a single-group design\" (Kim & Livingston, 2010, p. 286). Kolen and Brennan (2004) suggest that the minimum sample size for linear equating should be 400 and the minimum sample size for equipercentile equating should be 1500. However, large samples always may not be accessible in real test situations. Hence, a variety of methods has been recommended to cope with equating problem in small samples. These methods can be listed as identity, linear, chained equipercentile equating with log-linear presmoothing, circle arc, and synthetic equating (Babcock, Albano & Raymond, 2012). In this study, chained equipercentile equating, linear and circle arc methods are considered so information about these methods is given below."}, {"section_title": "Chained Equipercentile Equating", "text": "Chained equipercentile equating method is an alternative equipercentile equating method. Firstly, this method was described by  and then Dorans (1990) named this method as chained equipercentile equating. In this method, Form X scores are equated to common items scores. Then scores of common items are equated to the Form Y scores. Assume that Form A is an anchor test for Form X and Form Y. Population P takes the Form X and Population Q takes the Form Y. The scores of Form X are equated to scores of anchor test A using examinees from Population P. Then anchor test A scores are equated to Form Y scores using Population Q. Because of including a chain of two equipercentile equating, it is called as chained equipercentile equating (Kolen & Brennan, 2004). As Livingston (1993) reported smoothing in equipercentile equating is decreasing sample size requirements by about one half. Presmoothing and postsmoothing are two types of smoothing method which used in equipercentile equating. While the score distributions are smoothed in presmoothing, the equipercentile equivalents are smoothed in postsmoothing. Presmoothing can be done with a polynomial log-linear model or a strong true score model. In this study, we consider presmoothing with the polynomial log-linear model. For the polynomial log-linear presmoothing method, choosing the degree of the polynomial (C) is important because it limits how much smoothing is done. The C parameter is generally chosen from numbers from 1 to 10. After presmoothing, the fitted distribution has the moment preservation property. This means that first C moments of the fitted distribution are the same to sample distribution's first C moments. For instance, if C=2, the mean and standard deviation of the fitted distribution are the same to the mean and standard deviation of the observed distribution. Likelihood ratio chi-square goodness of fit statistic can be used for choosing the C parameter. For instance, the difference between chi-square statistics for C=3 and C=4 can be examined with one degree of freedom. A significant difference between chi-square values means that the more complex model (C=4) fits the sample data more than the more simple model (C=3). If the two models fit the data adequately the simplest model should be chosen (Kolen & Brennan, 2004)."}, {"section_title": "Linear Equating Methods", "text": "Linear equating assumes \"apart from differences in means and standard deviations, the distributions of the scores on Form and Form Y are the same\" (Crocker & Algina, 1986, p.458). Tucker and Levine are most prevalent (Kolen & Brennan, 2004) linear equating method in CINEG design and their use in small samples are supported by prior researches for small sample sizes (Parshall, Du Bose Houghtan, & Kromrey, 1995;Skaggs, 2005). In this study, we consider Tucker and Levine linear equating methods. Tucker equating was described by (Gulliksen, 1950) and he attributed it to Ledyard Tucker (as cited in Kolen & Brennan, 2004). Tucker equating method makes two important assumptions: regression slopes of the total test scores on the common item score for both populations are equal and variance on the common item score between both examinee populations are equal (Kurtz & Dwyer, 2013). Levine observed score equating is another equating method which used with CINEG design. There are three assumptions in Levine observed score equating which related to the observed scores in classical test theory: (1) there is a perfect correlation between the true scores of total test and true scores of anchor test in the old and new form populations, (2) the total test true scores' regression on to the anchor test true scores are assumed to be the same linear function for the old form and the new form populations, (3) the measurement error variance for X is the same for Populations 1 and 2 (Kolen & Brennan, 2004)."}, {"section_title": "Circle-Arc Equating", "text": "Livingston and Kim (2008 suggested the circle arc method for small-sample data. This method has a curvilinear equating function. There are two kinds of circle arc equating method: the symmetric circle arc and simplified circle arc. Circle arc equating gets its equating function due to arc connecting three points in a Cartesian coordinate system (Babcock et al., 2012). \"The upper end of the curve is determined by the maximum possible score on each form. The lower endpoint of the curve is determined by the lowest meaningful score on each form. The middle point on the curve is determined from the data, by equating at one point in the middle of the score distribution. If those three points happen to lie on a straight line, that line is the estimated equating curve. If three points do not lie on a straight line, they determine an arc of a circle.\" (Livingston & Kim, 2009, p.332). Livingston and Kim (2008) reported that the circle arc method typically yielded more precise and less biased results than other methods (mean, linear and smoothed equipercentile equating) in small samples."}, {"section_title": "Differential Item Functioning", "text": "The other issue that should be considered in national and international testing programs differential item functioning (DIF). The purpose of DIF analysis is to determine items that function differently for examinees which have the same underlying ability from different subgroups. DIF studies are usually carried out regarding to reference and focal groups that are established by considering manifest (observed) group characteristics such as gender and ethnicity. It is supposed that the observed groups are homogeneous subgroups. In line with this assumption, an item containing DIF is considered advantageous or disadvantageous for all individuals in any manifest groups. Therefore, with these studies, once a DIF item has been determined, there is little knowledge about the examinees for which the item functions differentially (Cohen & Bolt, 2005;De Ayala, Kim, Stapleton, & Dayton, 2002). However, there is a low relationship between the manifest characteristic associated with DIF and the actual advantaged or disadvantaged groups. Therefore, comparisons of item responses for manifest groups may lack sensitivity to determine the true source(s) of DIF (Cohen and Bolt, 2005;De Ayala et al., 2002;Oliveri, Ercikan, & Zumbo, 2013;Samuelsen, 2005). Latent group means that to set the group membership to an unknown homogeneous subgroup which can be determined by mixture modeling (McLachlan & Peel, 2000). In mixture modeling, while the item functions the same in a latent group, it functions differently among latent groups (Fieuws, Spiessens, & Draney, 2004). So the use of mixture IRT models can overcome this problem that rises with the use of manifest groups. They can make it possible to detect latent groups for which the items function differently (Cohen & Bolt, 2005). In this study, DIF analyses were conducted based on latent classes."}, {"section_title": "Latent DIF", "text": "The mixture model is defined by Rost (1990) as a \"Mixture Rasch\" model. It is a combination of latent class and the Rasch model. In this model, it is assumed that a population of examinees can be grouped into several discrete latent classes based on examinees response patterns. With this model, item parameters can be simultaneously estimated with individual's ability and the class he/she belongs (Alexeev, Templin, & Cohen, 2011;Cohen and Bolt, 2005;Mislevy and Verhelst, 1990;Rost, 1990). In these models, each latent class fits Rasch model but classes have different item difficulty parameters. Therefore, MixIRT models can simultenously determine subpopulations that display qualitative differences and quantify the differences in the ability within the groups (Mislevy & Verhelst, 1990;Rost, 1990). According to the model, the possibility of giving a correct answer is as follows. In equation 1, g=1,\u2026,G refers to index with specified latent class; j=1,\u2026,J refers to index with specified responders; \u03b8 jg : j. refers to examinees latent ability in g latent class; \u03b2 refers to difficulty parameter of i. item in class g. If the DIF detection is carried out during the test construction process, the test developers usually delete flagged items from the test. However, in many situations DIF can be detected after data have been collected. In these situations, deleting DIF detected items may not be a good idea because item deletion can affect test reliability and validity negatively (Elosua & Hambleton, 2018). Hu and Dorans (1989) reported that deleting both minimal and sizable DIF items resulted in different scaled scores after IRT true score re-equating and Tucker re-equating. They also noted that the deleting item itself had a noticeable effect on scale score and the effect size of the DIF item had a less prominent effect on the scale scores (cited in Kolen & Brennan, 2005). Therefore, it is important to determine DIF items during test equating process and apply methods which reduce the effect of these items on test equating constants (Hidalgo-Montesinos & Lopez-Pina, 2002). In literature, there are some researches (Atalay-Kabasakal & Kelecio\u011flu, 2015;Chu, 2002;Demirus & Gelbal, 2016;Turhan, 2006;Yurt\u00e7u & G\u00fczeller, 2018) which have been compared equating methods in presence of DIF items. In these studies, IRT equating methods were compared in the presence or absence of DIF items. There is not any study which compares small samples equating methods in the presence and absence of DIF items in tests. Therefore this study aims to compare the performance of different small sample equating methods in the presence and absence of DIF in common items."}, {"section_title": "METHOD", "text": "In this study, performance of existing small equating methods was compared in the presence and absence of DIF in common items with using real data. Therefore, this study was designed as descriptive survey. In descriptive survey design, there is not any attempt to change or influence the study situation, existing situation is described (Karasar, 2009)."}, {"section_title": "Data", "text": "The data used in this study is 8th-grade mathematics test item responses which obtained from Trends in International Mathematics and Science Study (TIMSS) 2015 Turkey (N=6079) sample. Item responses from Booklet-1 and Booklet-14 are chosen for this study. There are 14 dichotomous scored items common for both booklets. Booklet 1 consists of totally 32 dichotomously scored items and the maximum score which can be obtained are 32. Booklet 14 consists of totally 26 dichotomous scored and 1 polytomous scored (0-1-2) items and the maximum score which can be taken from Booklet 14 is 28. There are 199 students who took Booklet 1 and 224 students who took Booklet 14. "}, {"section_title": "Data Analyses", "text": "In this study, data analyses were completed in four steps. In the first step, the confirmatory factor analyses are carried out for Booklet 1 and Booklet 14 to assure the unidimensionality requirement for DIF detection and test equating methods. In the second step, DIF analyses are conducted with Mantel Haenszel (MH) and logistic regression (LR) methods based on latent class. In the third step, Booklet 1 is chosen as a base form and Booklet 14 chosen as a new form, then test equating is conducted under common item nonequivalent groups design. Test equating is done in two phases: the presence of DIF items in anchor test and removing sizable (C level) DIF items from anchor test. In this study, Tucker linear equating, Levine observed score equating, unsmoothed chained equipercentile equating, chained equipercentile equating with presmoothing (C=4), and simplified circle arc equating methods are considered. These equating methods are chosen because the researches showed that they gave accurate equating results in small samples (Babcock et. al, 2011, Kim & Livingston, 2010. Equating results are evaluated based on the standard error of equating, bias and root mean squared error (RMSE) index which provided from 1000 bootstrapped samples. The Mplus (Muthen & Muthen, 1998-2012 computer program is to assess unidimensionality assumption; the WINMIRA (reference) computer program is used to find how many latent groups exist in the data and to estimate item parameters MixRasch analysis; \"difR\" R package (Magis, Beland and Raiche, 2015) is used to find DIF items across latent class; \"equate\" R package (Albano, 2017) is used for test equating and calculating bootstrapped standard error, bias, and RMSE indexes. In \"equate\" package of R, as reported Albano (2017) \"standard errors are calculated as standard deviations over replications for each score point; bias is the mean equated score over replications, minus the criterion; RMSE is the square root of the squared standard error and squared bias combined.\" (p. 5)."}, {"section_title": "RESULTS", "text": ""}, {"section_title": "Descriptive Statistics and Testing Assumptions", "text": "The descriptive statistics for Booklet 1 and Booklet 14 are reported in Table 2. As seen in Table 2, Booklet 1 has 32 multiple choices (MC) items and Booklet 14 has 26 MC and 1 polytomous scored (0-1-2) items. In both forms, the 14 MC items are common. The mean for Booklet 1 is 14.51 and for Booklet 14 is 12.61 and mean test difficulty is equal for both forms (z=0.00, p>0.05). Cronbach alpha reliability is .93 for Booklet 1 and .91 for Booklet 14 and there is no statistical difference between forms' reliability level (z=.75, p>.05). Item discrimination of items in each form was calculated by using point-biserial correlation coefficient. The mean of point-biserial correlations was the same and .50 for Booklet 1 and Booklet 14 (see Table 2). Note. N: Total number of students; SD: Standard deviation; rpb: Point biserial correlation Prior to DIF analyses and equating test forms, confirmatory factor analyses (CFA) is carried out for Booklet 1 and Booklet 14 by Mplus (Muth\u00e9n & Muth\u00e9n, 1998-2012. Comparative fit index (CFI), Tucker Lewis index (TLI) and Root mean square error of approximation (RMSEA) indexes for Booklet 1 (CFI= .99, TLI= .99, RMSEA= .027) and Booklet 14 (CFI= .99, TLI= .99, RMSEA= .019) support that each form measures a unidimensional trait (Byrne, 2010;Hu & Bentler, 1999;Kline, 2005)."}, {"section_title": "Estimation of Model Parameters", "text": "To determine fitted latent classes to the model, results of model comparison criteria for mixture Rasch solutions given in Table 3 is examined. In the MixIRT model applications, information criteria Akaike information criterion (AIC) and Bayesian information criterion (BIC) have been widely used to select the model. Li, Cohen, Kim, and Cho (2009) suggested that the smallest BIC result should be used to determine the number of classes. Based on BIC values in Table 3, we can say that a model with two latent classes' best fit the data."}, {"section_title": "Results of DIF Analyses", "text": "DIF analyses were conducted by using Mantel Haenszel (MH) and logistic regression (LR) methods based on two latent classes. The DIF results are reported in Table 4. As seen in Table 4, the DIF analyses results showed that two items (7 and 13) sizable (C level) DIF based on Mantel Haenszel and logistic regression methods. In Mantel Haenszel method, Dorans and Holland's (1993) effect size and in Logistic regression method Gierl, Khaliq and Boughton's (1990) DIF cut points are used."}, {"section_title": "Results of Equating", "text": "In this study, five equating methods are considered: Tucker linear equating, Levine observed score equating, chained equipercentile equating no smooth, chained equipercentile equating with presmoothing (C=4), and simplified circle arc equating methods with nominal weights. As mentioned before for the polynomial log-linear presmoothing method, choosing the degree of the polynomial (C) is important. For this study, we compare chi-square values under different smoothing parameter. The moments and fit statistics for presmoothing are given in Table 5. Note. The chosen C parameter for presmoothing is shown in boldface. As seen in Table 5, for Booklet 1, C=4 the overall X 2 statistic is not significant (X 2 (28) =16.93, p>.05) and the difference statistics for chi-square X 2 C= 4-X 2 C=5 equals .81 and it is not significant at .05 level for one degree of freedom (X 2 <3.84). Based on results, for C\u22654 model fit the data and C=5 do not improve the fit of data. For Booklet 14, as seen in Table 5, C=4 the overall X 2 statistic is not significant (X 2 (28) =15.73, p>.05) and the difference statistic for chi-square X 2 C=4 -X 2 C=5 equals .29 and it is not significant at .05 level for one degree of freedom (X 2 <3.84). Based on results, again for C\u22654 model fit the data and C=5 do not improve the data fit. For Booklet 1 and Booklet 14, C=4 is chosen for presmoothing. In this study, Booklet 1 is a base form and Booklet 14 is a new form and test equating is conducted under CINEG design. Test equating is done in two phases: the presence of DIF items in the anchor test and removing sizeable DIF items from anchor test. The Figure 1 shows that equated scores versus total scores in the presence and absence of sizeable DIF items in the anchor test. In Figure 1, the red line represents Tucker linear equating, the yellow line represents Levine observed score equating, the green line represents chained unsmoothed equipercentile equating, the blue line represents chained presmoothed (C=4) equipercentile equating and the purple line represents simplified circle arc equating method. Also in Figure 1, the black line belongs to identity equating and it means that there is no equating between old form and new form scores. As seen for both conditions Tucker, Levine and circle arc methods yield similar equated scores; their lines in the graphics are almost top of each other. In unsmoothed chained equipercentile equating method, there are some irregularities between equated scores and total scores (see the green line in Figure 1). The random error in estimating equivalent scores causes these irregularities. As seen in Figure 1, these irregularities got lost in presmoothed equipercentile equating (see the blue line). The performance of different equating methods in presence and absence of DIF items in anchor test was evaluated based on standard errors of equating, bias and RMSE values which provided from 1000 bootstrapped samples and reported in Table 6. As seen in Table 6, when the common items include DIF items, simplified circle arc equating method has the least (.19) standard error of equating (se) and unsmoothed chained equipercentile equating method has the largest (.77) standard error of equating. On the other hand, when we consider bias as a criterion simplified circ-arc method has the largest (.69) amount of bias, 4-moments presmoothing chained equipercentile equating has the smallest amount of bias value. Levine and Tucker equating methods have the same (.52) and smaller bias values than unsmoothed equipercentile equating method. We can say that Tucker linear equating and Levine observed score equating methods show similar and better performance than the unsmoothed chained equipercentile equating method. According to last criteria of RMSE values, again smoothed chained equipercentile equating method has the smallest (.52) RMSE value and the unsmoothed equipercentile equating method has the largest (.94) RMSE value. Tucker linear equating and Levine observed score linear equating methods had the same (.63) and smaller RMSE values than simplified circle-arc equating method (.72). Again we can say that Tucker linear equating and Levine observed score linear equating methods show similar and better performance than the simplified circle-arc equating method. After removing two sizeable DIF items from anchor test, the similar results have been found (See Table 6). Again based on se criteria, the simplified circle arc method was the best and the unsmoothed chain equipercentile equating method was the worst. On the other hand, based on bias criteria the best equating method is presmoothed equipercentile equating method and the worst one is simplified circle arc equating method. Concerning RMSE values, the best one is again presmoothed chained equipercentile equating method and the worst one is unsmoothed chained equipercentile equating method. Also, according to results, we can say that performances of equating methods are similar with the presence and not presence of DIF items in anchor test and we can say that there is no notable change in se, bias and RMSE values. Another result of this study is that whether or not common items include DIF items, unsmoothed chained equipercentile equating method has larger se, bias and RMSE values than presmoothed (C=4) chained equipercentile equating method."}, {"section_title": "CONCLUSION AND DISCUSSION", "text": "In this study, five equating methods were considered: Tucker linear equating, Levine observed score linear equating, unsmoothed chained equipercentile equating, chained equipercentile equating with presmoothing (C=4), and simplified circle arc equating methods with nominal weights. Equating methods were compared in two phases: the presence of DIF items in anchor test and removing sizeable DIF items from anchor test. The results show that performances of equating methods are similar to presence and absence of DIF items in anchor test and there is no notable change in se, bias and RMSE values. Also, results show that according to the standard error of equating criteria, the circle arc equating method outperformed other equating methods but based on bias evaluation criteria its performance was the worst one in both situations. As Kolen and Brennan (2004) reported standard error of equating is the standard deviation of equivalent scores over replications of the equating process and random error indexed by the standard error of equating. Standard error equating is closely related with sample size and as the sample size becomes larger it becomes smaller. The result of this study showed that the circle arc method has the minimum se among other equating methods. The circle-arc method especially suggested for small samples (Livingston, & Kim, 2009) and in their study, Kim and Livingston (2010) showed that in small samples the circle arc method clearly outperformed other equating methods (chained equipercentile, Levine, chained linear, Tucker) based on bias, RMSD and, se evaluation indexes. The results of our study supported Kim and Livingston (2010) only in terms of random equating error. Also, among other equating methods the unsmoothed chained equipercentile equating has the largest se value and we can say that based on random equating error its performance was the worst. As seen in Figure 1 there are some irregularities between equated scores and total scores and Kolen and Brennan (2004) noted that the reason for these irregularities is random equating error. Also in our study, the results of unsmoothed chained equipercentile equating method based on se support this view. Based on bias and RMSE evaluation criteria, smoothed chained equipercentile equating method is the best equating method and unsmoothed chained equipercentile equating method is the worst method. In one of a simulation study, A\u015firet and S\u00fcnb\u00fcl (2016) shows that for sample size 200 presmoothed equipercentile equating method produced more accurate results than other methods (linear, circle-arc, mean). Our study results supports this finding with real data which has sample size roughly 200. Tucker linear equating and Levine observed score equating methods show similar and better performance than the unsmoothed chained equipercentile equating method. To all evaluation indexes, smoothed chained equipercentile equating has lower values than the unsmoothed equipercentile equating method. We can say that presmoothing tended to decrease random and systematic equating error as in shown other studies (A\u015firet & S\u00fcnb\u00fcl, 2016;\u00d6zdemir, 2017;Kelecio\u011flu & \u00d6zt\u00fcrk G\u00fcbe\u015f, 2013;Livingston, 1993;Skaggs, 2005). As a result, we can also say that presmoothed chained equipercentile equating yields more precise and accurate equating results than unsmoothed chained equipercentile equating as is assumed (Kolen and Brennan, 2004)."}, {"section_title": "RECOMMENDATIONS", "text": "This study is limited with 8th-grade mathematics data from Booklet 1 and Booklet 14 in TIMSS 2015 Turkey sample. The results showed that performances of equating methods are similar to the presence and not the presence of DIF items in anchor test and there is no notable change in the error of equating. This result should be interpreted carefully and in further researches effects of DIF on small sample equating methods should be examined with real and simulated data sets more detailed."}]