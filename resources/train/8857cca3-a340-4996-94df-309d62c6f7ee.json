[{"section_title": "Abstract", "text": "ABSTRACT There is ongoing research for the automatic diagnosis of Alzheimer's disease (AD) based on traditional machine learning techniques, and deep learning-based approaches are becoming a popular choice for AD diagnosis. The state-of-the-art techniques that consider multimodal diagnosis have been shown to have accuracy better than a manual diagnosis. However, collecting data from different modalities is time-consuming and expensive, and some modalities may have radioactive side effects. Our study is confined to structural magnetic resonance imaging (sMRI). The objectives of our attempt are as follows: 1) to increase the accuracy level that is comparable to the state-of-the-art methods; 2) to overcome the overfitting problem, and; 3) to analyze proven landmarks of the brain that provide discernible features for AD diagnosis. Here, we focused specifically on both the left and right hippocampus areas. To achieve the objectives, at first, we incorporate ensembles of simple convolutional neural networks (CNNs) as feature extractors and softmax cross-entropy as the classifier. Then, considering the scarcity of data, we deployed a patch-based approach. We have performed our experiment on the Gwangju Alzheimer's and Related Dementia (GARD) cohort dataset prepared by the National Research Center for Dementia (GARD), Gwangju, South Korea. We manually localized the left and right hippocampus and fed three view patches (TVPs) to the CNN after the preprocessing steps. We achieve 90.05% accuracy. We have compared our model with the state-of-the-art methods on the same dataset they have used and found our result comparable.\nINDEX TERMS Alzheimer disease classification, ALZHEIMER disease detection, Alzheimer disease diagnosis, convolutional neural network, deep learning, machine learning, medical imaging."}, {"section_title": "I. INTRODUCTION", "text": "Alzheimer's disease (AD) is the most predominant neurodegenerative brain disease affecting elderly people worldwide and is considered to be one of the prime reasons for dementia [1] . AD is an irreversible, progressive neurobiological brain disorder and multifaceted disease of unknown etiology that slowly destroys brain cells, causes memory and thinking skill losses, and ultimately accelerates the loss of ability to carry out even the simplest tasks [2] . The cognitive decline caused by this disorder progresses toward dementia. The disease progresses over time from its initial stage of normal that machine learning algorithms were able to classify AD more accurately than experienced clinicians [3] . Recently, deep learning has shown outstanding performance in classification and regression [9] . Therefore, deep learning-based approaches are becoming the obvious choice for the detection of Alzheimer diseases.\nThe state-of-the-art approaches either consider the whole brain in a single modality [10] , [11] or multimodal [6] datasets to train machine learning models, which have been shown to demonstrate greater accuracy than manual diagnosis. Magnetic resonance imaging (MRI), positron emission tomography (PET), biospecimens, genotyping and sequencing data and clinical datasets are different modalities for AD research. Investigating more than one data modality is time consuming and expensive. Moreover, modalities such as PET may have radioactive side effects on patients. Here, we consider structural magnetic resonance imaging (sMRI) as the modality of our experiments for the following advantages: 1) high degree of imaging flexibility; 2) high tissue contrast; 3) no need for ionizing radiation; 4) useful information about the anatomy of the brain;\nThe accuracy of an AD diagnosis mostly depends on the biomarkers of the disease. A biomarker serves as a determinant of health and disease; it is measured and evaluated as an indicator of normal biological processes, pathogenic processes or pharmacological responses to a therapeutic intervention [12] . Studies have revealed that there is an association between structural changes of the AD brain and cognitive loss. Hippocampal shrinkage is observed in the preliminary stages of AD, and it has a proven correlation with memory impairment. As reported in [13] , the yearly brain atrophy rate for AD patients is 2.4% \u00b1 1.1%, whereas the figure is 0.5% \u00b1 0.4% for age-matched control subjects with normal brains.\nThe hippocampal atrophy rate is higher than that of the whole brain. According to [14] , the average hippocampal atrophy rate for AD patients was approximately 4 to 6%, and the rate was 1 to 2% for age-matched control subjects with normal brains. Thus, we focus on the hippocampus region as the input feature for the convolutional neural network (CNN). We localize the hippocampus manually for each MRI; then, we generate 32 \u00d7 32 patches from the localized region.\nOur prime objective of this research is to design a simpler CNN. We have avoided feeding the entire MRI volume into the network to reduce unnecessary computations. The approach improved the computational efficiency as well as the prediction accuracy. It has also resolved problems due to data scarcity that are associated with deploying CNNs. To achieve our objective, we have generated 32 \u00d7 32 patches from each of the sagittal, axial and coronal views and merged them as a single sample. These three view patches (TVPs) are fed into the CNN.\nIn this paper, we have summarized related works in section II. Data collection and preparation are outlined in section III. Our framework is described in section IV. The experimental setup is illustrated in section V. The results are discussed in section VI, and section VII concludes the work."}, {"section_title": "II. RELATED WORKS", "text": "There are studies based on the conventional machine learning techniques which focused on developing models to detect anatomical and functional disorders due to AD in human brain [15] - [21] . These methods have primarily relied on manually designed features, which heavily depend on professional expertise, require repeated trials, and tend to be time-consuming and subjective processes. However, as the cause of AD is not completely understood, designing robust analysis methods for effective hand-crafted features using medical experts' knowledge is a challenging task. In contrast, deep learning is capable of automatically learning input features from a large set of training data. Many previous studies were conducted to further explore CNN architectures dedicated to generating robust AD features.\nGupta et al. [8] used cross-domain features to represent MRI data. They deployed a stacked autoencoder (SAE) to learn a set of bases from natural images and then applied a CNN to obtain a more effective feature representation for AD classification. Despite being very simple, they showed high classification performance in comparison with contemporary approaches. Liu et al. [2] also proposed an SAE-based multimodal neuroimaging feature learning algorithm from a region of interest (ROI) for AD diagnosis. This framework uses a zero-masking strategy for data fusion to extract complementary information from multiple data modalities.\nBrosch and Tam [22] learned a low-dimensional manifold of brain volumes with a deep belief networks (DBN) algorithm to detect the modes of variations that correlate to demographic and disease parameters for AD. Their primary contributions are following: 1) they introduced a much more computationally efficient training method for DBNs that allows training on 3D medical images with a resolution up to 128 \u00d7 128 \u00d7 128, and 2) they demonstrated that DBNs can learn a low-dimensional manifold of brain volumes that can detect modes of variations.\nPayan and Montana [23] used a sparse autoencoder to learn feature embedding and then feed these embeddings to a convolution neural network for AD classification. The authors built a learning algorithm that is able to discriminate between healthy brains and diseased brains using MRI images as input. They investigated a class of deep artificial neural networks and a specific combination of sparse autoencoders and CNNs. The main novelty of their approach is to use 3D convolutions on the whole MRI image. Li et al. [7] proposed a robust multitask deep learning framework using a dropout [24] and stability selection technique to improve the ROI feature representation for AD/MCI diagnosis.\nShi et al. [6] developed a robust deep learning framework for multimodal AD diagnosis from MRI and PET scans. They applied principal component analysis (PCA) to obtain features and then utilized a stability selection technique together with the least absolute shrinkage and selection operator (LASSO) method [5] to select the most effective features. The selected features were then processed by the deep learning structure. Model weights in the deep structure were first initialized by unsupervised training and then fine-tuned by AD patient labels. During the fine-tuning phase, the dropout layer was deployed to improve the model's generalization capability. Finally, the learned feature representation was used for AD/MCI classification by a support vector machine (SVM)."}, {"section_title": "III. DATA COLLECTION AND PREPROCESSING", "text": "In this section, we describe the dataset that we have used in our study. The preprocessing steps performed on the data also discussed here."}, {"section_title": "A. DATASET", "text": "Magnetic resonance imaging (MRI) is the de facto modality in brain studies due to its superior image contrast in soft tissue without involving ionizing radiation. MR images are widely used to examine other anatomical regions as well [25] . There are many MRI datasets for AD studies, such as ADNI, BgBrain, OASIS, and AIBL.\nData used in the preparation of this article were obtained from the ADNI 1 database (adni.loni.usc.edu). ADNI was launched in 2003 as a public-private partnership. The primary goal of ADNI has been to test whether serial MRI, PET, other biological markers, and clinical and neuropsychological assessments can be combined to measure the progression of mild cognitive impairment (MCI) and early AD. For up-todate information, see www.adni-info.org. 1 The Alzheimer's Disease Neuroimaging Initiative data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (http://adni.loni.usc.edu/). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in the analysis or writing of this report. A complete listing of ADNI investigators can be found at http://adni.loni.usc.edu/wpcontent/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf\nWe have used 352 MRI scans of the ADNI dataset that are separated into three different classes: AD, NC and MCI. Among these classes, we used only AD and NC data. There are 77 MRI scans labeled as AD, 129 are labeled as NC, and the remaining 146 are labeled as MCI. The size of the MRI volume is 170 \u00d7 256 \u00d7 256 in most of the cases. However, the large 3D size does not hinder the training as we consider only hippocampus regions.\nAfter training, validation and testing with ADNI data, we have retrained and tested our models with Gwangju Alzheimer's and Related Dementia (GARD) cohort dataset provided by the National Research Center for Dementia (NRCD), Gwangju, Republic of South Korea. The GARD dataset has 326 baseline MRI scans. All scans are from Korean patients with an age range of 49 years to 87 years. There are four labels in the dataset, namely, AD, NC, mAD and aAD. AD, NC and mAD are similar to the ADNI database classes of AD, NC and MCI, respectively. aAD is another stage known as asymptomatic AD, where there are no symptoms of AD or MCI in terms of recognition capability but the patients are biomarker positive [26] , [27] .\nIn the GARD database, there are 81 samples in the AD class, whereas the number is 171 for the NC class. The number of samples for the mAD and aAD classes is 39 and 35, respectively. We followed the same data separation rule and preprocessing techniques for the ADNI and GARD datasets."}, {"section_title": "B. MRI PREPROCESSING", "text": "The ADNI dataset was already corrected for intensity inhomogeneity. The axial, coronal and sagittal views of a sample MRI are shown in Figure 1 (b). We have normalized the intensity values by subtracting the mean intensity and then dividing by the standard deviation to have zero mean and unit VOLUME 7, 2019 FIGURE 2. An example of selected six points (viewed on the sagittal plane) that are used as the center of 8 \u00d7 8 \u00d7 8 cubes for generating reference points. The reference points are selected from the cube by a semirandom process. TVPs are generated on the reference points.\nvariance of the input. The normalization is defined in (1) .\nHere, I (i x , i y , i z ) is the intensity of (i x , i y , i z ) location before normalization, \u00b5(I ) is the mean intensity and \u03c3 (I ) is the standard deviation of the intensity; I is the normalized intensity of the MRI.\nAfter intensity normalization, we manually observed hippocampus locations (h x , h y , h z ) on the normalized MRI by using Mango, a multi-image analysis graphical user interface (GUI) [28] . Six example locations are presented in fig. 2 . Each location and its neighboring points up to \u03b1, \u03b2 and \u03b3 pixels in sagittal, coronal and axial direction, respectively, were used as a reference frame for patch generation. Careful selection of these shape constants; i.e., \u03b1, \u03b2, and \u03b3 ; ensured that each reference frame lies within the hippocampus region. In our experiment, we have selected \u03b1 = \u03b2 = \u03b3 = 4. Different values of these constants provides flexible shape of the reference frame to adjust with the shape of ROI. We have randomly chosen n x , n y , and n z number of coordinates in sagittal, coronal, and axial directions, respectively. As described in (2) , (3), (4) and (5), these co-ordinates are used to generate reference points for TVPs.\nT y = rand(h y \u2212 \u03b2, h y + \u03b2, n y ) (3)\nHere, T x , T y , T z represent uniformly distributed integer samples from the specified interval. The number of samples drawn from the interval are denoted by n x , n y , and n z respectively. rand(h x \u2212\u03b1, h x +\u03b1, n x ) returns n x number of uniformly distributed random integers from the interval (h x \u2212\u03b1, h x +\u03b1). The same explanation follows for (3) and (4).\nThe reference points were generated by taking all (i, j, k) tuples of T x \u00d7 T y \u00d7 T z (i.e., the cartesian product). The disjunction of all reference points obtained from each reference frame were used to generate TVPs. The equation (5) summarized the operation. The algorithm 1 concisely describes the reference point generation process.\nTo avoid data imbalance between two classes, we took relatively fewer samples from the MRIs of the NC class. Sample reduction for NC class was done by reducing the values of shape constants of reference frame in (2), (3), and (4). As our patch size is 32\u00d732 smaller number of samples did not affects in exploring the whole hippocampus volume.\nThere are three different patches in each sample for individual hippocampus classification. Patches were taken from each of the three orthogonal axial, coronal, and sagittal view planes for each reference point. For combined classification of the left and right hippocampus, the sample consists of six patches."}, {"section_title": "IV. METHODOLOGY", "text": "The proposed pipeline is depicted in Figure 1 (a). Our framework consists of three individual models for generating decision scores on individual patches, followed by a score aggregator and final classifier.\nAfter collecting data, we performed the preprocessing tasks, as stated in the previous section. Then, we performed manual localization of the left and right hippocampus, which we consider as the ROI for our experiment. Then, from the ROI, we generated TVPs of size 32 \u00d7 32 \u00d7 3 or 64 \u00d7 32 \u00d7 3."}, {"section_title": "Algorithm 1 Algorithm for Reference Point Generation", "text": "Input: H = {H1, H2, H3, H4, H5, H6}: Manually observed approximately equidistant locations inside the hippocampus Output: R: a set of reference locations,(x,y,z)\nWe trained three individual models with these generated patches. At first, we designed the classifier for left hippocampus. Then, we tried the architecture for right hippocampus classification. But, after several trial, we found the right hippocampus classifier with even simpler network. As the input size is different for both hippocampi classifier(i.e., LHRH model), we had to tweak the architecture of the related model. The performance of each model was measured individually. These three models were then added together, and a softmax classifier was used for the final distinction."}, {"section_title": "A. PATCH-BASED CLASSIFIER", "text": "It is well known that CNNs are highly susceptible to the sample size. The more the samples we have from each class, the more accurate the CNN performs. Classification accuracy is subject to the discriminating features among the available classes [29] . The availability of discriminating features of a class depends on the number of samples from the class. The main problem of AD diagnosis is the scarcity of data. We have a limited number of samples from each class. This scarcity of data may lead to an overfitted model. Therefore, we deployed a patch-based classifier because we can generate a sufficient number of patches for training.\nIn our proposed patch-based classification architecture, shown in Fig 1(a) , the input to the CNN is TVP. Each TVP is centered at the locations generated from the reference points. The CNNs are trained to predict individual TVPs as AD or NC. Based on the collections of individual TVP decisions, an MRI is classified into two objective classes. We keep kernel sizes less than or equal to 5 \u00d7 5 to extract detail information over the hippocampus. We use the rectified linear unit (ReLU) [30] as the activation function. The pooling operation [31] selects the activation from rectangular areas of specified size; it downsamples the patches by a factor of defined stride size. Max pooling and/or average pooling were used in the networks. We use batch normalization [32] before each convolution layer to enforce a normal distribution at the output of the layer."}, {"section_title": "1) CNN FOR THE LEFT HIPPOCAMPUS", "text": "The model for the left hippocampus classification is presented in Table 1 . The output of the third convolution layer is the feature embedding of the left hippocampus region. These features are further fed to the fully connected layers to classify AD versus NC. Adding a dropout of 0.75 in the first fully connected layer slightly improved the accuracy. We used softmax as the last layer activation and cross-entropy as the loss function. The Adam optimizer [33] and Xavier initialization [34] were used. The exponential decay rate for first and second moment estimates are 0.9 and 0.999 respectively. The architecture and structural details of the proposed CNN are noted in Table 1 . Total number of trainable parameter in the network is 105,826."}, {"section_title": "2) CNN FOR THE RIGHT HIPPOCAMPUS", "text": "The CNN architecture for the right hippocampus classification is illustrated in Table 1 . We tried different structures and hyperparameters. We determined the proposed network after several trials. There are three convolution layers and two fully connected layers in the model. Each convolution layer and fully connected layer are preceded by batch normalization and followed by the average pooling layer. The first convolution layer does not have the batch normalization, but inputs are normalized previously. Before the last fully connected layer, we used a dropout of 0.25, which converges the training process faster and increases the accuracy. The output of the last convolution layer is the feature embedding of the right hippocampus region. The optimization and weight initialization are the same as the model used for the left hippocampus classification. The architecture and structural details of the proposed CNN for the right hippocampus classification are noted in Table 1 . Total number of parameters in the network is 100,197 among which 99,925 parameters are trainable."}, {"section_title": "3) CNN FOR THE LEFT AND RIGHT HIPPOCAMPUS CLASSIFICATION", "text": "The architecture of the proposed CNN for the classification of both hippocampi is shown in Table 1 . There are seven convolution layers. Each follows batch normalization and/or drop out. It takes input of size 64 \u00d7 32 \u00d7 3. We merged the TVPs of size 32 \u00d7 32 \u00d7 3 from the left and right hippocampus to generate these input patches. We illustrate the merging operation in Figure 3 . The output of the seventh convolution layer is the feature embedding of the hippocampus region. These features are further fed to the fully connected layers to classify AD versus NC. The total number of parameters for this network is 409,666. The Adam optimization and Xavier initialization techniques were used in this model."}, {"section_title": "B. ENSEMBLE CLASSIFICATION", "text": "The reasoning behind using an ensemble is to bypass the weakness of individual models. Each model has its own hypothesis about the given input. By stacking different models with different assumptions about a class label, we can find VOLUME 7, 2019 a better classification that may not be possible with individual models.\nTo determine the most appropriate class label for a given patch, the results from all three models are combined. Each patch-based model produces decision scores of a patch that indicate how well the patch fits a class. The individual decisions of the relevant patches are combined in the stacking layer. Then, a softmax is applied to find normalized scores. The most likely value is selected as the final class for a patch. Two reference points are considered for a single score from the ensemble layer: one from the left hippocampus and one from the right hippocampus. The patches are classified by related patch-based classifiers. The final scores of patches are forwarded to stacking layer.\nFrom each test MRI, at least 32 pairs of TVPs are generated for classification. The majority decision from the stacking layer determines the class label of each MRI. This classification is grounded on weighted majority voting. Adding all the scores from each classifier ensures robust accuracy. The classification process is described in algorithm 2"}, {"section_title": "V. EXPERIMENTAL SETUP A. DATASET SEPARATION", "text": "From the ADNI dataset, we consider only those subjects whose disease status remains the same over different MRI scans. We have selected a total of 60 subjects from our dataset. For each subject, there are different MRI scans. We separate the training, testing and validation set in such a way that the conjunction of any two sets, keeping the subject ID of MRI scans as the key, yields the null set. This ensures the prevention of data leakage. We also ensure that MRI scans from each class are uniformly distributed among the three \nreturns TVPs of size 64 \u00d7 32 \u00d7 3 generated from pairs of TVPs of size 32 \u00d7 32 \u00d7 3 centering at the pair of locations (l,r). Here l \u2208 R L , r \u2208 R R and the function for mapping the corresponding locations is, F : R L \u2192 R R is one to one and onto. 11 return DS sets to address the class imbalance problem. We keep 60% of MRI scans as the training set, 20% for the test set and 20% for the validation set. We augmented the data of each class by applying shearing, rescaling and zooming of the patches. All the MRIs in the GARD dataset are baseline MRI scans, so we did not need to separate the MRIs according to patients. We divided the dataset into training, validation and a test set according to the procedure that we followed for the ADNI dataset separation. We also applied the same data augmentation techniques to the GARD dataset."}, {"section_title": "B. PLATFORM", "text": "We use the TensorFlow GPU 1.8, keeping Keras as the backend, on top of the Python 3.6 environment. An Intel(R) Xeon (R) CPU E5-1607 v4 @ 3.10 GHz with a 32 GB RAM machine was used. The GPU was NVIDIA Quadro M4000."}, {"section_title": "C. TRAINING", "text": "For patch-based classification, we trained different architectures with different hyperparameters. The presented models were trained for 20 epochs with a batch size of 32. We followed the 60-20-20 approach for using sample patches for training, validation and testing. We started the training with a learning rate of 0.001. If the validation loss stopped improving for 3 consecutive epochs, we reduced the learning rate by a factor of 10. It was observed that the learning rates were between 0.001 to 0.0001. The default parameter settings were used for the optimizers, regularizers and constraints."}, {"section_title": "VI. RESULTS", "text": "For evaluating the models, we have taken accuracy = we used an individual MRI as a sample. We generated at least 32 TVPs for each test MRI to obtain its label. First, we feed TVPs to the patch-based classifier to obtain the decision scores for each individual TVP. We then added the scores of all patches and normalized the scores. If the obtained score is greater than 0.5, we labeled the MRI as AD; otherwise, we labeled it as NC.\nWe have presented the results for both the ADNI and GARD datasets. The proposed model performed better on the GARD dataset in comparison to the ADNI dataset.\nGARD dataset was collected only from Korean patients. Therefore, the brain structure and other anatomical factors are rationally homogeneous in each MRI. Any deviation and atrophy is comparatively easier to detect. In addition to this, the dataset only provides baseline MRIs, and the number of patients in the GARD dataset (326) is much higher than that in the ADNI dataset (we used only 60).\nOn the other hand, ADNI participants were recruited at 57 sites in the USA and Canada, with ages from 55 to 90. As the selection was from diverse races, ethnicities and age groups, the dataset includes heterogeneous brains. In addition to these characteristics, there are several scans for the same patient, where the progression from MCI to AD is also demonstrated. Therefore, these AD scans might impede the model's capability to obtain an accuracy equivalent to that of the GARD dataset. In the following subsections, we briefly outline the performance of each of the models."}, {"section_title": "A. HIPPOCAMPUS LOCATIONS", "text": "We manually navigated throughout the MRIs to observe the location of both hippocampi. We used the multi-image analysis GUI (Mango) [28] for this purpose. We selected six different (roughly equidistant) locations inside the hippocampus. We crosschecked the manual hippocampus locations, H (h x , h y , h z ) by repeating the manual marking at different runs. We closely observed the detail views of three orthogonal planes to make sure that each patch contained the hippocampus. We considered a cube of side length 8, keeping each of the six different points as the center. From that cube, we semirandomly drew locations to generate TVPs. We have also carefully avoided the repetition of locations. We show an example of the selected six points for an MRI in Figure 2 . We show only the sagittal view here. "}, {"section_title": "B. RESULTS OF THE LEFT HIPPOCAMPUS CLASSIFIER", "text": "On the ADNI data, the presented model's overall accuracy for the left hippocampus classification was 80.40%. We tested the classifier on several runs with varying number of samples. During the prediction, we considered the class label for which the decision score is greater than 0.50. The reported one is found by feeding a balanced number of samples from both classes.\nThe area under the receiver operating characteristics curve for this model is 70.20%, as presented in figure 4 . The model diagnosed 87.88% of the actual AD-affected MRIs as AD, and a total of 77.97% of the AD diagnosed MRIs are actually AD affected.\nWhile testing with the GARD dataset, we obtained 83.27% overall accuracy. The precision, recall and f1 score for AD were measured as 76.88%, 90.50% and 84.34%, respectively; the measurements were 88.98%, 76.10% and 82.04%, respectively, for the NC class. The results indicate that 76.88% of the AD-classified MRIs are actually AD, and 90.50% of the actual AD-labeled MRIs are correctly classified."}, {"section_title": "C. RESULTS OF THE RIGHT HIPPOCAMPUS CLASSIFIER", "text": "The overall accuracy for classifying the right hippocampus as AD versus NC is 79.5% on the test set of the ADNI data. The number of samples drawn from the AD and NC class was kept balanced in the test set. Several test samples were generated for evaluating the model.\nIn total, 80% of the samples predicted as AD are true AD, and 79% of the AD-labeled samples are correctly classified by this model. The results are 78.89% and 79.69%, respectively, for the NC class. The assumption about the significance of the right hippocampus for AD diagnosis is strengthened by 82.0% area under the receiver operating characteristics curve (see figure 4(d) ). The model demonstrated 81.56% accuracy on the GARD test data. The precision, recall and f1 score of the AD class were 76.88%, 87.56% and 81.87%, respectively; the scores were 87.09%, 76.12% and 81.24%, respectively, for the NC class."}, {"section_title": "D. RESULTS OF LEFT AND RIGHT HIPPOCAMPUS CLASSIFIER", "text": "We achieved 82.35% accuracy in classifying both hippocampi as AD versus NC. The precision, recall and f1 score are 83.30%, 82.37%, and 82.83% for the AD class, respectively, and 81.31%, 82.33%, and 81.82% for the NC class. The accuracy that we determined from the combined hippocampus classifier is greater than the individual hippocampus classifiers.\nThe performance for the GARD data also increased in the classification of both hippocampi. The overall accuracy was 86.28%. The details are shown in Table 2 E"}, {"section_title": ". RESULTS OF ENSEMBLES", "text": "The ensemble of the three models, as shown in Table 2 , achieves 85.55% accuracy for the ADNI dataset. The ensemble provides precision, recall and the f1 score for the AD and NC class of approximately 85.5% \u00b1 0.1%.\nThe overall accuracy for the GARD dataset is 90.05%. The precision for the AD class is 91.11%, while it is 88.85% for the NC class. In addition, 90.22% and 89.85 % are the recall scores for the AD and NC class, respectively. The f1 scores for the AD and NC class are 90.66% and 89.35%, respectively.\nThe findings demonstrated that the ensembles of the models provide better performance than the individual models. As the decision scores of the individual models are added together and then softmax-normalized, each model's decision contributes to the final decision. The final decision-making procedure is a weighted voting strategy, as each decision score of an individual model can be considered as a weighted vote. This strategy alters the class label decided by individual classifiers with near to marginal scores. Therefore, the accuracy significantly increased after implementing an ensemble of the three models."}, {"section_title": "F. COMPARISON AND DISCUSSION", "text": "In previous methods, the whole brain and PET scan results were used to classify AD. The tendency is to use as much information as possible to train the model. In this paper, we show that we can obtain a comparable result using only the hippocampus. The comparative results for ADNI data are shown in Table 3 .\nIn the multimodal stacked deep polynomial approach (MMSDPN) [6] , as reported in the paper, the sparse connection in intermediate layers prevented overfitting. The authors used MRI, PET and cerebrospinal fluid (CSF) data to achieve 97.13% accuracy with 4.44% variation. The authors in [7] used the dropout technique to prevent overfitting in their multitask learning approach. Their approach demonstrated 91.4% accuracy on MRI, PET and CSF data. Both of the approaches were studied on the ADNI data (51 AD patients and 52 NC patients from each of the mentioned modalities).\nThe authors in [2] added a weight decay to regularize the objective function as a way to prevent overfitting. They experimented on 77 NC and 85 AD scans in both MRI and PET data from ADNI. Their model showed 82.59% accuracy with 5.33 variation in the MRI modality, which is 91.40%\u00b15.56% in the multimodal data. Payan and Montana [23] studied the MRI modality with 755 scans for each of the classes. This approach also added a weight decay term similar to [2] to regularize the objective function in order to prevent overfitting. The method demonstrated 95.39% accuracy in classifying AD versus NC. A feature selection method was deployed along with an l-norm penalty on weights to prevent overfitting in [11] . The method achieved 82% accuracy with spatial augmentation on MRI and PET images. The reported accuracy without spatial augmentation on the data was 77%. This method was studied on 149 PET and 183 MRI images from ADNI. To avoid the overfitting problem, Salvatore et al. [10] performed feature extraction and feature selection tasks separately for training-validation data and testing data. They reported 76% test accuracy on ADNI sMRI. The number of samples for AD was 137 and 162 for NC.\nThe method in [8] obtained 93.80% accuracy on ADNI MRI data. They did not explicitly discuss overfitting. This method learned a set of bases from natural images by deploying SAE and then used these bases to learn MRI features. The number of scans was 200 for AD and 232 for NC. VoxCNN in [35] reported 79.0% \u00b1 0.08% accuracy. By using the residual neural network (ResNet), the approach [35] obtained 80.0% \u00b1 0.07% accuracy. Here, the labeled sMRI scans included 50 AD and 61 NC from ADNI. Here, the overfitting problem was addressed by pre-training.\nIn this paper, the proposed patch-based ensembles of simple models demonstrate significant performance. We used only small patches (32 \u00d7 32) from the hippocampus of the brain MRIs and achieved comparable accuracy. Our patch generation reduces the scarcity of training data for generalization. Using the ensemble technique also contributed to building a robust model while avoiding the overfitting problem. It helps us to avoid obtaining an over-capacity network regarding the training time. The deployment of batch normalization [32] regularizes our models by enforcing that the inputs maintain a normal distribution in each layer. This regularization leads to better generalization. We also added dropout [36] in each model, which further address the overfitting problem.\nTo avoid problems of data imbalance, equal number of TVPs were generated from each of the majority(NC: 129 in ADNI and 171 in GARD) and minority (AD: 77 in ADNI and 81 in GARD) classes during training. We also avoided imbalance in the data during testing the individual models on TVP samples. For evaluating the model based on MRI, we kept the original ratio (20%) of the minority class dataset and took an equal number of MRIs from the majority class, i.e., we downsampled the majority class at test time."}, {"section_title": "VII. CONCLUSION", "text": "This work provides an efficient framework for AD diagnosis from brain MRI. We have considered the hippocampus, which is considered to be one of the most affected clinically studied biomarkers for AD detection. For the two different hippocampi in the brain, we had to deploy two patchbased classification models. However, deployment of another model for classifying both hippocampi increases the performance. We then designed ensemble models for an improved classification outcome. We designed the CNN classifiers based on TVPs on the semirandomly generated locations of the hippocampus region. This approach facilitated generation of the necessary data for training. After sufficient training, we combined the models to obtain the expected accuracy (85.55% for ADNI and 90.05% for GARD), which is comparable to the models designed in the MRI modality [1] ."}]