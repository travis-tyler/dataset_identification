[{"section_title": "Abstract", "text": "The MIT Joint Program on the Science and Policy of Global Change combines cutting-edge scientific research with independent policy analysis to provide a solid foundation for the public and private decisions needed to mitigate and adapt to unavoidable global environmental changes. Being data-driven, the Program uses extensive Earth system and economic data and models to produce quantitative analysis and predictions of the risks of climate change and the challenges of limiting human influence on the environment-essential knowledge for the international dialogue toward a global response to climate change.\nTo this end, the Program brings together an interdisciplinary group from two established MIT research centers: the Center for Global Change Science (CGCS) and the Center for Energy and Environmental Policy Research (CEEPR). These two centers-along with collaborators from the Marine Biology Laboratory (MBL) at Woods Hole and short-and longterm visitors-provide the united vision needed to solve global challenges.\nAt the heart of much of the Program's work lies MIT's Integrated Global System Model. Through this integrated model, the Program seeks to: discover new interactions among natural and human climate system components; objectively assess uncertainty in economic and climate projections; critically and quantitatively analyze environmental management and policy proposals; understand complex connections among the many forces that will shape our future; and improve methods to model, monitor and verify greenhouse gas emissions and climatic impacts.\nThis reprint is one of a series intended to communicate research results and improve public understanding of global environment and energy challenges, thereby contributing to informed debate about climate change and the economic and social implications of policy alternatives. Infrastructure located along the U.S. Atlantic and Gulf coasts is exposed to rising risk of flooding from sea level rise, increasing storm surge, and subsidence. In these circumstances coastal management commonly based on 100-year flood maps assuming current climatology is no longer adequate. A dynamic programming cost-benefit analysis is applied to the adaptation decision, illustrated by application to an energy facility in Galveston Bay. Projections of several global climate models provide inputs to estimates of the change in hurricane and storm surge activity as well as the increase in sea level. The projected rise in physical flood risk is combined with estimates of flood damage and protection costs in an analysis of the multi-period nature of adaptation choice. The result is a planning method, using dynamic programming, which is appropriate for investment and abandonment decisions under rising coastal risk. \nMany of the world's coasts have long been subject to the risk of severe storms and subsidence. One of the vulnerable areas is the U.S. East Coast and the Gulf of Mexico, and there is a long history of investment in large-scale shore protection by public agencies (U.S. Army Corps of Engineers, 2003) and by private entities guarding particular facilities. Traditionally this activity has been informed by 100 and 500-year flood maps, based on current climatology, that are prepared by the Federal Emergency Management Agency (FEMA, 2014). These maps are inputs to damage calculations-such as FEMA's HAZUS system for estimating the potential losses from disasters (HAZUS, 2014)-that are used to guide the protection investments. Of course, protection from events with a particular return period, like the 100-year flood, may be augmented by judgmental safety factors that take account of the particular economic damage and human lives at risk.\nInforming these protection decisions becomes more complex under projected climate change, which brings increasing risks of rising sea level and hurricane destructive potential (Emanuel, 2005 (Emanuel, , 2013 Knutson et al., 2010; Woodruff et al., 2013; Kopp et al., 2014) . Flood maps compiled under current climatology no longer convey adequate risk information for decisions with implications over more than a decade or two.\nMany coastal cities are proceeding to plan for increasing flood risks (e.g., Planyc, 2013), as are government agencies and private firms with vulnerable facilities, but there is limited information and analysis to inform the question as to when it is best on an economic basis to provide various levels of protection. Several previous efforts have included climate change impacts in analysis of sea level rise and storm surge. The EU-funded DIVA model (Hinkel and Klein, 2009 ) has been developed for analysis of vulnerability and adaptation from regional to global levels, assuming scenarios of sea level rise (Hinkel http://dx"}, {"section_title": "Introduction", "text": "Many of the world's coasts have long been subject to the risk of severe storms and subsidence. One of the vulnerable areas is the U.S. East Coast and the Gulf of Mexico, and there is a long history of investment in large-scale shore protection by public agencies (U.S. Army Corps of Engineers, 2003) and by private entities guarding particular facilities. Traditionally this activity has been informed by 100 and 500-year flood maps, based on current climatology, that are prepared by the Federal Emergency Management Agency (FEMA, 2014) . These maps are inputs to damage calculations-such as FEMA's HAZUS system for estimating the potential losses from disasters (HAZUS, 2014) -that are used to guide the protection investments. Of course, protection from events with a particular return period, like the 100-year flood, may be augmented by judgmental safety factors that take account of the particular economic damage and human lives at risk.\nInforming these protection decisions becomes more complex under projected climate change, which brings increasing risks of rising sea level and hurricane destructive potential (Emanuel, 2005 (Emanuel, , 2013 Knutson et al., 2010; Woodruff et al., 2013; Kopp et al., 2014) . Flood maps compiled under current climatology no longer convey adequate risk information for decisions with implications over more than a decade or two.\nMany coastal cities are proceeding to plan for increasing flood risks (e.g., Planyc, 2013) , as are government agencies and private firms with vulnerable facilities, but there is limited information and analysis to inform the question as to when it is best on an economic basis to provide various levels of protection. Several previous efforts have included climate change impacts in analysis of sea level rise and storm surge. The EU-funded DIVA model (Hinkel and Klein, 2009 ) has been developed for analysis of vulnerability and adaptation from regional to global levels, assuming scenarios of sea level rise (Hinkel http et al., 2014) . Other studies-such as those by Condon and Sheng (2012) , Yohe et al. (2011) and Tsvetanov and Shah (2012) focus on particular locations or facilities, but also are based on scenarios of sea level rise and/or fixed return periods of severe events. While these approaches produce useful pictures of expected increases in risk and of the adaptation challenge, they do not represent the rising risk over coming decades in a way that can support consideration of potential future adjustments when making today's investment choice. Installing protection prematurely would be wasteful; delaying too long could leave valuable facilities exposed to costly damage. Also, in some cases adequate protection against rising flood risk could cost more than the facility is worth, indicating abandonment as the preferred option.\nIn this paper we explore a method, applying dynamic programming that can be used to analyze investments in adaptation today when the coastal risk is rising over coming decades in an uncertain way. When there are opportunities for additional adaptation in the future, determining cost-effective action today requires consideration of these future options, and a main contribution of this type of analysis is to clarify their role in current choice-illuminating for decision makers the sequentialdecision nature of adaptation in the face of climate change.\nTo illustrate the method we use the experience of previous flooding events to construct an example of an energy plant located in one of the most vulnerable locations on the U.S. coast, Galveston Bay. Instead of relying on scenarios of sea level rise we apply an uncertainty analysis of this risk and combine it with uncertain storm surge, tide and subsidence. The region is low lying, with much of the energy infrastructure located there now sitting only a few meters above mean sea level. Moreover, in the past century parts of this region have sunk by as much as three meters."}, {"section_title": "Sequential decisions under rising risk", "text": "There are various ways to frame an analysis of adaptation choices. The use of current flood maps in fact involves an assumption that the probability of different water levels stays constant over time so that, other things being equal, only today's risk level is relevant to the decision to invest in protection or increased resiliency. If, as now expected, the risk is projected to increase decade to decade, dependence on current maps can be misleading and thus an expensive procedure. An economic decision to invest or abandon today depends on both the rate of change in future risk and on options to increase protection and/or resiliency at some later date. Analysis of this type of choice (i.e., where decisions at each time period affect those made at other time periods) is usefully formulated in a dynamic programming (DP) framework (Bellman, 1957) . DP applications to decisions under an uncertain climate include resource extraction (Torvanger, 1997) and investment in break waters (Chao and Hobbs, 1997) . It has also been used to study other types of problems, ranging from purchasing decisions (Kingsman, 1969) to fisheries harvesting (Mendelsohn, 1978) .\nTo formulate the decision to protect a facility from flooding under an uncertain climate future we define the decision space over a finite horizon (ending in 2100), where choices are made sequentially in discrete time periods. With an objective of minimizing the present value of the sum of the costs of expected future flood damage and protection, DP starts in the last time period and, applying Markov decision processes (Puterman, 1994) , solves through backwards induction. During each time period the state of the facility is defined as the level of protection in place (the height of an existing levee in meters in the example below), and the decision options consist of the additional level of protection (the number of additional meters) to build, including the option to do nothing. The expected cost at any time is a function of current and future states and actions, as well as flood damage.\nThe procedure begins by computing the costs for every possible state for the last time-period. The process then iterates, moving backwards through each time period, to compute decisions as a function of state, considering the previously calculated future state cost, and computes the least cost option for each state. The algorithm chooses the decision in each state that results in the cumulative least costs, discounted over time. More formally, we assume a risk-neutral decision maker and define S t to be the state, which is the height of protection at decade t. Action A t (S t ) is defined to be the best action to take in decade t, yielding the lowest expected costs for decade t in state S t , where the action in this context is the height of additional sea wall built in decade, t. Both S t and A t are real positive numbers. The cost C t (S t , A t ) is defined to be the expected costs during decade t given state S t and action A t . It is a function of the expected costs of damage due to flooding, the costs of building new protection and maintaining existing protection. The function V t \u00f0S t \u00de returns the action A t that produces the lowest cost, V t in state S t . For each time period, we calculate the best action and lowest value such that\nwhere r is the discount rate. C t (S t , A t ) is defined as\nwhere E(Damage|S t , A t ) is the expected damage during the decadal time period given the state and action made at the beginning of that period, c m is the cost of maintaining one meter-km of sea wall, c b is the cost of building an additional meter-km of sea wall, and m is the length of the sea wall in km. Since we choose to look at decisions made every decade, each of these costs are accumulated over the decade, at discount rate r. Then, by iterating backwards over time, we are able to derive the optimal levee height for each state. The estimation of expected damage, E(Damage|S t , A t ), involves a calculation of the probability that the annual maximum water level above today's mean sea level, X, during decade t will exceed the height of provided protection, S t + A t , in a facility located n meters above today's mean sea level, P t \u00f0X > n \u00fe S t \u00fe A t \u00de. If we assume damage to be a function of flood height, then to derive the expected damages over the t th decade we calculate the sum of expected damages for each year (discounted to the beginning of the decade):\nis developed in the context of an example below. To estimate the flood risk, P t (X > x), we require information on uncertain sea level rise, storm surge and subsidence at each time period."}, {"section_title": "Flood risk estimation", "text": "Several studies have incorporated climate change impacts into storm surge analysis (e.g., Mousavi et al., 2011; Lin et al., 2012; Irish and Resio, 2013) . Here we apply the method of Lin et al. (2012) . We analyze the annual surge risk and how it is projected to change between current climate, modeled as 1981-2000, and that projected for 2081-2100.\n1 The analysis begins with the generation of a large set of synthetic hurricanes that pass through Galveston Bay, driven by climatic conditions represented in the Global Circulation Models (GCMs) used and applying the statistical-deterministic hurricane model of Emanuel et al. (2006 Emanuel et al. ( , 2008 . Statistics are estimated, first for the intensity of the sample of storms and, after passing through a hydrodynamic model, for the associated storm surge level at the point of interest. The statistics for the storms that do arrive are then combined with analysis of the frequency of arrival to yield estimates of the annual exceedance probability of storm surge at different levels above mean sea level. The contribution of local astronomical tide is accounted for, assuming a uniform distribution for storm arrival times.\nSea level rise is modeled as a shifting probability distribution over time (Kopp et al., 2014) . Finally, since it is the relative sea level at the facility that matters, the contribution to future risk of subsidence is added. The ultimate result is an estimate of the probability that the facility will be flooded in 2000 and in 2100, which can be interpolated for the decades in between."}, {"section_title": "Storm surge", "text": ""}, {"section_title": "Storm generation", "text": "Each year approximately ten tropical storms develop in the North Atlantic, Caribbean, and Gulf of Mexico. On average six of these develop into hurricanes, of which only one or two make landfall in the U.S. (NOAA, 1999) . Because the record of hurricane activity is so limited, analysis of storm surge risk cannot be based strictly on historical data. We apply a statistical/deterministic hurricane model developed by Emanuel et al. (2006) and Emanuel et al. (2008) to generate a large number of synthetic storms, and a hydrodynamic model to generate the storm surges induced by these storms (Lin et al., 2010 (Lin et al., , 2012 . This statistical/deterministic model does not rely on the limited historical storm data but generates synthetic storms that are in statistical agreement with observations, and compares well with other methods used to study the effects of climate change on tropical cyclones (Emanuel et al., 2008; Emanuel, 2010; Knutson et al., 2010) .\nThe procedure starts with the climate conditions estimated by four GCMs, CNRM-CM3, ECHAM, GFDL-CM2.0, and MIROC 3.2, for current conditions and for 2100 under the IPCC AR4 A1B emissions scenario. The data were obtained from the World Climate Research Program (WCRP) third Climate Model Intercomparison Project (CMIP3) multimodel dataset (Meehl et al., 2007) . The predicted temperature increases by CNRM, ECHAM, GFDL, and MIROC for 2100 are 2.9\u00b0C, 3.4\u00b0C, 2.7\u00b0C, and 4.5\u00b0C, respectively (see Lickley et al. (2013) , for details). For each of the four climate models and for each of the two climate conditions, the statistical/deterministic hurricane model was applied to generate 3000 storms (with the annual frequency estimated) that pass within a 100-km radius of a reference point in Galveston Bay (29.3\u00b0N, 94.5\u00b0W) with a maximum wind speed greater than 15 m/s. The output of each storm track provides information on a 2-h time step including storm location, radius of maximum wind, maximum wind speed, and storm center pressure. The storm maximum wind speed provides one measure of storm intensity, and the generated 24,000 tracks provide a basis for constructing distributions of storm intensities at Galveston Bay.\nAs an example, Fig. 1 shows the cumulative distribution of the storm maximum wind speed when the storm is at its closest point to the reference location in the Bay, for the year 2000 for each of the four climate models, conditional on a storm arriving that meets the minimum wind speed criterion. These distributions then differ between 2000 and 2100 and the results for the GFDL model are presented in Fig. 2a , where the probability of an arriving storm having a maximum wind speed greater than 70 m/s increases from 0.2 in 2000 to 0.3 in 2100.\n2 As shown in Lickley et al. (2013) the other three models produce a smaller change despite projecting greater temperature increases. This difference among models is not surprising because storm intensity is influenced not only by sea surface temperature but also by other modeled conditions such as vertical wind shear, humidity, and temperature distribution of the upper ocean, all of which may be projected differently by the GCMs."}, {"section_title": "Surge simulation", "text": "The magnitude of a storm surge is determined by a number of factors including storm surface wind and pressure, as well as coastal geometry and bathymetry. To simulate the surge resulting from our synthetic storms, we apply the Sea, Lake and Overland Surges from Hurricane (SLOSH) model (Jelesnianski et al., 1992) used by the Natural Hurricane Center. The SLOSH model takes the information of storm track, intensity, and size (generated by the hurricane model) and generates surface wind and pressure fields internally to drive the hydrodynamic modeling. The model applies finite difference methods to solve the equations and uses a polar grid, which allows for a fine resolution in primary coastal regions of interest and a coarse resolution in the open ocean. SLOSH is computationally highly efficient and thus is suitable for risk analysis involving large numbers of possible scenarios. The performance of the SLOSH model has been evaluated using observations of storm surge heights from past hurricanes (Jarvinen and Gebert, 1986) ; the accuracy of surge heights predicted by the model is \u00b120% when the hurricane is adequately described (Jelesnianski et al., 1992) . When compared with higher-resolution finite element models, SLOSH performs well at simulating the maximum storm surge at locations with relatively simple coastal features, though subgrid-scale variations in the local surge will be averaged out (Lin et al., 2010) .\nFor this study, we use the EGL2 Galveston Bay mesh describing the Bay's coastal features with a resolution of about 1 km (with decreasing resolution in the ocean away from the coast). The storm surge heights used in this analysis are the maximum levels generated by SLOSH at the mesh grid point that is closest to our facility site. Note that neither astronomical tide or sea level rise is included at this point in the surge modeling; they are accounted for in the risk analysis, see below.\nWe can compare the differences in surge heights across climates by contrasting the probability density function in 2000 with that in 2100 for each model. The results for the GFDL model are shown in Fig. 2b , where the probability of an arriving surge exceeding 1.5 m at the site is projected to increase from about 5% in 2000 to 7% in 2100. This change is smaller for the other three models used in our analysis (Lickley et al., 2013) ."}, {"section_title": "Annual surge risk", "text": "We assume that hurricane arrival times are independent of one another and therefore they follow a Poisson process with annual frequency as the parameter. Each climate model projects different climate conditions and so produces a different storm frequency for Galveston Bay. These annual frequencies for each model's 2000 climate are calibrated to be in statistical agreement with the historical annual frequency, based on the hurricane database, HURDAT, which is a historical record of Atlantic storms (Landsea et al., 2004; Landsea and Franklin, 2013) , and the frequency for each model under the 2100 climate is similarly calibrated using the same model-specific adjustment factor (see Lickley et al., 2013) .\nWe use a numerical approximation to derive the surge risk (in terms of annual exceedance probability) from the Poisson storm arrival process under each climate condition. Define N k as the number of storm arrivals in one year. We sample 100,000 times by first sampling the number of storms, N k , from the Poisson distribution (determined by the annual frequency), then drawing N k times from the distribution of storm surges given an arrival (as shown in Figs. 1 and 2) . We store the highest of the N k storms for our yearly arrival height; if N k = 0 then the highest surge height is stored as zero for that sample.\nThe amplitude of the astronomical tide in Galveston is approximately 0.3 m. We linearly add tidal heights to the surge level by randomly drawing from a sinusoidal curve of amplitude 0.3 m, assuming the peak surge have equal probability to arrive at any time during a tidal cycle."}, {"section_title": "Sea level rise", "text": "The increasing risk from rising global average sea level under climate change is the result of oceanographic processes (mainly the thermal expansion of ocean water), the loss of mass from glaciers, ice caps and the great continental ice sheets, and (much smaller) the loss of water from storage on land. Local sea level rise can then vary from this global average because of isostatic adjustment to loss of glacial mass, tectonics and other local effects. We apply a probabilistic summary of these effects by Kopp et al. (2014) . They estimate median and confidence intervals by time period and region for three alternative RCPs. We apply their results for Galveston in 2030, 2050 and 2100-using the estimates under RCP4.5, which is close to the A1B scenario underlying our climate model inputs. Lognormal fits very closely approximate the estimated means and variance, and these fits are applied in the construction below of total flood risk (sea level rise, storm surge, tidal effects and subsidence) as it shifts over coming decades.\nThe Kopp et al. estimate of sea level rise from 2000 to 2100 can be compared with results achieved by other groups. The estimated risk for Galveston (median of 105 cm with a 5-95% confidence interval of 75-144 cm) is greater than the judgmental aggregation of sources by the IPCC, which for RCP4.5 shows a global 5-95% interval of 33-62 cm (IPCC, 2013) . On the other hand the estimate for New York City in 2050 (5-95% confidence interval of 19-53 cm) shows lower flood risk compared to an estimate prepared for the City for use in its planning, which has a 10-90% range of 18-88 cm (NPCC2, 2013)."}, {"section_title": "Subsidence", "text": "The rate of subsidence will depend on several factors including the rate of extraction of water, natural gas and oil. We base our estimate on the previous century's subsidence levels as measured and reported by the U.S. Geological Survey (Prince and Galloway, 2001 ). (For a map of Galveston area subsidence levels, see Lickley et al., 2013) . The study site sits in a zone that has seen roughly between 1 and 1.2 m of subsidence between 1906 and 2000. To express this coming century's subsidence, we use a triangular distribution and assume a slower rate than in the past, assuming a mean of 0.6 m, a lower bound of zero and upper bound of 1.2 m. The slower rate reflects an increased understanding of the human impacts on subsidence and therefore an increased ability to mitigate the human influence."}, {"section_title": "Annual flood risk", "text": "We combine the estimated distributions of surge (with the tidal effect), sea level rise and local subsidence to yield estimates of P t (X > x), the annual flood exceedance probability. For current conditions storms arrive with current intensities and frequencies, with tides considered at current sea level and ground elevation. For the 2100 climate we use a numerical approximation by sequentially sampling from each of the four risk distributions and linearly adding the sampled annual maximum surge, sea level rises, and subsidence. (Possible non-linear interaction between surge height and sea level is relatively small; for example, it is shown by Lin et al. (2012) to be negligible for coastal areas in New York). We repeat this Monte Carlo procedure 10,000 times for both the 2000 and 2100 climates. For intermediate decades we first linearly interpolate between 2000 and 2100 risks for storm surge and subsidence and for sea level rise we use the years 2030, 2050 and 2100 distributions estimated by Kopp et al. (2014) , and interpolate for the intermediate decades.\nWe then repeat the Monte Carlo procedure 10,000 times to combine sea level rise with storm and subsidence for each intermediate decade.\nThe results for the GFDL model across climates is shown in Fig. 3 , where a facility sitting at 1.5 m above current mean sea level has a 1.27% chance of flood heights reaching or exceeding that facility's elevation in each year under 2000 conditions, and this probability increases to 61.5% in 2100. (For results for other climate models, see Lickley et al., 2013) ."}, {"section_title": "The adaptation decision", "text": ""}, {"section_title": "A sample facility", "text": "To illustrate the application of the DP cost-benefit analysis approach above, we use the example of a hypothetical oil refinery located in Galveston Bay (near Texas City) located n = 1.5 m above the 1980-2000 mean sea level. These large oil processing facilities are owned by very large corporate organizations, which in general self-insure, so the risk-neutral assumption underlying the DP procedure is a reasonable approximation of their behavior."}, {"section_title": "3", "text": "Damage D(h) is modeled as a function of the level of inundation depth in the facility in a flood event, as protected by the levee, A t + S t . FEMA in its HAZUS system provides resources to aid in the analysis of flood damage to different types of facilities in the U.S. (Scawthorn et al., 2006; HAZUS, 2014) . For this illustration we supplement the FEMA approach with information on reported losses in the flooding of an oil refinery in an earlier hurricane-driven storm surge. The loss is estimated to be linear in the inundation level, from zero damage if h = 0 to a maximum of $1.5 billion if h P 2.5. For application in the DP algorithm this linear relation is approximated by a discrete function with 0.1 m increments.\nThe available adaptation option is assumed to be the construction or augmentation of levee protection, 4 and we estimate the maintenance and operating costs per meter of protection using estimates by Linham et al. (2010) . As an example of the types of economic information required for this type of analysis, the following estimates are employed in the decision analysis: We consider adaptation options between 2010 and 2100 and allow structures to be built in each decade (so the last time period under consideration is 2090-2100). Further, we allow for up to a 10 m levee to be added in 0.5 m increments."}, {"section_title": "Today's decision under rising risk", "text": "We apply these risk results to seek the decision-making sequence that yields the minimum expected cost: flood damage plus cost of protection. The derived sequence of decisions for levee height with this model is shown in Fig. 4 . The GFDL model indicates the highest level of protection over time, as it projects the largest increase in storm activity, increasing levee height to 6.5 m in 2070. CNRM and ECHAM yield a less aggressive protection sequence, ending with 4.5 m, and MIROC leads to a 5 m levee. The storm patterns projected by the climate models lead to different protection levels even under current climate (with no additional subsidence over the course of the century), with GFDL and CNRM indicating a 3.5 m levee today and ECHAM and MIROC suggesting a 2 m levee.\nThis decision-making framework also can be used to inform abandonment decisions. For example, the associated net present value of protection costs with the GFDL sequence is $22.8 million. If today's net present value of the facility were lower than this amount, then the expected damages would outweigh the benefits of keeping the facility in operation and the economic response would be to abandon the facility.\nThe value of careful exploration of the possibility of sequential adaptation over time can be seen in the additional cost if decision makers base protection decisions today on an estimate of risk in a distant future year. In this example if protection is extended now to the level indicated as appropriate for conditions expected in 2100, this premature action would add to the present value of the costs of flood risk (protection plus un-avoided damage) by between $15 and $23 million depending on the climate model."}, {"section_title": "Further extensions and application", "text": "The analysis approach applied here can serve a number of emerging problems in the adaptation to rising coastal risk. Firms with vulnerable facilities and city, state and regional authorities faced with issues of zoning, building standards and efforts to anticipate future investment in protection will need analysis of this type to inform both magnitude and timing of actions. Moreover, estimation of rising physical flood risk will be useful in framing changes needed in the FEMA flood mapping system and the flood insurance programs that are tied to it.\nFurther development of the method can incorporate more information, such as consideration of economies of scale in the construction of protection and adding the risk of failure. Also, the analysis can be extended to take account of potential changes over time in the cost of a flood event, as prices and physical facilities change, and to consider uncertainty in the emissions projections input to the climate analysis.\nThe sample application used here also highlights research on physical flood risk that will increase the usefulness of this approach to decision support. These would include the estimation of the decade-to-decade evolution of risk over time as displayed in Fig. 3 . Improvements in the computational efficiency of higher-resolution hydrodynamic models (e.g., Westerink et Dietrich et al., 2011) will improve surge analysis, and non-linear interaction between flood components may be incorporated numerically. Most important, all studies of coastal risk will benefit from ongoing research and analysis of sea level rise, especially the behavior of the continental ice sheets.\nThese detailed improvements may not be available in the near future and may or may not be needed depending on the application. What is most important is to make clear the limitations of choices based on 100-year flood maps under current climatology and to create an analytical environment that helps decision makers think about adaptation to projected climate change in terms of a sequence of actions under rising risk."}]