[{"section_title": "N", "text": ""}, {"section_title": "Beginning Postsecondary Students Longitudinal Study", "text": "Second Follow-up Field Test Report; BPS:90/94    Table IL1   Table IV.1 --Table IV.2   Table IV.3 --              Background and Purpose of BPS Tht need for national data concerning pressing postsecondary education (PSE) issues (such as: access, choice, enrollment, persistence, progress, curriculum, attainment, continuation into graduate/professional school, and rates of return to society), led NCES to develop an information system to provide comprehensive data on these conditions and outcomes. The base for these data is the National Postsecondary-Student Aid Study (NPSAS), first implemented in the 1986-87 school year, which yields a nationally representative cross-sectional sample of postsecondary students every three years. Cost efficiency, minimization of respondent burden, and maximization of utility of extant information dictated that the current BPS study use NPSAS:90 data collected from first time beginning/entering students (FTBs), and follow these students from their initial enrollment in PSE through completion of their education and entry into the workforce. The BPS concept represents a bold departure from previous longitudinal PSE studies based on high school grade cohorts, in that it starts with a cohort of individuals beginning their postsecondary studies, regardless of when they completed high school. Consequently, information will be available from BPS about \"nontraditional\" PSE students who have delayed the continuation of their education due to military service, family responsibilities, or other reasons. This is important, since the \"nontraditional\" student represents a steadily growing segment of the postsecondary student population. All types of postsecondary students (academic, vocational/occupational, and technical) are included in the study and can be represented in known proportions under the current design (rather than only stochastically under high school grade cohort designs)."}, {"section_title": "1", "text": "Major educational policy questions to be addressed by information collected during the study are: How and why do students continue their enrollment in PSE? How is postsecondary education financed? What courses are taken and what grades and credits are earned? What fields of study are pursued? How extensive is, and what are the patterns of, transfers between schools? What is the extent and timing of program completion? What is the extent of progress toward, and attainment of, degrees, licenses, or certificates? What is the nature and timing of application for, and continuation into, graduate or professional school? What is the impact of the PSE experience on subsequent life experiences (jobs, family formation, lifestyles), particularly as related to returns for the overall society? How are these features of PSE different for different types of starting PSE students? The focus of the BPS:90/92 data collection was continued education and experience, employment and financing, educational aspirations, and family formation since the base year study (NPSAS:90). BPS:90/94 data, when combined with BPS:90/92 data, will serve to monitor academic progress for/more than four years since initial PSE enrollment. BPS:90/94 areas of focus include: continued education experiences and financing, including degree attainment and graduate/professional school access; employment experiences; rate of return issues; educational and employment aspirations; and family formation since the base year study. The current BPS study is directed toward FTB PSE students in the base year (the 1989-90 school year for the full-scale study and the 1988-89 school year for this field test), who were interviewed during NPSAS:90 and its associated field test. These students are being interviewed at two year intervals, using Computer Assisted Telephone Interviewing (CATI) procedures, to determine their educational and related experiences since they were last surveyed. The BPS:90/94 second follow-up field test, involving an initial sample of 1,446 students entering 66 PSE institutions in academic year 1988-89 (AY88-89), has been completed. The full-scale study will be conducted during the spring and summer of 1994, and will involve 7,914 students who entered 1,008 PSE institutions during  The BPS:90/94 field test benefitted significantly from experiences and evaluations during prior surveys conducted in the NPSAS/BPS:90 series. Prior data for this longitudinal study were collected during NPSAS:90 and BPS:90/92. NPSAS:90 was conducted by Westat,"}, {"section_title": "II. DESIGN AND METHOD OF THE FIELD TEST", "text": "A. The HPSAS:90 and BPS:90 Samples The BPS:90 field test sample comprised students beginning PSE education for the first time in the 1988-89 school year (i.e., between July 1, 1988 andJune 30, 1989). The sample was composed of a subset of interview respondents from the NPSAS:90 field test sample, which covered all sectors of postsecondary education and all students enrolled in those sectors during the 1988-89 school year.' All institutions offering academically or vocationally oriented programs in postsecondary education were eligible for NPSAS participation.' Specifically, to be considered eligible for NPSAS, an educational institution must have: offered a program designed for persons who have completed secondary education; offered programs that were academically, occupationally, or vocationally oriented; made offerings available to persons other than those employed by the institution; offered more than just correspondence courses; offered programs lasting at least three months or 300 contact hours; and, been located in the 50 states, the District of Columbia, or Puerto Rico. Institutions were excluded if they: only served secondary students; only provided avocational, recreational or remedial courses (e.g., hang gliding schools, exercise classes, dance courses); or only provided seminars of relatively short duration. The base year (NPSAS:90) field test student sample was drawn from postsecondary institutions purposively selected to provide a full range of the institution types expected in the full-scale sample. The NPSAS:90 full-scale student sample was drawn from randomly selected institutions over multiple time segments (i.e., a fall sample and non-fall samples) to capture enrollments throughout the school year of interest (July 1, 1989through June 30, 1990.8 The NPSAS field test sample, however, was drawn only from those enrolled in eligible institutions in the 1988-89 fall term (or during October, 1988). To be eligible, a student must have been enrolled for one or more of the following purposes: taking course(s) for credit; in a degree or formal award program; or in an occupationally specific program. All other students, such as those who were only taking a course for remedial or avocational purposes and not receiving credit, those who were only auditing courses, or those who were only taking courses for leisure rather than as part of an academic, occupational or vocational program or course of study, were not eligible for NPSAS:90. In addition to these criteria, students who were only in a high school program were not eligible. Students were eligible irrespective of their U.S. residence or citizenship status and even if they were only enrolled part-time. Figure 11.1 shows the sequential process in selecting the BPS field test sample from the NPSAS:90 field test sample. From the full NPSAS:90 field test sample, 3,256 base year interview respondents, from 73 PSE institutions, comprised the set from which the BPS field test sample of FTBs was selected. The preliminary BPS sample (1,981 from 67 institutions) was determined on the basis of information provided by the NPSAS:90 institution and/or by the student in the base year interview. Based on information subsequently obtained in the BPS:90/92 interview, some of those originally classified as FTBs proved to be ineligible and were excluded from the sample.9 Deceased sample members were also excluded from the sample, and those institutionalized or incapacitated together with \"hostile refusals\") were considered permanent nonrespondents and, while still considered eligible sample members, were excluded from the working sample. The working sample for BPS:90/94 contained 1,446 current or former students from 66 institutions; it included both BPS:90/92 respondents (verified as study eligible) and nonrespondents (for whom eligibility status was not yet validated).\" Additional preliminary sample members (among those not contacted in BPS:90/92) were determined ineligible (non-FTB or deceased) based on BPS:90/94 interview responses or status reported by contacted parents or relatives (see fuller discussion in Section IV.A). Additional hostile refusals were also identified and removed from the working sample. Due to the markedly greater difficulty in locating nonrespondents to the first follow-up (see Chapter IV) and the known reduction in reliability of information obtained about more-and-more distal time points, we recommend that nonrespondents to both follow-ups (BPS:90/92 and BPS:90/94) be considered as permanent nonrespondents and excluded from the working field test sample in subsequent BPS:90 follow-ups. This exclusion is shown in Figure El. Table H.1 shows the sample sizes in the sequential studies by type of NPSAS:90 school. Sample distribution changes between the base year and first follow-up are principally related to the 4-year stratum, reflecting the loss of schools (with associated students) offering only graduate/ first professional programs and the loss of upper level students in other 4-year schools. Relative distribution changes between first and second follow-up are even less    Includes 4-year schools with graduate or first professional programs, in addition to undergraduate programs, as well as those with no graduate/first professional programs; while NPSAS:90 contained schools offering only graduate programs, BPS:90 did not (by defmition). Contains one proprietary 4-year school for NPSAS:90; this school contained no students eligible for BPS. 1.9 pronounced, and represent principally differential initial FTB misclassification rates within the different school types. B."}, {"section_title": "Overall Design and Integrated Control System (ICS)", "text": "The BPS:90/94 field test design involved tracing sample members to their current location and conducting a computer assisted telephone interview (CATI) with them about their exneriences since they were last surveyed.12 An additional experimental locating approach was introduced in the BPS:90/94 field test; namely, implementation of field locating activities for sample members unsuccessfully traced by telephone. Also, the second follow-up field test experimented with a motivational \"project information sheet\", included with the prenotification letter. Otherwise, most procedures used were result-based refinements of welltested activities implemented in the BPS:90/92 field test and full-scale studies. The flow of tracing and data collection activities for the BPS:90/94 field test is shown in Figure 11.2. The first locating step involved an address/telephone update by a national change of address (NCOA) service. NCOA provided: (1) updated addresses for each reported forwarding address during the preceding two year period; (2) an indicator of whether the forwarding address itself had proved undeliverable; and (3) where available, a telephone number.13 Following completion of NCOA activities, cases were classified as either: (1) directly eligible for CATI (previously contacted cases), or (2) requiring central telephone tracing (previously uncontacted cases). If this telephone tracing was unsuccessful, pre-CATI field locating was initiated to locate those individuals. Successful locating, in either case, provided more current information for use in subsequent contacting during CATI. Field trace locating failures were considered terminal \"unable to locate\" and were not pursued further in CATI operations. About three weeks prior to the start of CATI operations, packets were mailed to sample members for whom a valid address was available. Each packet (see Appendix B) contained: (1) a cover letter from the Commissioner of Education; (2) an additional letter describing the second follow-up and the forthcoming interview; (3) a leaflet describing the Longitudinal Studies Program and BPS; and (4) an address update sheet and return envelope. For a random half sample of the field test students, the packet also included a motivational information sheet with information about NPSAS:90, BPS:90/92, and BPS:90/94. The mailing provided written information to sample members close to the initiation of interview and provided a vehicle for sample-member-reported current address and telephone information reduce respondent burden (i.e., prior responses needed only respondent verification). Since all pre-CATI locating and file preparation activities involved the use of extant data or collection of only directory information, all such activities were accomplished prior to approval of the BPS:90/94 interview data collection by the Office of Management and Budget (OMB), which facilitated an early initiation of the interviewing activities. CATI locating and interviewing began after final OMB approval of the data collection instrument. Even with pre-CATI locating efforts, some cases were not located during initial CATI efforts; such cases (expected on the basis of BPS:90/92 results) were intensively traced through a CATI-external central-office telephone approach. Those not located through that approach were sent to post-CATI field locating. Due to the abbreviated field test data collection period (and the high cost of initiating field locating), only a subsample, of those identified as needing this final intensive locating was done. All locating, interviewing, and data processing and access activities were under the control of an Integrated Control System (ICS), consisting of a series of PC-based, fully linked modules. The various modules of the ICS provided the means to conduct, control, and monitor the seve.,a1 complex, interrelated activities required in the BPS:90/94 field test. All procedures related to locating, data collection, data processing, project management, report production, data analyses, and document archiving were integrated into this system. The ICS structure allowed for streamlining of related tasks and served as a centralized, easily accessible repository for project data and documents. The versatility of the system is highlighted by the fact that various tasks were performed at different sites (e.g., the CATIexternal locating component of the ICS was located remotely on a Novell network and linked through telecommunications channels). The ICS provided authorized project staff (and NCES staff as remote users) menu-driven access to data from prior studies (NPSAS:90 and BPS:90/92); subject matter information reference files; interview data; locator files; project documents and reports; and analysis files and systems. Its use also enabled the application of extensive quality control measures throughout the various project activities. Figure 11.3 presents a schematic of various components and features of the ICS. The central system resided on a DEC PATHWORKS PC network, accessible to remote users through a dedicated network modem. Case-level status as well as routine summary reports were available across all components of the system. Information was integrated through the implementation of a case-level control system which monitored status in the various stages of production: prenotification mailing; pre-CATI tracing (telephone and field); CATI locating; post-CATI intensive locating (telephone and field); interviewing; data abstraction; and data editing. Status from separate stages was transmitted to the master ICS to allow control of the flow of events in the system and monitoring of performance of study requirements. Within the system, Lotus cc:Mail and Lotus Magellan facilitated telecommunications among project personnel (within and between three contractor organizations and NCES) for project-wide correspondence, transfer of files, and information access. This provided vehicles for raising critical issues and allowed quick responses from appropriate staff members. Documentation of decisions reached were available for later reference as needed. Project scheduling was maintained and monitored with the use of TimeLine. Greater detail of the CAT1-external and CATI-related components of the ICS are provided in Chapter HI (sections B and C, respectively). Project activities were conducted in compliance with all applicable provisions of the Privacy Act of 1974 (5 U.S.C. 522a); Privacy Act Regulations (34 CFR Part 5b); Section 506(d) of the General Education Provisions Act, as amended by the Hawkins-Stafford Amendments of 1988 (PI,; and NCES Standards and Policies. RTI also maintains a standing Committee for the Protection of Human Subjects to ensure that all Institute studies of human populations comply with applicable regulations concerning informed consent, confidentiality, and protection of privacy. This committee independently reviewed the study design, instruments, and data collection/processing procedures to ensure that sample members' rights were fully protected. All contractor staff were fully informed of all applicable confidentiality, nondisclosure, and privacy requirements. Each project staff member who was involved in any way with personally identifiable information was required to sign a Confidentiality Agreement and to swear/affirm to compliance by signing an Affidavit of Nondisclosure. Original-signature, notarized copies of the Nondisclosure Affidavits for all staff were provided to NCES. Original-signature, notarized copies of the executed Confidentiality Agreements and Nondisclosure Affidavits were maintained by RTI for all project staff (including subcontractor staff), and additional original-signature, notarized copies were maintained by subcontractors for their specific staff members."}, {"section_title": "C.", "text": "\nCATI-Related Systems Figure III.1 summarizes some of the features of the BPS:90/94 field test CATI instrument and its associated configuration. The list of features highlights the complexity of the survey instrument to account for widely divergent response possibilities and branching patterns. Highlights of the system included: on-line help; validation/verification checks of data; special screens (user exits) for collecting complex data elements efficiently; conversion of dollar amount indeterminacies; and ample space for interviewer comments to facilitate the smooth handling of each case. The interview was programmed in CASES, which proved more efficient than the CATI language used for the first follow-up. CATI was supported within a PC environment; separate CAT' stations were networked through a DEC server. With minor changes in system configuration, the same system is planned for the full-scale study. In addition to the systems necessary to develop and implement the CATI program, a number of interrelated operational tasks required coordination while conducting data collection. These include: (a) preparing test data and special test applications for training; (b) preparing preload data and setting up cases for CATI production; (c) identifying cases requiring intensive locating and providing the mechanism to transfer data between CATI and CATI-external operations; (d) defining the calling plan scheduling procedures; (e) minimizing interview refusals; (f) refining procedures for reporting production results; and (g) testing and integrating changes to the instrument during field test operations so that problems could be pinpointed and corrected in preparation for full-scale survey operations. The control functions and systems used to accomplish these activities are described and evaluated in this section. \n\n"}, {"section_title": "Methodological Experiments and Evaluation Approaches", "text": "Evaluation of field test procedures have obvious implications for possible improvement of procedures for the subsequent full-scale study (as well as for enhancements for subsequent follow-up studies of the BPS:90 cohort or additional cohorts). Each major component of the field test was evaluated. Methodology consisted of both formative and summative evaluations. Formative evaluations were of an ongoing nature, designed to assess tasks at intermediate stages so that the effects of employing alternate methodologies could be analyzed and modifications and revisions could be employed and assessed prior to task completion. Summative evaluations assessed the results of the field test, including procedural changes instituted during the course of the study. Results of summative evaluations will be used to optimize procedures in the full-scale study. A summary of planned field test evaluations is provided in Figure 11 A critical part of the field test operational evaluation was regular quality circle meetings with survey operations staff, interviewers, interview monitors, and interviewer supervisors. These meetings provided an easily available forum for production staff and project management to address the important topic of work quality, discuss issues of concern, identify problems with the survey instrument, share ideas foi improving the instrument, and suggest various approaches for gaining cooperation from sample members. To implement suggested improvements arising from such meetings, the study instrument was refined a number of times over the course of the data collection period. On completion of data collection, final quality circle meetings were held, serving as debriefing sessions for the full operational period. Based in part on debriefing comments, additional adjustments will be Figure 11. 4  made to item wording, question format, and survey procedures, as necessary, to ensure more efficient and effective full-scale survey implementation. The field test included two methodological experiments, evaluated for possible implementation in the BPS:90/94 full-scale study: (1) Field Locating. In order to increase locating rate for the telephone survey, an in-person locating approach was evaluated. To examine cost effectiveness of the field effort, various field locating approaches were attempted. One approach which was examined involved separate treatment of cases that could be clustered within a relatively compact geographic area and those that were more dispersed. Because dispersed cases could not be handled cost effectively by any of the major contractors on the study team, these cases were assigned exclusively to. a commercial locating agency (Equifax), with over 300 offices throughout the country. Clustered cases were randomly allocated (by cluster) to Equifax and to contractor staff. Marginal per-attempted-case costs for the locating activities were standardized over methods used so that method efficacy would be reflected in contact rates. (2) Motivational Information Sheet. An additional effort to increase cooperation among sample members consisted of an informational sheet discussing interesting aspects of the study. A fifty percent subsample of the BPS:90/94 field test sample (stratified by NPSAS:90 school) was selected to receive the motivational material as an additional element in the prenotification mail packet. Fuller descriptions of, and evaluation results from, each of these experiments are presented in Chapter IV. As indicated in Figure 11.4, the study design included a reliability reinterview, conducted about 4-8 weeks after initial interview; this involved a random subsample of 100 respondents to the initial second follow-up interview. The reliability reinterview contained only a small subset of the initial interview items. Reliability reinterview results are discussed in section V.B. To the extent possible, BPS:90/94 field test operations were automated, involving the development/maintenance/use of a number of interrelated software products. Major systems/ procedures are defined and evaluated in this chapter. Of particular note is the fact that CATIexternal tracing activities were conducted separately (and actually at a different site) from CATI operations. The two operations were managed by separate systems; however, integrated control across the two systems (as well as control within CATI-related systems) was managed, on a case-level basis, by the RTI Fully Integrated Control System (FICS). A."}, {"section_title": "Preliminary File Work and CATI Pre loading", "text": "Data obtained previously during the field tests for BPS:90/92 and NPSAS:90 were used in the BPS:90/94 field test tracing and CATI operations. These data included locating information obtained from a number of possible sources: the BPS:90/92 field test interview locating section; intensive telephone tracing performed during BPS:90/92; pre-CATI mail returns or telephone contacts (including information from sample members, parents, other contacts, and institutions); and NPSAS:90 base year locating information. A total of 263 existing or preconstructed data elements were preloaded into the CATI record to customize each interview. In addition to \"current\" locating and tracing information described above, example data elements included postsecondary school information; jobs held in 1991; demographic information; and income data.' Other preload variables were indicators, to trigger the need to collect specific data elements during the BPS:90/94 interview (e.g., retrospective data for BPS:90/92 non-respondents); in other cases, missing preload data indicated the need to collect the information in BPS:90/94. Random assignment flags for information sheet treatment and random start points for subquestion sequences were also included among the preload variables, as was a pre-assigned random number for use in selecting respondents for reliability reinterviews. While random number generation within CATI is not difficult (and had been used in the previous follow-up), the pre-generation approach for random start points provided a straightforward method for balancing the ordering within each NPSAS:90 school (a procedure that would have required considerable CAT' program code);'5 given the advantages of pre-assignment of the random start points and the clear need for pre-assignment of the information sheet treatment, pregeneration of the additional random number seemed most efficient. Despite the increased number and complexity of preloads over those used in BPS:90/92, the preliminary file processing preload operation was considerably less difficult for the second follow-up. This is attributable to three basic factors: (1) BPS:90/92 files had 14 Sample member identification (ID) numbers were included; however, they were converted from the 10 digit code, used in BPS:90/92 and NPSAS:90, to an 8 digit code to reduce keystrokes required in accessing the cases (complete crosswalks of the unique 1D code mapping were maintained for subsequent data linking). been thoroughly cleaned and specifically developed to facilitate data extraction for BPS:90/94 preloads (also necessary cleaning of NPSAS:90 data had already been accomplished during the first follow-up); (2) the CATI software used for BPS:90/94 (CASES) was more amenable to data preloading than that used for the earlier follow-up; and (3) the data dictionary based CAT! Instrument and Data Management modules of the ICS (see section III.C.1) facilitated the development of the preload files. Generally, the procedures used were considered near optimal and will be replicated, with only minor refinements, for the full-scale study. B."}, {"section_title": "CATI-External System", "text": "The CATI-External Locating Module (CELM), operated and maintained at a site remote from CATI operations, was first developed during BPS:90/92; it was designed to accommodate original and updated locating information for students, parents, friends/relatives, and institutions. The CELM also selected and extracted information for the student mailing and stored pre-CATI and post-CATI intensive telephone and field tracing updates (where applicable) and maintained a per-case historical record of all update transactions."}, {"section_title": "I. CELM modifications", "text": "The CELM underwent some modifications, to assure compatibility with BPS:90/94 field test procedures. Distinct CELM modules provided for three phases of tracing activities: a pre-CATI phase that tracked and updated both telephone and field intensive tracing; a mailout to respondents that tracked outgoing and incoming mail and associated file updates; and a post-CATI phase that tracked and updated both telephone and field intensive tracing for cases that could not otherwise be located in CATI. The major CELM modification was including a field component to intensive tracing in both the pre-and post-CATI phases. The modifications allowed: subsampling' for field trace those cases failing the initial central-office telephone intensive tracing; subsequent allocation of the cases to Equifax or contractor team field tracing units;`' and full reporting of field operations. Menu options allowed for rapid transfer of cases to the field as well as updates from the field."}, {"section_title": "2.", "text": "\n\n\nEducation Experiences: Grades, Tests, and Expectations Table V.3 shows estimates of temporal consistency for items about undergraduate grades, expected level of overall education completion, and whether any graduate admissions tests were taken. The results indicate generally high levels of temporal consistency. This is particularly noteworthy for the item on education expectations, which is, \"Considering all practical constraints, what is the highest level of education you ever expect to complete?\" Tying the item to practical expectations seems to provide more response stability than was possible in \"aspiration\" versions of this question. While it seems likely that different respondents will consider different sets of \"practical constraints,\" individual respondents seem to interpret the item consistently across short term repeat interviews.  Work Experienct.7. General Job Informrlon Table V.4 presents reliability reinterview results for work-related information. Reports of any job since February 1991, number of jobs since February 1991, and first and most recent job start dates had high levels of response consistency. Furthermore, for these variables, the levels of consistency in Table V.4 are at least as high as, and in some cases higher than, the levels of temporal consistency observed in earlier reinterview studies. Table V.4 raise concerns. In particular, the 5-category item about primary role of the sample members while attending school (asked of those respondents who worked during or before their last education spell) has a low Recent icti ss,s ',, , , "}, {"section_title": "Transfer of files to and from CATI", "text": "The CELM selected addresses and telephone numbers for all 1,446 cases for preloading into the CATI system, with priority determined by recency of information and likelihood of successful CATI contact. Output data was formatted for direct integration into the CATI module. Cases that failed locating in the CATI phas:, were downloaded to the CELM for intensive tracing. This was accomplished through an automated CELM preloading function, which also generated the various locating materials used by the intensive tracing staff. Another menu option generated weekly updates containing updated student addresses 16Post-CATI subsampling was necessary because field test cases requiring field tracing exceeded the number budgeted. 3 0 and telephone numbers for loading into CATI. Those cases that were successfully .located during the intensive tracing were included in these weekly files. 3."}, {"section_title": "Evaluation of CELM", "text": "The field tracing modules were well integrated within the existing CELM framework, and provided sufficient detail of field case status to successfully manage the field tracing operation. The post-CATI intensive telephone tracing module, which had been tested during BPS:90/92, continued to be effective; the pre-CATI module (which was new for BPS:90/94 but based on the tested post-CATI module) was comparably effective. Automation of case transfer to field tracing during post-CATI operations proved quite valuable for case management and will be implemented for pre-CATI as well as post-CATI operations for the full-scale study, to facilitate case assignment to field tracing on a flow basis. The menu option, which preloaded the post-CATI telephone intensive tracing module of the CELM and subsequently produced all necessary information, allowed intensive tracing to begin almost immediately upon receipt of cases from CATI. Further, the automated CELM preparation of CATI-preload files allowed accurate and timely locating information to be returned to CATI with minimal effort. Both of these features of the CELM will be retained."}, {"section_title": "CATI Instrument and Data Management Modules", "text": "An integrated data dictionary approach to CATI instrument development was initiated for the BPS:90/94 field test. This approach uses a single master data dictionary together with associated relational files to: (a) produce, review, and revise the CATI instrument (both a hard copy facsimile and the actual executable program code); (b) generate on-line and hard copy question-by-question specifications; and (c) produce data files and related documentation. The approach enables a logical separation of each of the components of the system. It enables multiple instrument developers to work simultaneously on different components. The documentation of variables and linkage of components provide the tools necessary for constructing question by qiiestion specifications and analysis file products. While the initial developmental effort for such an approach is markedly greater than for normal CAT' development, overall efficiency (considering both the subsequent need for changes and the multiple uses in related study activities) is considerably enhanced. Moreover, use of a common, updatable base file virtually eliminates transcription errors in describing the procedures and documenting the resultant data files. The approach was particularly well suited for the basically linear and grammatically structured CATI programming used for BPS:90/94 (i.e., CASES). Figure 111.2 illustrates the various components of the CATI instrument and data management ICS module. Component files are indicated on the far left of the figure, within the dashed box; programs that operate on the files are shown in the middle, within double lines; and products from the programs are shown at the right. RTI's Fully Integrated Control System (FICS) was used as the database management system for this process. Linkage information for all component.files was maintained in the data dictionary; consequently, the instrument, data files, and associated documentation could all be produced from the same components. Major file components include the OMB data element specifications; question stems, response options, skip and consistency logic, and associated help screens; \"user exits\" (separate programs, typically programmed in C, to collect data on special screens or in special ways that were beyond the limits of the core programming language); preload variables; and data files. Instrument developers provided direct input into each file component. Partitioning the instrument into logical components also reduced entry and storage redundancies. In many cases, the same basic question stem wording was used for multiple items (e.g., repeat blocks asked the same set of questions, with only \"fill in terms\" --fills -such as school name or term changing from one block to the next). Such questions can be represented by a single stem, with the different \"fills\" defined in the data dictionary and entered separately in the item fill file. Allowable response alternatives were distinguished from stem wording, since many items used commonly occurring response alternatives (e.g., a large percentage of the CATI items were simply \"Yes/No\" questions, which required only one entry in the response alternatives dictionary).  Instrument flow and control (i.e., specification of CATI branching as well as on-line entry validation where appropriate) were based on logic, which was stored separately in the logic file and linked to the appropriate point in the instrument through the data dictionary. Another component file contained on-line help screens associated with each CATI screen (and from which hard copy question-by-question specifications could be easily generated). The data elements approved by OMB also comprised a separate component file; mapping of each interview item to a specific OMB-approved element was maintained in the data dictionary."}, {"section_title": "//", "text": "Despite the many advantages of CASES, optimal presentation of a number of questions could not be reasonably accomplished within the limitations of the programming language (e.g., full screen entry and editing). Consequently, special screens were created which used exec;utAle modules developed in other languages to present the question and collect and return responses to the main CATI program. These \"user exits,\" which operated analogously to subroutine calls, were stored in a separate component file. The dictionary provided mechanisms for linking CATI and these special screens together seamlessly. Major system programs shown in Figure 111.2 are the partitioning module, a code generator, a preload builder, and the data file builder/documenter. The file builder formats and locates data in appropriate relational data files, based on parameters entered in the dictionary. The documenter provides all material needed for electronic codebooks (ECBs), including frequency distributions, again based on dictionary linkages. The preload builder, discussed in greater detail in the following subsection, was used primarily during testing of the instrument; it allowed user definition of the preload values necessary to guide the interview section being tested. The partitioning module produces screen files or hard copy of subsets of the interview items and help screens. The major hard copy files produced were the question-by-question specifications (a listing of each item and its associated help screen -used as an interviewer training aid) and a facsimile interview (a copy of which is included in Appendix D). The partitioning module was also useful for review of items during development by creating the items, with logic and fills, from the separate component files. This feature could be used to review individual items, all items in a particular section, or all items linked to a specific OMB-approved data element (allowing compliance review). The code generator linked all interview-related components together and generated the actual CASES computer code for the interview. The instrument was basically developed by section (or subsection), and the code generator produced section/subsection modules separately. This approach allowed changes to a specific module and the regeneration of the code for that module, without affecting any other modules. Consequently, even large instrument changes could be incorporated in a very short period of time by amending the dictionary, amending the associated component files, regenerating the CATI code for affected modules, and linking changed and unchanged modules into an updated full instrument. The data dictionary approach proved to be an extremely effective mechanism for developing and refining the BPS:90/94 field test CATI instrument, and the file structure and systems to support the approach worked quite well. A major residual advantage of the adopted approach is that the current field test data dictionary facilitates considerably the development of the full-scale study instrument. Changes in question order or wording, question additions or deletions, additional validation/consistency checks, or even additional 23 36 user exits can be easily incorporated within the existing dictionary and files; consequently, the full-scale study instrument development process should proceed quite smoothly."}, {"section_title": "Testing the CATI Instrument", "text": "The BPS:90/94 field test instrument was tested thoroughly by both contractor and NCES staff before it was implemented. The flow logic of the field test CATI instrument was quite complex. The values of preload variables as well as the specific answers to prior questions in the current interview determined which questions were asked and which were skipped. Testing the instrument was critical to ensure that branching was appropriate; however, the instrument contained a large number of preload variables (see Section III.A).18 Consequently, as interview modules (sections or subsections) were developed, it was necessary to identify and include the preload items required for those sections in order to adequately test the programmed interview module. The \"preload builder\" enabled the user to define values of preload variables for the interview module (or set of modules) being tested. Values were specified by: (a) selecting a previously defined full set of preload values, (b) interactively defining a full set of new preload values, or (c) choosing an extant set of values and revising some values. After developing a new or revised set of preload values, an option was available to add this new set of values to the previously defined preload set. In general, the preload builder was an effective tool and very useful in testing some of the early versions of instrument modules; however, as the number of required preload variables grew (263 preload variables in the final CATI instrument), updating, testing, and maintaining an expanding preload builder application became impractical.19 Instead, test cases with specially designed preloads to test various branches of the instrument were prepared for use in training of interviewers. Instrument modifications for the full-scale study (as they affect preload data) are expected to be minor; consequently, the existing \"test\" data will be refined as needed for any additional testing and for full-scale interviewer training. Nonetheless, an expansion of the current preload builder to its full potential would be considered worthwhile for initial instrument development in subsequent follow-ups. 3."}, {"section_title": "CATI Case Assignment", "text": "Control of case flow/assignment in CATI was facilitated by the use of the CASES QUEUES automatic scheduler (this scheduler is used to assign individual cases to CATI interviewers). Each non-finalized case was assigned to a specific queue for work within CATI (while queue assignments were unique at a given time, queue reassignment was realized with changing case disposition). Within this software, scheduling priorities are governed by the unique queue to which a case is assigned (queue assignment is typically based on case history and study specific case priorities) and a timing algorithm based on scheduled callback time or elapsed time since last call. Priorities for call scheduling were defined and modified as needed over the course of the data collection period. Finalized cases (e.g., interview completion or other final disposition) were placed in a special queue that was not accessed by the automatic scheduler. The automatic scheduler was set to select new sets of potential cases every 15 minutes based ,on the established algorithms. Automatic scheduling was augmented with final refusal assignment lists during the last four weeks of operations. Final refusals, existing in the finalized case queue, were not accessible to the automatic scheduler system; however, to improve response rate, these cases were worked by interviewers who'were highly skilled in refusal conversion and gaining cooperation. Some problems were experienced in the field test in using the automatic scheduler. Typically, these problems were not inherent in the software per se, but could be categorized as resulting from two basic areas in its use: (a) bad or missing input data, or (b) insufficient queue specification. Problems of the former type were fewer and less troublesome; an example resulted when time zones were missing for some phone numbers, causing such cases to be assigned prematurely. That problem was resolved by (a) including more rigorous edits of preload numbers and numbers returned from intensive trace and (b) imbedding CATIinternal routines to map interviewer-identified numbers to time zones. Similar solutions were applied to other problems of this type. The most frequent and problematic scheduling difficulties resulted from less than optimal use of scheduling queues. One example involved prior refusals. Once an individual refused (either explicitly or by abruptly breaking off the interview), the case was moved to a refusal queue until finalized; that queue could be accessed and worked only by well-seasoned refusal converters and only by direct user request for a case from that queue.\" While this approach was completely successful in preventing contact with prior refusals by inexperienced interviewers, established appointments within the refusal queue were not automatically assigned; consequently, some appointments with prior refusals were missed. A similar constraint placed on the \"Spanish speaking\" queue created a comparable missed appointment problem in that queue. Appointment queues will be added for pending refusals and for Spanish speaking cases to eliminate this problem in the full-scale study. Another queue misspecification problem was experienced with cases requiring external tracing/locating. The intent was to place such cases (both those still in pre-CATI locating when CATI operations started and those subsequently requiring post-CATI locating) in inactive queues, to be activated (or reactivated) in CATI once new phone numbers and addresses were obtained. These \"inactive\" queues were improperly defined in the scheduler; consequently, the inactive cases were sometimes assigned to interviewers. This introduced inefficiencies in the CATI locating operation (in the pre -CATI phase, where this was most prevalent, the interviewers were presented with obsolete or bad phone numbers). To correct 20This was initiated to avoid additional contact with these cases by less experienced interviewers who might further solidify the sample members' refusal tendencies."}, {"section_title": "25", "text": "3 8 these problems in the full-scale study, cases being worked in intensive locating (either pre-CATI or post-CATI) will not be accessible to the CATI scheduler. Because call-ins are possible for these cases, procedures will be available for manual accessibility, if needed. Results from field test operations also suggested additional uses for queues during the full-scale study. Since it proved easier to contact and gain cooperation from first follow-up respondents than non-respondents, these two groups will be placed in separate queues during full-scale CAT! operations. In this way, the easier cases (prior respondents) will be assigned to less experienced interviewers, while prior nonrespondents (including implicit and explicit first follow-up refusals) will be assigned to more experienced interviewers, who are better at difficult tracing and/or refusal conversion. This approach should result in a more efficient operation.'"}, {"section_title": "4.", "text": "Other CATI-Related Systems Reporting Systems. Daily reports detailing the status of CATI operations were generated under system control; separate reports were available for the main interview and reliability reinterview operations. Reports included daily and cumulative response results, projections, and the extent to which projections were being realized. Reports also included latest CATI result code status reports and an accounting of phone numbers available to be worked for pending cases. Generally, these reports were quite useful to both NCES and RTI project management staff. Nonetheless, the reports will be refined for full-scale operations to: (a) be more automated and (b) include information concerning interviewing hours (hours worked; hours per completed interview). Reintroducing Externally Traced Cases. When new telephone numbers and address information were obtained, they were reloaded into CATI so that appropriate calls could be made and appropriate addresses could be referenced in the locator section of the CATI instrument. A special system was developed to handle this \"reloading\" or record update. Because of updating difficulty inherent in the CASES software, the program required manual recall of the case and execution of an embedded external program to enter the information and reassign the queue. Because cases for post-CATI tracing require some time in CATI to be identified and additional time in CATI-external central office tracing (and, possibly even more time in field trace) to be located, timeliness in returning located cases to CATI is quite important within a fixed data collection window. In a nontrivial number of cases, CATI contact was unsuccessful despite updated telephone numbers. Some reasons for this included: gatekeepers screening calls to sample members (and not identifying whether the person resided at a given number); answering machines left on, thereby preventing human contact; receipt of the case so late as to preclude working it fully in CA1 I; inability to contact cases before numbers went bad; and bad locating leads. To improve full-scale operations, a number of adjustments are planned. To ensure that all cases located in intensive tracing receive prompt and appropriate attention in CATI, a special trace queue for cases returned from tracing will be added; only interviewers specially skilled in locating and refusal conversion will have access to this queue. To expedite the transfer of cases from CATI-external trace back into CATI, the procedure for updating telephone numbers and addresses in CATI will be streamlined to take advantage of batch processing capabilities tested during field test operations.\n\nWork Experiences: Principal Job Information Table V.5 provides results for respondent-identified principal job in 1992. Consistent with first follow-up results, the data element for type of company yielded a high percentage 76 A11 categorical relationship measures are sensitive to perturbations in the joint distribution of the two variables. In this case, much higher percentages of respondents considered themselves students than employees and, over interviews, respondents were more likely to switch from \"employees\" during the first interview to \"students\" during the reinterview than to switch from \"student\" to \"employee\". of agreeing responses, and a relatively high value for Cramer's statistic. For the remaining items in the table, none of which had been previously evaluated, percent agreement values are moderately high but the Cramer's statistic values ranged from high to very low. For two data elements (whether respondents used tools, equipment or skills they were trained for and whether additional education or training was required for advancement), the measured reliability raises considerable concern (less than .15 and not differing significantly from zero)."}, {"section_title": "IV. DESCRIPTION AND EVALUATION OF LOCATING AND DATA COLLECTION", "text": "A."}, {"section_title": "Overview of Results", "text": ""}, {"section_title": "I. Locating", "text": "Location of student cases for the BPS:90/94 field test was initiated with information provided by the BPS:90/92 locating database. All student and tracing source contact information contained in that database was submitted to a national change of address (NCOA) service for updating. Cases not located during BPS:90/92 were forwarded directly to pre-CATI telephone tracing, and subsequently to field locating if intensive telephone tracing was unsuccessful. (Both pre-CATI telephone tracing and pre-and post-CATI field locating were new procedures introduced with the BPS:90/94 field test.) Prior to the start of CATI operations, a prenotification mailing was made to the student, enabling current contact information to be provided to interviewers for basic CATI locating efforts. In the event that CATI-locating was unsuccessful, cases were seat to post-CATI central trace for telephone tracing and, again as necessary, field locating. Figure IV.1 shows the flow of field test cases among each of the distinct locating activities (subsequent to NCOA updates and excluding student prenotification), including (1) basic CATT locating efforts; (2) pre-CATI intensive central office (telephone) and field locating, as needed; and (3) post-CATI intensive telephone and field locating for cases not located in the basic CAT' operation. Figure W.1 also identifies the sequencing of telephone tracing and field tracing, differentiates pre-and post-CATI activities, and shows that cases entered CATI in three separate waves. The initial wave contained 1,162 cases not requiring pre-CATI locating; the secondary wave contained 254 cases located in pre-CATI trace. The third wave into CATI reflects \"reactivations\" of cases previously in CATI but identified for post-CATI trace and successfully located through those additional tracing procedures. While Figure P1.1 is useful in distinguishing case flow through several related operations, overall locating outcomes are not easily extracted. Consequently, the flow of locating outcomes is shown in Figure IV.2. (The first page of Figure 1V.2 is directed towards locating outcomes, while the second page of the figure is devoted to interview outcomes, among those located.) It should .be noted from Figure W.2 that during tracing operations, 20 cases were identified as \"exclusions\"; this classification included those who were: (a) out of the calling area;23 (b) deceased; (c) institutionalized or physically/mentally incapacitated and unable to respond to the survey; or (d) otherwise unavailable for the entire data collection period. Discounting these exclusions (locatir,7, was neither planned nor funded to reach such cases), Figure IV.2 shows that 1,288 were located and 138 were not, yielding a raw overall  locating rate of 90.3 percent. This rate is markedly greater than the comparable BPS:90V92 field test rate (84.8 percent), reflecting the better locating information obtained in the earlier follow-up. Raw locating rates were greater for those interviewed in BPS:90/92 (94.1 percent) than for those who were not (83.8 percent)' Further discounting the estimated 10 remaining non-FTBs in the uncontacted group shown in Figure IV.2, the estimated locating rate among applicable sample members is 91 percent. 2."}, {"section_title": "Interviewing and Eligibility Determination", "text": "For sample members who had not responded to BPS:90/92, FTB status had not been confirmed; eligibility was determined for this group in Section A of the interview.25 Among the 530 former nonrespondents (counting 12 exclusions), FTB status was determined for 387 in BPS:90/94. Of these 387, 44 (11A percent) were determined to be non-FTB.26 Table IV.1 shows second follow-up FTB rates, by type of NPSAS:90 school. FM rate varied with both institutional control and (more markedly) level of offering (directionally consistent with first follow-up findings) 27  "}, {"section_title": "953", "text": "NOTE: Statistics are based on BPS:90/92 non-respondents for whom eligibility status was determined in BPS:90/94. a Includes schools offering doctoral, first professional, and other graduate-level programs, as well as those that do not; no proprietary schools are included at this level. 24Alternately, about 60 percent of those not located were BPS:90/92 nonrespondents, while such nonrespondents comprised less than 37 percent of the working sample. 25A total of 236 non -FTBs (20.5 percent of the respondents) had been identified during BPS:90/92. 26The non-FTB rate among BPS:90/92 nonrespondents is considerably lower than that previously experienced among BPS:90/92 respondents; this reflects, among other things, the fact that \"reentering (older) students, who did not meet requirements for FTB determination, were easier to locate and interview during the first follow-up. 27 Initial BPS eligibility was determined from school records. FTB confirmation rate differences may reflect higher percentages of students who previously attended other PSE institutions among the \"first year\" students at different school types and/or differences over school types in information available concerning prior schools attended."}, {"section_title": "33", "text": ""}, {"section_title": "46", "text": "BEST COPY AVAILABLE As shown on the second page of Figure IV.2, a total of 1,208 sample members were fully (N = 1,084) or partially (N = 124) interviewed;28 this represents a 93.8 percent raw interview rate among those located, considerably exceeding the rate realized in BPS:90/92 (81.4 percent). Discounting the estimated remaining non-FTBs among the 2-time nonrespondents, the estimated conditional interviewing rate among applicable sample members was 94.2 percent. Of those located but not interviewed, more than two-thirds explicitly refused to participate in the study. Some of the remainder of those located but not interviewed can be attributable in part to the abbreviated field test data collection period; however, most represent implicit refusal cases (e.g., those with sequences of broken interview appointments; those using answering machines (or other people acting as \"gatekeepers\") to screen incoming calls). 3.\n"}, {"section_title": "Response Rate", "text": "Because interviewing rate has been reported conditional on locating, overall response rate (including both locating and interviewing) can be obtained as the product of the previously reported rates.29 The raw overall field test response rate was 84.7 percent; excluding estimated remaining non-FTBs, the estimated response rate for appropriate sample members is 85.7 percent. Comparable rates for the first follow-up were about 70 percent. A great deal of this improvement is attributable to the more deta led and current tracing information obtained from, and the rapport established with, B PS:90/92 respondents. About 83.8 percent of BPS:90/92 nonrespondents were contacted in the second follow-up; of the contacted former nonrespondents, full or partial interviews were obtained for about 79 percent (these rates are comparable to the o, erall rates obtained in the first follow-up). BPS:90/92 respondents, however, were contacted (94.1 percent) and interviewed when contacted (96.1 percent) at markedly higher rates. Other likely reasons for the improvement (presumably increasing rates that would otherwise have been achieved for both prior nonrespondents and respondents) include: better updated address and telephone number information resulting from the NCOA update; expanded but more focussed pre-CATI locating efforts;3\u00b0 addition of a field locating component; better success in refusal conversion; a shorter instrument; and a slightly longer data collection period."}, {"section_title": "B.", "text": ""}, {"section_title": "Details and Evaluation of CATI-External Locating Procedures", "text": "1."}, {"section_title": "Nature and Overall Results of CATI-External Tracing", "text": "The primary purpose of CATI-external locating was to supply CATI interviewers with current and accurate student directory information. CATI-external locating activities prior to and during CATI were designed to enable interviewers to spend as much time as possible 2sFor those identified as non -FTBs in the interview, the interview was terminated, and considered complete, as soon as they had completed that portion of Section A determining eligibility. Partial interview was defined as completing section A, which, by definition, is applicable only to confirmed FTBs. 29 Rates can also be determined directly from Figure IV.2, with appropriate adjustments of the base. 30 Cf., Burkheimer, el al., 1992 for operational differences. 34 47 contacting and interviewing respondents by reducing the necessary locating time. CATIexternal locating consisted of four distinct components: (a) the submission of all student and tracing source addresses and telephone numbers to an NCOA service; (b) the pre-CATI student mailout; (c) pre-CATI telephone and field' intensive tracing; and (d) post-CATI telephone and field intensive tracing. The results of these components are examined in two contexts: productivity (frequency of student directory information updates or confirmations obtained --CATI-external results); and effectiveness (relationships of obtained CATIexternal information to ultimate student contacts in CATI or other resolutions).3' Table IV.2 shows the overall efficiency in direct student contact of four CATIexternal locating approaches (including existing numbers in the BPS:90/92 locating database). In this table, locating efficiency is defined by the percent (of all contacted sample members) who were directly contacted at a telephone number provided or confirmed by that source. Since the phone number at which a sample member was contacted could have been confirmed by more than one source/activity, source attribution in this presentation is not unique.' Also, it should be emphasized that efficiency rates shown here are confined to direct contact of sample member at the number. If indirect contact (i.e., obtaining the contact number through a third party at another number provided by a listed source) were considered, the non-unique overall efficiency rates of locator file information would be much greater. NCOA confirmations/update efficiency rates would also increase markedly; however, rates for student mailouts and intensive tracing would show less increase. 31 CATI contact/resolution Indicates that the student was either successfully contacted in CATI or was determined to be an exclusion (e.g., out of the country, deceased, or unavailable during the survey period)."}, {"section_title": "32", "text": "For example, an extant phone number in the locator file could have been confirmed in NCOA, also confirmed by the sample member in the mail return, and reconfirmed in intensive trace (assuming CATI tracing efforts initially failed); in this example the contact number would be attributable to all sources shown in the table. Telephone numbers already available from BPS:90/92 provided the direct contact number for 40 percent of those located. Overall efficiency rates for the student mailing and intensive trace are reduced by the relatively low frequency of occurrence of such cases; relative efficiency rates for these approaches (i.e., those contacted at a source-provided phone number, expressed as a percentage of the total provided from that source) are considerably greater, as shown in subsequent sections. For example, only 107 sample members supplied confirmations/updates in response to the pre-CATI mailing; of those, however, 92 (86 percent) provided the contact number. Table IV.3 shows unique incremental contact efficiency as additional activities are considered, beyond the base of locator file information already available from previous data collections. 33 Of the 1,288 cases finally contacted in CATI, 57.5 percent were contacted directly at a telephone number provided by one of the four sources/activities considered previously. The bulk of these were from telephone numbers already existing in the BPS:90/92 locating files (some of which were subsequently confirmed in other activities). (Among the existing numbers that proved to be the contact number, sample members were contacted at the previously collected \"local\" telephone number more often than at their \"permanent\" number.) 1,288 100.0 NOTE: Effectiveness defined as direct contact with sample member at number provided uniquely by source/activity considered. The specific order in which sources are considered affects its unique incremental contribution; the order used in this table generally reflects the order in which activities/ sources were considered and/or available. a Percentages are based on the cumulative count of 1,288 located sample members. b Over 90 percent of this locating could be attributed to indirect contact with sample member through a number provided by one of the other sources (i.e., obtaining current number from a third party at a telephone number provided through CATI-external sources --including preloads); the remainder resulted from directory assistance calls by CATI interviewers. 33T he order in which sources are considered generally reflects the cc ler in which an activity was implemented."}, {"section_title": "49", "text": "The remaining 42.5 percent were contacted at new numbers obtained during CATI locating efforts (suggesting a highly mobile sample for this follow-up). Nearly all such new numbers were obtained from individuals contacted at numbers previously preloaded in the CATI system (including those with NCOA update/confirmation)."}, {"section_title": "Address/Telephone Updates and Mailing", "text": "In November of 1992, CATI-external locating activities began with the submission of the BPS:90/92 field test locator file, containing \"most recent\" previously collected locating information (names, addresses, and telephone numbers for student, parent, and other locator source), to a secure vendor for NCOA updates. NCOA services provide: (a) changes of address from a database of all mail forwarding information obtained by the U.S. Postal Service over the past two year period; (b) address standardization to postal regulations (to speed delivery); and (c) a flag, for each address, indicating potential undeliverability.34 NCOA can also provide telephone information, for as many names and addresses as possible, by matc1,g them to a telephone number file.' This search could produce a new telephone number, confirm an existing number, or yield no number at all. For the local and/or permanent addresses submitted to NCOA for all 1,446 working sample members, production rates were relatively small. Updated permanent addresses were provided for 133 (9.2 percent) of the sample members; updated local/current addresses were provided for 277 (19.2 percent) 36 Table IV.4 shows both production and relative effectiveness (in terms of CAT! contact or resolution) for NCOA telephone number updates. Production statistics for the NCOA operation are acceptable; at least one update (to current) local and/or permanent telephone number) was obtained for 12.1 percent of the sample. Confirmed current and/or permanent telephone numbers (and no updates) were provided for another 25.9 percent. One measure of relative effectiveness (namely, ultimate case resolution or contact during CATI)37 of the NCOA activities is shown in Table 1V.4. Among cases for whom NCOA provided at least one updated phone number, CATI contact/resolution was achieved for almost 95 percent, and the contact/resolution rate for those with only confirmed phone numbers was over 92 percent. These rates must be considered, however, in light of the almost 89 percent contact/resolution rate achieved for those receiving no updates or confirmations from NCOA. 34No explicit confirmation of old addresses is provided by the service. 35 Telephone numbers in the file come from a variety of sources, but primarily from Donne ley directory files. 36\". he difference in update rates,for permanent and current addresses is attributable to the fact that non-blank permanent addr:,,;si:s were available for only 57 percent of the cases. 37 CATI contact is defined as speaking with sample member directly or reaching a phone number identified by a third pai ty as sample members residence or place of work; resolution is defined as contact or identifying the sample member as deceased, incapacitated, institutionalized, or otherwise unavailable during the data collection period."}, {"section_title": "37", "text": "JO effectiveness (in terms of CATI contact or resolution) for NCOA telephone number updates. Production statistics for the NCOA operation are acceptable; at least one update (to current/ local and/or permanent telephone number) was obtained for 12.1 percent of the sample. Confirmed current and/or permanent telephone numbers (and no updates) were provided for another 25.9 percent. One measure of relative effectiveness (namely, ultimaW case resolution or contact during CATI)37 of the NCOA activities is shown in Table IV.4. Among cases for whom NCOA provided at least one updated phone number, CATI contact/resolution was achieved for almost 95 percent, and the contact/resolution rate for those with only confirmed phone numbers was over 92 percent. These rates.must be considered, however, in light of the almost 89 percent contact/resolution rate achieved for those receiving no updates or confirmations from NCOA. The effectiveness of NCOA operations can also be examined in terms of CATIexternal intensive tracing outcomes; these effectiveness measures are shown in Table IV.5. Because the criteria for initiating pre-CATI trace were independent of NCOA outcome for most cases, the differences in pre-CATI activation rates reflect factors other than the NCOA outcome (and are provided only for consistency). However, pre-CATI trace did use NCOA results as input; consequently, CATI contact/resolution rates do reflect (to some extent) the fact that .\"effective\" pre-CATI tracing was aided by NCOA outcomes. While these rates for NCOA \"confirm only\" cases activated in pre-CATI trace do not differ significantly (p k .30) Percent is based on activated post-CATI count for row under consideration; the abbreviated data collection period limited the types of tracing activities that could be applied in post -CATI trace. 37 CATI contact is defined as speaking with sample member directly or reaching a phone number identified by a third party as sample members residence or place of work; resolution is defined as contact or identifying the sample member as deceased, incapacitated, institutionalized, or otherwise unavailable during the data collection period. 38 5 1 si:bsampling for post-CATI field tracing precluded implementation of full intensive tracing p .ocedures (likely differentially applicable to the three NCOA outcome groups).38 Information packets (described in Chapter II, and included in Appendix B) were mailed to 1,434 sample members with sufficient address information in April 1993 notifying them of the survey and requesting updated directory information. Packets were sent to either sample member's current or permanent address, based on address completeness and recency (pre-CATI intensive tracing39 was \"more recent\" than NCOA updates/confirmations). When recency and completeness were equivalent, the posting was to the permanent address.' Some 51 mailings were sent to a parent address when neither student address was acceptable for mailing and the parent address was. The 121 packets that were returned as undeliverable with a forwarding address were remailed to the new address. Table IV.6 indicates the results of the initial mailing and the remailing, by the address to which it was posted. Response to the mailing was not particularly impressive, reflecting the experience in BPS:90/92. Only 107 sample members returned properly completed locator update sheets; five returned written refusals to participate; another five provided partial locating information with no updated or confirmed telephone numbers; and one parent returned a form indicating the sample member was out of the country. Ten percent of the packets were returned by the postal service as \"undeliverable\" with no forwarding address. The \"never returned\" group comprises over 80 percent of those posted. Established mailing priorities appear to have been appropriate since student return rate from permanent address mailings (11.1 percent) is more than twice (p .001) the rate from Outcome categories include final results from initial mailing as well as from 121 remailings to those returned \"undeliverable\" but containing a forwarding address. b Percent is based on column total, except as otherwise indicated for \"total\" row. Percent is based on row total. d Includes five refusals, five incomplete updates, and one identified out of the country. 3s completeness of the full set of post-CATI tracing activities and the likelihood of being selected for field trace were inversely related to the time in CATI before the case was identified as needing intensive trace. To the extent that NCOA provided information that lengthened the period of initial CATI tracing, it would also reduce the likelihood of the completeness of trace activities that could be applied, including field tracing."}, {"section_title": "39", "text": "Pre-CATI updates were considered to relate to local address unless otherwise specified by the student.\n"}, {"section_title": "44)", "text": "Permanent addresses were given priority since student mobility tends to quickly outdate local addresses."}, {"section_title": "52", "text": "FST COPY AVAILABLE local address mailings (4.8 percent). Although the parent mailing group is small, mail outcomes in that group more closely approximate those from the local address mailing than those from the permanent address mailing. As a field test methodological experiment, a motivational information sheet (the BPS Vanguard) was included in a half sample of the packets. To assess possible effects of this experimental treatment, mail returns from the Vanguard and non-Vanguard samples were compared; results, shown in Table W.7,41 indicate no positive effect of the motivational information sheet on the request for updated address and telephone information from sample members."}, {"section_title": "3.", "text": "Pre-and Post-CATI Intensive Tracing CATI-external intensive tracing (involving a central office telephone operation) had been used effectively in BPS:90/92 to locate cases that could not be readily located through the information available to CATI interviewers. These procedures were expanded in the BPS:90/94 field test to include a field tracing component; also both central office and field tracing were implemented prior to the start of CATI operations for sample members who had not been successfully contacted during the first follow-up. As indicated earlier in this chapter in Figure IV.1, the bulk of the cases required no CATI-external intensive tracing (1,033 cases contacted/resolved in normal CATI operations). Also shown is the increasing difficulty in locating through the various steps (e.g., of the 1,162 cases initially assigned directly to CATI, 1,033 (89 percent) were contacted/resolved; contact resolution among the 284 cases requiring pre-CATI trace was 75 percent (212 cases); and for the 156 cases requiring post-CATI trace, 63 (40 percent) were contacted/resolved).42 a."}, {"section_title": "Pre-And Post-CATI Central Office Tracing", "text": "Central office trace was based on information from: BPS:90/92 locator files (including tracing data previously provided by institutions), NCOA updates, responses to the prenotification mailing, interviewer comments and other information in the BPS:90/94 CATI record, and CBI and TRW credit bureau checks. A total of 284 cases in the BPS:90/94 field test working sample had not been successfully contacted during the first follow-up. These cases were selected for pre-CATI tracing which began in December of 1992. Post-CATI telephone tracing began after the start of CATI, as unlocatable cases were identified by the CATI interviewers. A total of 156 cases were identified as needing post-CATI tracing; however, nine of these were identified so late in the CAT! process that tracing was no3,/7. initiated. Consequently, only 147 cases were activated in the post-CATI telephone tracing. Production results for both pre-and post-CATI telephone tracing are provided in Table IV.8. Pre-CATI trace located telephone numbers for 206 (72.5 percent) of the 284 cases activated; however, one of these explicitly refused additional study participation. An additional 3 cases were resolved. The remaining 75 unresolved cases (including those determined to have no phones or unlisted phone numbers) were activated in pre-CATI field tracing. The post-CATI operation was relatively less successful than pre-CATI, resulting in new or confirmed student telephone numbers for 82 (55.8 percent) of the 147 activated cases:\" The remaining 65 cases (5 of whom were identified as having non-published telephone numbers) were activated in post-CATI field tracing. The notable difference between pre-and post-CATI telephone tracing production is mainly attributable to the abbreviated data collection period. Post-CATI intensive tracing Deotenseit ', , . ' Nine additional cases were identified for intensive tracing too late in the data collection period for tracing to be effectively begun. b One of these 82 cases called in to the toll-free CATI number while in CATI-external telephone trace. updates needed to be returned to the CATI interviewers as quickly as possible to allow them sufficient time to contact/resolve the cases before data collection ended. Therefore, an accelerated tracing procedure was designed to locate as many cases as quickly as possible; this precluded full use of some of the more time-intensive locating procedures. Table IV.9 shows CATI contact rates for cases located in pre-and post-CATI telephone intensive tracing, by locating activity providing the telephone number.44 The \"Other Locating Procedures\" category shown in the table includes cases which were located at parent/other NCOA telephone numbers, NCOA-confirmed telephone numbers, student mailout information (post-CATI only), student call -in to CATI prior to CATI-external locating, and various \"final resort\" locating procedures (e.g., calling tax-assessor offices). Of the 20545 located pre-CATI cases, 171 (83.4 percent) were contacted during CATI without subsequent intensive tracing. Fifty-four (65.9 percent) of those located during post-CATI telephone trace were contacted in CAM Within both pre-and post -CATI operations, variation is observed in the relative effectiveness of the different tracing sources (although small sample sizes preclude meaningful generalizations in many cases). The source of a telephone number is based on the final disposition of that case. b Count includes only cases which did not require subsequent post-CATI intensive tracing. Includes one case that called in to CATI before CATI-external trace was completed. Of particular note is that CBI and TRW credit bureau checks are quite effective in tracing (reflecting results from the first follow-up). CBI potentially provides two addresses (current and former), one telephone number, and an employer name; based on prior success of the method, it was one of the first locating steps. The low cost of CBI (less than $1.50 per case), the short time required to follow up on CBI data, and the high contact effectiveness suggest that CBI should continue to be one of the first locating steps attempted during the full-scale study. The substantially higher cost of a TRW report and the fact that TRW does not supply employer information or telephone numbers made TRW one of the later locating steps; however, due to its high contact efficiency, it remains a very useful source. The most commonly used (and relatively effective) source of student telephone numbers during pre-CATI tracing was the phone numbers attempted during the BPS:90/92 field test; however, this source may not be as effective in the full-scale study. An important first follow-up operational distinction was that the majority of the field test pre-CATI cases were not subjected to intensive tracing, while the majority of full-scale pre-CATI cases were. Related ly, however, 36 of the pre-CATI tracing cases had been unlocatable during BPS:90/92 intensive central office tracing. Original plans had been to activate these cases initially in field tracing; however, since new locating information may have been obtained in BPS:90/94 (e.g., NCOA updates), they were first activated in central office tracing instead. Of the 36, 18 were located by telephone, of whom 15 were ultimately contacted in CATI. These results suggest that improvements in central office intensive tracing methods justify further telephone intensive tracing attempts for previously unlocatable cases. b."}, {"section_title": "Pre-and Post-CATI Field Intensive Tracing", "text": "Field tracing was introduced in the BPS:90/94 field test for cases not located during central office intensive tracing; this new tracing component was needed for 75 cases during pre-CATI tracing and 65 during post-CATI tracing. Because of time and budget constraints, only 23 of the 65 post-CATI field trace cases were actually fielded.47 Where possible, field tracing cases were clustered into distinct geographical areas (i.e., four or more unlocatable sample members were last known to reside in the area) to reduce travel costs for field locating. Six such clusters were established for field tracing (four during pre-CATI and two more in post-CATI), including the metropolitan areas around Memphis, Dallas-Fort Worth, Harrisburg, Atlanta, Washington, D.C. and Colorado Springs. Even with this clusterinr the remaining cases (representing about 40 percent of the field test group)' remained widely dispersed. This had been anticipated during study planning stages and it had also been recognized that costs for field tracing of this group using contractor staff would have been prohibitive. Consequently, unclustered cases were traced by Equifax, a professional locating firm with over 900 offices throughout the country; also, to 47 To be eligible for field trace, cases had to first fail normal CATI locating and next fail telephone intensive tracing; only 23 such cases were identified in sufficient time to implement a reasonable field tracing operation and be returned to CATI within the abbreviated field test data collection period. 4sIn pre-CATI trace, 30 (40 percent) of 75 identified field trace cases could not be clustered; the unclustered rate for post-CATI field trace is not meaningful, since the group actually fielded (which was selected from all eligibles) was oversampled from existing clustered areas."}, {"section_title": "43", "text": ""}, {"section_title": "J6", "text": "provide meaningful comparisons between clustered and unclustered locating, some clusters were also assigned to Equifax.' As with telephone intensive tracing, the goal of field intensive tracing was to obta'n an updated/confirmed telephone number for the sample member without directly contacting him/her; however, if the sample member had no phone or an unlisted number that would not be revealed, then field locators were to provide him/her with the toll-free number for call-in to CATI. The field locators generally visited sites where the student had been known to reside or work, or where various tracing sources had been known to reside. Other possibilities for field intensive tracing included visiting various government agencies, private organizations, and utility companies. Since field locators had a large variety of intensive tracing options, most of which were location-or even case-specific, it was not feasible to give the field staff a specific set of ordered intensive tracing procedures to follow. Rather, they were given extensive training materials, copies of the mailed materials (see Appendix B) for sample members who did not receive them, various locating aids, the telephone intensive tracing materials for each case, and scripts and informational materials to help elicit the cooperation of sources and students. Contractor field staff were in regular (at least weekly) telephone contact with central staff and supervisors. Comparisons between the three different field locating situations are provided in Table IV.10. Results are based on the combined 75 pre -CATI cases and 23 post-CATI cases that were identified in sufficient time for application of reasonable field tracing activities. Comparisons provided in the table include production (i.e. field locating), effectiveness (CATI contact/resolution), and marginal (variable) unit costs. Relatively unconfounded comparisons can be made between contractor clustered tracing and Equifax clustered tracing or between clustered and unclustered Equifax tracing. All table results are somewhat tainted by the relatively poorer performance (due to time constraints) of the post-CATI operation. Generally, only minor differences (none of which even remotely approach a .05 level of statistical significance) exist among the three distinct tracing situations. Production rates are almost identical for all three conditions included within a range of less than 4 percentage points.' Effectiveness rates show slightly larger differences, but both nonconfounded differences are less than a standard error unit.51 It should be noted that rates for Equifax tracing are probably depressed relative to those for contractor tracing, since more steps (and time) were required in getting cases to and from Equifax and their field staff, reducing both the time available to trace the cases and the time to contact cases once returned to CAT!. This potentially confounding delay would be expected to affect the Equifax unclustered cases (typically handled by smaller, more remote Equifax field offices) more than the clustered \"To facilitate production comparisons, the per-case cost for Equifax locating was set approximately equal to the estimated contractor variable per-case costs. 50Pre-CATI production rates were higher in all conditions, and all were within a range from 68 to 70 percent. 51 Considering pre-CATI cases only, all rates were higher (by from 3.5 to 8 percentage points) but neither contrast exceeded a standard error unit. depressed (or inflated) due to time constraints on all post-CATI tracing. activities (because of additional steps --and time --involved in transferring cases to and from Equifax, effects of time limitations were greater on their statistics). a Production rate is based on located cases divided by total cases. b Effectiveness rate is based on cases resolved or contacted in CATI divided by total cases. cases, and may be reflected in the depressed unclustered effectiveness rate. Variable cost differences are also relatively small; however, the expected trend of higher costs for unclustered cases is observed. Since costs per located case and per contacted/resolved case are based (respectively) on the numerator used to compute the production and effectiveness rates, the previously discussed potential confounds in the Equifax rates also potentially inflate their costs. When other costs associated with field locating are considered, the costs per contractor-traced clustered cases (regardless of which unit cost is considered) are substantially higher than those of even the unclustered Equifax cases S2 Cost effectiveness suggests that full-scale study field locating be handled exclusively by Equifax. Equifax costs were further examined as a function of the nature of cases provided to them, within the specific locating cost-structure agreed upon with them. In Equifax field test tracing costs negotiations, it was assumed that providing them a confirmed address for an intensive tracing case would facilitate locating that case and obtaining an associated telephone number. Consequently, a cost structure was implemented to reflect this assumption. Cases failing central office tracing and assigned to Equifax were classified as: \"suspect address,\" \"confirmed address,\" or \"no telephone\"' (cases in the latter two categories included an address believed to be correct). If an address was not provided to Equifax, or if an incorrect address was provided, Equifax charged $350 per case for whom they provided a telephone number. Otherwise, Equifax charged $50 per hour worked on each case, regardless of whether a telephone number was obtained. Table IV.11 shows Equifax tracing effectiveness rates and costs for clustered and unclustered cases, broken down by address confirmation category.54 Within the clustered cases, CATI-contact resolution rates and costs are quite comparable for the two address classifications; this is much less the case for unclustered cases. While tabled differences between address available classifications within the unclustered locating group do not approach significance at the .05 level, the disparity is of some interest, particularly in light of other results. While the total Equifax pre-CATI unclustered group had highest production rate for obtaining telephone numbers (70 percent), the \"no address\" component of this unclustered group had the lowest effectiveness rate (38 percent). Thus, the obtained numbers for this group were less useful for CATI contact/resolution than those for other groups. Since the cost structure for this group was based on a located head count (i.e., production), there is some suggestion that Equifax field staff may have been somewhat NOTE: Because of the small number of post-CATI cases and the potential for confounded results resulting from post-CATI time constraints, results presented in this table are based exclusively on pre-CATI field tracing. Three \"no telephone\" cases (one of which was contacted in CATI at a per contacted case cost of $944) have been included in this category. 53 For these cases field tracing staff (both contractor and Equifax) were to determine a number at which the sample member could be reached at some specified time or to leave a number to which the sample member could call in for his/her interview. 54Due to the small number of post-CATI cases and the time constraints involved with those cases, only pre-CATI cases are considered here. Only three instances of \"no telephone\" cases existed (all in the unclustered group); therefore \"no telephone\" has been combined with the \"confirmed address\" group."}, {"section_title": "5;", "text": "WEST COPY AVAILABlE overanxious to accept a telephone number (probably provided by a third party) as being the correct number for contacting the sample member. This possibility has been discussed with Equifax staff, who have provided assurances of closer monitoring of their field staff in the full-scale study. Statistical evaluation of full-scale study results (which will be based on larger numbers of cases) should determine if this anomaly is more than chance variation. 4."}, {"section_title": "Recommendations for Full -Scale Study", "text": "During debriefing, telephone locators indicated that refusal of locating sources to provide addresses and telephone numbers led to both: (a) a reduction in locating rate (which increases the number of cases assigned to the more costly field locating), and (b) use of more time consuming and costly telephone intensive tracing procedures. Consequently, it is planned to focus more effort on refusal conversion in the full-scale study during both pre-and post-CATI telephone intensive tracing. Each telephone number previously attempted during CATI (but especially parents and other sources) will be approached as a potential refusal. While it is recognized that the majority of the telephone numbers previously attempted in CATI are not explicit or hidden refusals, an attempt will be made to identify potential refusals before they are contacted, and immediately assign them to a locator specifically trained to convert potential refusals. As a result of the field test experinient, Equifax has been determined to be the most cost effective approach to field locating. Because of the lack of significant differences between clustered and unclustered Equifax cases, there seems to be no need to implement clustering (which itself would contribute unneeded costs); however, the procedure involving classifying cases on the verity of their addresses will be maintained (and further evaluated at the conclusion of the full-scale study)."}, {"section_title": "Details and Evaluation of CATI Operations", "text": "Field test CATI data collection began on April 27, 1993. Interviewer training (involving a 4 day training session for each of three interviewing \"shifts\") was conducted during the prior week. Only 1,424 members of the working sample were activated in CATI; this excluded 22 cases who were not located during pre-CATI tracing. CATI data collection ended on July 25.55 About 50 interviewers were used in CATI data collection (this number decreased over time) in three shifts: weekdays, weeknights, and weekends. Times covered by this staffing were: 8:00 AM to 11:00 PM on weekdays, 9:00 AM to 5:00 PM on Saturdays, and 2:00 PM to 10:00 PM on Sundays. The field test data collection period was markedly abbreviated from that for the full-scale study; consequently, most results discussed should be taken as lower bound estimates of full-scale study results. 1."}, {"section_title": "Incidental CATI-Related Operations", "text": "In addition to standard CATI operations, some activities were undertaken for special subgroups of respondents or under unusual circumstances. A brief description and evaluation of these operations is provided below. Reliability Reinterviews. Data collection included a reliability reinterview for a subsample of those complelng the BPS:90/94 production interview. A total of 113 were selected to yield the targeted 100 cases agreeing to participate in a reinterview. Reinterviews were conducted between June 28 and July 17; 95 full reinterviews were completed during this time frame. Reliability estimates from these interviews are provided in Section V.B. Remailings. Sample members were asked at the start of the interview if they received the letter (and also the Vanguard, if applicable) included in the pre-CATI information packet. Although not included as an explicit question in the interview, those who did not receive the mailing could ask to have it sent to their current address. Those desiring such a mailing were asked for a current address; this information was written, together with the sample member's name, to an external file, which was accessed daily to print labels for the remailings. Those who did not receive the letter and did not request remailing were read the more relevant information from the letter. Only 34 students (less than three percent of those contacted) requested that the material be remailed. Of those, all but one (97 percent) were subsequently full or partial interview respondents.56 Call-in Capability. While the bulk of interviews was achieved by interviewers initiating contact with, sample members, some respondents called-in to a toll free number to be interviewed. Call-ins were actually solicited in mailed material (particularly in the Vanguard) and in messages left on sample members' answering machines. Also, field locators provided the call-in number to sample members who were located but who had no telephone (or would not reveal their unlisted number). The call-in phone was staffed by the shift supervisor; when a call came in, the supervisor would request the sample member's name and determine the appropriate case ID number from a master directory ordered alphabetically by name. The call and case ID would then be transferred to an available CATI interviewer. Final CATI records identified 38 sample members who called iii at some point during data collection; of those, 28 completed all or part of the BPS:90/94 interview. The reported call-in count is a lower bound of actual call-ins since the call-in variable was overwritten if subsequent attempts were made to reach the sample member (e.g., in case of a break -off) or in certain instances of supervisory case review. To achieve more accurate reporting capability in the full-scale study, a separate counter will be established to record the number of call-ins per case. Problem Sheets. Interviewers were instructed to communicate, on hard-copy problem sheets, generic or case-specific problems which they experienced. These sheets were 56In the BPS:90/92 field test, sample members were asked explicitly if they wanted a retrial] before continuing with the interview; 12.5 percent of those contacted requested a rernail, and 74 percent of those were subsequently interviewed. It was suspected that sample members were using this option as an implicit refusal; consequently, the procedure was changed for the BPS:90/92 full-scale study and carried forward to the BPS:90/94 field test. forwarded to project staff for review and action and, in general, were a very useful means of detecting program and data problems. A total of 410 problem sheets were completed; they were reviewed and acted upon on a daily basis. Most reported problems in the first four weeks of operations concerned a software bug in the CASES CATI package. A fix for this bug was provided by the program developers; the fix was tested and then implemented during subsequent CATI operations. A large number of other reported problems reflected only interviewer misunderstandings of proper procedures; however, some useful corrections to item wording and logic errors were noted on problem sheets. The remaining reports concerned items requiring correction of the CATI data. These included: delayed recall by the respondent of certain items (such as schools attended and employment spells); respondent initial misreport of information which was later clarified; and entry errors detected by the interviewer but not feasible to correct in CATI. 2."}, {"section_title": "Complete and Partial Responses", "text": "Contacting results for the total sample and interviewing rates for contacted eligibles are shown in Table IV.12.57 Among the 1,164 eligible sample members who were interviewed, 124 (10.7 percent) completed only partial interviews. Partial interviews resulted from sample members breaking off the interview after completing at least the first section.58 Such break-offs included explicit refusals to continue (in many cases after earlier refusals) as well as respondent requests to resume the interview at a later time followed by unsuccessful recontact (many of these latter cases probably represent implicit refusals). All but three of the break-offs occurred either in Section B, the education experiences section (105 break-offs, representing 9 percent of all respondents), or section C, the employment experiences section (16 break-offs). No break-offs occurred in the final three sections. Partial respondents and contacted non-respondents comprise 16.3 percent of the contacted eligible sample members; because of the pattern of early break-off, information available from partial respondents is only slightly greater than that for nonrespondents. Since typical partial respondents provide full information on only the earliest sections of the interview (one reason for the order of interview section administration), study objectives might be better served if these cases instead completed a small set of critical items. A possible consideration for the full-scale study is to ask such a set of those who refuse to participate in the interview otherwise. An abbreviated \"critical item\" interview should take less than five minutes to administer and should only be offered when all other attempts at refusal conversion have failed. Suggested items include: degree attainment information (what degree, when received and where); current educational enrollment status (whether or not attending school); and current employment status (whether or not working). "}, {"section_title": "Prior Response Status", "text": "A marked difference in contact and interview rates is evident based on first follow-up response status. First follow-up nonrespondents included thoSe who were not located in BPS:90/92, those for whom time ran out after contact but before interviewing, and those who explicitly refused to take part in the first follow-up interview. First follow-up nonrespondents differed from prior respondents in a number of ways; first, lack of response meant that available phone and address information was at least four years old and less reliable in many cases (making both CATI and CATI-external locating more difficult). BPS:90/92 explicit (and implicit) refusals had already shown resistance to cooperation. The BPS:90/94 interview was also somewhat different and longer (see Section IV.C.5) for prior nonrespondents (as well as for some prior partial respondents); eligibility determination was necessary for that group only, and retrospective data (for the 2-year period covered by the BPS:90/92 interview) were needed for important items (e.g., spells of education and employment, income, and education financing data). Table IV.12 (presented previously) shows contact and interview rates (among eligibles) by BPS:90/92 response status. Here, prior nonrespondents have been classified as prior explicit refusals and \"other.\" When compared to BPS:90/92 respondents, contact rates were significantly lower for prior refusals (p < .05) and for other prior nonrespondents (p < .001). Prior refusals, all of whom by definition were located in the first follow-up (but contributed no updated locating information), were easier to contact (p < .05) in BPS:90/92 than those in the other nonresponding group (most of whom had not been located in BPS:90/92)."}, {"section_title": "L1EST COPY AVAILABLE", "text": ""}, {"section_title": "63", "text": "Among those contacted (excluding identified ineligibles), interviewing rates also differ significantly. Once contacted, BPS:90/92 respondents were much more likely to cooperate again in the second follow-up than either of the prior nonresponding groups (p < .001 for prior refusals; p < .01 for other prior nonrespondents --including some implicit refusals). Contacted and BPS:90/92 explicit refusals were less likely to be interviewed (p < .05) than other first follow-up nonrespondents. While contacted and interviewed prior respondents and prior other nonrespondents demonstrated comparable partial interview rates, prior refusals showed a markedly higher rate (p < .05). The differences in contact and interview rates between first follow-up respondents and nonrespondents suggest that locating and subsequently gaining cooperation is much more difficult for those that have missed a data point in the longitudinal series. This suggests that BPS:90/92 nonrespondents be given specialized attention by interviewers specially trained and skilled both in locating and gaining cooperation from sample members. By distinguishing first follow-up respondents from nonrespondents, more experienced interviewers may be assigned to work the nonrespondents. Effort may also be concentrated early in CATI operations to send such cases as needed to CATI-external intensive locating in a timely fashion. This should allow for the greatest possible opportunity to locate these cases. These results also argue for excluding nonrespondents to both prior follow-ups from the BPS:90%96 field test working sample; to the extent that similar results are observed in the full-scale study (even after planned refinements in strategies), a similar argument would apply for the full-scale sample. Locating of certain cases will now be based on information that is over 6 years old. Collecting retrospective data over a 6-year period would impose an even more inflated response burden on those who were willing to participate, and the information from the first two years would be less reliable. L. is expected that yield from two-time nonrespondents would be considerably less than that experienced for one-time nonrespondents and, perhaps, inefficient relative to the level of effort that would be required."}, {"section_title": "b. Vanguard and Prenotification Mailing", "text": "In an attempt to increase cooperation among sample members and thus reduce longitudinal attrition, the BPS:90/94 field test investigated the effectiveness of an information sheet. Research suggests that sample members should be more willing to complete interviews when interview participation is viewed as a method for repaying or responding to a perceived favor or benefit or if they develop a sense of \"belonging\" to the study or importance because of it.59 The information sheet used, the Vanguard, was intended to evoke social reciprocity and group identification by: (1) emphasizing the importance of individual participation to achieve study goals; (2) expressing appreciation for past participation; (3) introducing the study team; and (4) providing concrete information about how interview responses would be 59Gro' yes, R. M., & Cialdini, R. B. (1991). Toward a useful theory of survey participation. Paper presented at the Second International Conference on Survey Nonresponse. 51 61 used. The Vanguard was designed with input from NCES staff so that it would not bias second follow -up survey results. As a methodological experiment, members of the field test sample were randomly assigned to two groups; one group was mailed the information sheet with their prenotification packet, and the other was not.6\u00b0 If the information sheet was effective, those who received the Vanguard would be more likely to cooperate than those who did not. There is certainly no reason to expect that the information sheet would affect locating rate, and, by coincidence, the same number of sample members (644) were located in each of the treatment groups. In the interview, sample members were asked if they received the mailing. If in the information sheet sample, they were asked if they received the Vanguard; if so, they were asked if they read it; and if so, if they found it informative. Percentage responses to these questions and cooperation rates within different groups is shown in Table IV.1?/. Of the 644 located sample members in the Vanguard group, only 28 percent reported actually receiving it. This statistic is somewhat deceiving since only 45 percent of the cases (in both groups) even reported receiving the mailing.' Still, only slightly more than 60 percent of those in the Vanguard sample, who reported receipt of the mailing, also reported receiving the information sheet. Of those who reported receiving the Vanguard, over two-thirds reported reading it; of those reporting reading it, 86 percent said it was informative. NOTE: Statistics are based on the 1,288 sample members who were contacted in CAT!. ' For the first three rows, the parent group on which the percentage is based is the first row; for subsequent rows, the parent group is those in the previous row. b Cooperation is defined as completing a full or partial interview. Cooperation rate was 93.2 percent among the combined groups of those not selected in the Vanguard sample (N=644) and those in the Vanguard sample who did not report receipt (N=465). The difference of 4 percentage points is statistically significant (z = 2.77; p < .005, one-tailed). 60Random allocation within each NPSAS:90 school was used to control for potential institution effects. The major question resolved in Table IV.13 is the effect of the Vanguard on cooperation by sample members, where cooperation is defined as either an eligible complete or partial interview or determined ineligibility for a non-FTB.' The only comparison in Table IV.13 that allows clear inference is that between the two groups created by random assignment. Cooperation rates are slightly (1 percentage point), but not significantly (p > .22, using a one tailed test),63 higher for the Vanguard group; consequently, no inference can be made that the information sheet had any positive effect on cooperation. A significant difference (z = 2.77, p < .005) does exist, however, when rate of cooperation among sample members who reported receiving the Vanguard (97.2 percent) is compared to that of the combined group of those not in the Vanguard group plus those who did not report receiving the Vanguard (93.2 percent). (Cooperation rate also increases further among those who reported reading the information sheet and among those who reported. finding it informative.) The inference from this finding is clouded, however, since causality cannot be inferred. While it is possible that receipt of the information sheet improves the likelihood of cooperation, it is also possible that those with higher propensity to cooperate are more likely to open and look through the BPS:90/94 prenotification packet. Cooperation rate was also examined in relation to self report of receiving the prenotification packet, regardless of whether the Vanguard was included, to consider the possibility that simply receiving a prenotification (pre--interview) mailing was, by itself, sufficient to motivate sample members to cooperate. Cooperation rates among sample members who reported receiving any pre-interview mailing are shown in Table 1V.14. Cooperation rates among those who reported receiving the pre-interview mailing were significantly higher than among those who did not, for the overall group (p < .005), for the Vanguard group (p < .01), and for the no Vanguard group (p < .05). Again, for the same reasons specified above for the self report of Vanguard receipt, directional causality cannot be safely assumed. Although the results of the information sheet analysis do not support continued use of the Vanguard, they do suggest that any preinterview mailing, as long as its receipt is recalled by the sample member, may be sufficient to improve the rate of interview participation. The prenotification mailings will, therefore, be continued, without the Vanguard information sheet. In addition to the possible positive impact on cooperation rates, the prenotification mailings serve a number of other purposes warranting their continued use, such as updating contact information, and disseminating required informed consent and confidentiality statements. "}, {"section_title": "Refusal Conversion", "text": "Efforts to gain cooperation from sample members during the field test included an extensive refusal conversion activity. Once a case initially refused to participate (or if the case was a BPS:90/92 final refusal), that case was placed in a special queue, to which only a refusal conversion team of experienced interviewers could gain access. If members of this 62This was the requirement for complete interview for those determined ineligible. 63 One-tailed tests were used throughout these analyses since directionality of difference was predicted. team also obtained refusals, the case was assigned to a subset of these interviewers who had previously demonstrated superlative effectiveness in refusal conversion. Table IV.15 presents results of the conversion effort among the 240 sample members who initially (and in some cases subsequently) refused to participate. The effort was quite successful; cooperation (defined as full or partial interview from eligibles or ineligibility determination) was ultimately obtained from nearly three-fourths of the initial refusals."}, {"section_title": "53", "text": "As indicated in Table IV.15, conversion efforts were somewhat more successful with those who had responded to the first follow-up. While none of the pairwise, within-column differences of percentages shown are statistically significant (typically, p < .15), results are NOTE: All statistics based on the 240 contacted cases who initially refused (at least one time) to participate in the BPS:90/94 field test interview; all percentages are based on row totals. a Includes identified non-FTBs as well as partial and complete interviews. b Includes 2 hostile refusals; one among prior respondents and one among prior other nonrespondents. c Includes principally those who could not be recontacted for interview after first contact (including preliminary refusals).\u00b0 Includes only partial and complete interviews."}, {"section_title": "54", "text": "6 ( directionally suggestive. Of particular interest is the fact that BPS:90/92 nonrespondents who had not explicitly refused (but were assumed to include implicit refusals) remained as \"other\" nonrespondents at over three times the rate of the other two BPS:90/92 final status categories. In the field test, all final refusals from BPS:90/92 were placed initially in the refusal queue, assuming that they might be more prone to refuse again in the current study. Interviewer feedback and supporting data suggest that while the prevalence of initial refusals is significant, final yields do not differ markedly from other BPS:90/92 nonrespondents. Consequently, for the full-scale study, first follow-up refusals will not be differentiated from other first follow-up nonrespondents; both will be placed in a special prior nonrespondent queue, however, so that they will be handled only by the most experienced interviewers."}, {"section_title": "d. Answering Machines", "text": "A factor affecting both contact and interview rates as well as the number of calls placed to sample members was the frequency with which answering machines were reached. Answering machines are becoming more and more commonplace, and it is suspected that a nontrivial percentage of those using answering machines use them to screen calls when they are at home. Approximately one-third of all results associated with initiated calls resulted in contact with an answering machine. For the first half of the data collection period, interviewers were instructed not to leave messages on answering machines. During the latter stages of data collection, they were instructed to leave a single message and schedule a callback appointment for three days later, thus giving the sample member a window in which to respond to the message. Identifiable call-in cases were, in fact, more prevalent (about 3 to 1) during the latter half of data collection; however, it is not possible with available data to attribute this increase to messages left on answering machines.\" It is expected that the increasing prevalence of answering machines will present an increasing obstacle in attempts to gain cooperation from sample members. Additional innovative \"answering machine conversion\" approaches will be needed, and in the full-scale study attempts will be made to collect additional operational data to better address the answering machine problem."}, {"section_title": "Quality Circles and Interviewer Debriefing", "text": "Regularly scheduled quality circle meetings with interviewers and supervisors were planned components of the field test operations and evaluation. Project technical staff and interviewers/supervisors met to discuss operational issues including: production obstacles; wording and structure of interview items; special screens (such as employment/enrollment history and on-line coding applications); quality control monitoring; CATI locating activities; gaining cooperation; accessing cases to work; and the overall interviewing environment. 641n addition to the fact that the call-in indicator could be overwritten through subsequent case access (which would be more likely in earlier stages of the study), other requests to sample members to call in (e.g., from field trace staff) were operative in the latter half of the data collection period."}, {"section_title": "68", "text": "These meetings were enthusiastically received by interviewers and supervisors alike. The frequency of the meetings (once every two weeks) allowed for implementation of suggested improvements to the instrument and procedures on an ongoing basis in order to test systems adequately prior to the full-scale data collection period. A final meeting, following completion of data collection, served as a final debriefing and a forum for additional recommendations for full-scale implementation. In addition to recommended wording revisions to clarify confusions and improve the presentation of the interview, other interviewer recommendations that are being considered for the full-scale study are: providing more historical and tracing source information in the CATI tracing module to facilitate CATI locating; moving, to earlier in the interview, questions confirming contacted person's identity, to eliminate time spent on those subsequently identified as the wrong person; including subject's name and phone number as a header on all screens, to facilitate personalization of the interview; displaying reference dates as headers or fills where appropriate (e.g., date last enrolled at given school when additional terms are collected); and improving the nature and consistency of special screens,65 to improve performance. Regarding the latter recommendation, interviewers expressed considerable frustration with the field of study and industry/occupation on-line coding applications (see Section V.D. for discussi ln of coding), and with differential requirements of some CASES \"multiple response\" screens. An improvement planned for the full-scale survey is to ensure, to the extent possible, that the various CASES multiple response screens are consistent with each other in both presentation and data entry requirements. Also, to the extent possible, CASES-external callable screens will be similarly standardized. Based on the field test success, weekly quality circle meetings will be implemented in full-scale operations so that areas of concern may be addressed in a timely fashion in improving the overall quality of interviewing. To increase frequency of meetings, while controlling costs and ensuring adequate staffing of the interviewing unit at all times, interviewers will be scheduled to attend these meetings on a rotating basis. Those attending a given meeting would act as liaisons for non-attenders. All interviewers would be given the chance to attend a meeting at least once every four weeks, perhaps more frequently if needed."}, {"section_title": "Effort Expended in CATI Locating and Interviewing", "text": "65Specui* I screens include routines external to CASES which are necessary to collect data in a fashion not possible in CASES, such as on-line coding applications and recording of enrollment history. They also include multi-question items collected in a single CASES screen. Examples include \"code all that apply\" screens and frequency of participation in school activities. The major variable expenses for CATI locating and interviewing involve interviewer time and toll charges, which are considered here. Shifts were staffed to jointly optimize contact likelihood and toll charges; however other influencing factors (e.g., answering machines and other gatekeepers) have been discussed elsewhere in this report. Details of interview administration time are provided in Section IV.C.6."}, {"section_title": "a. Interviewer Hours", "text": "A total of 3,723 interviewer hours (exclusive of training, supervision, monitoring, administration, and quality circle meetings) were expended to obtain interviews from field test sample members. This represents: 3.08 hours per interview (full or partial, eligible or ineligible); 3.20 hours per eligible full or partial interview; and 3.58 hours per eligible full interview. The latter two estimates are obviously inflated by the time required to locate and interview those subsequently determined to be ineligible. Had ineligibles not been included in the sample, estimates per eligible interview would have been closer to the first estimate. Since the time to actually administer the interview was approximately 40 minutes, it can be seen that the large majority of interviewer time was spent in other activities. A small percentage of this other time was required to bring up a case, review its history, and close the case (with appropriate reschedule, comment, and disposition entry) when completed. The bulk of the other time, however, was devoted to contacting (or attempting to contact) the sample members. Consequently, any attempts at marked improvement in interviewer efficiency must address greater efficiency in CATI locating (reflecting the considerable attention paid throughout this report on methods to improve locating efficiency)."}, {"section_title": "b. Number of Calls Made", "text": "As indicated previously, a large effort was devoted to locating, contacting, and recontacting sample members. Table IV.16 shows the number of calls made, in total and as a function of prior response status, for: all field test sample members; those contacted; and those interviewed. (Additional detailed call analyses are provided in Appendix C.) In all call analyses, the number of calls is inflated somewhat since all entries into a given case have been counted, including supervisory review which may or may not have resulted in an actual call being placed. Call-ins to cAn on the toll-free number are included in the analyses. The average number of calls made to all sample members (exclusive of those never worked in CATI)6\u00b0 was just over 20. One notable feature of Table IV.16 is that the average number of calls decreases monotonically from the full sample to that portion of the sample which was contacted to that portion of the sample that was interviewed. This holds for the total group as well as for both respondents and nonrespondents to the previous follow-up. This pattern of results presents an additional indication that the heaviest calling burden is in 66Twenty-two cases were never worked in CATI because they were unlocatable in pre-CATI locating. 1926 NOTE: The number of calls is inflated slightly since the variable used is number of times the case was accessed, which includes supervisory review with no call made; it should be noted that the sample defined for this table differs slightly from that defined in Table IV.12. a Excludes 22 sample members not located in pre-CATI trace and not activated in CATI. b Includes identified non-FTBs, partial interviews, and complete interviews."}, {"section_title": "57", "text": "Standard deviation. contacting the sample members (both before and after initial contact). While not presented in the table (but derivable therefrom), the mean number of calls to non-contacted cases was 25.84 and average calls to contacted but not interviewed cases was 48.44. The very large number of calls made to sample members who were contacted but not interviewed reflects a common calling pattern assumed to represent a form of implicit refusal. Specifically, the pattern involved one or more call sequences in which the sample member would schedule a future date for interviewing but then be unavailable for the scheduled cali-back and a number of subsequent calls (including calls resulting in contact with answering machines or others who appeared to act as gatekeepers). Average number of calls to those not contacted is smaller than was expected. This is attributable to the abbreviated data collection period and the fact that most of these cases were in CATI-external tracing during a substantial portion of that period. Table IV.16 also shows that more calls were required for nonrespondents to the first follow-up than for first follow-up respondents, regardless of the group considered. These differences are significant for the full CATI sample (t = 2.79, p < .01) and for those contacted (t = 2.39, p < .02), but not for those interviewed (t = 1.34). These results are, consistent with other differences reported between these two groups."}, {"section_title": "Timing of Interview", "text": "Time to administer the BPS:90/94 field test interview, both overall and by first followup response status, is shown in Table IV.17.. (Additional timing results, including breakdowns by NPSAS:90 institution level and control are provided in Appendix C.) The principal utility of this and other timing analyses is providing empirical data to estimate time to administer the full-scale instrument, and to identify where efficiencies might be gained with instrument modification."}, {"section_title": "BEST COPY AVAILABI", "text": ""}, {"section_title": "58", "text": "Section timing could only be computed for those completing the section in one interview session.67 Total interview timing was determined in two ways: (a) directly for those completing the entire interview in one session; and (b) summing the section administration time means. These methods yielded comparable results, an average administration time of slightly more than 39 minutes. This was almost 20 minutes less than for the BPS:90/92 field test and about 3 minutes less than for the BPS:90/92 full-scale study. The timing target for the second follow-up full-scale instrument is 35 minutes or less. BPS:90/92 nonrespondents took about 3 minutes longer to complete the interview than BPS:90/92 respondents. This difference had been expected to be larger, since nonrespondents to the previous interview had to provide information in many portions of the interview which prior respondents had provided in BPS:90/92. Two factors explain the smaller than expected difference: (a) retrospective data collection was also required for BPS:90/92 partial respondents who had not previously provided the information; and (b) prior non respondents were represented disproportionately from the less than 4-year schools (thus they typically had only two years worth of schooling to report while the prior respondents from 4-year schools also had additional schooling since the first follow-up to report)."}, {"section_title": "67", "text": "Although sufficient time stamps were available for timing analyses, completing a particular section in more than one interview session yielded timing data from different dates and times; procedures are being instituted to rectify this deficiency for the full-scale study so that all timing statistics can be computed for all respondents. 59 72 ''r ST COPY AVAILABLE The second of these factors is reflected in the section level averages. Section A (which included eligibility determination and base year data verifications for prior nonrespondents) took more than half a minute longer for prior nonrespondents; Section C (involving job information since last interviewed) took over 2 minutes longer; Section F (involving verification of base year financial aid information for prior nonrespondents) took over half a minute longer; and section G (involving personal and family finances since last interviewed) took over a half minute longer. These sections alone more than account for the overall difference. On the other hand, Section B (involving considerable detail on education since last interviewed) was administered in about the same amount of time to both groups, and no marked differences existed for other sections between the two groups. As suggested previously, administration time was also related to the level of offering of the NPSAS:90 institution from which the subject was sampled. Interviews averaged seven minutes longer for those sampled from institutions offering at least a 4-year program than those from two-to-three year schools. Those from less than two year schools averaged 3.5 minutes less than those from 2-to-3 year schools. The major contributions to these differences are from the education-related sections: Section B (education experiences); Section F (education financing); and Section H (graduate school plans/actions). To estimate administration time for the full-scale study from the field test results, it is necessary to adjust for factors: (a) on which the field test and full-scale sample are differentially distributed and (b) that have been shown to be related to timing and/or response rate. The estimates, the factors, and the adjustment procedure are shown in Table IV.18. The factors included in the adjustment were: (a) level and control of NPSAS:90 institution and (b) first follow-up response status.\" Field test response rates and interview administration time are reported separately for each of these groups; and a weighted average is obtained based on full-scale sample distributions on the adjustment variables. Despite the timing and prior response differences on the adjustment factors and the differential field test/full-scale distributions, the estimated administration time for the full-scale study, 38.9 minutes, differed very little from that experienced in the field test since the contributions from the two differential distributions tended to offset one another. The full-scale sample contains a higher percentage of prior respondents than the field test sample (82 percent compared to 63 percent), and the percentage of BPS:90/94 full-scale sample members from four-year NPSAS:90 institutions is higher than for the field test group. Note: Estimation is based on field test results of timing and response propensity within both NPSAS:90 school type and response status during first follow-up (R = prior respondent, NR = prior nonrespondent), as applied to percentage distributions of the full-scale sample on these variables."}, {"section_title": "60", "text": "Within a specific school type (row), i, full-scale minutes to complete is estimated as a weighted average of the field test minutes for the two prior response types, weighing by both the known full-scale distribution of prior response types and the estimated (from field test) new response propensity within the prior response. The weighted average can be expressed in terms of the table column labels, as follows: "}, {"section_title": "71", "text": "UST COPY AVAILABLL Table IV.19 shows estimated full-scale section administration time (computed in a completely analogous manner as total time). Recommended reductions in these times for the full-scale study (provided below) are based, in part, on these section timing estimates as well as additional timing results provided in Appendix C. Suggested time reductions also consider: recommendations of the BPS Technical Review Panel (TRP); reliability analyses (discussed in Section V.B); and thorough review of the field test instrument components. A number of minor changes to the instrument, specified below, as well as modifications to the software to achieve efficiencies (discussed at the end of this subsection) should reduce the respondent burden and at the same time provide a clearer, more efficient instrument. An estimated 0.3 minutes can be saved in Section A (introduction and eligibility determination) by eliminating now inapplicable Vanguard questions, and reducing wording. Approximately 1.1 minutes can be removed from Section B (education experiences) by (a) restricting the licensing exam question (B26A) to only actually taking an exam (i.e., excluding those planning to take such exams) and (b) changing response alternatives from a four point to a two point scale (satisfied or dissatisfied) for questions about satisfaction with various school features (B3bD and B3cA), as recommended by the TRP. An additional 0.8 minutes in savings could be realized in Section C (employment experiences) by: (a) substituting a two point, scale for satisfaction with aspects of most recent primary job (C68A); (b) eliminating infrequently mentioned options in certain \"code all that apply\" questions; and (c) eliminating the unreliable question about importance of life-style factors in determining lifelong work plans (C94A). In combination, Sections E (family and demographics), F (education financing), and G (financial information) could yield a total of 0.4 minutes through: (a) wording changes and (b) logic changes (F and G only) requiring only one refusal in sequences of financial questions. Only the screener portion of Section D (other education and training) was administered to the full sample; the remainder of the section was not administered to more than threefourths of the respondents (who had no such training). Wording reduction for the screener question and refinements to wording in the remaining questions are expected to yield 0.1 minutes from this section. Sections I (public service) and J (locator information) offer little room for time reduction other than minor wording refinements, estimated at 0.1 minutes for these two sections combined. Timesavings in Section H (graduate school plans) are limited by the fact that many respondents skip this section entirely. Considerable improvements or elimination of questions are possible, including: (a) collecting scores on graduate school exams only on the Graduate Record Examination (GRE);69 (b) collecting the GRE general exam component scores separately (to avoid confusion) and possibly providing a category for GRE subject matter tests; (c) eliminating questions (based on TRP recommendations and other considerations) that deal with the reasons for not applying to graduate school (H05), the reasons for applying to graduate school (H06), and the reasons for attending the current (most recent) graduate school (H09A). Because of the limited percentage of respondents to whom these changes would be applicable, net savings are estimated at only 0.1 minute. Recommendations indicated above, if implemented, are estimated to reduce the administration time of the full-scale interview to 36.2 minutes. It is anticipated that an additional two minute reduction can be achieved through adjustments to the system. Certain files commonly accessed during the interview will be stored in the Random Access Memory (RAM) portion of each interviewer's computer to elhainate swapping and 1/0 time, On-line coding of fields of study, industry, and occupation took more than three minutes on average for the field test interview (see Appendix C for timing results): however, improvements to the coding dictionaries (as a result of the field test) are expected to reduce this time. Other efficiency gain potential exists with some of the special screens."}, {"section_title": "7.", "text": ""}, {"section_title": "Summary of Recommended Changes", "text": "Refinement of the CATI instrument and associated procedures in preparation for the full-scale study as based on this evaluation should result in an improved CATI operation requiring less burden for respondents and facilitating interviewer operations. Some of the suggested changes discussed in Section IV.0 are summarized below. Expediting transfer of post-CATI tracing cases in and out of CATI. Special queues and interviewers for any CATI-external trace cases. Special queues and interviewers for BPS:90/92 nonrespondents."}, {"section_title": "More extensive editing of preload data.", "text": "A minimal questionnaire option for hard-core refusals. 69This was the only graduate/professional exam taken by BPS field test respondents with sufficient frequency to justify collecting score received."}, {"section_title": "77", "text": "Making special screens more consistent and operationally more efficient. Refinements, simplifications, and deletions of specified interview items."}, {"section_title": "V. EVALUATION OF DATA QUALITY", "text": "A."}, {"section_title": "Reliability of Prior Data", "text": "A small set of items in the BPS:90/94 field test instrument required verification and/or update of responses from either the BPS:90/92 first follow-up interview or the NPSAS:90 interview. These items included high school graduation status, names of preload schools attended, and preloaded jobs.\" Table V.1 shows the percent agreement for each of these data elements during the verification. The items on high school status were asked in interview Section A if not verified previously (principally of BPS:90/92 nonrespondents). Agreement for both data elements approached 100 percent. For these two items, results are comparable to those reported for BPS:90/92, even though two additional years had passed. In prior data collection efforts, sample members had indicated attendance at up to four schools (in addition to the NPSAS:90 school). Information about these schools was preloaded for each case. In Section B of the instrument, the respondent was asked to confirm attendance at these \"other\" schools (if any); more than 97 percent confirmed the preload information. Job information from the BPS:90/92 interview was also preloaded for up to two jobs held during 1991.71 More than 98 percent of those previously reporting such jobs confirmed holding the preloaded jobs. B."}, {"section_title": "Reliability Reinterviews", "text": "As in previous BPS data collections, the BPS:90/94 field test study included a reinterview study, described in Section IV.C, to evaluate temporal consistency of BPS interview responses. Each new reliability reinterview is designed to build on previous 70 Demographic information (e.g., gender, race, and ethnicity) had been verified/updated in BPS:90/92, but due to the demonstrated high reliability of these items, they were not verified in BPS:90/94; rather such items were only asked if missing for both prior surveys. 71 The purpose of this was to verify information from the first follow-up interview and to update end dates for jobs which were currently held at the time of the BPS:90/92 interview."}, {"section_title": "65", "text": ""}, {"section_title": "79", "text": ")Iv COPY AVAILABl E analyses by targeting revised or new items, and items not previously evaluated. This analysis, as in previous reinterviews, was designed to focus on two general sets of items: (1) new items not used in BPS:90/92 (including marital history, education and employment aspirations, and graduate school experiences); (2) previously used items that had not been evaluated through reinterview. Reinterview analyses generally focused on data items that were expected to be stable for the relatively short time period between the initial interview and the reinterview. Reinterview respondents were contacted four to eight weeks after completing the initial interview. Reinterview respondents were asked a subset of questions covering educational experiences, work experiences, family history, education finances, personal finances, and graduate school experiences. Analyses were based on the 95 respondents who completed reinterviews (or applicable subsets thereof). Effective sample sizes are presented for all results because analyses were restricted to cases with determinate responses to the applicable items on both interviews. Reliability analyses were implemented only for the appropriate subset of respondents and responses; for example, analyses of school-specific or job-specific responses were implemented only after checking to ensure respondents were reporting on the same school or the same job across the two interviews. Percent agreement and appropriate correlational statistics were used to estimate response stability. Percentages of agreeing responses were calculated in either of two ways: (1) for nominal and ordinal variables, agreement proportions were computed based on the number of responses that were exactly the same in both interviews; (2) for continuous variables, agreement proportions were computed based on the number of paired responses within one standard deviation unit of each other.72 Three relational statistics were used (where appropriate) as measures of temporal consistency: (1) Cramer's statistic for items with discrete, unordered response categories;73 (2) Kendall's Tau coefficient for items with discrete, ordered response categories;74 and (3) Pearson's product moment correlation coefficient for variables with continuous response categories.\nstandard descriptors associated with identified codes were displayed for the interviewer (sorted in descending order by likelihood of match); (3) the interviewer selected a listed standard descriptor.\" To evaluate the accuracy and effectiveness of the coding operations, all coding was subjected to 100 percent quality control (QC) coding. The re-coding also offered the opportunity to provide feedback to the interviewing staff about using the software more effectively!' Another result from the QC coding was obtaining information for NCES staff to use in refining the software since the BPS:90/94 field test was the first NCES project to use it. Weekly incremental coding logs were delivered to NCES to assist in updating the coding dictionaries, and QC coding was used to help assign appropriate codes for dictionary updates and to ensure that the verbatim text phrases were reflected accurately by the list of selected dictionary words. Results of the on-line IPEDS coding operation are shown in Table V.14. Institution coding had an overall discrepancy rate slightly less than 13 percent. A number of the errors were due to interviewers selecting inappropriate state/city/school combinations for the specified school by pressing the <ENTER> key too often and not confirming the selected school with the respondent.\" Another source of error was that interviewers in many cases did not supply sufficient text fields for the school name. About 96 percent of the cases were assigned an IPEDS code, comparing favorably to the 82 percent coded using a different approach in the BPS:90/92 full-scale survey. The discrepancy.rate for those cases which were coded on-line was 10.7 percent in the BPS:90/94 field test compared with 4.8 percent in the BPS:90/92 full-scale study. Weighted combination of coding discrepancies plus \"uncoded\" cases subsequently coded. 84If the interviewer was unable to find an appropriate code, the phrase could be identified as uncodeable. he software was new to the interviewers, and they experienced problems learning system features. Common problems were identified and reported to interviewers so that they could become more comfortable with the software. 86The nature of the table look-up approach is such that the first state that appears can be selected by pressing <ENTER> immediately; the same is true for city and school within city. Because of this form of interviewer error, the first listed dictionary element (the Gaither School of Hair Design in Albertville, Alabama) was coded a disproportionate number of times."}, {"section_title": "Education Experiences: Primary School Information", "text": "Reliability indices for primary school identification and satisfaction with specified features of school climate at the primary school are presented in Table V.2. Temporal consistency was high for the items on primary school identification, supporting the use of the \"primary school\" strategy in collecting general information about education experiences. Percentages of agreement for satisfaction with school climate features range from 52 to 70 72The percentage agreement approach for continuous variables can result in relatively high percent agreement and relatively low test-retest correlation when variance in main interview and reinterview responses is high relative to their covariance. 73 Cratner's statistic is a simple function of a chi-square statistic, normalized to vary between 0.0 and 1.0; for items with only two categorical outcomes, Cramer's statistic is equal to the Phi coefficient for 2x2 tables. 74 Kendall's Tau coefficient is a measure of consistency of ranks or other data with only ordinal properties (in simplest form reflecting the difference between the proportions of consistent responses and those of reversed responses). A Tau value near 0.0 indicates that consistent and reversed positions are equally likely, reflecting little predictability across the two interviews; a Tau of 1 represents perfect consistency. 66 0 percent and values of Kendall's Tau range from .43 to .64. The correlational measures are comparable in magnitude to results for satisfaction with school services that were reported for the first follow-up (these measures in BPS:90/92 ranged from .36 to .72). Based on the consistently low measures of temporal consistency and on suggestions from the BPS Technical Review Panel (TRP), two general revisions for items on rated satisfaction are suggested. Question wordings will be revised to emphasize and anchor the question time frame. This revision should enhance response consistency and response accuracy by reminding respondents that the item asks about attitudes across a well-defined time period. Additionally, the number of response categories could be reduced from four ratings of satisfaction to two: satisfied and dissatisfied. Research indicates that reducing the number of response categories can increase rating response accuracy when respondents have difficulty distinguishing between graded response options.75"}, {"section_title": "Some of the reliability indices in", "text": ""}, {"section_title": "8?", "text": "percentage agreement and a value of Cramer's statistic that is not significantly different from zero. To examine whether these low values could be attributed to the relatively fine distinctions within the \"primarily student\" and \"primarily employee\" response categories, a second set of consistency measures was computed for a variable collapsing responses into a dichotomous student or employee variable. Marked improvement was realized in percentage agreement for the recoded dichotomous variable; however, the value of Cramer's statistic increased only slightly (but to significant levels due to reduced degrees of freedom). Potentially fine distinctions among the full set of five response options appear to be a major factor in reducing exact agreement across repeated interviews but are not solely responsible for the low categorical relationship measure, which appears to be partly a distributional artifact coupled with a differentially high instability of the \"employee\" classification.76 Based on these results and discussions with members of the TRP, response options could focus on distinguishing respondents who are \"primarily students\" from respondents who are \"primarily employees.\" Overall agreement should increase as a result of revised response options. Improvement should also be achieved by refining question wording to more clearly define the relevant time period. The percent agree:.neat and relational measure are also somewhat disparate for the item concerning whether respondent planned to work in five years; over 95 percent of the reinterview respondents gave the same responses in the main interview (the bulk of these indicating they planned to be working full-time); however, the Cramer's statistic is relatively low for this item. The contribution to unreliability is primarily from the small number of respondents who indicated that they do not plan to be working or that they plan to work oply part-time. Respondents were more likely to report that they did not expect to be working in five years during the reinterview than during the main interview; and among the few cases giving inconsistent responses, most reported \"part-time\" working goals .during the main interview and shifted to the \"full-time\" response during the reinterview. (The unreliability of full-and parttime work distinction is also reflected in similar items about first and most recent job, as shown in Table V .4.) This finding also supports a basic unreliability among dual purpose items (i.e., asking in one item both whether one plans to be working in five years and, if so, whether full-or part-time), which research suggests are difficult for respondents to interpret and answer correctly. Because of the high percent agreement and the additional interview time that would be required if this item were transformed into two single-purpose items, no change in the item is recommended."}, {"section_title": "69", "text": "Confusing or vague question wording may be responsible for inconsistencies on these items. The \"used tools\" item asks respondents whether they agree with the statement \"I did not use tools /equipment/skills I was trained to use.\" The question was worded negatively to avoid sample members' establishing a response set; however, subjects have difficulty interpreting negative question wordings in some situations.' Reworking the question for the full-scale study is clearly indicated, and the revised question should be re-evaluated. The item on \"training for advancement\" asks, \"Was additional education or training required for advancement in your 1992 job with .... ?\" The question goal may be unclear, particularly to respondents who have been working for a while (e.g., whether or not past training/advancement or \"on the job training\" should be considered in selecting a response). Wording that more clearly defines the types of training respondents should consider and the types of advancements relevant to answering the item will be drafted for the full-scale instrument, and the revised item will be re-evaluated."}, {"section_title": "Work Experiences: Job Search Activities", "text": "Information about respondents' job search activities vas collected through an \"open ended\" question (e.g., interviewers read a general question on what types of activities individuals pursued in seeking their 1992 primary job but did not read the set of response options except as needed to prompt the respondents). Interviewers then categorized the \"free form\" responses and verified the categorization with the respondent. If the respondent did not mention a specific activity (or something that the interviewer classified as that activity), then a \"not pursued\" response was assumed; \"not pursued\" was the modal response for all activities in both the main and reliability interview. Here, different responses over time can reflect differential (situational) recall and interviewer interpretation as well as changes in question interpretation and response selection strategies on the part of the respondent. Agreement and reliability indices are presented in Table V  Complete agreement of responses in the two interviews was achieved for six of the activities; .in all of these instances this represented a \"not pursued\" assumption for all respondents in both administrations (the resulting lack of response variation precluded computation of a relational statistic). Two activities yielded \"all but one case\" agreement. For \"agency/professional recruiter\" all responses were assumed \"not pursued\" in one administration but one respondent mentioned the activity in the other administration (again Cramer's statistic was undefined). On both administrations, a few respondents mentioned that they used a campus-based job placement service; however, one of these failed to mention it on one of the administrations; because of the relatively small overall number of individuals mentioning the activity, this one disagreement reduced Cramer's statistic to .81. The joint frequency distributions for these items reflect high response variances within respondents and in some instances between respordents. Such increases might have been expected under the free form response format used here. If these items are maintained in the full-scale instrument, special training of interviewers is needed to evoke appropriate recall and to code responses consistently. Some options include: defining more clearly for interviewers 71"}, {"section_title": "85", "text": "PFST COPY AVAILABLE what should be included and what should be excluded from codes for each job search activity and clearer and more explicit definitions for the response options."}, {"section_title": "6.", "text": "Work Experiences: Satisfaction with Most Recent Primary Job Table V.7 contains reliability indices for reports of satisfaction with specified aspects of most recent primary job. Consistency measures of satisfaction with these job-related features were generally higher than those for satisfaction with principal school characteristics, and for most data elements considered here the indices are acceptable for satisfaction items. Higher consistency for job satisfaction may result, in part, from the greater recency of the most recent primary job (relative to their 1991-92 principal school). As with the items on school satisfaction discussed above, the job satisfaction items in the full-scale study questionnaire should clearly define and anchor the question time frames in order to improve reliability; however, by its nature satisfaction is expected to vary over time and improvements to reliability may not be realistic. The field test questionnaire items on job satisfaction used three response options: (1) satisfied, (2) neutral or no opinion, and (3) dissatisfied. For the full-scale study, TRP members recommend eliminating the \"neutral\" response option, since such an option is always available to the respondent by answering \"I don't know.\" 7."}, {"section_title": "Work Experiences: Factors in Employment Goals", "text": "Three items asked respondents to rate the importance of several factors in determining lifelong work; reliability indices for these attitudinal items are provided in Table V.8. With attitudinal items, response inconsistency may reflect real changes in attitudes across the two interviews. For general factors and incentive factors percent agreement measures are typically in the seventies and eighties, which is generally acceptable for these types of items; Kendall's Tau values suggest only marginally acceptable temporal stability. Among general and incentive factors the reliability indices for importance of \"previous experience\" and of \"educational opportunity\" are particularly low. Some of the unreliability in these sets of items are probably due to identified (by TRP members and project staff) complexities in some question wording. A prime example is the item about the importance of \"Good income or fringe benefits to start or within a few years.\" This item compounds income and benefits as well as time frame, probably creating a difficult frame of reference for some respondents. In the full-scale study, separate items for income and benefits are recommended, possibly concentrating on general importance of these two aspects of work, rather than introducing the compound time frame. Items about the importance of social factors are even less reliable (percent agreement generally in the sixties and reliability coefficients as low as .23). Based on this, the TRP recommended eliminating the full set or correcting particularly vague question wordings in this set of items."}, {"section_title": "8.", "text": "Marital History Table V.9 presents estimates of temporal consistency for item.; designed to gather detailed information on marital history. The reliability indices are generally high. The lower 73 8 7 BEST COPY AVAILABLE reliability of most recent marital status (1993) appears counterintuitive; however, it reflects the fact that 1989 status was, in most cases, a verification of previously collected data. While respondents provided very consistent reports of their marital status and changes in marital status, it is important to note that 87 percent of the reinterview respondents who answered the marital status items reported no changes in marital status since February, 1989. The, least reliable of these items was the number of marital status changeS since that time; however, among those few cases reporting status changes in both interviews, the date of the first status change was very reliable. The high reliability of historical marital status over a 4-year period reflects, at least in part, the unchanged nature of that status!' Nonetheless, findings are encouraging for continued use of the items (subsequent follow-up studies will only collect history from the last reported status). As the BPS sample ages, however, the likelihood of changing marital status will increase; thus, these items should be re-evaluated in a subsequent follow-up."}, {"section_title": "9.", "text": "Education Finances Table V.10 shows reliability indices for items concerning financial aid at the respondent-identified primary school for the 1991-92 academic year. The percent agreement measures are generally high (with one exception, ranging from 85 to 100 percent). The exception (80 percent agreement) involved receipt of \"other\" aid, which was also associated with a Cramer's statistic that did not differ significantly from zero. Most of the 20 applicable respondents answered, in both interviews, that they had not received \"other\" aid; however, the small number of cases reporting to have received such aid typically did so inconsistently. The Cramer's statistic magnifies inconsistency within small groups. 78Single-point-in-time marital status, collected in NPSAS:90 and BPS:90/92, was considered analytically inadequate. Obtaining full marital history (including base year) was recommended by the TRP for BPS:90/94 (and subsequent studies);. since such data had not been collected previously, the BPS:' I '4 historical period was 4 years.  A similar situation existed for the question asking if any aid was received from the school. Twenty (of 25) respondents consistently responded yes; 2 consistently responded no; and 3 were inconsistent in their response. The three inconsistencies, coupled with the low overall frequency of no responses, led to a Cramer's statistic of only .51 (which does not differ significantly from zero). Extremely skewed joint distributions were also evident for receipt of the two types of employer benefits. All but one of the applicable respondents indicated in both interviews that they had not received these forms of aid; the exception case reported receipt in one interview and non-receipt in the other. In both cases, restricted response variation precluded computation of Cramer's statistic. The reliability of total amount of aid received in a given year at a given school is quite high, but total loan burden over all postsecondary schools and years is less reliable (as might be expected). BPS:90/94 items collecting information on amounts of aid and loans initially asked for an open-ended response; if respondents were unable or unwilling to give an answer, then interviewers probed for categorical dollar ranges. Statistics were computed for the open-ended responses alone and for combined responses including recoded categorical responses. Results are stable across the two types of response formats. Results are also comparable with BPS:90/92 field test results, despite different frames of reference involved in the queries across the two follow-up studies. Taken together, results of the two studies provide convergent evidence of reasonable levels of temporal consistency for these items."}, {"section_title": "75", "text": ""}, {"section_title": "89", "text": "COPY AVAILABLE 10. Personal Finances Table V.11 presents reliability indices for the set of examined data elements related to personal finance. Indices are high for owning or renting a residence and, among those owning or renting, for amount of monthly payments. Indices for owning or leasing a vehicle are noticeably lower; however, among those consistently reporting having a vehicle, the monthly payment amount is quite reliable. Monthly expenditures for nonrecreational items is the least stable of the expenditure amounts (the reliability coefficient of .60 is borderline unacceptable), probably reflecting the facts that: (a) the inclusion/exclusion rules for nonrecreational items varies over time (and probably over interviewer), and (b) such expenditures also vary by month much more than home and car payments. Reported total income for 1992 also showed low correlations across the two administrations, even with high percent agreement (within one standard deviation unit). Obtaining reliable income data (a sensitive area for most respondents) has been consistently problematic. The previously described two-stage interviewing approach to collecting dollar amounts was also used with the income question. The degradation in the correlation over that for only the free-form response is due, in part, to the width of the income intervals used (interval midpoints were used in analyses where free-form responses were not available for one or the other interviews).\nREST COPY AVAILABLE personal income) took a bit under a minute for those giving determinate amounts, and less than 1.5 minutes for those with indeterminacies. Since there were at least two years of personal income data collected and up to four years worth if not available from BPS:90/92, the increased time burden for doing the conversion is not considered unreasonable. The results indicate that indeterminacy conversion was very successful with respect to initial DK responses, and it achieved the goal of reducing high rates experienced in BPS:90/92. It is recommended that this approach be continued for DK responses in the fullscale study. The appropriate amount approximations for each item will be refined based on a review of the field test frequency distributions. On the other hand, this approach does not provide a high enough conversion rate for initial refusals to justify its continued use. G."}, {"section_title": "Order Effects", "text": "The BPS:90/94 field test interview asked sample members for frequency ratings (i.e., never, once, several times, often) for each of several subitems pertaining to the respondent's 76 90 school-related activities while attending their defined principal school (question B.32).79 Frequently, responses to such \"ratings questions\" can be contaminated by changes in response propensities for individual subitems depending upon the order in which they are administered. These changes can result from general factors (such as the respondent becoming familiar with the response options or the tendency for responses to become less extreme over a sequence of rankings) or from factors more specific to the subitems themselves (such as changes in the interpretation of the meaning of a particular subitem within the context of previously administered subitems).8\u00b0T o control for order effects in the administration of question B.32, sequential subitems were presented to respondents in the field test with a random start point within the sequence. Prior to data collection, start points were allocated randomly, within specific NPSAS:90 school, to all sample members in an attempt to equalize the distribution of start points (both within and across schools) of those who responded. Within the eight subitems, the joint distributions of responses by different random start points were examined using the x2 test of independence (equivalent to a test of congruence of conditional distributions within each start-point group). A significant value of the x2 statistic indicates differential conditional distributions and suggests order effects. Analyses for the subitems were restricted to those respondents who provided a determinate response for all subitems in the set. Because multiple tests were to be performed, a significance level of .005 was adopted. Analytic results are shown in Table V.12. With four response options and eight different start points, the degrees of freedom for each x2 variable were 21. A total of 566 respondents contributed to the analysis, with approximately 71 cases per random start points' Expected frequencies less than 5 were observed in three analyses although, in all of these cases, the associated X2 statistic was not statistically significant. Since the effect of low expected frequencies is to artificially inflate the X2 statistic., these occurrences are not considered problematic. The x2 value departed significantly from chance expectations for only one of the activities. Other x2 values shown in Table V.12 were fairly tightly distributed about the expected value (equivalent to the degrees of freedom, 21) of the applicable x2 distribution, suggesting no major order effects. The one subitem showing a statistically significant order effect asked sample members how frequently they had informal or social contacts with an advisor or other faculty member outside of the classroom/office. The distribution of responses by order in which this subitem 79 Depending on enrollment patterns, some sample members were administered (either in addition to or instead of B.32) a comparable question (B.35) about the 1992-93 school year. If B.35 was asked instead of B.32, the B.35 responses have been included in these analyses; otherwise, they have not. so Both of these effects are more likely when questions are administered verbally than in visually administered questionnaires which allow respondents to view all subitems and response options in a single gestalt rather than receiving them sequentially. Respondents also typically change previous responses more frequently when the question and responses are written. was presented is shown in Table V.13. A markedly different distribution results when the subitem is presented first or second compared to its presentation at any other time. In fact, over 54 percent of the contribution to the x2 is attributable to those instances in which the subitem was administered first or second. These results suggest either: (1) respondents lacked a general frame of reference for rankings when starting, or (2) presenting the item first 23.3 NOTE: Percentages provided are conditional within each row; these percentages may not add to 100 within row due to rounding error. ' Represents the order in which this specific item was administered to sample member, given the random start point generated. b Over 28 percent of the contribution to the x2 value is attributable to entries in this row. PEST COPY AVAILAM E 78 92 or second failed to provide sample members with the frame of reference received when the item was presented in any other position. The more feasible explanation for this finding is the latter. When the subitem\"was presented early in the series, respondents did not have the frame of reference needed to answer the question. Without the benefit of two preceding subitems --which dealt with academic discussions with faculty outside of class and meetings with advisors about academi plans --contact outside of the classroom/office could have been taken to mean any kind of contact with a faculty member. When the subitem was presented third, however, respondents had the opportunity to discern the context of the question and could respond more appropriately. That is, the distribution of their ratings of social contacts with faculty became more consistent with the other subitems. Based on these results, results from the BPS:90/92 full-scale study,' and recommendations of the TRP, random start points will continue to be used for these items to account for possible order effects. However, the first three subitems shown in Table V.12 will be presented as one block of subitems and the fourth and fifth subitems as a second block; both blocks will have separate random starting points. It is anticipated that this approach will establish an appropriate frame of reference for respondent ratings while still controlling for order effects. D."}, {"section_title": "On-Line Coding", "text": "Computer-assisted on-line assignment of codes to literal responses was accomplished by interviewers in three substantive areas: school identification; field of study; and industry/occupation. Each coding operation was subjected to quality control (QC) review and recoding procedures. All computer-assisted interviewer coding utilized software developed by the Postsecondary Longitudinal Studies (PLS) branch of NCES to standardize computer assisted coding across studies and contractors. Institutional coding was needed to assign a six digit Integrated Postsecondary Education Data System (IPEDS) school identifier for new schools specified by the respondent.\" The system relied on a look-up table, or coding dictionary, of institutions constructed from the IPEDS institutional database. Other information in the dictionary (institutional sector combining level of offering and institutional control, and annual undergraduate tuition information) was retrieved into CATI for later use (e.g., for branching or as prompts for certain questions), once the school was identified and confirmed. Field of study coding and industry/occupation coding utilized a dictionary of word/code associations. The on-line operation for these coding operations consisted of three major steps: (1) the interviewer keyed the verbatim text provided by the respondent; (2) 82Sirrular order effects involving these three subitems were observed in the BPS:90/92 full-scale study; also other order effects were observed in this set of items. 83 Previously identified schools were coded prior to CATI as part of BPS:90/92 operations; codes and related institutional information for these schools were preloaded into the CATI data file."}, {"section_title": "94", "text": "In general, the lower percentage of uncodeable schools indicates that the new system facilitated on-line assignment of school codes. The higher discrepancy rate relative to BPS:90/92 full-scale operations is attributable in large part to a lack of familiarity with the software and to a lesser extent, to interviewer carelessness. BPS:90/94 full-scale training will highlight some of the commonly made coding mistakes as well as provide ample time for hands-on practice with the IPEDS coding software. These measures should reduce the discrepancy rate while maintaining a high percentage of cases to which codes are assigned. Tables V.15 through V.18 contain results of the other on-line coding operations. Overall coding discrepancy rates ranged from 25 percent for industry coding to 43 percent for occupation coding. The high discrepancy rates reflect the difficulty in achieving consistency in coding as part of an on-line interviewer operation. The interviewers were asked to focus primarily on collecting and keying the best possible text for the given coding application. The code assignment was secondary to the responsibility of getting a sufficient and accurate verbatim text response so that post-hoc coding could be implemented according to whatever rules were desired and so that enough information was available for improving the coding  Weighted combination of coding discrepancies plus \" uncoded\" cases subsequently coded."}, {"section_title": "95", "text": "REST COPY AVAILABLE Percent is of total to be coded. Percent is of previous row. Weighted combination of coding discrepancies plus \"uncoded\" cases subsequently coded. dictionaries. Unfortunately, this also made the on-line coding operation more challenging since the likelihood of having words not found in the coding dictionary was greater with longer, more detailed text responses. The number of possible codes also made the task difficult. In particular, there were well over 100 possible codes for field of study coding. The field of study, industry, and occupation dictionaries were not updated until after field test data collection ended. As a result, the interviewers did not benefit from any of the refinement. On average, on-line coding tasks consumed nearly ten percent of each interview. Appendix C contains timing information for the on-line coding operation. The amount of time spent coding and the discrepancy rates are indicative of the difficulty of the on-line coding task for the interviewers. The dictionaries have been updated substantially and the software has been refined to simplify the complex task of assigning these codes. Interviewers will be more experienced with the coding applications when full-scale data collection begins. It is expected that the combination of updated dictionaries, greater familiarity/comfort of the interviewers with the software, and continued feedback through QC coding will lead to more efficient and more consistent coding."}, {"section_title": "E.", "text": "Up-coding \"Other, Specify\" Items Eleven items were administered in the field test interview that included, in addition to the fixed response options, an \"other\" option for which the respondent could subsequently specify the nature of this other. Generally, the \"other, specify\" format was restricted to either: (1) new items for which the determination of specified response alternatives needed evaluation, (2) items for which data from NPSAS:90 or BPS:90/92 were already in that form, or (3) items for which this additional information was considered useful for subsequent classification or coding. For new items, explicit response option frequencies were also reviewed toward improving the questions for the full-scale study. Choice of \"other\" options by respondents usually results from: (1) actual incompleteness of the existing fixed response options in covering a unique situation, or (2) misunderstanding by the respondent and/or interviewer of either the question or how a situation can be subsumed under an existing fixed response option. In the latter case, \"other\" is implicitly an inappropriate choice, and data can be corrected through a post hoc modification of the main item response based on the verbatim information specified (if any) and other related data items (if any), a process typically called \"up-coding\". To ensure data quality, this manual operation was performed on 946 occurrences of \"other\" responses to one of the eleven involved questions. Table V.19 contains a summary of the up-coding results. Most of the items specified in the table require little discussion; such items generally (1) are applicable only to a small subset of respondents, (2) represent a small percent of those for whom they are applicable, or (3) can be up-coded in a large percentage of occurrences. While the last six of these items were used for the first time in BPS:90/94, none yielded sufficient frequencies of unique \"other\" responses to justify additional fixed response options, and none suggested systematic interviewer error that could be addressed in training. Results for the licensing exam question suggest that this item should be modified to include the most frequently occurring exams, while continuing to provide an \"other, specify\" option. No up-coding was possible for the licensing exam responses, but a number of responses occurred with sufficient frequency to be considered fixed response options. These included: the National Teacher Exam; nursing exams; and communications (FCC) licenses. A category for \"other medical profession licenses\" would also subsume about 20 percent of the specified other resporses. Nearly half of those who indicated that they took a job not related to their field of study gave an \"other\" reason as to why they did this. About two thirds of these other responses could be up-coded to a listed response option, suggesting a lack of clarity of the available options to the interviewers (who were to code free form responses to the question). Among up-coded cases, the large majority (185 of 201) dealt with financial needs.87 Commonly occurring reasons (which were not up-coded) included summer or part-time jobs, and flexible or convenient hours. This item should undergo careful revision to clarify Analyses based on all cases indicating \"other\" as response to item, regardless of whether subsequently specified. a Percent based on total cases for whom item was applicable. b Percent based on number of \"other\" responses. response alternatives and interviewer training should focus on some of the commonly occurring \"other\" responses that are up-codeable. Responses to the question \"what was done to find the [principal] job?\", a new item in BPS:90/94, elicited the greatest absolute frequency of \"other\" responses. Only slightly more than half of these were up-coded, most of which reflected interviewer error in appropriately classifying into existing categories the free form response requested by this question. Other possible new categorieS for the full-scale survey might include: (1) internship/co-op/ apprenticeship experience leading to the job; and (2) asking company about openings. Since interviewers had considerable difficulty with the question, it will be reworked to facilitate the interviewer choice selection task, and interviewer training will focus on how certain \"other\" responses to this question can be appropriately classified. One other item, asking the nature of any student loan deferment, required post-hoc coding. This question was completely open-ended, with no specified response options; it was posed this way in order to develop fixed response options for the full-scale study. There were 151 text responses for this item. The 151 text responses suggest that five response categories would subsume the bulk of the answers: (1) still enrolled in school or returning to school; (2) grace period for loan has not expired; (3) unemployment; (4) financial or medical hardship; and (5) approved service activity (e.g., Peace Corps or teaching in depressed geographic area). It is, however, recommended that an \"other, specify\" response be provided for this item."}, {"section_title": "F.", "text": ""}, {"section_title": "Indeterminate Responses", "text": "Allowances were made in the CATI program to accommodate (both as a fixed response alternative and by special keyed entry) responses of \" Don't Know\" (DK) and \"Refusal\" (RE) to any question. Such responses represent indeterminacies in the data set and must be resolved by imputation or subsequently dealt with during analyses; consequently, they need to be reduced where possible. Refusal responses generally occur for items considered sensitive by the respondent, but DK responses result from a number of potential sources; these include (1) question wording not being understood by the respondent (and lack of explanation by the interviewer), (2) hesitancy on the part of the respondent to provide \"best guess\" responses (and insufficient prompting from the interviewer), (3) the answer being truly unknown by, or inappropriate for, the respondent, and 4an implicit refusal to answer the question. A summary of DK and RE responses for the BPS:90/94 field test, by interview section, is provided in Table V.20. Statistics are provided for both the number and percentage of items in each section in which any RE or DK response was given, and for maximum item-level DK and RE counts and rates for respondents, within each section. Respondent-based rates are based on only those sample members for whom each item was applicable and asked; as such, maximum counts and maximum rates do not necessarily apply to the same item' Item refusal was quite low except in Sections F, G, and J. Education financing information was collected in Section F, which included sensitive questions concerning loan amounts, and amount of respondent contribution of personal savings to education financing. Section G collected respondent, parent and household income, as well as mortgage, rent, and expense amounts. Each of these specified items in Section G had refusal rates over two percent. Refusal rates were high in Section J, as well, where locator information was collected to facilitate re-contact efforts in the BPS:90/96 third follow-up study. Respondents, in many cases, were unwilling to provide contect information for parents and others. DK responses occurred in less than 10 percent of the possible items. With the exception of Sections F and G, very few items had more than 10 DK responses. Section F questions with high DK occurrences included amount of financial aid received at a particular school for a particular year (up to four years ago), loan amount and date questions, and dependency information. Section G questions eliciting high DK rates were income items for respondent, parents, and household. As mentioned above, these also elicited high refusal , rates. 88As an example, if 60 of 3,000 applicable sample members refused to answer one question and 6 of 12 applicable sample members refused to answer another question, the maximum count of refusals would be 60 while the maximum rate would be 50 percent. 85 99 00 00   NOTE: Statistics are based on sample members with full or partial interviews or appropriate subsets for whom specific sections/questions were applicable and reached. Indeterminacies in Sections F and G are prior to conversion. (See Table V.22 for indeterminacy conversion results.) Includes all possible repeats of questions and responses within user exits. b Percent is based on number of items in section. Percent is based on number of cases for whom question was applicable (i.e., reaching the point in the interview, not legitimately skipped, and not determined \"not applicable\"). It should be noted that, under this definition, the maximum percentage reported may not correspond to the same question as maximum count reported. Maximum percent is based on items with a single applicable respondent, who refused. Maximum percent is based on two items with a single applicable respondent, who reported not knowing answer. Maximum percent is based on an item with but four applicable respondents, two of whom reported not knowing answer."}, {"section_title": "BEST COPY AVAILABI I", "text": "The only items in section A eliciting high DK rates were the questions asking whether the respondent received the informational packet which was mailed (A02C and AO3A), and the month that the respondent first attended a postsecondary institution (A08F).89 Section B had two items with high DK frequency. Thirty respondents (nearly 3 percent) answered DK to item B26A, whether they took or planned to take any professional licensing/certification exams. The plan to take portion of this item will be removed from the full-scale instrument, which should minimize this indeterminacy. Additionally, highest level of education expected (B40A) elicited 5 percent DK responses. Whether the respondent expected to be working in five years (C90A), another long-term planning question, elicited higher than usual DK frequency (17), though the rate was less than 2 percent. Sections D and E had very few DK responses. In Section H, there were 28 DK responses to the GRE score question. There are two suspected causes for this: the item only allowed for one value for score, whereas there are three components to the GRE general test; and, many respondents may in fact not have received the results of their exam at the time of the interview. TRP members have recommended that the three component scores of the GRE be collected separately to reduce item confusion. Three percent of those answering item HO2A, whether applied or plan to apply within the year to graduate/professional school, did not know. This reflects a level of respondent uncertainty similar to other items asking for expected plans. Other items in Section H with relatively high DK rates were those asking whether or not the respondent was accepted or received aid to attend a particular graduate school. Respondents may not have received notification from the specified graduate schools at the time of the interview. Section I had no item receiving more than one DK response. There were four items in Section J eliciting more than ten DK responses, none of which were unexpected or require any modification to the instrument. Items include whether the current address will be the same in two years and the relationship code for the specified contact person. Items asking the respondent to specify income and other money amounts have traditionally yielded high indeterminacy rates compared with other items in the BPS surveys. In the BPS:90/94 field test instrument, these questions included: financial aid received for a given year at a particular school; total loan burden and amount owed; and respondents' own, parents', and household income. The field test instrument included questions which routed DK and RE initial responses through a series of screens seeking closer and closer gross estimates for the financial questions. Table V.21 presents DK and refusal data for personal, parent, and household income questions in Section G. The indeterminacy rates for these items in the BPS:90/94 field test are less than half of those for the BPS:90/92 full-scale where DK/RE conversion was attempted.9\u00b0 The indeterminacy rates are higher for parent income than either household or 89This item was asked as part of a series of questions to determine BPS eligibility for nonrespondents of the BPS:90/92 survey. 90Indetenninazy conversion was not attempted for respondent's earned income. The indeterminacy rate for this item was lower than in BPS:90/92 full-scale but the improvement was not as great as that for gross income. NOTE: Statistics are based on the sample members for whom items were reached and were applicable. a Rates reflect conversions of original indeterminate responses through the series of \"estimation\" screens associated with each item. Resultant converted total indeterminacy rates are typically less than half those experienced for the same set of questions in the BPS:90/92 full-scale survey."}, {"section_title": "102", "text": "b The earned income question was incorporated into the CATI instrument after production interviewing began; consequently, the question was not available for those completing the interview prior to incorporation. Also, the earned income question was not asked if reported gross income for the year was zero. Finally, indeterminacy conversion was not attempted for the earned income question, which explains the higher indeterminacy rates than those for gross income. Rates are deflated, since data from the BPS:90/92 field test were preloaded into these variables if they were available and determinate. d For a given year, either parent income or household income was collected, depending on sample member's dependency status during that year. For years prior to 1991, data were not collected if determinate responses had been obtained in the BPS:90/92 field test. personal income. Respondents are less likely to know their parents' income than their own, and to a lesser extent, that of their household. Indeterminacy conversion was attempted for six different financial amount items, as specified in Table V.22.9' Conversion rates for each of these items neared or exceeded 50 91 An item was not considered converted if the respondent provided a first level approximation but then provided a DK or RE response to a request for closer approximation. For instance, if the respondent indicated a gross income (in item GOaC) of \"more than $30,000\" but refused to provide any further approximation, this was not a conversion. Yet if the same respondent (in item GOaG) had said that the amount was between $40,001 and $50,000, this was considered a satisfactory conversion. If a respondent answered DK/RE to a given open-ended amount question and again indicated  NOTE: Statistics are based on items in Sections F and G in which respondents initially answered \" Don't Know\" or refused to answer the given amount question. Items for which some additional, but not complete, information was collected were not included in the Converted total. a Information collected for each school in each applicable academic year (from 0 to 9 responses per respondent). b Information collected for each applicable calendar year (up to 4 possible responses per respondent). Based on dependency status, either Parent or Household Income (but not both) was requested for any applicable year. percent, and the total success rate was 54 percent. Nearly two-thirds of the initial DK responses were converted whereas only eight percent of the initial refusals were converted successfully. DK conversion was successful more than half of the time for all items, and as high as 80 percent successful for amount of loan still owed by the respondent. Refusal conversion yields were quite low in all categories, with the highest rate being 11 percent for parents' gross income. Three item types yielded no refusal conversions. Feedback from interviewers suggests that refusal conversion is counterproductive in some cases. They indicated that many respondents became increasingly annoyed that they had to refuse to answer a given amount question twice because of the conversion attempt. Naturally, it took longer to administer the successive approximation questions for DK/RE responses than the open-ended amount questions with determinate responses. The block of respondent personal income questions (up to four years' worth of gross/earned DK/RE to the conversion attempt, conversion attempts were terminated for that question."}, {"section_title": "Quality Control Monitoring", "text": "Monitoring telephone data collection in progress serves a number of goals. These goals are: to provide information about the overall level of error in the facility, to ensure that the interviewing process remains in statistical control (and to reduce overall error to acceptable limits); to obtain information on the interview process that can be used in improving study design; and to improve interviewer performance by reinforcing good interviewer behavior and discouraging poor behavior. The RTI telephone monitoring system was used for interviewer monitoring in the BPS:90194 field test. The system provides for random sampling of interviewers and interview items during CATI operations. Monitors listened to sampled interviews and observed the data collection using remote monitoring telephone and computer equipment and software. They recorded their findings on laptop computers which contained computerized monitoring forms. Monitors listened to up to twenty questions in the sampled ongoing interview, and for each question, recorded whether the interviewer: changed the wording of the question (with a distinction of major wording change, minor change, and no change); probed the respondent correctly; and provided appropriate levels and kinds of feedback to the respondent. Each of these measures was quantified, and monitoring results for the field test operation were produced periodically. In addition to periodic facility-level reports, the system generated interviewer reports with the intent that interviewers could see how their results compared with the facility results. Changes in wording proved not to be useful for monitoring interviewer performance accurately. At the same time, though, question administration is a crucial data collection activity and appropriate monitoring is warranted. A suggested change to the monitoring system for the BPS:90/94 full-scale data collection is to define types of questions in the instrument (e.g., questions in which all response options must be read regardless; and questions in which response options are riad only if respondent needs prompting) and to develop rules associated with each question type. Monitoring would entail identifying whether or not the given question was administered according to the rules for that question type. This would replace the monitoring of question wording changes. It is also suggested that data entry errors be monitored in the BPS:90/94 full-scale study. Despite the expectation of under-reporting of entry error because the next screen is often painted before the keyed response is visible, it would be useful to monitor performance in this area based on those errors which were detected. It is recommended that feedback and probing continue to be monitored in the full-scale study. As part of field test operations, interviewers were given individual monitoring reports along with the facility level report. It is felt that individual level reports raise undue concern on the part of the interviewers in terms of performance evaluation. Eliminating the individual reports will focus attention appropriately on the facility statistical control status, and the entire telephone survey unit will be able to work together in reducing the sources of error. Daily reporting of facility-level monitoring results will allow for careful tracking of the statistical control of the CATI unit. The changes proposed for the full-scale study are expected to provide more information for evaluating the statistical quality of the interviewing operation and enable mechanisms to improve the quality over the course of the data collection period. H."}, {"section_title": "Changes Recommended to Improve Data Quality", "text": "The results of the various evaluations of the BPS:90/94 field test data quality justify some procedural and substantive actions for the full-scale study. The reliability reinterview design offered an extremely valuable mechanism to assess portions of the BPS instrument. Suggested changes that grew out of this evaluation include: anchoring certain questions to reference dates for clarification, as applicable; reduction in the degree of satisfaction scale to two options (satisfied, dissatisfied); specific clarification of question wordings to enhance the likelihood of consistent, reliable responses; and, elimination of items concerning social factors in determining lifelong work goals. The presence of an order effect in the series of questions pertaining to the respondent's activities while attending the specified principal school implies that the use of a random starting point for these questions should continue in the full-scale study. With the aim of minimizing the potential order effect further, multiple questions in the series may be treated as a unit when random start points are determined. BPS project staff and TRP members suggested a number of instrument revisions based in part on the evaluation of up-coding of \"other, specify\" text. The item concerning licensing exams may be restricted to those having taken any such exams, rather than including expectations. The reason for taking a job not related to one's field of study might be limited to those having attained a postsecondary degree, diploma or certificate (which was the original intent of this item). Response option modifications and clarifications for a number of items may also facilitate improvements in data quality and efficiency in data collection. In the graduate school experiences section, a number of items are suggested for elimination: reasons for not applying to graduate school; reasons for applying to graduate school; and reasons for enrolling at a particular school. Another recommendation is to restrict dollar amount indeterminacy conversion attempts to respondents answering \" Don't Know\" to those questions. While this approach was quite successful in reducing indeterminacies for such cases in the field test, refusal responses were converted much less frequently. The evaluation of the BPS:90/94 field test survey allows the opportunity for positive adjustments to the design and implementation of the study. The full-scale survey will reap the benefits of this evaluation. Given the longitudinal nature of the BPS series, later studies should also receive direct benefit from the BPS:90/94 field test survey. The National Center for Education Statistics (NCES) in the U.S. Department of Education has a mandate to provide Congress and other policymakers with information about the condition of education in the United States. This includes data on access to and persistence in postsecondary education, as well as information about experiences in the workforce and how these experiences are related to education. The Beginning Postsecondary Students (BPS) Longitudinal Study, in which you have been a continuing participant, is one of the major surveys used to provide this information. NCES is now preparing to conduct the second BPS followup to further examine these issues, and has authorized Research Triangle Institute and Abt Associates Inc. to conduct the study. I am writing to thank you for your previous support and to encourage your continuing participation. As the name Beginning Postsecondary Students suggests, individuals were selected for participation in BPS when they first enrolled in postsecondary education. Only a limited number of individuals were selected. Therefore, you and each of the others selected represent hundreds of similar students who first entered a college or vocational school at the same time you did. You have provided a lot of information to us in the past, and we greatly appreciate this. Now we need to ask you a few more questions about what has happened since we last contacted you. Only you, as a past respondent, can help us with these questions. The answers to these questions will help to inform national policy and to assure that the U.S. Department of Education responds to the needs of all students in its efforts to improve the quality of education in our country. BPS is authorized by law [20 U.S.C. 1221e-1 and PL 100-297, Sections 300(i) and 300(k)], and is subject to strict non-disclosure provisions. NCES and its contractors adhere to strict confidentiality standards in protecting the privacy of individuals involved in our studies. Stringent measures are in place to safeguard the confidentiality of participants during the collection, analysis, and reporting of all survey data Data are used only for statistical purposes. The identity of individuals will not be disclosed, and no individual data will be reported. We sincerely appreciate your c,00peratiow in helping us with this important study. Your contribution is of unique importance, and we recognize your personal efforts in taking the time to tell us about your experiences. If you have any questions about the study, please contact Terry Blake, toll free, at As you may remember, BPS was designed to identify students when they first began their postsecondary education (hence the name), regardless of age or length of time out of high school, and to collect information concerning educational and employment experiences at selected intervals after entry. The data will be used to provide information to policymakers and others concerning educational participation and progress, program completion, and workforce experiences in an increasingly competitive environment. In the previous interviews, you gave us information on how to contact you so that you could continue to take part in this important study. We are now gathering current telephone and address data to prepare for this round of BPS. Please take the time to verify, correct, or update the information on the enclosed update sheet, especially if what we have is not correct, or if you plan to move during the period we will be conducting the survey. Please return it to Abt Associates in the postage-paid envelope provided. You have contributed important information in the past, and your participation has made a valuable contribution to the NCES Postsecondary Longitudinal Studies Program. You and thousands of others have taken part in this program over time, and we sincerely hope you will continue to do so. The information that has been provided through BPS serves as a vital resource for educators and policymakers as they address the challenges and debate about the quality of education, the effect of that education on the lives of Americans, and the most productive way to support participation in postsecondary education. NCES is mandated by Federal Law [20 U.S.C. 1221e-1] to conduct the Beginning Postsecondary Students Longitudinal Study. BPS collects data about the education and employment experiences of people who have continued their schooling after high school. Only a limited number of researchers will be authorized by NCES to access information that may identify individuals. They ma: use the data only for statistical purposes and are subject to fines and imprisonment for disclosure or misuse. Data will be combined to produce statistical reports for Congress and others. No individual data will be reported. Your participation in BPS is strictly voluntary. However, we need your help collecting these data, as you were selected to represent thousands of others like yourself. Your responses are necessary to make the results of this study accurate and useful. An interviewer from RTI will call to conduct a telephone interview with you sometime during the period February through August of this year. During the interview you will be asked questions about such things as your education, the school(s) you attended or are attending, your employment experiences both while in school and after, how you financed your education, and your goals and aspirations. RTI staff normally controls interviewer quality by monitoring interviewer behavior during parts of some interviews. The interview is estimated to average about 30 minutes, including the time for reviewing instructions, searching existing data sources, gathering and maintaining the data needed, and completing and reviewing the collection of information. You may send comments regarding this estimate of the time necessary to complete the interview or any other aspect of this collection of information, including suggestions for reducing the burden of response, to the U. Enclosed you will find a letter from the Commissioner of the National Center for Education Statistics, and a leaflet with a brief description of BPS as well as greater detail about the confidentiality procedures which will be in place. You will also find an address correction update sheet and postagepaid return envelope. If you would like more information about BPS, please call Terry Blake at the following toll-free number: 1-800-886-4993. We thank you for your past participation and look forward to your continuing help in this exciting and important study. Thank you for your cooperation and participation. This information is strictly confidential. Pleas (' return "}, {"section_title": "19.79", "text": "Note: Statistics based on the 1,288 cases contacted in CATI; number of calls is inflated slightly due to supervisory review of cases. Abbreviations used are: N=Number; MEN=Minimum; MAX=Maximum; AVG=Mean; SD=Standard Deviation. Includes schools offering doctoral, first professional, and other graduate-level programs, as well as those that do not; no proprietary schools included at this level. C-2 1 2 3     Note: Statistics based on the 1,208 cases interviewed in CATI (including identified non-FTBs, partial interviews and complete interviews); number of calls is inflated slightly due to supervisory review of cases. Abbreviations used are: N=Number, MIN=Minimum; MAX= Maximum; AVG=Mean; SD=Standard Deviation. Includes schools offering doctoral, first professional, and other graduate-level programs, as well as those that do not; no proprietary schools included at this level."}, {"section_title": "C-3", "text": "124 \"-ST COPY AVAILABLE These estimates are based on those who completed the interview in one session. These estimates are based on the sum of the section completion averages for each section. Included in section completion averages are those that completed a given section (but not necessarily the entire interview) in one session. Includes schools offering doctoral, first professional, and other graduate-level programs, as well as those that do not; no proprietary schools included at this level."}, {"section_title": "C-4", "text": "RPTF COPY AVAILABLE             4.,,,,,,,w,kv4Aw.....,,,,,,,6*.,,,. , . ; ,,z.,,       '' .04 so' 441 -Q. 4 : , , . Let me summarize the letter. In the spring of 1989 you participated in a study about student financial aid, and you were also selected for the first follow-up in the spring of 1991. The National Center for Education Statistics is mandated by Federal law (20 U.S.C. 1221e-1) to conduct this study. The study collects data about the education and employment experiences of people who have continued their schooling after high school. These data will be used [r]only [n] for statistical reporting. Only a limited number of researchers will be authorized to have access to information which could be used to identify individuals By law, they may only use the data for statistical purposes.  [n] (Since this study will determine how student participation in higher education can be better supported and encouraged, your continued participation will be extremely helpful to future students and others who are interested in improving postsecondary education.) The interview takes, on average, slightly more than 30 minutes. Because you provided information before, some questions will be based on your previous responses. Neither your participation in this study nor any answers you provide will affect any benefits you are receiving or expect to receive. You may decline to answer any question and may stop at any time. As we said in the letter, you first participated in this study in the spring of 1989, and were selected for participation in the follow-ups at that time. The National Center for Education Statistics is mandated by Federal law (20 U.S.C. 1221e-1) to conduct this study, which collects data about education and employment experiences of people who have continued their schooling after high school. These data will be used [r]only [n] for statistical reporting. Only a limited number of researchers will be authorized to have access to information which could be used to identify individuals. By law, they may only use the data for statistical purposes. The interview takes, on average, slightly more than 30 minutes. Because you provided information before, some questions will be based on your previous responses.  [n] Your participation in the study has been, and continues to be, voluntary and neither your participation in this study nor any answers you provide will affect any benefits you are receiving or expect to receive. You may decline to answer any question and may stop at any time. Since this study will determine how student participation in higher education can be better supported and encouraged, your continued participation will be extremely helpful to future students and others who are interested in improving postsecondary education. The next few questions are about your educational experiences since we last spoke with you. We are interested in all schools you attended for credit toward a certificate, license, diploma, or other formal award -- [r]but not correspondence courses [n], and the terms (time periods of registration or for which you pay separately) in which you were enrolled at those schools. We are interested in all terms you were enrolled, even if you did not complete the term. W A R N I N G: ONCE YOU SAY THESE ARE ALL THE SCHOOLS TO BE ADDED, YOU WILL BE \"LOCKED OUT\" OF ADDING ANY MORE SCHOOLS; BUT YOU WILL STILL HAVE AN OPPORTUNITY TO DELETE LATER. [n] <1> NO MORE NEW SCHOOLS TO BE ADDED <2> NEED TO ADD SCHOOL(S) ===> The next few questions are about the terms when you were enrolled, since we last spoke (,in those schools you just identified). (Generally, terms are identified by when you register for new classes and/or when you pay additional tuition/fees.) [r]INTERVIEWER: READ THE FOLLOWING TO R. ONLY IF HE/SHE SEEMS TO HAVE TROUBLE WITH WHAT \"TERMS\" MEANS [n] \"Terms\" means different things at different postsecondary schools and colleges, depending on the calendar system used. Some colleges are on a quarter system or semester, trimester, 4-4-1, etc. which defines terms. These schools may also have one or more summer sessions, which are additional terms. Other schools have specific fixed-length courses of instruction that may start at different times during the year and that may or may not be broken into smaller units. In this case, the entire course of study may be a term. For each additional term at this school, we need to know the start and end dates (month and year), the number of courses you took, D-22 [fill \"your school level(e.g., sophomore, grad student),\" IF LEVEL-01] and whether the school considered you to be enrolled Full time, Less than full time, but at least half time, or Less than half time. [r] "}, {"section_title": "D-33", "text": "The only school you reported attending as an undergraduate between 1 July 1992 and the present was [fill NAME OF APPLICABLE SCHOOL]. Do you consider this school to be a principal school in your undergraduate postsecondary education? [r]NOTE THAT DK(-8) AND RE(-7) ARE NOT ACCEPTABLE. [n] <1> YES <2> NO [IF B31A=NO,SET 92 1) \"Talk with faculty about academic matters outside of class time\" 2) \"Meet with advisor concerning academic plans\" 3) \"Have informal or social contacts with advisor or other faculty members outside of classrooms/office\" 4) \"Participate in study groups with other students outside of the classroom\" 5) \"Participate in 1 or more student assistance centers/programs(eg,counseling, skills,minority serv.,health)\" 6) \"Go places with friends from school (e.g.,concerts, movies, restaurants, sporting events)\" 7) \"Participate in school clubs (e.g., student government, religious clubs, service activities)\" 8) \"Attend academic or career-related lectures, conventions, or field trips with friends\"] Roughly, how often [r]per year [n]  * 1 = Respondent DID use service/activity/facility 2 = Respondent did NOT use service/activity/facility ** 1 = very dissatisfied 2 = somewhat dissatisfied 3 = somewhat satisfied 4 = very satisfied Use arrow keys & TAB to move to desired field, then type response. Press F2 for help. Press F10 when done. The next few questions are about any jobs you may have held since we last spoke with you (regardless of when the job was started). If you left a job and sometime later went back to the same job, count that as two jobs for our purposes. We want you to consider [r]any job you [n] [r]held for pay [n], including summer jobs, work-study jobs, apprenticeships, and co-ops.  :.sba:s:,. ..k.\\,.; . ..,4...\\ -,i,-,\\::;:\"..!..:.*:.....  'S DATA FOR FOLLOWING VARIABLES: Cy3B,Cy3E,Cy3L,Cy3M,Cy3N,Cy30,Cy3P,Cy3Q,Cy3R,Cy3S,Cy3T,Cy3U,Cy3V,Cy3W,Cy3X,Cy3Y,Cy3Z (Cy3 BLOCK CONCERNS WHAT THINGS DONE TO GET JOB); ALSO, IF JOB NOT NEW (OCCUPATION AND EMPLOYER SAME AS EARLIER PRINCIPAL JOB), STORE PRIOR JOB'S DATA FOR: CylA, Cy1C, Cy1Q, Cy1R, Cy IS, Cy 1T, Cy2A, Cy2C, Cy4C (THE REST OF THE PRINCIPAL JOB BLOCK DATA), AND GO TO NEXT PRINCIPAL JOB (NEXT CyOA) UNTIL  [r]INTERVIEWER: READ EACH LINE AND \"TOGGLE\" TO \"YES\" IF IT APPLIES [n] [r]CODE ALL THAT APPLY: [n] <1> 1 was able to apply most of what I learned in school <2>1 he job was different from my education/training <3> I did not use tools/equipment/skills I was trained to use <4> I could have gotten this job without my training/education [r] [r]since February [fill \"1989\" IF NO BPS:92 JOB INFORMATION, ELSE \"1991 [n], that you worked for [fill CURRENT YEAR PRIMARY JOB EMPLOYER NAME], did you participate in any education/training program [r]provided [n] by them (other than routine on-the-job training)? (\"Employer-provided education/training includes not only training [r]conducted by the employer [n] but also training [r]paid for [n] by the employer (such as tuition for courses at a local school); or [r]arranged by the employer [n] (such as bringing in an outside agent, union, or private training firm; or sending employee for a course conducted at some other location).) [ r] [r]any [n] of these education/training programs at [fill CURRENT YEAR PRIMARY JOB EMPLOYER NAME] one of th.. following types? [ r]INTERVIEWER: READ OPTIONS AND \"TOGGLE\" TO \"YES\" IF APPLICABLE. [n] [r]CODE ALL THAT APPLY: [n] <1> formal registered apprenticeship, sponsored by the state or a labor union <2> employer provided job training during working hours on premises <3> payment for job-related education/training at an educational institution or training center during working hours <4> payment for job-related education/training at an educational institution or training center outside of working hours <99> NONE OR [r]ALL DONE [n] [GOTO C6bA] ===> [IF C6aC A= 99 in the following areas (that I will read to you)? [r]INTERVIEWER: READ HIGHLIGHTED RESPONSE OPTION AND ENTER APPROPRIATE RESPONSE. TREAT \"DK\" AS \"NO OPINION.\" [n] a. Pay and fringe benefits b. Opportunity for promotion c. Job security and permanence d. Opportunity for further education e. Importance of work 1. Difficulty and challenge of work g. Overall satisfaction with job <1> SATISFIED <2> NEUTRAL OR NO OPINION <3> DISSATISFIED <-7> REFUSE [r] Were you either (a) holding a job but on temporary layoff, or (b) holding a lob and waiting to report to work during this [r]attire [n] [n] seeking work during [r]any [n]  Five years from now (spring of 1998), do you intend to be working [r]for pay [n], either full-time or part-time? If you are not sure of the answer, please give your best estimate."}, {"section_title": "Yes", "text": ""}, {"section_title": "No", "text": "[r]INTERVIEWER: BE SURE TO DETERMINE FULL-OR PART-TIME PLANS. [  In determining the kind of work you plan to be doing for most of your life, how important are each of the following [r]general [n] factors? Please answer: not important, somewhat important, or very important. [ r] In determining the kind of work you plan to be doing for most of your life, how important are each of the following [r]incentive [n] factors? Please answer: not important, somewhat important, or very important. [r]INTERVIEWER: READ EACH FACTOR AS TT IS HIGHLIGHTED, AND ENTER APPROPRIATE RESPONSE. ENTER 9'? TO EXIT SCREEN. [n] a. Good income or fringe benefits to start or within a few years b. lob security and permanence c. Opportunity for promotion d. Opportunity for further education and/or training <I> NOT IMPORTANT <2> SOMEWHAT IMPORTANT <3> VERY IMPORTANT <-7> REFUSE [r] In determining the kind of work you plan to be doing for most of your life, how important are each of the following [r]lifestyle [n] factors? Please answer: not important, somewhat important, or very important. [r] The next few questions are about your participation in education programs other than the ones we have already discussed. Other than formal postsecondary education for credit, education and training provided by your employer, and military training, we would like to find out about your participation in any other educational programs. Such programs may include registered apprenticeships, government (local, state, or federal) training programs, personal enrichment, correspondence courses, newspaper,ra'lio, or television courses, [r]non-creditinj courses/activities at a regular school or college and courses from private companies or individuals. We are interested in all subject matter areas: from wordprocessing to ceramics and from automotive repair to aerobic exercise. The next few questions we will be asking about \"student financial aid\" that you may have received for your postsecondary education and [r]related [n] expenses. [r]The kinds of aid we mean are grants, scholarships, work-study, assistantships, fellowships, student loans (that is loans to you), and/or assistance with education costs by your (or your parents') employer or by the military. [n] This aid [r]does not [n] include loans, gifts or other financial assistance from family members, relatives, or friends; also it [r]does not [n] include loans that were made to parents or others, even if it was for your education. Did you receive any \"student financial aid\", as we have defined it since 1 July 19[fill EITHER 89 (ALL 4 YRS REQUIRED) OR 91] for your [r]undergraduate [n]  We also need to clarify just a few responses, about aid, that you gave during your 1991 interview. ENTER <1> TO CONTINUE ==> [goto FO8A] [REPEAT BLOCK FOR 16 SCHOOL/YEAR COMBINATIONS AS BUILT IN FO7B. Fxx=F08/F11/F14/F17/F20/F23/F26/F29/F32/F35/F38/F41/F44/F'47/F50/F53 Fyy=F09/F12/F15/F18/F21/F24/F27/F30/F33/F36/F39/ F42JF'45 [r]INTERVIEWER: READ EACH OPTION AND \"TOGGLE\" TO \"YES\" IF RECEIVED. [n] [r]CODE ALL THAT APPLY: [n] <1> grant(s) <2> scholarship(s) <3> student loan(s), excluding loan(s) from family or friends or loans to parents <4> tuition benefits or other education assistance (prepaid) from employer or union <5> tuition benefits or other education assistance (reimbursed) from employer or union <6> other financial aid received [r] [n] student financial aid you received (i.e., awarded and accepted) as an undergraduate student at"}, {"section_title": "D-71", "text": "For the entire period from 1 July [fill START YEAR OF ACADEMIC YEAR] through 30 June [fill END YEAR OF ACADEMIC YEAR], (over all schools you may have attended,) did you use any of these other sources to help pay for your [r]undergraduate [n] education or [r]related [n] living expenses? [r]INTERVIEWER: READ ALL RESPONSE ONIONS THE FIRST TIME YOU REACH THIS SCREEN; AFTER, READ ONLY IF NEEDED AS PROMPTS. \"TOGGLE\" ANSWERS TO \"YES\", IF APPLICABLE. NOTE PARENTHETICAL DEFINITION IN STEMS. [n] [r]CODE ALL THAT APPLY: [n]  Was the amount of these loans more than $10,000, less than $10,000, or right about $10,000? [ r] "}, {"section_title": "213", "text": "<1> LESS THAN OR EQUAL TO $1,000 <2> $1,001 -$2,000 <3> $2,001 -$3,000 <4> $3,001 -$4,000 <5> $4,001 -$5,000 <6> $5,001 -$6,000 <7> $6,001 -$7,000 <8> $7,001 -$8,000 <9> $8,001 -$9,000 <10> $9,001 -$9,999 Was the amount of these loans more than $10,000, less than $10,000, or right about $10,000? [r] "}, {"section_title": "D77", "text": ""}, {"section_title": "215", "text": "<1> LESS THAN OR EQUAL TO $1,000 <2> $1,001 -$2,000 <3> $2,001 -$3,000 <4> $3,001 $4,000 <5> $4,001 -$5,000 <6> $5,001 -$6,000 <7> $6,001 -$7,000 <8> $7,001 -$8,000 <9> $8,001 -$9,000 <10> $9,001 -$9,999 <-8> DON Is the amount you owe on these loans more than $10,000, less than $10,000, or right about $10,000? [r]INTERVIEWER: R. SHOULD SURELY BE ABLE TO MAKE THIS DETERMINATION. A \"DK\" RESPONSE DOESN'T MAKE ANY SENSE HERE. [n] <1> LESS THAN $10,000 Have you ever applied, or do you intend to apply within the next year, to graduate school or other professional school giving an advanced professional degree (Law School, Medical School, Dental School)? [r]INTERVIEWER: OTHER EXAMPLES TO USE AS PROMPTS INCLUDE \"ADVANCED\" [n] [r]DEGREES (THAT IS, AFTER UNDERGRADUATE DEGREES) IN: DENTISTRY, [n] [r]MEDICINE, CHIROPRACTIC, OPTOMETRY, OSTEOPATHY, PODIATRY, [n] [r]THEOLOGY, VETERINARY MEDICINE, AND LAW [n] [r]PLEASE DISTINGUISH BETWEEN \"HAVE APPLIED\" AND \"INTEND TO APPLY\" [n] <1> YES, HAVE APPLIED <2> [r]pritnary [n] reason that you attended the graduate/professional school you (last) attended in 1993? [r] In the last two years (since the start of 1991), have you [r]actively [n] participated in any way (for pay or as a volunteer) in anyone's campaign for elective office? [r]INTERVIEWER: DO NOT READ RESPONSE ALTERNATIVES UNLESS NEEDED AS [n] [r]PROMPTS. VERIFY YOUR CODING OF ANSWER WITH RESPONDENT, IF [n] D-103 We are almost finished, but we need to ask a few more questions to help us keep in touch with you for the third follow-up in about two years. This information will be kept strictly confidential, as we explained earlier. Please tell me the name, address, and telephone number of a person Mother than your parent or guardian [n] (preferably a relative) [r]who lives at an address different from yours [n] and who would always know how to get in touch with you. [r] Hello, my name is (interviewer': name). I'm calling from Research Triangle Institute for the U.S. Department of Education. Recently, you answered some survey questions as part of the Beginning Postsecondary Study, and we'd like to ask you some of the questions again to check our procedures and our work. Again, participation is voluntary and information will be kept confidential. This interview will be shorter than the first, and should take no more than 15 minutes.  [r]INTERVIEWER: FEATURES ARE ON NEXT SCREEN. READ EACH HIGHLIGHTED FEATURE AND ENTER APPLICABLE RESPONSE. AFTER A VALID ENTRY, THE NEXT APPLICABLE FEATURE WILL BE HIGHLIGHTED. TO CHANGE ANSWER ENTER 20. WHEN DONE, ENTER 99. \"DK\" AND \"11E\" ALLOWED. [n] <1> ENTER 1 TO CONTINUE <-7> REFUSE  The next few questions are about any jobs you may have held since 1 February 1991 (regardless of when the job was started). If you left a job and sometime later west back to the same job, count that as two jobs for our purposes. We want you to consider [r]any job you held for pay [n], including summer jobs, work-study jobs, apprenticeships, and co-ops."}, {"section_title": "ENTER <1> TO CONTINUE >C01A< [allow 2]", "text": "Have you held any full-time or part-time job for pay (including coops, college work study, summer jobs, National Guard, military service or military reserve) at any time since 1 February 1991? [r]NOTE THAT DK(-8) IS NOT ACCEF1ABLE. [ We would like to verify some information about the(se) job(s) and find out about any additional full-or part-time jobs, for pay, you have held since 1 February 1991 (including co-ops, college work study, summer jobs, National Guard, or military reserve). For each new job please tell us: a. Who your employer was, b. What month and year you started the job, c. What month and year you ended the job (if applicable), and d. Whether the job was full-or part-time. [r]INTERVIEWER: ENTER DATA AND DETERMINE PRINCIPAL JOBS IN UX. VERIFY END DATES ON PRELOADS. END DATES FOR \"CURRENT\" PRELOAD JOBS WILL BE BLANK. BE SURE TO GET ALL JOBS WHILE IN USER EXIT, BECAUSE ONCE YOU LEAVE IT, YOU'LL BE PERMANENTLY LOCKED OUT FROM ADDING JOBS OR DATES. <1> ENTER 1 FOR USER EXIT <-7> REFUSE TO ANSWER ANY SUCH QUESTIONS >CO2A< [allow 2] Since you were employed before your last reported term in school, how did you view your primary role in relation to education and work? [r]INTERVIEWER: READ ALL CHOICES BEFORE RECORDING RESPONSE! WE MUST HAVE A RESPONSE TO THIS QUESTION, SO FORCE A CHOICE. [n] <I> a student who works to help pay expenses while in school/college <2> a student who works to earn extra spending money while in school/college <3> an employee who attends school/college to gain skills necessary for job advancement <4> an employee who attends school to expand new career possibilities <5> an employee who attends school to expand personal knowledge/skills not related to employment <-7> REFUSE"}, {"section_title": ">C50A< [allow 2]", "text": "In your interview a few weeks ago, you told us that in 1992 your principal job was with [fill PRINCIPAL JOB EMPLOYER NAME FOR 1992]. What kind of company or organization was this? [r] INTERVIEWER: READ ALTERNATIVES AND FORCE A CHOICE, EVEN \"OTHER\". \"DK\" IS NOT APPROPRIATE HERE. [ Please tell me which of the following statements (which I will read to you) apply to your 1992 [fill \"new\" IF APPLICABLE] job with [fill PRINCIPAL JOB EMPLOYER NAME FOR 1992]. [r]INTERVIEWER: READ EACH LINE AND \"TOGGLE\" TO \"YES\" IF IT APPLIES [n] [r]CODE ALL THAT APPLY: [n] <1> I was able to apply most of what I learned in school <2> The job was different from my education/training <3> I did not use tools/equipment/skills I was trained to use <4> I could have gotten this job without my training/education kj<99> ALL DONE [n] >C53B< [allow 2] What were the primary things you did to find this 1992 job with [fill PRINCIPAL JOB EMPLOYER NAME FOR 1992 Five years from now (spring of 1998), do you intend to be working [r]for pay [n], either full-time or part-time? If you are not sure of the answer, please give your best estimate. [r]INTERVIEWER: BE SURE TO DETERMINE FULL-OR PART-TIME PLANS. [n] <1> YES, FULL-TIME <2> YES, PART -TUB? <3> NO <-8> DON'T KNOW <-7> REFUSE ===>"}, {"section_title": ">C92A< [allow 2]", "text": "In determining the kind of work you plan to be doing for most of your life, how important are each of the following [r]general [n] factors? Please answer: not important, somewhat important, or very important. In determining the kind of work you plan to be doing for most of your life, how important are each of the following [r]incentive [n] factors? Please answer: not important, somewhat important, or very important. [r]INTERVIEWER: READ EACH FACTOR AS IT IS HIGHLIGHTED, AND ENTER APPROPRIATE RESPONSE. ENTER 99 TO EXIT SCREEN. [n] a. Good income or fringe benefits to start or within a few years b. Job security and permanence c. Opportunity for promotion d. Opportunity for further education and/or training <1> NOT IMPORTANT <2> SOMEWHAT IMPORTANT <3> VERY IMPORTANT <-7> REFUSE [r] In determining the kind of work you plan to be doing for most of your life, how important are each of the following [r]lifo-style [n] factors? Please answer: not important, somewhat important, or very important. [r]INTERVIEWER: READ EACH FACTOR AS IT IS HIGHLIGHTED, AND ENTER APPROPRIATE RESPONSE. ENTER 99 TO EXIT SCREEN. [n] a. Meeting and working with sociable people b: Work that has high status and prestige c. Work that lets you establish roots 8c not have to move d. Work that leaves lots of time for other things in life e. Work that allows a great deal of travel <1> NOT IMPORTANT <2> SOMEWHAT IMPORTANT <3> VERY IMPORTANT <-7> REFUSE [r] Was that also your marital status in February of that year (1989)? [r]INTERVIEWER: IT IS VERY IMPORTANT TO DIFFERENTIATE BETWEEN SINGLE, NEVER MARRIED AND DIVORCED, LEGALLY SEPARATED, OR WIDOWED. ALSO NOTE THAT THE \"LIVING AS MARRIED\" CATEGORY IS FOR THOSE NOT MARRIED BEFORE."}, {"section_title": "D-128", "text": "IF R. SAYS HFJSHE WAS \"LIVING AS MARRIED\" BUT HAD BEEN MARRIED PREVIOUSLY (NOT A CATEGORY HERE), ENTER \"NO\" HERE. WE WILL PICK UP THE \"LIVING AS MARRIED, PREVIOUSLY MARRIED\" CATEGORY IN THE UX [n] [r]NOTE THAT DK(-8) IS NOT ACCEPTABLE. [ First, we will be asking about \"student financial aid\" that you may have received for your postsecondary education and [r]related [n] expenses. [r]The kinds of aid we mean are grants, scholarships, work-study, assistantships, fellowships, student loans (that is loans to you), and/or assistance with education costs by your (or your parents') employer or by the militaryln] This aid [r]does not [n] include D-129 266 loans, gifts or other financial assistance from family members, relatives, or friends; also it [r]does not [n] include loans that were made to parents or others, even if it was for your education. Did you receive any \"student financial aid\", as we have defined it since 1 July 1991 for your [r]undergraduate [n] postsecondary education? [r]NOTE THAT DK(-8) IS NOT ACCEFfABLE. [n] <1> YES <2> NO <-7> REFUSE >F08A< [allow 2] While you were attending [fill NAME OF GIVEN SCHOOL], as an undergraduate student, between 1 July 1991 and 30 June 1992, did you receive any student financial aid? Please do not count financial assistance from family or friends. [r]NOTE THAT DK(-8) IS NOT ACCEPTABLE [n] <1> YES <2> NO <-7> REFUSE >F08C< [allow 2] While attending [fill FO8W], as an undergraduate student, between 1 July 1991 and 30 June 1992, did you receive: [r]INTERVIEWER: READ EACH OPTION AND \"TOGGLE\" TO \"YES\" IF RECEIVED. [n] [r]CODE ALL THAT APPLY: [n] [fill hill <1> grant(s) <2> scholarship(s) <3> student loan(s), excluding loan(s) from family or friends or loans to parents <4> tuition benefits or other education assistance (prepaid) from employer or union <5> tuition benefits or other education assistance (reimbursed) from employer or union <6> other financial aid received [r]<99> ALL DONE [n] ==--> >FOSE< [allow lino erase] [r]YOU MUST ENTER AT LEAST ONE \"YES\" TO EXIT THIS SCREEN ! [n] ENTER <1> TO CONTINUE [goto FO8C] >F09A< [allow 5] What was the total amount of [r]all [n] student financia! aid you received (i.e., awarded and accepted) as an undergraduate student at [fill NAME OF GIVEN SCHOOL] between 1 July 1991 and 30 June 1992? Please do not count financial assistance from family or friends."}, {"section_title": "D-130", "text": ""}]