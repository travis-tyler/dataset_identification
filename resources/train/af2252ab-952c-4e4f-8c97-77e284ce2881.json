[{"section_title": "Introduction", "text": "Hurricane surge risk assessment has received increased attention in the past decade, partly in response to the destructive 2004, 2005, and 2008 hurricane seasons (Dietrich et al. 2010;Kennedy et al. 2011a, b). Conventional approaches to this assessment are based on parametric or nonparametric analysis of data from historical storms (Borgman et al. 1992) or on simulation of hurricane design events. A different methodology, initially presented in 1975 (Ho and Myers 1975; and frequently referenced as the joint probability method, relies on a simplified description of hurricane scenarios through a small number of model parameters (Resio et al. 2009). Description of the uncertainty in these parameters, through appropriate probability models, leads to a probabilistic characterization of the hurricane risk. This risk is ultimately expressed as a probabilistic integral over the uncertain parameter space, and its estimation requires numerical evaluation of the hurricane inundation for a large number of scenarios resulting from the adopted probabilistic description of the model parameters (Resio et al. 2009;Toro et al. 2007). This probabilistic framework is increasingly being adopted as a tool for hurricane risk evaluation (Irish et al. 2009(Irish et al. , 2011Niedoroda et al. 2008). Other measures for such risk evaluation include the National Oceanic and Atmospheric Administration's (NOAA's) maximum of maximum (MOM) storm surge levels for Saffir-Simpson hurricane categories 1-5 (www.nhc.noaa.gov) and probabilistic surge (P-surge) estimates of surge exceedence likelihood for approaching hurricanes based on Sea, Lake and Overland Surges from Hurricanes (SLOSH) model simulations (Glahn et al. 2009). A significant recent advance in hurricane surge modeling is the development of high-fidelity numerical simulation models for reliable and accurate prediction of surge responses for specific hurricane events or regions (Cheung et al. 2003(Cheung et al. , 2007Resio and Westerink 2008). These models permit a detailed representation of the hydrodynamic processes, albeit at a greatly increased computational effort. This development increases the computational cost for estimating the hurricane wave and surge risk significantly because the models need to be evaluated for a large number of hurricane scenarios. To alleviate this problem, a lowcost dimensional surge response function was proposed (Irish et al. 2009), but it only addressed the variation with respect to hurricane storm size, intensity, and track and was restricted to hurricane surge only and limited to specific locations of interest on the Texas coast. Udoh and Irish (2011) recently presented preliminary discussions for extending these surge response functions to address additional hurricane model parameters, namely, the forward speed and heading. This paper offers a versatile theoretical and computational framework for evaluation of hurricane wave and surge risk with particular emphasis on real-time automated estimation during landfalling hurricanes and implementation for the Hawaiian Island of Oahu. We offer significant advancements in that (1) risk for a given hurricane scenario may be calculated rapidly for hundreds of thousands of locations in a coastal region of interest and for any modeled quantity representing hurricane impact (e.g., surge, wave height, run-up), (2) all parameters used to describe the hurricane characteristics may be varied over their appropriate ranges, and (3) the framework may be used to develop automated risk-assessment tools that ultimately can be used by end users without high technical expertise, thus crossing over barriers for adoption of such advocated technologies. This versatile framework has few constraints in its applicability; although we consider the evaluation of hurricane risk for the Island of Oahu in Hawaii, the proposed theoretical/computational developments may be readily applied to other locations. The foundation of the framework is the parameterization of each hurricane scenario by a small number of model parameters and the development of a computationally efficient surrogate model for approximation of the impact for any hurricane scenario. Simply put, the input hurricane parameters are the variables that drive a smart interpolation (based on precomputed high-fidelity numerical simulations) of a hurricane response surface (the response surface being the water surface elevation, wave height, or inundation line). A moving least-squares response surface approximation is adopted for the surrogate model. This selection provides the intended versatility of the framework, does not rely on any assumptions for the variability of the hurricane wave/surge response with respect to the hurricane model parameters, and is efficiently implemented for any quantity representing the hurricane impact (e.g., wave height, surge, inundation time, etc.). The proposed framework ultimately facilitates rapid real-time risk assessment, but it establishes this at a considerable upfront computational cost, namely, to perform high-fidelity simulations to create the surrogate model. During a landfalling hurricane, the National Hurricane Center forecasts the most probable hurricane track ( Fig. 1) and also provides standard climatologic errors for track, strength, etc. associated with this prediction. This forecast leads to a probabilistic integral quantifying hurricane risk that can be estimated in real time at a small computational cost using the surrogate model. Leveraging this computationally efficient framework, we establish an assessment tool that allows nonexpert end users to take advantage of the full potential of this methodology to assess risks. Fig. 2 presents an overview of the methodology, which may be applied with little change to various coastal regions. As an illustrative example, we implement these methods for hurricane risk estimation on the Island of Oahu in Hawaii and present results. The following sections describe the probabilistic framework, the high-fidelity hurricane modeling, the surrogate modeling, and the automated risk assessment before focusing on the application to Oahu."}, {"section_title": "Probabilistic Framework for Risk Quantification and Estimation", "text": ""}, {"section_title": "Hurricane Modeling", "text": "In this framework, each hurricane event is approximated by only five parameters, all corresponding to its characteristics at landfall: (1) landfall location x o , (2) track heading u, (3) central pressure c p , (4) forward speed v f , and (5) radius of maximum winds R m . These variables ultimately constitute the model parameters vector x describing each hurricane scenario where \u00bd T 5 matrix transpose. The temporal and spatial variability of the hurricane track and characteristics prior to landfall also can be important, but directly incorporating this variability in the hurricane description would increase the number of model parameters significantly. Instead, this variability is addressed by appropriate selection of the hurricane track history prior to landfall (i.e., giving the track a typical curvature over time for the appropriate landfall heading) so that typical variations in path are described adequately based on historical data. If the sensitivity of wave and surge response to such parameter variations is small, then this approach can address this issue efficiently (Resio et al. 2009); sensitivity will differ in different locations (e.g., Hawaii or Gulf of Mexico). Similar comments apply with respect to tides; unless the tide level is included in the model description, as another parameter in x, a constant tide level must be assumed (Resio et al. 2009). In the application considered in this study, we assume a high tide for all model runs. To represent hurricane impact on the coastal region of interest, and ultimately, to quantify risk, several response quantities may be examined simultaneously in this framework. Examples of such quantities are (1) the still water level (SWL), i.e., storm surge, defined as the average sea level over a several-minute period, (2) the wave run-up level (WRL), defined as the sea level including run-up of wind waves on the shore, (3) the significant wave height H s (possibly along with the corresponding peak period T p ), and (4) the time that normally dry locations are inundated. In this study we will focus on the first three. Once the response parameters of interest have been determined, we let z\u00f0x\u00de denote the vector of response quantities [i.e., z\u00f0x\u00de is the response as a function of the spatial variables x and y]. This vector will be referenced herein as the response vector and will be used to quantify hurricane risk. Each component of z\u00f0x\u00de pertains to a specific response variable (any of the four just described) for a specific location. The augmentation of all these responses for all different locations in our region ultimately provides the n zdimensional vector z. Thus the dimension of n z can be very large. For a specific hurricane scenario described by the model parameter vector x, the hurricane impact [vector z\u00f0x\u00de] may be accurately estimated by numerical simulation once an appropriate high-fidelity model is established (Resio and Westerink 2008;. These models have a significant associated computational cost, requiring thousands of computational hours for analysis of each hurricane scenario. This is intensified by the fact that for appropriately assessing the hurricane impact, the simulation may need to extend five days or more prior to landfall. This is essential for both numerical convergence and to capture all significant changes in the wave and surge environment (Dietrich et al. 2010). This task will be discussed in more detail in the following section. For alleviating the computational cost associated with the risk evaluation, a response surface surrogate modeling approach is described in more detail in following sections. The surrogate model is based on information provided by a number of evaluations (scenarios) of the computationally intensive high-fidelity model (called support points) and ultimately establishes an approximation for the hurricane impact for any other hurricane scenario. For greatest accuracy and to avoid extrapolation, the support points (scenarios simulated with the high-fidelity model) need to span the entire value range of potential parameters. When using the surrogate model for predictions, large extrapolations are ultimately avoided by truncating each parameter within the range defined by the available support points, expanded by a moderate length (chosen as 25% here) in each direction. As such, the initial selection for support points determines the region of model parameter values for which the surrogate model can be used. The approximation for z\u00f0x\u00de established through the surrogate model will be denoted by\u1e91\u00f0x\u00de herein. If z i \u00f0x\u00de andb z i \u00f0x\u00de denote the ith components of vectors z\u00f0x\u00de and\u1e91\u00f0x\u00de, respectively, then the relationship between them is where e i 5 total prediction error established through the various introduced approximations. This error can be decomposed into two independent components: where \u025b i pertains to errors introduced by the surrogate modeling approximation of the high-fidelity numerical model, and h i pertains to modeling errors introduced through the fundamental assumptions established for the theoretical framework, i.e., the simplified description of the hurricane scenario through only five variables (rather than its entire history) and to any approximations established in the high-fidelity simulation model, e.g., exclusion of tides, accuracy of bathymetry, etc. The study by Resio et al. (2009) provides a detailed discussion for potential sources impacting h i . As is standard, both of these errors are assumed to be zero mean (because the contrary indicates a bias in the modeling approach) Gaussian random variables with standard deviations s \u025bi and s h i , respectively. Note that this choice of probability distribution (Beck 2010; Taflanidis and Beck 2010) incorporates the largest amount of uncertainty (Jaynes 2003) in terms of information entropy (Shannon 1948) under the constraints that only the mean and variance are known. Based on Eq. (3), the total prediction error then follows a Gaussian distribution with SD The variance for the \u025b i error can be estimated by comparison between the surrogate and high-fidelity models. For this purpose, a set of hurricane scenarios is selected to act as validation points; s 2 \u025b i then corresponds to the average value of the difference \u00bdz i \u00f0x\u00de 2b z i \u00f0x\u00de 2 over this set. Quantification of the variance for the h i error is more challenging. It requires definition of a realistic set of regional hurricane scenarios and, for each scenario, calculation of the response, denotedz i , and comparison with the response that corresponds to the idealization of the scenario in terms of the established simplified parameterized framework z i \u00f0x\u00de (Resio et al. 2009). The variance for the h i error is then similarly given by the statistics of the square difference \u00bdz i \u00f0x\u00de 2z i 2 ."}, {"section_title": "Risk Quantification", "text": "Hurricane risk may be quantified in terms of the response\u1e91\u00f0x\u00de provided by the surrogate model and the probability density function p\u00f0x\u00dedescribing the uncertainty in the input hurricane parameters. For real-time risk evaluation, i.e., predicting the risk owing to an approaching hurricane, p\u00f0x\u00de may be constructed through standard climatologic error estimates provided by the National Hurricane Center (AEhttp://www.nhc.noaa.gov/verification/verify4.shtmlae). This information then can be used to adopt a probabilistic description for the model parameters. In this case, each component of x can be selected to follow an independent Gaussian distribution with mean equal to the forecast quantities and standard deviation equal to the associated statistical error. For long-term hurricane risk evaluation for a region (Resio et al. 2009), p\u00f0x\u00de is selected based on statistical data or atmospheric models for the entire region (Emanuel et al. 2006), and it further incorporates information on occurrence rates for hurricanes, not just on relative plausibility of the model parameters. The study by Resio et al. (2009) includes a detailed discussion for selection of p\u00f0x\u00de for the Gulf of Mexico. Risk is finally expressed as some desired statistic of the response z, e.g., the probability that the wave height will exceed some specific threshold or the median wave run-up. The exact selection used for these statistics leads to definition of the risk-consequence measure h\u00bd \u00d7 . Ultimately, for any component z i of the response vector, the risk, denoted R i , is provided by the probabilistic integral where X corresponds to the region of possible values for x. Through appropriate selection of h\u00bd \u00d7 , different risk quantifications can be addressed through this framework. Appendix I provides further details on this selection for h\u00bd \u00d7 ."}, {"section_title": "Risk Estimation", "text": "The risk integral in Eq. (5) can be estimated by stochastic simulation (Robert and Casella 2004). For the simplest approach (direct Monte Carlo), and using N samples of x randomly selected from p\u00f0x\u00de, the estimate for R i is given by where vector x k 5 sample of the uncertain parameters used in the kth simulation. As N \u2192 ', then b R i \u2192 R i , but even for finite, largeenough N, Eq. (6) gives a good approximation for the risk given by Eq. (5). The quality of this approximation is assessed through its coefficient of variation d obtained by (Robert and Casella 2004)  The estimation accuracy may be improved further by adopting some advanced stochastic simulation approaches such as importance sampling (Taflanidis and Beck 2008). This stochastic simulation-based risk assessment facilitates an efficient estimation of risk for different quantifications (different selections for h\u00bd \u00d7 ) because the response \u00bdb z i \u00f0x k \u00de; k 5 1, :::, N needs to be estimated only once; the various selected risk quantifications then require merely estimation of the different risk-consequence measures h\u00bd \u00d7 , which is computationally straightforward."}, {"section_title": "High-Fidelity Hurricane Response Evaluation", "text": "Hurricane Wave and Surge Response For any given hurricane scenario, the parameter vector x along with the chosen hurricane track history defines the wind and pressure fields over time through a parametric hurricane model (Phadke et al. 2003). Using these wind and pressure fields, the surge and wave response for the entire coastal region of interest can be calculated with a high-fidelity model, which for this study is Simulating Waves Nearshore (SWAN) 1 Advanced Circulation Model for Oceanic, Coastal and Estuarine Waters (ADCIRC) (Dietrich et al. 2010(Dietrich et al. , 2011. ADCIRC solves the shallow-water equations for water levels and the momentum equations for currents. The variables are defined on unstructured triangular finite-element grids at the vertices (Luettich et al. 1992;. The model has been applied extensively along the Gulf and East coasts of the United States to study hurricane storm surge (Bunya et al. 2010;Dietrich et al. 2010;Tanaka et al. 2011;. ADCIRC is applied in vertically integrated mode for these computations. Waves are computed using the unstructured version of the SWAN phase-averaged wind-wave model, which solves for wave action density in time, geographic space, and spectral space. Source and sink terms account for wave growth by wind; dissipation owing to whitecapping, surf breaking, and bottom friction; and nonlinear interaction between spectral components in deep and shallow waters. The unstructured grid version of SWAN is based on triangular elements with the action density function defined at the vertices (Zijlema 2010). Waves and circulation interact despite being well separated in frequency space. As wind waves transform, either through growth or dissipation, they impart a force on the upper portions of the water column known as the wave-radiation stress. This drives currents and increases the still water level (wave setup) along the coast and increases inland flood propagation. Water levels have been shown to increase by at least as much as 35% owing to local wave-driven setup on U.S. mainland coasts (Dietrich et al. 2010;Resio and Westerink 2008), but larger contributions are expected in Hawaii because the steep offshore slopes increase the breaking heights of nearshore waves. In addition, waves affect vertical mixing of momentum and therefore can influence the vertical structure of the currents as well as the bottom stress. The water surface elevation and currents also impact the wind-wave propagation through the depth of the water column and wave-current interaction. SWAN 1 ADCIRC has been fully integrated into a comprehensive modeling system that allows full interaction between model components. Because the variables for both models are defined at identical locations (i.e., vertices) and run on identical grids, no interpolation is performed between the two models. Furthermore, the combined SWAN 1 ADCIRC code is implemented in a highly efficient parallel environment (Dietrich et al. 2010)."}, {"section_title": "Wave Run-up", "text": "Wave action can increase inundation considerably in the swash zone, which is intermittently wet and dry from wave run-up and drawdown. Run-up parameterizations exist for simple geometries, such as for a one-dimensional planar beach or a breakwater (Van Der Meer 2002), but steep island bathymetries/topographies tend to be much more complex. Phase-resolving Boussinesq models do a good job of resolving and simulating the setup, run-up, and swash for arbitrary topographies (Chen et al. 2000;Kennedy et al. 2000;Nwogu 1993;Nwogu and Demirbilek 2010). However, full two-dimensional Boussinesq models are too computationally expensive to be used over the large areas and for the large numbers of computational runs required to evaluate inundation over the entire coastal region of interest. Given this limitation, an approximate approach is adopted in which a large number of one-dimensional transects are defined across the shoreline in the region of interest. A three-dimensional array of still water levels and wave parameters is then defined at the offshore end of each transect, with values based on the anticipated wave environment characteristics provided through the SWAN 1 ADCIRC runs. One-dimensional Boussinesq model analysis then is performed for all these parameter combinations, yielding a prediction for the wave run-up along each transect using the One-Dimensional Boussinesq Model (BOUSS-1D) (Demirbilek et al. 2009;Nwogu and Demirbilek 2001). These results then provide an estimate of maximum inundation distance along that transect (using as reference point the offshore end of the transect) for any wave and water level input required for the risk assessment. If z wj is the wave run-up at transect j and H sj , T pj , and z sj are the corresponding wave height, wave period, and still water level, respectively, then this approach leads to the mapping where D 5 data obtained through the Boussinesq analyses for each transect, and g\u00f0 \u00d7 \u00de 5 chosen interpolation mapping."}, {"section_title": "Response Surface Surrogate Modeling", "text": "A response surface (Myers and Montgomery 2002) surrogate model approach is adopted here to approximate in real time the response z obtained by the computationally expensive ADCIRC 1 SWAN numerical model. This is established by expressing each z i \u00f0x\u00de, where x is the n x 5 5-dimensional vector defining the hurricane characteristics, through j 5 1, :::, NB preselected basis functions b j \u00f0x\u00de through introduction of coefficients a ij fxg: where b\u00f0x\u00de and a i fxg 5 vectors containing the basis functions and coefficients, respectively. A common choice for basis functions is a complete second-order approximation leading to The coefficients a i fxg are calculated by initially evaluating z i \u00f0x\u00de (through the high-fidelity numerical model) for a set of hurricane scenarios with characteristics spanning the entire region of interest for x (representing probable and significant future hurricane scenarios) and then by minimizing a weighted mean squared error over these scenarios between z i \u00f0x\u00de and the approximation established through Eq. (9) (Choi et al. 2001;Myers and Montgomery 2002). The weights in this mean squared error are also a function of x (what is formally known as moving least-squares response surface approximation); this improves the efficiency of the approximation by giving higher importance to high-fidelity hurricane scenarios (support points) that are similar to the new scenario that we are trying to approximate (Taflanidis 2012). Appendix II provides further details on these tasks. Ultimately, the approximation established through the moving least squares response surface is given by where matrices Mfxg, Lfxg, and F i are defined in Appendix II and depend on the weighting functions selected for the interpolation as well as vectors b\u00f0 \u00d7 \u00de and z\u00f0 \u00d7 \u00de evaluated at the locations of the support points. The approximation for the entire vector z is established by approximating each z i through Eq. (12) and augmenting the results in vector format. This is ultimately expressed in a simple mathematical form a\u015d This is a computationally inexpensive approximate model (based only on matrix manipulations); thus it can be exploited for real-time predictions but requires that a large number of high-fidelity model simulations be performed in advance (this information is ultimately included in F). Note that information from models runs (SWAN 1 ADCIRC) is only incorporated in F; the rest of the matrices in Eq. (13) are independent of any model. As such, the approach is extendable to any output of interest and can be easily augmented to include information from additional high-fidelity simulations as they become available. Finally, the statistics of the prediction error owing to the response surface surrogate modeling can be approximated by the statistics of the difference \u00bdz i \u00f0x p \u00de 2b z i \u00f0x p \u00de over any sample set of hurricane scenarios described by [x p , p 5 1, ..., N E ] chosen to act as validation points for the response surface. This leads to the following maximum-likelihood estimate for the prediction error SD (Grimmett and Stirzaker 2001)  This completely defines the probability model for the prediction error \u025b i because it has already been assumed to follow a zero mean Gaussian distribution."}, {"section_title": "Automated Risk Assessment", "text": "The theoretical/computational developments in the proposed framework share one significant drawback: their complexity creates a technological barrier that effectively isolates them from nonexpert users. As a result, these advances may not translate directly to the intended benefactors of the risk-assessment framework (decision makers) and the constituents they serve (public at large). To reduce this barrier between this methodology and end users, automated risk-assessment tools can be developed by leveraging the computational simplicity and speed of the proposed surrogate model, which is based only on simple matrix manipulations. These resulting stand-alone tools require minimum computational resources to run because they perform only simple mathematical manipulations expressed through Eqs. (6) and (13), as well as access to the database of high-fidelity model runs. Fig. 3 shows the graphical user interface of the tool developed for the illustrative example that will be considered next, which examines inundation risk over the coastline of Oahu for an approaching hurricane. Through this tool, the user can define track and strength characteristics [and thus p\u00f0x\u00de, as discussed earlier] and then request evaluation of the response \u00bdb z i \u00f0x k \u00de, k 5 1, .:: , N, which is rapidly computed based on the surrogate model [Eq. (13)]. The user then can select some risk quantification for a specific response of interest (e.g., probabilistic mean surge or wave height when a storm is 48 h from landfall), which defines h\u00bd \u00d7 , as discussed earlier, and finally request calculation of the risk based on Eq. (6). Ultimately, such tools allow nontechnical end users to leverage the full power of the proposed risk-assessment framework and can be used for planning emergency responses during real hurricane events approaching landfall or for making long-term planning decisions based on fictitious scenarios."}, {"section_title": "Application to Real-Time Risk Estimation for Oahu", "text": "The framework developed in the preceding sections is demonstrated for the real-time hurricane risk estimation for Oahu. The computational domain developed for this study encompasses a large portion of the northern Pacific Ocean from 0 to 35\u00b0north and from 139 to 169\u00b0west. The unstructured Hakou-2010-r2 grid resolves the deep ocean with 10-km elements, incorporates all the main Hawaiian Islands (Hawai'i, Maui, Kaho'olawe, Lana'i, Moloka'i, Oahu, Kauai, and Ni'ihau), represents Oahu and Kauai with significant detail up to 30-m resolution, and extends inland to the 4-m contours. Fig. 4 shows the bathymetric details of the grid close to the islands of Oahu and Kauai. The grid incorporates 1,590,637 vertices and 3,155,738 triangular elements. The coarsest resolution at the domain edge is 10 km, and the finest resolution of 30 m is found in complex coastal areas and overland. Bathymetric and topographic data applied to the grid came from a variety of sources. Deepwater bathymetric data came from the General Bathymetric Chart of the Oceans, Earth Topography, and NOAA's Laboratory for Satellite Altimetry. Scanning Hydrographic Operational Airborne LiDAR Survey (SHOALS) data were used from shore to a depth of 40 m, whereas in deeper areas, multibeam data from the University of Hawaii School of Earth Science and Technology were used. Additionally, in certain bays and harbors, data were taken from navigation charts (variable resolution, NOAA National Ocean Service, multiple years). On shore, topographic data up to the 15-m elevation contour were taken from LiDAR data sets taken by FEMA and the U.S Army Corps of Engineers (USACE). Topographic data from the USGS National Elevation Dataset covers the rest of Oahu and Kauai. For the numerical simulation, SWAN applies 10-min time steps, whereas ADCIRC applies 1-s time steps. A SWAN 1 ADCIRC simulation ran in 16 wall-clock min/day of simulation on 1,024 cores on Diamond, a 2.8-GHz dual-quad core-based cluster with a 20-Gb/s InfiniBand network (AEhttp://www.erdc.hpc.mil/ae). The Hakou-2010-r2 model (Kennedy et al. 2012) was validated by simulating tides and by hindcasting Hurricane Iniki (in 1992) and comparing with measured water levels as well as wave data. More details on the model itself and the validation may be found in Kennedy et al. (2012)."}, {"section_title": "High-Fidelity Simulations", "text": "Based on historical storms and in collaboration with the National Weather Service (NWS) Central Pacific Hurricane Center, a suite of hurricane scenarios was created for surge and wave modeling using SWAN 1 ADCIRC. These scenarios provide the information required for building the surrogate model and were chosen so that they cover future hurricane events that are anticipated to have significant impact on Oahu. Five basic storm tracks were considered, representing different angles of final approach u, as shown in Fig. 5. Landfall was defined to correspond to the point where each hurricane crosses 21.3\u00b0north, and six different landfall locations x o were chosen for the grid of storms corresponding to 157.7, 157.9, 158.1, 158.3, 158.6, and 158.9\u00b0west. Three different values for the central pressure c p were used, 940, 955, and 970 mbar, and similarly, three different cases for the forward velocity v f were considered, 7.5, 15, and 22.5 knots. Finally, two different values were considered for the Fig. 3. Graphical user interface for automated risk-assessment tool radius of maximum winds R m : 30 and 60 km. These values ultimately define the range of hurricane characteristics for which the surrogate model can be applied with confidence; for example, and using the 25% allowed extrapolation discussed earlier, for the central pressure, the surrogate model can be used to forecast storms between 932.5 and 977.5 mbar. A suite of 350 storms then was selected to efficiently describe the entire grid of hurricane scenarios generated through these potential parameter values. The response for these storms then was computed by the SWAN 1 ADCIRC model, a process that ultimately required more than 600,000 computational hours. Such simulations can be performed outside hurricane season, removing real-time forecasting constraints on time to execute runs and perform quality control. All results of interest were stored, including maximum significant wave heights and surge levels throughout each simulation. Fig. 6(a) shows illustrative results of contours for the wave height obtained from one hurricane scenario through the high-fidelity model analysis. All analyses were performed assuming high tides of 0.4 m, taken to represent the worst-case scenario for the hurricane risk in this region. For the wave run-up, 750 transects were considered around the island, with each transect extending up to 1 km inland and 2 km offshore. For each transect, a matrix of 169 combinations of wave height and water level was created, with maximum and minimum parameter values selected based on the information from the largescale runs. For each case, the inundation then was predicted based on the Boussinesq model analysis, which produced the run-up data D for development of the mapping described in Eq. (8)."}, {"section_title": "Response Surface Surrogate Model", "text": "Using the information from the precomputed 350 storms, a moving least-squares response surface surrogate model was built. Full quadratic basis functions were chosen for x o , u, c p , v f , and linear basis functions for R m , and the common Gaussian weight function in Eq. (25) is adopted with D d adaptively selected so that it includes for each x 150 support points (out of the possible 350). To evaluate the fit of the surrogate model through the average error from Eq. (28) and further estimate the prediction error variance based on Eq. 14, an additional 20 hurricane scenarios were generated to represent the validation set for the surrogate model. The characteristics for these scenarios were randomly generated within the possible values for the hurricane model parameters x. The average (over the entire region of interest) mean error of the response surface approximation was 4% for the significant wave height and 3% for the still water level. Over the entire domain, the average prediction error SD s \u025b i is 0.31 m for the significant wave height and 0.14 m for the still water level, but both these quantities vary significantly over the region of interest. A comparison of significant wave height between the high-fidelity numerical model evaluation and the optimal response surface surrogate model prediction is shown in Fig. 6 for a sample hurricane track with parameters x 5 \u00bd158:1\u00b0210\u00b0955 mbar 22 knots 30 km. The comparison between Figs. 6(a and b) shows good agreement, which further demonstrates the accuracy of the established response surface approximation. It should be noted that the cost of the surrogate model for a single evaluation is at least 10 7 times less than that for the highfidelity models, which is what allows it to be used for real-time prediction on a PC. This surrogate model then is used to predict the response, in particular, the still water level b z sj and significant wave height b H sj , for any desired hurricane scenario and simultaneously for all locations of interest around the island. The interpolation scheme of Eq. (8) then can be used to calculate the wave run-up for each transect using the surrogate model predictions of wave and water level properties as input. In this study, the h i modeling error, introduced through the fundamental assumptions established for the theoretical framework and the approximations established in the high-fidelity simulation model (e.g., high tide assumption), is ignored. Thus, e i is completely defined by knowledge of \u025b i . This choice is necessitated by the lack of sufficient data for quantifying its SD s h i , as discussed previously. An alternative approach would have been to select an appropriate value for s h i based on engineering judgment."}, {"section_title": "Risk Assessment", "text": "The established response surface surrogate model can be used to estimate hurricane risk efficiently. For real-time assessment, the probability distribution for the model parameters p\u00f0x\u00de can be based on information provided by the NWS prior to landfall, information that is updated during regular time intervals as the storm approaches landfall. As discussed earlier, an independent Gaussian distribution with the mean prediction of the NWS and standard deviation equal to the associated statistical error is adopted in this study. This probability model then can be used to quantify risk as in Eq. (5). The risk is then estimated through stochastic simulation by Eq. (6) using the previously developed surrogate model to evaluate the response for each configuration x 5 \u00bdx o u c p v f R m T . For automated implementation of this risk estimation, a stand-alone tool is developed (Taflanidis et al. 2011), as shown in Fig. 3. The model accepts as input the parametric configuration x for the most probable hurricane track, as well as the estimate for time until landfall, which is used to select p\u00f0x\u00de. Based on this input and the precomputed information from the high-fidelity simulations, the surrogate response surface approximation can be used to predict either the output for the most probable hurricane (i.e., single evaluation of surrogate model) or the hurricane risk, estimated as the threshold with a prespecified probability of exceedence (i.e., multiple evaluations). In the latter case, N 5 2;000 evaluations of the surrogate model are used for the stochastic simulation of Eq. (6). The outputs from the risk estimation then are presented graphically as contours for the surge and wave run-up inundation around the island, as well as contours for the significant wave height in the region around Oahu. A sample implementation is presented for the hurricane illustrated in Fig. 3 approaching landfall on Oahu. The mean values for hurricane parameters (e.g., landfall longitude, heading at landfall in degrees, central pressure in millibars, forward speed in knots, and radius of maximum winds in kilometers) are x mean \u00bc \u00bd158.12\u00b0205\u00b0950 mbar 16 knots 45 km T \u00f015\u00de For defining p\u00f0x\u00de, these parameters are assumed to follow independent Gaussian distributions with the mean value given in Eq. (15) and the SD selected based on prediction errors for time until landfall equal to 42 h s x \u00bc \u00bd0:28\u00b017.5\u00b010.5 mbar 3.5 knots 2.5 km T \u00f016\u00de Results are shown in Figs. 7-12, for the general area around the Hawaiian Islands, as well as zoomed in around Oahu. Figs. 7 and 8 include the expected value for the significant wave height; Fig. 9, the probability that the significant wave height will exceed 9 m; and Fig. 10, the significant wave height with 10% probability of being exceeded during the storm. Similarly, Figs. 11 and 12 show the still water level and wave run-up level contours around Oahu with 10% probability of exceedence. The total time needed for this risk assessment is 6 min on a 3.2-GHz single-core processor with 4 GB of random-access memory (RAM). This corresponds to a tremendous reduction of computational time compared with the high-fidelity model, which required more than 1,500 h for analyzing a single hurricane scenario and ultimately is the foundation that allows for the risk quantification and assessment described here. Thus a risk computation requiring independent evaluations of the surrogate model for N 5 2,000 different scenarios is still approximately 10 4 times faster than evaluating a single hurricane scenario using the high-fidelity models SWAN 1 ADCIRC. The average (over all grid points) coefficient of variation for the risk estimate is 0.42% for the expected value of the significant wave height (Figs. 7 and 8) and 6.71% for the significant wave height with 10% probability of being exceeded (Fig. 10). The latter corresponds to a rare risk description, which is why it is characterized by a larger coefficient of variation. The small established coefficient of variation along with the required small computational time for establishing these risk estimates and the accuracy of the surrogate model illustrate the efficiency of the proposed theoretical and computational scheme. It should be stressed that the proposed approach explicitly incorporates in the analysis the prediction error established through the surrogate modeling."}, {"section_title": "Summary and Conclusions", "text": "A probabilistic framework has been developed here for hurricane risk estimation with particular emphasis on real-time risk evaluation. This methodology uses hundreds of high-fidelity model runs, an efficient surrogate model that reproduces the high-fidelity model well, and a probabilistic description of uncertainty to estimate wave, water level, and run-up probabilities at a much reduced computational cost. The framework is efficient enough that it may be used easily for prediction of risk as a hurricane approaches landfall. However, there is a large upfront computational cost to evaluate the high-fidelity models. As an illustrative example, implementation of hurricane risk estimation for the Island of Oahu in Hawaii was presented. Results demonstrate the versatility of the proposed approach for creating an efficient tool that is simple enough to cross over technology adoption barriers. In this stand-alone tool, the interface provided to the user requires only a basic understanding of the five parameters used for the simplified description of each hurricane scenario. The total time needed for risk assessment using the developed stand-alone tool was 4 min on a 3.2-GHz single-core processor with 4 GB of RAM. High accuracy was established for the risk estimates since the coefficient of variation for such estimates using 1,500 sample runs was very small. This framework may be applied readily to other regions and other problems where high-fidelity model runs may be run in advance, but risk estimates are required in real time. This methodology also may be readily extended to include derivative products such as building and infrastructure damage during a storm or loss estimates. Closely related examples include tsunami inundation warning and modeling, where a similar framework could be implemented with few structural changes."}, {"section_title": "Appendix I. Risk Quantification", "text": "This appendix provides details for selection of h\u00bd \u00d7 to address different risk quantifications. For example, if R i corresponds to the expected value (average) for z i , then we have where E i 5 range of possible values for e i . Using the assumption that prediction error is uncorrelated from x and that it is zero mean, we get which means that h\u00bdb z i \u00f0x\u00de 5b z i \u00f0x\u00de in terms of the risk quantification of Eq. (5). If, alternatively, R i corresponds to the probability that some z i will exceed some threshold b i , then Using the assumption that prediction error is uncorrelated from x and that its distribution is symmetric, we get  where F e i \u00f0b z i 2 b i \u00de 5 cumulative distribution function for the model prediction error e i . Thus, in this case, h\u00bdb z i \u00f0x\u00de 5 F e i \u00bdb z i \u00f0x\u00de 2 b i in terms of the risk quantification given by Eq. (5). This simplifies to for the proposed case of Gaussian distribution for the model prediction error, where F[\u00d7] denotes the standard Gaussian cumulative distribution function. Note that this measure explicitly includes model error estimates because model predictions less than the threshold b i still may have a small probability of exceeding the threshold, and vice versa. As the model prediction error approaches zero, the risk measure collapses to zero if b z i \u00f0x\u00de , b i and one if b z i \u00f0x\u00de . b i .  This appendix provides further details on response surface surrogate modeling. The coefficients a i fxg for the surrogate model [Eq. (9)] are calculated by initially evaluating z i \u00f0x\u00de in a set of NS . NB hurricane scenarios (called support points for the surrogate model) [x I , I 5 1, . . . ; NS] and then by minimizing the mean squared error over these points between z i \u00f0x\u00de and the approximation established through Eq. (9) (Myers and Montgomery 2002). In the moving-least-squares (MLS) approach, the coefficients depend on x and are selected by minimizing a weighted sum of squared error with weights that are a function of x (Taflanidis 2012) where the following quantities have been introduced and w\u00bdd\u00f0x; x 1 \u00de is a weight function that depends on the distance d between the point x for which the approximation is established and each of the support points with v i representing the relative weight for each component of x j (Taflanidis 2012). The introduction of the weights wfdg aims at reducing the approximation error at each point by performing a weighted local averaging of the information obtained by the support points that are closer to it. Without these weights, the coefficient vector a i would be constant over the whole domain for x, which means that a global approximation would be established (global least squares). The efficiency [i.e., fit to z i \u00f0x\u00de] of global approximations depends significantly on the selection of the basis functions, which should be chosen to resemble z i \u00f0x\u00de as closely as possible. Such a selection is not always straightforward. The moving least squares circumvents such problems by establishing a local approximation for a i fxg around each point in the interpolation domain. This leads to a smaller dependence of the fit on the type of basis functions used (Breitkopf et al. 2005). On the other hand, the efficiency of the moving-least-squares interpolation depends on the weighting function chosen. This function should prioritize support points that are close to the approximation point and should vanish after an influence radius D d . This radius should be selected so that a sufficient number of neighboring support points are included to avoid singularity in the solution for a i fxg. This means that D d should include at least NB points. As weighting function in this study, the exponential type of function wfdg \u00bc e 2\u00f0d=cD\u00de 2k 2 e 2\u00f01=c\u00de 2k . 1 2 e 2\u00f01=c\u00de 2k if d , is selected, with c 5 0:4 and k 5 1 (Gaussian). The relative weights v i ultimately define the moving character of the approximation within the different directions in the X space and should be chosen to (1) establish a normalization for the different components of x but, more important, (2) provide higher importance for components that have a larger influence on the values of z i \u00f0x\u00de (Taflanidis, 2012). Finally, the minimization of Eq. (22) is a standard quadratic optimization problem and yields solution where Ultimately, D d in Eq. (25) should be selected so that M is invertible. Finally, Eq. (9) yields The fit of the response surface approximation may be judged by selecting a number of hurricane scenarios to represent the validation points (control points), denoted by x p , p 5 1, :::, N E , and then evaluating the mean error ME, given by (Myers and Montgomery 2002) "}]