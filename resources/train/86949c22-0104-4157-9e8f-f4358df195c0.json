[{"section_title": "Abstract", "text": "Background In studies with surrogate outcomes available for all subjects and true outcomes available for only a subsample, survival analysis methods are needed that incorporate both endpoints in order to assess treatment effects. Methods We develop a semiparametric estimated likelihood method for the proportional hazards model with discrete time data and a binary covariate of interest. Our proposed method allows for real-time validation of surrogate outcomes and flexible censoring mechanisms. Results Our proposed estimator is consistent and asymptotically normal. Through numerical studies, we showed that our proposed method for estimating a covariate effect is unbiased compared to the na\u00efve estimator that uses only surrogate endpoints and is more efficient with moderate missingness compared to the complete-case estimator that uses only true endpoints. We further demonstrated the advantages of our proposed method in comparison with existing approaches when there is real-time validation. We also illustrated the use of our proposed method by estimating the effect of gender on time to detection of Alzheimer's disease using data from the Alzheimer's Disease Neuroimaging Initiative. Conclusion The proposed method is able to account for the uncertainty of surrogate outcomes using a validation subsample of true outcomes in estimating a binary covariate effect. The proposed estimator can outperform standard semiparametric survival analysis methods and can therefore save on costs of a trial or improve power in detecting treatment effects."}, {"section_title": "Introduction", "text": "In clinical trials, interest often lies in comparing the effects of treatment on time to an event. The Cox proportional hazards model is a common method for analyzing true survival outcome data, but true outcomes are often unavailable due to invasiveness or cost restrictions of diagnostic tests. Surrogate outcomes are often used as an alternative to true outcomes since they are more widely available, but mismeasured surrogate outcomes may produce mismeasured survival estimates. Sometimes, data on both the mismeasured surrogate outcomes and a subset of true outcomes are available.\nFor example, we consider estimation of the time to pathological detection of Alzheimer's disease (AD), which can be measured by a cerebral spinal fluid (CSF) assay of amyloid beta (Ab) protein concentrations. Although the gold standard for pathological diagnosis of AD is autopsy, a diagnosis after death does not provide information about time to AD. Furthermore, abnormality of CSF values is highly correlated with autopsy diagnosis and is a well-accepted measure for pathological diagnosis of AD among living participants in research studies, 1 and thus represents the true outcome without error in this study. However, the CSF assay requires a lumbar puncture to extract spinal fluid, which is considered too invasive for some patients. Therefore, a CSF-based outcome has limited availability. Alternatively, clinical detection of AD based on cognitive tests may be used as a surrogate for the pathological detection of AD since it is easier to obtain. However, the clinical symptoms of AD present differently from the pathological signs and therefore measure the true outcome with error. The clinical assessment is more widely available, so we have surrogate outcomes on all subjects, whereas we only have true pathological assessments for some subjects. Using both the mismeasured surrogate outcome on all subjects and the true outcome on a subsample, called the validation sample, estimates of covariate effects can be improved.\nPrevious methods for estimating mismeasured survival outcomes assumed known mismeasurement rates of the surrogate, such as sensitivity and specificity of the diagnostic test used. [2] [3] [4] [5] These previous methods did not incorporate a validation subsample of true outcomes. Pepe 6 developed an estimated likelihood method for data with surrogate outcomes on all subjects and true outcomes on only a subsample, but the method was not specifically for a survival outcome. Magaret 7 extended the estimated likelihood method for discrete survival data without real-time validation using a proportional hazards model. Because validation cannot be conducted in real time, Magaret's method relies on the assumption that true outcomes are censored after false positives and that true and surrogate censoring times are equal when the surrogate outcome is censored (second full paragraph on page 5459 of Magaret 7 ) . In some situations, however, real-time validation is possible and the true and surrogate outcomes follow separate trajectories. In the AD example, subjects in the validation subsample can undergo regular clinical screenings while they also separately undergo independent CSF testing, so true outcomes are not necessarily censored after false positives. Therefore, the time to pathological AD detection may be before or after the time to clinical detection. Also, true and surrogate censoring times can be completely different. Zee and Xie 8 adapted the estimated likelihood method 6 to nonparametrically estimate a survival function assuming real-time validation is possible. The method does not assume known mismeasurement rates of the surrogate outcome and allows for flexible censoring mechanisms.\nIn this article, we extend the work of Zee and Xie 8 to a semiparametric estimated likelihood method to estimate a parameter representing a binary covariate effect for discrete survival data with surrogate outcomes on all subjects and true outcomes on a subsample. Although we express our approach with a binary variable for ease of notation, the method can be easily modified for categorical variables with more than two levels. The rest of the article is organized as follows. We first describe the estimated likelihood and asymptotic properties for the estimated effect of a binary covariate. The ''Simulation study'' section contains results from testing the performance of our proposed estimator. In the ''Data example: effect of gender on time to pathological detection of AD'' section, we demonstrate the use of our method using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) to estimate the effect of gender on time to detection of pathological AD. Finally, we summarize our results and discuss implications of using our proposed method in the ''Discussion'' section."}, {"section_title": "Semiparametric estimated likelihood with a binary covariate", "text": ""}, {"section_title": "Maximum estimated likelihood estimation", "text": "We let T represent the true time to event and C represent the true right censoring time. The true observed time is represented by X = min{T, C} and true observed event indicator by d = I (T  C) . Similarly, the surrogate outcome is denoted with asterisks, with T * and C * representing the surrogate event and censoring times, respectively; X * = min{T * , C * } representing the surrogate observed time; and d * = I(T * C * ) representing the surrogate event indicator. We let X k represent the kth unique, ordered observed true time point for k = 1, ., K, where K is the total number of unique true observed times. Let F 0 represent the baseline survival function of the true time to event. We assume a proportional hazards model with F(t) = F 0 (t) exp(bZ) , where Z 2 {0, 1} is the binary covariate of interest, and b represents the log hazard ratio comparing Z = 1 to Z = 0. We assume that the covariate is available for all subjects. Finally, to allow for random censoring, we let G represent the censoring survival function.\nAs in the standard Cox model, we assume independent censoring conditional on covariates, and we allow the censoring mechanism to be fixed or random. Fixed, or type 1, censoring refers to a special case of administrative censoring where all subjects enter and leave the study at the same time, so censoring time is known at the start of the study and is equal for everyone. Random censoring refers to the situation where censoring time is unknown at start of study and may occur randomly due to circumstances such as loss to followup. A third type of censoring, sometimes known as generalized type 1 censoring, occurs when censoring time is known in advance, but subjects enter the study at random times. In this case, subjects who are censored have different, random observation times and are therefore also called randomly censored in this article. Because we assume real-time validation is possible, the true and surrogate outcomes can have different censoring times, which may occur if a subject drops out of only one part of a study. In the AD example, a subject may agree to both clinical and CSF screenings at the start of study, but after some time opt out of only the CSF screenings.\nHere, the subject's true censoring time may occur before the surrogate censoring time.\nLet V represent the validation set, the set of subjects for whom both the surrogate and true outcomes are available. Let V represent the non-validation set, in which only the surrogate outcome is available and the true outcome is missing. There are n total subjects in the sample and n V in V. We assume V is a representative sample of the entire study cohort, that is, those missing the true outcome are missing completely at random (MCAR). The estimated likelihood is a function of the log hazard ratio, b, and possible survival function values for the baseline event distribution and censoring distribution at each time point. In semiparametric survival analysis, the parameter of interest is often only the log hazard ratio, so the survival functions can be considered nuisance parameters. Using similar arguments as in Pepe 6 and Zee and Xie, 8 the estimated likelihood is given b\u0177\nThe outer sum is summed over all possible time points, and the inner sum is summed over all possible event indicators. The conditional probability is estimated empirically with proportion\u015d\nwhere I(\u00c1) is the indicator function. Given surrogate outcome and covariate values of a non-validation set subject and given each possible true outcome value, the estimated conditional probability is calculated by counting the number of subjects in the validation set with that set of values, out of the number of subjects in the validation set with that set of true outcome and covariate values. Therefore, the values observed in the validation set are used to estimate the association between the true and surrogate outcomes to determine the likelihood contributions of the non-validation set subjects. The form of the empirical probabilities assumes that the covariate values are important in estimating the relationship between the true and surrogate outcomes.\nor when the covariate is uninformative about the association between outcomes, the covariates can be removed from the indicator functions in the probability estimates.\nFor subjects i 2 V, the contribution to the likelihood is\nwhere x k i is the observed time for subject i. This expression is exactly what it would be for a standard proportional hazards model. For subjects j 2 V , the contribution to the likelihood i\u015d\nThe marginal distribution of the true outcome given covariates, P(x k , djZ j ), for the non-validation set has the same form as it did for the validation set. However, unlike in the validation set contribution, the outer sum prevents the censoring distribution from being factored out, which is necessary to obtain a consistent estimator for b in the presence of random censoring.\nAlthough the parameter of interest is often only b, we maximize the estimated likelihood jointly over all possible parameter values. As in the nonparametric case, the maximum estimated likelihood estimate for the event (censoring) survival function is a step function that falls only at event (censoring) times observed in the validation set. We solve for maximum estimated likelihood estimates using the Nelder-Mead algorithm with constraints on both survival functions to be monotonically non-increasing and bounded between 0 and 1. To obtain initial estimates for the event distribution parameters, we used the complete-case Kaplan-Meier estimates based on the true observed times and true event indicators from the validation set. Initial parameters for the censoring distribution were determined by the complete-case Kaplan-Meier estimates calculated by inverting the event indicator to obtain a censoring indicator. The initial parameter for the covariate effect is set at 0 and is unconstrained."}, {"section_title": "Asymptotic properties ofb", "text": "We assume that the proportion of subjects in the validation set out of the total number of subjects does not have a zero limit, or lim n!' n V =n = p V .0. Then, similar arguments as in Theorem 3.1 of Pepe 6 imply thatb is a consistent estimator for b as n!N and\nwhere s 2 is the [1,1] element of the full variance covariance matrix\nwhere I is the information matrix based on the (nonestimated) log likelihood and K is the expected conditional variance of the score function of the nonvalidation contribution to the log likelihood 6\nfor parameters u = {b, F 0 , G}. The first term in the S matrix represents the variance based on the maximum likelihood estimator, as in standard maximum likelihood analysis. The second term of S is needed in order to account for the additional variability introduced by estimating the likelihood with empirical probabilities. The I and K matrices can be estimated consistently by the expressions\nIn practice, we can calculate the estimated variance with numerical derivatives, analytical forms for derivatives, or using bootstrapping. As in the nonparametric case, we found that the numerical derivatives were sometimes incalculable with large amounts of missing data or a large number of parameters to estimate."}, {"section_title": "Simulation study", "text": "To test the performance of our proposed method in estimating a covariate effect, we conducted a series of simulations. We sampled values Z ; Bernoulli(0.5) for the binary covariate and set the log hazard ratio at b = 1. We sampled true event times assuming a proportional hazards model with baseline distribution, TjZ = 0 ; Unif [1, 5] , where survival time can only take integer values. For now, we assumed fixed right censoring at C = 4. The surrogate time to event was calculated as T * = T + e, where e ; Unif[0, z] and e is independent of T. The maximum integer value of the discrete uniform distribution for e was calculated as z = ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi\nwhere a b c represents the largest integer not greater than a and r represents the correlation between T and T * . We considered correlations of r 2 {0.01, 0.25, 0.50, 0.75, 1}. We set the right censoring time for the surrogate endpoint to be fixed also at C * = 4. To create a representative validation subsample, we simulated data MCAR by randomly selecting a proportion r 2 {0.25, 0.50} of the sample to be missing true outcomes.\nWe conducted 500 simulation repetitions for each set of parameter values and used a total sample size of n = 500 for each repetition. For each simulated dataset, we used the proposed method to calculate estimates of the log hazard ratio,b. We also calculated the complete-case estimate of the log hazard ratio using only available true outcomes in the validation set, the na\u0131\u00a8ve estimate using only surrogate outcomes from all subjects, and the true estimate using true outcomes from all subjects (which would not be possible with real data but is consistent and optimally efficient). For each of the three standard estimators, we used the maximum likelihood estimates (MLEs) rather than partial likelihood estimates. Although partial likelihood estimates are more common in practice, the MLEs are more accurate and compare better to our proposed method which is also an MLE method. For each method, we calculated estimated bias (parameter estimate 2 true parameter values), observed sample standard deviations (SDs), estimated standard errors ( c SE), relative efficiency (RE) compared to the true estimator (where lower RE implies greater efficiency and RE equal to 1 implies optimal efficiency), mean squared error (MSE) estimates, and 95% coverage (Cov). We note that for all simulations presented in Tables 1 and 2 , the observed sample SDs are similar to the standard error estimates calculated using the asymptotic theory for the proposed estimator. Table 1 shows the results from the simulations with fixed (type 1) censoring. The log hazard ratio estimates estimated by our proposed method and the completecase estimator always have little bias, whereas the na\u0131\u00a8ve estimates are biased whenever the correlation between outcomes is less than 1. In a few cases, the standard errors for our proposed estimator were slightly higher than the complete-case estimator due to the penalty that is added to the variance of our proposed estimator for estimating the likelihood. However, the penalty is small, and overall, our proposed estimator has similar standard errors compared to the complete-case estimator when the correlation between outcomes is low. When the correlation between outcomes increases, though, our proposed estimator is able to incorporate more information from the non-validation set subjects and therefore improves in efficiency, far surpassing the added penalty. At correlation of 1, which can be interpreted as the situation where we have a perfect surrogate, our proposed estimator has optimal efficiency. As we observed in the nonparametric case, the efficiency gains of our proposed estimator changed with different amounts of missingness and correlation between true and surrogate outcomes. We conducted additional simulations to explore and confirm this relationship by testing amounts of missingness between 0 and 80. We found that our proposed estimator was more efficient than the complete-case estimator when the missingness was low. With high missingness, our proposed estimator became less efficient than the complete-case estimator. The point at which our estimator crosses from more to less efficient differs with the correlation between outcomes-as correlation between true and surrogate outcomes increases, the amount of missingness at the crossing point increases. Sometimes the correlation between outcomes is unknown or unable to be estimated. Regardless of the correlation between outcomes, our simulations showed that our proposed estimator has similar or greater efficiency than the complete-case estimator when the amount of missingness is 50% or less of the total sample. This was consistent with previous work. 8 The previous simulations assumed measurement error was uniformly distributed and positive. We also conducted simulations assuming measurement error was geometrically distributed or uniformly distributed but could be positive or negative. Results are not shown but were similar to those seen above.\nAlthough we assumed true outcomes were MCAR, we also conducted additional simulations with data missing at random (MAR) to test the robustness of this assumption. To do so, we assumed that the probability of validation was p R 2 {0.60, 0.70, 0.85} for subjects who had a positive surrogate outcome, and the probability of validation for subjects who had a negative surrogate outcome was 1 2 p R . Under this scenario, we found that our proposed method and the complete-case method were both somewhat biased, particularly with higher values of p R . However, our proposed estimator was less biased and therefore had better coverage than the complete-case estimator with higher correlations between outcomes.\nWe also simulated data assuming random censoring and changed the amount of censoring by sampling true censoring times C from a uniform distribution. We considered a small amount of censoring (approximately 17%) using C ; Unif [3, 4] , a moderate amount of censoring (approximately 36%) using C ; Unif [1, 4] , and a large amount of censoring (approximately 84%) using C ; Unif [1, 2] . Surrogate censoring times were simulated by C * = C + g, where g ; Unif[0, 2]. The results of these random censoring simulations are shown in Table  2 . Similar to the results with fixed censoring, our proposed estimator and the complete-case estimator have little bias compared to the na\u0131\u00a8ve estimator, and our proposed estimator is more efficient than the completecase estimator at any amount of censoring.\nTo demonstrate the utility of our proposed method with real-time validation, we compared our proposed method to Magaret's. 7 We sampled event times assuming a baseline hazard rate of 2, a binary covariate, Z ; Bernoulli(0.5), and assumed a proportional hazards model with log hazard ratio of b = 0.70. Surrogate event times were calculated as T * = T + e, e ; Unif[0, 2]. We simulated fixed but unequal true and surrogate censoring times, C = 4 and C * = 5, and also simulated random censoring by sampling C from Unif [4, 5] , Unif [2, 5] , and Unif [1, 3] , resulting in approximately 25%, 35%, and 55% censoring, respectively. For random censoring, surrogate censoring times were calculated as C * = C + g, g ; Unif[0, 2]. We used total sample sizes of n 2 {240, 420, 630} with 50% missingness. For each simulated dataset, we used our proposed method and Magaret's method to calculate estimates of the log hazard ratio. Bias and observed sample variances for these comparisons are shown in Table 3 . Our proposed estimator has little bias; however, the method developed by Magaret 7 is biased when we have real-time validation."}, {"section_title": "Data example: effect of gender on time to pathological detection of AD", "text": "To illustrate our proposed method, we considered data (retrieved on 26 July 2013) from the ADNI study. 9 Participants in this observational study were assessed at predetermined time points for genetic, biomarker, and imaging markers related to AD. A description of the ADNI study is given in Online Appendix 1. In this study, participants who were non-AD at baseline were included (n = 186). Participants had to be non-AD by both a clinical and a CSF-based assessment at baseline to ensure all participants were event free for both outcomes at study entry. For the CSF-based outcome, Ab . 192 pg/mL was classified as non-AD and Ab 192 pg/mL was classified as AD. 1 CSF assessments were completely independent from clinical assessments.\nThe true outcome of interest was time to pathological detection of AD, measured in years. Since AD is a chronic disease with slow progression 10 and annual follow-up times were predetermined by study design, survival time was considered to be discrete. For every patient, the surrogate outcome of time to clinical AD or last follow-up was determined. A subset of 110 patients continued to have CSF assays performed after baseline, independently from clinical screenings, from which the true time to pathological AD or last follow-up was determined. Therefore, the validation set was approximately 59% of the total sample size. All patients in the study also had information on gender.\nUsing our proposed method, we estimated the log hazard ratio,b, of AD in females compared to males. We also estimated the log hazard ratio with the complete-case estimator using only 110 CSF diagnoses and the na\u0131\u00a8ve estimator using only 186 clinical diagnoses. For the standard estimators, we conducted estimation using both the maximum likelihood method and the more widely used partial likelihood method with Efron's 11 approximation for ties. Table 4 shows the log hazard ratio and standard error estimates for gender. Both our proposed estimator and the completecase estimator found a small positive log hazard ratio comparing females to males, which is similar to some literature indicating higher incidence of AD in women. 12, 13 However, the na\u0131\u00a8ve estimate is large and negative. In this particular example comparing genders, the estimated standard errors from our proposed method and the complete-case method were similar."}, {"section_title": "Discussion", "text": "We extended the nonparametric estimated likelihood method for data with surrogate endpoints and an internal validation subsample to the proportional hazards model with a binary covariate. Our method allows for real-time validation and flexible censoring mechanisms. Our proposed log hazard ratio estimator is consistent and asymptotically normal. Through simulation studies, we found that our proposed estimate is unbiased and its variance decreases as correlation between the surrogate and true outcome increases. By using both surrogate and true endpoints, the proposed covariate effect estimator can outperform both complete-case and na\u0131\u00a8ve estimators.\nAs in the nonparametric case, we found that our proposed estimator behaves similarly to the complete-case estimator when the correlation between the uncertain and true outcomes is low. As correlation increases, the non-validation set subjects contribute more information and therefore decrease the variance of the parameter estimate by providing more power. When correlation between outcomes is 1, or when the surrogate outcome has no measurement error, our proposed estimator reduces to the MLE based on complete true outcomes (no missingness). Therefore, our proposed method is most useful when correlation between outcomes is high.\nIn this study, we evaluated the use of an estimated likelihood method for survival data with a single binary covariate. The method can easily be extended to consider multiple covariates, which would be useful in order to adjust for confounding variables or to consider categorical variables with more than two levels. Further study on the number of allowable covariates is warranted; however, based on the events per variable (EPV) testing in Zee and Xie, 8 it is expected that a similar EPV of 4 would apply to multivariable models. This would imply that a minimum of four events should be observed for each parameter to be estimated. For continuous covariates, however, a modified approach must be developed, since likelihood contributions would often be 0 for non-validation set subjects when using the current method. An estimated likelihood method that incorporates smooth kernel functions in the empirical probability estimates of the likelihood function is currently under investigation. Additionally, other applications may involve interval censoring, for which our methods would need to be modified to be suitable.\nOptimal study design strategies are currently under investigation, in order to determine the total size of the sample and size of the validation subset that would be needed to design new trials with these data characteristics. For example, in clinical trials that aim to evaluate a treatment effect, it is valuable to calculate sample sizes that would be needed in each treatment group to achieve a pre-specified power to detect the effect size. The improvements in efficiency that can be obtained from using the proposed method would decrease the number of true outcomes needed compared to using standard methods.\nDue to the difficulty in obtaining true outcomes on many subjects, the methods we have proposed have useful applications in clinical trials. Designing studies such that surrogate outcomes are collected on all patients and true outcomes only collected on a subsample of patients can save on trial costs and ensure that an adequate number of patients are enrolled. Using our proposed semiparametric estimated likelihood method to analyze these data can provide accurate and powerful statistical inference to evaluate treatment effects."}]