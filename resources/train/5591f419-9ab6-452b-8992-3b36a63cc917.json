[{"section_title": "Abstract", "text": "Structural MRI(sMRI) imaging is used to extract morphometric features after Grey Matter (GM), White Matter (WM) for several univariate and multivariate method, and Cerebro-spinal Fluid (CSF) segmentation. A new approach is applied for the diagnosis of very mild to mild AD. We propose the classification method of Alzheimer disease patients from normal controls by combining morphometric features and Gaussian Mixture Models parameters along with MMSE (Mini Mental State Examination) score. The combined features are fed into Multi-kernel SVM classifier after getting rid of curse of dimensionality using principal component analysis. The experimenral results of the proposed diagnosis method yield up to 96% stratification accuracy with Multi-kernel SVM along with high sensitivity and specificity above 90%.\n\u2022"}, {"section_title": "", "text": "Structural MRI(sMRI) imaging is used to extract morphometric features after Grey Matter (GM), White Matter (WM) for several univariate and multivariate method, and Cerebro-spinal Fluid (CSF) segmentation. A new approach is applied for the diagnosis of very mild to mild AD. We propose the classification method of Alzheimer disease patients from normal controls by combining morphometric features and Gaussian Mixture Models parameters along with MMSE (Mini Mental State Examination) score. The combined features are fed into Multi-kernel SVM classifier after getting rid of curse of dimensionality using principal component analysis. The experimenral results of the proposed diagnosis method yield up to 96% stratification accuracy with Multi-kernel SVM along with high sensitivity and specificity above 90%."}, {"section_title": "Introduction", "text": "Several noninvasive efficient diagnosis methods can be applied like structural or functional magnetic resonance imaging (sMRI or fMRI), Single Photon Emission Computerized Tomography (SPECT), and Position Emission Tomography(PET) [1, 2] . Several studies have unveiled that AD can be stratified from MCI and healthy controls(HC) by applying automatic or semi-automatic measurement of diversified priori brain Region of Interest (ROI). AD causes significant cerebral atrophy in several brain ROIs, especially the entorhinal cortex and hippocampus [3] . The regional volume of several ROIs has morphometric differences. The CAD tool has been designed [4] applying SPECT and PET image using NMF and SVM classifier for the classification of AD patients from healthy controls yielding up to 91% accuracy with high sensitivity and specificity rates (above 90%). M. L\u00f3pezet al [5] developed PCA and Bayesian rule based tool for the diagnosis of AD patients from normal controls using SPECT and PET image dataset stratifying 98.3% accurately for SPECT and 88.6% accurately for PET dataset. The detection tool to identify CDR scale of entire 457 MR images has been developed using PCA and Artificial Neural Network, and classification accuracy achieving up to 89.92%. Multimodal approach [7] has been applied to discriminate AD and MCI patients from healthy controls using sMRI, FDG-PET, and CSF (cerebro-spinal fluid) as biomarkers. The stratification accuracy yielded up to 93.2% for AD vs HC and 76.4% for AD vs MCI. They have achieved accuracy of 86.5%, 72% differentiating AD from HC controls and MCI from HC respectively using best individual modality biomarker among them. Independent Component Analysis based AD classification [8] from HC controls has been proposed using both all gray matter images and whole brain images from the OASIS dataset.\nThe classification accuracy they got is 73.7% \u00b1 4.5% of gray matter images from OASIS, and 71.1% \u00b1 4.5% of gray matter images from the ADNI. The discriminating accuracy of AD from MCI [9] has yielded up to 88.89% using SOM and PSO-SVM. Four different models of Artificial Neural Networks (ANNs) [10] have been applied to the same dataset, and 83% stratification accuracy has been achieved. Several univariate and multivariate method can be applied while Gray Matter (GM), White Matter (WM), and Cerebro-spinal Fluid (CSF) regions are being segmented. The preprocessing steps for VBM feature extraction follows Spatial Normalization, Segmentation, Smoothing, and Statistical Parametric Mapping. In this method, PCA based VBM and GMM feature along with MMSE score, and Multi-Kernel Support Vector Machine (ML-SVM) with train and test strategy is applied to classify new unknown subject."}, {"section_title": "System model and methods", "text": "The sMR images used here are spatially normalized into standard template with the help of SPM8 toolbox. Then images are segmented into GM, WM, and CSF. Then next steps follow smoothing of GM images and voxel-wise statistical tests to extract features. The feature along with GMM components and MMSE score are fed into multi-kernel SVM after applying two sample t-test and PCA for dimensionality reduction."}, {"section_title": "Overview of the experimental data", "text": "All sMRI images are collected from OASIS cross-sectional dataset. Ninety-eight women MRI data has been selected from there. Among them, 49 patients are diagnosed with very mild to mild AD and rest 49 patients are non-demented controls. The age, education, socio economic status, CDR, and MMSE score details of those are depicted in Table 1 . The CDR details is summarized in Table 2 ."}, {"section_title": "Preprocessing and VBM feature extraction", "text": "The different steps of feature extraction process are depicted in Fig 1. The feature extraction process is illustrated elaborately by Darya Chyzhyk et al [11] . VBM toolbox has been used for preprocessing. The OASIS cross sectional MR images have been already averaged, registered, resampled with 1-mm isotropic image in atlas space and bias field corrected there.\nThe images are realigned with template image, spatially normalized, segmented to get GM components, and then spatially smoothed with FWHM of the Gaussian kernel to 10mm isotropic prior to analyze voxel base statistics. A GM mask has been formed with the mean of GM segmentation volumes of the 98 subjects. The binary mask was generated by using thresholding operation on the average GM segmented volumes consisting of all voxels with probability greater than 0. Gaussian mixture model is a histogram probabilistic model, and can also be considered as an estimation of probability density. Image histogram reflects the frequency appears in a certain gray value. Given a grayscale image with N pixels divided into S different regions, so it depicts S peaks in the histogram. Thus, the image can be expressed by these S statistical models. Optimum thresholding for image segmentation problem is achieved by estimating the parameters of GMM. GMM based features provides promise accuracy to distinguish between target and ghost/clutter regions [12] .\nGMM is a parametric probability density function represented as a weighted sum of R Gaussian component densities,\nwhere, \ue0fc is the N dimensional continuous valued data vector, \ue0fb \ue0ed is the mixture weight, and \ue0eb is the component Gaussian densities.\nEach component density is evaluated by the N-variate Gaussian function,\nwhere, \ue0f1 \ue0ed is the average of a vector, \ue013 is the transpose, and \ue067 \ue0ed is the covariance matrix. Using mean vectors, covariance matrices, and mixture weights from all component densities, the GMM was parameterized. These parameters are denoted as,\nThe variance (\ue015 \ue0ed ) of components is extracted from the diagonal of the covariance matrix. The feature vector from three components of GMM,\nwhere, \ue0fb, \ue0a8, and \ue0fa are the weight, average, and variance of GMM components."}, {"section_title": "PCA and MK-SVM Classifier", "text": "In this paper, a noble approach is applied combining VBM feature and GMM parameters along with MMSE score, reducing the curse of dimensionality by using PCA, differentiating very mild AD to mild AD from healthy controls with Multi-kernel SVM as shown in Fig 2. "}, {"section_title": "Principal component analysis", "text": "In this method, PCA is used to reduce the dimensionality of higher dimensional featured after performing two sample t-tests. The PCA is summarized as -Finding mean of the data matrix and zero mean matrix. -Constructing covariance matrix. -Getting the eigenvalue and the eigenvector. -Projecting the data matrix with eigenvectors corresponding to highest to lowest eigenvalues."}, {"section_title": "Multi-Kernel SVM", "text": "Support Vector Machine is supervised learning model, and it performs efficiently for binary classification of both linearly separable and non-separable data. Kernel trick is applied to make linearly non separable data to separable. The algorithm is about to get optimum hyperplane between two classes which has optimum distance form support vectors of both the classes. While it needs to take decision for new data point, classifier does that based on the hyperplane.\nThe kernel mapping function,\n\ue0ea \ue044\ue0ea \ue056 \ue0e8\ue045 , translates the feature data to higher dimensional space so that it becomes linearly separable. The linear decision boundary is\nLet consider the training set \ue04b \ue044\ue0fc \ue0ed \ue052 \ue0fd \ue0ed \ue045 \ue04c \ue0ed \ue047 \ue034 \ue0f2 , where\nis training sample and its corresponding class level is \ue0fd \ue0ed \u2208\ue04b\ue046 \ue034\ue052\ue048 \ue034\ue04c, for SVM with L1 soft margin regularization can be solved with the primal problem,\nwhere \ue013 is the trade-off parameter of training error and margin, \ue0b3 \ue034\ue043 \ue0f2 is slack vector having non-zero elements.\nThe Lagrange dual space optimization solution is\nwhere, \ue0ef is the kernel matrix \ue09d \ue0ed , \ue09d \ue0ee are the Lagrange multipliers.\nMulti-kernel approach combining various kernels such as polynomial, Gaussian, sigmoid kernel can also be applied instead of using single kernel. Convex combination of multiple kernels is considered where each kernel has its kernel weight based on their discriminative power.\nwhere, \ue09e \ue0f1 is the weight of m-th kernel.\nMulti kernel learning Support Vector Machine can be solved in primal problem,\n(9) The solution of optimization problem in Lagrange dual space, To integrate multiple kernels instead of using single kernel [13] , [14] , [15] , the program solution can be defined with L2 regularizations as The decision function is\nIn our proposed, polynomial, Gaussian, and sigmoid kernel are integrated. All the convex problem of kernel learning and muli-kernel learning is solved by second order cone programming (SOCP) using CVX and Sedumi package [16] for optimizing over symmetric cones. The kernel weights are calculated in dual space, and it provides importance of each kernel to distinguish AD from HC, or AD from MCI. RBF kernel SVM applied here is solved using LibSVM package [15] ."}, {"section_title": "PCA and MK-SVM Classifier result", "text": "To train MK-SVM for differentiating very mild to mild AD from normal controls, training samples are selected and tested the performance using selected testing samples. For drastically reducing the dimension of feature, PCA is applied on it. The supervised learning of GM segmented image features and GMM parameters along with clinical measurement (MMSE score) is used for diagnosis of AD patients. Multi-Kernel SVM is applied, and the accuracy and efficiency are measured with MMSE score or without the MMSE score. I have come across that when MMSE score is incorporated with GM segmented image features and GMM parameters, the classification accuracy, sensitivity, and specificity gets higher as portrayed in Table 3 ."}, {"section_title": "Conclusions", "text": "A diagnosis method stratifying normal controls from very mild to mild AD is unveiled here. The noble method is enhanceded by performing PCA which reduces the dimensionality of feature space significantly and selecting the most three principal components in terms of ability to differentiate for training and testing Multi kernel SVM for the diagnosis. We obtained almost 96% accuracy with high sensitivity and specificity which outperforms the classification accuracy of 83% obtained by Savio A. et al [10] , 73.7% \u00b1 4.5% by WenluYanga et al [8] , and 89.92% by R. Mahmood et al [6] . "}]