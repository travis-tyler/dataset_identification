[{"section_title": "Base Year Sample Design Report Preface", "text": "The purpose of this technical report is to document the sampling procedures and the results of data collection for the National Education Longitudinal Study of 1988 (NELS:88) base year survey of eir,hth graders. This version of the NELS:88 Base Year Sample Design Report is an abridgement, prepared by Kathryn L. Dowd, of a more extensive contractor report (Spencer et al., 1989) on the base year sample design. In accordance with the confidentiality provisions of Public Law 100-297, it was necessary to abridge the report in order to avoid reporting any information that could potentially be nd to statistically disclose school identities. This version of the report is designed to be used as a companion to the NELS:88 Bnse Year Data File User's Manuals and is intended specifically to provide additional documentation on sampling issues that may be of interest to users of the public release data tapes. Copies of the data collection instruments; a description of the data collection, preparation, and processing procedures; and a guide to the data files and codebook, can be found in the four (student, parent, teacher and school administrator) NELS:88 Base Year Data File User's Manuals (Inge ls et al., 1990/a,b,c,d). The base year data tapes are available from the National Center for Education Statistics."}, {"section_title": "iii", "text": "Base Year Sample Design Report"}, {"section_title": "Executive Summary", "text": "The purpose of this technical report is to document the sampling procedures and the results of data collection for the National Education Longitudinal Study of 1988 (NELS:88) base year survey of eighth graders. Chapter 1 gives an overview of the NELS:88 base year survey. Chapter 2 summarizes the base year sample selection procedures and gives the results of data collection. Chapter 3 describes the calculation of sample case weights and the adjustment of the weights for nonresponse. Chapter 4 examines survey and item nonresponse. Chapter 5 describes procedures for computing sampling errors and design effects. Following is a brief summary of the major contents of the report: The National Education Longitudinal Study of 1988 is a survey of the school-related experiences and accomplishments of a nationally representative sample of eighth graders. The target population consisted of all public and private schools containing eighth grades in the fifty states and District of Columbia. Included in the NELS:88 sample is a supplementary sample of Hispanic and Asian/Pacific Islander students (and their parents and teachers) sponsored by the Office of Bilingual Education and Minority Language Affairs (OBEMLA), and a supplement of hearing-impaired children for which additional audiological data were obtained. The student population excludes students with severe mental handicaps, students whose command of the English language was not sufficient for understanding the survey materials (especially the cognitive tests), and students with physical or emotional problems that would make it unduly difficult for them to participate in the survey (5.35 percent of the potential student sample). Because the excluded students from the base year are a possible source of undercoverage bias, plans have been made to follow a substantial subsample of them in the NELS:88 first and second follow-ups. The NELS:88 survey used a two-stage stratified, clustered sample design. At the first stage, about 69 percent of the initially selected schools participated. School administrator data was obtained from ninety-eight percent of the participating schools. At the second stage, about 93 percent of the sampled students agreed to participate. Roughly equally high percentages of teachers and parents of the participating students also agreed to take part in the survey. Weights for school administrators and students were adjusted to compensate for nonresponse. Separate weights were not provided for parents or teachers. School-level response rates were lower for public schools and for non-Catholic private schools. These results are similar to those in the High School and i3eyond Base Year sample. School response rates were somewhat higher for urban schools than for suburban or rural schools, in contrast to High School and Beyond where urban schools were the least likely to participate. Analysis of design effects indicates that the NELS:88 sample was slightly more efficient than the High School and Beyond sample. Based on student questionnaire data for all students, the average design effect in NELS:88 was 2.54; the comparable figure was 2.88 for the High School and Beyond sophomore cohort and 2.69 for the senior cohort. This difference is also apparent for subgroup estimates, especially for students attending Catholic schools. In NELS:88, the average design effect for Catholic schools is 2.70; in High School and Beyond, it was 3.60 for the sophomores and 3.58 for the seniors.   "}, {"section_title": "Introduction", "text": "The National Education Longitudinal Study of 1988 (NELS:88) base year survey was conducted during the winter, spring, and summer of 1988. This report provides information that fully documents major technical aspects of the sample selection and implementation, describes the weighting procedures, examines the possible impact of nonresponse on sample estimates, and evaluates the precision of estimates derived from the sample."}, {"section_title": "1.1", "text": "Overview of the National Education Longitudinal Study of 1988 1.1.1 NCES's Longitudinal Studies Program The U.S. Department of Education's National Center for Education Statistics (NCES) is mandated to \"collect and disseminate statistics and other data related to education in the United States\" and to \"conduct and publish reports on specific analyses of the meaning and significance of such statistics\" (Education Amendments of 1974-Public Law 93-380, Tide V, Section 501, amending Part A of the General Education Provisions Act). Consistent with this mandate and in response to the need for policy-relevant, time-series data on nationally representative samples of elementary and secondary students, NCES instituted the National Education Longitudinal Studies (NELS) program, a continuing long-term project. The general aim of the NELS program is to study the educational, vocational, and personal development of students at various grade levels, and the personal, familial, social, institutional, and cultural factors that may affect that development. The NELS program currently consists of three major studies: The National Longitudinal Study of the High School Class of 1972 (NLS-72), High School and Beyond (HS&B), and the National Education Longitudinal Study of 1988 (NELS:88). Figure 1-1 illustrates the increasing number of issues that have become part of NCES's National Education Longitudinal Studies research agenda. A brief description of these studies is followed by a review of NELS:88. 1.1.2 The National Longitudinal Study of the 1970s  The first of the NELS projects, the National Longitudinal Study of the High School Class of 1972 (NLS-72), began in the spring of 1972 with a survey of a national probability sample of 19,001 seniors from 1,061 public, private, and church-affiliated high schools. The sample was designed to be representative of the approximately three million high school seniors in more than 17,000 schools in the spring of 1972. Each sample member was asked to complete a student questionnaire and a 69-minute test battery. School administrators were also asked to supply survey data on each student, as well as information about the school's programs, msources, and grading system. At the time of the first follow-up, an additional 4,450 students from the class of 1972 were added to the sample. Five followups,-conducted in 1973Five followups,-conducted in , 1974Five followups,-conducted in , 1976Five followups,-conducted in , 1979Five followups,-conducted in , and 1986 have been subsequently completed. For the Fifth Follow-Up a subsample consisting of 14,489 of the 22,652 student.% who participated in at least one of the five previous waves were interviewed. In addition to background information, the NLS-72 base year and follow-up surveys collected data on respondents' educational activities, such as schools attended, grades received, and degree of satisfaction with their educational institutions. Participants were also asked about work experiences, periods of unemployment, job satisfaction, military service, marital status, and children. Attitudinal information on self-ooncePt, goals, participation in political activities, and ratings of their high schools are other topics for which respondents have supplied information. hence also comparability to the High School and Beyond and NLS-72 samples--will be maintained by sample \"freshening\" in the first (1990) and second (1992) follow-ups of NELS:881. The NELS:88 base year survey comprises four components. First, the study examines characteristics of the school itself, providing data on admissions and academic policies, school climate, and teacher and student composition. Second, the study examines students' school experiences, both in terms of their own reports and in terms of reports of teachers. The teachers' reports contain substantial detail about classroom instructional practices. Finally, the study provides data on the student's family and home experiences. This is done frst by obtaining students' reports but is supplemented and enhanced by interviewing parents. Wluie the previous longitudinal education studies have obtained some information from teachers and parents for subsamples of students, NELS:88 provides extensive information from these sources for all students."}, {"section_title": "1.2", "text": ""}, {"section_title": "Overview of Chapters 2 through 5", "text": "Chapter 2 summarizes the ba s!.. year sample selection procedures, including procedures for oversampling, stratification, and sample allocations. Chapter 3 describes the calculation of sample case weights that adjust for differential probabilities of selection and for nonresponse within weighting cells. Chapter 4 examines the possible impact of survey nonresponse, a potential source of bias. The amount of bias depends on the proportion of nonrespondents and the magnitnde of any difference between respondents and nonrespondents on variables of interest. Often in surveys it is impossible to estimate accurately the amount of bias because, although the proportion of nonrespondents is known, there is usually no satisfactory way to estimate the difference between respondents and nonrespondents. Fortunately, we were able to collect background information on a substantial proportion of nonresponding base year schools, providing a basis for studying the impact of school nonresponse. chapter 4 provides the details for this analysis. We also report extensive item nonresponse analysis for the student questionnaire and cognitive tests. BeL.ause item nonresponse on the teacher and school I While most NELS:88 eighth graders will be in tenth grade in 1990, it should be noted that some students who were in the eighth grade in 1988 will not be in schoel in 1990, while other 1988 NELS:88 eighth graders will be in a grade other than the tenth grade in the spring of 1990. Moreover, the population of students enrolled in the tenth grade in 1990 contains students who were not in the eighth grade in 1988. Sample freshening will give 1990 tenth graders who were not in the eighth grade in 1988 some chance of selection into the NELS:88 rust follow-up survey, so that the first follow-up sample may represent tenth grade students in the United States in the 1989-1990 schoolyear. A four step freshening procedure will be used to ensure that a valid probability sample of all students enrolled in the tenth grade in 1990 is achieved: 1) For each school that contains at least one base year tenth grade student selected for interview in 1990 a complete alphabetical roster of all tenth grade students will be obtained; 2) An examination will be made of the student immediately following the selected base year student on the roster. If the base year student is last on the roster, the examination will be undertaken for the first student on the roster; 3) If the student designated for examination is enrolled in the eighth grade in the United States in 1988 the process will terminate for that school. If the designated student is not enrolled in eighth grade he or she will become part of the freshened sample; 4)If a student is added to the freshened sample step 3 will be applied to the next student listed on the raster. The step 3 and 4 sequence will be repeated (and students added to the sample) until a student who was in the eighth grade in the United States in 1988 is reached on the roster. Assuming that the tenth grade rosters are comg1ete, this method will generate a prwability sample of tenth grade students who were not enrolled in the eighth grade nited States in 1988. The procedure explicitly \"links\" each tenth grade student not in the eighth grade in 1988 with and only one student who was in the eighth grade in 1988. Thus students in the former population have a known -zero probability of selection, a probability sample of the elements (students) of this population is achieved, and a \"freshening\" sample is obtained to add to the NELS:88 eighth grade cohort sample members who have been followed in 1990."}, {"section_title": "5 4", "text": "Base Year Sample Design Report 1.1.3 High School and Beyond of the 1980s (HS&B) The next major longitudinal study sponsored by NCES was High School and Beyond (HS&B). HS&B was initiated in order to capture changes that had occurred in educational and social conditions, federal and state programs, and needs and charareristics of students since the time of the earlier survey. Such changes have been particularly prominent over the succeeding decade and are clearly continuing. Thus, HS&B was designed to maintain the flow of educational data to policy makers at all levels who need to base their decisions on information that is reliable, relevant, and current. Base year data collection was conducted in the spring of 1980. As in NLS-72, students were selected using a two-stage probability sample with schools as the first-stage units and students within schools as the second-stage units. There were 1,015 public, private, and church-affiliated secondary schools in the sample and a total of 58270 participating students (sophomores and seniors). Unlike NLS-72, HS&B cohorts included both tenth graders and twelfth graders. Since the base year data collection in 1980, three follow-ups of the HS&B cohorts have been completed, one in the spring of 1982, one in the spring of 1984, and the last in the spring of 1986."}, {"section_title": "National Education Longitudinal Study of 1988", "text": "The National Education Longitudinal Study of 1988 (NELS:88) is the most recent in a series of longitudinal studies conducted by the National Cemer for Education Statistics (NCES) at the U.S. Department4f Education. As in the preceding studies, students were selected using a two-stage probability sample with schools as the first-stage units and students within schools as the second-stage units. The NELS:88 survey obtained participation from 1,057 public, private, and church-affiliated secondary schools and 24,599 participating students. Similar to the previous longitudinal education studies, NELS:88 begins with a baseline assessment of school experiences, with the purpose of relating these experiences to current academic achievement and to later achievement in school and in life. However, NELS:88 has been designed with, a number of enhancements that will increase the analysis and pilicy-informing potential of the NELS:88 data. Like the two preceding longitudinal studies conducted by the NCES, the National Longitudinal Study of the High School Class of 1972 and the High School and Beyond study of the 1980 sophomore and senior cohorts, NELS:88 examines school experiences of a national probability sample of students. Unlike the two previous studies, NELS:88 begins with a survey of eighth graders. This focus, combined with a series of planned follow-up surveys of the NELS:88 sample, will enable a longitudinal data base to be created that will give researchers the opportunity to study the ways eighth grade experiences affect high school performance and relate to high school completion. Because the majority of students who drop out are still enrolled in school during the eighth grade, the NELS:88 base year survey will also provide researchers with baseline data for a representative sample of the majority of future high school dropouts in this age cohort. Representativeness of the NELS:88 student sample--1 4 Base Year Sample design Report administrator questionnaires was extremely low (typically around 1 percent), we did not undertake an analysis of it. We do, however, report summary nonresponse statistics for the parent questionnaire. Chapter 5 describes procedures for computing sampling errors and design effects. Because it is clustered, stratified, and disproportionately alloc.ted, the NELS:88 base year samp1e presents some special difficulties in estimating sampling errors. Chapter 5 discusses the approach NORC has taken to solve this problem. Sampling errors and design effects are presented for a number of variables for both the entire sample and for important domains or subgroups. Finally, several \"rules of thumb\" are offered for estimating standard errors under various circumstances."}, {"section_title": "6 5", "text": "Base Year Sample Design Report 2."}, {"section_title": "Sample Design and Implementation", "text": ""}, {"section_title": "Base Year Survey Sample Design", "text": "The sample design for NELS:88 is similar in many respects to the designs used in the two prior studies of the National Education Longitudinal Studies Program, the NLS-72 and High School and Beyond. The principal difference between NELS:88 and these other two studies is tlut in its base year NELS:88 sampled a cohort of eighth graders rather than high school students. Included in the NELS:88 sample is a supplementary sample of Hispanic and Asian/Pacific Islander students (and their parents and teachers) sponsored by the Office of Bilingual Education and Minority Language Affairs (OBEMLA). A few states contracted separately to supplement the NELS:88 sampled schools in their state with additional schools for the purpose of obtaining reliable state estimates. In describing the sampling it is sometimes necessary to refer to these schools in this document even though most of these additional schools are not represented in the public use data files. When these additional schools are mentioned, they will be referred to as \"augmentation schools\" or \"state sample augments; tions\". In the base year survey of NELS:88, students were sampled through a two-stage process. First, stratified random sampling and school contacting resulted in the identification and contacting of 1,655 eligible public and private schools from a universe of apprrcimately 40,000 schools containing eighth grade students (see chapter 2 for a discussion of school eligibility and for a discussion of how sampled schools were divided into primary and secondary, or backup, sampling pools). Of the eligible schools contacted, 1,057 participated in the survey. A full discussion of the sampling plan and response rates is presented in chapters 2 and 3. The principal, headmaster, or headmistress of each of these schools was asked to provide school-level information for the bithool-based component of the survey. The second stage included random selection of about 26 students per school (on average, 24 regularly sampled students and 2 OBEMLA supplement Hispanic and Asian/Pacific Islander students) from these cooperating schools. The number of students sampled in each school ranged from I (6 schools) to 73 (1 school). Owing to the greater representation of small private schools, and to the impact of a within-school strategy of oversampling Hispanics (and Asians), there is considerably greater variability in within-school sample size in the NELS:88 base year sample than in the HS&B base year sample. The target population for the base year consisted of all public and private schools containing eighth grades in the fifty states and District of Columbia. Excluded from the NELS:88 sample are Bureau of Indian Affairs (BIA) schools, special education schools for the handicapped, area vocational schools that do not enroll students directly, and schools for dependents of U.S. personnel overseas. The student population excludes students with severe mental handicaps, students whose command of the Englishlanguage was not sufficient for understanding the survey materials (especially the cognitive tests), and students with physical or emotion s'. problems that would make it unduly difficult for them to participate in the survey."}, {"section_title": "Exclusions From the Sample", "text": "Exclusion of students. To better understand how excluding students with mental handicaps, language barriers, and severe physical and emotional problems affects population inferences, data were obtained on the numbers of students excluded as a result of these restrictions."}, {"section_title": "7 6", "text": "Base Year Sample Design Report Seven ineligibility codes defining categories of excluded students were employed at the time of snident sample selection: Aattended sampled school only on a part-time basis, primary enrollment at another school. Bphysical disability precluded student from filling out questionnaires and taking tests. Cmental disability precluded student from filling out questionnaires and taking tests. Ddropout: absent or truant for 20 consecutive days, and was not expected to return to school. Edid not have English as the mother tongue AND had insufficient command of English to complete the NELS:88 questionnaires and tests. Ftransferred out of the school since roster was compiled. Gwas deceased. The designation of students as ineligible could occur either before sampling or after sampling but before or on survey day. Before sampling, school coordinators were asked to examine the school sampling roster and annotate each excluded student's entry by assigning one of the exclusion codes. Because eligibility decisions were to be made on an individual basis, special education and Limited English Proficiency (LEP) stne'nts were not to be excluded categorically. Rather, each student's case was to be reviewed to determine the extent of limitation in relation to the prospect for meaningful survey participation. Each individual student, including LEPs and physically or mentally handicapped students, was to be designated eligible for the survey if school staff deemed the student capable of completing the NFLS:88 instruments, and excluded if school staff judged the student to be incapable of doing so. School coordinators were told that when there was doubt, they should consider the student capable of participation in the survey. Exclusion of students after sampling occurred either during the sample update or on survey day. Such exclusion after sampling normally occurred because of a change in student status (for example, transfer or death) although in rare instances such exclusions reflected belated recognition of a student's pre-existing ineligibility. Regardless of when an exclusion designation occurred, excluded students were divided into those who were full-time students at the school (categories B, C, and E) and those who were not (categories A, D, F, & G). Our main concern here is with students who were full-time students at the school but were excluded from the sample, because excluding these students may have an effect upon estimates made from the sample. Students in categories A (n=329), D (n=733), F (n=3,325), and G (n=6) were either not at the school or were present only part time (with primary regristration, hence a chance of selection into NELS:88 associated, at another school). Thus excluding them has no implications for making estimates to the population of eighth grade students. It should be noted that students in category F, those who had transferred out of the sampled school, had some chance of being selected into the sample if they transferred into another NELS:88 sampled school just as transfers into NELS:88 schools from non-NELS:88 schools had a chance of selection at the time of the sample update. The sampling of transfer-in students associated with the sample update allowed us to represent transfer students in the NELS:88 sample. It should also be noted that a follow-up study NORC conducted of the students designated as dropouts by the school coordinators suggested that most of the Base Year Sample Design Report students originally designated as dropouts had actually transferred to another school (for details, see Appendix E of the NELS:88 Base Year: Student Component Data File User's Manual).2 Thus, as in the cue of students properly designated as transfers out, all but the 29 true dropouts (as identified by the NORC dropout study) had a chance of being selected into the sample. Figure 2-1 gives the number and percentage of students who fall into each of the three exclusion categories (B, C, and E) that may have implications for estimates drawn from base year sample and subsequent study waves. The total eighth grade enrollment for the NELS:88 sample of schools was 202,996. Of these students, 10,853 were excluded owing to limitations in their language proficiency or to mental or physical disabilities. Thus 5.35 percent of the potential student sample (the students enrolled in the eighth grade in the 1,052 NELS:SS schools from which usable student data were obtained) were excluded. Less than one half of one percent of the potential sample was excluded for reasons o: physical or emotional disability (.41 percent), but 3.04 percent was excluded for reasons of mental disability, and 1.90 percent because of limitations in English proficiency (we estimate that this is about 45 percent of the total number of LEP students in the schools). Put another way, of the 10,853 excluded students, about 57 percent were excluded for mental disability, about 35 percent owing to language problems, and less than 8 percent because o; physical or emotional disabilities. Because current characteristics and probable future educational outcomes for these groups depart in many ways from the national norm, the exclusion factor should be taken into consideration in generalizing from the NELS:88 sample to eighth graders in the nation as a whole. This implication for estimation carries to future waves. For example, if the overall propensity to drop out between the eighth and tenth grades is twice as high for excluded students as for non-excluded students, the dropout figures derivable from the NELS:88 First Follow-Up (1990) study would underestimate these early dropouts by about ten percent. It should be noted that in a school-based longitudinal survey, such as NELS:88, excluded students have a second implication for future waves, in addition to their possible impact on estimation. To achieve a thoroughly representative tenth grade (1990) and twelfth grade (1992) sample comparable to the High School and Beyond 1980 sophomore cohort (or, for 1992, the HS&B 1980 senior cohort and the base year of NLS-72), the follow-up samples must approximate those which would have come into being had a new baseline sample independently been drawn at either of the later time points. In 1990 (and 1992) one must therefore freshen, to give \"out of sequence\" students (for example. in 1990, those tenth graders who were not in eighth grade in the spring of 1988) a chance of selectioi,i into the study. But also one should ideally accommodate excluded students whose eligibility status has changed--for they too (with the exception of those who fell out of sequence in the progression through grades) would potentially have been selected had a sample been independently drawn two years later, and must have a chance of selection if the representativeness and cross-cohort comparability of the follow-up sample is to be maintained. Thus, for example, if a base year student excluded because of a language barrier achieves the level of proficiency in English that is required for completing the NELS:88 instruments in 1990 or 1992, that student should have some chance of re-entering the sample. It is planned, subject to availability of funds, to follow a substantial subsample of the base 20 21 year ineligibles in the NELS:88 first and second follow-ups, and to reassess their eligibility status and gather information about their demographic characteristics, educational paths, and life outcomes. Data on persistence in school to be obtained from this subsample will be used to derive an adjustment factor for national estimates of the eighth grade cohort's dropout rates between spring of 1988 and spring of 1990, and later, between 1990 and 1992. Exclusion of schools. Just as certain students were considered to be ineligible, so too certain kinds of schools were ineligible for selection. The eligible populations of schools are restricted to \"regular\" schools in the U.S., private as well as public. Excluded from the sample are Bureau of Indian Affairs (BIA) schools, special education schools for the handicapped, area vocational schools that do not emull students directly, and schools for dependents of U.S. personnel overseas. Additionally, a sample list school was considered ineligible if the school no longer existed (closed or merged) or did not enroll any eighth grade students in the spring term of 1988. Most of the sample list schools declared ineligible were schools that had closed or were small, private schools that had no eighth graders enrolled in the spring of 1988. Finally, a school was ineligible if it had opened its door after the fmal sampling frame was constructed.3 The number of schools in this category is likely to be small. We believe that these exclusions will not have a large impact on estimates made from the current NELS:88 sample. Information from various sources suggests that approximately 90 percent of American Indian school children attend schools not affiliated with BIA (by \"affiliated\" we mean schools directly operated by BIA and those operated by American Indian communities under contract to BIA). Investigators should take this degree of undercoverage into account when attempting population estimates. If this group is substantially different from American Indian eighth graders not attending BIA schools a substantial bias in estimates may result. Other sources suggest that fewer than 10,000 eighth graders attended Department of Defense Dependent Schools (DODDS) serving dependents of U.S. personnel overseas in the 1987-88 school year. This estimate suggests that fewer than .3 percent of all eighth graders are in DODDS schools. To the extent that these students resemble the general eigth grade population in the 1987-88 school year, the rate of undercoverage is not alarming. To the eaent that certain characteristics are disproportionately represented in DODDS students, the undercoverage problem becomes more serious. However, since such a small number of students fall into this group, the undercoverage is not likely to result in serious bias in population estimates unless the DODDS students are extremely homogeneous on certain important education-related characteristics, and these characteristics occur rarely among other eighth graders. It should be noted that DODDS students who returned to the U.S. between spring of 1988 and autumn of 1989 and who are enrolled in tenth grade during the 1989-90 school year, have a chance of selection into the NELS:88 First Follow-up survey through sample \"freshening\". Of course, students who are educated at home or in private tutorial settings, and those who have dropped out of school before reaching the eighth grade, also fall outside the NELS:88 base year sample. The size of the pre-Grade 8 dropout population in winter-spring 1988 is uncertain. The National Center for Education Statistics has recently reported that 12 percent of dropouts ages 16-24 in 1988 had completed six or fewer years of school (Frase, 1989). However, over 31 percent of Hispanic dropouts age 16-24 had completed only six, or fewer, years of schooling. This finding both con-"}, {"section_title": "3", "text": "The sample frame represented information anent through April, 1987. 22 firms the fact that there is a sizable group of students who leave school before entering eighth grade, and suggests that the biasing effect of this phenomenon on NELS:88 data may be much more pronounced for some subgroups than others. Any of the school-level exclusions may have implications for national inferences based on NELS data, although their impact on such estimates is generally ev pected to be small. Minimizing NAEPINELS Building-Level Overlap. In order to minimize burden to individual participating schools and protect response rates for both studies, the NELS:88 core sample was designed to minimize the overlap with the NAEP sample for the 1987-88 school year. To accomplish this goal, the selection of the NELS:88 schools involved a two-phase process. The first phase was the NAEP selection. Any schools that were not selected for NAEP were eligible for NELS:88 selection and any schools that were selected for NAEP were not eligible for NELS:88 selection. In principle, then, no school was eligible for selection in both surveys. Exceptions to this principle could have occurred in practice because not all of the schools originally selected for NAEP agreed to participate, and therefore substitute schools were selected. While NORC was able to eliminate the originally selected NAEP schools from the NELS:88 sample, it was not able to screen out NAEP substitute schools. Substitutions. Additional sample selections within superstrata were made for schools that refused to participate in NELS:88. No additional selections were made for students who, for whatever reason, failed to participate. Each school (and student) was assigned a weight equal to the inverse of the unit's selection probability. The derivation of student case weights is discussed below. Use of weights properly projects estimates (within sampling error) !o the population of eighth grade schools and eighth grade students in United States schools in the 1987-1988 academic year, and for specific subgroups within that population. The current weights give estimates reasonably close to those from other data sources. For example, the 1989 Digest of Education Statistics cstimates the fall 1987 public school eighth grade enrollment to be 2,838,671. The estimate derived from the NELS:88 data by summing the nonresponseadjusted student weight for respondents in public schools is 2,633,959. The 6.9 percent underestimate can be accounted for by the fact that the NELS:88 sample excluded certain classes of ineligible students. While the overall ineligibility rate is around 5.4 percent for the entire sample, ineligible students were proportionally more likely to be found in public schools than in pevate schools; therefore, the ineligibility rate for public schools is expected to be higher than the overall rate. The Current Population Survey Report No. 429 estimates the total number of eighth graders in public and private schools in 1986 to be 3,235,000 students, composed of 1,679,003 male and 1,556,000 female students. This is reasonably close to the corresponding NELS:88 estimates of 3,008,080 students, 1,507,074 males and 1,501,005 females. Once again, the discrepancy can be accounted for fairly well by the excluded students, because males are generally known to be disproportionately represented in the specified population."}, {"section_title": "2.2", "text": ""}, {"section_title": "Sampling Frame", "text": "In designing a sample frame, one can use either an explicit or an implicit list of the elements to be sampled. For NELS:88, the creation of an explicit list of all eighth grade students in the U.S. would have been an impossible task. NORC therefore elected to use an implicit list of students by using a list of public and private schools in the U.S. It was important that the list of schools be complete and accurate and have complete data for variables to be used in the subsequent stratification."}, {"section_title": "23", "text": "Investigation of various sources indicated that the most readily available source for a complete and accurate frame was the data base compiled by Quality Education Data, Inc. (QED) of Denver, Colorado. The data base includes both public and private (parochial and non-parochial) schools. QED performs annual, late-summer updates by telephoning each public school district, each Catholic diocese, and all private schools on its records. In addition, QED frequently receives updated information from agencies such as the National Catholic Educational Association (NCEA), the Council of American Private Education (CAPE), the Association of Christian Schools, and others, regarding school openings and closings, enrollments, and so forth. The QED records were successfully employed in the five-state field test and proved highly accurate. The number of eighth-grade students attending schools not included in the QED lists is estimated by QED, Inc. to be less than 1 percent and comprised mostly of a small number of schools that had opened after a particular data file was created and released, and a small number of home schools in rural areas. An analysis undertaken by NORC comparing the QED files with files from Department of Education's Common Core of Data (CCD) showed a very high correspondence between the files both in the public schools listed and their characteristics. The QED data base contained Census information about whether a school's location was urban, suburban, or rural. NORC used this information for stratification purposes. The QED list did not contain information about the racial/ethnic composition of public schools usable for constructing the NELS:88 sampling frame. NORC obtained racial/ethnic composition data (on public schools only) from Westat, Inc., a subcontractor for the NELS:88 survey. As part of another federal contract (the National Assessment of Educational Progress, NAEP), Westat had obtained data from the Office of Civil Rights (OCR) and from other sources (e.g., district personnel) that indicated those public schools with a combined black and Hispanic enrollment of greater than 19 percent. The schools for which the OCR data were available tended to be large schools in large SMSAs. Westat also obtained black and Hispanic percentages directly from district personnel in public school districts that, according to the QED list, had large proportions of black or Hispanic students. These data were compiled only for public schools in the primary sampling units of the Year-17 NAEP survey. In all, less than half of the eighth-graders in the NELS:88 frame came from schools for which such racial composition data were available. However, this partial data allowed NORC to create sampling strata containing public schools with very large percentages of Vack or Hispanic students. In addition, data from the QED list allowed identification for stratification purposes of schools as public, Catholic (private), or other private. The stratification procedures are discussed in more detail in the following sections."}, {"section_title": "Stratification", "text": "The sampling frame was sorted in such a way as to create groups of schools, called strata, that were contiguous on the frame. Each stratum contained schools that were relatively similar in terms of certain variables deemed relevant to the survey's objectives (public/private, region, urbanicity, and percent minority). The actual selection of schools occurred independently within each stratum. Schools were stratified by superstrata and substrata. First, schools were sorted into combinations of school type and geographic region (superstrata). Next, substrata were formed according to values on an urbanization variable, (i.e., whether the location of a school was urban, suburban, or rural), and according to the minority classification mentioned previously: minority substrata were not created for private schools. Finally, within substrata schools were sorted in order of their estimated eighth grade enrollment. The sort order alternateA between ascending and descending from one substratum to the next. In the following tables and on the data tape, the divisions that comprise the public and private schools' superstrata are the same as those used by the Census Bureau. During sample construction, however, nine large states constituted nine of the individual public school superstrata. Similarly, one private school stratum was comprised entirely of schools in one state. Because Public Law 100-297 mandates the protection of respondents to NCES surveys from the risk of disclosure, including disclosure through statistical means, the public use data file collapses schools in these strata into their respective census division strata and completely suppresses information on substrata. This collapsing of grata will lead to standard errors being slightly overstated when CTAB, or other programs that account for the sample design, are used to calculate or approximate standard error estimates.4 Table 2.3 shows the number of schools in the sampling frame and the number of schools sampled for each of the strata reported in the public use data file. It should be noted that a certain percentage of schools were found to be ineligible after they were sampled and contacted. These schools were excluded from the sample (see section 2.1.1 for a discussion of excluded schools) and were replaced with schools from an additional pool of schoolisampled to accommodate such occurrences. The numbers in Table 2.3 do not reflect these schools. However, subsequent descriptions of the sample do account for the ineligible schools, which is why the number of schools reported in subsequent tables differs slightly from the numbers reported here."}, {"section_title": "Allocation of Numbers of Schools to Be Sampled", "text": "The number of public schools to be zolected for the core sample from each superstratum was set to be proportional to the aggregate ostiinated eighth grade enrollment of all the schools in that superstratum. For this calculation, the, eighth grade enrollment in each school was estimated by dividing the enrollment figure from the QED list by the number of grades in the school; this procedure implicitly assumes an equal number of students in each grade in the school. The allocation of the sample size to substrata within the public school superstrata was proportional to the sum of a measure of size (MOS) of the schools in the substrata. The calculation of the measure of size is discussed in section 2.5; the measures of size were proportional to the eighth grade enrollments. The determination of the numbers of schools to be selected from each of the private strata reflected a compromise between competing analytic needs. Private schools as a whole were oversampled relative to public schools. Policy anklysts are particularly interested in certain types of private schools, and oversampling these types has the obvious benefit of increasing the number of cases available for analysis, but at the cost of decreased precision for statistics based on the other types of private schools. The allocation was designed to give policy analysts the minimum numbers of schools necessary for their work, while not deviating too far from allocation proportional to eighth grade enrollment, so that statistics based on all types of private schools would not lose too much precision."}, {"section_title": "4", "text": "CTAB is the name of a program designed to calculate standard erron for two-stage cluster samples. See Curoll, C. Dennis, (October, 1988 "}, {"section_title": "Selection of Schools within Strata", "text": "A sample design objective was that each student sampled from the selected schools would have an equal chance of selection. 'lb accomplish this, a measure of size (MOS) was calculated for each school that was not selected by NAEP: MOS F * G * max (24, G8 enrollment). (1) Schools selected by NAEP had MOS set to zero. The MOS was equal to an adjustment factor, F, times another factor, G, times the maximum of 24 (which was die desired number of regular students per school to be sampled) or the estimated eighth grade enrollment of the school. The factor F varied from school to school and was designed to adjust for the fact that NAEP did not select schools with equal probability. F was set equal to the reciprocal of 1-P, where P was set equal to each school's probability of selection into NAEP,5 enstuing that choosing schools with probabilities proportional to MOS would yield school selection probabilities proportional to the estimated eighth grade enrollments. The latter is desirable because if the school selection probabilities are proportional to the eighth grade enrollments and if 24 students (or all students, if fewer than 24 are enrolled) are to be selected at random from each selected school, then all students have equal probabilities of selection. The effect of G is to undersample small private schools where very few students could be sampled. With a fixed school sample size, this has the effect of increasing the number of large other private schools thus increasing the total number of other private students in the sample. The factor G is present in (1) to ensure that a sufficient number of \"other private\" school students are included in the sample. Many of the \"other private\" schools had estimated eighth grade enrollments considerably under 24, and if the factor G were not present in (1) then the number of sampled students in \"other private\" schools would be undesirably low. The factor G was set equal to 1 for all schools in all stratr except for the superstratum of \"other private\" schools. For the schools in the latter superstratum, G was set equal to 1 if the estimated eighth grade enrollment was 8 or more, and G was set equal to 0.5 if the estimated eighth grade enrollment was less than 8. The selection of the public schools WAS accomplished using systematic sampling with random starts in each public superstratum and sampling intervals in each superstratum that were proportional to MOS. The selection of the private schools was accomplished using systematic sampling with random starts in each private substratum and with the sampling intervals proportional to MOS. Use of systematic sampling in thi: way produced the beneficial effect of implicit stratification by estimated eighth grade enrollment within each substratum."}, {"section_title": "5", "text": "For each school, define the following probabilities: P(NELS) probability of selection into NELS P(NELS/NAEP) = probability of selection into NELS given selection into NAEP P(NELS/not NAEP). probability of selection into NELS given nonselection into NAEP P probability of selection into NAEP Also, let ENROLL denote at estimate of the grade 8 enrollment in the school. Then, P(NELS) P(NELS/NAEP) P + P(NELS.Inot NAEP) (1-P) Note that P(NELSINAEP)=0. Thus, P(NELS).P(NELS/not NAEP) * (1-P) If we set P(NELS) proportional to ENROLL then we have P(NELS/not NAEP) proportional to ENROLL/(l -P). "}, {"section_title": "Design Allowance for School Nonresponse", "text": "Despite the best efforts of any data collection agency, not all units selected for a survey agree to participate. One problem caused by nonresponse is possible systematic error in statistics calculated from data from just the participating schools (see section 4 for further discussion of systematic error due to nonresponse). A second problem is a decrease in the size of the sample from which data are obtained. lb cope with the latter problem NORC drew extra schools in the initial selection process. The extra schools rmere brought into the sample in order to attain the desired sample sizes despite nonparticipation by some of the schools. The extra schools were chosen at random from the same superstratum and substratum as nonresponding schools. Specifically, the sample drawn was larger than the sample NORC intended to field; schools were randomly assigned to two pools, with pool 1 as the target sample and pool 2 containing backup schools. Our best attempts were made to obtain cooperation from each pool 1 selection, but when cooperatinn was impossible, an additional school was taken at random from pool 2 from the same superstratum and substratum as the nonresponding school. This procedure had the effect of controlling the number of cooperating schools from each superstratum and substratum. Schools selected randomly within each substrata were alternately assigned to either pool 1 or pool 2. That is, each school had an equal chance of being in pool 1 or pool 2. All of the pool 1 schools were fielded. If the number of responding schools in a stratum was below a prespecified target number (see Table 2.8-2) schools from pool 2 were contacted. It is importan: iri note that not all of the pool 2 schools were fielded. Once the target number of schools within a stratum was obtained, additional pool 2 schools were not fielded. School weights were dervied conditional on the number of pool 1 and pool 2 schools that were contacted, ignoring the pool to which the school was initially assigned. Our final sample size consisted of all pool 1 schools and all pool 2 schools from whom cooperation was requested; pool 2 schools that we did not contact (because we had already obtained cooperation from a sufficient number of schools in the corresponding superstratum and substratum) were not counted. That final sample size (adjusted for numbers of ineligible schools) was used as the denominator of the unweighted response rate for schools. The sample design weight for each extra (pool 2) school that was brought into the survey was calculated in the same manner as the weights for the pool 1 schools, i.e., as the reciprocal of the selection probability conditional on the final sample size for the school's superstratum and substratum."}, {"section_title": "2.7", "text": "Selection of Students, Parents, Teachers and School Administrators. The basic sampling procedure resulted in the selection of up to 24 students per school, or all of the eighth grade students in the school if they numbered fewer than 24. An additional procedure was implemented to augment this basic sample of 24 students per school with an oversample of Asian-Pacific Islander and Hispanic students. The target was to achieve a total oversample of 2,200 additional students with these ethnic characteristics. The oversampling was done only for those schools that fell into the \"core\" sample. The student sampling procedure can be described as follows. First, three lists of eighth graders were obtained from each participating school, one of Asian students, one of Hispanic students, and one of all the other students (see section 2.1.1 for a discussion of determining which students were eligible to be sampled and which were excluded from the sample). Second, random samples of Asians, Hispanics, and others were independently selected from each of the three lists. Sample sizes were calculated using the following formulae: nH= (CS * CH * NH/F) + (24 * NH/11), nA = (CS * CA * NA/F) + (24 * NAN). nO = 24 * NO/N, where nH, nA, and nO are sample sizes for the sample of Hispanic, Asian, and Other students. NA, NH, and NO denote the numbers of students on the lists of Asians, Hispanics, and others, respectively, and N denotes the total number of students on all the lists. F denotes the first-stage selection probability of the school, CA and CH are constants used for the selection of Asian and Hispanic students, and CS is a constant used for the selection of Asian or Hispanic students in stratum S. CA, CH, and CS were constants of proportionality constructed to obtain desired total sample sizes for Asian, Hispanic and other students across schools. Upper limits on nH and nA were set to ensure that the number of students per school did not get impractically large. The specifications of CS, CA, and CH were empirically determined to ensure that two goals were achieved: (1) sufficient numbers of Asian and Hispanic students were sampled, and (2) selection probabilities did not vary excessively across students. By keeping selection probabilities relatively homogeneous across students, design effects were also kept from becoming too large."}, {"section_title": "Sample Updating", "text": "A representative from each school submitted a list of eligible students from which a sample was drawn (see Section 2.1.1 for criteria used to determine student eligibility). These lists, called school rosters, were submitted and an initial sample was drawn, starting in November of 1987. To adjust the student sampling frame for student attrition and change in the eighth grade population of the sampled school, NORC conducted a sample update seven to ten days prior to the school's scheduled survey session. The NORC survey representative went over the sample list with the school coordinator to ascertain whether all sampled students were still eligible and to ensure that transfer-iasthat is, any student who had joined the school's eighth grade between the time of original sampling and the time of the update--were added to a supplementary roster from which additional students would be selected. The supplementary roster was annotated for eligibility and ethnicity and the transfer-in students were sequentially numbered. Selections for inclusion in the sample were based on the same set of computer-generated random numbers used to select the original sample and Asian/Hispanic oversamples for that particular school. While in the High School and Beyond Base Year Survey substitutions were made for students who were ineligible or had died, there were no student-level substitutions in NELS:88."}, {"section_title": "Selection of Parents", "text": "Conceptually, the universe of parents of eighth grade students consisted of all parents or legal guardians of eligible eighth grade students in the winter-spring 1988. The selection of parents thus did not require the construction of a formal universe or list. One parent questionnaire was sought per student, regardless of whether the student resided in a one-or two-parent household (or joint custody arrangement, in the case of divorced parents of a NELS:88 eighth grader). Once the student sample had been selected, the parent who was \"best informed\" about the child's educational activities was asked to complete a NELS:88 parent question-naire. The parent questionnaire was delivered by means of an envelope addressed \"To the Parents of (Name of Student)\" and was accompanied by a letter introducing the study to the parent. Both the letter and the instructions printed on the parent survey instrument stressed that \"the questionnaire should be completed by the parent or guardian who is most familiar with the student's current school situation and educational plans.\" The questionnaire packet was initially carried home by the student, who was instructed to give it the parent who was best informed about the student's school situation. In a few cases, schools insisted that the questionnaire be mailed to the home rather than distributed in school, in which case the packet was again addressed \"To the Parents of (NAME OF STUDENT)\". Nonresponse follow-up was by mail and telephone for households with telephones and by mail and in person for households lacking telephones. Telephone and in-person interviewer scripts stressed once more the requirement that the initial contact in the household be asked to identify the more knowledgeable parent respondent. Thus, the parent respondent was essentially self-selected, though most certainly the mode of delivery, the screening selection exercised by the eighth grade student, and chance factors created unequal opportunities for self-selection between the two-parent home or between multiple households with dual child custody arrangements. No effort was made to identify parents who had more than one chance of selection (that is, had more than one child in the eighth grade). After parent and student data had been collected, the parent sample was further restricted to the parents/guardians of participating base year students. Thus parent data from the base year nonparticipants was systematically excluded from the final data file."}, {"section_title": "Selection of Teachers", "text": "All full-and part-time teachers who were teaching classes in mathematics, science, English/language arts, and social studies to eligible eighth graders in the winter-spring of 1988 were included in the NELS:88 universe of eighth grade teachers. The actual sample was restricted to teachers who provided instruction in the four subject areas to the selected sample of eighth grade students within the sampled schools. Thus there was no need to construct a formal universe list of eighth grads mathematics, science, English and social studies teachers prior to their selection. In cases whr-re the teacher had changed between the autumn and spring terms, the spring term teacher was designated as the preferred respondent. To achieve the objective of \"linking information from the teacher to data about individual students in the NELS:88 sample,\" two teachers were identified as respondents to the teacher questionnaire for each student. Selection of respondents to the teacher questionnaire for each student was based on the assignment of two curriculum areas per school included in the NELS:88 base year sample. Specifically, each of the sample schools was assigned one of the following combinations of curriculum areas: Each sampled student's current teacher in each of the two designated curriculum areas was selected to receive a teacher questionnaire. The assignment procedure was designed to achieve approximately balanced representation of the four combinations of curriculum areas across the sampling variables of school type and levels of urbanicity and/or minority population. Additionally, there was an attempt to balance assignments within geographical categories and by school size. Finally, the assignment process was intended to ensure representation of mathematics or science and English or social studies in all base year sampled schools. Once the data file listing all sampled schools was compiled, it was sorted in the order of sample selection; that is, by geographical category within school type, then by urbanicity/rninority level, by whether the school was selected initially as a sample school or a replacement school, and finally by a measure of size. Next, the four curriculum combination areas were ordered in a random sequence and the start combination randomly selected. This start combination (mathematics and social studies) was assigned to the first school in the sorted listing, and curriculum combination areas were assigned to the schools in repeating cycles of the sequence (i.e, mathematics and social studies, mathematics and English, science and English, and science and social studies). Following the assignment of curriculum combination areas to sampled schools and the selection of the student sample in a participating school, a matrix of student-subject-teacher information was obtained from school records. A Class Schedule Form used in the teacher-respondent selection process contained 30 rows (one per sampled student) and two columns (one for each assigned curriculum area). For each cell in the form, that is, for each student-curriculum combination (subject), the following information was entered: Class identification (e.g., usually perioe number or hour); Course title; and Name of the student's current teacher in that subject. In completing the teacher matrix, the school cc-rdinator was asked to report the current teacher, or where there were multiple current teachers, to report the teacher who had the greatest assigned responsibility for teaching the sampled student. The assignment of subject matter pairs to schools ensured that data were collected from two teachers of each student (assuming more than two teachers for the eighth grade Class, and that both the student's teachers chose to participate in the study) and that survey response burden for teachers in the school was limited. Because of the universality of the four subject matters in the required curriculum of the eighth grade, virtually all sampled students were enrolled in classes in the assigned subject combination during some portion of the 1987-88 school year. Thus no subject substitution was necessary. However, occasionally, a student was enrolled in more than one spring term class in a particular subject. When this was so, the following decision rule was invoked to determine which class would be entered on the teacher matrix: when there are two or more candidate classes in the same subject for a given student, take the course in which the student will have spent the most class time between the start of school and survey day; if this rule is not sufficient to eliminate all but one of the candidate classes, select the class th ,-.i. involves the most advanced subject matter. Other cases were encountered in which there was more than one teacher for a designated class (for example, team teaching arrangements). In these cases, the teacher with the greatest assigned responsibility (identified from the Class Schedule Form) was chosen to complete the teacher questionnaire. The use of the this sampling scheme for the NELS:88 base year resulted in the identification of varying numbers of teacher-respondents per panicipating school, ranging from 1 to 19, with an average number of 5.5 teachers per school. It should be noted that the resulting NELS:88 base year sam-31 plc of teacher-respondents to the teacher questionnaire does not constitute a statistical or representative sample of eighth grade teachers for analysis and reporting purposes. Rather, the results of this questionnaire are intended to provide information about student-related characteristics, teacher practices, and curriculum exposure that may affect long term student outcomes. It should also be noted that once data collection had been completed the sample was further restricted to teachers of base year participants (here defined as students for whom student questionnaire data was available). That is, data collected from teachers of base year nonparticipants were systematically excluded from the data files."}, {"section_title": "Selectiun of School Administrators", "text": "The head administrators (principals, headmasters, or headmistresses) of all eligible eighth grade schools in the universe of schools constituted the universe of school administrators. One school administrator questionnaire was sought from each school. A head administrator entered the sample if his or her school was selected as an eligible NELS:88 school."}, {"section_title": "2.8", "text": ""}, {"section_title": "Data Collection Results", "text": "The NELS:88 base year study collected data from students, parents, teachers, and school administrators. Tables 2.8-1 through 2.8-4 summarize data collection results for the NELS:88 base year study. Table 2.8-1 is a summary of NELS:88 school, student, parent, and teacher survey completion rates. Table 2.8-2 describes details of the school sampling and enlistment procedure. The first column presents target sample sizes overall and for the different types of schools. These target sample sizes were determined based upon statistical and analytic considerations such as establishing an acceptable level of precision in estimation and ensuring that an adequate number of schools of each type were available for subgroup analyses. The original selections, or pool 1 (as contrasted to the independently drawn sample of replacement schools, or pool 2) are identical in number--both totally and by superstratum and substratum--to the target N. The second and third columns break schools initially selected (pool 1 schools) into those eligible for NELS:88 participation (based upon the criteria for ev-iasion discussed earlier) and those ineligible or excluded. The fourth column gives the frequency and percent of eligible schools from pool 1 that agreed to participate in the survey. The difference between the targeted number of schools and the number participating from pool 1 made necessary the selection of a substantial number of additional schools from pool 2 to reach the target sample sizes. The number of additional schools surveyed from pool 2 are in column 5. A total of 550 eligible schools were contacted from pool 2. Thus the pool 2 completion rate is 359/550 or 65.27 percent. The last column shows the final obtained sample sizes; as indicated, they exceeded the targets in two of the three categories. Tables 2.8-3 and 2.8-4 present two sets of completion statistics for the four study components that constitute the NELS:88 public use sample. The statistics are presented according to the sampling stratification variables. Table 2.8-3 displays weighted and unweighted completion rates based on the overall study/sample design in which the participating student constitutes the basic unit of analysis. The student design weight, the only weight applicable to respondents and non-respondents, was used in calculating the weighted completion rates with the exception of the teacher questionnaire rate and the school administrator questionnaire rate. No design weight was available for teachers because teachers were not sampled, but entered into the sample only by virtue of their attachment to a sampled student. However, when student-level teacher ratings were considered, it was appropriate to use the student design weight. For the school administrator questionnaire the school design weight was used. Information about the construction of the student and school design weights can be found in chapter 3.  For purposes of this table, the completion rate was calculated as the ratio of the number of completed interviews divided by the number of in-scope sample members. Note that the student population is, in the strictest sense, the sole independent sample, and that the other populations, for example parent and teacher, are defined in relation to participating students. Because the parent or teacher of a base year student nonparticipant was defined as out-of-scope (even though they may have completed questionnaires), these out-of-scope respondents have been subtracted from both the numerator and the denominator in the response rate calculation. Given this definition of response rate, weighted completion rates exceed 93 percent for each class of respondents as well as for the teacher ratings of students. In the case of the teacher survey, the statistics given represent more strictly a coverage rate than a teacher response rate. Note that reports were sought from two teachers of each student. The teacher ratings statistics in Table 2.8-1 depict the percentarz of base year participating students for whom observations were obtained from one or more teacners. Table 2.8-4, in contrast, presents the weighted and unweighted completion rates for each survey based on the initial sample selections--that is, the response rate denominator includes base year nonparticipants, even though the parents and teachers of base year nonparticipant respondents were defined as out of scope. Utilizing this definition, the completion rates decrease by several points to around the 90 percent mark. Because in both instances ineligible (or out-of-scope) schools and students were removed from the sample prior to data collection, completion rates are computed direcdy by simply dividing the number of participating respondents/schools by the number of selections. As in Table 2.8-3, the teacher statistics represent a coverage rate, rather than a teacher response rate. In considering participation rates, it is important to note that while school-level and individuallevel response rates are often considered separately, effects of nonresponse in a two-stage sample are for many purposes multiplicative across the two stages. A true indication of the response rate for students can be computed by multiplying school participation rates by individual participation rates. Thus, for example, defining school participation in terms of the percentage of originally selected pool 1 schools that held survey days, and multiplying that percentage by the overall response rate for students, one derives an overall response rate of about 65 percent (.697 x .9341=.651) for students and about 68.8 percent (.697 x .9892=.689) for school administrators. As a point of comparison, these participation rates are similar to those of the 1980 High School and Beyond Base Year Survey (for students, .70 x .82 = .574, or 57.4 percent; for principals, .70 x .982 = .687, or 68.7 percent).  ' Indicates a coverage rate."}, {"section_title": "4 22", "text": ""}, {"section_title": "3S", "text": "Base Year Sample Design Report 3."}, {"section_title": "Sample Weights", "text": "3.1"}, {"section_title": "General Approach to Weighting", "text": "The general purpose of the weighting scheme was to compensate for unequal probabilities of selection into the base year sample and to adjust for the fact that not all individuals selected into the sample actually participated. Construction of respondent weights was a multistage process that began with constructing weights for schools. Next, preliminary student weights were constructed, then adjusted for nonresponse. In addition, nonresponse-adjusted weights were constructed for schools."}, {"section_title": "Weighting Procedures", "text": "A school design weight, SCHWT, was calculated for each school by taking the reciprocal of each school's selection probability: where PH is the selection probability for the ith school. To calculate PH, we must first consider the unconditional probability that a school was selected into pools 1 or 2. Unconditional probability means that a school's chance of selection is not conditioned on the event that it was or was not selected into the NAEP sample. For schools selected into two of the state augmentation samples, the sampling was performed independently of the NAEP sampling, and so the unconditional probability is the same as the conditional probability. For schools selected into the core sample or into the augmentation samples for two other states, however, the conditional probability of selection into NELS:88 given selection into NAEP was zero. Thus, for these schools the unconditional probability of selection in NELS:88 is itself the product of the following two factors: Pal , the conditional probability of selection in NELS:88 given nonselection into NAEF, and 1 -PNI1 , where Nii is the probability of selection into NAEP. Puii, the unadjusted unconditional probability of selection in NELS:88, is obtained as follows: Pm, the probability of selection into NAEP, was not known for most cf the schools and had to be estimated. Westat, Inc., the organization that selected the NAEP sample, provided NORC with the NAEP selection probabilities for the schools that were selected into NAEP, but did not know and so could not provide NORC with NAEP selection probabilities for the schools that were selected into NELS:88. lb estimate the latter probabilities, regression analyses were used to predict the NAEP selection probabilities from school variables that were in the sampling frame. The predictor variables used were the number of students enrolled in the school, the estimated number of students in the eighth grade, the type of school (public, Catholic, or other private), and the percents of students who were white, black, and Hispanic. With the known values of Pal and with the estimated values of PNil we estimated the unconditional selection probabilities for all schools that were eligible for the NELS:88 sample. Two sets of probabilities were computed, one for the core sample plus private schools in one state augmentation sample, and another for the core sample plus all of the state augmentation samples. The former set is used for the weights for the national public use file. The latter set of probabilities is used for weights for all of the state augmentation samples and for estimating response propensities for schools (dis-3 5 L Base Year Sample Design Report cussed below). The results of the regression were tested against subsamples of schools for which NAEP prot ahilities were known. To smooth out the possible effects of errors in the estimates of the NAEP selection probabilities, we multiplied the unconditional selection probabilities by factors in each stratum to force their sum in the stratum to equal the number of schools that were sampled (i.e., that we attempted to contact) from that stratum. Thus, Puti-adj, the adjusted unconditional selection probabilities, were calculated as with Eje s(i) denoting summation over all schools j in the stratum to which school i belonged and Ili denoting the number of schools sampled from that stratum. We then calculate Pi 1 according to with Puil-adj defined as the adjusted conditional probability that the school was selected into pool 1 or 2, and Fil defined as the fraction of schools in pools 1 and 2 for the school's stratum that we attempted to include in the survey. Taking the reciprocals of the selection probabilities yielded the sample design weights for the schools, SCHWTI = 1/(Puit-adi * Fit)."}, {"section_title": "Nonresponse-Adjusted Weights For Schools", "text": "Nonresponse-adjusted school weights were derived as the product of the school's sample design weight times a nonresponse adjustment factor. Initial approximations to the nonresponse adjustment factors were calculated by first using linear and nonlinear logistic regression to estimate a propensity function, which gives the school's conditional probability of participation expressed as a function of school characteristics. The regression-based propensity function approach was used rather than the traditional weighting cell approach in order to include a number of variables in the adjustment process while avoiding the problem of small cells. Each school's design weight was divideu by its estimated propensity. These first approximations were multiplied by factors so that the products would sum to known totals for the superstrata. When estimating the propensity function it is important to have available a set of variables that correlate well with participation in the survey. In many surys data necessary to accurately estimate propensities are either severely limited or unavailable. For NELS:88, NORC conducted a special survey of nonparticipating schools in pool 1, in which a small selection of key items from the school questiotmaire were Leled, in order to obtain data to estimate propensities. This sample will be referred to as the \"followback\" sample and is used in the following analysis. The response rate in the followback sample was 97 percent. The followback sample and the sample of responding schools were combined, and a dummy variable representing participation was created such that the followback sample schools were coded \"1\" and the responding schools were coded \"0\". This variable (represendng nonparticipation) was used as the dependent variable in the following regression analyses used estimate tne propensity to nonrespond The followback survey provided a basic set of descriptive information about nonresponding schools that, combined with the same information on responding schools, could be used as a set of independent variables in the regression analyses for estimating propensity to nonrespond."}, {"section_title": "Boise Year Sample Design Report", "text": "To estimate the propensity function, stepwise linear regression was used to choose a subset of variables that c)rrelated well with participation. Next, logistic regression was used to fit the propensity function. Once the logistic regression function was estimated, propcnsity estimates were produced for all of the schools for which school questionnaires and student questionnaires were available. For a small fraction of schools (about 2 percent) we obtained student data, but were unable to obtain school data. For these schools, propensity estimates were calculated for the construction of the nonresponse adjusted school weight, BYADMWT, which is used for the construction of weights for students and parents. The propensity estimates for these schools were derived from a reduced regression model that used only variables that were available from the sampling frame. The reduced model included as variables school type (public, Catholic, and other private), urbanicity (urban/suburban/rural), geographic division (ba.d on the nine Census divisions), and the estimated number of students in the eighth grade class. In addition to those variables, the full model included an indicator of whether entrance tests were used as a criterion for acceptance into the school (school questionnaire item 24) and a measure of the frequency with which standardized test results were provided to the family (school questionnaire item 37). The propensity estimates were constrained to be at least 0.4, so that their reciprocals did not exceed 2.5. Dividing SCHWT by the appropriate estimated propensity yielded a preliminary approximation to BYADMWT: where PROPH is the estimated propensity for school i. The final weight was developed by multiplying this preliminary approximation by a factor that was constant within but varied across superstrata. The factor was chosen to ensure that for each superstratum the sum of BYADMWTFelim times an estimate of eighth grade enrollment, say Y, over all schools with school questionnaires was equal to the sum of Y in that superstratum in the frame. Thus, BYADMWT; = BYADMWTprelim,i *E jE S(i) Yj AE jE S(i) Yj * BYADMWTprelimj * PARP ) with PARji = 1 if school j participated, and 0 otherwise, and Eje S(i) denotes summing over all schools j in the stratum i to which school i belongs."}, {"section_title": "Second-Stage Sample Design Weights for Students", "text": "The second-stage sample design weight for students, RAWWT, is equal to the reciprocal of Pi2 ,the conditional probability that the student was selected given that his or her school was selected into the base-year sample. That is, RAWWTi = 1/P12 . Student Selection Probabilities. Within each selected eighth grade school, rosters of all eighth grade students were obtained from the school coordinator. At the time that this list was prepared, the school coordinator was also asked to classify each student into three groups: (1) Asian-Pacific Islander, (2) Hispanic, and (3) a:' others. The rosters were used as within-school sampling Base Year Sample Design Report frames, and the ethnic classification was used in the oversampling of students of Asian-Pacific Islander and Hispanic descent. NORC office personnel used this initial roster and classification to construct three separate lists of students who were designated either as Asian-Pacific Islander, Hispanic, or non-Asian-Pacific Islander and non-Hispanic. These three lists were separately sampled with selection probabilities determined as follows: a. Subject to two upper bounds discussed below, students designated as Asian-Pacific Islander in the ith school were sampled at a rate equal to 0.054/pu, where pa is equal to the probability of selection for the ith school (the same as F in the equations in section 2.7.1), and 0.054 is the empirically derived proportionality constant (see section 2.7 for a discussion of the criteria used to derive the proportionality constants). This probability, 0.54/pn, was subjected to the following upper bounds prior to its application. First, it was bounded at unity (1.0) and second, it was bounded by a cap at 25 on the number of Asian-Pacific Islander students that would be selected in a sample school. Thus, the sampling rate for Asian -Pacific Islander students was set at pia = min (0.054/Thi, 1, 25/Na;), with NA; defined as the number of eligible Asian-Pacific Island students in school i."}, {"section_title": "b.", "text": "Subject to two upper bounds, students designated as Hispanic in the ith school were sampled at a rate equal to 0.035/pii, where p11 is equal to the probability of selection for the ith school, and 0.035 is the empirically derivied proportionality constant (see section 2.7 for a discussion of the criteria used to derive L. proportionality amstants). This probability, 0.035/pii, was subjected to the following upper bounds prior to its application. First, it was bounded at unity (1.0) and second, it was bounded by a cap at 25 on the number of Hispanic students who would be selected in a sample school. Thus, the sampling rate for Hispanic students was set at pita = min (0.054/pii, 1, 25/Nhi), with Nhi defined as the number of eligible Hispanic students in school I."}, {"section_title": "c.", "text": "Students designated as non-Asian-Pacific Islander and non-Hispanic (i.e. other) in the ith school were sampled at a rate equal to pio2 = 24/TSIZEii, where TSIZEil is equal to the total number of eighth graders not pre-identified as Asian or Hispanic on the roster for the ith school."}, {"section_title": "Student Weighting-First", "text": "Step. The process of producing student weights involved a number of steps. One of these steps involves weighting that is linked to the selection of students within sample schools. In this step, the weight factor for each student was equal to the inverse of the student's probability of selection into the sample within the sample school. For example, if within a certain school a selected student received a probibility of selection equal to 1/20 = 0.05, the student's conesponding weight would be equal to 1/0.05 = 20.0."}, {"section_title": "42", "text": "It should be noted that a student's probability of selection was determined by the initial classification (Asian-Pacific Islander, Hispanic, or other) that the student was given at the time of selection. In those situations where the initial classification was incorrect, the probability of selection for the student is equal to the selection probability actually used, rather than a theoretical probability under the assumption of perfect classification. Asian-Pacific Islander and Hispanic (OBEMLA) Oversamples. As part of the overall design of NELS:88, Asian-Pacific Islander and Hispanic students were oversampled in order to ensure adequate sample sizes for subgroup analyses. This oversampling was implemented by increasing the probability of selection at the within-school stage of the selection process. For oversampling purposes, Asians and Pacific Islanders were defined as students whose predominant ancestral orgin was in the Asian countries of the Pacific Basin (except Soviet Asia), or the Pacific Islands. Thus students whose ethnic or racial orgins were in China, Japan, Korea, Malaysia, Brunei, Indonesia, Kampuchea (Cambodia), Vietnam, Laos, or the Philippines were to be oversampled, as were students whose orgin was in a Pacific Island (such as Guam, or Samoa--and including Native Hawaiians). Other Asians--for example, students with ethnic or racial origins in South Asia (Bangladesh, India, Pakistan, Sri Lanka) or in other parts of Asia not part of the Pacific Basin, were not included in the Asian oversampling. (Though not oversampled, these other Asians are separately identified by subgroup on the data files, through their own self-reports). For purposes of Hispanic oversampling, schools were instructed to consider a student to be Hispanic if the student's ethnicnational orgin was in any of the traditionally Spanish-speaking countries of North, Central, or South America, including the islands of the Caribbean. Gallaudet University Supplement of Hearing-Impaired Students. Gallaudet University contracted with NORC to include a supplementary sample of hearing-impaired students. Because of the relatively small universe of students in this category, it was decided to try to interview all such students, within each of the NELS:88 base year core sample schools, who satisfied the Gallaudet University criteria for hearing impairmcnt. Students were included as part of the hearing-impaired sample if they met both of the following requirements: I. The student had on file an Individualized Education Program classification (IEP) indicating that the stude;.i was reported to the state Department of Education as hardof-hearing."}, {"section_title": "2.", "text": "The student was currently mainstreamed with regular hearing eighth graders in the school building for English or mathematics classes. All students who met both eligibility criteria were selected into the sample. If the eligible hearing-impaired student was not included in the sample as the result of the regular sampling procedures, he or she was added to the sample at the end of the school sampling procedure. Information concerning hearing-impaired students was obtained by an initial phone inquiry followed by a mailing to the school coordinator. In the majority of cases, once a school agreed to participate, both instructions for identifying candidates for the hearing-impaired sample and a Hearingimpaired Student Roster Attachment Form were sent to the school coordinator, along with the regular roster annotation instructions and form. A small number of schools had already completed and re-turned their rosters prior to the finalization of the procedure for selecting hearing-impaired students. These schools were contacted by telephone to obtain a list of eligible hearing-impaired students. School coordinators were asked to follow certain guidelines in determining which students to designate as eligible. In determining student eligibility they were urged to confer with available staff specialists in hearing impairment. They were also instructed to include any special education students who might appear on separate ungraded rosters if they met the two eligibility criteria. Once they determined which students met the two eligibility requirements, they were asked to review the regular NELS:88 ineligibility codes to determine whether any of these students should be excluded (for example, because they were part-time students, had transferred to another school, ta had a mental or physical disability that precluded them from completing the questionnaire and test). However, they were also reminded that a hearing-impaired student is not automatically ineligible because of physical disability. Next, school coordinators were asked to designate the hearing-impaired children listed on the roster as Hispanic, Asian-Pacific Islander, or other. In all, data were collected on 77 hearing-impaired students. Only 10 of these students belonged to the sample obtained through the normal within-schools sampling procedure and appear in the public use file. The remaining 67 comprised the participating part of the supplementary sample drawn for Gallaudet University."}, {"section_title": "Nonresponse-Adjusted Weights", "text": "The basic nonresponse-adjusted student weight, BYQWT, was derived as the product of the student's sample design weight (RAWWI) times a nonresponse adjustment factor. The factor is intended to adjust for the fact that some of the sampied students did not participate, that is, did not return a completed questionnaire. To derive the nonresponse adjustment factor we used a weighting cell approach. First, the group of all sampled students was partitioned into weighting cells such that each sampled student belongs to exactly one cell. We attempted to construct the cells so that students in the same cell have similar propensities to participate in the survey. Next, the nonresponse adjustment factor for a student in a given cell was computed as the ratio of the sum of BYADMWT (the nonresponse-adjusted weight for schools) times RAWWT for all students in the cell to the sum of BY-ADMWT times RAWWT for all of the students in the cell who participated. The reciprocal of this factor may be interpreted as an estimate of the participation propensity for students in the cell because it is -'mply the weighted proportion of students who did participate. Thus, the nonresponse adjustment /actor, FAC, for weighting class c is defined by FACc = Ejec BYADMWT; * RAWWTI aiec B YADMWT; * RAWWT1* PARi2 ) where Xi Ec denotes summation over all students in the sample in weighting class c, and PARI2 = 1 if student i participated, and 0 otherwise. BYQW1'preihn,j, the preliminary nonresponse-adjusted student weight for student i, is defined by BYQWI'prellin,i = BYADMWTI * RAWIPT, - FAC(j) where c(i) denotes the weighting class to whiuti the student belongs. The cells were based on classification of the students according to data that were available from the school rosters and from the sampling frame for the schools. The cells were set up as shown in Table 3.2-1. (The abbreviation \"API\" means Asian or Pacific Islander, ethnicity of \"other\" means not Hispanic or API, region of NE means Northeast, MA means mid-Atlantic, and \"other\" means not NE or MA.) Classification by school type and region was based on information included in the sampling frame. Ethnicity was obtained when rosters were collected from the schools. Gender was indicated on some but not all of the rosters; where it was not indicated, it was inferred on the basis of the students' first names. Comparison of the inferences with responses to the questionnaires showed a high degree of accuracy for the guesses. In the weighting cell formation, however, the questionnaire data for gender were not used even when available, so that the classification for participants and nonparticipants was consistent. To obtain the final nonresponse-adjusted student weight, the nonresponse adjustment factor wu modified by multiplication by a \"polishing\" factor. The polishing factors were defined for eight classes of students, four school types by two sexes. The polishing factor for a class was set equal to the ratio of the sum of BYADMWT times RAWWI' for all students in the class to the sum of BYQWT times the (preliminary) adjustment factor for all of the students in the class who participated. Polishing preserves the sums of the weights across the eight classes. The polishing factor for any one of the eight classes of students, say class k, is POLk , and is defined by POLk = Eie k BYADMWT; * RAWNVIVEie k BYQWTprelim,i *PAk.i. ) where Ejek denotes summation over all students i in the sample in class k."}, {"section_title": "4'", "text": "Base rear Semple Design Report Then BYQWT for student i is calculated as BYQWTI g POLkw * BYQWTprelim,i where k(i) denotes the \"polishing\" class to which student i belongs. Mtge 3.2-2 gives statistical properties of the school (BYADMWT) and the student (BYQWT) nonresponse adjusted weight- 1This value is slightly less than the sample frame total reported in Table 2.3 because it was adjusted to account for the small (less than 2%) number of ineligible schools. 2 Based upon the number of schools represented in the school survey data file. 3 Based upon the number of students represented in the student survey data file."}, {"section_title": "6", "text": "32 Base Year Sample Design Report 4.\nPool 1 school perticipadon mmibers and response rates reported in the school nonresponse analysis differvery slightly from those in Table. 2.8-1 and 2.8-2. The nonresponse analysis excluded Pool 1 panicipating schools for which no echool administrator questioemake was obtained, becmse comparison of responding and nonresponding schools depended on data supplied by school administrators. It also excluded several schools that participated at some level but for which no student data wen obtained or for wk.% student data were lost in transit. It included among eligible non-participants two schools that were later reclassified as ineligible (an eighth grade consisting of one student, in which the student tramfened out prior 10 the survey day rid no new eighth grade student transferred im a school in transition to year-round status that had eighth graders who wece sampled hi the alum of 1987 and a new eighth grade starting in the summer term of 1988 but no eighth grade in the spring term of 1988). When adjustments are made to compensate for these factors, the number of eigibk Pool 1 sch.Jols increasm r 1,002 and the number of participating Pool 1 schools to 698. The effect of these adjustments is to increase the participation rate assumed by the nonresponse analysis by just less than eight tenths of one percent."}, {"section_title": "School Nonresponse Analysis", "text": "Although the sample design in theory yields a sample that mirrors the population within sampling error, nonresponse can introduce distortions. It is useful to think about the impact of nonresponse in terms of a population that consists of two strata (Cochran, 1977, p. 361). One stratum includes all units that would have provided data had they been selected for the survey; the other stratum consists of all units that would have been survey nonrespondents. The actual sample of responding units necessarily represents only the respondent stratum. To the extent that populations of respondents and nonrespondents differ, the sample statistics will be biased as estimates of the characteristics of the entire population. According to this analysis of the effects of nonresponse, the magnitude of the bias introduced into means and proportions by nonresponse depends on two factors--the size of the nonrespondent stratum and the difference between units in the two strata: in which YR and YNR are the population values characterizing the respondent and nonrespondent strata, and PNR is the proportion of the total population in the nonrespondent stratum. The latter can be estimated from the observed rates of nonresponse in the sample. In NELS:88, there were two stages of sample selection and therefore two stages of nonresponse. During the base year survey, schools were asked to permit the selection of eighth grade students from school rosters and to hold survey and makeup days for the collection of student data. Not all of the selected schools agreed to take part in the study. In addition, not all of the individual students selected for the sample within cooperating schools (or the teachers or parents linked to these students) provided the data sought from them. The effects of the two types of nonresponse are additive: in which P1 is the proportion of all students attending schools in the school nonrespondent stratum; P2 is the proportion of all students attending schools in the school respondent stratum who would have been student nonrespondents; Y1R and Y1NR are population parameters characterizing students attending schools in the school respondent and nonrespondent strata respectively; and Y2R and Y2NR are population values for the strata of responding and nonresponding students attending schools in the school respondent stratum. More intuitively, equation 2states that students at cooperating schools may not represent all students and that cooperating students may not represent all students within cooperating schools. The bias due to nonresponse at both the school and student level can be further analyzed into two additive components. One component represents error in the estimate for particular subgroups; the other component represents error in the relative frequencies given to the subgroup3. For example, nonresponse may bias the estimate for a particular subgroup in the NELS:88 sample, such as girls. Such bias would arise if responding girls differed on the relevant characteristic from nonresponding girls. The second bias component would be relevant if the response rate for girls differed from the response rate for boys. If girls had a higher response rate, this could bias the overall estimate because girls would be overrepresented in the sample of respondents. Nonresponse adjustments to the weights are an attempt to compensate for the second component of the bias; they do not adjust for nonresponse bias within subgroups defined by the weighting adjustment cells. To apply equations (1) and (2) requires estimates of the relevant population parameters. Although PI and P2 can be estimated from the observed school-level and student-level response rates, the absence of survey data for nonrespondents usually prevents the estimation of the nonresponse bias. The NELS:88 survey is an exception to this general rule. During the base year survey, versions of the NELS:88 school questionnaire were sent to nonresponding schools in pool 1; more than 97 percent of these schools provided data. These data give us some basis for assessing the impact of schoollevel nonresponse on base year estimates. It is worth noting that school-level nonresponse is of panicular concern because it carries over into successive rounds of NELS:88. That is, students attending schools that did not cooperate in the base year were not sampled and have little or no chance of selection into the follow-up samples. To the extent that students at noncooperating schools differ from students at cooperating schools, the student level bias introduced by base year school noncooperation will persist during subsequent waves of observation. (Of course out-of-sequence students, who are tenth graders in 1990 but were not eighth graders in 1988, are unaffected by this bias, because they have some chance of being selected into the freshened sample of twelfth graders in 1992.) Within cooperating schools, student noncooperation is not carried over in this way because nonresponding students in these schools remain eligible for sampling in later waves of the study. Our analysis of school nonresponse is presented in two parts. The first examines rates of school nonresponse in the core sample as a function of sampling strata and other variables, such as school size, available from the sampling frame. As is apparent from equations (1) and (2), overall rates of nonresponse are a crucial determinant of the level of nonresponse bias; still, the focus of the first part of the analysis is mainly on the antecedents of nonresponse. The second part of the analysis is more directly concerned with the consequences of school nonresponse. There we present estimates of the magnitude of the bias resulting from school noncooperation. These bias estimates are based on data from the school questionnaires, which were completed by a sample of nonrespondent schools as well as by the schools that cooperated in the survey."}, {"section_title": "4.1", "text": ""}, {"section_title": "School Nonresponse Rates", "text": "For the purpose of this analysis, we concentrate on the pool 1 schools selected into the originally defined basic national sample, which we refer to as the core sample. This sample differs slightly from the sample defmed for the public use data file. For the public use data file sample, augmentadon schools in some of the states were included. This only occurred in those states for which the augmentation sample was drawn at the same time as the onginal sample. Details of this procedure are presented in chapter 2, the chapter that explains the sampling procedures. This slight discrepancy between the public use sample and the core sample explains why some of the figures referring to the total number of schools sampled and the total cooperating presented in this chapter are slightly different from figures in other chapters referring to what appears to be the same phenomena. We consider only the pool 1 core sample schools because not all of the selected schools in pool 2 were contacted. Therefore, we do not have an estimate of the response rate for pool 2. Because sampled schools were divided into pools on a random basis we expect that nonresponse rates would be the same, within sampling error, for the pool 2 schools. Another reason for concentrating on pool 1 schools is that 98 percent of these schools (over 98 percent of participants, and over 97 percent of Pool 1 refusal schools) provided data on a few key variables that will be useful in the analysis of nonresponse bias in the next section. It is possible to define school nonresponse in several ways--according to whether the school provided student data, data from teachers, parent data, data on the school itself, or some combination of these types of data. For the purpose of this analysis we define a responding school as one in which a student questionnaire was obtained from at least one sampled student in the school and a school administrator completed a school questionnaire. Of the 987 eligible original selections, 681 schools, or 69 percent, provided school administrator and student data.6 When nonresponse rates were valyzed across superstrata, marked variation was found. The unweighted response rates for the initial core selections range from lows of 20 percent for other private schools in the Northeast to highs of 100 percent in four of the Catholic superstrata and in other private schools in the Northeast. In general, the rates of cooperation were higher for Catholic schools (unweighted responses rate of 77.6 percent) than for the public and other private schools (unweighted response rates of 67.4 and 73 percent). Tb achieve the desired sample sizes, additional schools from pool 2 were selected to replace initial selections that proved to be ineligible or that refused to participate in the study. Including both the initial (pool 1) selections and the replacement (pool 2) schools, a total of 1,692 schools were selected for the NELS:88 core sample; of these, 1,624 were eligible for the study, and a total of 1,035, or 64 percent of the eligible schools, provided both student and school administrator data. The weighted response rate, calculated using the school design weight (SCHWT), was 65 percent. This rate can be used as an estimate of P1 in equation 2. Because limited data on all the noncooperating schools are available from the sampling frame, school-level nonresponse can be further analyzed as a function of certain school characteristics. These data include the school's location and type (public, Catholic, and other private), which were used to classify the schools into sampling strata; in addition, the level of urbanization of the area in which the school is located, two measures of school size (overall and eighth grade enmIlment), and two measures of minority enrollment (percent black and percent Hispanic) are available for analysis. School-level response rates for the combined pool 1 and pool 2 selections fluctuated markedly across superstrata, ranging from lows of 12.5 percent and 19.2 percent for some non-Catholic private school strata in the Northeast to highs of 100 percent for urban Catholic schools in the Northeast and for rural Catholic schools in the South. As with the initial selections, the Catholic schools generally showed relatively high levels of cooperation, with an overall weighted response rate of 75.1 percent. By contrast, the response rates were lower for the public schools (overall response rate of 62.5 percent) and for some other private schools (overall response rate of 62.5 percent). These results are quite similar to those in the High School and Beyond Base Year sample (see Frankel et aL, 1981, pp. 101-103). School response rates were somewhat higher for urban schools (71.0 percent) than for suburban or rural schools (62.6 and 63.5 percent). This is a bit of a surprise; in High School and Beyond, urban schools were the least likely to participate (Frankel et aL, 1981, page 104)."}, {"section_title": "9", "text": "School response rates were examined by two measures of school sizeoverall enrollment and eighth grade enrollment. For either measure, school size did not seem to have a clear-cut relationship with school-level response rates. A weak relationship between school size and response rates was also observed in High School and Beyond (Frankel et al., 1981, 106). School response rates also did not vary dramatically across the minority enrollment quartiles. Response rates ranged from 58.3 to 67.2 percent for quartiles determined by levels of black enrollment. Overall, schools in the lower two quartiles had a response rate of 66.8 percent; those with higher proportions of black students had an overall response rate of 60.3 percent. This is consistent with the results for the High School and Beyond sample (Frankel et al., 1981, page 105). Similarly, school-level response rates ranged from a low of 60.8 percent to a high of 67.2 percent acrosr quartiles determined by the percent Hispanic enrollment. The schools with relatively few Hispanics (i.e., those in the lower two quartiles) had an overall response rate of 65.4 percent; the high Hispanic schools (those in the higher two quartiles) had an overall response rate of 63.8 percent."}, {"section_title": "4.2", "text": ""}, {"section_title": "Estimating the Magnitude of School Nonresponse Bias", "text": "The analyses presented so far describe only the extent of school-level nonresponse and the variables related to nonresponse. It is possible to go beyond these analyses and assess the bias produced by nonresponse by examining responses to the school questionnaire. Versions of this questionnaire were completed not only by responding schools but by nearly all of the nonresponding schools that were in initial selections. Data from these questionnaires can thus be used to compare the responding and nonresponding schools in the initial sample. These comparisons can be used to estimate the school-level component of the overall nonresponse biasthat is, PI(Y1R-YINR) in equation 2. Table 4.2-1 shows the results for 14 such comparisons. They are based on the \"high priority\" items in the school questionnaire; data on these items were obtained from virtually all the initial selections. The first column of Table 4.2-1 shows the results for both respondent and nonrespondent schools. The estimates in the first column are weighted using the unadjusted design weight (SCHWT). The estimates in the second column are based only on the responding schools; these estimates are also weighted, this time using BYADMWT, which attempts to adjust for school-level nonresponse. Comparisons between statistics in the first two columns are estimates of the bias produced by school-level nonresponse. For example, the estimated bias in the mean eighth grade enrollment is 2.0 (83.6 for the responding schools versus 85.6 for all schools). It is difficult to evaluate the magnitude of nonresponse bias across items unless the bias is expressed on a uniform scale across items. For example, it is not clear whether -2.0 is a large bias or a small one. The fifth column in the table presents the bias estimates as a proportion of the estimate based on data from all schools in the initial sample; that is, the fifth column presents the difference between the estimates in the first two columns divided by the estimate in the first column. Summing across the absolute values of the relative measures of bias (Relbias) in Table 4.2-1, and dividing by 14 to take the average, gives the value .045. This suggests that, on average, estimates contaminated by school nonresponse differ from those not contaminated by school nonresponse by about 4.5 percent. It is worth noting that the estimates of the bias are subject to sampling error and may differ significantly from zero. The statistical significance of the bias depends on the significance of the difference between respondents and nonrespondents (see equations [1] and 12]). The third and fourth columns present the results for the responding and nonresponding schools separately; the results in both All four of the significant differences may reflect the relatively low response rate for public schools. Because public schools were less likely to cooperate in the study, they constitute a smaller proportion of the responding schools (.566) than of the nonresponding schools (.656). The reverse is true for other private schools--they constitute a greater proportion of the responding than of the nonresponding schools (.237 versus .182). lb put it another way, the sample of initially selected responding schools underrepresents public schools and overrepresents private schools. Given this, it is perhaps not surprising that lower proportions of responding schools than nonresponding schools have their students assigned by geographic area and employ a departmental organization. Both of these features are probably more characteristic of public than private schools. The use of school questionnaire data to assess bias in estimates concerning characteristics of the student population is not entirely straightforward. In equation 2, Y IR and YINR characterize the populations of students attending responding and nonresponding schools. The data summarized in Table 4.2-1 characterize the school themselves. Still, to the extent that school characteristics are closely related to the characteristics of the students attending them, then estimates based on school questionnaire data can serve as reasonable proxies for more direct estimates of Y IR and YINR. Despite this limitation on the data, it is still informative to examine the magnitudes of the relative bias estimates. Seven of the fourteen unsigned estimates are less than 2.5 percent. Thble 4.2-2 gives descriptive statistics for the unsigned relative bias estimates. The estimates range in absolute value from 0.1 percent to 15.5 percent, with an overall mean of 4.5 percent and a standard deviation across the 14 estimates of 4.7 percent. As we noted earlier, only four of the fourteen bias estimates differ significantly from zero. These estimates are somewhat higher than those observed in the High School and Beyond sample; in that study the mean relative bias for 31 statistics was 1.7 percent (see Tourangeau et al., 1983, page 45). The school-level response rate in High School and Beyond was almost identical to the one achieved in the initial selections analyzed hue (70 percent in High School and Beyond versus 69.7 percent for the initial NELS:88 sample). We suspect that the difference in the bias estimates reflects the availability of different variables for the analysis and the effects of sampling error."}, {"section_title": "Item Nonresponse Analysis", "text": "Analysis of survey error is important for understanding potential bias in making inferences from an obtained sample to a population. Sampling and nonsampling errors are the key constituents of total survey error. Elsewhere in this report, sampling error analyses for NELS:88 document design effects, and standard errors for key variables are presented. The bias associated with unit and item nonresponse, one source of nonsampling error, must also be described and quantified. In a two-stage sample such as the NELS:88 base year sample, one type of nonsampling error, unit nonresponse, can occur at either stage. Unit nonresponse can occur at the first selection stage when a school declines to participate, or at the second stage when an individual respondent within a panicipating school (in this case, the student, the parent, the teacher, or the school administrator) does not participate. This report documents the magnitude and effect of nonresponse at the first, or school, level of sampling, and makes inferences about the effect of the second level. Item nonresponse occurs when a respondent fails to complete certain items on the survey instrument. While bias associated with unit nonresponse at both the school and the individual level has been controlled by making adjustments to case weights, item nonresponse has generally not been compensated for in the NELS:88 data set. There are two partial exceptions to this generalization. The first partial exception is consistency editing, through which, occasionally, certain nonresponse problems are rectified by imposing interitem consistency, particularly by forcing logical agreement between filter and dependent questions. Thus, for example, the missing response to a filter question can often be inferred if the dependent question has been answered. The second partial exception is that some key student classification variables have been constructed in pan from additional sources of information when student data are missing. Thus, data from school records (for example, student sex or race/ethnicity as given on the sampling roster) or from the parent or teacher questionnaire (for example, limited English proficiency status) have been used to replace missing student data. However, apart from these special cases, missing values have not been imputed in the NELS:88 data. Because item nonresponse is an important potential and uncorrected source of data bias, it is necessary to measure its impact so that analysts can properly take potenum esponse biases into account. There are two main purposes to this analysis. One purpose is to quantify nonresponse bias for key variables on the student questionnaire and tests. A second purpose .., to describe nohresponse patterns, both in terms of characteristics of items and in terms of characteristics of respondents. The item nonresponse analysis reported here concentrates on the student questionnaire and the composite test responses because these form the heart of the study. A limited analysis of parent questionnaire item nonresponse was conducted and is reported at the end of this section. No analysis of the school atuninistrator or teacher questionnaire item nonresponse was conducted because the rate of nonresponse was quite low, typically around 1 percent. The present item nonresponse analysis employed the machine-edLed data, not the original raw data. The analysis proceeded in three stages. In the first stage, average nonresponse rates were calculated for each item. In the second stage, nonresponse was evaluated as a function of item characteristics: (1) position in the questionnaire, (2) topic, and (3) whether the item was contingent on a filter. Items with relatively high nonresponse rates were selected for further analysis in stage two. In the 53 Bate Year Sampk Design Report third stage, nonresponse rates for selected high nonresponse items and for test scores were examined as a funcdon of respondent characteristics. i. Population and data file definitions. DEFINITION 1: \"ITEM.\" For purposes of this analysis, \"item\" refers to each data element or variable. For question composed of multiple subparts, each subpart eliciting a distinct response is counted as an item for item nonresponse purposes. (Thus, a single question that poses three subquestions is treated as three variables). DEFINITION 2: \"RESPONSE RATE.\" . NCES standards stipulate that item response rates (Ri) \"are to be calculated as the ratio of the number of respondents for which an in-scope response was obtained (i.e., the response conformed to acceptable categories or ranges), divided by the number of completed interviews for which the question (or questions if a composite variable) was intended to be asked.\": Ri= weighted # of respondents with in-scope responses weighted 0 of completed interviews for which question was intended to be asked. In-scope responses were considered to be valid answers (including a \"don't know\" response when this ymx a legitimate response option.) Out-of :cope responses were multiple responses to items recruir4 only a single response, refusals, and missing responses.  Item-level nonresponse. Weighted nonresponse rates equal to the proportion of eighth graders who failed to answer a particular item (that is, 1-Ri) were calculated for each item in the student questionnaire. The average item nonresponse rate (across all items) is 4.7 percent (standard deviation, 3.5 percent). Items deviate markedly from this average. For some items nonresponse is zero. For other items the nonresponse rate is as high as 21.6 percent.  2The number of items used in this analysis is the total number of items in the student questionnaire minus those items that were part of a \"mark all that apply\" sequence. These \"mark all that apply\" items were excluded because it was impossible to distinguish a msponse indicating the item did not apply from a nonresponse. 3 Unequal numbers of items in each of the thirds result because items were divided into thirds before the \"mark all that apply\" items were excluded. This practice served to preserve the equal cutting of the questionnaire into thirds regardless of whether each item in each of the thirds was used in the analysis. 4For this category, the last two sets of items were removed. These sets consisted of a combined total of '1 school and outside-school activities. The respondent was asked to indicate whether he or she did not participate in each activity, or whether he or she participated as a member or an officer. Table 4.3-1 shows descriptive statistics for the item nonresponse rates overall and for items grouped into categories depending upon their position in the questionnaire, the topic they addressed, and whether they were part of a skip or filter pattern. When items were grouped into thirds based on their serial position in the questionnaire, mean nonresponse rates appeared to differ across thirds. A slightly higher nonresponse rate is found for items near the beginning of the questionnaire, and a substantially higher nonresponse rate is found for items near the end of the questionnaire. The last two sets of items require students to indicate their participation in a number of activities. It is possible that fatigue effects, naturally occurring at the end of a long questionnaire, may be exacerbated by these somewhat tedious questions, accounting for most of the differences in the last third of the questionnaire. When nonresponse rates for the last two sets of items are compared with nonresponse rates for all the preceding items, average nonresponse for the last two sets appears to be higher (last 31 items, average proportion nonresponding=.098, versus all preceding items, average proportion nonresponding=.040). Nonresponse rates across thirds of the questionnaire were recalculated after removing the last 31 items. As shown in Table 4.3-1, differences among thirds formed in this way are smaller, but the pattern of higher nonresponse in the last third persists. This suggests that the last two sets of items account for some, but not all, of the higher nonresponse at the end of the questionnaire. Nonresponse rates for the various configurations of serial position are presented in Table 4.3-1. The NELS:88 base year student questionnaire was constructed such that questions in each of the nine sections formed topical blocks. Table 4.3-1 also shows the average nonresponse rates by topic. Although there are differences by topic, the substantially discrepant numbers of items in each of the topical categories, ranging from 2 to 69 items, suggests a cautious interpretation. The pattern suggests that nonresponse rates for questions on student participation in activities are higher than nonresponse rates for other topics, a result discussed above. The section comprising questions about language use differed in nonresponse from the sections on self-esteem/locus of control and jobs and chores. The remaining sections did not differ much from one another. Item nonresponse was also observed as a function of whether the item was part of a filter-dependent question. Thirty-two items were of this type, and nonresponse for these items was compared to the two hundred and forty-four items that were not in a dependent relationship with a filter item. As Table 4.3-1 shows, there is a slightly higher nonresponse rate for items that were filtered than for those that were not. Critical Items. A number of items in the student questionnaire were dubbed \"critical items\" because of their special interest to analysts, their policy relevance, or their usefulness in locating the student for subsequent follow-up studies. These items were edited by the NORC field personnel who administered the survey. If the response to one or more of the critical items was missing, undecipherable, or had multiple categories marked when only one response was required, the NORC field staff member privately pointed out the problem to the student. lf, after prompting, the student indicated that he or she had chosen not to answer the question, the NORC staff member marked a \"no retrieval\" response for the item. (\"No retrieval\" was indicated by filling in an oval positioned to the left of each critical item). The \"no retrieval\" responses were used later during the machine editing process to assign a \"refused\" response to the critical items. Most editing and retrieval for the student questionnaire was conducted in this manner. In a very small number of instances (fewer than 300 cases), additional critical item data retrieval had to be conducted after the questionnaire reached NORC."}, {"section_title": "Base Year Sample Design Report", "text": "The item nonresponse rate for each of the critical items is shown in Table 4.3-2. The items in this table represent the majority but not the total set of critical items. Critical items that were part of the locator information were excluded from .his analysis. With the exception of the item asking about Asian-Pacific Islander ethnic subcategorizations, the nonresponse rates for the critical items are all under .10, or 10 percent, with most being considerably less. Indibidual differences in norsesponse. Nine questions with the highest nonresponse rates were selected for analysis to determine the relationship between nonresponse and student characteristics. These questions and their nonresponse rates are listed in Table 4.3-3. Table 4.3-4 shows the proportion nonresponding to the nine items with the highest nonresponse rates by selected student characteristics. A composite nonresponse variable was created by counting the number of items for wlich a nonresponse was given across items 24, 29, 67A, 67C, 67AA, 67AC, 67AD, and 831 from Table 4.3-3 (the high nonresponse items available for the full sample of students) for each student. This composite, which could range from zero to six, was compared across levels of students' sex, racial/ethnic background, socioeconomic status, and test composite quartile. The results suggest that boys are more likely to be nonrespondents on these items than girls, averaging nonresponse to .9612 items compared to .6692 items for girls. There also appears to be different nonresponse iates across the five racial/ethnic groups. Blacks appear most likely to be nonrespondents, averagiu nonresponse to 1.417 items across the six item scale. Hispanics appear next most likely, averaging 1.026 nonresponding items. Native Americans come next with an average of .9256 items, followed by Asians, who average .8774 items. Finally, whites had the least tendency toward nonresponse, averaging .6611 items. 'hat Scores. Nonresponse patterns for test scores were observed by examining the number of items not attempted for each of the four cognitive tests. These values for the entire student sample and by sex, racial/ethnic, and SES subgroups are shown in Table 4.3-5. For all test subjects, lower SES seemed to be related to higher nonreaponse. Girls showed slightly less nonresponse than boys on math and or reading. Another method for assessing test nonresponse is to examine the percent of students who gave an answer to the final item in each test. This has been proposed as an index of test \"speededness.\" Generally, a test is considered to be \"unspeeded\" if over 80 percent cf the test takers attempt the last item. Table 4.3-6 shows that test speededness was not a problem for these broad categories of students. This suggests that the appropriate amount of time was given for completion of each of the four cognitive tests. A detailed analysis of the psychometric properties of the NELS:88 cognitive test battery can be found in the NELS:88 Base Year Psychometric Report (Rock & Pollack, 1990). Parent Questionnaire Item Nonresponse. An abbreviated item nonresponse analysis was conducted for the parent questionnaire. Item nonresponse was defined as described at the beginning of this section. As for the student questionnaire, the few items that required a \"mark all that apply\" response were eliminated front the analysis. The following percentages are based upon weighted data. Item nonresponse was some 'tat higher for the parent questionnaire than for the student questionnaire. For the parent questionnaire the average percent nonresponding is 7.58; the range extends from .2 percent to 67.57 percent. Item nonresponse is higher for items falling in the first (9.91%) and last (7.70) thirds of the questionnaire than for items falling in the middle third of the questionnaire.  outside-school activities this year, either as a member, or as an officer (for example, vice-president, coordinator, team captain)?--OTHER Note: Proportions were calculated using weighted data. Note: Proportions were calculated using weighted data. The relatively high item average percent nonresponding in the first part of the questionnaire can be accounted for by the presence of a small number of items with fairly high nonresponse rates. Most of these items involve small subgroups of iespondents. For example, about 25 percent of Asians failed to give a response designating their subgroup (BYPIOA). Between 23 and 34 percent of respondents who were answering questions about experience of the eighth graders' biological parents before or right after these parents came to the USA did nut respond to some of these questions (BYP13, BYPIS, BYP16, BYP18). Thirty-eight percent of parents did not respond to the question of which grade their foreign-born eighth grader was placed in when the child began school in the United States (BYP21). A very high (59.33% to 68.57%) percentage of parents whose child had skipped a grade did not respond to the questions asking why the child had been double-promoted (BYP42A through BYP42C), and a somewhat lower (25.70% to 43.06%) percent of parents whose child was held back a grade did not give reasons for why the child had been failed (BYP45A through BYP45C). In contrast, items with these dramatically high nonresponse rates did not appear in the middle third of the ques-donnahe. In the final section of the questionnaire, the set of questions asking about what kinds of financial arrangments parents had made for their eighth grader's education after high school (BYP84AA to BYP84AG) yielded nonresponse rates in the range of 20 to 24 percent.  "}, {"section_title": "Standard Errors and Design Effects", "text": "Measures of the variability of a statistic, such as the standard error of estimate and error variance, are familiar to most researchers. Less familiar, however, are issues that arise in calculating accurate standard errors and error variances for statistics derived from complex samples like the one used in the NELS:88 survey. Complex sample designsthose that use stratification, clustering, and unequal selection probabilities--require procedures for estimating sampling variability that are markedly different from the ones that apply when the data are from a simple random sample. In general, such complex designs yield statistics with larger sampling errors than those from a simple random sample. The impact of the sample design on the sampling error of a given statistic is often assessed by the design effect, which is the ratio of the actual error variance of the statistic to the variance that would have been obtained had the sample been a simple random sample. In this section of the report, we briefly review the procedures used to estimate standard errors for selected statistics based on the NELS:88 data, present design effects for these statistics, and discuss the meaning and use of these design effects."}, {"section_title": "Estimation Procedure", "text": "In a simple random sample, the mem is estimated as x = Exi/n. Only the numerator (i.e., the sample total) is subject to sampling error; the denominator (the sample size) is fixed. In more complex designs, such as the NELS:88 design, the mean is estimated as a ratio of estimates; for NELS:88, this ratio can be expressed as in which yhij is the weighted value for student j from school i in stratum h and xhij is the weight for that student. The numerator in equation 2is an estimaie of the relevant nnoulation total; the denominator is an estimate of the population size. Both estimates are subject to sampling"}, {"section_title": "MOE", "text": "In a classic paper, Kish and Frankel (1974) distinguished three main approaches to the computation of standard errors for ratio estimates from complex samples--balanced repeated replication, jackknife repeated replication, and Taylor Series approximation; work by Frankel (1971) andothers (e.g., iburangeau et al., 1983) indicates that the three approaches give very similar results. Consequently, it is largely a matter of convenience as to which approach is taken. We used the Taylor Series procedure to calculate the standard errors here. Kish (1965, pp. 206-208) has shown that the variance of r (as defined in equation in which E(r-R)2 = the expected value of the squared difference between the population ratio, R and the sample estimate, r, 49 63 dy = the difference between the sample estimate of the population total, y, and the population total, Y; X = the population size; X = the difference between the sample estimate of the population size, x, and the actual population size, X. If the term involving the relative error in the estimate of the population size (i.e., dx/X) is ignored, equation 3  In equation 4, Var(y) and Var(x) refer to the variance of y and x, and Cov(xy) refers to weir covariance. All of these terms can be estimated from sample data (i.e., r would replace R, x would replace X, and so on). Estimates of the variance terms are based on the variation of individual school means around the estimated stratum mean. Various rationales have been offered for the use of equation 4as a approximation to equation 3. One line of argument is based on a standard mathematical tool called Taylor Series approximation. It is this rationale which has given the approach its name."}, {"section_title": "Design Effects for NELS:88", "text": "Regardless of which method is used to calculate the standard errors for statistics derived from the NELS:88 data, they will be different from the standard errors that are based on the assumption that the data are from a simple random sample. The NELS:88 sample departs from the assumptions of simple random sampling in three major respects--the sample of students was clustered by school, both schools and students were selected with unequal probabilities, and the sample was stratified by school characteristics. A simple random sample is, by contrast, unclustered and unstratified; in addition, in a simple random sample, all members of the population have the same probability of selection. Generally, clustering and unequal selection probabilities increase the variability of sample statistics relative to a simple random sample; stratification decreases variability. The impact of these departures from simple random sampling on the precision of sample estimates is often measured by the design effect. The design effect isthe ratio of the estimate of the variance of a statistic derived when the sample design is taken into account (for example, for means and proportions based upon NELS:88 student data, variances calculated using equation (4)), to that obtained from the formula for simple random samples (i.e., var(y)/n). We calculated standard errors and design effects for 30 means and proportions based on the NELS:88 student, parent, and school data. The 30 variables for each of these computations were selected randomly from the relevant questionnaires. We calculated the standard errors and design effects for each statistic, both for the sample as a whole and for selected subgroups. For both the student and parent analyaes, the subgroups were based on the student's sex, race, and ethnicity (Asian, black, Hispanic, and white and other), school type (public, Catholic, and other private), and socioeconomic status (lowest quartile, middle two quartiles, and highest quanilc). For the school analysis, th. hgroups were based on school type and eighth grade enrollment (at or below the median and ab ... the median). Tables 5.2-1, 5.2-2, and 5.2-3 below giva the mean design effects (DEFFs) and mean root design effects (DEM) for each data set and subgroup. (The mean root design effect, or DEFT, is the mean of the square root of the design effects across the 30 items. It is the inefficiency of the survey design expressed as the ratio of standard errors, rather than as the ratio of variances). The appendices present the full set of estimated standard errors and design effects based on the student (Appendix 1), parent (Appendix 2), and school (Appendix 3) questionnaires.  On the whole, the design effects indicate that the NELS:88 sample was slightly more efficient than the High School and Beyond sample. For means and proportions based on student questionnaire data for all students, the average design effect in NELS:88 was 2.54; the comparable figure was 2.88 for the High School and Beyond sophomore cohort and 2.69 for the senior cohort. This difference is also apparent for subgroup estimates. Frankel et al. (1981) present design effects for ten subgroups 66 defined similarly to those in Table 5.2-1 above. For eight of the ten subgroups, the NELS:88 design effects are smaller on the average than those for the High School and Beyond sophomore cohort; for nine of the ten subgroups, the average NELS:88 design effects are smaller than those for the senior cohort. The increased efficiency is especially marked for students attending Catholic schools. In NELS:88, the average design effect is 2.70; in High School and Beyond, it was 3.60 for the sophomores and 3.58 for the seniors. The smaller design effects in NELS:88 may reflect the somewhat smaller cluster size used in the later survey. The High School and Beyond base year sample design called for 36 sophomore and 36 senior selections from each school; the NELS:88 sample called for the selection of only 24 core students from each school. Clustering tends to increase the variablity of survey estimates, because the observations within a cluster are similar and therefore add less information than independently selected observations. The impact of clustering depends mainly on two factorsthe number of observations within each cluster and the degree of within-cluster homogeneity. When cluster sizes vary, the impact of clustering (DEFFc) can be estimated by DEffc = 1 + (13 -1) rho (5) in which b refers to the average cluster size (the average number of students selected from each school) and rho refers to the intraclass correlation coefficient, a measure of the degree of withincluster homogeneity. If the value of rho (which varies from one variable to the next) averaged about .03 in both studies, the reduced cluster size in NELS:88 would almost exactly account for the reduction in the design effects relative to High School and Beyond. The design effects for the estimates based on parent questionnaire data (see Table 5.2-2) are similar to those for the student questionnaires. For estimates applying to all students, the mean design effect was 2.48 for the parent data and 2.54 for the student data. For a number of subgroups, however, the mean design effect is lower for the parent data than for the student data. For all but one of the subgroups, the average design effect for the student items is about the same as, or larger than, the average design effect for parent items. This suggests that the homogeneity of student responses within clusters is about the same as, or greater than, the homogeneity of parent responses within the domain clusters. Given the students' shared school experiences, in generd, sod the uniform questionnaire administration procedures, in particular, this is not surprising. For private schools, the design effect for the parent items is considerably larger than the design effect for the student items. This suggests that parents within private schools give strikingly similar responses to the 30 NELS:88 item' used in the design effects analysis. The design effects for the school questionnaire data, presented in Table 5.2-3, reflect only the impact of stratification and unequal selection probabilities; the sample of schools was not clustered. As a result, the design effects for estimates based on the school data tend to be small compared to those for estimates based on the student and parent data. The mean design effect for estimates concerning all schools is 1.80."}, {"section_title": "5.3", "text": ""}, {"section_title": "Design Effects and Approximate Standard Errors", "text": "Researchers who do not have access to software for computing accurate standard errors can use the mean design effects presented in Tables 5.2-1, 5 2-2, and 5.2-3 to approximate the standard errors of statistics based on the NELS:88 data. Standard errors for a proportion can be estimated from 67 the standard error computed using the formula for the stanoard error of a proportion based on a simple random sample and the appropriate mean root design effect (DEFT): Similarly, the standard error of a mean can be estimated from the weighted variance of the individual scores and the appropriate mean DEFT: SE = DEFT * (Var/n)la. Tables 5.2-1 through 5.2-3 make it clear that' the design effects and root &sign effects vary considerably by subgroup. It is therefore important to use the mean DEFT for the re!evant subgroup in calculating approximate standard errors for subgroup statistics. Standard error estimates may be needed for subgroups that are not tabulated here One rule of thumb may be useful in such situations. The rule of thumb states that design effects will generally be smaller for groups that are formed by subdividing the subgroups listed in the tables. (This is befatin smaller subgroups will be affected less by clustering than larger subgroups; in terms of equation [5], b will be reduced.) Estimates for Hispanic males, for example, will generally have smaller design effects than the corresponding estimates for all Hispanics or all males. For this reason, it will usually be conservative to use the subgroup mean DEFT tn approximate standard errors for estimates concerning a portion of the subgroup. This rule only applies when the variable used to subdivide a subgroup crosscuts schools. Sex is one such variable, because most sell( )ols include students of both sexes. It will not reduce the average cluster size to form groups that are based on subsets of schools. Standard errors may also be needed for other types of estimates than the simple means and proportions that are the basis for the results presented here. A second rule of thumb can be used to estimate approximate standard errors for comparisons between subgroups. If the subgroups crosscut schools, then the design effect for the difference between the subgroup means will be somewhat smaller than the design effect for the individual means; consequently, the variance of the difference estimate will be less than the sum of the variances of the two subgroup means from which it is derived: in which Var(b-a) refers to the variance of the estimated difference between the subgroup means, and Var(a) and Var(b) refer to the variances of the two subgroup means. It follows from equation (8) that Var(a) + Var(b) can be used in place of Var(b-a) with conservative results. A final rule of titumb is that more complex estimators show smaller design effects than simple estimatorsAish & Fienkel, 1974). Thus, correlation and regression coefficients tend to have smaller design effects than subgroup comparisons and subgroup comparisons have smaller design effects than means. This implies that it will be conservative to use the mean root design effects presented here in calculative approximate standard errors for complex statistics, such as multiple regression coefficients. The procedure for calculating such approximate standard errors is the same as with impler estimates: first, a standard error 's ,:alculated using the formula for data from a simple random sample; then, the standard error is multiplied by the appropriate mean root design effect. One analytic strategy for accommodating complex survey designs is to use the mean design effect to adjust for the effective sample size resulting from the design. For example, one could create a weight which is the multiplicative inverse of the design effect and use that weight (in conjunction 7'--\\ Base Year Sample Design Report with sampling weights) to deflate the obtained sample size to take into account the inefficiencies due to a sample depign that is a departure from a simple random sample. Using this procedure, statistics calculated by a statistical program such as SPSS will reflect the reduction in sample size in the calculation of standard errors and degrees of freedom. Such techniques capture the effect of the sample design on sample statistics only approximately. However, while not providing a fu'l accounting .he sample design, this procedure provides some adjustment for the sample design, and is probably better than conducting analysis that assumes the data were collected from a simple random sample. The analyst applying this correction procedure should carefully examine the statistical software he or she is using, and assess whether the program treats weights in such a way as to produce the effect described above. BD Tourangeau, R., McWilliams, H., Jones, C., Frankel, M., & (\"Brien, F., High School and Beyond First Foilow-Up (1982) Sample Design Report (Chicago: NORC, 1983 "}]