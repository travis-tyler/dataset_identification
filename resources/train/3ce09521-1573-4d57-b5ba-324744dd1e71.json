[{"section_title": "Abstract", "text": "Abstract Distinguishing progressive mild cognitive impairment (pMCI) from stable mild cognitive impairment (sMCI) is critical for identification of patients who are at risk for Alzheimer's disease (AD), so that early treatment can be administered. In this paper, we propose a pMCI/ sMCI classification framework that harnesses information available in longitudinal magnetic resonance imaging (MRI) data, which could be incomplete, to improve diagnostic accuracy. Volumetric features were first extracted from the baseline MRI scan and subsequent scans acquired after 6, 12, and 18 months. Dynamic features were then obtained using the 18th month scan as the reference and computing the ratios of feature differences for the earlier scans. Features that are linearly or non-linearly correlated with diagnostic labels are then selected using two elastic net sparse learning algorithms. Missing feature values due to the incomplete longitudinal data are imputed using a low-rank matrix completion method. Finally, based on the completed feature matrix, we build a multi-kernel support vector machine (mkSVM) to predict the diagnostic label of samples with unknown diagnostic statuses. Our evaluation indicates that a diagnosis accuracy as high as 78.2 % can be achieved when information from the longitudinal scans is used-6.6 % higher than the case using only the reference time point image. In other words, information provided by the longitudinal history of the disease improves diagnosis accuracy."}, {"section_title": "Introduction", "text": "Mild cognitive impairment (MCI) is a brain disorder characterized by noticeable impairment of cognitive functions, such as memory loss, beyond the decline due to normal aging, but not to the extent of significantly affecting activities of daily life. MCI is caused by neurodegeneration (loss or death of brain neuron cell), and is generally known as a transitional stage between normal aging and Alzheimer's disease (AD). Studies suggest that 3-19 % of adults older than 65 years may be affected by MCI (Gauthier et al. 2006) . Although some MCI patients recover over time, more than half will progress to dementia within 5 years (Gauthier et al. 2006) . To identify patients who need to receive early treatment, it is vital to distinguish between For the Alzheimer's Disease Neuroimaging Initiative.\nData used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (http://adni.loni.ucla.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ ADNI_Acknowledgement_List.pdf.\nprogressive MCI (pMCI) patients, who will eventually progress to AD, from stable MCI (sMCI) patients, whose conditions will cease to deteriorate.\nIn the past few decades, a lot of studies have been focused on identifying possible biomarkers for AD/MCI diagnosis using neuroimaging data acquired with magnetic resonance imaging (MRI) (Weiner et al. 2013) . MRI is a noninvasive technology that can be used to quantify gray and white matter integrity of the brain with high reproducibility (Haacke et al. 1999; Jack et al. 2008) . MRI-based studies indicate that AD/ MCI subjects are normally associated with cortical thinning and brain atrophy in the temporal lobe, parietal lobe, frontal cortex, and other brain regions (Jack et al. 2002; Stefan et al. 2006; Whitwell et al. 2008) . Based on these findings, different types of MRI-based measures, e.g., region-of-interest (ROI)-based volumetric measures (Jack et al. 1999; Zhou et al. 2011; Li et al. 2015) , cortical thickness measures (Querbes et al. 2009; Wee et al. 2013) , morphometric measures (Ashburner and Friston 2000; Hua et al. 2008) , functional measures Wee et al. (2012a Wee et al. ( , 2014 , and diffusion measures Wee et al. (2011 Wee et al. ( , 2012b have been proposed for AD/MCI diagnosis. In addition to MRI, other modalities such as positron emission topography (PET), cerebrospinal fluid (CSF), and genetic data have also been used (Kohannim et al. 2010; Hinrichs et al. 2009; Huang et al. 2011; Wang et al. 2012; Zhang and Shen 2012a) . Compared with other modalities, structural MRI offers important advantages such as non-invasiveness, rapid acquisition, lower acquisition cost, and greater data availability. In this study, we focus our analysis on MRI data, even though our framework can be generalized and extended to include other modalities.\nThere are two broad categories of AD studies: crosssectional and longitudinal. In cross-sectional studies, only data from one time point are involved. Most studies using the Alzheimer's Disease Neuroimaging Initiative (ADNI) 1 dataset employ only the baseline data (i.e., the first screening data) and, therefore, belong to this category (Davatzikos et al. 2011; Hinrichs et al. 2011; Weiner et al. 2013) . The preference for cross-sectional studies is partly due to the completeness of the baseline data, both in terms of the number of subjects in different cohorts (i.e., AD, MCI and NC) and the number of modalities (i.e., MRI, PET and CSF) when compared with the data collected at subsequent time points. Due to greater data availability in the baseline, cross-sectional studies using the baseline data might benefit from greater statistical power. Moreover, complementary data from multiple modalities, which are most complete at the baseline, can be employed for more accurate AD/MCI analysis (Wang et al. 2012; Zhang and Shen 2012a; Thung et al. 2014; Liu et al. 2014; Huang et al. 2015; Zhu et al. 2015; Thung et al. 2015; AdeliMosabbeb et al. 2015) . However, cross-sectional studies can only provide information on a single biological and neuropsychological state of the disease and is hence insufficient for characterizing the temporal evolution of the disease.\nIn longitudinal studies, data collected at multiple time points are involved Liu et al. 2013; Weiner et al. 2013) . In contrast to data from a single time point, longitudinal data capture temporal dynamics, such as the rate of deterioration, of disease progression. For example, we can quantify pathological changes using MRIbased measurements from multiple time points to determine whether an MCI patient is progressing toward AD. However, due to various practical issues (e.g., subject dropouts, image quality issues, etc.), longitudinal data are quite often incomplete. The easiest way to deal with this problem, as done in most studies, is by discarding samples with missing data Weiner et al. 2013 ) and performing analysis only using data that are complete. By doing so, sample size is inevitably reduced and hence statistical power is sacrificed (van der Heijden et al. 2006; MacCallum et al. 1996 ). An alternative to dealing with missing data is to perform data imputation, using methods such as k-nearest neighbor (KNN) (Speed 2003; Troyanskaya et al. 2001) , expectation maximization (EM) (Schneider 2001) , and low-rank matrix completion (Cand\u00e8s and Recht 2009; Goldberg et al. 2010; Sanroma et al. 2014) .\nIn this study, we utilize longitudinal MRI data, with explicit consideration of missing data, for pMCI/sMCI classification. We hypothesize that pMCI patients undergo a trajectory of brain structural changes that is different from sMCI patients. This trajectory, which is reflected by the longitudinal MRI data, can be used to improve pMCI/ sMCI classification. Our main contributions and findings are as follows:\n1. We developed a pMCI identification framework using longitudinal incomplete data. Unlike longitudinal studies that discard incomplete data, we impute missing feature values and diagnostic labels of a testing sample simultaneously by utilizing all available samples. 2. We developed a feature selection method for incomplete longitudinal data, extracting, at each time point, features that are correlated linearly and non-linearly with the class labels. 3. We found that dense temporal sampling is not always necessary for improving pMCI identification. Our investigation shows that scans that are appropriately spaced out in time are often sufficient.\nOur framework uses both static and dynamic features to predict the diagnostic label of a sample. The static features are the gray matter volumes of 93 ROIs extracted from each MR image in the longitudinal series. The dynamic features are the volumetric change ratios computed between the image at a reference time point and images at other time points. By concatenating the static and dynamic features, each sample can be represented as a feature vector. All the feature vectors are stacked to form a feature matrix. Instead of performing imputation based on the whole matrix, we apply feature selection to reduce the matrix to contain only discriminative features so that imputation can be carried out more effectively. More specifically, features that show linear and nonlinear relationships with clinical diagnostic labels are selected via least-squares and logistic elastic net regressions (Liu et al. 2009 ). Missing feature values in the reduced feature matrix are then imputed using a label-guided low-rank matrix completion method (Goldberg et al. 2010 ). Lastly, we will build a classifier using multiple kernel SVM (Rakotomamonjy et al. 2008) , which combines a set of linear, Gaussian, and polynomial kernels. Experimental results show that our framework improves identification accuracy when longitudinal MRI data with dynamic features are used."}, {"section_title": "Materials and data processing ADNI background", "text": "Data used in the preparation of this article were obtained from the ADNI database (http://adni.loni.ucla.edu). The ADNI was launched in 2003 by the National Institute on Aging (NIA), the National Institute of Biomedical Imaging and Bioengineering (NIBIB), the Food and Drug Administration (FDA), private pharmaceutical companies and non-profit organizations, as a $60 million, 5-year publicprivate partnership. The primary goal of ADNI has been to test whether serial MRI, PET, other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of MCI and early AD. Determination of sensitive and specific markers of very early AD progression is intended to aid researchers and clinicians to develop new treatments and monitor their effectiveness, as well as lessen the time and cost of clinical trials. The Principal Investigator of this initiative is Michael W. Weiner, MD, VA Medical Center and University of California, San Francisco. ADNI is the result of efforts of many co-investigators from a broad range of academic institutions and private corporations, and subjects have been recruited from over 50 sites across the U.S. and Canada. The initial goal of ADNI was to recruit 800 subjects but ADNI has been followed by ADNI-GO and ADNI-2. To date, these three protocols have recruited over 1500 adults, ages 55-90, to participate in the research, consisting of cognitively normal older individuals, people with early or late MCI, and people with early AD. The follow-up duration of each group is specified in the protocols for ADNI-1, ADNI-2 and ADNI-GO. Subjects originally recruited for ADNI-1 and ADNI-GO had the option to be followed in ADNI-2. For up-to-date information, see www.adni-info.org."}, {"section_title": "Materials", "text": "In the ADNI dataset, there are about 400 MCI subjects scanned at screening time (i.e., the baseline). After the baseline scan, follow-up scans were acquired every 6 or 12 months for up to 84 months. MCI subjects who progressed to AD after some period of time were retrospectively labeled as pMCI subjects. Following this convention, the labeling of pMCI/sMCI is affected by the reference time point and the time period in which the patients are monitored for conversion to AD.\nWe chose the 18th month as the reference and 30 months as the time period to monitor for conversion, so that there is a sufficient number of earlier scans (i.e., baseline, 6th, and 12th month) in each cohort (i.e., pMCI and sMCI). Thus, patients who converted to AD within the 18th to 48th month (a duration of 30 months) are labeled as pMCI patients, and those whose conditions do not deteriorate are labeled as sMCI patients. Patients who converted to AD prior to the 18th month were excluded from the study because they were no longer MCI patients at the reference time point. Patients who converted to AD after the 48th month were also excluded to avoid labeling uncertainty. Based on the criteria mentioned above, a total of 60 pMCI and 53 sMCI subjects were available for this study. The associated demographic information is shown in Table 1 . Using the data at and before the 18th month, we investigated whether longitudinal MRI scans are conducive to improving pMCI/sMCI classification."}, {"section_title": "MR image processing", "text": "A maximum of 4 time points were used-baseline, 6th month, 12th month, and 18th month. The processing steps involved are described as follows. Each MRI T 1 -weighted image was first anterior commissure (AC)-posterior commissure (PC) corrected using MIPAV 2 , intensity inhomogeneity corrected using the N3 algorithm (Sled et al. 1998) , skull stripped ) with manual editing, and cerebellum-removed . We then used FAST (Zhang et al. 2001) in the FSL package 3 to segment the image into gray matter (GM), white matter (WM), and cerebrospinal fluid (CSF), and used HAMMER (Shen and Davatzikos 2002) to register the images to a common space. GM volumes obtained from 93 ROIs defined in (Kabani 1998) , normalized by the total intracranial volume, were extracted as features.\npMCI/sMCI classification framework"}, {"section_title": "Image features", "text": "Let fT t 2 R n\u00c2r ; t \u00bc 1; . . .; 4g denote the matrices containing the r \u00bc 93 ROI volumetric features of n subject samples at baseline (t \u00bc 1), 6th month, 12th month, and 18th month (t \u00bc 4), respectively. Additional dynamic features are computed as the volumetric change ratios between 18th month (reference time point) and earlier time points, i.e., fD t \u00bc \u00f0T t \u00c0 T 4 \u00de \u00c1 =\u00f0T 4 \u00fe c\u00de; t \u00bc 1; . . .; 3g, where \u00c1= denotes element-wise division, and c is a small constant to avoid division by zero. Since the ADNI longitudinal data were not acquired exactly at the design time points, we normalize the dynamic features using the following method. The normalizer for the jth row (i.e., jth subject) of D t is denoted as\nThe corrected dynamic features is given by\nwhere W t \u00bc diag\u00f0w t;1 ; w t;2 ; \u00c1 \u00c1 \u00c1 ; w t;j ; \u00c1 \u00c1 \u00c1 ; w t;n \u00de. \nDenoting y 2 R n\u00c21 as the target label vector (1 for pMCI, \u00c01 for sMCI), our objective is to predict y given X."}, {"section_title": "Framework overview", "text": "The proposed framework (see Fig. 1 ) consists of three components: feature selection, data imputation using matrix completion, and classification using multi-kernel support vector machine (mkSVM). We partitioned the feature matrix into training and testing samples:\nOur model was trained with X trn 2 R n trn \u00c2d , and was tested with X tst 2 R n tst \u00c2d . The corresponding target label vectors for the training and testing data are denoted as y trn 2 R n trn \u00c21 and y tst 2 R n tst \u00c21 , respectively. Since some feature values are possibly missing in X due to the incomplete dataset, data imputation is needed before it can be used to train a classifier. To reduce the number of values that need to be imputed and also to restrict imputation to only discriminative features, we first reduce the size of X trn via feature selection."}, {"section_title": "Feature selection", "text": "Sparse feature selection methods such as least absolute shrinkage and selection operator (Lasso) (Tibshirani 1996) and its variants (Zou and Hastie 2005; Friedman et al. 2010; Simon et al. 2013; Zhou et al. 2012; Zhang et al. 2012 ) have been shown to be effective in dealing with high-dimensional data. However, these feature selection methods can only be applied to complete data with no missing values. w t;j \u00bc Actual interval between the t-th and the reference scan time of subject j Designed interval between the t-th and the reference scan time :\nOne workaround for this problem is to apply feature selection on complete subset(s) of the data. Three methods can be used to extract such subsets, as shown in Fig. 2 . Each feature submatrix in the figure represents data from one time point (volumetric or dynamic features), and each color box in the figure represents the subset of data. For simplicity, we use an example of incomplete data with only three feature submatrices. The first method (Fig. 2a) , which is widely used, discards samples with missing values and retains only complete samples, thus throwing away a significant amount of useful information. The second and third methods (Fig. 2b , c) avoid this problem using multiple complete subsets. Specifically, the second method defines subsets according to feature submatrices, while the third method defines subsets according to combinations of features. To maximize the number of samples for each subset, the third method allows overlaps among subsets. In this study, the second method was used. In longitudinal data, the volumetric features vary gradually over time and are highly correlated. Based on our experience, if sparse method such as Lasso is used for feature selection on data from two or more time points, similar but not necessarily redundant features will be removed. We avoid this problem by performing elastic net feature selection for each submatrix X \u00f0i\u00de trn , i.e., second method in Fig. 2b . In addition, we use two types of elastic net regressions, leastsquares and logistic regressions both with elastic net regularizer (Liu et al. , 2009) , to select features that are linearly and non-linearly correlated with the target labels (see feature selection component in Fig. 1 ). The problems associated with the two sparse regressions are given as . . .g. This implies that the same set of features will be selected for all feature submatrices in X trn . Features found discriminant at one time point will also be selected at other time points."}, {"section_title": "Matrix completion", "text": "After feature selection, we obtain feature-reduced matrices Z trn and Z tst , which are possibly incomplete due to missing data. While there are a number of existing data imputation methods (Schneider 2001; Goldberg et al. 2010; Ma et al. 2011; Troyanskaya et al. 2001) , we chose to use a labelguided low-rank matrix completion algorithm (Goldberg et al. 2010; Thung et al. 2014) . This is because the longitudinal data are highly correlated and each column of Z \u00bc Z trn Z tst ! could potentially be represented by a linear combination of other columns in Z, satisfying the low-rank assumption of low-rank matrix completion algorithm. Following (Goldberg et al. 2010) , we augmented Z with the target label vector before performing matrix completion. More specifically, matrix completion is performed on\n, where y tst is unknown and will be imputed . The imputed or completed matrix is denoted as b\n, where b y trn is same as y trn as there is no missing label in the training set. The imputation optimization problem is given as:\nwhere X Z and X y denote the set of indices of the observed non-missing values in Z and the label vector y, respectively, |.| denotes the cardinality of a set, : k k \u00c3 denotes the trace norm, q ij and b q ij denote the original and the predicted values, respectively. The first term in the above equation is to ensure that the predicted matrix b Q is low rank. The second term is the squared loss term L 1 \u00f0u; v\u00de \u00bc 1=2\u00f0u \u00c0 v\u00de 2 , which accounts for errors in feature imputation. The final term is a logistic loss term L 2 \u00f0u; v\u00de \u00bc log\u00f01 \u00fe exp\u00f0\u00c0uv\u00de\u00de, which accounts for errors in label imputation. k m and l are the positive-valued tuning parameters. The imputed label vector y tst can be taken as the predicted labels without explicitly building a classifier.\nTo improve the prediction accuracy, we detail in the following how classifiers can be constructed based on the completed feature matrix."}, {"section_title": "Classification", "text": "Various classifiers can be constructed based on the completed feature matrix b Z trn to predict the class label vector y. First, we applied least-squares elastic nets, logistic elastic net, linear kernel SVM, and Gaussian kernel SVM on b Z trn as a whole for prediction.\nSecond, we used the kernel trick to combine different feature submatrices into a single matrix for prediction. Specifically, we built one or more kernels for each feature submatrix, and combined them into a single kernel using either multiple kernel learning algorithms, such as multikernel SVM (mkSVM) or averaging algorithms, such as mean multi-kernel SVM (mean-mkSVM)."}, {"section_title": "Least-square sparse regression elastic net (leastR)", "text": "We learn a sparse weight b that, when mapped by Z, best explains y trn . This is achieved by solving a l 2 -norm and l 1 -norm-regularized least-squares problem. That is\nThe label vector y tst is predicted based on b Z test and is given by b Z test b."}, {"section_title": "Logistic sparse regression elastic net (logisticR)", "text": "This is similar to leastR classifier, but with the leastsquares function replaced by the logistic loss function. That is\nThe label vector y tst is given by b"}, {"section_title": "Support vector machine (SVM)", "text": "The dual formulation of SVM (Chang and Lin 2011) is given as\nwhere a \u00bc \u00bda 1 ; . . .; a j ; . . .; a n trn 2 R n trn \u00fe is a vector of dual variables, C is the regularization parameter, K\u00f0b z j ; b z k \u00de is the kernel that indicates the similarity between sample b z j 2 R 1\u00c2n f and b z k 2 R 1\u00c2n f in b Z trn , and n f is the number of features for each sample. We built linear kernel\n=s 2 ) using b Z trn for the prediction of y tst ."}, {"section_title": "Multi-kernel SVM (mkSVM)", "text": "Combining multiple kernels (G\u00f6nen and Alpayd\u0131n 2011) has been shown to improve classification performance. Thus, instead of using a single kernel, we combined multiple kernels using multi-kernel support vector machine (mkSVM). Specifically, we built multiple kernels (i.e., linear, Gaussian and polynomial kernels with different parameters) for each submatrix in b Z trn , and then used multiple kernel learning (MKL) to estimate the kernel combination weights.\nThe dual formulation of mkSVM (Rakotomamonjy et al. 2008 ) is given as\nwhere a 2 R n trn \u00fe is a vector of dual variables, g 2 R Q\u00c2m \u00fe is the kernel weight matrix, C is the regularization parameter, Q is the total number of kernels, and K q \u00f0b z\n. We employed 5 values for s and 3 values for p. Thus, each submatrix in b z trn corresponds to a total of Q \u00bc 9 kernels, and for m submatrices, we have a total of 9m kernels. These kernels were combined using the learned weight matrix g in\n, which is similar to l 1 -norm, causes g to be sparse, and only the most discriminative kernels were selected. After obtaining a and g from the training data, the class label for a test sample b z tst was estimated by "}, {"section_title": "Mean multi-kernel SVM (mean-mkSVM)", "text": "Mean-mkSVM is similar to mkSVM, but instead of learning the kernel weights, the kernels are combined by simple averaging and, therefore, all kernels are combined with equal weighting. Since nonlinear kernel is very sensitive to parameter selection, only the linear kernel is used for mean-mkSVM, similar to ."}, {"section_title": "Results and discussions Performance measure", "text": "We used accuracy, sensitivity, and specificity to evaluate the performance of the proposed classification framework.\nIn addition, the Receiver Operating Characteristic (ROC) curve was used to summarize the classifier performance over a range of trade-offs between true-positive and false-positive error rates (Swets 1988 ROC Curve (AUC) (Lee 2000; Duda et al. 2012; Bradley 1997) , used as the fourth measure, is independent to the decision criterion used and is less sensitive to data imbalance. Thus, AUC is the most representative performance measure in this study. All the results reported in this paper are the average of 10 repetitions in tenfold cross-validation."}, {"section_title": "Parameter selection", "text": "We evaluated our framework using different classifiers, including leastR, logisticR, linear kernel SVM (SVMlin), Gaussian kernel SVM (SVMrbf), mkSVM, and meanmkSVM. All the hyperparameters (e.g., k; C; and s) used for the classifiers (and the feature selections) are determined through fivefold cross-validation, using only the training data. Also included are the classification results obtained using low-rank label imputation technique (LRMC), as described in our previous work (Thung et al. 2013 . The parameters k m and l of LRMC are set at 0.1 and 10 \u00c05 , respectively, based on our previous work )."}, {"section_title": "Feature combinations", "text": "Three feature combinations were tested in this study (see Fig. 3 ), i.e., F T -volumetric features only; F TD -volumetric and all the corresponding dynamic features; F T4D -volumetric features from the fourth (18 month) time point and all the corresponding dynamic features. The number of dynamic features used was determined by the number of time points used in the experimental study. For example, if The best results are marked in bold using data from more earlier time points. The plots in Figs. 4 and 5 also indicate that the performance of the framework is not always improved when using data from more than one time point. For example, when compared with time point 4 (reference or ''latest'' time point), the performance of the proposed classifier (i.e., mkSVM) drops slightly when including additional data from time point 3, but increases afterward when using additional data from time points 2 and 1. Similar trend is observed using other classifiers (e.g., leastR, SVMlin, LRMC, mean-mkSVM), but with a much significant drop in performance using the dynamic features from time points 4 and 3. This is potentially due to the relatively smaller time separation between time points 4 and 3 (only 6 months apart). The images are hence relatively similar and do not offer much additional information, and also the dynamic features could be noisy. Nevertheless, unlike other classifiers, the proposed mkSVM, which assigns different weights to the kernels derived from different data, can avoid this issue effectively."}, {"section_title": "Evaluation: effect of scan time interval", "text": "We investigate here the effect of scan time interval between two MRI scans on classification accuracy. Figures 6 and 7 show the results of pMCI identification using data from the reference time point and one additional earlier time point. From the figure, it can be seen that when the scanning time interval of the 2 time points increases, so does the classification accuracy and the AUC. This trend is seen for all the three feature combinations and for almost all the classifiers. The reason behind this observation could be that the brain structures of the subjects change more significantly when scan time interval is increased, making the dynamic features more useful. pMCI subjects encounters a greater rate of neurodegeneration than sMCI subjects. The dynamic features capture this difference from the longitudinal data and improve pMCI/ sMCI classification accuracy. We also note that the improvement gained using 2 time points is comparable to that using more time points. This observation indicates that the time interval between scans is a more important factor in pMCI identification than the number of scans."}, {"section_title": "Evaluation: effect of feature combination", "text": "When comparing the performance difference among F T , F TD , and F T4D , we found that F TD and F T4D , which utilize dynamic features, are significantly better than F T , which uses only MRI ROI-based features. This observation can be seen in Figs. 4, 5, 6 and 7. For example, considering all time points, the best AUC values are 0.783, 0.823, and 0.829, for F T , F TD , and F T4D , respectively. If considering only the first and fourth time points, the best AUCs achieved by F T , F TD , and F T4D are 0.793, 0.843, and 0.833, respectively. In other words, using F TD and F T4D gives an increase of accuracy of about 4-5 % over F T , when using time points 1 and 4, achieved by our proposed classifier (mkSVM)."}, {"section_title": "Evaluation: effect of classifier", "text": "The performance of the classifiers depends on the feature combination used. For F T , the performances of SVMlin, SVMrbf, LRMC, mean-mkSVM, and mkSVM are comparable, while the performances of leastR and logisticR are relatively inferior. For F TD and F T4D , mkSVM in overall performs better than other classifiers for all combinations of time points. For F TD , the highest AUC value (0.843) is achieved by mkSVM, using data from time points 1 and 4. This is 6.6 % higher than its AUC when using only time point 4. Similar conclusions can also be drawn when using F T4D . The complete results in terms of accuracy, sensitivity, specificity and AUC are shown in Tables 2, 3 , 4 and 5. In terms of sensitivity, mkSVM always performs the best for all these feature combinations."}, {"section_title": "Evaluation: effect of feature selection methods", "text": "We compared the performance when either Eq. (3) (igLeastR) or Eq. (4) (igLogisticR) is applied independently. Student's t test was also applied for feature selection by removing features that are not statistically significantly correlated with the class label (igt test). We also compared our method with least-squares (glLeastR) and logistic (glLogisticR) group Lasso regressions (Liu The best results are marked in bold Table 6 Classification AUC values for the proposed classifier (mkSVM) using different feature selection methods The best results are marked in bold et al. 2009). Specifically, regression is performed using the submatrix of complete data extracted from the full feature matrix (Fig. 2a) , and the weight matrix is penalized via l 2;1 -norm. Weights corresponding to features of the same ROI from different feature submatrices are grouped using the l 2 -norm and the different groups are associated using the l 1 -norm. or from two well-separated time points (e.g., T 4;1 ). The relatively lower performance of group Lasso is probably because (1) group Lasso tends to select ROIs that are discriminant for all time points, which possibly removes discriminant features that only appeared at certain time points, (2) only samples with a complete feature set are used in feature selection. In contrast, the proposed method avoids the weaknesses above using elastic net regression on each feature submatrix. The superior performance of the proposed method over leastR and logisticR methods also shows the advantage of combining features selected by two elastic net regressions within each feature submatrix while at the same time requiring common feature sets across feature submatrices. Fig. 9 Frequencies of features selected using the proposed feature selection method and F T4D data, under different combinations of time points"}, {"section_title": "Frequency map of selected features", "text": "Figures 8 and 9 show the frequency map of selected features using the proposed feature selection method, for F T and F T4D , respectively. The left and right maps of the figure show the feature selection frequency maps for ROIs from the left hemisphere and right hemisphere of the brain, respectively. Each row of the map denotes feature from one ROI, with its name shown between the two maps. There are 47 rows in each map, corresponding to 46 ROIs for each hemisphere and 1 ROI for the corpus callosum. Each column denotes a combination of time points. Comparing Figs. 8 and 9, more ROIs are selected due to the existence of dynamic features. Some frequently selected ROIs include precuneus, hippocampal formation, peririhinal cortex, entorhinal cortex, amygdala, middle temporal gyrus, inferior temporal gyrus, and lateral occipitotemporal gyrus. These regions are involved in memory formation, processing and storing (Burgess et al. 2002; Stanislav et al. 2013; Smith and Kosslyn 2006; Poulin et al. 2011; Ranganath 2006; Yonelinas et al. 2001 ). The corpus callosum, which is associated with AD and MCI (Di Paola et al. 2010; Wang et al. 2006) , is also frequently selected."}, {"section_title": "Probability density function of dynamic features", "text": "We have suggested previously that the improvement of pMCI identification using longitudinal data with longer scan time interval (Figs. 6, 7) is due to the larger volumetric differences and hence more significant dynamic features. To evaluate the validity of this observation, we used 31 most frequently selected ROIs (Fig. 9) and plot the distribution of the corresponding volumetric differences (i.e., fT i \u00c0 T 4 g) in Fig. 10 . The results indicate that the distribution of T 1 \u00c0 T 4 is the broadest, followed by T 2 \u00c0 T 4 and T 3 \u00c0 T 4 . This observation is consistent with our assumption that D 1 , which represents the dynamic features obtained from MRI data with relatively larger scan interval, is able to capture the variability between pMCI and sMCI subjects."}, {"section_title": "Conclusion", "text": "We have proposed a pMCI/sMCI classification framework that harnesses the additional information given by possibly incomplete longitudinal data. The experimental results demonstrate that classification performance improves when data from multiple time points with sufficient separation in scanning times are used. We also observed that, rather than using only static volumetric features, dynamic features can be used to improve classification performance. Feature selection using combined elastic net regressions has been shown to be more effective than feature selection using these regression techniques independently. The proposed classifier (mkSVM) outperforms other state-of-the-art classifiers when dynamic features are used. The best classification accuracy using two time points was achieved by mkSVM at 78.2 %, which is 6.6 % more than the accuracy it achieved using only the reference time point. Similarly, the best AUC using two time points was achieved by mkSVM at 84.3 %, which is 6.6 % more than the AUC it achieved using only the reference time point.\nIn the future, we will extend the current framework to work with multiple imaging modalities (e.g., PET and CSF) and multiple clinical scores (e.g., MMSE Wechsler 1945 and ADAS Doraiswamy et al. 1997 ) to further improve classification performance. "}]