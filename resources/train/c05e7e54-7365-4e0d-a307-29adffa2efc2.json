[{"section_title": "Abstract", "text": "ABSTRACT Machine learning has been successfully applied to many areas of science and engineering. Some examples include time series prediction, optical character recognition, signal and image classification in biomedical applications for diagnosis and prognosis and so on. In the theory of semi-supervised learning, we have a training set and an unlabeled data, that are employed to fit a prediction model or learner, with the help of an iterative algorithm, such as the expectation-maximization algorithm. In this paper, a novel non-parametric approach of the so-called case-based statistical learning is proposed in a low-dimensional classification problem. This supervised feature selection scheme analyzes the discrete set of outcomes in the classification problem by hypothesis-testing and makes assumptions on these outcome values to obtain the most likely prediction model at the training stage. A novel prediction model is described in terms of the output scores of a confidence-based support vector machine classifier under class-hypothesis testing. To have a more accurate prediction by considering the unlabeled points, the distribution of unlabeled examples must be relevant for the classification problem. The estimation of the error rates from a well-trained support vector machines allows us to propose a non-parametric approach avoiding the use of Gaussian density functionbased models in the likelihood ratio test.\nINDEX TERMS Statistical learning and decision theory, support vector machines (SVM), hypothesis testing, partial least squares, conditional-error rate."}, {"section_title": "I. INTRODUCTION", "text": "Machine learning has been successfully applied to many areas of science and engineering [1] . Some examples include time series prediction [2] , optical character recognition [3] , signal and image classification in biomedical applications for diagnosis and prognosis [4] , etc. The support vector machine (SVM) is a recently developed paradigm in machine learning [5] with applications to brain image processing and classification [6] - [11] . In this scenario, the purpose of these techniques is to provide objective clinical decisions and an early detection of abnormal perfussion/metabolic patterns [11] .\nThe performance control of a SVM is a major requirement in any classification problem [12] , i.e. the development of computer-aided diagnosis (CAD) systems [13] , [14] . Several sophisticated CAD systems have been recently proposed for the diagnosis of AD [15] - [18] . As an example, in [18] a viewaligned hypergraph learning method based on the sparsity representation is proposed. Although, these systems achieve a good performance in terms of accuracy and a reasonable computational cost they employ all original features for model construction, while there may exist noisy or redundant information in original features [18] . It is interesting to select those most informative features in terms of class-separability for subsequent model construction but, in the neuroimaging field with an uncertain labeling process (ground truth), the learning ability of such methods could be significantly affected. Nevertheless, this is the main goal of the proposed methodology, to use the class-information at the validation stage to propose more accurate models.\nTypically, the performance control is specified in terms of minimum error rate or overall accuracy, although many factors including noise, the inherent complexity of the classification task, computational constraints, etc., may inhibit the system from achieving the performance requirements for an specific application [19] . Fortunately, other solutions based on the optimal classification theory proposed in [12] , i.e. the ones based on controlled error rates [20] , have been analyzed and demonstrated demonstrated their reliability and efficiency as methodologies for the classifier design. As an example, this methodology was firstly presented in the neuroimaging field in [13] , where the development of the CAD systems using functional image modalities, such as positron emission tomography (PET) or single-photon emission tomography (SPECT), established a confidence level in diagnostics.\nOn the other hand, decision theory [21] , that is, the application of statistical hypothesis testing to the detection problem, is a well-known statistical technique that allows model/feature selection in the cross-validation (CV) loop [10] , [22] . The so-called case-based learning (CSL) employs a model selection algorithm in order to select the optimal classifier that minimizes the CV error (see figure 1 ) in a semi-supervised fashion. In a nutshell, this method consist in performing hypothesis testing [21] on the set of unlabeled validation responses or outcomes by the extraction of extended datasets under null&alternative hypotheses. Other approaches for model/feature selection are based on Information Theory, filter methods, embedded and wrapper methods, etc. [23] , [24] . Unlike the latter methods CSL evaluates a likelihood ratio test on the class-dependent features and selects the most probable model among them. In particular, supervised feature extraction (SFE) allows to obtain different datasets of features by hypothesizing on the unknown outcomes or responses of the validation pattern. As an example, in the binary classification problem, with classes \u03c9 0 and \u03c9 1 , two different datasets can be derived with two prediction models for each validation pattern, corresponding to the null H 0 : \u03c9 0 & alternative H 1 : \u03c9 1 hypotheses. The difference between them can be assessed in terms of probability by using either a model-based hypothesis testing framework, as preliminary proposed in [10] , or the classifier configuration derived from the novel datasets, i.e. in the output-score space of the support vectors, as shown in this paper. The influence of the validation pattern on these prediction models, i.e. the trained SVM, will depend on the relevance of the features that represent the samples in feature space and on the inherent complexity of the classification task beforehand. Here, it is measured in terms of the output scores of a confidence-based support vector machine classifier whilst in [10] this issue was not managed. In addition, this paper effectively demonstrates the benefit of the proposed approach by theoretically simulating the histogram of two classes under the class-hypotheses, showing the reduced overlap between distributions when the real hypothesis is considered.\nThis paper is organized as follows. In section II, a background to the Bayes theory for solving classification problems is provided. A connection of this theory to the CSL methodology is derived in section III providing a novel likelihood ratio based on the error-rate margin under the classhypotheses. In the following subsection III-B two classical feature extraction methods are proposed for construction the extended datasets, such as Least Squares (LS) and Partial LS. In addition an implementation using the SVM classifier is shown in subsection III-C where the two-class classification problem is assumed, although it can also be extended to a multi-class case. Finally, section IV, presents experimental results to demonstrate the efficiency of the proposed method using synthetic and medical image databases. A full experimental framework is provided to demonstrate the benefits of the CSL acting on baseline approaches, i.e. using LS and PLS FE methods and SVM classifiers for leave one out-CV error minimization. In section V, conclusions are drawn."}, {"section_title": "II. BAYES FORMULATION OF THE CLASSIFICATION PROBLEM", "text": "Consider a set of patterns Z = {X \u2208 R p , Y \u2208 R}, represented by a set of vectors X in a d-dimensional Euclidean space and admissible classes Y \u2208 {w 0 , w 1 }. The evidence of the feature vector can be written as:\nwhere p(w i ) is the prior probability of class w i and, accordingly to Bayes' formula, the posterior probability is defined as:\nGiven the ideal learner or mappingf : R d \u2192 {w 0 , w 1 } that assigns each feature vector to its real class, the classification problem can be tackled by minimizing the sample conditional error with respect to the set of mappings {f }:\nThe classifier f naturally divides the feature space R d into two regions named R 0 and R 1 , at least, assigning any new pattern to the category lying on the same side of the decision surface. The error rates E i can be computed by integrating on these subspaces the conditional probabilities:"}, {"section_title": "III. A NOVEL CASE-BASED LEARNING ON THE CONDITIONAL ERROR", "text": "Under the CSL approach [10] , a class is considered as an hypothesis on a Neyman-Pearson hypothesis testing framework, that is, H i = w i for i = {0, 1}. Thus we try to maximize the probability of detection P D = P(w i ; w i ) of one of the hypotheses (classes) when it is true for a given significance level or probability of false alarm P FA (w i ; w j ), for i = j. In particular, w 1 is decided if the LRT holds:\nwhere \u03b3 is a constant threshold. Although this ratio is equivalent, in terms of ability to classify, to having the class posteriors for optimal classification [25] , class posteriors allows us to introduce a non-parametric approach in this framework by formulating an overall error-rate ratio test from the integrated version of the conditional probability in equation 3 as:\nwhere E j (w k ) = R j p(w i |x; w k )p(x)dx is the error rate under w k hypothesis in region R j for i = j and k = {0, 1}. The precision in that regions can be defined as\nThe hypothesis w 0 is decided if the LRT in equation 6 holds, that is, the one with minimum error rate in regions R 0 and R 1 ."}, {"section_title": "A. SAMPLE REALIZATION UNDER CLASS-HYPOTHESES", "text": "In the CSL approach the sample realizations x = (x 1 , . . . , x d ) of the input pattern under the class-hypotheses w k , denoted by (x; w k ), are obtained by using a SFE scheme [10] . In this case, equation 6 allows us to select the class whose conditional-error rate is minimum when one of the two classhypotheses is true. Classical methods for signal detection and classification, such as LDA or QDA, are based on a LRT similar to the one shown in equation 5, but evaluating hypothesis testing on the raw data, i.e. the input pattern X is assumed to be observed under the null & alternative hypotheses in order to check which state is more likely. The result of the test is affected by several factors such as the presence of noise or redundant features in high dimensional spaces [26] . This is partly compensated by the use of SFE which allows us to obtain p-dimensional features of the d-dimensional input patterns with p d under H 0 and H 1 . Given a validation pattern x, the admissible classes\nextended training sets are built for SFE as:\nwhere Y = [y 1 , . . . , y N ] T , is the training label vector."}, {"section_title": "B. P-LS FOR THE CLASS-HYPOTHESIS-BASED FE", "text": "The LS method provides a vector of parameters w by minimizing a squared error cost function [27] . The LS solution under the CSL approach can be expressed as:\nand the preprocessed extended datasets as\nOn the other hand, PLS [28] is a statistical method which models relationships among sets of observed variables by means of latent variables. In its general form, PLS is a linear algorithm for modeling the relation between X and Y k by decomposing them into the form:\nwhere\nmatrices of loadings and R k , S k are (N + 1) \u00d7 p matrices of residuals (or error matrices). The x k -scores in X k are linear combinations of the input variables and can be considered as good ''summaries'' of them. Finally, the novel datasets are extracted as"}, {"section_title": "C. A NOVEL IMPLEMENTATION USING SVM", "text": "To demonstrate the effectiveness of the proposed methodology, it is implemented using SVM as the baseline classifier because of its strong theoretical foundation and high generalization ability [5] . The non-parametric method used here, in order to implement equation 6, is based on the empirical cumulative density (ECD) function for a trained SVM as defined in [20] . Many works have been reported on transforming output scores to probabilities [29] therefore the probabilities detailed throughout the paper can be estimated by them. The score output by the SVM for each feature indicates the likelihood that the input pattern belongs to a class thus it ranks input samples from the most likely members to the most unlikely members of a class [20] . Given a extended training dataset X e with N samples, consisting of N i samples of class w i , the ECD function for class w j under hypothesis w k is defined in the output-score space of the SVM as:\nFollowing equation 4 the error rate function E i in the region\nas:\nwhere\nThe selection of the limits t 1 , t 2 under the confidence based-classifier design theory [20] allows to define a negative/positive bound below/above which the error rate is smaller than a targeted error and therefore, a decision on the input pattern can be achieved (x \u2282 R 0 /R 1 ). On the contrary, the samples are rejected (x \u2282 R r ) because the decision is too risky.\nIn order to be conservative we need to include all the available samples of the dataset in the computation of error rates, thus these magnitudes are computed by locating the limits t 1 and t 2 on the boundaries of the regions. Thus, we select the decision surface of the SVM (f (x) = 0) and the minimum f min (maximum f max ) output-score value for class w 1 (w 0 ) in the previously defined region R 0 (R 1 ). In other words, R r is assumed to be negligible or the targeted error to be huge. Finally, taking into account the definition of the errorrate and its correspondent ratio test, the decision rule can be formulated in terms of precision in regions R 0 and R 1 as:\nwhere the precision functions are defined as\nAs a conclusion, we take advantage of the misclassified support vectors and rank them according to their output scores from the minimum/maximum value to zero. All the samples with scores included in these regions allows us to compute an approximation for the error rates as shown in equation 13."}, {"section_title": "IV. EXPERIMENTS", "text": "A set of experiments are carried out on synthetic and image databases where the small sample size problem is typically an issue, i.e. brain image databases [30] , [31] . To this purpose, a fair comparison using the same FE and statistical validation schemes for the proposed non-parametric approach and the baseline methods is performed. In both cases the error estimation is obtained by LOO-CV and a linear SVM classifier to avoid over-fitting. The number of extracted components for the FE methods should be small since the proper estimation of any precision or error rate in the output score space of this methodology must fulfill some conditions as detailed in the Appendix, i.e. only a few components features will be analyzed, showing average results and standard deviations."}, {"section_title": "A. SYNTHESIZED EXAMPLE", "text": "Firstly, we evaluate the posterior probability-based decision on a 2D experiment with known distributions. Two hundred samples are randomly drawn from two Gaussian distributions with means \u00b5 0 = (0, 0) and \u00b5 1 = (1, 1) and covariance matrices S 0 = [1.4;\n.41] and\nThe samples together with the LS-decision surfaces under class-hypotheses w 0 and w 1 for a specific validation pattern (red circle) are shown in figure 2 . At the FE stage of the proposed method LS is applied to the input data to obtain the extended datasets described in section III-B. Under the class-hypotheses the extended datasets and the different SV configurations are obtained as shown in figure 3 , where the same validation pattern is considered. A zoom on these figures reveals an increase in the number of support vectors in the wrong subspace, that is, the conditional probability VOLUME 5, 2017 p(w i |x) for the computation of the error rate on this subspace R j , for i = j, is increased. As shown in these figures, the sample (close to the margin) used to describe the operation of the proposed method is relevant [10] in the sense that a substantial change between the extended datasets and their SV configuration is obtained. The SVM-based classification stage on the selected dataset would benefit from the right assumption (the real pattern class) following a good performance of the SVM classifier. On the contrary, the selection of the validation pattern class would not considerably affect the target performance of the current classification system. See for example in figure 4 where all the samples, the non-relevant ones and the improvements of the non-parametric CSL approach on the baseline (without class assumptions) are shown. The samples, drawn in yellow font, are correctly classified independently of the assumption made on the processed pattern. It is worth mentioning that for a correct operation of the algorithm the conditional-error regions must be filled with samples (see Appendix), in other case the posterior probability estimation would be biased and the likelihood ratio would fail. This issue may be controlled by the trade-off between the number of samples N and the feature dimension d.\nBy increasing the number of input patterns up to 500 samples, a smoothed histogram of the SVM output scores, figure 5 , where the minimum margin (R 0 and R 1 ) with less conditional-error rate can be selected. Note that this class selection is not intended for classification purposes but to improve the feature vector extraction prior to classification. The confusion matrix on the CV loop using a linear-SVM for 500 samples is depicted in table 1. Notice again the limitation of the proposed approach when estimating the pdf of the error rate with small sample sizes. A significant sample realization on the SV margin is required to estimate the fraction of samples that are correctly/incorrectly classified using the SVM. This drawback is briefly explained in [20] and detailed in the Appendix. In this sense this limitation could be a challenge when dealing with biomedical datasets (d >> N ). Hopefully, for example, brain image datasets, such as the ADNI dataset [11] , are continuously increasing the sample size and this limitation may be overcome. Additionally, there are several works [9] , [13] that show clear advantages of using a reduced number of discriminative features in this scenario, thus reducing the dimension of the features can relive this problem when the sample size is unavoidable small. This issue is experimentally shown in figure 4 at the bottom, where an increase in sample size reveals further improvements on the baseline."}, {"section_title": "B. SPECT-IMAGE DATABASE", "text": "Baseline SPECT data from 96 participants were collected from the ''Virgen de las Nieves'' hospital in Granada (Spain) [30] . The patients were injected with a gamma emitting 99m Tc-ECD radiopharmeceutical and the SPECT raw data was acquired by a three head gamma camera Picker Prism 3000. A total of 180 projections were taken with a 2 deg angular resolution. The images of the brain volumes were reconstructed from the projection data using the filtered backprojection (FBP) algorithm in combination with a Butterworth filter for noise removal. The SPECT images were spatially normalized, using the SPM software [32] , in order to ensure that the voxels in different images refer to the same anatomical positions in the brain. After the spatial normalization a 95 \u00d7 69 \u00d7 79 voxel representation of each subject was obtained, where each voxel represents a brain volume of 2.18 \u00d7 2.18 \u00d7 3.56 mm 3 . Finally, the intensities of the SPECT images were normalized with a maximum intensity value I max , which is computed for each image by averaging over the 3% highest voxel intensities. The SPECT images were visually classified by experts of the ''Virgen de las Nieves'' hospital using four different labels: normal (NOR) for patients without any symptoms of Alzheimer Disease (AD), and possible AD (AD1), probable AD (AD2) and certain AD (AD3) to distinguish between different levels of the presence of typical characteristics for AD. Overall, the database consists of 41 NOR, 29 AD1, 22 AD2 and 4 AD3 patients. Table 2 shows the demographic details of the database and in figure 6 some examples of the dataset are depicted. "}, {"section_title": "C. RESULTS AND DISCUSSION", "text": "Additionally to the aforementioned preprocessing steps, the SPECT images are converted into feature vectors, prior to classification, by means of two masking procedures. Firstly, all the brain-volume voxels are consider as features in the classification task. Secondly, several standardized brain regions in MNI space [33] , are extracted from subjects and then classified, separately. In the latter case, 20 out of the that is, by filling up regions R 0,1 with sample realizations (see figure 3) . However, in this real case, we cannot afford this problem by increasing N but to reduce the number of features d using the PLS method. To this purpose, only the first PLScomponent is considered (highest variance) transforming a complex task into a one-dimensional classification problem, as shown in the previous examples with Gaussian pdfs and the classical LS.\nThe statistical measures to assess the performance of the CSL approach on the SPECT dataset are summarized in table 3, where a linear SVM classifier in a CV loop is used for classification. This table shows how even using a smallsample size the improvement on the baseline, under the same experimental framework, is substantial. The PLS-based CSL method outperforms in 18 out of 20 BA the baseline although this improvement consist only in 4 positive samples and 14 negative samples. This performance yields an accuracy rate higher than the baseline in one point, as shown in table 3 and figure 10 . As an analysis example, note the configuration of the SVs and the number of misclassified vector in the negative-output subspace (positive SVs) using a control subject from the SPECT database. The one-dimensional feature is relevant in the aforementioned sense thus the wrong assumption increases the number of misclassified vectors in the negative subspace R 0 and thus increasing the error precision in that region (see figure 11) .\nA more detailed analysis of the proposed system outcome reveals that the CSL provides an additional improvement only in the overlap area between the analyzed classes (NOR vs. AD) as expected. In this database this area mainly contains AD1 labeled patients, which corresponds to the typical MCI clinical pattern, and controls. This is shown in figure 8 where the whole brain volume is considered for the binary classification. As shown from this figure the number of hits and misses of the both approaches vs. the former four categories or classes reveals an improvement on two subjects in NOR and AD1 classes. By analyzing the first three principal components with maximum variance the subjects can be drawn in a 3D-projection space in figure 9 where the improvement subjects are located on the boundary (diamond marker).\nAlthough it is reasonable to optimize parameters in the development of CAD systems by minimizing CV error rates, the resulting classification rates are usually biased estimates of the actual risk due to the small sample size problem. This is a common setting in healthcare database studies, where CV-based error estimation is usually selected as validation method [8] . In [10] a full simulation is provided to compare bias and variance in the error estimation of the CSL approach with the ones obtained by baseline approaches. As a conclusion, the difference between empirical and true errors was lower than 5%, and both were statistically similar over this simulation, where the mean estimator was considered following the same strategy as in the experimental part [10] . Although the bias of the CV error estimate is not significant for none of the aforementioned methods on this classification task, we could obtain a close to unbiased estimate of the actual risk by using the results of several resampling and optimization methods [34] , [35] ."}, {"section_title": "V. CONCLUSIONS", "text": "In this paper, the application of the CSL method to a neuroimaging dataset and some connections with previous approaches are presented. The non-parametric CSL approach is evaluated on synthetic/SPECT image datasets [30] . The CSL approach combines FE, hypothesis testing on a discrete set of expected outcomes and a cross-validated classification stage. This methodology provides extended datasets (one per class-hypothesis) by means of FE methods, which are scored probabilistically using the output scores of a properly trained SVM inside a CV loop. Our results demonstrate that, although the method can only be applied to the low-dimensional problem, due to the poor estimate of the conditional-error probability for a low ratio N /d, the resulting system provides a CV error estimate that outperforms the one obtained by baseline methods that do not consider such FE optimization. In future works we will consider the extension of different resampling methods, such as K-fold CV, where the influence of a set of patterns on the classifier configuration is expected to be more evident."}, {"section_title": "APPENDIX", "text": "In this section we demonstrate what are the limitations of the current proposed method and the benefits of using homogeneous linear classifiers such as SVMs [36] in high dimensional problems. To this purpose we make use of the theory presented in [37] which applies the classical combinatorial geometry to develop the separating capacities of decision surfaces.\nDefinition: Given X an arbitrary set of feature vectors in the Euclidean space R d , a dichotomy {X \u2212 , X + } of X is said to be homogeneously linearly separable (HLS) if there exists a linear threshold function f w :\nIn other words, the separating hyperplane passes through the origin and is the (d \u2212 1)-dimensional orthogonal subspace to w.\nThe main question here is to find the relation between the elements of the dichotomy and their labels, that is, given a training set in general position X = {x 1 , . . . , x N } and the family of decision surfaces w correctly separating the training set, will the validation pattern x be categorized by them into just one of the two categories?. If this holds the pattern x is said to be non-ambiguous relative to the family f w . The following theorem establishes the number of groupings that can be formed to separate training data into two classes.\nFunction Counting Theorem: Given N arbitrary samples in general position in R d the number of HLS dichotomies is:\nwhere stands for the binomial coefficients. The demonstration of this interesting theorem can be followed in [37] . From this expression we can compute the probability that any of these dichotomies, assuming they have equal probability, is equal to the one assigned by the label set Y:\nThis probability clearly tends to one with increasing d thus our proposed method, that estimates the ECD function by the evaluation of R 0 and R 1 subspaces, will fail under this condition since the latter regions would be empty of samples. In other words, the classification problem is HLS, i.e no misclassified support vectors can be found in X k . Moreover, based on this theorem we can easily derive the following: Proposition: Let X\u222a{x} be the extended datasets in general position in d-space, then the validation pattern x is ambiguous with respect to the C(N , d \u22121) dichotomies of the training set X relative to the class of all decision surfaces w. Moreover, the probability P a (N , d) that x is ambiguous with respect to a dichotomy of X is:\nProof: Given the training set X, from Theorem 1, there are C(N , d) HLS dichotomies defined by the set of decision surfaces f w . If a dichotomy {X \u2212 , X + } is separable then the extended dataset X 0 = {X \u2212 \u222a {x}, X + } or X 1 = {X \u2212 , X + \u222a {x}} is separable. Moreover, both are separable (ambiguity) by some decision surfaces if and only if the orthogonal (d \u22121) VOLUME 5, 2017 FIGURE 12. Asymptotic probability of HLS classes (above) and ambiguous generalization (bottom).\ndimensional subspace to w contains x (small displacements of these hyperplanes will allow arbitrary classification of the pattern without affecting the old dichotomies). The projection of X in that space is also separable and in general position, therefore, again from theorem 1, the number of dichotomies in that space is C (N , d \u2212 1) . Finally, the probability that x is ambiguous w.r.t the dichotomy of X is the ratio between all of these dichotomies in the (d \u2212 1)-space and the total number of dichotomies. This probability, shown in equation 17, tends to one when the ratio \u03b2 = N /d is close to 0, then in a high dimensional problem the ambiguity of the pattern is assured under both class-hypotheses. Under these conditions, a well-trained linear SVM on the extended feature datasets X k , generates a HLS dichotomy {X \u2212 k , X + k }, independently of the classhypothesis H k , that arbitrarily places the validation pattern in both sides of the hyperplane. The consequence is that, with increasing d, the information extracted from the pattern is useless for feature extraction or classification. CSL is based on the fact that some properties on the extended datasets can reveal a statistical difference on the features extracted under the class-assumptions. Sure enough, SFE on the extended datasets provides a set of feature vectors X k in R d , where d is the number of components. In order to select between both subsets, the non-parametric approach assesses the output score of a well-trained SVM on them by computing the conditional-error probabilities, thus we must assure that the regions R k are full of samples (non HLS classes) and the pattern is not ambiguous under class-hypotheses. Fulfilling these conditions with a low d the performance of the systems will be satisfactory (see figure 12) . "}]