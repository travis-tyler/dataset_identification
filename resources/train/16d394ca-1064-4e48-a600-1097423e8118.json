[{"section_title": "Abstract", "text": "Alzheimer's Disease (AD) is the most common neurodegenerative disorder associated with aging. Understanding how the disease progresses and identifying related pathological biomarkers for the progression is of primary importance in the clinical diagnosis and prognosis of Alzheimer's disease. In this paper, we develop novel multi-task learning techniques to predict the disease progression measured by cognitive scores and select biomarkers predictive of the progression. In multi-task learning, the prediction of cognitive scores at each time point is considered as a task, and multiple prediction tasks at different time points are performed simultaneously to capture the temporal smoothness of the prediction models across different time points. Specifically, we propose a novel convex fused sparse group Lasso (cFSGL) formulation that allows the simultaneous selection of a common set of biomarkers for multiple time points and specific sets of biomarkers for different time points using the sparse group Lasso penalty and in the meantime incorporates the temporal smoothness using the fused Lasso penalty. The proposed formulation is challenging to solve due to the use of several non-smooth penalties. One of the main technical contributions of this paper is to show that the proximal operator associated with the proposed formulation exhibits a certain decomposition property and can be computed efficiently; thus cFSGL can be solved efficiently using the accelerated gradient method. To further improve the model, we propose two non-convex formulations to reduce the shrinkage bias inherent in the convex formulation. We employ the difference of convex (DC) programming technique to solve the non-convex formulations. We have performed extensive experiments using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI). Results demonstrate the effectiveness of the proposed progression models in comparison with existing methods for disease progression. We also perform longitudinal stability selection to identify and analyze the temporal patterns of biomarkers in disease progression."}, {"section_title": "INTRODUCTION", "text": "Alzheimer's disease (AD), accounting for 60-70% of agerelated dementia, is a severe neurodegenerative disorder. AD is characterized by loss of memory and declination of cognitive function due to progressive impairment of neurons and their connections, leading directly to death [21] . In 2011 there are approximately 30 million individuals afflicted with dementia and the number is projected to be over 114 million by 2050 [36] . Currently there is no cure for Alzheimer's and efforts are underway to develop sensitive and consistent biomarkers for AD.\nIn order to better understand the disease, an important area that has recently received increasing attention is to understand how the disease progresses and identify related pathological biomarkers for the progression. Realizing its importance, NIH in 2003 funded the Alzheimer's Disease Neuroimaging Initiative (ADNI). The initiative is facilitating the scientific evaluation of neuroimaging data including magnetic resonance imaging (MRI), positron emission tomography (PET), other biomarkers, and clinical and neuropsychological assessments for predicting the onset and progression of MCI (Mild Cognitive Impairment) and AD. The identification of sensitive and specific markers of very early AD progression will facilitate the diagnosis of early AD and the development, assessment, and monitoring of new treatments. There are two types of progression models that have been commonly used in the literature: the regression model [10, 31] and the survival model [30, 34] . Many existing work consider a small number of input features, and the model building involves an iterative process in which each feature is evaluated individually by adding to the model and testing the performance of predicting the target representing the disease status [18, 35] . The disease status can be measured by a clinical score such as Mini Mental State Examination (MMSE) or Alzheimer's Disease Assessment Scale cognitive subscale (ADAS-Cog) [10, 31] , or the volume of a certain brain region [16] , or clinically defined categories [9, 26] . When high-dimensional data, such as neuroimages (i.e., MRI and/or PET) are used as input features, the methods of sequentially evaluating individual features are suboptimal. In such cases, dimension reduction techniques such as principle component analysis are commonly applied to project the data into a lower-dimensional space [10] . One disadvantage of using dimension reduction is that the models are no longer interpretable. A better alternative is to use feature selection in modeling the disease progression [31] . Most existing work focus on the prediction of target at a single time point (baseline [31] , or one year [10] ); however, a joint analysis of data from multiple time points is expected to improve the performance especially when the number of subjects is small and the number of input features is large.\nTo address the aforementioned challenges, multi-task learning techniques have recently been proposed to model the disease progression [39, 43] . The idea of multi-task learning is to utilize the intrinsic relationships among multiple related tasks in order to improve the generalization performance; it is most effective when the number of samples for each task is small. One of the key issues in multi-task learning is to identify how the tasks are related and build learning models to capture such task relatedness. One way of modeling multi-task relationship is to assume all tasks are related and task models are closed to each other [11] , or tasks are clustered into groups [4, 20, 32, 41] . Alternatively, one can assume that the tasks share a common subspace [2, 7] , or a common set of features [3, 29] . In [39] , the prediction of different types of targets such as MMSE and ADAS-Cog is modeled as a multi-task learning problem and all models are constrained to share a common set of features. In [43] , multitask learning is used to model the longitudinal disease progression. Given the set of baseline features of a patient, the prediction of the patient's disease status at each time point can be considered as a regression task. Multiple prediction tasks at different time points are performed simultaneously to capture the temporal smoothness of the prediction models across different time points. However, similar to [39] , the formulation in [43] constrains the models at all time points to select a common set of features, thus failing to capture the temporal patterns of the biomarkers in disease progression [6, 19] . It is thus desirable to develop formulations that allow the simultaneous selection of a common set of biomarkers for multiple time points and specific sets of biomarkers for different time points.\nIn this paper, we propose novel multi-task learning formulations for predicting the disease progression measured by the clinical scores (ADAS-Cog and MMSE). Specifically, we propose a convex fused sparse group Lasso (cFSGL) formulation that simultaneously selects a common set of biomarkers for all time points and selects a specific set of biomarkers at different time points using the sparse group Lasso penalty [14] , and in the meantime incorporates the temporal smoothness using the fused Lasso penalty [33] . The proposed formulation is, however, challenging to solve due to the use of several non-smooth penalties including the sparse group Lasso and fused Lasso penalties. We show that the proximal operator associated with the optimization problem in cFSGL exhibits a certain decomposition property and can be solved efficiently. Therefore cFSGL can be efficiently solved using the accelerated gradient method [27, 28] .\nThe convex sparsity-inducing penalties are known to introduce shrinkage bias [12] . To further improve the progression model and reduce the shrinkage bias in cFSGL, we propose two non-convex progression formulations. We employ the difference of convex (DC) programming technique to solve the non-convex formulations, which iteratively solves a sequence of convex relaxed optimization problems. We show that at each step the convex relaxed problems are equivalent to reweighted sparse learning problems [5] .\nWe have performed extensive experiments to demonstrate the effectiveness of the proposed models using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI). We have also performed longitudinal stability selection [43] using our proposed formulations to identify and analyze the temporal patterns of biomarkers in disease progression."}, {"section_title": "A CONVEX FORMULATION OF MOD-ELING DISEASE PROGRESSION", "text": "In the longitudinal AD study, cognitive scores of selected patients are repeatedly measured at multiple time points. The prediction of cognitive scores at each time point can be considered as a regression problem, and the prediction of cognitive scores at multiple time points can be treated as a multi-task regression problem. By employing multi-task regression, the temporal information among different tasks can be incorporated into the model to improve the prediction performance.\nConsider a multi-task regression problem of t tasks with n samples of d features. Let {x1, \u00b7 \u00b7 \u00b7 , xn} be the input data at the baseline, and let {y1, \u00b7 \u00b7 \u00b7 , yn} be the targets, where each xi \u2208 R d represents a sample (patient), and yi \u2208 R t is the corresponding targets (clinical scores) at different time points. We collectively denote\nT \u2208 R n\u00d7t as the target matrix, and W = w 1 , \u00b7 \u00b7 \u00b7 , w t \u2208 R d\u00d7t as the weight matrix. To consider the missing values from the target, we denote the loss function as:\nwhere matrix S \u2208 R n\u00d7t indicates missing target values: Si,j = 0 if the target value of sample i is missing at the jth time point, and Si,j = 1 otherwise. The componentwise operator is defined as follows: Z = A B denotes Zi,j = Ai,jBi,j, for all i, j. The multi-task regression solves the following optimization problem: minW L(W ) + \u03a9(W ), where \u03a9(W ) is a regularization term that captures the task relatedness.\nIn the multi-task setting for modeling disease progression, each task is to predict a specific target (e.g., MMSE) for a set of subjects at different time points. It is thus reasonable to assume that the difference of the predictions between immediate time points is small, i.e., the temporal smoothness [43] . It is also well believed in the literature that a small subset of biomarkers are related to the disease progression, and biomarkers involved at different stages may be different [19] . To this end, we propose a novel multi-task learning formulation for modeling disease progression which allows simultaneous joint feature selection for multiple tasks and task-specific feature selection, and in the meantime incorporates the temporal smoothness. Mathematically, the proposed formulation solves the following convex optimiza-tion problem:\nwhere W 1 is the Lasso penalty, the group Lasso penalty\nis the fused Lasso penalty, R is an (t \u2212 1) \u00d7 t sparse matrix in which Ri,i = 1 and Ri,i+1 = \u22121, and \u03bb1, \u03bb2 and \u03bb3 are regularization parameters. The combination of Lasso and group Lasso penalties is also known as the sparse group Lasso penalty, which allows simultaneous joint feature selection for all tasks and selection of a specific set of features for each task. The fused Lasso penalty is employed to incorporate the temporal smoothness. We call the formulation in Eq. (2) \"convex fused sparse group Lasso\" (cFSGL). The cFSGL formulation involves three non-smooth terms, and is thus challenging to solve. We propose to solve the optimization problem by the accelerated gradient method (AGM) [27, 28] . One of the key steps in using AGM is the computation of the proximal operator associated with the composite of non-smooth penalties defined as follows:\nIt is clear that each row of W is decoupled in Eq. (3). Thus for obtaining the ith row wi, we only need to solve the following optimization problem:\nwhere vi is the ith row of V . The proximal operator in Eq. (4) is challenging to compute due to the presence of three non-smooth terms. One of the key technical contributions of this paper is to show that the proximal operator exhibits a certain decomposition property, based on which we can efficiently compute the proximal operator in two stages, as summarized in the following theorem:\nThen the following holds:\nProof: The necessary and sufficient optimality conditions for (4), (5), and (6) can be written as:\nwhere SGN(x) is a set defined in a componentwise manner as:\nand\nIt follows from (10) and (12) "}, {"section_title": "\u03c0FL(v).", "text": "It is easy to observe that, 1) if the i-th entry of \u03c0FL(v) is zero, so is the i-th entry of \u03c0GL(\u03c0FL(v)); 2) if the i-th entry of \u03c0FL(v) is positive (or negative), so is the i-th entry of \u03c0GL (\u03c0FL(v) ). Therefore, we have:\nMeanwhile, 1) if the i-th and the (i + 1)-th entries of \u03c0FL(v) are identical, so are those of \u03c0GL(\u03c0FL(v)); 2) if the i-th entry is larger (or smaller) than the (i + 1)-th entry in \u03c0FL(v), so is in \u03c0GL(\u03c0FL(v)). Therefore, we have:\nIt follows from (9), (10), (13), and (14) that:\nSince (4) has a unique solution, we can get (7) from (8) and (15). 2 Note that the fused Lasso signal approximator [13] in Eq. (5) can be effectively solved using [24] . The complete algorithm for computing the proximal operator associated with cFSGL is given in Algorithm 1. "}, {"section_title": "Algorithm 1 Proximal operator associated with the Convex Fused Sparse Group Lasso (cFSGL)", "text": "Input: V \u2208 R d\u00d7t , R \u2208 R t\u22121\u00d7t , \u03bb1, \u03bb2, \u03bb3 Output: W \u2208 R"}, {"section_title": "NON-CONVEX PROGRESSION MODELS", "text": "In cFSGL, we aim to select task-shared and task-specific features using the sparse group Lasso penalty. However, the decomposition property shown in Theorem 1 implies that a simple composition of the 1-norm penalty and 2,1-norm penalty may be sub-optimal. Besides, the sparsity-inducing penalties are known to lead to biased estimates [12] . To this end, we propose the following non-convex multi-task regression formulation for modeling disease progression:\nwhere the second term is the summation of the squared root of 1-norm of wi (wi is the ith row of W ), and is called the composite (0.5,1) -norm regularization. Note that it is in fact not a valid norm due to its non-convexity. It is known that the 0.5 penalty leads to a sparse solution, thus many of the rows of W will be zero, i.e., the features corresponding to the zero rows will be removed from all tasks. In addition, for the nonzero rows, due to the use of the 1 penalty for the rows, many features within these nonzero rows will be zero, resulting in task-specific features. Thus, the use of (0.5,1) penalty leads to a tight coupling of between-task and within-task feature selection. In addition, the 0.5 penalty is expected to reduce the estimation bias associated with the convex sparsity-inducing penalties. We also consider an alternative non-convex formulation which includes the fused Lasso term of each row within the square root, resulting in a composite (0.5,1) -like penalty:\nA good merit of using non-convex penalties is that they are closer to the optimal 0-'norm' (minimizing which is NPhard) and give better sparsity [15] . In addition, a practical advantage of the non-convex progression models presented in Eqs. (16) and (17) is that there are only 2 regularization parameters to be estimated, compared to 3 parameters in the convex formulation in Eq. (2). However, one disadvantage of the non-convex penalties is that the associated optimization problems are non-convex and global solutions are not guaranteed. A well-known method for solving nonconvex problems is to approximate the non-convex formulation by a convex relaxation via the difference of convex (DC) programming techniques [17] . Next, we show how the nonconvex problems can be solved using DC programming and then relate the relaxed formulations to reweighted convex formulations."}, {"section_title": "DC Programming", "text": "The formulations in Eq. (16) and Eq. (17) can be expressed in the following general form:\nwhere (W ) and h(W ) are convex. Since x \u2212 \u221a x is convex,\nwe decompose the objective function in Eq. (18) into the following form:"}, {"section_title": "Denote the two functions as f (W ) = (W ) + h(W ) and g(h(W )) = h(W )\u2212 h(W ).", "text": "We can express the formulation in Eq. (18) in the form of difference of two functions:\nUsing the convex-concave procedure (CCCP) algorithm [38] , we can linearize g(h(W )) using the 1st-order Taylor expansion at the current point W as:\nwhich is the convex upper bound of the non-convex problem.\nIn every iteration of the CCCP algorithm, we minimize the upper bound:\nand the objective function is guaranteed to decrease. We obtain a local optimal W * of Eq. (18) by iteratively solving Eq. (20) . The CCCP algorithm has been applied successfully to solve many non-convex problems [8, 22, 37] ."}, {"section_title": "Reweighting Interpretation of Non-Convex Fused Sparse Group Lasso", "text": "We first consider the non-convex optimization problem in Eq. (16), whose convex relaxed form corresponding to Eq. (20) is given by:\nwhere \u03bci = 1/ wi (k) 1 + and is a small number included to avoid singularity. It is clear that the convex relaxed problem in each iteration is a fused Lasso problem with a reweighted 1-norm term. If we omit the fused term, the general (1,0.5) -regularized optimization problem is of the following form:\nwhich, under DC programming, involves solving a series of reweighted Lasso [5, 23] problems:\nIt is known that the reweighted Lasso reduces the estimation bias of Lasso, thus leading to a better solution. Similarly, for the non-convex optimization problem in Eq. (17), we iteratively solve the following convex problem:\nwhere \u03bdi = 1/ Rwi\nIn this case, in each iteration, we solve a fused Lasso problem with a reweighted 1-term and a reweighted fused term.\nThe non-convex optimization problems may be sensitive to the starting point. In our algorithm in Eq. (21), for example, if all elements in row i of the model wi are initialized to be close to 0, then in the next iteration \u03bci will be set to a very large number. The large penalty forces the row to stay at 0 in later iterations. Therefore, in our convex relaxed algorithms in Eq. (21) and Eq. (22), we propose to use the solution of a problem similar to fused Lasso as the starting point. For example, the starting point we use in Eq. (21) is:\nThis is equivalent to setting \u03bci/2 = 1. Similarly, in Eq. (22) we set \u03bdi/2 = 1."}, {"section_title": "ANALYZE TEMPORAL PATTERNS OF BIOMARKERS USING LONGITUDINAL STABILITY SELECTION", "text": "We propose to employ longitudinal stability selection to quantify the importance of the features selected by the proposed formulations for disease progression. The idea of longitudinal stability selection is to apply stability selection [25] to multi-task learning models for longitudinal study. The stability score (between 0 and 1) of each feature is indicative of the importance of the specific feature for disease progression. In this paper, we propose to use longitudinal stability selection with cFSGL and nFSGL to analyze the temporal patterns of biomarkers. The temporal pattern of stability scores of the features selected at different time points can potentially reveal how disease progresses temporally and spatially.\nThe longitudinal stability selection algorithm with cFSGL and nFSGL is given as follows. Let F be the index set of features, and let f \u2208 F denote the index of a particular feature. Let \u0394 be the regularization parameter space and let the stability iteration number be denoted as \u03b3. For cFSGL an element \u03b4 \u2208 \u0394 is a triple \u03bb1, \u03bb2, \u03bb3 , and for nFSGL is a tuple of the corresponding parameter pairs. Let B (i) = {X (i) , Y (i) } be a random subsample from input data {X, Y } of size n/2 without replacement. For a given \u03b4 \u2208 \u0394, let W (i) be the optimal solution of cFSGL or nFSGL on B (i) .\nThe set of features selected by the model\u0174 (i) of the task at time point p is denoted by:\nWe repeat this process for \u03b3 times and obtain the selection probability\u03a0 \u03b4 f,p of each feature f at time point p:\nwhere I(.) is the indicator function defined as: I(c) = 1 if c is true and I(c) = 0 otherwise. Repeat the above procedure for all \u03b4 \u2208 \u0394, we obtain the stability score for each feature f at time point p:\nThe stability vector of a feature f at all t time points is given by: S(f ) = [S1(f ) . . . St(f )], which reveals the change of the importance of feature f at different time points. We define the stable features at time point p as:\nand choose \u03b7 = 20 in our experiments. We are interested in the stable features at all time points, i.e., f \u2208\u00db = \u222a t p=1\u00dbp . Note that S(f ) is dependent on the progression model used.\nWe emphasize here that unlike the previous work which gives a list of features common for all time points [43] , our proposed approaches yield a different list of features at different time points. Note that in the above stability selection we use temporal information via fused Lasso. Consequently the distribution of stability scores also has temporal smoothness property: for each feature the stability scores are smooth across different time points (as shown in experimental results in Section 6.3). If simply using Lasso in stability selection, then we obtain independent probability lists at each time point, and therefore such temporal smooth pattern cannot be captured."}, {"section_title": "RELATION TO PREVIOUS WORK", "text": "In our previous work [43] , we proposed to use the temporal group Lasso (TGL) regularization to capture task relatedness, which involves the following optimization problem:\nwhere \u03b81, \u03b82 and \u03b83 are regularization parameters. The TGL formulation in Eq. (24) contains three penalty terms. The first term penalize the 2-norm of the model to prevent overfitting; the second term enforces temporal smoothness using 2-norm, which is equivalent to a Laplacian term, and the last 2,1-norm introduces joint feature selection. We argue that it is more natural to incorporate the within-task feature selection and temporal smoothness using a composite penalty as in our proposed cFSGL formulation in Eq. (2) .\nFor example, the only sparsity-inducing term in TGL formulation in Eq. (24) is the 2,1-norm regularized joint feature selection. Therefore an obvious disadvantage of this formulation is that it restricts all models from different time points to select a common set of features; however, different features may be involved at different time points. In addition, one key advantage of fused Lasso compared with the Laplacian-based smoothing used in [43] is that under the fused Lasso penalty the selected features across different time points are smooth, i.e., nearby time points tend to select similar features, while the Laplacian-based penalty focuses on the smoothing of the prediction models across different time points. Thus, the fused Lasso penalty better captures the temporal smoothness of the selected features, which is closer to the real-world disease progression mechanism.\nIn the TGL formulation, the temporal smoothness is enforced using a smooth Laplacian term, though fused Lasso in cFSGL indeed has better properties such as sparsity continuity. We have used this restrictive model in TGL, in order to avoid the computational difficulties introduced by the composite of non-smooth terms ( 2,1-norm and fused Lasso). We show in this paper that the proximal operator associated with the optimization problem in cFSGL exhibits a certain decomposition property and can be computed efficiently (Theorem 1); thus cFSGL can be solved efficiently using accelerated gradient method. Another contribution of this paper is that we extend our progression model using a composite of non-convex sparsity-inducing terms, and we further propose to employ the DC programming to solve the non-convex formulations."}, {"section_title": "EXPERIMENTS", "text": "In this section we evaluate the proposed progression models on the data sets from the Alzheimer's Disease Neuroimaging Initiative (ADNI)\n1 . The source codes can be found in the Muli-tAsk Learning via StructurAl Regularization (MAL-SAR) package [42] ."}, {"section_title": "Experimental Setup", "text": "The ADNI project is a longitudinal study, where a variety of measurements are collected from selected subjects including Alzheimer's disease patients (AD), mild cognitive impairment patients (MCI) and normal controls (NL), repeatedly over a 6-month or 1-year interval. The measurements include MRI scans (M), PET scans (P), CSF measurements (C), and cognitive scores such as MMSE and ADAS-Cog. We denote all measurements other than the three types of Table 2 . The date when the patient performs the screening in the hospital for the first time is called baseline, and the time point for the follow-up visits is denoted by the duration starting from the baseline. For instance, we use the notation \"M06\" to denote the time point half year after the first visit. Currently ADNI has up to 48 months' follow-up data for some patients. However, many patients drop out from the study for many reasons (e.g. deceased). In our experiments, we predict future MMSE and ADAS-Cog scores using various measurements at the baseline. For each target we build a prediction model using a data set that only contains baseline MRI features (M), and another data set that contains both MRI and META features (M+E). In the current study, CSF and PET are not used due to the small sample size. The MRI features are extracted in the same way as in [43] . There are 5 types of MRI features used: white matter parcellation volume (Vol.WM.), cortical parcellation volume (Vol.C.), surface area (Surf. Area), cortical thickness average (CTA), cortical thickness standard deviation (CTStd). The sample size and dimensionality for each time point and feature combination is given in Table 1 ."}, {"section_title": "biomarkers (M, P, C) as META (E). A detailed list of the META data is given in", "text": ""}, {"section_title": "Prediction Performance", "text": "In the first experiment, we compare the proposed methods including Convex Fused Sparse Group Lasso (cFSGL) and the two Non-Convex Fused Group Lasso: nFSGL1 in Eq. (16) and nFSGL2 in Eq. (17) with ridge regression (Ridge) and Temporal Group Lasso (TGL) on the prediction of MMSE and ADAS-Cog using selected types of feature combinations, namely M and M+E. Note that Lasso is a special case of cFSGL when both \u03bb2 and \u03bb3 are set to 0. For each feature combination, we randomly split the data into training and testing sets using a ratio 9 : 1. The 5-fold cross validation is used to select model parameters. For the regression performance measures, we use Normalized Mean Squared Error (nMSE) as used in the multi-task learning literature [40, 3] and weighted correlation coefficient (R-value) as employed in the medical literature addressing AD progression problems [10, 31, 18] . We report the mean and standard deviation based on 20 iterations of experiments on different splits of data. To investigate the effects of the fused Lasso term, in cFSGL we fix the value of \u03bb2 in Eq.(2) to be 20, 50, 100, and perform cross validation to select \u03bb1 and \u03bb3. The three configurations are labeled as cFSGL1, cFSGL2 and cFSGL3 respectively.\nThe experimental results using 90% training data on MRI and MRI+META are presented in Table 3 and Table 4 . Overall our proposed approaches outperform Ridge and TGL, in terms of both nMSE and correlation coefficient. We have the following observations: 1) The fused Lasso term is effective. We witness significant improvement in cFSGL when changing the parameter value for the fused Lasso term.\n2) The proposed cFSGL and nFSGL formulations witness significant improvement for later time points. This may be due to the data sparseness at later time points (see Table 1 ), as the proposed sparsity-inducing models are expected to achieve better generalization performance in this case.\n3) The non-convex nFSGL formulations are better than cFSGL in many tasks. One practical strength of the non-convex nFSGL formulations is that they have fewer parameters to be estimated (only 2 parameters)."}, {"section_title": "Temporal Patterns of Biomarkers", "text": "One of the strengthens of the proposed formulations is that they facilitate the identification of temporal patterns of biomarkers. In this experiment we study the temporal patterns of biomarkers using longitudinal stability selection with cFSGL and nFSGL. Note that because the sample size at the M48 time point is too small, we perform stability selection for M06, M12, M24, and M36 only.\nThe stability vectors of MRI stable features using cFSGL nFSGL1 and nFSGL2 formulations are given in Figure 1 , Figure 2 and Figure 3 respectively. In the figures, we collectively list the stable features (\u03b7 = 20) at the 4 time points. The total number of features may be less than 80 because one feature may be identified as a stable feature at multiple time points. In Figure 1(a) , we observe that cortical thickness average of left middle temporal, cortical thickness average of left and right Entorhinal, and white matter volume of left Hippocampus are important biomarkers for all time points, which agrees with the previous findings [43] . Cortical volume of left Entorhinal provides significant information in later stages than in the first 6 months. Several biomarkers including white matter volume of left and right Amygdala, and surface area of right Bankssts provide useful information only in later time points. On the contrary, some biomarkers have a large stability score during the first 2 years after baseline screening, such as cortical thickness average of left inferior temporal, left inferior parietal, and cortical thickness standard deviation of left isthmus cingulate, right lingual, left inferior parietal, and cortical volume of right precentral, right isthmus cingulate, and left middle temporal cortex.\nThe stability vector of stable MRI features for MMSE are given in Figure 1(b) . We obtain very different patterns from ADAS-Cog. We find that most biomarkers provide significant information for the first 2 years and very few of them contain information about the progression in later stages. The lacking of predictable MRI biomarkers in later stages is a potential factor that contributes to the lower predictive performance of MMSE than that of ADAS-Cog in our study and other related studies [39] . These results suggest that ADAS-Cog may be a better cognitive measurement for lon- gitudinal study. The different temporal patterns of biomarkers for these two scores also suggest that restricting the two models for predicting these two scores to share a common set of features as in [39] may lead to sub-optimal performance. We also perform stability selection of nFSGL1 and nFSGL2 using only MRI biomarkers. The results are given in Figure 2 and Figure 3 . We observe that most biomarkers identified in cFSGL are also included in the top feature lists in nFSGL. This demonstrates the consistency between these two approaches. We also observe that the patterns of temporal selection stability differ from that of cFSGL in that fewer features have high probability. In nFSGL2 there is only one feature, namely cortical thickness average of right Entorhinal cortex, that has high probability at all time points, compared to 5 in cFSGL longitudinal stability selection. In nFSGL2 we observe that white matter volume of left Hippocampus also maintains a high stability vector. The higher temporal sparsity observed in nFSGL may be due to the non-convex (0.5,1) -norm penalty."}, {"section_title": "CONCLUSION", "text": "In this paper, we propose a convex fused sparse group Lasso (cFSGL) formulation for modeling disease progression. cFSGL allows the simultaneous selection of a common set of biomarkers for multiple time points and specific sets of biomarkers for different time points using the sparse group Lasso penalty and at the same time incorporates the temporal smoothness using the fused Lasso penalty. We show that the proximal operator associated with the optimization problem exhibits a certain decomposition property and thus can be solved effectively. To further improve the model, we propose two non-convex formulations, which are expected to reduce the shrinkage bias in the convex formulation. We employ the difference of convex (DC) programming technique to solve the non-convex formulations. The effectiveness of the proposed progression models is evaluated by extensive experimental studies on data sets from the Alzheimer's Disease Neuroimaging Initiative (ADNI) data sets. Results show that the proposed progression models are more effective than an existing multi-task learning formulation for disease progression. We also perform longitudinal stability selection to identify and analyze the temporal patterns of biomarkers for MMSE and ADAS-Cog respectively. The presented analysis can potentially provide novel insights into the AD progression.\nOur proposed formulations for disease progression assume that the training data is complete, i.e., there are no missing values in the feature matrix X. We plan to extend our formulations to deal with missing data."}]