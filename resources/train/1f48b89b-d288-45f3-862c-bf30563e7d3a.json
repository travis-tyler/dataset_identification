[{"section_title": "Introduction", "text": "This study examines the characteristics of an institution that correlate with the share of its BAs that ultimately receive a PhD, or its PhD production rate. We examine four fields separately, the humanities, life sciences, physical sciences, and social sciences. Despite long reported poor job market prospects for new humanities PhDs, graduate programs in the humanities have continued, sometimes with smaller cohort sizes, to churn out PhD students in the humanities. On the other hand, STEM fields, the life sciences and physical sciences, strongly contribute to the economy, foster innovation, and lead PhDs in these fields to receive high salaries. We study a number of factors of institutions, such as expenditure measures, the number of total students, and other student body characteristics that might be associated with predicting the PhD production rate. In addition to producing PhDs, departments also educate undergraduate students, and one focus of this paper is to see whether the scope and quality of an institution's PhD programs are related to the likelihood that its undergraduate students go on to receive PhDs. Put another way, we are interested in the dual role that departments at universities play in producing new PhDs and in generating undergraduate student interest in going on for doctoral degrees, typically at other institutions. We focus our attention on doctoral institutions. 1 Using data from the National Science Foundation's Survey of Earned Doctorates (SED) linked to the IPEDS Completions Survey, we calculate that for 1980-2001 BA recipients, the fraction of BAs in a given field that receive PhDs in the same field is higher at baccalaureate institutions 2 than at doctoral institutions. 3 However, because of the differences in the sizes of these two types of institutions, doctoral institutions produce far more total PhDs than baccalaureate colleges. Doctoral institutions produce 5.8 times as many PhDs than do baccalaureate colleges for the physical sciences; they also produce 2.7 times as many for the humanities, 4.7 times as many for the life sciences, and 3.7 times as many for the social sciences. 4 With the exception of life sciences, the ratios are even higher for Master's colleges. 5 Therefore, knowledge of the characteristics of the Carnegie doctoral institutions that lead to more undergraduates going on to receive PhDs is of importance in understanding the determinants of PhD production. Lemke (2009) is among the latest studies to examine the determinants of undergraduate institutions in \"generating\" (producing) PhD recipients. 6 This study computed a PhD production rate by dividing publicly-available data on the number of PhDs in all fields earned by alumni of selective liberal arts colleges by the number of BAs earned by the students at these institutions. Because of this study's use of public-use data, while it was known where a PhD recipient received her BA, it was not known when he or she received it. Lemke (2009) assumed a 6-year time-todegree (TTD) window from BA to PhD, which often is too short of a window. We use restrictedaccess, individual-level SED data to observe the actual matches of where and when each individual PhD graduated from his or her BA institution. For each field, we compute one observation per institution-BA year on the percentage of BAs who received PhDs. This is in contrast to the one of PhDs in a given field is divided by the total number of BA recipients in the same field. Some of the PhD recipients received BAs in other fields. We include only observations for which there is a match with the Completions Survey data. The institution must have produced at least 5 PhDs over the SED data, without regard to TTD. Other sample restrictions are as in Section A. 4 This calculation makes use of the sample restrictions described later, including that the institution must have produced at least 5 PhDs, except that here, all TTDs are considered. The BA years are 1980-2005 We define Master's institutions as 2015 Carnegie Categories 18: Master's Colleges and Universities: Larger Programs; 19: Master's Colleges and Universities: Medium Programs; and 20: Master's Colleges and Universities: Small Programs. 6 Many other studies have addressed the determinants of PhD students' times to degree and completion rates. See, for example, Knapp and Goodrich (1951), Tidball (1986), Schapiro et al. (1991), Ehrenberg et al. (2007), Groen et al. (2008), Ehrenberg et al. (2009), Bowen and Bok (2016), and Groen (2016). observation per institution in the Lemke (2009) study, where both the numerator and denominator of the dependent variable are averages over different 10-year periods. When using individual-level data to compute the PhD production rate, there is an inherent truncation problem. Not all PhDs from a given BA cohort are observed because some individuals complete their PhD after the conclusion of the data. The truncation problem increases in severity for later cohorts. We use truncation correction methodology, where early cohorts' patterns of timeto-degree are used to predict the number of PhDs that later cohorts will eventually produce. After correcting for truncation, we examine which features of the institution predict the share of graduates that ultimately receive a PhD. In our main specification using the full sample (with Carnegie category 15-17 institutions), we find the number of an institution's departments that are in the top 10th percentile of the National Research Council (NRC) 1995 doctoral rankings in each field is one of the four variables that is correlated with the PhD production rate across each field. Put simply, the number of high-quality doctoral programs is an important predictor of the rate at which undergraduates go on for PhD study. The three other variables that are statistically significant across all fields are the incoming student 75th percentile test scores-which is positively related to the PhD production rate-and the total number of students as well as the percentage of total BAs that are in the field, which are both negatively related. Other variables, such as institutional expenditures per student, are significant for some, but not all, fields. In a robustness check, we restrict the sample to Carnegie category 15 institutions (Highest Research Activity), which have the highest level of research activity. We also conduct an analysis along the lines of that done in Lemke (2009). For both, we find generally similar results to the baseline specification, although the NRC top 10th percentile variable and the percentage of total BAs that are in the field are each significant in only two fields in the Carnegie category 15 regressions."}, {"section_title": "PhD Production", "text": "Following Lemke (2009), we assume that characteristics of the undergraduate institution generate, or produce, PhDs. We first list the explanatory variables considered in the analysis. We break down these characteristics into four categories: institutional, expenditure, student body, and graduate programs. Appendix A provides details on the data."}, {"section_title": "Institutional Characteristics", "text": "First, we include the student-faculty ratio. The smaller the class size, the greater the potential for students to interact with the professor, which may lead to the students being more interested in pursuing graduate studies. Next, we include a public institution indicator. If the quality or amount of interaction between faculty and students differs between public and private institutions, this may lead to differing propensities to pursue a PhD. The mission of the institution may be generally different between the two types and may also influence the propensity."}, {"section_title": "Expenditures", "text": "We include instructional expenditures per student. Greater instructional expenditures-of which faculty salaries are a large part-may attract faculty who are more research productive. Students may then want to emulate the career paths of these faculty by pursuing PhDs themselves. We also include research expenditures per full-time faculty. A higher level of research spending likely leads to higher quality research. This may influence students, making a career in research via a PhD more attractive."}, {"section_title": "Student Body Characteristics", "text": "As a proxy for the institution's academic ability, we consider first-year incoming student test scores, specifically a combination of 75th percentile SAT and ACT scores, weighted by the percentage of scores from each. We emphasize that the scores are for first-year, not seniors. We additionally include several measures of the size and composition of the student body: the total number of students (including graduate and professional students), the percentage of the students that are undergraduates, the percentage of undergraduates that are female, and the percentage of undergraduates that come from underrepresented minority groups. Because the importance on campus of, say, the humanities could play a role in generating interest in humanities PhD study, in the humanities regressions, we include the percentage of BAs that were received in the humanities in our humanities equations. We similarly include the percent of degrees received in the other three fields in their relevant equations."}, {"section_title": "Graduate Programs", "text": "Finally, we are interested if having highly-ranked doctoral programs is associated with having a higher PhD production rate. Having more highly-ranked doctoral programs is presumably correlated with having more faculty at the cutting edge of their field, and, to the extent that these faculty interact with undergraduate students, this may influence students' decisions to obtain a PhD in, say the, the humanities. Thus, we include a set of variables indicating the number of humanities' doctoral programs an institution has in several percentile bins: 0-10, 11-25, 25-50, and 51-100. 7 We calculate these variables as follows: For a given subject (e.g., comparative literature) in the field, we compute percentiles for all programs that appear in the NRC data for that subject, including programs not in our paper's sample. After repeating this for all subjects in the field, we the construct an institution's percentile bin counts by summing up the number of programs an institution has-across all subjects in the field-in each percentile bin. Not all institutions have doctoral programs for a given subject; it also happens that even though an institution does has a doctoral program, that the NRC also does not consider it in its rankings. We include a \"missing\" 7 The smaller the percentile, the higher ranked the department. NRC variable for institutions for which we do not observe any program in any NRC subject ranking (within the field). 8 We use percentiles as opposed to the number in the top 10, etc., because there is a large amount of heterogeneity in the number of departments ranked in each subject. 9 For example, many more departments are ranked in English Language and Literature than in French Language and Literature, and a department being ranked, e.g., in the 11-25 range (the original ranking, not percentile), may mean something very different for the former subject than the latter. 10 We illustrate the National Research Council (NRC) variables using the counts of several universities; all examples are specific to the humanities. Each university is presented with four numbers, corresponding to the numbers of departments ranked in the following percentile ranges: 0-10, [11][12][13][14][15][16][17][18][19][20][21][22][23][24][25], and 51-100. The subjects that make up the counts for each school are found in the corresponding footnote. UC-Berkeley: [0-10]: 7, [11][12][13][14][15][16][17][18][19][20][21][22][23][24][25]: 3, [26-50]: 0, [51-100]: 0. UCLA: 3, 0, 6, 1. UC-Riverside: 0, 0, 1, 3. UC-Irvine: 0, 3, 4, 0. Stanford: 3, 4, 3, 1. Wisconsin: 1, 3, 3, 3. Illinois: 0, 3, 2, 5. Georgia: 0, 0, 2, 4. 11 It is very rare for a university to have more than one ranked department in a given subject. These examples highlight the fact that universities with more programs (which are often bigger and/or richer) will have higher counts. 12 In a robustness check, we use only the university's highest ranked department to construct mutually-exclusive ranking categories. One shortcoming of this paper is that our NRC measures are constant over all years for a given university because we use only the 1995 NRC data. Results should be interpreted accordingly."}, {"section_title": "Estimating Equation", "text": "In order to obtain estimates, we employ weighted least squares regression with the PhD production rate, defined in Section 4.3, as the dependent variable. We estimate the regression separately by PhD field. Specifically: where is the PhD production rate defined If an observation is missing or has 0 BAs-the denominator of the PhD production ratethen it is not included in the regression. We weight the institution-field-BA year observation by the number of field-specific BAs. 13 Robust standard errors are clustered at the institution-level. In addition to the main regression above, in a robustness check, we also present results where we restrict the sample to Carnegie category 15 institutions (Research Activity), which have a higher PhD production rate as well as a smaller amount of missing data as compared to the other two categories. We next perform an analysis in the spirit of Lemke 2009, although there are some differences. As opposed to constructing the PhD production rate as described in Section 4, we instead divide the total number of PhDs produced over the 1994-2003 time period by the total number of BAs produced over the 1989-1998 time period. Truncation correction is explicitly not used in this analysis because TTD is assumed and PhDs are not matched to their actual BA-year. We use the same set of institutions as appear in the main analysis, and there is now only one observation per institution per field. If an observation produced PhDs but had no BAs, it is excluded from the regression. For the explanatory variables, we take the average value over the same 1989-1998 period among non-missing observations. If all observations for a variable are missing, we assign that variable a value of 0 and a corresponding missing variable a value of 1. We use robust standard errors, and weight by total BAs over the 1989-1998 period. 14 Finally, we perform several additional robustness checks. First, there is a concern that our 13 It can happen that an institution is predicted to have a very high (even greater than 1) PhD share. In such cases, it is likely that the institution awards a very small number of BAs in this field, but produced a disproportionately high number of PhDs. This can be because a relatively large share of the BAs in the field went on to earn a PhD or BAs in other fields went on to earn a PhD. Because there are very few BAs, these observations will receive a small weight, and not be very influential in the estimates. 14 Results differ somewhat if we do not weight by BAs. An advantage of using weighting is that there are some observations that have very few BAs in a given field, but produce many PhDs, including those with BAs from a different field. This causes the PhD production rate to be very large, even over 100. Weighting greatly reduces the influence that such observations have in the regression. main sample may rely too heavily on the truncation correction, which might lead to an upward bias in the number of degrees produced. This would be the case if, for example, TTD has decreased over time. 15 In order to lessen the reliance on the truncation correction, we use a shorter maximum TTD (defined below). Second, our main NRC variable-the number of departments in a given percentile range that a university has-gives preference to universities that have more departments, perhaps because they are larger or richer. These variables are also difficult to interpret. In this robustness check, we therefore define new NRC variables: mutually-exclusive indicator variables for the highest-ranked department within the field for two different measures: a) percentiles (0-10, 11-25, 26-50, 51-100, and no departments in the rankings), and b) the raw count ranking (1-10, 11-25, 26-50, 51 and up, and no departments in the rankings). Third, our main sample classifies PhDs to field of study based on the PhD field of study, but does not take into account the BA field of study. In this robustness check, we restrict the sample only to PhDs who also received their BA in the same field of study as the PhD. Finally, our fields as currently classified are quite broad, and use information on subjects within the same field that may not be particularly relevant to students who majored in other subjects. Here, we consider several well-defined subjects separately: economics, history, mathematics, 16 physics, and chemistry. These also have the advantage of having NRC fields that have rankings that are easily put into the mutually-exclusive indicator variables, as in the second robustness check."}, {"section_title": "Constructing the PhD Production Rate and Correcting for Truncation 15", "text": "We do see evidence that TTD has fallen over time. See, for example, https://www.nsf.gov/statistics/2018/nsf18304/report/what-influences-the-path-to-the-doctorate/time-to-degree.cfm. 16 We group mathematics and statistics together. We define the PhD production rate as the fraction of BAs for a given institution-BA year that obtain a PhD. We return to the details of its construction in Section 4.3 after first introducing the concept of maximum TTD in Section 4.1 and, in Section 4.2, how we employ truncation correction to address not being able to observe PhDs granted after the SED sample ends. First, though, we clarify our matching process. We classify an individual to a given field (humanities, life science, physical science, or social science) based solely on their PhD field. Their BA field of study is not used (except in a robustness check in which we restrict the sample to be those with the same BA field as PhD field). The individual is associated with their BA institution (the institution from which they graduated with a BA) and BA year (the year they graduated with their BA). Then, for each field and for each BA institution-BA year combination, the number of individuals who ultimately earn a PhD and who also graduate from the given BA institution in the given BA year are tabulated. This produces the numerator of the PhD production rate, as described in the sections below. The denominator of the PhD production rate is taken from the IPEDS Completions Survey, and is the total number of individuals who graduated with a BA from the BA institution-BA year; this does not depend on the BA major of PhDs in the SED."}, {"section_title": "Maximum TTD", "text": "We measure the PhD production rate for students who earn a PhD within a given time, which we denote as the maximum TTD. Earning a PhD often takes many years. To illustrate, Figure 1 plots the distribution of TTD by field for BA years 1980 and 1981, the two earliest years in the sample. Because the final year of the SED sample is 2016, the longest observable TTD for BA-year 1980 is 36 years and for BA-year 1981 is 35 years, and all PhDs earned after this are truncated. This figure shows that nearly all PhDs are earned by 30 years for the life and physical sciences, with a larger tail for the humanities and social sciences. The peaks of these latter two fields are also longer, suggesting a longer average TTD. [ Figure 1 near here.] This truncation problem is the most pronounced for BA-year 2005, the final BA-year in the sample. For this year, the longest observable TTDs is only 11 years. We use the truncationcorrection technique presented below in Section 4.2 to estimate how many PhDs will be produced by the maximum TTD. The higher the maximum TTD, the higher the number of PhDs are observed. But there is a tradeoff: The higher the maximum TTD, the fewer the number of BA-years there are for which we observe all PhDs for the maximum TTD, and the more truncation correction is necessary. In this paper, we use field-specific maximum TTDs based on Table 1. This table displays separately by field the cumulative percentage of PhDs that were received by each TTD for BA-years 1980 and 1981. For each field, we select the smallest TTD for which at least 90% of PhDs had been earned. 17 Thus maximum TTDs of 23, 20, 16, and 22 years are used for humanities, life sciences, physical sciences, and social sciences, respectively. Choosing the maximum TTD this way puts all of the fields on an equal playing field, taking into account the different TTD distributions. This means, however, that the extent to which we must use truncation correction and thus artificially generate the outcome variable differs by field, being the most in the humanities and the least in the physical sciences. For the humanities, we observe the maximum TTD for BA-years 1980-1993 (because we observe PhDs until 2016), and use truncation correction for remaining BA-years, 1994BA-years, -2005 [ "}, {"section_title": "Truncation Correction", "text": "We employ the simple truncation correction technique used in Prenovitz et al. (2016) to estimate how many PhDs would be produced if we had observed PhDs up to the maximum TTD. Using the number of PhDs produced by 2016 as a baseline, we predict how many more would be produced had we observed the entire maximum TTD. The training sample that goes into the prediction uses the BA-years for which we observe the maximum TTD; as an example, for the humanities this is 1980-1993. Using these years only, we compute the percentage of PhDs were received by each TTD, up to the maximum TTD. These cumulative percentages are then applied to the remaining years (1994-2005 for humanities) at the institution-BA year level to estimate the number of PhDs. An example will illustrate. Suppose that in the humanities, a given institution-field with BA-year 2003 produced 6 PhDs by 2016. Because the maximum observed TTD is 13, the cumulative percentage for a TTD of 13 will be used. Suppose that this is calculated to be x. The predicted number of PhDs that will be produced by 2026 (23 years after BA-year) is computed to be \ufffd 1 \ufffd 6. Here, we formalize the above idea. The prediction is based on the years for which the full TTDs exist (1980-1993 for the humanities). Restricting the sample to these years only, we compute the cumulative percentage of PhDs that occur after TTDs of 4,...,MaxTTD, where MaxTTD is the maximum TTD (4,...,23 years for the humanities). Specifically, we first compute the cumulative number of PhDs produced by each TTD for each institution, separately by field. 18 Define the number of PhDs produced by institution i, field f, BA-year y, at TTD t, to be \u210e , , , where t \u2208 {4, MaxTTD}. 19 The cumulative number of PhDs for institution i by TTD t is then:"}, {"section_title": "=4", "text": "These are aggregated across institutions to the TTD-level to produce \u210e : Sum across TTDs to produce the total number of PhDs at the maximum TTD: Now, obtain the cumulative percentage of PhDs granted for each TTD: 20 = \u210e \u210e . We next apply the cumulative percentages to each institution-BA year for years for which we do not observe the max TTD (1994-2005 for humanities) to obtain the estimated total number of PhDs for each institution-BA year. This is done by multiplying the number of PhDs produced by the maximum observed TTD by the inverse of the corresponding cumulative percentage: where MaxObsTTD is the maximum TTD that it was possible for the BA-year to have; this is equivalent to 2016 \u2212 y. For example, MaxObsTTD for BA-year 2003 is 13. Note that the CumPct value takes on the value corresponding to MaxObsTTD. If an observation produced 0 PhDs by 2016, we assign it a predicted value of 0. This method uses information from past years to predict PhD rates for future years. It assumes that the production patterns of PhDs does not vary by year, or that the pattern from the earlier estimating sample will occur in the later prediction sample. For each field, we use the entire sample of institutions in the training sample, regardless of which Carnegie category the institution is in."}, {"section_title": "PhD Production Rate", "text": "After implementing the truncation correction, we compute the PhD production rate for each institution BA year, and do so separately by field. For BA-years for which we observe the max TTD (1980-1993 for humanities), this is: and for BA-years for which we do not observe the max TTD (1994-2005 for humanities): where \u210e , , and \u210e , , are defined the same as previously, and , , is the total number of BA degrees for institution i in field f in year y. 21 Because we multiply by 100, the PhD production rate is a percentage. [ Table 2 near here.] Note that the numerator includes all individuals who earned a PhD in the field, whether or not they also earned a BA in the same field. To illustrate, Table 2 shows the BA fields of study for PhDs in a given field. Over 93% of Physical Science PhDs also received a BA in Physical Sciences, while only 64% of Social Science PhDs received a BA in the Social Sciences. The denominator of the PhD production rate includes only the major that is listed as the first major. If the individual dual-majored in a field other than their first-listed major, then this will not be included in the other field. This is because the Completions Survey data on BAs includes only this major, with the exception of the final years of our BA data. In order to be consistent across years, we do not include dual majors in the analysis. 22\nThis is broken up by PhD field. There were fewer PhDs granted in the humanities and social sciences than in the life and physical sciences. These latter two fields both experienced a large amount of growth beginning in the mid-1990s; the former fields also experienced growth, but to a lesser degree. [ Activity), meanwhile, produced many more PhDs, and steadily increased this number beginning in the late-1990s. The plot in Figure 3 Panel A displays the PhD production rate, in contrast to the total number of PhDs. Here, at the aggregate level, this is calculated by taking the overall number of 22 Dual majors are not included in the analysis because the IPEDS Completions Survey only has dual majors for the final years of our BA data. In order to be consistent across years, we do not include them. The SED includes information on only one BA. Only this BA is used in the robustness check where we restrict to BAs in the same field as the PhD. PhDs produced divided by the overall number of BAs. 23 The PhD production rates in the life and physical sciences largely track each other and are very similar in magnitude for much of the time period. These rates are also generally at least two times as large as the rates for the humanities and social sciences, which do not experience the same growth at the end of the sample. [ Figure 3 near here.] Finally, Figure 3 Panel B shows that the trends in the PhD production rate among the various Carnegie categories are similar, but the levels differ substantially. The rate is highest for category 15 (Highest Research Activity), while lower rates for category 16 (Higher Research Activity), and again for category 17 (Moderate Research Activity).  23 This is as opposed to taking the average of individually-computed PhD production rates. 24 Observations that do not contribute to regression results because e.g., they did not have any humanities BAs, are still included in the summary statistics. 25 Summary statistics for the PhD Production Rate variables in Table 3 are computed using only the institutions of their respective field's sample. All other variables are computed using the institutions that are in Life Sciences sample. We use the Life Science sample because it is the sample with the most institutions-recall that that the set of institutions included in the samples of the different fields varies somewhat due to the sample requirement that institutions produced at least five PhDs over the time frame. Table 4 shows summary statistics for all variables computed separately by the field's sample; but only for all Carnegie Classifications together (analogous to Columns 1-3 of Table 3; Columns 4-6 of Table 4 are equivalent to Columns 1-3 of [ Table 4 near here.]"}, {"section_title": "Descriptive Statistics", "text": "We first describe several plots relating to the PhD production rate, and then proceed to present summary statistics for the independent variables considered in the analysis. Figure 2 plots the total number of PhDs produced for each BA-year from 1980 to 2005."}, {"section_title": "Independent Variables", "text": "For brevity, we do not present the summary statistics split by Carnegie category for the three other fields, but in Table 4 we do show the Carnegie categories grouped together (this is analogous to the first three columns of Table 3). Because the sample institutions are quite similar across fields, most of the means are very similar across fields. The exception (besides the PhD production rate) is the NRC variables, which vary much more across fields. For example, the values for physical science are roughly twice those of social science."}, {"section_title": "Results", "text": "Baseline Regression Table 5 presents the baseline weighted least squares results, where each column is from a separate regression and corresponds to one of the PhD fields: humanities, life sciences, physical sciences, and social sciences. The mean value of the dependent variable, the PhD production rate, is displayed directly below the regression coefficients. Recall that this variable is a percentage, not a share. In line with Figure 3 Panel A, this value is much larger for the life and physical sciences. The explanatory power for these models is fairly high-around 0.5-for all but the social sciences, where it is lower. [ Table 5 near here.] There are four variables that are consistently statistically significant across the fields: the 75th percentile test score, the number of total students at the institution, the percentage of total BAs that are in the given field, and the number of doctoral programs ranked in the top 10th percentile. The direction of each is also consistent across fields. The higher the test scores, the higher the PhD production rate. Students with higher college entrance exams are more likely to excel in school and have lower costs of continuing to acquire education. That the total students variable has a negative coefficient means that smaller institutions produce PhDs at a greater rate, everything else equal: having 1,000 more students is associated with a drop in the PhD production rate of between .04 for social sciences and .11 for physical sciences. The percentage of the total BAs that are in a given field is negatively related to the PhD production rate. Interestingly, this negative relationship goes against the hypothesis that a field having a larger presence is positively related to producing PhDs. One possible explanation is that those in smaller programs are less likely to get lost in the shuffle and are more likely to gain more individual attention from mentors such as faculty members. Having more highly-ranked doctoral programs, especially in the top 10th percentile, is strongly related to producing PhDs. For the humanities, an increase of one program in the top 10th percentile is associated with a higher PhD production rate of 0.44, which is 17% of the (weighted) mean. The coefficients and percentages for the other fields are as follows: life sciences (0.65; 13%); physical sciences (0.54; 12%); and social sciences (0.29; 13%). Top-ranked departments tend to have stellar faculty, which may influence students in several ways, including attracting them to the institution in the first place. Several more variables are statistically significant for at least two of the four fields: instructional expenditures per student, the percentage of undergraduates who are female, and the institution being in Carnegie category 17 (Moderate Research Activity) as opposed to category 15 (Highest Research Activity). Instructional expenditure per student, measured in $1,000s, is positively related for the humanities and physical sciences, but the magnitude is rather small. It may be the case that having higher quality faculty inspires students. Research expenditures per full time faculty is not related to the PhD production rate. For life and social sciences, the more men there are, the more likely students are to get a PhD. One factor that may be involved is the lower rate in general that women pursued PhDs, especially at the beginning of our sample timeframe. The percentage of total students who are undergraduates is never significant. The percentage of undergraduates who are a minority is positively related to the PhD production rate, but only for social sciences, and the coefficient is small. Compared to Carnegie category 15 institutions, those in category 17 have a lower PhD production rate for both the physical and social sciences. This is after controlling for all other variables and is thus not a raw difference as in Figure 3 Panel B. Neither the student-faculty ratio nor being a public (versus private) institution relate to the PhD production rate. In tables not shown, results are quantitatively unchanged if we use a maximum TTD based on the first year in which at least 80% or 85% of individuals receive their PhD."}, {"section_title": "Robustness Checks", "text": "Restricting to Carnegie 15 Institutions [ Table 6 near here.] In Table 6 everything is the same as above except that we now restrict the sample to Carnegie category 15 institutions (Highest Research Activity). The PhD production rates and explanatory power of these regressions are both higher. The 75th percentile test score and total students variables are again statistically significant across all fields, with coefficients larger in magnitude compared to the baseline. The coefficients across fields for the number of doctoral programs ranked in the top 10th percentile are roughly of the same magnitude as those found in Table 5, although only humanities and physical sciences continue to be statistically significant. Similarly, the coefficients for the percentage of total BAs that are in the given field are of similar magnitude as in Table 5, but only life sciences and social sciences remain statistically significant. With some exceptions, the same general findings of significance and sign hold for the other variables even though the standard errors have increases as the sample size has decreased. Table 7 shows the results from performing the analysis along the lines of Lemke (2009)."}, {"section_title": "Lemke-Style Regression", "text": "Compared to Table 5, the sample size is much smaller because each institution includes only 1 observation in this table, while each institution contributes 26 observations in Table 5; 26 this smaller sample size could influence statistical significance. Compared to the baseline regression, the findings are again largely the same. The test score and total student variables exhibit very similar patterns. The doctoral 0-10 percentile rankings variables are statistically significant for the same fields as when we restrict the sample to Carnegie category 15 institutions. Compared to the baseline regression, most of the same coefficients are significant statistically and have similar magnitudes. [ Table 7 near here.]"}, {"section_title": "Additional Robustness Checks", "text": "First, for each field, we define the maximum TTD to be the smallest TTD for which at least 70% of PhDs had been earned. This is as opposed to 90%, which is used in the main results. While we observe fewer PhDs, the number of years that rely on truncation correction is fewer. The maximum TTDs for humanities, life sciences, physical sciences, and social sciences are 16, 12, 10, and 15 years; there is no truncation correction for physical sciences, and only one year of truncation correction for life sciences. Online Appendix Table A.1 shows that while the PhD production mean is smaller, results are largely the same as the baseline, Table 5. Next, we address the concern that the NRC variables as originally defined favor universities that have more PhD programs, we redefine these variables using mutually exclusive indicator variables based only on the ranking of the university's highest-ranked program within the field as opposed to the number of programs within each category as before. First, we use percentile rankings, and create the following categories: 0-10, 11-25, 26-50, 51-100, and not ranked. Compared to the number of programs within each category, these categories are more easily interpreted: the coefficient for each group is the increase in the PhD production rate as compared to the omitted category, not ranked. Second, we use the raw count rankings, meaning that they are not converted to percentiles. The categories are as follows: 1-10, 11-25, 26-50, 51 and up, and no departments in the rankings. 27 Appendix Table 1 shows results for the percentile indicators. Focusing on the NRC variables, across fields, all 0-10 variables are significantly different than the reference category of not being ranked. For humanities, the coefficients decrease in magnitude across the percentile groups. This pattern holds for life sciences and physical sciences, although the 51-100 group is not different from the reference group. The only category that is significant for social sciences is 0-10. Online Appendix Table A.2, which is for the raw count indicators, follows roughly the same pattern. In other results in the Online Appendix, 28 when we restrict to be Carnegie Category 15 institutions only, results for both percentile and raw count indicators are similar for humanities and life sciences, but no categories are significant for physical sciences and social sciences. In Appendix Table 2 we include only those whose BA is in the same field of study as their PhD. Noting that coefficients are expected to be of a smaller magnitude due to the smaller PhD production rate mean, results are fairy similar to Table 5. Appendix Table 3 shows our final robustness check, which is to consider several subjects individually: economics, history, mathematics, physics, and chemistry. Here, we show the version using NRC percentile indicators. The PhD production rate mean varies greatly across subjects, ranging from 1.6 in economics to 12.3 in chemistry to 21.5 in physics. To a large degree, results are substantively similar to the main results. Across all subjects, student test scores are positive and significant. Both total students and the percentage of total BAs in the subject are negative and significant across all subjects but physics. Percentage female is negative and significant for all but history. The coefficients on the NRC variables are mostly monotonically decreasing, with most of the 0-10, 11-25, and 26-50 categories being statistically significant. The coefficients on the 0-10 category are quite large for mathematics, chemistry, and physics; relative to the PhD production mean, the coefficient for mathematics is particularly large. This may mean that these programs are influential to the students; it could also mean that students who are likely to pursue PhDs are attracted to the types of universities ranked highly in their PhD programs. The Online Appendix shows a number of additional robustness checks, which are different versions and combinations of the above. Results are, for the most part, substantively similar."}, {"section_title": "Conclusion", "text": "By using the restricted access Survey of Earned Doctorate individual-level data, we are able to determine for each field each PhD recipient's time-to-degree and, given the heterogeneity in timeto-degree, to compute better estimates of the share of bachelor's graduates at an institution in a year that subsequently received PhDs in the field than did the early method used by Lemke (2009). Our method required us to address the truncation problem that occurs when using the individual level SED data, that we will not observe PhDs earned after the data end. We conduct a weighted least squares analysis to examine which factors are associated with the PhD production rate. We point out that because we are not exploiting exogenous variation, our results should not be interpreted causally, as there are likely variables that we did not control for. With that said, our major empirical finding is the strong positive association between the number of highly-ranked PhD programs in a field at an institution and the number of its undergraduate students that go on for PhDs in that field. Strong doctoral programs in a field thus seem to contribute to the supply of PhDs both through the number of PhDs they directly generate and through their impact on the number of their undergraduate students that go on for PhDs. We also find that higher entering test scores for undergraduate students and a lower percentage of BAs that are received in the field are both associated with more undergraduates at the institution going on for PhDs."}, {"section_title": "Declaration of Interest Statement", "text": "The authors declare no conflicts of interest. For this reason, the life and physical sciences have a somewhat larger sample than the humanities and social sciences. We use the IPEDS Completions Survey to obtain the number of BA degrees granted by each undergraduate institution each year, by field. See Online Appendix A.2 for classification details. We consider only first reported BA major, which is problematic if the student received two (or more) majors in different fields. The number of BA degrees is combined with the SED data on PhDs produced to compute the humanities PhD production rate, described more fully in Section 4.3. We also calculate the percentage of total BAs earned in each field from this dataset. The Completions Survey data includes a FICE code (from the older HEGIS survey) for all years and a UnitID code (from the IPEDS survey) for 1987 and after. There are a small number of instances where a FICE code maps to more than one UnitID; this may happen, for example, when there are satellite campuses. If the first year that we observe an institution having more than one UnitID is 1987, we aggregate all UnitIDs together for this institution for all years. Because 1987 is the first year that we observe UnitID, it is likely that the multiple campuses were included in the FICE in 1986 and before. This results in an inflated number of BAs for some institutions. If the first year we observe a FICE mapping to multiple UnitIDs is after 1987, we exclude the UnitIDs associated with institution names that do not appear to correspond to the main campus. In general, we will observe too many BAs for observations where the satellite UnitIDs were not broken out into their own UnitIDs. 31 We obtain additional explanatory variables, which are matched at the undergraduate institution-BA year level, from a number of sources. The public institution indicator comes from the SED. We use the IPEDS Fall Enrollment dataset to obtain the total number of students, including graduate and professional students. Specifically, we sum full-time students, having a weight of 1, with part-time students, having a weight of 0.4, regardless of institution type or level of student. 32 This dataset is also used to compute the share of students who are undergraduates, the share of undergraduates who are female, and the share of undergraduates who are from underrepresented minority groups. 33 Each of these is measured as percentage (out of 100). These variables also use the same weights. We divide total students by full-time faculty counts from the IPEDS Fall Staff Survey to compute the student-faculty ratio. 34 Data from the Delta Cost Project (Lenihan, 2012) allow us to compute instructional expenditures per student and research expenditures per full-time faculty. The former is obtained by dividing instructional expenditures by full-time students; the latter comes from dividing research expenditures by full-time faculty. 35 We construct the 75th percentile score variable from the Annual Survey of Colleges Standard Research Compilation. We combine the incoming freshmen 75th percentile ACT scores and 75th percentile SAT scores together; we do not use just one of the tests because institutions vary widely in the propensity of their students to take these tests. 36 This dataset includes the 32 The weight of 0.4 for part-time students is very similar to the weight used for undergraduate in the IPEDS \"Calculation of FTE Students\" (IPEDS, 2018). The weight listed for Public 4-year institutions is .403543 and for private 4-year institutions is .392857. The weights are somewhat further from 0.4 for part-time graduate students: the weights at public 4-year and private 4-year institutions are .361702 and .382059. 33 Due to problems with subcategories not adding up correctly in early years, when computing certain of these variables, we did not use certain subcategories such as FT or PT UG Unclassified. When computing race, the sum of the race categories was often less than expected; we assume that the race categories do not include the non residence variable. 34 Full-time faculty counts come from the sum of the \"Faculty, full-time men\" and \"Faculty, full-time women\" variables. 35 Instructional expenditures come from the instruction01 variable-\"Expenditures for instruction-current year total.\" Research expenditures come from the research01 variable-\"Expenditures for research-current year total\". 36 The score data include two identifies, ID and FICE. However, there is not always a 1:1 mapping between the two. After cleaning the test score data, in cases where the FICE code maps to multiple IDs, we select one ID to use based first on the one that seems to match the institution name the best, second based on the one that has non-missing test scores, and finally based on the one with the highest test scores. If ID maps to multiple FICE, we use the FICE that is in the main dataset for matching. Finally, there are cases where there is no match between datasets via FICE, but the institution name is associated with a different FICE code in the score data. We manually make these matches. Finally, we manually match a number of cases by institution name if the FICE codes do not match between datasets. We dropped the data from 1983 as it is unreliable. In one case, the campus the ID referred to apparently changed over time; we used all years. In addition, there were no reported percentage taking ACT or SAT for 1984-1987, so we percentage of students with reported SAT scores and reported ACT scores. We weight the ACT score by the percentage of the total that reported ACT scores, and do similarly for SAT scores. For example, if 70% reported a SAT score and 35% reported an ACT score (a student can take both tests), then the SAT will get twice the weight as the ACT. We do this after converting the SAT to be on the ACT scale. 37 We note that after conversion and among observations with both scores, the mean ACT is 1.5 points lower than the mean SAT; the standard deviation for both tests is approximately 3. We obtain doctoral program rankings from the \"scholarly quality of program faculty\" ranking from the 1995 National Research Council (NRC) doctoral program rankings (Goldberger et al., 1995). The newer, 2000s rankings do not provide point estimates of program rankings and are thus not used (Ostriker et al., 2011). We also do not use the 1980s rankings, which contain a smaller set of institutions (Jones et al., 1982). These rankings and the 2000s rankings, with important exceptions, generally have a high correlation with the 1995 rankings. 38 Because we only backfilled the 1984 values with the earliest observation before 1993. We then interpolated the percentage taking ACT and SAT. 37 The composite SAT score, the sum of an institution's verbal and math 75th percentile scores, was converted to its ACT equivalent using the conversion found at https://web.archive.org/web/20160130223549/http://act.org/ solutions/college-career-readiness/compareact-sat/ -accessed April 27, 2016. We rounded up to the ACT score if the SAT score was outside one of the listed ranges. For example, if the range was 1290-1320 for a 29 and 1330-1350 for a 30, we converted a score of 1325 to 30. We made note of the re-centering of the SAT scores in 1995 and have converted all scores prior to 1995 to their current day equivalent (separately for math and verbal) using the SAT I Individual Score Equivalents conversion table from the CollegeBoard (see http://research.collegeboard.org/programs/sat/data/equivalence/sat-individual-accessed April 27, 2016). The chart presented single conversion numbers. We rounded to the higher number. For example, a verbal score of 690 is recentered to 750 and 700 is re-centered to 760. We re-centered 695 to 760. For cases where there was a test score for SAT, but not ACT, we used the SAT score entirely and vice versa. As there was a great deal of missing data on earlier years of percentage taking each test, we backfill years missing this data with the earliest reported percentages. We also fill in some of the missing score data using linear interpolation. 38 The 1980s ranking include fewer institutions than the 1995 rankings, but for departments that appeared in both rankings (and after re-ranking them to reflect only the departments in both rankings and using only the highest ranked department within a subject), the correlations (considering all fields at once) of the number of departments at an institution in each percentile rankings interval are: 0 to 10 (.94), 11 to 25 (.84), 26 to 50 (.78), and 51-100 (.90). For the percentile indicator bins (using only the highest-ranked subject in the field) are: 0 to 10 (0.82), 11 to 25 (0.49), 26 to 50 (0.40), 51 to 100 (0.75). The correlations of the 2000s and 1995 rankings follow a similar pattern, but are somewhat lower. For the number of departments ranked in each percentile, the rankings are: 0 to 10 (.90), 11 to 25 (.77), 26 to 50 (.72), and 51-100 (.86). For the percentile indicator bins, they are: 0 to 10 (.76), 11 to 25 (.43), 26 to 50 (.38), and 51-100 (.72). use the 1995 data, the ranking variables are constant within institution-field. We first classify the different subjects into the four broad fields; classification details appear in Online Appendix A.3. For a given subject, such as comparative literature (in the humanities), we construct percentile rankings over all departments, including departments at institutions not in our sample. We then count the number of departments a given institution has across all subjects in the field that are in each of the following percentile ranges: 0-10, 11-25, 26-50, and 51-100. 39 We face the challenge of missing data. For many of the control variables, we use a linear extrapolation to fill in data for years that are bookended by two other years with non-missing data. When variables do not contain bookended data due to the survey starting after 1980, we backfill using the first possible year of data (for example, we fill in 1980-1983 with the 1984 value). 40 After this process, if an observation is still missing, we code it as 0 and create a missing data dummy that takes on value 1 for missing and 0 otherwise. Universities not included in the 1995 NRC are coded as having zero programs in each of our quality intervals and we do not include a missing dummy variable for such institutions. 39 Using the set of institutions in the life sciences, correlations within percentile ranges and across fields range from 0.33 to 0.75, with progressively higher correlations for the 0-10 range than lower ranges in general. These calculations include 0's. 40 More specifically, we fill in full-time faculty of years 1980-1986 with 1987; test scores of 1980-1983 with 1984; student-faculty ratios of 1980-1986 with 1987; and instructional and research expenditures of 1980-1986 with 1987. We note that this is not ideal as it does not take into account trends, only takes care of cases that have a non-missing value for the first year of data, and results in a lack of variation over these several years. We compute student-faculty ratio, instructional expenditures per student, and research expenditures per faculty after backfilling."}]