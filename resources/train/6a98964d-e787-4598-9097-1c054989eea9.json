[{"section_title": "", "text": "The Neoclassical theory of production establishes that a competitive firm's optimization problem is characterized by a dual relationship between the value function (profit, cost, or revenue function) and the underlying production function (e.g., Mas-Colell, Winston, and Green 1995) . In particular, the functional form of the production function implies a specific functional form of the profit, cost, or revenue function. Alternatively, for a given functional form used to approximate the firm's value function, there exists an underlying production function wherein the value function parameters appear in a specific way.\nThis dual relationship has been widely used in empirical work as a tool to estimate production parameters without explicitly specifying the parametric form of the production function. Shumway (1995) and Fox and Kivanda (1994) ii. Deriving a set of input demand and output supply equations by applying Shephard's lemma or Hotelling's lemma.\niii. Using econometric methods to jointly estimate the parameters of the system described in (ii). In some instances, value function parameters are estimated together with those of the input demand and output supply system. iv. Using estimated parameters from (iii) to draw conclusions about, for example, substitution elasticities, price elasticities, and/or returns to scale.\nConclusions from duality applications may be influenced by the choice of specific functional forms. As a result, there is a vast literature analyzing and testing theoretical properties such as monotonicity and curvature, with Gagn\u00e9 and Ouellete (1998) , Terrell (1996) , and Diewert and Wales (1987) as prominent examples. Also, a large number of studies focus on investigating the most preferable (flexible) functional forms for empirical purposes (Guilkey, Lovell and Sickles 1983; Dixon, Garcia and Anderson 1987; Thompson and Langworthy 1989) . These studies used simulated datasets that assumed the basic tenets underlying duality theory, including perfect competition, profit maximizing behavior, and certainty. Therefore, they only consider empirical deviations from duality theory assumptions stemming from the choice of functional form; in other words, the datasets arising from the data generating process (DGP) are free from problems commonly encountered in data available to practitioners, 1 preventing these studies from evaluating the effects of the mentioned problems on the ability of the approach to recover production parameters.\nSimilarly, early attempts to analyze empirical properties of duality theory include Burgess (1975) , Appelbaum (1978) , and Lusk et al. (2002) . With the exception of Lusk et al. (2002) , they fail to identify the source of the discrepancy between conclusions from the primal and dual approaches. There are two reasons. They use real-world data with unknown production parameters, and furthermore, they use non-dual functional forms.\nIn general, the primal and dual approaches cannot provide perfectly matching parameters if a self-dual function is not used, or if there is noise. The presence of random noise weakens the dual-1 Examples of such problems include optimization under uncertainty; prediction errors in prices and quantities of variable netputs; omitted variable netputs; output and input data aggregation; measurement errors in the observed variables; and endogenous output and input prices. Other potential sources of noise are the incorrect specification of variable inputs as quasi-fixed, and vice versa.\nprimal relationship, and it is not clear whether the imprecision in the dual model caused by the noise is amplified or reduced in the primal model. The present study explores this issue, and provides evidence suggesting that the noise amplifies the imprecision in the primal model. Given that all of the real world-data sets which have been used for duality applications depict considerable amounts of noise, the scant attention paid to the impact of noise on the empirical performance of duality theory is nothing but surprising.\nAs a first and critical step to analyze the implications of noisy data for dual-primal relationships in practice, the contribution of the present study consists of showing how to generate a pseudodataset by Monte Carlo simulations calibrated to replicate key features of U.S. agriculture; more precisely, a panel of price and quantity variables based on a set of profit-maximizing firms with heterogeneous technology, which decide the quantity of variable netputs, facing variable netput prices, and conditioned on a set of quasi-fixed netputs.\nA set of initial or simulated parameters is calibrated such that variables' behavior exhibit the main features of observed and widely used agricultural datasets. In particular, the one constructed and maintained by Eldon Ball for U.S. input/output price and quantities (USDA-ERS), the USDA Agricultural Resource Management Survey database (USDA-ARMS), the U.S. Agricultural\nCensus database (USDA-NASS), and the Chicago Mercantile Exchange (CME) future prices database. We chose the first dataset because it is publicly available and it has been used for applications of duality theory in several widely cited papers (Ball 1985; Ball 1988; Baffes and Vasavada 1989; Shumway and Lim 1993; Chambers and Pope 1994) . The remaining three datasets provide useful information for calibrating cross-sectional and time-series parameters, as well as noise directly observed (e.g., price variability and length of time series) and unobserved (e.g., firm heterogeneity). We adopt the criteria of calibrating parameter values to favor recovery of initial production parameters, especially for those that are unobservable.\nTo illustrate the usefulness of the simulated dataset in the context of the duality theory approach, we aggregate the pseudo-data set over heterogeneous firms, construct time series of prices and quantities, and estimate the set of netput elasticities with respect to prices in the dual model. 2 By comparing estimated elasticities with the underlying and known (primal) parameters of the pseudo-data, we show the performance of duality theory in empirical work when the only source of deviation from duality theory assumptions is the aggregation over heterogeneous firms.\nAnother legitimate exercise constitutes the evaluation of the ability of the duality theorem to recover the dual parameters based on the parameter estimation performed on the primal problem,\ni.e., the reverse direction: dual-primal. This is an exercise as interesting and relevant as the one pursued here. However, due to space limitations, the present study only addresses the dual-primal direction. Our choice is based on the fact that the estimation performed on the dual parameters has been the most preferred approach in empirical applications, as our literature review shows.\nCertainly, analyzing the performance of the primal-dual direction constitutes an important topic for future research.\nThese pseudo-data are intended to serve as the basis or first step to conduct a thorough analysis of the performance of the dual approach in empirical work. Importantly, they can also be used in other applications, especially when knowledge of the underlying parameters is useful for the objectives of the study. Datasets used by practitioners in empirical applications are noisier relative to the pseudo-dataset generated here; however, these sources of observable and unobservable noise (listed in footnote #1) are straightforward to incorporate to the pseudo-dataset, and can be calibrated using the aforementioned agricultural datasets. As these issues have not been addressed in depth in previous studies, future work in this direction constitutes relevant contributions to the literature.\nThe layout of the paper is as follows. After providing the theoretical framework, we proceed to describe the creation of the simulated the pseudo-dataset, including an explanation of the DGP used for that purpose. Then, model parameters are estimated using these data, and finally a comparison is made between the simulated and the estimated parameters in the results section.\nConcluding remarks are presented in the last section."}, {"section_title": "Model of a Single Firm", "text": "Consider a producer who chooses the level of netputs 3 to maximize profits. The producer's problem can be described as follows:\nwhere is a choice vector of variable netput quantities, is a vector of variable netput prices normalized by 0 or the price of the numeraire commodity 0 . The augmented vector\nis referred to as the production plan of the production possibilities set S which is a subset of 3 We use the standard definition of netput, where a positive value represents a net output and a negative value represents a net input.\n1+ + , with equal to the number of quasi-fixed netputs (denoted as the vector ) that constrain the production possibilities set.\n4 Jorgenson and Lau (1974) showed existence of a one-to-one correspondence between the set S (with properties described in footnote 3) and a production function defined as:\nWe follow the convention that {\u2205} = \u2212\u221e, where {\u2205} is defined as the empty set, such that the value of the production function is positive infinity if a production plan is not feasible. The set of quasi-fixed netputs that constrains the set S also restricts the production function . The maximization problem can be rewritten as:\nThe solution to problem (3) is a set of netput demand equations * ( , ) and a restricted profit function ( , ) which are dependent on the vector of normalized netput prices and the vector of quasi-fixed netputs. Lau (1976) derived the relationships between the Hessian of the production function ( , ) and the Hessian of the restricted profit function ( , ) under the assumption of convexity and twice continuous differentiability of both functions. Omitting the arguments of each function to simplify notation, the identities are as follows: 4 The properties of the set S include: i) the origin belongs to S; ii) S is closed; iii) S is convex; iv) S is monotonic with respect to 0 ; and v) non-producibility with respect to at least one variable input, which implies at least one commodity is freely disposable and can only be a net input in the production process (a primary factor of production). 5 The properties of the production function are: i) the domain is a convex set of + that contains the origin; ii) the value of at the origin, say (0), is non-positive; iii) is bounded; iv) is closed; and v) is convex in { , }. Convexity (instead of concavity) is required because the convention used in Lau (1974) to define the production function contains a negative sign, that is: 0 = \u2212 ( , ). \nBy defining, in a similar fashion, the production function Hessian sub-matrices as , the identities can be rewritten in the following more compact form:\nThe Hessian relationships allow us to \"transform\" the estimated parameters of the restricted profit function into parameters of the underlying production function, and then compare these transformed parameters with the initial parameters of the production function. More precisely, as the first derivatives of the restricted profit function ( ( , )) with respect to netput prices and quasi-fixed netputs produce the system of input demands and output supplies (Hotelling's Lemma), whose parameters we estimate econometrically, this system's first-derivatives are all that is required to obtain the Hessian of the profit function (which end-up containing as its entries the marginal effects with respect to netput prices and quasi-fixed inputs). We then transform marginal effects into elasticities and compare against their \"initial\" counterparts using the Hessian identities.\nUltimately, the Hessian identities allow us to assess how precisely demand and supply elasticities are estimated.\nTo make this problem operational, we assume a quadratic flexible form for the production function \ufffd , ; \ufffd:\nwhere 1 and 2 are ( \u00d7 1) and ( \u00d7 1) vectors of , coefficients, 11 is a symmetric and nonsingular ( \u00d7 ) matrix, 12 and 22 are ( \u00d7 ) and ( \u00d7 ) matrices of firm , and term is a mean-zero, firm-and time-specific production shock. Submatrices 11 , 12 and 22\nform a symmetric and positive semi-definite \ufffd( + ) \u00d7 ( + )\ufffd matrix of , coefficients. 6 We collectively denote all , and , coefficients by .\nThe quadratic functional form is selected for three reasons. First, it is self-dual-the functional form of the constrained or unconstrained profit function consistent with this production function is also quadratic. This favors recovery of the starting production parameters because the estimation is free from errors arising from functional form specification. Second, the Hessian matrices of both the production and profit functions are only functions of parameters; this proves to be useful because the comparison of the profit and production function Hessians does not depend on the set of model variables at which Hessians are evaluated. Third, the normalized quadratic profit function is extensively used in empirical analysis (Schuring, Huffman and Fan 2011; Arnade and Kelch 2007; Lusk et al. 2002; Lim and Shumway 1993; Huffman and Evenson 1989; Thompson and Langworthy 1989) .\n7"}, {"section_title": "Simulation of Panel Data", "text": "The DGP considers variability of prices and quantities over time within three regions composed of heterogeneous firms. Heterogeneity across regions is assumed to be higher than heterogeneity of firms within each region. The simulated DGP consists of a panel of = 10,000 farms, in = 3 regions, over = 50 years ( \u00d7 \u00d7 = 1.5 million) for each element of the vector\n, where and index firms and time periods (years) respectively, 8 conditional on the initial (*) value of the production parameter set * .\nThe parameter vector * does not depend on time, which implies that technology remains unchanged from period one through . This assumption favors the recovery of starting production parameters because the estimation is free from misspecification that may arise from the evolution of technology over time. This is equivalent to postulating a specific form of netput technological change and proceeding to estimation by exactly specifying its form as if the econometrician knew it with certainty. A different model specification of the mentioned technical change would only add noise to the estimation process. The study of productivity changes over time, their measurement, and their effects on the recovery of production parameters is a relevant research topic which is beyond the scope of this paper and is left for future research. Figure 1 shows the data simulation process. We start by creating the variables conditioning the firm's decisions problem in (3). First, we generate the set of starting production function parameters * and the quasi-fixed netputs * . Second, conditioning on these values, we draw normalized variable netput prices * * . Data generation of * , * , and * * are explained in subsections A through C. Third, we solve a profit maximization problem to obtain the variable netput quantities * * (subsection D). This study focuses on time-series estimation and therefore we aggregate variables across heterogeneous firms before proceeding to estimation of the profit function parameters that are then transformed to production function parameters by Hessian identities (subsection E). The result is a set of estimated production parameters denoted as \ufffd ."}, {"section_title": "A. Simulation of initial production function parameters: *", "text": "The value of * characterizes the firm's technology and is unobserved, making its simulation more challenging. From (6), * consists of the submatrices 1 , 2 , and (formed in turn by 11 , 12 and 22 ). As we mentioned above, firm heterogeneity exists both within and across regions, such that technology is more similar between firms in the same region than across regions. Hence,\nwe select values of the elements of for a \"generic\" firm such that the symmetric \ufffd( + ) \u00d7 ( + )\ufffd matrix is positive-semidefinite. To induce variation across regions we obtain \"regional\" sets as deviations from . Then, firm heterogeneity within a region comes from generating parameters in the firm-specific set as deviations from their corresponding regional . To assure the matrix and its inverse are positive-semidefinite, we draw the entries of the upper triangular matrix , the Cholesky decomposition of matrix \ufffd \ufffd \u22121 , such that the latter is formed as the matrix product \u2032 (Hamilton 1994, p. 147 ).\nThe size, dispersion, and skewness of the elements in determine the size, dispersion, and skewness of the netput quantity variables, * , according to the first-order conditions (FOCs) of the firm's optimization problem. Therefore, these elements must be calibrated so as to yield a realistic distribution of quantities produced and used. We rely on the 2002 U.S. Agricultural\nCensus (USDA-NASS), the USDA Agricultural Resource Management Survey databases (USDA-ARMS), and weather data from PRISM at Oregon State University to accomplish this objective (see Appendix for further details).\nWe calibrate the skewness of the firm-specific deviations from the \"regional\" by fitting a standard Beta distribution to the county-level data of the Census variable \"Total sales, Value of sales, number of farms\" which serves as a proxy for firm size. 9 The shape parameters are estimated by maximum likelihood, yielding a positive skewed distribution. This is consistent with the higher proportion of small firms observed in each region.\nThe size of the elements in is tackled by inducing positive rank correlation among the Beta random shocks, such that a firm producing high levels of output is more likely to use greater amounts of inputs.\nFinally, to calibrate the unobserved dispersion of from , we assume that observed yield dispersion in a region is a function of unobserved technology heterogeneity and observed random weather shocks. If all firms used the same technology, the observed yield variability would come only from weather shocks. At the other extreme, where all firms differ but no weather shocks occur, all yield dispersion comes from heterogeneity across firms. Since it is most likely that reality 9 The Beta distribution is chosen because it can mimic the different levels of skewness observed in the distribution of these variables at the firm level. Skewness can be manipulated by appropriately choosing its two shape parameters. We generate a set of firm-specific normalized exogenous prices for each region (i.e., prices normalized by the price of the numeraire good). Exogeneity is with respect to the aggregated netput quantity produced, but not with respect to quasi-fixed netput quantities * and starting technology parameters * . While we acknowledge the existence of price endogeneity, we generate them exogenously in order to have a dataset with minimal sources of noise.\nWe begin by simulating \"national\" netput prices to match the properties (mean, standard deviation, and serial autocorrelation) of those found in a time series of normalized futures output prices (soybeans, corn and livestock) from the CME and of normalized input prices (hired labor, energy, chemicals, materials, and capital) from Eldon Ball's (USDA-ERS) dataset. We assume firms base their production decisions on expected output prices and current input prices. The numeraire good is wheat. 13 The choice of the numeraire good, the outputs and the inputs is based on the importance of these goods in the region, and follows previous work with the same datasets by Schuring, Huffman, and Fan (2011) .\nWe model normalized netput prices as lognormally distributed and behaving according to AR(1) processes:\nwhere \" \" indexes netputs and is an error term distributed N\ufffd0, 2 \ufffd. Parameters are estimated by OLS regressions using Eldon Ball's dataset. Table 1 shows results for each of the regressions.\nDropping the \" \" subscript to ease notation, taking unconditional expectations in (10)\nThe variance of the error term in (7) can be calibrated from the observed price variation in Eldon Ball's datasets:\n2 which implies that\n. In this case, we calibrate price variation from a combination of data observed variance and regression results, and not exclusively from the latter. To draw exogenous log-normal netput prices, we fit (7) with the estimated parameters, set From the regional prices, we generate firm-specific random prices per region as deviations from the regional average:\n, where is a symmetric mean one shock distributed as\nShocks , , and are independent. Parameters of the distribution are calibrated using prices from the USDA-ARMS dataset, such that they yield a coefficient of variation of 0.08. This coefficient of variation is twice as large as the one observed in the USDA-ARMS dataset, which should favor the recovery of the production parameters using the dual approach.\nThe simulated netput prices are correlated with quantities at the aggregate level, but independent at the firm level. While actual prices received and paid may arguably be correlated with firm size, we assume independence so as to favor parameter identification. Also, observed prices in USDA-ARMS show the majority of firm-level prices are concentrated in four or fewer 14 The same procedure and shocks are used for * * . 15 The values of are 0.90, 1.00, and 1.10 for regions 1 through 3 respectively. different clusters in each region; however, we generate a \"continuum\" of firm-specific prices to favor identification."}, {"section_title": "D. Profit Maximization Problem", "text": "The panel dataset is formed by variable netput quantities and prices, and quasi-fixed netputs: \ufffd * * , * * , * \ufffd. We solve the problem in (3) with exogenous prices received or paid * * . These results are used in our exercise of applying duality theory to recover production technology using time-series data whose only source of noise is aggregation across heterogeneous firms. This constitutes the minimum possible noise when interested in applying duality theory with time series.\nThe potential of the generated dataset is not restricted to this application; future research may use it to address, for example, the analysis of duality theory with cross-sectional data, or the performance of duality theory when data available (both time-series or cross-sectional) embed sources of noise that depart from duality theory assumptions.\nUnder the normalized quadratic production function \ufffd * * , * ; \ufffd in (8), the FOCs are: * * \u2212 1 \u2212 11 * * \u2212 12 * = 0\nThis system is jointly solved for the vector of optimal variable netput quantities * * as a function of the vector of variable netput prices * * , the vector of quasi-fixed netput quantities * , and the production parameters * . The solution is: * * \ufffd * * , * ; * \ufffd = 11 \u22121 ( * * \u2212 1 12 * )\nThis produces a panel dataset of production plans for the ( \u00d7 ) firms over T time periods that can be used to recover production parameters using time-series or cross-section. We denote this dataset as follows:"}, {"section_title": "Data for Estimation", "text": "In agreement with this study's objective of using the dual approach with time-series data, before estimation we proceed to aggregate the sub-vector \ufffd * * , * * , * \ufffd across the = 10,000\nheterogeneous firms for each of the = 50 periods of time, as if data came from a \"single firm.\"\nThis aggregation is performed on the data described in (10)."}, {"section_title": "16", "text": "For netput quantities, we aggregate by adding across firms since they are homogeneous commodities. The th netput price at period ( ) is a quantity-weighted average of the firmspecific netput prices. * * = \u2211 * * * = \u2211 * * * = ( * * ) \u22121 \u2211 * * * *\nThe literature on properties for consistent aggregation is vast, including Gorman (1968 ), Richmond (1976 , Stoker (1984) , and applications in agricultural production economics by Chambers (1988) , Chambers and Pope (1991, 1994) , Davis (1997) , and LaFrance and Pope (2008). However, a linear aggregation is sufficient to achieve the results intended here. 17 The time-series dataset used in estimation is denoted as follows:\nThe dataset in (12) includes all = 8 netput quantities and prices, and = 1 quasi-fixed netput.\nVariable netput prices are exogenous from quantities, but have serial autocorrelation. This aggregation results in a dataset of 50 observations for each variable per region. To avoid the addition of another source of noise coming from heterogeneous technology across regions, we select region 1 to conduct the estimation, and compare results with the initial production parameters of that same region."}, {"section_title": "Estimation", "text": "We approximate the restricted profit function ( , ) , which solves problem vector of random variables is jointly normally distributed with mean equal to an ( \u00d7 1) vector of zeros and an ( \u00d7 ) covariance matrix . This covariance matrix induces contemporaneous correlation between the equations. Also, the DGP of netput prices-both exogenous and endogenous-was constructed as an AR(1) process, implying serial autocorrelation in the independent variables that needs to be accounted for in the estimation.\nWe derive the set of input demands and output supplies by taking first derivatives of (13) with respect to netput prices (Hotelling's lemma), yielding the system to be estimated:\nWe conduct estimation by iterated SUR, which converges to maximum likelihood, and is the most common method employed in empirical studies based on duality theory. We impose symmetry cross-equation restrictions ( = , \u2260 ) in matrix 11 . We do not estimate the parameters of the profit function because the parameters needed to evaluate the production parameters of interest are present in the demands and supplies.\nIt has to be noted that the core of the empirical applications of Duality theory consists of performing an econometric estimation along the lines described in the preceding two paragraphs, plus a thorough interpretation of the results. In contrast, what is unique about this study is that we not only employ these widely accepted estimation procedures, but also generate the pseudo-data and compare the estimated parameters with their \"initial\" counterparts.\nWe treat mean-independence violations in estimation by noting that an inspection of the autocorrelation and partial autocorrelation functions of the time series suggests first differentiation of the data for estimation. 18 This is a consequence of the DGP of price data as AR (1) processes.\nThe estimated values of matrix 11 and vector 12 are the focus of our attention; they are, respectively, the first derivatives (marginal effects) of netput quantities with respect to prices and quasi-fixed netputs, and therefore they are the base to construct the estimated profit function 18 This implies that the elements of the vector 1 are not identified. However, knowledge of their values is not required because they do not enter the formulae of output supply and input demand elasticities with respect to prices, only elements of 11 do (e.g., the elasticity of the i th netput with respect to the j th price is computed as This implies that estimation with a dataset constructed as the aggregation across heterogeneous firms (as if it belonged to a representative firm) is able to recover elasticities that are not only within the relevant range of the distribution but also fairly close to the median and the mean."}, {"section_title": "20", "text": "A second conclusion arises by noting that the point estimates are closer to the median of the distribution than to the mean. In other words, the representative firm is better described by the median of the distribution than the mean. The root mean squared error (RMSE) helps illustrate this conclusion. The RMSE is the average difference between each entry of the estimated elasticity matrix versus its corresponding simulated elasticity, expressed in elasticity units. We show two alternative values to describe the simulated elasticity: the median of the firm-specific elasticity distribution and its mean. When compared to the median of the distribution, the RMSE is:\nwhere = 10,000 is the number of draws from the limiting distribution of the SUR parameter estimates and the subscript indicates the th draw of the th parameter, with , = 1,2, \u2026 ,8. For comparison with the mean we substitute \ufffd by \ufffd . The RMSE averages over all of the 64 \u00d7 squared differences. We also provide a measure of its dispersion by calculating the standard deviation of these 64 \u00d7 values before averaging over them. The RMSE standard deviation contains two sources of variation or error. One is due to the SUR estimation error within each of the 64 parameters, and the other is associated with the variation of the difference between the estimated and the starting value of the elasticity across the 64 parameters.\nAs shown in Table 2 , RMSE is 0.029 in the case of the median and almost the double (0.057) for the mean. To put these values into perspective, we calculate the percentage deviation of the RMSE with respect to the descriptive statistics of the starting distribution of elasticities. Relative to the median it yields a difference of 9.2% and, as expected, it is higher relative to the mean, 17.3%. The RMSE standard deviation is 0.051 for the median and 0.127 for the mean. Given that the SUR estimation provides only a minor source of error (because the point estimates are all highly significant due to the use of a data with only minor sources of noise), 21 the majority of the RMSE standard deviation is attributed to the deviations between the estimated and the initial value across elasticities. Since the skewness of initial production parameters drives the skewness of netputs quantities, and here firms are linearly aggregated, these deviations would most likely decrease under nonlinear aggregation.\nA third conclusion from Figure 3 is that the estimated standard errors of the SUR estimators exhibit a substantial downward bias, i.e., the SUR elasticity estimates are far less precise than indicated by their SUR estimated standard errors. The evidence for this result is that for the majority of the elasticities shown in Figure 3 , the population mean is outside of the respective 95% confidence interval. In fact, only 10.9% (= 7/64) of the 95% confidence intervals include the corresponding means, whereas by construction one would expect approximately 95% of such intervals to do so. 22 If each confidence interval comprised the mean with 95% probability, and intervals were independent, the probability of having at most 7 out of 64 means lie inside their respective confidence intervals would be essentially 0. Figure 4 illustrates the estimated results of the eight netput quantity elasticities with respect to the quasi-fixed input. The SUR estimated elasticities (circles) are within the interval of simulated elasticity distribution for all cases and, similar to the variable netputs case, closer to the median of the distribution than to its mean. As Table 2 indicates, the RMSE is 0.036 in the case of the median, and 0.040 for the mean. The size of the RMSE standard deviation also suggests high variation (of their dispersion relative to the initial value) across the 8 elasticities. Our estimated elasticities are 21 Results are available from the authors. 22 Even if one were to argue that the SUR elasticity estimate represents the underlying population median rather than the mean, the conclusion would apply because only 23.4% (= 15/64) of the 95% confidence intervals include the corresponding medians.\n9.0% apart from the median absolute value of the initial elasticity and 9.6% from the mean absolute\nvalue. Figure 4 also shows that the downward bias found in the estimated standard errors of the SUR estimators of the price elasticities also applies to the quasi-fixed input elasticities, as just 25% (= 6/8) of the 95% confidence intervals include the corresponding population means."}, {"section_title": "Conclusions", "text": "The dual relationship between the production function and the profit or cost function established by the Neoclassical theory of the firm has been widely applied in empirical work with the objective of obtaining, among others, price elasticities, substitution elasticities, and return to scale estimates.\nThis empirical method, usually referred to as \"the duality theory approach,\" has the advantage of providing the mentioned features of the production function using market data on input and output prices and quantities, without the requirement of explicitly specifying the parametric form of the production function. However, the duality theorem requires a set of assumptions about the DGP which are unlikely to hold in practice; or in other words, the market data typically employed in this type of studies bear levels of noise that prevent the theorem from holding exactly. If this is the case, the estimated technology need not provide a good estimate of the starting underlying technology.\nThe impact of random noise on the empirical performance of the dual approach is an area that has received scant attention in the literature. The present paper aims at contributing to this area of inquiry by showing how to generate a pseudo-dataset that replicates key aspects of U.S.\nagriculture. Model parameters are calibrated using datasets (both time-series and cross-sectional)\nwidely employed in empirical applications. We start by selecting a parametric form of the production technology and choosing its set of parameter values. By means of Monte Carlo simulations, we generate observations of quantities and normalized netput prices such that they are consistent with those found in data on U.S. agriculture. More precisely, we compute a panel of production and price data for successive periods of time, originated from a population of technologically heterogeneous firms that belong to different regions.\nWe illustrate the usefulness of the simulated dataset by applying it to analyze the ability of the duality approach to recover the underlying production technology when the only source of deviation from the basic tenets of the theory is the aggregation across heterogeneous firms.\nEstimated parameters (and resulting elasticities) come from applying standard econometric methods to a system of input demands and output supplies with the simulated data. Because the initial parameters are known from the outset, we can judge the degree to which the econometric approach applied on the dual problem recovers these parameters. Comparison between starting and recovered parameters relies on the use of Hessian identities. Our exercise, restricted to the dual-primal direction, can be complemented in future research with another carried out on the primal-dual direction.\nWhile the main results of this applications are as expected, that is, the dual approach applied to a time-series coming from aggregation across technologically heterogeneous firms is able to recover elasticities that are not only within the support of the distribution of initial elasticities, but also considerably close to the mean and median of such distribution, it is also found that the estimated standard errors of the SUR elasticity estimators exhibit a substantial downward bias (i.e., the SUR elasticity estimates are far less precise than implied by their SUR estimated standard errors). More importantly, this simulated dataset allows us to set a solid base to study any agricultural problem that requires knowing the underlying production parameters that generated the data. For example, a more in-depth understanding of the empirical properties of duality theory can be achieved with this pseudo-dataset, because it is straightforward to calibrate and incorporate other sources of observable and unobservable noise commonly found in datasets used by practitioners, such as uncertainty, prediction errors, omitted variables, netput aggregation, and endogeneity. This pseudo-dataset may also be used to investigate whether increasing the level of noise impacts linearly or nonlinearly the ability to recover the underlying technology parameters, or whether there is a noise threshold above which it is not worth applying the dual approach.\nAnother examples are the evaluation of alternative functional forms, or the various methods used to estimate technical change (e.g., the pseudo-dataset can be generated such that firm parameters change over time due to the evolution of technology, and study how precise are these methods in identifying and recovering the evolution of technology when it is explicitly accounted for in the estimation). Such analyses are complex and beyond the scope of the present study, which is meant to provide a building block for them. However, future research should address it to yield a better understanding of the empirical properties of duality theory.\nThese values are based on profit function estimated parameters found in literature using Eldon\nBall's dataset (Schuring, Huffman, and Fan 2011) . We transform the original estimates to meet the desired convexity properties and convert them to production function parameters using Hessian identities. This provides us with a first approximation of the parameters' values. 1 through 3 respectively, so the Beta distributions replicate the coefficient of variation of the technology parameters estimated by the fixed-effects regression using USDA-ARMS and PRISM datasets described in the text. Because parameters determine firm size, we impose a positive correlation of 0.9 between the shocks, so that firms producing high output quantities also use more inputs. In all cases, correlation is imposed by the method in Iman and Conover (1982) .\nMatrices 11, and 11, : We generate the inverse of the regional and firm-specific matrices 11, and 11, , because the latter is the one entering the FOCs of the firm's optimization problem. Then, to obtain the firm-specific submatrices \ufffd 11, \ufffd \u22121 in each region, we induce variation on the We calibrate the width of the Beta intervals enumerated above by trial and error such that they yield a set of firm-specific production parameters * in each region whose coefficient of variation is consistent with \ufffd 0 estimated with the fixed-effects model. These are 0.06, 0.17, and 0.43 for regions 1, 2, 3, respectively, as shown in table A1."}, {"section_title": "Appendix B. Estimation of firm's unobserved heterogeneity", "text": "Yields are specified as a function of a county-specific constant (the fixed effect) representing the average county's technology, and cumulative precipitation and average temperature over the growing season, and we assume the constant is correlated with the weather variables. This allows us to isolate the \"between\" effects (i.e., the variation in yields across counties not attributable to weather) from the \"within\" effects (i.e., the variation in yields within a county over time).\nFirm-level yields are specified as follows: \nwhere the \" \u0305 \" indicates means over time (used in demeaning the model) and the \" \ufffd \" indicates the point estimate of the parameters. Table A1 provides estimation results.\nFinally, the coefficient of variation of \ufffd 0 , representing variation across counties, serves to calibrate the unobserved dispersion of the production parameters around the regional mean that are not attributable to weather changes. 23 Note that this coefficient of variation represents the variation across counties of the fitted production coefficients, rather than the estimation standard error of the parameter. Variable \u0308 denotes demeaned farm-specific crop yields. Accent character \" \u2022\u2022 \" represent a demeaned variable. Standard errors in parenthesis. 23 We calibrate the production parameter variation equal to variation between counties, as opposed to between firms. Firstly, we do not have firm-specific weather data to calculate the between firms effects. Secondly, in a given region the data are likely to have smaller variation at the county (and more aggregated) level than at the firm level, favoring parameter recovery."}]