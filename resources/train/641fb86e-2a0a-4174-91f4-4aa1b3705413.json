[{"section_title": "Abstract", "text": "Summary: Medical imaging data with thousands of spatially-correlated data points are common in many fields.\nMethods that account for spatial correlation often require cumbersome matrix evaluations which are prohibitive for data of this size, and thus current work has either used low-rank approximations or analyzed data in blocks. We propose a method that accounts for nonstationarity, functional connectivity of distant regions of interest, and local signals, and can be applied to large multi-subject datasets using spectral methods combined with Markov Chain Monte Carlo sampling. We illustrate using simulated data that properly accounting for spatial dependence improves precision of estimates and yields valid statistical inference. We apply the new approach to study associations between cortical thickness and Alzheimer's disease, and find several regions of the cortex where patients with Alzheimer's disease are thinner on average than healthy controls."}, {"section_title": "Introduction", "text": "Alzheimer's disease (AD) is a neurodegenerative disease associated with severe memory impairment and cognitive deficits (McKhann et al., 1984) . The prevalence of AD is increasing, with some projections estimating over 100 million cases across the globe by 2050 (Rocca et al., 2011; Brookmeyer et al., 2007; Wimo et al., 2006) . To accelerate the pace of preventative and therapeutic strategies, the Alzheimer's Disease Neuroimaging Initiative (ADNI) (Mueller et al., 2005) was established as a large multi-center observational prospective cohort and consortium for sharing clinical, neuroimaging, and other biomarker data for public access.\nIn this paper, we use ADNI data to identify regions of the cortex most affected by AD.\nModeling spatial dependence in neuroimaging studies such as the ADNI study is key to efficient and valid inference (e.g. Spence et al., 1997) . A major obstacle in modeling the spatial dependence is the computational burden associated with handling the spatial covariance. Hyun et al. (2014) and Zhu et al. (2014) modeled spatial dependence with lowrank models and use Wald-based inference. While this is computationally convenient, lowrank approximations can fail to adequately capture local dependence (Stein, 2014) . Recently, many approaches have been proposed to model spatiotemporal data. For example, Kang et al. (2012) introduced a spatio-spectral mixed effects model for fMRI time courses that is able to capture the multiscale spatial covariance structure as well as stationary temporal dependence. To handle the temporal correlation the authors work in the Fourier domain (similar to Lange and Zeger, 1997) , as the Fourier coefficients are approximately uncorrelated across frequencies. Recently Castruccio et al. (2016) proposed a spatio-temporal model with spatially varying coefficients and multi-resolution error structure. Model estimation and inference is based on a three-step likelihood approximation approach (Castruccio and Guinness, 2017) , which is computationally expensive.\nA Bayesian approach is appealing because it naturally accounts for uncertainty in all model parameters (Handcock and Stein, 1993) . A drawback to the use of Bayesian methods in large imaging problems is their heavy computational burden, and thus previous work in this area focuses largely on computational approximation. Woolrich et al. (2004) proposed a fully-Bayesian model for spatiotemoral fMRI data. To facilitate computation, they assumed spatial independence given the previous timepoint, and thus it is not clear how to apply this method to our case with multiple subjects but only a single image per subject. Bowman (2007) aggregated data to regions of interest and then performed spatiotemporal modeling on these regional summaries. Musgrove et al. (2017) proposed a Bayesian variable selection model that divides the brain into regions and performs spatial variable selection within each region. Zhang et al. (2014) and Zhang et al. (2016) used variational Bayesian techniques to approximate the posterior distribution. While this methodology is computationally feasible and has been shown to provide good point estimates, its downside is that it yields poor estimates of the posterior variances, which may result in misleading inference.\nIn this paper, we propose a flexible model for cross-sectional multi-subject brain imaging data. The effect of covariates is allowed to vary spatially and is given a spatial shrinkage prior to encourage sparsity. The residual covariance is a combination of stationary covariance for flexible local dependence and a low-rank nonstationary covariance to capture functional connectivities between distant regions of the brain. The full spatial covariance matrix is cumbersome for large imaging datasets, and we therefore project to the spectral domain (Fuentes and Reich, 2010) to decorrelate the responses and facilitate a fully-Bayesian analysis simultaneously for all subjects. We apply this method to simulated multi-subject data and the ADNI data, where we find evidence of nonstationary spatial correlation and that properly accounting for this dependence improves fit."}, {"section_title": "Description of the ADNI data", "text": "This work is motivated by data from the Alzheimers Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). The ADNI was launched in 2003 as a public-private partnership, led by Principal Investigator Michael W. Weiner, MD. A primary goal of ADNI has been to test whether serial magnetic resonance imaging (MRI) can be used to measure the progression of mild cognitive impairment (MCI) and early Alzheimers disease (AD). We consider measures of cortical thickness estimated using FreeSurfer (Fischl and Dale, 2000a) from the MRI acquired in the ADNI (Jack et al., 2008) in 199 subjects with AD, 409 subjects with mild cognitive impairment which is thought to be an early stage of AD in some cases, and 231 healthy controls aged 54-91. The data were collected as part of ADNI1/GO, and all of the subjects were imaged on 1.5T scanners. Cortical thickness is estimated for each subject as the distance between the pial and white matter surfaces at each location on the cortex. Postprocessed maps of cortical thickness were downloaded from the Laboratory of NeuroImaging repository (http://adni.loni.usc.edu/). Briefly, the T1-weighted images were inhomogeneity corrected (Sled et al., 1998) and processed using version 4.3 of FreeSurfer (Fischl and Dale, 2000b) to estimate thickness across the cortex. These measurements were then registered across subjects to the fsaverage5 template. The images are smoothed at 10mm on the surface as part of preprocessing. Cortical thickness is known to decrease with normal aging, but more dramatic and spatially dissociable changes have been widely documented in AD (Lerch et al., 2005 (Lerch et al., , 2008 Dickerson et al., 2009 ). These changes have been assessed primarily using location-specific regression modeling which is known to be suboptimal. We aim to use the proposed spectral method to study the spatially-varying effects of a diagnosis and the ADNI composite memory score T for subject i. Associated with each subject are an intercept X i0 = 1 and covariates X i1 , ..., X ip . We assume that the n locations of interest form a regular grid and we do not specify a model that can extrapolate beyond these n voxels because typically imaging data are defined on an inherently bounded and discrete domain. The model for subject i is\nwhere the B k are the spatially-varying fixed-effect processes; Z j are known basis functions that explain the large-scale spatial structure; \u03b3 i = (\u03b3 i1 , ..., \u03b3 iJ ) T are the corresponding random effects; and E i (v) are small-scale spatial deviations. The residual spatial process E i is a mean-zero Gaussian process with isotropic Mat\u00e9rn covariance function (Stein, 1999) Cov\nwhere\nand K is the modified Bessel function of the second kind. The Mat\u00e9rn covariance has four parameters, \u03b8 = (\u03c3 2 , \u03c4 2 , \u03c6, \u03bd): \u03c3 2 is the variance of the non-spatial error (nugget); \u03c4 2 is the variance of the spatial process (partial sill); \u03c6 is the spatial range; and \u03bd dictates the shape of the spatial correlation function. The parameter \u03bd is referred to as the smoothness parameter because for a continuous process defined at an uncountable number of locations (as opposed to our discrete domain), realizations of the process are \u03bd times differentiable.\nThe large-scale spatial structure is determined by the J random-effect covariates Z j .\nSelection of the Z j depends on prior knowledge of the process and the desired inference.\nOne option is binary indicators for J regions of interest, such as anatomical regions. Another approach is to select data-driven basis functions using principal components. We use outer products of B-spline or Gaussian kernel basis functions, which have the advantage of being defined locally (e.g., each basis function has a unique maximizing voxel) and spanning the entire space of continuous functions as J \u2192 \u221e. The random effects are distributed\nThe overall covariance combining both large-scale nonstationary covariance and small-scale stationary covariance is\nThe spatially-varying intercept B 0 and slopes B k for k = 1, ..., p are modeled as independent (over k) Gaussian processes with mean\nand Mat\u00e9rn covariance with covariance parameters\nAs with the spatial covariance in (4), the mean functions have both large-scale trends given by the basis functions Z j and small-scale variation given by a Mat\u00e9rn spatial process. Of course, it is possible to use different basis functions for the mean and covariance, but we select the same basis for notational simplicity.\nAs we expect the intercept to be smooth over space we give normal priors \u03b2 0j indep \u223c Normal(0, \u03b4 2 0 ). In contrast, for the slope parameters we assume that for most of the spatial domain B k (v) = 0. To encode this prior, we use a continuous shrinkage prior for the \u03b2 kj for k > 0 that concentrates prior mass near zero to give sparsity, but also has heavy tails to allow for the signal to emerge. The shrinkage prior we select is the horseshoe prior (Carvalho et al., 2010) , which is \u03b2 kj |\u03b4 kj \u223c Normal(0, |\u03b4 kj | 2 ) and \u03b4 kj \u223c Cauchy(0, \u03b4 k )."}, {"section_title": "Description of the model in the spectral domain", "text": "The covariance in (3) is composed of a low-rank nonstationary component and a full-rank stationary component; the full-rank stationary Mat\u00e9rn process poses the computational challenge for large data sets. To decorrelate the Mat\u00e9rn process and permit an analysis of the entire spatial domain we transform to the spectral domain. Because the data are defined on a sphere, we use the spherical harmonics transformation (SHT) for the B k and E i . The E i (B k are expanded similarly) are represented as\nwhere S m (s 1 , s 2 ) are the spherical harmonic functions, \u03c9 = ( , m), and E i (\u03c9) are the unknown coefficients. The spherical harmonic functions are\nwhere P is the associated Legendre polynomial. The full representation of the process requires L = \u221e, but we approximate the process with a finite L. Transformations (e.g., from (6)) are conducted using least squares, and the unique real components\u1ef8 i (\u03c9) and\u1ebc i (\u03c9) are extracted from the complex Y i (\u03c9) and E i (\u03c9). That is, Fully-Bayesian spectral methods for imaging data 7 the SHT returns complex conjugate pairs of the form a i (\u03c9) + ib i (\u03c9) and a i (\u03c9) \u2212 ib i (\u03c9), and we extract the unique real values a i (\u03c9) and b i (\u03c9).\nas the real-valued (i.e., the unique real values extracted from the complex data in the spectral domain) spectral representations of the processes Y i (v),\nand E i (v), respectively, for frequency \u03c9. Since the SHT is a linear operator, the spatial model in (1) becomes\nThe Gaussian processes E i and B k are stationary and defined over a discrete spatial domain and thus their spectral counterparts\u1ebc i andB k can be expressed in terms of independent normal random variables (Yaglom, 2012) (including terms from the same conjugate pair)\nwhere the spectral density function\u03bb is determined by the Mat\u00e9rn covariance parameters.\nFollowing Guinness and Fuentes (2016) , we take the variance of\u1ebc i (\u03c9) to be \u03bb (\u03b8) = \u03c3 2 + \u03c4 2 (\u03c6 \u22122 + 2 ) \u2212\u03bd\u22121/2 . This mimics the flexibility of the Mat\u00e9rn covariance function based on Euclidean distance in that \u03c6 controls the range and \u03bd controls the smoothness of the process.\nThe basis functions Z j (v) are also projected into the spectral domain. The spectral processesZ j (\u03c9) will be nonzero for all j and \u03c9. However, since the B-spline basis functions are smooth functions, they can be approximated accurately with a small number of low-frequency terms with \u03c9 \u2208 L. Therefore, to improve computational efficiency, we setZ j (\u03c9) = 0 for terms with \u03c9 / \u2208 L, and select L to include roughly J terms."}, {"section_title": "Priors and computing details", "text": "The model laid out in Sections 3 and 4 can be written succinctly as follows (with all terms being defined conditionally on lower levels of the hierarchy). For terms with \u03c9 \u2208 L,\nAfter the spectral transformation,\u1ef8 i (\u03c9) andB l (\u03c9) are independent across \u03c9. The Mat\u00e9rn parameters are reparameterized from \u03b8 = (\u03c3 2 , \u03c4 2 , \u03c6, \u03bd) (and similarly for the \u03b8 k ) to overall variance v = \u03c3 2 + \u03c4 2 , the logit of the spatial variance proportion r = logit [\u03c4 2 /(\u03c3 2 + \u03c4 2 )], and the log range and smoothness, \u03c6 = log(\u03c6) and \u03bd = log(\u03bd). To complete the Bayesian model we specify the following uniformative priors: \u03a3 \u223c InvWishart(J + ,\n, and \u03c6 , \u03c6 0 , \u03c6 1 \u223c Normal(0, 1/ 2 ) with = 0.1; r, r 0 , r 1 \u223c Normal(0, 1) so the signal-to-noise ratio has prior mass spread over [0, 1]; and \u03bd , \u03bd 0 , \u03bd 1 \u223c Normal(\u22122, 1) to avoid too much mass on large smoothness parameters.\nWe explore the posterior with a standard combination of Gibbs and Metropolis sampling.\nB k (\u03c9), \u03b3 i , \u03a3, and v k are updated from their conjugate full conditional distributions. The remaining Mat\u00e9rn parameters are updated using random-walk Metropolis sampling with Gaussian candidate distributions tuned to give acceptance probability around 0.4. R code is available in the online supplementary materials."}, {"section_title": "Simulation study", "text": "In this section we conduct a simulation study to explore the effects of model misspecification on Bayesian inference. Each simulated dataset includes m subjects and p = 1 binary covariate. The first m/2 subjects have X i = 0 and the remaining have X i = 1. Data are generated on the unit sphere (i.e., v We fit several versions of the model given in Sections 3 and 4. For all models we approximate the process with L = 29 levels giving n = 900 terms. The J = 25 basis functions Z j (s) are taken to be Gaussian kernel basis functions with J knots forming a 5 \u00d7 5 rectangular grid over the observed latitudes and longitudes and L = {\u03c9|l < 5}. The kernel bandwidth is set to the minimum distance between knots, and distances between knots and data locations are computed using spherical distance. For each dataset we fit the model with (\u03c4 > 0,\n\"Mat\u00e9rn\") and without (\u03c4 = 0, \"Independent\") spatially-dependent residuals; with (\u03a3 = 0, \"Nonstationary\") and without (\u03a3 = 0, \"Stationary\") nonstationarity; and with (\u03b4 1 > 0, \"SP\") and without (\u03b4 1 = 0, \"NSP\") sparsity in the prior for B 1 . For each method and dataset we generate 10,000 MCMC iterations and discard the first 2,000 as burn-in. The full model takes around 8 seconds per 1,000 MCMC iterations for m = 50 subjects and 11 seconds per 1,000 MCMC iterations for m = 100 subjects using R on an ordinary PC.\nFor each dataset we compute the posterior meanB 1 (v) and 95% equal-tailed credible set\nfor all voxels and then compute the spatial average mean squared Table 1 reports the average of the mean squared error and coverage across the 100 simulated datasets for each scenario. The stationary model with independent residuals often has high MSE and low coverage. For the models with independent residuals, adding the low-rank non-stationary covariance improves performance, but coverage remains low in some cases. Coverage is at or near the nominal 0.95 level for the full model for all scenarios. The inclusion of nonstationarity and horseshoe priors improve performance in the anticipated settings.\n[ Table 1 about here.]"}, {"section_title": "Analysis of the ADNI data", "text": ""}, {"section_title": "Model comparisons", "text": "In this section we apply the model proposed in Section 3 to the ADNI data. Unlike Section 3, the covariate effects do not have mean zero because for this application we do not expect all fixed effects to be centered on zero (e.g., the confounder age should have an increasing effect throughout the brain). Therefore we modify (5) \nwhere theB k is the overall mean and the \u03b2 kj are have the same prior in as Section 3.\nFor our analysis we use L = 30 giving a reduction from 18,715 observations for each subject in the spatial domain to 931 observations in the spectral domain. Exploratory analysis in the Supplementary Materials (Figure 1) shows that terms beyond L = 30 do not appear to contain additional spatial signal. As in Section 3, we allow for a different set of spatial covariance parameters for the residual process E i and the fixed effects B k .\nPreliminary analysis shows that the nugget variance is estimated to be near zero and so it is excluded. Also, while we allow the six fixed effects terms to have a different variance \u03c3 2 k , we assume they have to same spatial range (\u03c6) and smoothness parameter (\u03bd) to improve MCMC convergence. The overall effectB k have normal priors with mean zero and variance 1,000, and for the remaining hyperparameters we use the same uninformative priors as in Section 6. The MCMC algorithm is run for 100,000 iterations, the first 10,000 are discarded as burn-in, and the remaining samples are thinned by 10 to remove autocorrelation. The full model with Mat\u00e9rn correlation, J = 225 basis functions, Horseshoe priors and nonstationary covariance fit to these data takes around 11 minutes per 1,000 iterations.\nWe compare several methods using test set prediction. We randomly (across subject and resolution) remove 10% of the\u1ef8 i , fit the model to the remaining 90%, and evaluate the agreement of the posterior predictive distributions and the test set observations. The models fit vary by the specification of the residual correlation, fixed and random effects as in the simulation study, and also the number of basis function J. Methods are compared in Table 2 using mean squared error of the posterior predictive means, the average (across all observations) of the posterior predictive variances, the empirical coverage of the 90% credible sets, and the posterior predictive mean of the log-likelihood of the test set observations. Comparisons are made by Legendre resolution in Supplementary Materials (Figure 2) . Inclusion of the Mat\u00e9rn covariance is the most important factor in determining the predictive log-likelihood, and including the nonstationary covariance with J = 225 also improves fit. For these data, the Gaussian prior for fixed effects is sufficient, and we proceed with Gaussian priors for the fixed effects and covariance that includes both the stationary Mat\u00e9rn component and non-stationary component with J = 225 basis functions.\n[ Table 2 Posterior z-scores plotted in Figure 3 . For the z-score maps, the threshold produced using the method of Sun et al. (2015) to control Bayesian false discovery rate at 0.01 (separate for all covariates) are given in the caption and statistically significant regions base on this threshold are displayed in the Supplementary Materials ( Figure 5 ). In this multiple testing problem, the one-sided null and alternative hypotheses are H 0 : \u03b2 k (s) 0 and H 1 : \u03b2 k (s) < 0, respectively, for each covariate except ADNI-Mem which uses H 0 : \u03b2 k (s) 0 and\nThese plots broadly resemble the least squares results in Figure 1 , but are smoother and in some cases reveal stronger results. Age and ADNI-Mem are the strongest predictors.\nAge is negatively correlated with the response throughout the cortex and has the strongest estimates in the anterior, specifically in the bilateral parahippocampal gyrus, temporal pole, dorsolateral prefrontal cortex, and superior frontal cortex. ADNI-Mem is positively correlated throughout the brain with the largest estimates in the anterior/inferior region, including bilaterally in precuneus, middle and inferior temporal lobes, and medial orbitofrontal gyrus.\n[ Figure However, AD is associated with significantly thinner cortex bilaterally in the inferior temporal lobes (including the parahippocampal gyrus), the occipitoparietal junction and dorsal prefrontal cortex. These regions overlap with well-established findings of parahippocampal atrophy in the ADNI by Greene et al. (2010) , and are understood to be involved in the earlier stages of pathology (Braak and Braak, 1991) . Atrophy in occipitoparietal junction is another well-documented change, and pronounced pathology in this area has been associated with a posterior cortical variant of AD (e.g., Crutch et al., 2012) which manifests in more visuospatial and visuoperceptive deficiencies. Predominant atrophy of the frontal cortex accompanies the frontal variant of AD which manifests in behavioral changes (Dubois et al., 2014) . Many of these regions overlap with those found to be associated with ADNI-Mem scores (Nho et al., 2012) ."}, {"section_title": "Discussion", "text": "In this paper, we proposed a spectral spatial model for large imaging data sets. The model accommodates features often seen in brain imaging data including nonstationary spatial covariance and local covariate effects. The spherical harmonics transformation permit a fullyBayesian application to large datasets while respecting the spherical nature of the data. Using simulated data we show that properly accounting for residual spatial correlation is necessary for efficient estimation and valid inference. When applied to the ADNI data we find that both stationary Mat\u00e9rn and low-rank nonstationary covariance terms improve fit, and that spatial methods identify regions of the cortex that differ by Alzheimer's disease status.\nWe have focused on multi-subject studies, however our methods could be applied for a single-subject study where data is collected over time. For example, after a discrete Fourier transform over time at each voxel (Kang et al., 2012 ) the images for different frequencies can be considered as independent (analogous to independence across subjects) and the proposed method can be applied. Another area of future work is to improve computation by exploiting parallel computing. The likelihood factors across subjects and frequencies, and therefore likelihood evaluations are embarrassingly parallelizable and many parameters could be updated in parallel which might improve computation time by an order of magnitude."}, {"section_title": "Supplementary Materials", "text": "Web . Posterior z-scores (i.e., the ratio of the posterior mean and standard deviation) for the fixed effectsB k + B k (s) for the ADNI data (the intercept and gender effect are omitted). We declare results statistically significant while controlling Bayesian false discovery rate at 0.01 if z < \u22121.16 for age, z > 0.88 for ADNI-Mem, z < \u22121.70 for AD; there are no significant results for MCI using this criteria. The gray region is the medial wall where the two hemispheres meet and does not include measures of cortical thickness. This figure appears in color in the electronic version of this article. Table 1 Simulation study mean squared error and pointwise coverage probability for B1(v) for data on a sphere. Data sets are generated with m subjects, with or without random effects (\"RE\") to induce nonstationarity, and with the true signal following the fixed hot-spot pattern (\"Hot spot=Y\") or from a Gaussian process (\"Hot spot=N\"). The model is fit with independent or Mat\u00e9rn small-scale residuals; with or without random effects to allow for nonstationarity, and with (\"SP\") or without (\"NSP\") Bayesian shrinkage priors. All values are multiplied by 100 and the maximum of each row's standard error is given in the final column.\n(a) Mean squared error "}]