[{"section_title": "Abstract", "text": "Abstract-Although both economists and psychometricians typically treat them as interval scales, test scores are reported using ordinal scales. Using the Early Childhood Longitudinal Study (ECLS-K) and the Children of the National Longitudinal Survey of Youth (CNLSY), we examine how order-preserving scale transformations affect the evolution of the blackwhite reading test score gap from kindergarten entry through third grade. Plausible transformations reverse the growth of the gap in the CNLSY and greatly reduce it in the ECLS-K during the early school years. All growth from entry through first grade and a nontrivial proportion from first to third grade probably reflects scaling decisions."}, {"section_title": "I. Introduction", "text": "E CONOMISTS who use test scores in their analyses have largely treated them as interval scales (like temperature). In reality, test scores are measured on ordinal scales (like utils). As with utility functions, any monotonic transformation of the test score scale is also potentially a valid scale. Surprisingly, there has been little attention to this issue among economists, although there are some exceptions. Lang (2010) raises concerns about ordinality in the context of value-added measurement. Cascio and Staiger (2012) consider how changes in scaling affect estimates of the fade-out of teacher value-added. As discussed in greater detail in the conclusion, similar concerns arise in the happiness literature.\nIn this paper, we take the ordinality problem seriously in the context of the debate over when the black-white test score gap emerges and how it evolves during the school years. In their influential and controversial studies, Levitt (2004, 2006) challenge the accepted view that a large blackwhite test score gap emerges in early childhood (Jencks & Phillips, 1998) . They find that the gap in kindergarten is both modest and largely \"explained\" by a small number of socioeconomic characteristics but widens sharply in the early years of schooling. 1 We perform a bounding exercise in which we examine how our estimates of the change in the black-white test score gap between kindergarten and third grade depend on choice of scale. Unfortunately, this puts only weak bounds on how the test score gap evolves. Without additional restrictions, we can conclude only that the change in the black-white test score gap between kindergarten and third grade is somewhere between 0 and .6 standard deviations.\nOne solution to the ordinality problem is to relate the test score to some outcome, as in Cunha and Heckman (2008) and Cunha, Heckman, and Schennach (2010) . When we choose scales that maximize the ability of earlier scores to predict performance on later tests, we generally find little growth in the gap between kindergarten and first grade but a significant widening of the gap by third grade. However, one such scale suggests no growth between kindergarten and third grade.\nThe extent to which family background, environmental measures, and parental behaviors can explain the test score gap is also controversial, in part because the influence of such factors varies among data sets and in part because of conceptual issues. 2 Jensen (1969) argues that controlling for such factors is subject to the \"sociological fallacy\": family background may include heritable factors. Dickens and Flynn (2001) argue that the environment is endogenous to ability (for example, students who appear to have high cognitive ability may be placed in more challenging classes).\nDespite these caveats, we also examine the relation between family background and the test score gap and ask whether it is robust to choice of scale. Most of the scales we derive show similar growth in the adjusted test score gap, but there is one notable exception, which reduces the estimated growth in the gap between kindergarten entry and third grade. Perhaps most striking, although our scales provide quite different estimates of the unadjusted gaps at entry and in third grade, there is almost no difference in the adjusted gaps at entry and only modest variation in the adjusted gaps in third grade. Thus the scales lead to very different conclusions about the importance of socioeconomic factors in accounting for the racial test score gap and its growth.\nIn the next section, we show numerical examples of how scaling decisions can be important in interpreting the test gap. In section III, we provide a short primer on item response theory scales, which some researchers claim are interval scales. We then describe the data used for this study (section IV) and present our approach (section V). Finally, we give our results in section VI and then provide some concluding remarks in which we discuss the use of ordinal scales more broadly."}, {"section_title": "II. Scaling Issues", "text": "For ease of exposition, we begin with a scale that takes a small number of discrete values. We trust, however, that it will be apparent that the issues we raise apply to continuous EVOLUTION OF THE BLACK-WHITE TEST SCORE GAP 1469 scales. Suppose we have a perfect test of mastery of each of three progressively difficult skills. We assume that the skills are cumulative either because skills are learned in this order or skill 2 cannot be mastered before skill 1 (two-digit addition requires one-digit addition) and there is no partial mastery. The test therefore produces scores of a (no skills mastered), b (only skill 1 mastered), c (skills 1 and 2 mastered), and d (all three skills mastered).\nIt might seem natural to assign the values 0, 1, 2, and 3 to these scores since these are the number of skills mastered. But the marginal value of all three skills need not be equal. Skill 1 might be the ability to recite the alphabet, 2 the ability to recognize letters, and 3 to read. Or skill 1 might be the ability to read and write English, skill 2 the ability to read and write Latin, and skill 3 the ability to converse fluently in Latin. In the latter case, as economists, we are inclined to view the marginal value of 1 as much greater than that of 2, which is in turn much greater than 3, but there are surely other admissible scales.\nSuppose we have a population of two blacks and two whites. Initially, the blacks score a and c, while the whites score b and c. If we use the naive scale, 0, 1, 2, 3, then the difference in the means is .5, or about .6 standard deviation. If instead we make the difference between a and b arbitrarily small relative to the difference between b and c, the test gap goes to 0. If we make the gap between b and c arbitrarily small relative to the gap between a and b, we get the maximum gap of about 1.15 standard deviations. Without some external reference for determining the proper scale, all we can say is that the test score gap is between 0 and 1.15 standard deviations.\nThe situation worsens when we ask how the gap evolves over time. A year later, we readminister our test and find that each student has progressed exactly one skill level (blacks score b and d, whites score c and d). Using the \"natural\" scale, the gap is unchanged. If we set b arbitrarily close to c, the gap falls from 1.15 to 0 standard deviations. And if we choose a \u223c = b and c \u223c = d, the gap grows from 0 to 1.15 standard deviations.\nLike economists, noneconomists frequently ignore the complexities associated with working with interval scales. However, as early as 1983, Spencer pointed out that the performance of two groups can be strictly ranked only if the cumulative distribution functions (cdf) of their scores do not cross (one is higher than the other in the sense of first-order stochastic dominance) and suggested comparisons based on these cdfs.\nSince percentiles are invariant to scale, a common solution is to use percentile ranks rather than test scores. The most prominent such measure is the percentile-percentile (PP) curve, which plots the percentile associated with a given score for one group (typically the lower performer) against the percentile associated with that score for the other. 3 If the PP curve does not cross the 45 degree line, the scores of one group are lower than the other in the sense of stochastic dominance. If one PP curve lies above another, the gap appears to be smaller for the comparison captured by the higher curve. Likewise, if the PP curves are identical, it would appear that the gaps are also identical. 4 Unfortunately the conclusions based on shifts (or lack thereof) of the PP curve can be misleading. In the above example, the PP curve does not shift between administrations of the test. But we saw that the gap might have increased or decreased. Moreover, the PP curve would also suggest no change if instead blacks scored a and d on the second administration. \"No change\" can be right in both instances only if a \u223c = b \u223c = c, in which case the first administration was completely uninformative."}, {"section_title": "III. Some Background on Educational Measurement", "text": "We have sometimes been asked how experts in educational measurement have responded to this paper. The simple answer is, \"With much kindness and patience for the gaps in our knowledge.\" The acknowledgments list several extremely helpful experts. One of them reported discussing the paper with others in the field and receiving reactions divided between \"that is deeply disturbing\" and \"that cannot be important.\" A few have implied that we underappreciated the degree to which psychometricians are aware of the issues we raise, an error that we attempt to correct in this section. None has suggested that test scores are in fact measured on interval scales. A distinct minority of researchers in the field believe that certain scales are interval scales, but they either did not read our earlier drafts or did not bother to correspond with us. 5 The concern that test scores are ordinal scales is longstanding. The earliest reference we found is Stevens (1946) . Among the giants of the testing field, Thorndike (1966, p. 124) makes the point emphasized in this article: \"It is assumed that the numerals in which the variables are expressed represent equal increments in some attribute. It is also recognized that this assumption is usually not well supported. But for 'rough and ready' studies of relationship, the violation of the assumption usually does not hurt much. However, when starting to deal with something as fragile as a change score, the violation of this basic assumption becomes a good deal more critical.\"\nHere we provide a brief primer on item response theory (IRT) scaling so that economists and others can understand why some researchers might believe that it generates interval scales. We again thank our tutors for their help and absolve them of responsibility for our errors. Readers familiar with the relevant literature or who do not care about it can skip this section. We refer readers who want more detail to Baker (2001) ."}, {"section_title": "THE REVIEW OF ECONOMICS AND STATISTICS", "text": "IRT models are defined by their number of parameters. Across all models, questions are identified by a difficulty parameter b, while individuals vary by a parameter \u03b8. The goal of IRT is to estimate \u03b8 for each test taker.\nIn the most general (three-parameter) logistic IRT model, the probability that a student i will answer question q correctly is given by 6 p iq = c q + 1 \u2212 c q 1\n\u03b8, b, and a measure student ability, question difficulty, and how well the question distinguishes among students of different abilities. c is guessability, or the probability that a student of extremely low ability will get the answer right.\nThe two-parameter model assumes no guessability (c = 0). It is universally recognized that the parameters are defined only up to a linear transformation. Adding the same constant to \u03b8 and b or multiplying \u03b8 and b and dividing a by the same constant does not change p. We can normalize the scale by, for example, setting the mean to be 100 and the standard deviation to be 15. Some authors claim that \u03b8 is uniquely identified given this normalization. The argument seems to be that given this normalization and adequate data, the \u03b8 i are unique. However, as discussed by Lord (1975) , one of the fathers of IRT, if \u03b8 = f (\u03b8) where f is a strictly increasing function, then replacing \u03b8 with f \u22121 \u03b8 fits the data with the same likelihood. As noted, most psychometricians accept Lord's argument.\nIn the special case where all questions are equally informative, a can be normalized to 1, which gives the one-parameter IRT model or Rasch scale. This has the property that regardless of question difficulty, a 1 unit increase in \u03b8 increases the log odds of getting the answer correct by 1. As a result, it is more common among individuals in the field to believe that Rasch scales can be interpreted as interval scales. 7\nWe should not necessarily dismiss distributions that primarily distinguish the very high and very low performers from everyone else. While the large spike at the mode when using the growth-minimizing transformation initially appears problematic, the implied distribution is not obviously more implausible than the U.S. earnings, income, and wealth distributions. However, it is perhaps more problematic that the growth-minimizing transformation requires this large spike to appear between school entry and third grade. The relation between the original and transformed scales is shown in figure 3 . The growth in the gap is minimized if differences in very low scores (roughly 15 to 40) and very high scores (roughly those over 140) are very informative but those in between are relatively uninformative. The transformation that maximizes the growth of the test score gap does the opposite, at least at the bottom of the scale, treating most differences among very low scores as uninformative. This would be appropriate if most children arrive in kindergarten knowing very little of the material covered by the ECLS so that throughout most of the distribution, differences in performance are relatively unimportant and only children with very high scores differ substantially from the mass of kindergarten entrants.\nThe results in this section bring out the fragility of any conclusion about how much the test score gap increases between school entry and the end of third grade. The bounds permit conclusions ranging from, \"There is essentially no gap when students begin school and a very sizable gap by the end of third grade,\" through, \"There is a modest gap at entry and essentially no growth in the gap over this period.\" For the PIAT, there are scales that imply that \"black children moderately lag behind white children in achievement when they enter school but match the achievement of their white peers by third grade.\" As is often the case with bounding exercises, the range of possible results is too large to be helpful.\nTherefore, determining the right scale is important in establishing how the gap between blacks and whites evolves. We could attempt to choose scales that produce aesthetically appealing distributions of test scores, but there is no consensus on what the distribution of childhood ability should look like. Well-accepted childhood tests, including the PIAT and the ECLS-K assessments, produce widely varying distributions of achievement. And unintuitive distributions of ability may be plausible for both young children and adults. We next consider a more formal approach to choosing the appropriate transformation. "}, {"section_title": "IV. Data", "text": "We use the Children of the National Longitudinal Survey of Youth (CNLSY) and the Early Childhood Longitudinal Study Kindergarten Class of 1998-1999 (ECLS-K)."}, {"section_title": "A. Children of the National Longitudinal Survey of Youth", "text": "The CNLSY is a biennial survey of children of women in the National Longitudinal Survey of Youth 1979 cohort (NLSY79), a longitudinal survey that initially followed a sample of 12,686 youths who were between the ages of 14 and 21 as of December 1978. Beginning in 1986, the children of women in the NLSY79 sample were surveyed and assessed biennially. The assessments included a battery of tests of psychological, socioemotional, and cognitive ability and questions on the home environment. Children exit the sample at age 15 and enter a separate sample of young adults. As of 2008, 11,495 children born to 4,929 unique female respondents had been surveyed.\nOur sample consists of children from age 3 or 4 through third grade, or roughly age 9, and so underrepresents children of older mothers, since children born after 1998, when the mothers would have been 34 through 41, will not have reached third grade. It also underrepresents children born before 1982, when the mothers were 17 to 25, since such children would be older than 4 in 1986.\nThe primary advantage of the CNLSY is that it includes both the Peabody Individual Achievement Test (PIAT), which is similar to the test in the ECLS-K on which Fryer and Levitt (2004) find little difference in the performance of blacks and whites entering kindergarten, and the Peabody Picture Vocabulary Test (PPVT), which is similar to tests on which a test score gap is measured before kindergarten entry.\nThe PPVT is a test of receptive vocabulary, that is, according to the CNLSY User's Guide, designed to provide a quick estimate of scholastic aptitude. The User's Guide reports that the PPVT was administered when children were 4 or 5 and again when they were 10 or 11. It appears to us that in fact, the earlier administration occurred between the ages of 36 and 60 months. To avoid measuring differences that might reflect kindergarten quality, we analyze PPVT scores only for children who were less than 4 years old when they took the test. We further limit our sample to black and white youths. We have a total of 1,655 scores (1,072 white and 583 black). 8 There are no repeat exam takers. The highest score attained by any child was 77. Two children scored 0, the lower bound. Based on the official scale, the test score gap is .97 standard deviation.\nAlthough not tied to a particular curriculum, the PIAT is designed to measure the types of skills typically taught in school. It covers a sufficiently wide range of material that the scores are not subject to boundary effects at the top, although this is somewhat of a concern at the bottom. The PIAT was administered at each survey to all children ages 5 to 14. Because the survey is conducted in alternate years, we typically observe a child in kindergarten and second grade or in first and third grade but not both. Table 1 shows descriptive statistics for our sample of PIAT test scores. Sample size is fairly consistent across the grades we study. Average scores rise steadily from 17 in kindergarten to 39 by third grade. The standard deviation of the test scores also rises. While there is at least one child who scores a 0 in each grade, these children are severe outliers in the later grades. In the third grade, for instance, the three lowest scores are 0, 2, and 3, while the fourth-lowest score is 15. The gap between blacks' and whites' PIAT scores is quite modest in kindergarten (.25 standard deviation), but expands over the first four years of school, reaching .61 standard deviation in third grade. These results are in line with those in Fryer and Levitt (2006) and our own from the ECLS-K, which we describe in section IIA.\nRecall that the prekindergarten gap on the PPVT is almost a full standard deviation, in line with the results in Jencks and Phillips (1998) . These two findings are suggestive of the conclusion in Murnane et al. (2006) that the difference between the results in Levitt (2004, 2006) and the prior literature reflect the differences in the tests."}, {"section_title": "B. Early Childhood Longitudinal Study", "text": "The ECLS-K is a nationally representative longitudinal survey that follows children who entered kindergarten in the 1998-1999 school year. Information was collected in the fall and spring of kindergarten, and the springs of first, third, fifth, and eighth grades. 9 In each survey year, the student's parents and teacher were interviewed about the child's background, home, and school environment. In addition, students took tests in reading, mathematics, and general knowledge or science. 10 The material covered on the test was constant through first grade but was modified in third grade to reflect students' growing knowledge. Children took a short \"routing test\" that directed them to a more comprehensive exam, the difficulty of which was determined by the routing test. According to the User's Guide, overall scores are calculated using IRT and represent the estimated number of questions the test taker would have answered correctly had she taken the entire test. In principle, a 112 on the kindergarten entry test represents the same level of accomplishment as a 112 on the third-grade test. We will focus only on the evolution of the test score gap through third grade but in some cases also draw on the fifth-grade data to scale the earlier scores. Therefore, we use the scores released with the fifth grade data file.\nWe mimic Fryer and Levitt's (2006) sample construction methods. We focus on the reading scores because they show the most striking growth in the early years in the Fryer/Levitt study. We drop students who are missing a valid reading score from kindergarten through third grade or do not have a valid entry for race. We use the sampling weights associated with kindergarten through grade 3 for child assessment studies and drop children who do not have a valid set of these weights. For much of the analysis, we use only the test score and race data, but in one table we control for sociodemographic characteristics. Table 2 shows descriptive statistics for our ECLS-K sample. We have 11,414 observations, of whom 62% are white and 17% are black. The baseline scale shows a modest (.4 standard deviation) test score gap at the beginning of kindergarten, rising steadily to a gap of three-quarters of a standard deviation toward the end of third grade. The second column of table 2 shows the corresponding figures from Fryer and 1472 THE REVIEW OF ECONOMICS AND STATISTICS Levitt. Although our sample is somewhat larger, and with a higher proportion of whites and blacks than theirs, the test score gap evolves in very similar ways in the two samples.\nWe note that there is only a modest amount of overlap in the entry and third-grade scores. About 95% of students received scores on the entry test that were below the lowest score on the third-grade test. Still, the remaining 5% scored better than at least some third graders, and two students entering kindergarten scored above the third-grade mean using the original test score scale."}, {"section_title": "V. Methods", "text": "Following the literature, we define the test score gap as the difference between the mean test scores of whites and blacks divided by the standard deviation of test scores in that grade. Our approach, described more formally below, is to look for order-preserving transformations of the test score scale used in the original data set and choose the ones that satisfy certain properties. Initially we look for the transformation that maximizes the growth in the scales and the one that minimizes the growth. When the number of points on the scale is small, we can literally transform every point on the scale except the highest and lowest. When the number of points is large, we use a very flexible order-preserving function. In addition to finding the bounds, we also choose the transformations that maximize the ability of an early test to predict performance on a later test.\nWe first search for the monotonic transformations of the original scale that maximize and minimize the growth of this gap. We impose the transformation\nwhere t is the original scale, T is the transformed scale, and a t+1 is a real number. Since the gap is unchanged by a linear transformation, we normalize T (0) equal to 0 and T (t max ) equal to t max where t max is the highest score observed in that grade. 11 Define G g to be the test gap in grade g,\nwhere G g is the gap in grade g, and N w , N b , and N are the sizes of the white, black, and total sample. We choose the remaining values of a to minimize the objective function given by\nwhere D is the difference between the test gap in grade 3 and the test gap in kindergarten, and a refers to the vector of coefficients. We define D max similarly for the maximum. In practice, not all scores are observed each year. We normalize a t+1 to 0 if no member of the sample in that grade has an initial test score of t + 1. This nonparametric approach gives bounds on the gap but produces implausible scales with only one or two steps. Additionally, it cannot be used when the test score is a continuous variable, as the ECLS-K assessment approximately is. Therefore, we use a sixth-degree polynomial given by\nwhere \u03b2 0 \u2212 \u03b2 6 and c are constants. This function is very flexible and can approximate a wide array of continuous functions. However, it need not be monotonic. Our algorithm checks for monotonicity and rejects parameters that violate this condition. 12 Of course, some monotonic functions may not be well approximated by even a monotonic 6 degree polynomial. We therefore cannot rule out the possibility that some other transformation could generate results outside the range we present here. Again, G is unchanged by linear transformations. When showing the density of the test scores, we normalize their standard deviation to equal 1 and choose \u03b2 0 so that their mean is 0. However, when showing the relation between the two scales, we fix the highest and lowest scores to be equal across scales. 13 If the test score distributions on entry and in third grade were disjoint, then (subject to a minor caveat about the ability of a 6 degree polynomial to simultaneously approximate two different distributions), we would find D max by minimizing the test score gap at entry and maximizing it in third grade. Conversely, to find D min , we would maximize G e and minimize G 3 . Because the two test score distributions overlap, we cannot do the maximizations and minimizations separately. 14 Nevertheless, because there is not much overlap, the process of selecting the transformations closely mimics this approach.\nAs we will see, in both data sets, the implications of D min and D max are very different. In the latter case, the black-white gap is trivial when children first enter school but grows substantially by the end of third grade. In contrast, in the former case, the black-white gap in the ECLS-K is modest but not trivial at school entry and changes little over the next four years. In the CNLSY, the gap under D min actually shrinks.\nThese bounds are not very helpful. Therefore, we select among the possible transformations, including less extreme 12 This introduces a discontinuity into our objective function that creates many local minima (maximuma). We therefore searched from several different starting locations for each transformation and report that which produced the smallest (largest) gap. 13 In practice, it was easier to do the estimation by setting the constant term to 0 and constraining the linear term and only subsequently transforming the estimated coefficients.\n14 It is not entirely obvious that we should treat the difference between getting exactly the first six and the first five questions right as identical regardless of when the student took the test, but we impose this assumption. Gaps are average white score minus average black score on the PIAT-RC and are measured in standard deviations. \"Minimum\" and \"Maximum\" are scale transformations that minimize and maximize the growth of the test gap from kindergarten through third grade. \"Corr Max\" is the transformation that maximizes the correlation between the PIAT-RC at kindergarten and the PPVT at age 3. Standard errors in parentheses.\n* p < .1, * * p < .05, and * * * p < .01.\nones, by choosing the transformations with the most predictive power for future test scores. For the CNLSY, we maximize the correlation between the PPVT at age 3 and the PIAT reading test administered during kindergarten. For the ECLS-K, we maximize the correlation between the entry and third-grade tests."}, {"section_title": "VI. Results", "text": ""}, {"section_title": "A. Maximizing and Minimizing the Growth of the Gap", "text": "The first column of table 3 shows the black-white test gaps in the PIAT, using the original scale. It increases significantly from a modest .25 standard deviation in kindergarten to .61 standard deviation by third grade. Under the growthminimizing scale (column 2), the black-white test gap shrinks by .18 standard deviation during the first four years of education. The test gap in kindergarten is similar to that of the baseline at .24 standard deviation, but declines to .18 standard deviation in first grade and .08 standard deviation in second grade, ending at .06 standard deviation in third grade. The growth-maximizing transformation reduces the gap at kindergarten to just .05 standard deviation, but at higher grades, the gaps are similar to those in the baseline model, with blacks performing .63 standard deviation worse than whites in third grade, only slightly worse than they perform using the baseline scale. Thus, the gap grows by .58 standard deviation over the first four years of school.\nThe extreme transformations produce scales that differ noticeably from the baseline. They are essentially step functions, with scores that are almost constant within tiers separated by large jumps. Though this may not be intuitively appealing, it is not unlike tests that have \"proficiency\" cutoffs. If kindergartners differ only in their possession of a few meaningful skills such as the ability to recognize letters, the ability to recognize words, and the ability to read for comprehension, this could be an appropriate scale at that grade. In fact, the PIAT reading test is designed somewhat like this. Students must pass a reading recognition test to advance to a reading comprehension test. The modal score in both our kindergarten and first-grade sample is 18, the highest score at Gaps are average white score minus average black score on the ECLS-K reading assessment measured in standard deviations. \"Minimum\" and \"Maximum\" are scale transformations that minimize and maximize the growth of the test gap from fall kindergarten through spring third grade. \"Corr Max\" is the transformation that maximizes the correlation between the fall kindergarten and spring third-grade test. Standard errors are in parentheses.\n* p < .1, * * p < .05, and * * * p < .01.\nwhich students do not advance to the reading comprehension section. Table 4 shows the achievement gaps on the ECLS-K reading assessment from the beginning of kindergarten through the spring of third grade. The first column repeats the baseline pattern from table 2. The second column shows the transformation that minimizes the growth in the gap. The resulting growth is only .05. The gap at kindergarten is .46, only slightly higher than in the baseline, 15 but in third grade, the gap is only .51 and thus noticeably less than in the baseline. Note that in principle, minimizing growth between entry and third grade could still generate large swings in the first-grade gap. However, there is no noticeable change in the gap between any pair of tests when this scale is applied.\nColumn 3 shows the results of choosing the transformation that maximizes the growth of the gap between kindergarten and third grade. The transformed gap at the beginning of kindergarten is now only .11 standard deviation, which is .29 less than in the baseline. The transformed gap increases by .10 standard deviation to .21 between the fall and spring kindergarten tests and then rises a further .22 standard deviation by the spring of first grade, so that the estimated gaps are similar to the baseline for the first and third grades. The end result is a growth of .64 standard deviation in the racial test gap in the first four years of education, almost twice that using the baseline scale. 16 Figure 1 shows the density function of test scores at kindergarten entry associated with each ECLS-K scale. Note that the baseline density is skewed with a long right tail. In contrast, visually, the density using the minimizing transformation more closely approximates a normal distribution. The density associated with the maximizing transformation is somewhat aesthetically displeasing and possibly unattractive on other grounds. Most of this distribution's weight is in a narrow band around its mode, with none substantially below. Nevertheless, this representation of the scores is not entirely counterintuitive. It is plausible that most children have few reading, math, and general knowledge skills and that the modest differences over much of the range are uninformative. On the other hand, there are a few, best represented by the two who are already operating solidly at the third-grade level, who are truly distinct from the rest of the pack. Moreover, in some respects, the density of the growth-maximizing transformation is more aesthetically pleasing than the income or wealth distribution in the United States. It is less skewed than either. The 50-10 spread (measured in standard deviations) is plausibly larger than it is in the wealth distribution. 17 How do these transformations affect the test score distributions in third grade? As previously noted, the transformation that minimizes the growth in the gap will be close to the one that minimizes the third-grade gap, while the choice of T (t) that maximizes the growth of the gap produces a thirdgrade gap very close to the one in the baseline. Figure 2 shows the densities of the three test score distributions. As for the kindergarten scores, the key to minimizing the thirdgrade gap, and thus growth, is compressing the middle of the distribution so that most students appear quite similar and spreading out the differences among very high and very low scores. In contrast, the growth-maximizing transformation leaves the distribution of test scores looking similar to that associated with the baseline."}, {"section_title": "B. Selecting Transformations", "text": "We do not expect kindergarten or first-grade scores to perfectly predict third-grade scores. Performance on each test is random, and students' academic progress varies. Indeed the point of the exercise is to ask whether blacks and whites progress academically at different rates during the first four years of school.\nNevertheless, tests measure related skills. Students who perform well on one test generally perform well on other tests. A reasonable criterion is to select the transformation that allows us to best predict future performance using information from previous tests. 18 We therefore choose transformations that maximize the correlation between test scores. If the tests measure a common underlying latent variable, this maximizes reliability, but we do not require this interpretation\nFor the CNLSY, we construct a sample of 398 white and 253 black children who took both the PPVT before age 4 and the PIAT while in kindergarten. The racial test gaps (.97 standard deviation on the PPVT and .2 on the PIAT) for this subsample are very close to those for the full sample. The correlation between the untransformed test scores is .32.\nWe use monotonic sixth-degree polynomial transformations to find the scales that maximize the correlation between individuals' PPVT and PIAT scores. These scales increase the correlation between these two tests only moderately, to .35, and do not noticeably alter the racial test gaps. The PIAT gap falls to .24, while the PPVT gap increases to .98. 18 Kimeldorf and Sampson (1978) refer to this as the monotone correlation.\nIn figure 4 , we plot the correlation-maximizing transformations, normalizing each scale to have the same range as the baseline. The transformed PPVT is similar to the original but compresses the highest scores. The PIAT transformation magnifies differences among the highest test scores while compressing the scores somewhat below the highest. This suggests that a high PIAT test score may be a more important predictor of performance than a high PPVT test score, but this inference is based on only a few observations.\nColumn 4 of table 3 shows the evolution of the PIAT gap under this scale. Surprisingly given the modest transformation, the pattern differs substantially from the baseline. The kindergarten gaps are similar, but the gap drops to .19 in third grade for a decrease from kindergarten through third grade of approximately .05 standard deviation. One caveat is that roughly 18% of the third-grade sample scored above the highest kindergarten score. This problem is less important for the first-and second-grade tests. Yet the gap using the transformed scale is essentially constant from kindergarten through second grade while it grows substantially using the original scale.\nThe ECLS-K can only maximize the correlation across reading assessments. First we maximize the correlation between the kindergarten entry and third-grade tests. This new scale substantially increases the correlation between the two scores from .54 (R 2 = .29) at baseline to .62 (R 2 = .39), and produces gaps close to the potential maximums at kindergarten entry and in third grade.\nAs shown in column 4 of table 4, the growth in the gap from kindergarten entry through third grade is .26 standard 1476 THE REVIEW OF ECONOMICS AND STATISTICS deviation, .09 smaller than the growth in the baseline. All the growth occurs between the end of first grade and the end of third grade. This differs from the baseline steady increase throughout the first four years of schooling.\nWe also explored the scales that maximize the R 2 from regression of the third-and fifth-grade scores on all prior scores. Both approaches yield results similar to those in column 4."}, {"section_title": "C. Controlling for Socioeconomic Factors", "text": "Surprisingly, Fryer and Levitt (2004) find that the modest black-white test score gap at school entry can be accounted for fully by a small number of socioeconomic characteristics: child's age, child's birth weight, a socioeconomic status measure, WIC participation, mother's age at first birth, and number of children's books in the home. In table 5 we ask whether the same is true for the scales we have developed. Strikingly, the kindergarten entry results are robust to the choice of scale. Whether the scale shows an unadjusted gap of .11 or .47, after controlling for this small number of factors, the remaining gap is actually reversed and favors blacks by between .03 and .05 standard deviation.\nIn contrast, the effect of the controls in third grade depends on the scale. For three of the four scales, controlling for the socioeconomic factors reduces the gap from approximately .75 to about .3 standard deviation. This still indicates a substantial deterioration in the relative performance of black children over the first three years of school. In contrast, the transformation that minimizes the growth of the unadjusted gap shows a noticeably more modest adjusted gap of .17. In Gaps are the coefficient on a white indicator variable with black as the excluded variable and are measured in standard deviations of the ECLS-K reading assessment. Each regression controls for SES, number of books in the home, gender, birth weight, indicators for whether the mother was a teenager or over 30 years old at first birth, and WIC recipiency. \"Minimum\" and \"Maximum\" are scale transformations that minimize and maximize the growth of the raw test gap from fall kindergarten through spring third grade. \"Corr Max\" maximizes the correlation between the fall kindergarten and spring third grade test. Standard errors in parentheses.\n* p < .1, * * p < .05, and * * * p < .01.\nthis case, two-thirds of the unadjusted gap is accounted for by the measured characteristics, a somewhat larger proportion than the little over half accounted for when the other scales are used. Thus, the choice of scale significantly affects the magnitude of the increase of both the adjusted and unadjusted gap. 19 Another surprising result in Fryer and Levitt (2006) is that the growth of the black-white test gap is virtually unaffected We further analyze the robustness of this result in table 6. In the first two columns, we minimize the ratio of the growth in the controlled gap to the growth in the uncontrolled gap. Using this transformation, the raw test gap grows by .59 standard deviation from kindergarten through third grade, while the controlled gap grows by only .28. In columns 3 and 4, we instead maximize the difference between the growth of the raw test gap and the growth of the controlled gap. The pattern under this transformation is similar."}, {"section_title": "D. Scale Sensitivity", "text": "The choice of scale limits the potential magnitude of between-group differences. An example may clarify this point. A researcher administers a test to 50 black and 50 white children to determine whether they are \"learning-to-read\" or \"reading to learn.\" If he sets the scale so that 50 students are assigned to each group, if all the learning-to-read students are white and all the others are black, the test score gap is almost exactly 2 standard deviations. In contrast, if he sets the cutoff for reading to learn so that only 25 students earn this score, the highest possible gap is about 1.1 standard deviations. The gap can be bigger in the former case than in the latter.\nThe lower half of the scores in the ECLS-K kindergarten test are clustered within 1 standard deviation of the median. This characteristic of the test and scaling affects the potential for a large test score gap in kindergarten. To analyze the potential effect of such clumping on the evolution of the racial Gaps are average white score minus average black score on the ECLS-K reading assessment and are measured in standard deviations. \"Minimum\" and \"Maximum\" are scale transformation that minimize and maximize the growth of the test gap from fall kindergarten (Fall-K) through spring third grade (Spring-3). \"Corr Max\" is transformation which maximizes the correlation between the fall kindergarten and spring third-grade test. Maximum test gap is the test gap that would be observed if all the lowest scores belonged to blacks and all the highest scores belonged to whites. test gap in the ECLS-K, we calculate what the test gap would be in each grade if blacks had all the lowest scores and whites all the highest. Denoting w j as the weighted number of children with score t j , where j is the rank (from low to high) of the score and W B is the weighted number of blacks in the sample, we create a set of weighted test scores B = {t j |j \u2208 [1, m]} where m solves the problem\nWe likewise assign all the highest scores to whites, based on their weighted proportion of the sample. 20 The first column of table 7 shows the weighted test gaps and the upper bounds. While the observed black-white test gap increases over time, so does the bound. The theoretical maximum gap based on the distribution at the beginning of kindergarten is 1.5 standard deviations. This rises to 2.2 standard deviations by the end of third grade. Consequently, the observed racial test gap as a percentage of the possible test gap hardly changes over time, from 27% at the beginning of kindergarten to 33% at the end of third grade. This raises the concern that part of the large observed increase in the racial achievement gap in the ECLS-K reflects changes in scale and test sensitivity rather than changes in the real achievement gap.\nColumns 2 and 3 of table 7 show the maximum test gap for the transformations that minimize and maximize the growth of the gap. The minimizing transformation yields a test gap that is 24% of the bound at kindergarten entry but 53% at the end of third grade despite virtually no growth in its absolute size in standard deviations. In contrast, using the maximizing transformation, the gap shrinks from 46% of the bound at the start of kindergarten to 35% at the end of third grade despite a nearly 700% increase in its absolute size. Our transformations appear to act mainly by changing the potential sensitivity of the scale to the racial test gap. The test gap at third grade can be no larger than .95 standard deviation under the minimizing transformation, compared to 2.23 standard deviations in the baseline. The maximizing transformations can have a gap no larger than .24 at kindergarten, which is not only lower than Panel A shows the test gaps given each grade's ranking, were that grade's scores scaled as they were on the Fall-K and Spring-3 test. Panel B shows the test gaps given each grade's scale were they to have the same ranking as Fall-K and Spring-3. Gaps are average white score minus average black score on the ECLS-K reading assessment and are measured in standard deviations.\nthe maximum in the baseline of 1.5, but lower also than the actual observed test gap in the baseline of .4 standard deviation. Column 4 shows the bounds for the test gap under the transformation that maximizes the correlation across tests. Strikingly, the maximum gap is almost identical at kindergarten entry and third grade. The increase in the estimated gap as a proportion of the maximum gap therefore reflects changes in the former rather than the latter. Recall, however, that the increase in the estimated gap with this scale is smaller than with the base scale.\nAn alternative approach is to look at the gap holding the test scale constant. Denote F g (r) as the function that maps a child's performance rank to a test score. Panel A of table 8 shows the evolution of the test gap if F g (r) did not vary with g. That is, we choose an initial grade and then take a child's rank on each grade's exam and reassign to him or her the score given to the child who was at that rank on the initially chosen exam. 21 When we impose either the fall kindergarten or the spring third-grade mapping, we see virtually no growth in the test score gap until the third grade, but substantial growth for this test.\nPanel B instead supposes that we fix r while varying F g (that is, changing the scales across grades as occurs de facto in the ECLS-K.) Even if the rank order of students did not change, the test gap using the baseline scales in the ECLS-K would grow by .09 standard deviation from entry to third grade, due simply to changes in the spacing between ranks over time. Likewise, using the third-grade rank order, we would observe a .13 standard deviation increase in the test gap. Most of the increase in this gap occurs between the spring first-grade and spring third-grade tests, .05 standard deviation using entry rank and .07 using the third-grade rank.\nTable 8 strongly suggests that the growth in the test gap from kindergarten through first grade reflects scales and not achievement. Moreover, a significant portion of the growth 21 Since we are using weights, we cannot map rank in one grade directly to rank in the other grade. Instead we view rank as a continuum and look at masses at each score. This results in some children receiving a weighted average of two consecutive scores. The results are sensitive to the way in which ties at scores are broken, but since there are very few ties given the quasi-continuous nature of the scoring system, this sensitivity is only beyond the fourth decimal point. from first to third grade also reflects scaling decisions. Taken together, tables 7 and 8 suggest that when we use the base scale, something on the order of 8 to 13 percentage points of the growth in the gap between entry and third grade reflect scale sensitivity."}, {"section_title": "VII. Summary and Conclusion", "text": "Our findings suggest that we should exercise great caution when using test scores to determine when a black-white test score gap first emerges and whether it widens in the early school years. Our message is not limited to the blackwhite test score gap but applies whenever test scores or other ordinal measures are used as dependent variables. There is increasing pressure in the United States and elsewhere to use \"value-added measures\" to determine teacher compensation and retention. Lang (2010) presents a simple example in which the ranking of teachers is highly sensitive to the choice of scale. Before linking important decisions mechanically to value-added measures, we should ensure that they are robust to arbitrary scaling decisions.\nCascio and Staiger (2012) consider a related issue. Economists frequently renorm scales to have mean 0 and variance 1 each year. For any given year, this is simply a linear transformation. However, if the variance of the true scale grows as students progress through school, then we are using different linear transformations each year and the scales are no longer strictly comparable. If variance grows quickly as students progress through school, whatever students learned earlier will seemingly become less important because the true scale is divided by a higher number later in school. Therefore, fade-out can be a mechanical result of the convention of normalizing test scores. They find evidence of such an effect but conclude that it is only of modest importance. However, Cascio and Staiger do not address ordinality. Their approach depends on the ability to calculate correlations among test scores, which is possible only if they are measured on an interval scale. We believe it is unlikely that their result would hold for arbitrary monotonic transformations.\nWe are not the first to recognize the difficulties of working with ordinal test score scales. Cunha and Heckman (2008) and Cunha et al. (2010) tie test scores to adult outcomes. This gives test scores a valid external reference but is not a panacea. The scale will differ if it is tied to wages or to log wages, and the choice between the two is essentially a welfare judgment. We do not know whether plausible variation in the choice of anchor would affect the outcome of their research.\nNor is our message limited to the education and child development literature. Similar problems arise in the happiness literature. A common, perhaps the central, finding in this literature is that the relation between happiness and income is clearly positive within countries but at best weak across countries. Happiness is typically measured on an ordinal scale of 3 to 5 points and rarely more than 11. Researchers routinely average the answers to find the mean level of happiness or life satisfaction. 22 Without analyzing the underlying data, it is difficult to know how problematic this is.\nIn this paper, we have shown that scaling matters but differently for different tests. More broadly, our findings suggest that economists and other researchers should be much more circumspect in their use of test scores and other ordinal scales as dependent variables, particularly when comparing changes across groups. While many findings will be robust to scale changes, many will not be. 22 Two noted happiness researchers went further and treated the scale as a ratio scale: \"Our data showed that people who earned $55,000 were just 9% more satisfied than those making $25,000\" (Dunn & Norton, 2012) "}]