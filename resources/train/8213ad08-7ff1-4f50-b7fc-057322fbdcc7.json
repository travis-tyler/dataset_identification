[{"section_title": "Abstract", "text": "Abstract: The aim of this paper is to conduct a systematic and theoretical analysis of estimation and inference for a class of functional mixed effects models (FMEM).\nSuch FMEMs consist of fixed effects that characterize the association between longitudinal functional responses and covariates of interest and random effects that capture the spatial-temporal correlations of longitudinal functional responses. We propose local linear estimates of refined fixed effect functions and establish their weak convergence along with a simultaneous confidence band for each fixed-effect function. We propose a global test for the linear hypotheses of varying coefficient functions and derive the associated asymptotic distribution under the null hypothesis and the asymptotic power under the alternative hypothesis are derived. We also establish the convergence rates of the estimated spatial-temporal covariance operators and their associated eigenvalues and eigenfunctions. We conduct extensive simulations and apply our method to a white-matter fiber data set from a national database for autism research to examine the finite-sample performance of the proposed estimation and inference procedures."}, {"section_title": "Introduction", "text": "There has been an increasing interest in the analysis of massive functional data sets, many of which originate from brain imaging in large-scale longitudinal biomedical studies such as the Alzeimer's Disease Neuroimaging Initiative (ADNI) (Evans and Group, 2006; Mueller et al., 2005; Greven et al., 2010; Yuan et al., 2014; Zipunnikov et al., 2014) . In such studies, longitudinal functional data from n different subjects are usually observed at or are registered to a large number of locations in a common space, denoted by S, across multiple time points {t ij : j = 1, . . . , T i ; i = 1, . . . , n}, where T i is the total number of time points for the i\u2212th subject. Here we use the term \"functional data\" for data that are measured densely in S, \"spatial correlation\" for correlations within the functional data, and \"longitudinal data\" and \"temporal correlation\" for data that are measured sparingly in {t ij : j = 1, . . . , T i , i = 1, . . . , n} to distinguish them.\nThe sheer size and complexity of longitudinal functional data poses substantial challenges to most existing statistical methods for analyzing univariate or multivariate longitudinal data (Diggle et al., 2002; Fitzmaurice et al., 2004) . The major challenges include: (i) complexity of the temporal-spatial covariance structure, (ii) determining how to take advantage of the temporal-spatial smoothness, and (iii) theoretical justification of inference procedures. The first challenge is how to introduce random effects to characterize the spatial-temporal covariance structure of longitudinal functional responses. The second one is how to incorporate temporal-spatial smoothness into both estimation and inference procedures to improve statistical efficiency (Ramsay and Silverman, 2005) . The third one is to systematically investigate the theoretical properties (e.g., consistency) of estimation and inference procedures for statistical models developed for longitudinal functional data.\nModels for longitudinal functional data fall into a general functional mixed effects modeling framework, which serves to characterize functional data with various levels of hierarchical structures (Guo, 2002; Wu and Zhang, 2002, 2006; Morris and Carroll, 2006; Di et al., 2009; Greven et al., 2010; Zhou et al., 2010; Zhu et al., 2011; Shi and Choi, 2011; Cao et al., 2012; Chen and M\u00fcller, 2012; Horvath and Kokoszka, 2012; Meyer et al., 2015; Reiss et al., 2014; Scheipl et al., 2015; Zipunnikov et al., 2014; Staicu et al., 2015; Cederbaum et al., 2016) . The term functional mixed effects models (FMEMs) for correlated functional data was introduced in Guo (2002) , while Morris and Carroll (2006) and subsequent work by this group developed general functional mixed effects models with multiple levels of random effect functions as well as curve-to-curve deviations. Recently, a general framework of functional additive mixed models was introduced by (Scheipl et al., 2015) . Moreover, several FMEMs have been developed for longitudinal functional data (Greven et al., 2010; Yuan et al., 2014; Zipunnikov et al., 2014; Di et al., 2014) . To the best of our knowledge, most papers on functional mixed effects models focus on challenges (i) and (ii) above, while our focus in this paper is challenge (iii), the theoretical challenges.\nTo address challenge (iii), we provide a comprehensive theoretical analysis for a class of FMEMs. Our FMEM consists of a measurement model at each grid point s \u2208 S and a hierarchical factor model. The measurement model primarily includes fixed effects to characterize the varying association between longitudinal functional responses and the covariates of interest. The hierarchical factor model primarily uses random effects to capture the medium-to-long-range spatial covariance and the local covariance structure. Formally, we establish the weak convergence of the estimated varying association function, the uniform convergence rate of the spatial-temporal covariance estimator, the asymptotic distribution of a global test statistic for linear hypotheses of the regression coefficient functions, and an asymptotic simultaneous confidence band for each varying fixed effect function. The code and documentation for FMEM written in Matlab along with its documentation are freely accessible from the website http://www.nitrc.org/projects/fadtts."}, {"section_title": "FMEM: Functional Mixed Effects Model", "text": ""}, {"section_title": "Model Setup", "text": "Suppose that we observe longitudinal functional data and clinical variables from n independent subjects. Let T i be the total number of longitudinal measurements for the i-th subject, i = 1, . . . , n, and t ij be the j-th measurement time point for the i-th subject, so j = 1, . . . , T i . Throughout this paper, we focus on a fixed number of time points and sparse longitudinal data, that is, max i\u2264n T i < T 0 < \u221e. Let s m represent a specific grid point of the functional template space S for m = 1, . . . , M . Specifically, for the i-th subject at time t ij , we observe functional data, denoted by y ij (s m ) = y i (t ij , s m ) for 1 \u2264 m \u2264 M , and a p x dimensional covariate vector x i of interest, denoted by x ij = x i (t ij ), at time t ij . The x i may include time-independent as well as time-dependent covariates, such as age, gender, and genetic markers. For ease of notation, it is assumed throughout this paper that S = [0, 1] and 0 = s 1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 s M = 1, but our results can be easily extended to higher dimensions, when S is a compact subset of a Euclidean space.\nWe consider a FMEM consisting of a measurement model and a hierarchi- \nof the fixed-effect functions of s, and\nvector of the random-effect covariates associated with the random effects b i (s).\nT is a vector of the random effects that characterize the spatial temporal correlation structures across the functional domain space; whereas e ij (s) is a spatial random process delineated from b i (s), i.e., after\nMoreover, e ij (s) and b i (s) are independent. In many applications, \u00b5(x ij , \u03b2(s)) = x T ij \u03b2(s) is a linear function of x ij , similar to the setting of traditional linear mixed-effects model, so we focus on this special linear case in the paper. Extensions to nonlinear cases is discussed in Remark 1. Since marginally, for a fixed s, model (2.1) with \u00b5(x ij , \u03b2(s)) = x T ij \u03b2(s) is a standard linear mixed effects model, this motivates us to adopt standard notation for linear mixed effects models. Moreover, since z ij may include time-independent, as well as time-dependent, covariates, the inclusion of z T ij b i (s) allows us to capture a large portion of the variation in the spatial and temporal correlation structures.\nThe spatial random process e ij in (2.1) is further decomposed into two parts,\nwhere e ij,G (s) is a smooth stochastic process representing the global dependency that depicts the medium-to-long-range spatial dependence, e ij,L (s) is a measurement error representing local variability, and e ij 1 ,G (\u00b7) and e ij 2 ,L (\u00b7) are independent for any j 1 and j 2 . Since e ij,L (s) are measurement errors, we assume that e ij 1 ,L (s) and e ij 2 ,L (s ) are mutually independent whenever either j 1 = j 2 or s = s . We also assume that, for any j 1 = j 2 , e ij 1 ,G (\u00b7) and e ij 2 ,G (\u00b7) are mutually independent. This assumption is equivalent to assume that the random effects\nT explains all the within-subject correlation along the longitudinal direction, which is a common assumption in linear mixed-effects \nwhere 1(\u00b7) is an indicator function."}, {"section_title": "Estimation Procedure", "text": "Our primary goal is to find efficient procedures for estimation and inference for \u03b2(\u00b7). Inspired by novel ideas from the literature (Yao et al., 2005; Greven et al., 2010; Zipunnikov et al., 2014) , we develop a procedure to estimate \u03b2(\u00b7),\n, and the eigenvalue-eigenvector pairs of \u03a3 bkk (\u00b7, \u00b7),\nand \u03a3 e,G (\u00b7, \u00b7). Compared with the estimation methods of Greven et al. (2010) and Zipunnikov et al. (2014) , our method is an improvement over the ordinary least square methods to estimate \u03b2(\u00b7) by incorporating spatial and/or temporal smoothness in longitudinal functional data. Explicitly, we incorporate the within-subject correlation among T i longitudinal observations to gain statistical efficiency as stated in Theorem 1.\nFrom hereafter, we focus on \u00b5(x ij , \u03b2(s)) = x T ij \u03b2(s), but the proposed estimation procedure can be extended to a nonlinear mean function \u00b5(x ij , \u03b2(s)), which is discussed at the end of Section 2.2. There are four key steps in the estimation procedure as described below.\nStep (I): Calculate an initial estimator\u03b2(s) of \u03b2(s) for each s \u2208 S.\nStep (II): Calculate estimates of the covariance operators \u03a3 bkk (\u00b7, \u00b7) and \u03a3 e,G (\u00b7, \u00b7) and their spectral decompositions, and obtain the estimate of \u03a3 e,L (\u00b7, \u00b7).\nStep (III): Use the estimated covariance operators obtained from Step (II) to improve the estimate in step (I) with a refined estimator of \u03b2(s), denoted by\u03b2(s).\nStep (IV): Obtain individual random effect functions u ij,G (s) = z T ij b i (s) + e ij,G (s).\nStep (I): We employ a local linear smoother (Fan and Gijbels, 1996) to obtain an initial estimator of \u03b2(\u00b7) without incorporating spatial-temporal correlation. Specifically, we apply a Taylor expansion for \u03b2 at s,\nwhere\nrescaled kernel function with bandwidth h. We estimate A(s) by minimizing the following weighted least squares function:\nLet a \u22972 = aa T for any vector a and C \u2297 D be the Kronecker product of two matrices C and D. For an\nLet\u00c2(s) be the minimizer of (2.4). Then 6) where I px is a p x \u00d7 p x identity matrix. In practice, we may select the bandwidth h 1 by using leave-one-curve-out cross-validation. Specifically, we pool the data from all n subjects and select a bandwidth h 1 by minimizing the cross-validation score given by\nwhere\u03b2(s, h 1 ) (\u2212i) is the local linear estimator of \u03b2(s) with the bandwidth h 1 based on data excluding all the observations from the i-th subject.\nStatistica Sinica: Newly accepted Paper (accepted author-version subject to English editing)\nStep ( \nwhere j 1 =j 2 denotes the sum over all j 1 , j 2 = 1, . . . , T i such that j 1 = j 2 .\nThe least squares method in (2.8) has been considered in the literature (Di et al., 2009; Greven et al., 2010; Cederbaum et al., 2016) , where previous authors used penalized splines smoothing instead of local linear regression.\nand\u03a3 LS e (s m , s m ) be the minimizers of (2.8). Then we have\nwhere a 2 = ( Specifically, we estimate \u03a3 bkk (s, s ) and \u03a3 e,G (s, s ) by minimizing the following weighted least squares functions:\nThe bandwidths h 2 and h 3 are selected through the leave-one-curve-out cross-validation method.\nFinally, we perform the spectral decomposition of\u03a3 bkk (s, s ) and\u03a3 e,G (s, s )\nand then calculate\u03a3 e,L (s m , s m ) by usin\u011d\nStep ( \nWe estimate A(s) by minimizing the following weighted least squares function:\nwhere h \u03b2 is a bandwidth.\nLet\u00c3(s) be the minimizer of (2.12). Then, we have\nTo select the bandwidth h \u03b2 , we pool the data from all n subjects and select a Statistica Sinica: Newly accepted Paper (accepted author-version subject to English editing) bandwidth h \u03b2 that minimizes the cross-validation score,\n(2.14)\nwhere\u03b2(s, h \u03b2 ) (\u2212i) is the local linear estimator of \u03b2(s) with the bandwidth h \u03b2 based on data excluding all the observations from the i-th subject.\nStep (IV): We use the local linear regression method to smooth {\u0169 ij (s m ) =\nand then obtain an estimate of u ij,G (s) = z T ij b i (s) + e ij,G (s) for each i and j. Since the local linear regression is a standard method (Fan and Gijbels, 1996; Wand and Jones, 1995) , we omit the detailed steps for the approximation of u ij,G (s). Furthermore, if there is an interest in recovering the subject-specific random effect b i (s), one could use the best linear unbiased predictors, which are commonly employed in linear mixed-effects models, to estimate b i (s) at each point s and then smooth over s.\nRemark 1: To extend the estimation procedure to nonlinear mean functions \u00b5(x ij , \u03b2(s)), such as exponential functions or power functions, one needs to modify steps (I) and (III) by applying a Taylor expansion for \u00b5(x ij , \u03b2(s m )) at s,\nThen, one estimates A(s) by minimizing a nonlinear weighted least squares function:\nIn this general case,\u00c2(s) does not have an explicit form, but it can be estimated by using optimization algorithms, such as the Gaussian Newton algorithm or Levenberg-Marquardt algorithm (Seber and Wild, 1989) . Similar to L n (A(s)), we can modify (2.12) in step (III)."}, {"section_title": "Computational Complexity", "text": "The computational complexity of our estimation procedure is extremely important for high-dimensional neuroimaging data, which usually contain a large number of locations, especially when they correspond to the voxel locations of the image. For instance, M can have a magnitude of tens of thousands. For the linear mean function, the computational complexity of our estimation procedure\n. If we use leave-one-out cross-validation, then the computational effort increases by a factor of n.\nWe first discuss steps (I) and (III). In step (I), we need to calculate the local linear estimator of \u03b2(s m ) at each grid point s m across S 0 = {s m , m = 1, . . . , M }.\nThe computational complexity of step (I) is almost the same as that in standard point-wise linear regression analysis. An alternative is to fit a linear mixed-effect model at each grid point s m using the maximum likelihood. However, this step is not necessary as it only applies to an initial estimate, which then is improved in step (III).\nFor step (III), we only need to calculate the weighted least squares estimators\u03b2(s m ) in (2.13) across s m \u2208 S 0 , which is computationally straightfor-\nTo improve computational efficiency, we standardize all covariates and then use a single tuning parameter h 1 to smooth all the coefficient functions \u03b2 j (s).\nSince this strategy works best for coefficient functions that exhibit similar degrees of smoothness, it may be necessary to use different tuning parameters for different coefficient functions (Fan and Zhang, 2008) \nA major computational hurdle is to calculate \u03a3 b (s, s ) and \u03a3 e,G (s, s ) for all possible (s, s ). If M is relatively large, it can be computationally challenging Zipunnikov et al. (2014) and Xiao et al. (2016) to the estimation of \u03a3 b (s, s ) and \u03a3 e,G (s, s ). These methods include a fast implementation of the sandwich smoother for covariance smoothing and a two-step procedure where one first obtains the singular value decomposition of the data matrix and then smooths the eigenvectors.\nRegarding the computational complexity of step (IV), we note that, similar to step (II), smoothing u ij,P (s) for all possible (i, j) is computationally light. The overall computational complexity is approximately O(nT 0 h s M 2 ), where h s is the bandwidth of the local linear method.\nRemark 2: We discuss two possible extensions of (2.2). The first is to extend the estimation procedure from S = [0, 1] to a D\u2212dimensional compact subset of a Euclidean space. For this, we only need to modify steps (I) and (III) by changin\u0121 \u03b2 l (s) and s m \u2212 s into D \u00d7 1 vectors. The second extension is to assume that e ij 1 ,G (s) and e ij 2 ,G (s) for j 1 = j 2 are dependent and have a separable covariance\nis usually a pre-specified correlation function of unknown parameter \u03b8, such as the exponential correlation model with Fitzmaurice et al., 2004) . However, we found empirically that the use of the correlation function dramatically increases the computational complexity but does not lead to much efficiency gain for the estimation of \u03b2(\u00b7)."}, {"section_title": "Theoretical Results", "text": "We systematically investigate the asymptotic properties of all estimators proposed in Section 2.2 and investigate several inference procedures based on the asymptotic properties. For any smooth function f (s), we use the notation\u1e1f (s) =\nfor q = 1 and 2, and || \u00b7 || 2 for the Euclidean norm.\nStatistica Sinica: Newly accepted Paper (accepted author-version subject to English editing)"}, {"section_title": "Assumptions", "text": "Throughout the paper, the following assumptions are used to facilitate the technical details. Some of the assumptions might be weakened but the current version simplifies the proof.\n(A.1) The grid points in S 0 = {s m , m = 1, . . . , M } are independently and identically distributed with a density function f (s), which has a continuous secondorder derivative and bounded support S. Moreover, for some f l > 0 and f u < \u221e, 2) The covariate vectors x ij = (x ij1 , . . . , x ijpx ) T and z ij = z i (t ij ) = (z ij1 , . . . , z ijpz ) T , may or may not be time-dependent. Nevertheless, we use the notation x ijl = x il (t ij ) for 1 \u2264 l \u2264 p x , and z ijl = z il (t ij ) for 1 \u2264 l \u2264 p z . We assume that \nwhere T 0 is a fixed constant, and h could be h 1 , h \u03b2 , h 2 , and h 3 . is not essential and can be removed if we put restrictions on the tail of K(\u00b7).\nAssumptions (A.4)-(A.5) are smoothness conditions on the coefficient functions, random functions and their covariances. The smoothness condition in assumption (A.5) can be relaxed with substantial additional efforts (Zhu et al., 2012) .\nAssumption (A.6) is a weak condition on n, M and h, where h 1 is the bandwidth used in Step (I) for the initial estimate of \u03b2. Assumptions (A.7) and (A.8) require uniform bounds on certain high-order moments of the random functions, which are standard assumptions in the literature (Zhu et al., 2012; Li and Hsing, 2010) . Assumption (A.10) on simple multiplicity of the first E eigenvalues is only needed to investigate the asymptotic properties of the eigenfunctions. It is also a standard assumption in the literature."}, {"section_title": "Asymptotics of Estimation Procedure", "text": "We state the following theorems, for which detailed proofs can be found in the supplementary document. The first theorem tackles the theoretical properties of {\u03b2(s) : s \u2208 S} obtained from step (III). (i) The asymptotic bias and covariance of\u03b2(s) for s \u2208 (0, 1) are\n(ii) If log M = o(M h \u03b2 ) and there exists \u03b3 n \u2192 \u221e with n 1/2 \u03b3 1\u2212q n = o(1) and n \u22121/2 \u03b3 n log M = o(1) for some q > 2 that satisfies (A.7), then as n \u2192 \u221e, \u221a n{\u03b2(s) \u2212 E(\u03b2(s)|S)} converges weakly to a centered Gaussian process\n. Theorem 1 (i) provides theoretical justification of steps (I)-(III) for the refined estimator\u03b2(s). It has several important implications. First, the estimator \u03b2(s) obtained in step I has asymptotic covariance\n(details can be found in the proof of Theorem 1), which is larger than that of\u03b2(s). The improvement by the refined estimator\u03b2(s) is due to the incorporation of within-subject correlations among T i longitudinal observations, and can lead to substantial efficiency gain in estimating {\u03b2(s) : s \u2208 S}. Second, if\nwe use the maximum likelihood (or the restricted maximum likelihood) estimators at each of the observed data at s m , the asymptotic covariance, given by\n, is larger than that of\u03b2(s m ). The improvement achieved by\u03b2(s m ) is due to incorporating the smoothness in the functional data. Therefore, one can construct more efficient estimators of \u03b2(s) by simultaneously accounting for the smoothness in functional data and the within subject covariance, since these functions are measured repeatedly and longitudinally.\nMoreover, the asymptotic bias of\u03b2(s) is of the order h 2 \u03b2 , which is similar to that of nonparametric regression for independent responses; whereas the asymptotic variance of\u03b2(s) is of the order n \u22121 .\nWe note here that the efficiency gain discussed above is not in conflict with the results in Lin and Carroll (2001) , where they show that the most efficient estimator of the nonparametric function through kernel smoothing is achieved by ignoring the dependence structure among functional observations. In our setting, this means that kernel smoothing in the direction of s should be implemented as we did in Step (I) by ignoring the dependence structure among functional observations. However, in the FMEM setting of longitudinal functional data, it is possible to improve the \u03b2 estimate as we did in Step (III) by incorporating the covariance structure \u03a3 y i ,G (s, s). The analogy here is the standard linear mixedeffects model with just longitudinal data (i.e. no functional components), since FMEM is an extension of linear mixed-effects model. It is clear that in linear mixed-effects model one needs to do weighted least square to gain efficiency for the \u03b2 estimator and this is what we did in Step (III) to refine the \u03b2 estimator through a weighted least square estimator with weights from \u03a3 yi,G (s, s). We emphasize that we could implement Step (III) only after we have obtained a covariance estimate in Step (II), which relies on an initial unweighted least square estimator of \u03b2 in Step (I). This explains why we need three steps to complete the estimation of \u03b2."}, {"section_title": "Theorem 1 (ii) establishes the weak convergence of the centered estimator", "text": "Statistica Sinica: Newly accepted Paper (accepted author-version subject to English editing) \u03b2(s) \u2212 E(\u03b2(s)), which is essential to carry out the statistical inference for \u03b2(s) in Section 3.3 below. Let h = n \u03b1 , M = n \u03b2 and \u03b3 n = n \u03b3 . Anything that satisfies \u03b1 < 0, \u03b1 + \u03b2 > 0 and \u2212 1 2(1\u2212q) < \u03b3 < 1 2 will satisfy the assumptions, where q > 2 is a constant that satisfies the moment condition given in (A.7).\nThe second theorem provides the theoretical analysis of the estimators of and h 3 = O(log n/n) 1/4 , then we have the following results:\nTheorem 2 characterizes the uniform convergence rates of\u03a3 e,G (s, s ) and the associated eigenvalues and eigenfunctions. It can be regarded as an extension of Theorems 3.3-3.6 of Li and Hsing (2010) , which established the strong uniform convergence rates of these estimates under a simpler model."}, {"section_title": "Asymptotics of Inference Procedure", "text": "In this subsection, we derive the asymptotic theory of a global test for testing linear hypotheses of \u03b2(\u00b7) and the theory for simultaneous confidence bands (SCB)\nfor each component of \u03b2(\u00b7). These are key tools for statistical inference for the coefficient functions.\nWe first consider linear hypotheses for \u03b2(s),\nwhere C is a q \u00d7 p x matrix with rank q, and \u03b2 0 (s) is a given q \u00d7 1 vector of functions. We define a global test statistic S n as\nwhere d(s) = C\u03b2(s) \u2212 bias(C\u03b2(s)) \u2212 \u03b2 0 (s). For simplicity and computational efficiency, we do not consider estimating the bias of C\u03b2(s), since it is negligible based on our simulation results reported below. It follows from Theorem 1 that\nwhere \u21d2 denotes weak convergence and G C (\u00b7) is a centered Gaussian process with covariance function {CQ * (s, s)\nwe can derive the asymptotic distribution of S n under the null hypothesis and its asymptotic power under local alternative hypotheses. = o(1) and n \u22121/2 \u03b3 n log M = o(1) for some q > 2 that satisfies (A.7), we have the following results:\n, where \u03c4 is any scalar in [0, 1), S n,\u03b1 is the upper 100\u03b1 percentile of S n under H 0 , and 0 < S ||d(s)|| 2 ds < \u221e.\nTheorem 3 can be regarded as a generalization of theorem 7 of Zhang and Chen (2007) and theorem 2 of Zhang (2011) . The test statistic S n has a weighted \u03c7 2 -type asymptotic distribution under H 0 . Zhang and Chen (2007) (after theorem 7) provided a discussion of the estimation for the null distribution of S n by \u03c7 2 -approximation and bootstrapping, which also applies to the case we considered here. It is easy to see that part (ii) still holds when the critical value S n,\u03b1 is replaced by some estimated critical value.\nNext, we construct simultaneous confidence bands for the coefficient functions, which can then be used for statistical inference for FMEM. For a given confidence level \u03b1, we construct a simultaneous confidence band for each \u03b2 l (s),\n(s) are the lower and upper limits of the SCB. Specifically, a 1 \u2212 \u03b1 simultaneous confidence band for \u03b2 l (s) is:\nStatistica Sinica: Newly accepted Paper (accepted author-version subject to English editing)\nwhere C l (\u03b1) is the critical value of sup s\u2208S |G(s)| associated with\u03b2 l (s) in Theorem 1.\nTo carry out the inference procedure developed above, we approximate both C l (\u03b1) and S n,\u03b1 . Because the asymptotic distribution of S n is quite complicated and it is difficult to directly approximate the percentiles of S n under the null hypothesis, we use a wild bootstrap method to approximate the critical values of S n . The wild bootstrap idea has been used by Zhu et al. (2012) ; details are presented in the Appendix. Let G (q) (\u00b7) be the bootstrapped samples for q = 1, \u00b7 \u00b7 \u00b7 , Q, where Q is the total number of wild bootstrap samples. The following theorem lays the ground for the wild bootstrap method to construct a simultaneous confidence band of \u03b2(s) and to approximate the null distribution of S n .\nTheorem 4. Under assumptions (A.1)-(A.9) and given the data, the bootstrapped process G (q) (s) converges in distribution to G(0, R), which is defined in part (ii) of Theorem 1, as n \u2192 \u221e."}, {"section_title": "Simulation Studies", "text": "In this section, we present four sets of simulations to examine the finitesample performance of the proposed estimation and inference procedures. In the first two simulations, we consider two competing methods, including waveletbased functional mixed models (WFMM) (Morris and Carroll, 2006) and functional additive mixed models (FAMM) (Scheipl et al., 2015) . All computations for these numerical examples were carried out using Windows 7, 3.60GHz quardcore Intel Core i7 CPU and 16GB DDR3 1066MHz memory. One can further reduce the computational time for FMEMs by using other computer languages, such as C++.\nAll simulated data sets were generated from the model:\nwhere\n, and e ij,L (s) \u223c N (0, \u03a3 e,L ) for i = 1, . . . , n. Each subject was observed up to 3 times in this sample, among which 5%, 30% and 65% have only one, two and all three observations, respectively. We set s m = (m \u2212 0.5)/M . The first covariate x ij,1 was simulated from N (0, 1) and fixed across time for subject i and the second covariate x ij,2 was assumed to vary with time, where the increments x ij,2 \u2212 x i(j\u22121),2 were independently sampled from a uniform distribution on [0, 1]. Both covariates were standardized to have zero mean and unit variance. Moreover, we set \u03bb b k = \u03bb e k = 2 1\u2212k for k = 1, 2, and \u03a3 e,L = 0.01. The functional coefficients and eigenfunctions were selected as\nWe fitted FMEM, WFMM, and FAMM to each simulated data set and calculated all the unknown quantities. The average computational times per simulated data set with n = 100 and M = 40 for FMEM, WFMM, and FAMM are, respectively, 19.6 seconds, 2.32 seconds, and 1.15 hours.\nSimulation 1. The first simulation aims at evaluating the performance of the estimates for \u03b2 j (\u00b7). We set n = 100 and M = 40 and 60 and then simulated 1,000 data sets from model (4.1) as described above. Table 1 summarizes the mean integrated absolute error (MIAE) and mean integrated squared error (MISE) of all estimated coefficient functions based on 1,000 simulations. The results in Table 1 indicate satisfactory performance of our estimators since all MIAE\nand MISE values are quite small. As expected, all the errors decrease as the number of grid points increases. Moreover, FMEM outperforms WFMM and FAMM in terms of both MIAE and MISE. However, this comparison may be unfair to WFMM, since it is designed for spiky data, not the intrinsically smooth functional data.\nSimulation 2. The second simulation is to evaluate the accuracy of the estimators of the eigenvalues and eigenfucntions of the covariance functions \u03a3 b (., .), \u03a3 e,G (., .) and \u03a3 e,L . We used the same parameter values as those in Simulation 1. We set c = 0.1 and n = 50 and 100, and generated 1,000 datasets for each Simulation 3. The third simulation is designed to evaluate the type I error rate and power of the global test statistics S n . We are interested in testing H 0 : \u03b2 3 (s) = 0 for all s, against H 1 : \u03b2 3 (s) = 0 for some s. All parameters in FMEM were specified as above except that \u03b2 3 (s) was set as 4cs(1\u2212s)\u22120.4c, where we first set c = 0 to assess the type I error rate of S n and then c = 0.04, 0.06, 0.08, and 0.1 to examine the power of S n at different effect sizes. Furthermore, we set n = 50 and 100 and used 1, 000 replications to estimate the rejection rate of S n .\nThe p-value of S n was approximated by the wild bootstrap method with Q = 500 bootstrap samples. Fig. 4 .5 presents the rejection rates of S n across all effect sizes at the two significance levels \u03b1 = 0.05 and 0.01. Type I error rates are well maintained at the two significance levels for n = 100. Specifically, at \u03b1 = 0.05 (or 0.01), the Type I error rates of S n is 0.066 (or 0.014) for n = 50 and 0.055 (or 0.012) for n = 100, respectively. As expected, the statistical power for rejecting the null hypothesis increases with the sample size, the effect size c and the significance level.\nSimulation 4. The fourth simulation aims at evaluating the coverage probability of the simultaneous confidence bands for \u03b2 j (s). We use the same data generated from Simulation 1 above. Based on the 1,000 simulated data sets, we fitted FMEM, WFMM, and FAMM to each simulated data and then calculated SCB for each component in \u03b2(s). Table 2 presents the empirical coverage probabilities of all three methods for \u03b1 = 0.01 and 0.05. The coverage probabilities improve with the number of grid points M . When M = 60, the coverage probabilities are quite close to the pre-specified confidence levels. Since FAMM only provides level (1 \u2212 \u03b1) confidence interval at each grid point, we use the Bonferroni method to approximate its simultaneous cover probabilities. Again, FMEM outperforms WFMM and FAMM in terms of the coverage probability. However, this comparison may be unfair to WFMM and FAMM, since they do not have any valid method to construct simultaneous confidence bands of \u03b2 j (s) yet. Fig.   4 .6 displays typical 95% and 99% simultaneous confidence bands for coefficient functions \u03b2 l (s), l = 1, 2, 3 based on FMEM as M = 60."}, {"section_title": "Data Analysis", "text": "The data set was taken from the national database for autism research (NDAR) (http: //http://ndar.nih.gov/), an NIH-funded research data repository that aims at accelerating progress in autism spectrum disorders (ASD) research through data sharing, data harmonization, and the reporting of research results.\nA total of 416 MRI scans are selected for 253 normal children (126 males and 127 females) following standard protocol. Table 3 contains demographic information and distribution of scan availability.\nThe diffusion tensor imaging (DTI) data were processed by two key steps including a weighted least squares estimation method (Basser et al., 1994) to construct the diffusion tensors and a pipeline for tract-based spatial statistics (TBSS) (Smith et al., 2006) to register DTIs from multiple subjects to create a mean image and a mean skeleton. Specifically, maps of fractional anisotropy (FA)\nwere computed for all subjects from the DTI after Eddy current correction and automatic brain extraction using FMRIB software library. FA maps were then fed into the TBSS tool, which is also part of the FSL. In the TBSS analysis, the FA data for all subjects were aligned into a common space by a non-linear registration method and the mean FA images were created and thinned to obtain a mean FA skeleton, which represents the centers of all white matter tracts common to the group. Subsequently, each subject's aligned FA data sets were projected onto this skeleton. While several DTI fiber tracts were tracked, we chose to focus in this paper on the corpus callosum (see Fig. 4 .7 (a)) to illustrate the applicability of our method in assessing the effects of covariates of interest, such as patient age and gender. In this case, there are M = 45 grid points along each fiber tract.\nThe FA values were extracted at each grid point across multiple times (1 to 9 times) along the selected fiber tracts for all 253 infants.\nThe goal of the data analysis is to delineate the development of skeleton diffusion properties across time. We fitted FMEM (2.1) and (2.2) with x i = (1, Gender, log(Age), {log(Age)} 2 ) T and z i = (1, log(Age)) T to the selected FA tracts obtained from all 253 subjects. The coefficient functions associated with log(Age) and {log(Age)} 2 were included to detect age effect in FA changes. In addition, as shown in Fig. 4 .7, there are random subject-to-subject variations in FA measures at each grid point along this tract as well as those in the age effect on FA measures. We included random intercept and age effects in the model in order to account for the inter-subject variations.\nWe applied FMEM, WFMM, and FAMM to this data set and estimated all unknown quantities but will only discuss the results based on FMEM below.\nThe results for WFMM and FAMM are provided in the supplementary document.\nThe computational times for FMEM, WFMM, and FAMM are, respectively, 55.8 seconds, 7.9 seconds, and 6.078 hours.\nFor FMEM, the estimated functional coefficients of \u03b2(s) and their 95% simultaneous confidence bands were constructed along with the global test statistic S n to test for the significance of gender and age effects on FA values. The p-value of S n was approximated using the resampling method with Q = 1, 000 replications. Figure 4 .8, the simultaneous confidence band contains the horizontal line crossing (0, 0) for the gender effect, whereas the horizontal line is out of the 95% simultaneous confidence band for the age effect, indicating a significant age effect. This agrees with our analysis results based on S n for the gender and age effects. We obtained the p values of 0.215 and < 0.0001 for the gender and age effects, respectively, indicating significant age but no gender effect. of the total variation are explained by the random functional intercept and the subject-specific random slope, respectively. The within-curve measurement error explains only 5.35% of the total variation. Figure 4 .9 shows the first five and Statistica Sinica: Newly accepted Paper (accepted author-version subject to English editing)\nInstead, we propose using a wild bootstrap method to obtain critical values of S n . The wild bootstrap consists of the following three steps.\nStep 1. Fit (2.1) and (2.2) under the null hypothesis H 0 , which yields\u03b2 * (s m ),\nfor all i, j and m = 1, . . . , M .\nStep 2. Generate a random sample \u03c4 \nThen, based on\u0177 ij (s m ) (q) , we recalculate\u03b2(s) (q) , and\nSubsequently, we compute\nStep 3. Repeat\nStep 2 Q times to obtain {S (q) n : q = 1, . . . , Q} and then\nIf p is smaller than a pre-specified significance level \u03b1, say 0.05, then one rejects the null hypothesis H 0 ."}, {"section_title": "Wild Bootstrap Methods for Simultaneous Confidence Bands of \u03b2(\u00b7)", "text": "Although there are several methods of determining C l (\u03b1) including random field theory (Worsley et al., 2004) , we develop an efficient resampling method to approximate C l (\u03b1) as follows (Kosorok, 2003) .\n\u2022 We calculater i (s m ) = y i (s m ) \u2212 X T i\u03b2 (s m ) for all i, j, and m.\n\u2022 For q = 1, . . . , Q, we independently simulate {\u03c4 (q) i\n: i = 1, . . . , n} from N (0, 1) and calculate a stochastic process G(s) (q) given by\n\u2022 We calculate sup s\u2208[0,1] |e l G(s) (q) | for all q, where e l is a p x \u00d7 1 vector with the l-th element 1 and 0 otherwise, and use their 1 \u2212 \u03b1 empirical percentile to estimate C l (\u03b1). . The magenta, green solid, and red dash-dotted curves are, respectively, the true curves, the estimated functional coefficients and their corresponding 95% and 99% confidence bands. Statistica Sinica: Newly accepted Paper (accepted author-version subject to English editing)"}]