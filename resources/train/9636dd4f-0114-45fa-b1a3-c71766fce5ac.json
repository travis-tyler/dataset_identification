[{"section_title": "Abstract", "text": "Motivated by increasing trends of relating brain images to a clinical outcome of interest, we propose a functional domain selection (FuDoS) method that effectively selects subregions of the brain associated with the outcome. View each individual's brain as a 3D functional object, the statistical aim is to distinguish the region where a regression coefficient \u03b2(t) = 0 from \u03b2(t) = 0, where t denotes spatial location. FuDoS is composed of two stages of estimation. We first segment the brain into several small parts based on the correlation structure. Then, potential subsets are built using the obtained segments and their predictive performance are evaluated to select the best subset, augmented by a stability selection criterion. We conduct extensive simulations both for 1D and 3D functional data, and evaluate its effectiveness in selecting the true subregion. We also investigate predictive ability of the selected stable regions. To find the brain regions related to cognitive ability, FuDoS is applied to the ADNI's PET data. Due to the induced sparseness, the results naturally provide more interpretable information about the relations between the regions and the outcome. Moreover, the selected regions from our analysis show high associations with the expected anatomical brain areas known to have memory-related functions."}, {"section_title": "Introduction", "text": "More than 25 million people in the world today suffer from dementia, mostly caused by Alzheimer's disease (AD). Indeed, the number of individuals affected is expected to significantly rise with a worldwide phenomenon of population ageing, as ageing is the greatest risk factor for the development of AD (Evans et al., 1989; Brookmeyer et al., 1998; Bianchetti and Trabucchi, 2001; Brookmeyer et al., 2011) . Specifically, an estimate shows that in 2050, approximately 80 million people will suffer from AD worldwide. In view of the current prevalence and the projection, the identification and validation of biomarkers for diagnosing AD and other forms of dementia are increasingly important. However, an accurate and early diagnosis of AD is difficult as early symptoms of the disease are shared by a variety of disorders, as reflected by their common neuropathological features (Jacobs et al., 1995; Nestor et al., 2004; Swainson et al., 2001; Humpel, 2011) .\nAD is a severe neurodegenerative disorder of the brain defined by loss of memory and cognitive decline. A probable diagnosis of AD can be established based on clinical criteria, including medical history, physical examination, laboratory tests, neuroimaging techniques and neuropsychological tests (Khachaturian, 1985; Nyg\u00e5rd, 2003; Chong and Sahadevan, 2005; Vemuri et al., 2008; McKhann et al., 2011) . In particular, many studies have shown that neuroimaging techniques can provide invaluable information about AD and are crucial for the early detection of AD (Matsuda, 2007; Ferreira and Busatto, 2011; Petrella et al., 2003; a\u00ebl Chetelat and Baron, 2003; Mosconi et al., 2007) . Moreover, preclinical AD is known to have an association with changes in both cognitive ability and brain images (Caselli et al., 2007 (Caselli et al., , 2009 Twamley et al., 2006) . Thus, finding the association between such measures can be of help for the early detection of AD. For example, Duchesne et al. (2009) discovered a linear relationship between baseline magnetic resonance imaging (MRI) and a decline in cognitive ability after a year of scanning. Taking the association into consideration, when mild cognition impairment (MCI), or AD is diagnosed, one should combine neuroimaging techniques with neuropsychological tests that measure cognitive impairment to track progression of the illness and examine effectiveness of the treatment. To this end, in this article, we introduce a new statistical methodology, intended to select regions of the brain associated with cognitive decline. We further build a predictive model based on the selected region to predict cognitive ability of a new subject using his/her brain scan.\nStructural MRI and metabolic positron emission tomography (PET) are the most clinically used and promising brain imaging techniques to detect abnormalities in individual brains which might be at risk for AD. Fludeoxyglucose (FDG) PET images analyzed in this article were acquired from the Alzheimer's Disease Neuroimaing Initiative (ADNI) database. A more detailed description of the initiative is explained in the supplementary material. Many statistical methods have been introduced to reveal a relation between brain images and a clinical outcome of interest. Univariate methods are intended to build a separate statistical model, either for each voxel, or for each region of interest (ROI). See for example, Herholz et al. (2002) , Worsley et al. (2002) and Lazar (2008) , among many others. In such univariate approaches, where a separate model is fitted for each voxel (or ROI), one must consider an appropriate adjustment to account for multiple comparisons and aggregate the results. Alternatively, in multivariate covariate approaches, every voxel is treated as a single predictor. Since the number of voxels is far larger than the number of images, ordinary least squares for standard linear regression cannot be employed without applying, for example, some regularization or dimension reduction.\nTraditional univariate and multivariate approaches mentioned above share a common drawback: they do not consider important spatial information of the brain. To avoid loss of the information, new approaches incorporating the spatial structure have been introduced. For example, principal component analysis (PCA) can be carried out as dimension reduction, and then the selected principal components of the brain are used for further analysis (Friston et al., 1996; Kerrouche et al., 2006; Caffo et al., 2010) . Also, a wide range of Bayesian methods have been introduced (Bowman et al., 2008; Kang et al., 2011; Zhang et al., 2014) . In these Bayesian approaches, complex spatial correlation between voxels is modeled using appropriate prior distributions. More recently, the functional data analysis (FDA) framework has gained notable attention in the analysis of brain images. Functional data refers to the data whose visual representations are in functional forms, such as curves or images (Ramsay and Silverman, 2005; Ferraty and View, 2006) . Reiss and Ogden (2010) used a functional version of PCA accounting for spatial features of the brain. Motivated by brain imaging studies on cognitive impairment in elderly subjects, Wang et al. (2014) proposed 3D regularized functional regression, which accounts for the spatial information among neighboring voxels via Haar wavelets. Reiss et al. (2015) introduced and compared several wavelet based predictive models to assess whether brain imaging data can predict presence or absence of attention deficit hyperactivity disorder (ADHD). Motivated by the complex structure of multidimensional brain image data, Zhou et al. (2013) formulated a general tensor-based regression framework and applied the method to the ADHD data.\nIn this article, we take the functional view point and consider each individual's brain image as a 3D functional object, denoted by X i (t), i = 1, ..., n, where t refers to spatial location. Suppose E(Y ) = X(t)\u03b2(t)dt, where Y is a clinical outcome of interest and \u03b2(t) is a coefficient function or image. \u03b2(t) can be thought of as a weight function, in that some parts of \u03b2(t) with relatively large weights have a large influence on Y , while other parts with weights that are close to zero have nearly no impact. The brain is an complex object, consisting of a huge collection of small parts. Each of the parts has its own specific function, and very often they work together constructing a complicated relationship with each other. As functions of the brain are fairly regional, it would be natural to assume that only few parts of the brain are associated with a particular clinical outcome, and our goal is to find these specific subregions. However, most conventional estimation methods for \u03b2(t) outlined in Section 2.1 do not allow \u03b2(t) = 0 for some t, so identifying subregions where \u03b2(t) = 0 is not possible. We propose a new statistical methodology which, we call stable and predictive functional domain selection (FuDoS), which can effectively differentiate areas where \u03b2(t) = 0 from \u03b2(t) = 0. Due to the sparse representation, the estimation result can naturally provide more interpretable information about the influence of such region of \u03b2(t) = 0. This information cannot be extracted from standard functional regression approaches, where the estimated \u03b2(t) is continuous across the domain, and the effects at specific areas are ignored.\nThe article is outlined as follows. In Section 2.1, we review some existing literature on the estimation of \u03b2(t), including functional linear models and point of impact models. For ease of presentation, we explain and detail the proposed methodology with 1D functional data, then extend it to the 3D case in Section 2.7. The two estimation stages, segmentation and selection are explained in Section 2.3 and Section 2.4, respectively. A stable representation of the subdomain is determined in Section 2.5. Predictive models are built using the selected stable subregions in Section 2.6. In Section 3, simulations for 3D functional data are given and finally the proposed method is applied to the analysis of ADNI's PET data. Section 4 is the conclusion. In the supplementary material, we implement extensive simulations for 1D functional data, and illustrate the proposed method using the 1D gasoline data set.\n2 Stable and predictive functional domain selection (FuDoS)\n2.1 Review on the estimation of \u03b2(t)\nWe review some of the literature on functional regression, where the response Y is scalar and the predictor X is functional. Existing literature has mainly focused on linear models, where Y is associated with X through E(Y ) = X(t)\u03b2(t)dt. In practice, we only observe samples of a finite size, but the estimation target is intrinsically infinite dimensional which can give rise to identifiability issues. To deal with the problem, restrictions are often imposed on \u03b2(t). Such restrictions generally involves a basis expansion with regularization, or penalization. In penalized B-splines approaches (Marx and Eilers, 1999; Cardot et al., 2003) , a penalty is often associated with a measure of the roughness, \u03b2 (d) (t) 2 dt, where \u03b2 (d) (t) denotes the dth order derivative, and d = 2 is the most common choice. To account for sparsity and achieve variable selection, Zhao et al. (2012) developed a wavelet-based lasso (WLasso) estimator, in which the subspace of \u03b2(t) is restricted to the span of wavelets, and the wavelet coefficients are estimated via the lasso approach. It can result in sparse \u03b2(t), but does not allow \u03b2(t) to be exactly zero.\nMost conventional estimation methods for \u03b2(t), including those methods mentioned above, are not be able to identify regions of \u03b2(t) = 0. Moreover, these methods often produce wiggly estimates that are hard to interpret. To aid interpretation, James et al. (2009) developed a new methodology, called functional linear regression that's interpretable (FLiRTI). In this approach, first, one digitizes the domain into a fine set of points, and determine whether the dth derivatives of \u03b2(t), d = 1, 2.., is zero or not at each point. The method is flexible in terms of the shape of \u03b2(t). Also it can produce a highly interpretable estimate.\nWe also review some methods that concern the identification of points of impact. Ferraty et al. (2010) proposed a method to detect predictive points of a predictor. The method is built around a stepwise forward algorithm, which selects a sequence of points giving the best prediction for Y . Although the algorithm works reasonably well, it does not explicitly account for the functional nature, treating each point as a separate predictor. Another possible limitation would be that it is unable to explore all possible combinations of points due to the nested nature of the algorithm. To deal with the second limitation, Ferraty and Hall (2015) advanced the forward selection and proposed a new methodology, called nonparametric variable selection (NOVAS). It enlarges the class of possible combinations of predictors while keeping the computational cost in a reasonable range. It is based on nonparametric regression, so it can take interaction effects between variables into account. To our knowledge, their algorithm has not been extended to the FDA context, and in this article we aim to investigate the extension.\nA different approach for point of impact was introduced by McKeague and Sen (2010) . In their approach, X is assumed to be a fractional Brownian motion with a Hurst parameter H, and the sensitive point, say t * , is estimated based on a least squares approach. It is shown that the least-squares estimators of the points are consistent, and the convergence rate rises as X gets more ragged, i.e., as H gets smaller. Kneip et al. (2016) generalized the point impact model, which incorporating both global and local effects. The key assumption ensuring the identifiability of those two effects is that the process exhibits specific local variation. This implies that at least some part of X in a small neighbourhood of t * is essentially uncorrelated with the remainder of the trajectories outside of the interval, where t * denotes a true impact point. It is emphasized that the identifiability of the model does not impose any restriction on the degree-of-smoothness of X(t). However, it is clear that detection of points of impact will benefit from a highly local variability that generally goes along with the non-smoothness."}, {"section_title": "FuDoS model representation", "text": "Denote Y a centred scalar response, and X a centred functional predictor, which can be thought of as a function X(t), t \u2208 [0, T ], where t here refers to time or location. For the 3D case that we study later, t becomes a 3D coordinate (h, v, z) . Suppose that only few parts of X have an association with Y , and our aim is to find the regions where \u03b2(t) = 0. Let X [l] = {X(t) : t \u2208 (s l\u22121 , s l ]} be the lth segment of X defined by two boundary points s l\u22121 and s l , with s 0 = 0 and s L = T , where l = 1, 2, ..., L refers to a segment index. Given a set of boundary points {s l } L l=0 , we seek the best subset of segments, denoted by X J * , where J * = [\u03ba 1 , \u03ba 2 , ..., \u03ba K ] is a collection of segment indices of the best subset. Denote \u03b2 [l] a piece of \u03b2(t), corresponding to X [l] , that is, \u03b2 [l] = {\u03b2(t), t \u2208 [s l\u22121 , s l )}. Given the segments, FuDoS model is formally written as\nwhere f 1 , f 2 = f 1 (t)f 2 (t)dt, and is an i.i.d observational error, with mean zero and variance of \u03c3 2 . We can generalize the above model to binary or count data using a link function, but in this article we only focus on the case where Y is continuous. Typically, the size of the best subset, K, is far smaller than the total number of segments L, so the model can serve as a functional version of variable selection, so we call functional domain selection."}, {"section_title": "Segmentation of X", "text": "In some cases, segments can be naturally given. In most cases, however, they are unknown, and we must estimate them. When correlation of X is high, the adjacent points are likely to have similar values, consequently, they can result in similar predictive performance. Additionally, the presence of high correlation can deteriorate selection algorithms, by selecting an incorrect subset. This tendency becomes more severe when samples of Y are corrupted by a sizeable error, or a regression coefficient is sufficiently smooth. This issue is also addressed in the context of points of impact. For instance, McKeague and Sen (2010) state that the convergence rate of the estimators of points of impact decreases when X gets smoother. So the existence of high local variability can be helpful when trying to find points of impact (Kneip et al., 2016) . Taking empirical and the two theoretical results into consideration, we divide X into several parts based on the correlation.\nOur segmentation procedure is sequential. Denote C(t, t ) an absolute value of correlation between X(t) and X(t ), that is,\nStep 1 of the segmentation, we select the boundary point s \u2208 [0, T ] that minimizes a loss function as\nwhere w u1,u2 is the size of the corresponding segment in %, i.e., w u1,u2 = 100 \u00d7 |u 2 \u2212 u 1 |/T . Here, the inverse 1/w .,. serves as a normalizing constant. Denote the solution to (2) by s * 1 , where the lower index indicates the step number of the procedure, and denote the corresponding minimized error by U (s *\nStep 2, we find the location that optimizes either\nor min s * 1 \u2264s\nSimilarly, we write its solution as s * 2 and the corresponding minimized error by U * 2 . A loss function of the subsequent steps is built in similar fashion: a new integral term is progressively added to the preceding loss function. We report some simulated segmentation results in the supplementary material. The algorithm seems to perform reasonably well as it splits region with low correlation, while keeping region with high correlation intact.\nThe segmentation procedure is essentially equivalent to approximating the complete correlation by its subdiagonal parts. Without a normalizing constant, i.e., setting w .,. = 1, the above loss function amounts to squared sum of off-subdiagonal parts of C. In such case, the minimized approximation error U * j where j is the step number, would always get larger as the algorithm progresses. On the other hand, the normalizing constant can have an opposite effect as it penalizes a segment with large size. Typically, the minimized approximation error path U * j is convex. When the error path begins to rise, we terminate the segmentation procedure. However, when X is extremely rough, the path can constantly decrease, so the algorithm will produce a too complex segmentation. To regulate complexity of the segmentation, we introduce a penalty, as an increasing function of number of segments, to the loss function. This produces the following penalized loss function\u0168 *\nwhere \u03c1 is a tuning parameter that controls complexity of the segmentation, and\u0168 * j is the approximation error penalized by the number of segments L j at jth step. We employ a subsampling scheme for selecting the amount of segmentation, as detailed in Section 2.5.\nThe underlying assumption imposed on the proposed segmentation procedure is that the correlation has a bandable-shape, i.e., entries of the correlation decay as they move away from diagonal. Such a correlation structure naturally arises in a wide range of cases, including temporal or spatial data, the two most common forms of functional data. When the shape of the correlation largely deviates from the standard bandable-shape, e.g., functional data with a periodic pattern, the above procedure may not produce a sensible segmentation as non-adjacent segments actually have higher correlation. In this case, one may alter the above to incorporate these types of structure, allowing the combination of correlated segments that are far apart."}, {"section_title": "Identifying predictive subdomain of X", "text": "Having obtained the segments, we now seek the most predictive subset of segments. Denote J a subset of segment indices r = 1, ..., L, so that X J means the subdomain of X associated with segments included in J = {l 1 , ..., l J }. In other words, it is a collection of segments, i.e., X J = \u222a J j=1 X [lj ] , with X [lj ] as defined earlier in Section 2.2. In each step of the selection procedure, we build a sequence of distinct potential subsets, denoted by J 1 , ..., J R , for different values of R, and perform prediction of Y based on each subset. Once we obtain a measure of the predictive performance for each subset using a 5-fold cross-validated (CV) error, the subsets are ordered in an ascending order of the CV error. We denote the ranked subsets by J (1), J (2)..., J (R), and use them to construct a new sequence of subsets for the subsequent step. For instance, if the 5th segment has the smallest CV error, followed by 10, 4, 12..., then J 1 (1) = {5}, J 1 (2) = {10}, J 1 (3) = {4} and J 1 (4) = {12}..., where the upper index indicates the algorithm step number, and the number in round brackets means the rank. In Step 2, we merge the ordered sequence in a pairwise manner to create a new sequence of potential subsets, leading to J 1 = {5, 10}, J 2 = {4, 5}, J 3 = {5, 12},...J r1 = {4, 10}.... Similar to Step 1, after performing the prediction based on each subset, J 1 , ..., J R , the sets are ranked according to their predictive ability, producing\n.., a new sequence of subsets for Step 3 are J 1 = {2, 5, 7} and J 2 = {1, 2, 5, 9}...., and so on. In Step 3, our procedure produces subsets of size 3 or 4. Again, we perform the prediction based on each subset, J 1 , ..., J R , and rank the sets based on their predictive performance. We continue the selection until we attain the best subset, and terminate the algorithm when the subsequent minimal CV error does not satisfy a rule, as given in (7).\nNaive use of the above sequential algorithm can raise a computational concern. Specifically, in\nStep 1, we search the most predictive single segment over R = L sets, where L is the total number of segments. In\nStep 2, we select the best combination of two segments, so we have to explore R number of subsets, where\nFrom a computational view point, it is undesirable because even a moderate value of L can produce a large R, e.g., L = 20 leads to R = 190. Inspired by Ferraty and Hall (2015) , we reduce computational labour by keeping only the first top \u221a q subsets when building the sets for the subsequent step, where q is an unknown value depending on the capability of the computational resources. If a set of segments does not seem to be useful on its own, ranked as top\n.., it is less likely to be useful when working with other sets. In practice, the functional variable X is never continuously observed over the whole interval [0, T ], but at a grid of measurements of size p. Unlike the NOVAS method (Ferraty and Hall, 2015) , we somehow already reduce the computational burden by focusing our attention on the previous L subdomains of X coming from our segmentation step (instead of the p digitized points). When we used q = L for 1D functional data, the result was not satisfactory since L is already quite small. In our 1D numerical study presented in the supplementary material, L is not larger than 20, and p is only a few hundred. Based on our empirical experience, we suggest q = p/2 for 1D functional data, as rule of thumb. On the other hand, in the ADNI's PET brain image analysis that we study in Section 3, the involved number of voxels is approximately 2.5 million, so we use q = L/2. We note that in our brain image analysis,L ranges between 1290 and 5380. Using the suggested cut-off rule as above, we reduce the computational time without affecting the performance of the proposed method.\nAs we mentioned earlier, in each step of the selection algorithm, we rank the subsets based on their predictive ability, and for its quantification, we use a 5-fold CV. Specifically, we randomly divide n pairs of samples (X i , Y i ) drawn from (X, Y ) into 5 roughly equal parts. Holding out each sample fold for use as a validating set, we train the model with the remaining samples, yielding\u011d \u2212j (X Jr ), where\u011d \u2212j (X Jr ) is the leave-one-out estimator exclusive of the jth sample fold based on the subdomain X Jr , and we use a linear form for g as explained in the subsequent paragraph. Using\u011d \u2212j (X Jr ), we perform prediction of Y i , i \u2208 I j , where I j is the jth part of sample indices. Repeating this procedure for each j = 1, ..., 5, we compute the mean squared prediction error of the rth subset as\nwhere n j is the number of samples in I j . Once we obtain CV error for each potential subset, the sets are ordered in an ascending order of CV error as CV(\n, where k is the step number. As explained, the ordered subsets are merged in a pairwise manner and a new sequence of subsets is built for the next step. The selection algorithm is terminated at Kth step when the successive minimum CV error satisfies the following criterion\nwhere CV * K is the minimum value of CV error at the Kth selection step, that is, CV * K = CV(J K (1)), where J K (1) denotes the most predictive subset at Kth selection step. The unknown tuning parameter c controls the degree of selection. When c is large, the algorithm will stop early, leaving out potentially relevant segments. If c is small, by contrast, the selected subset can include false segments as a result of over-fitting. We provide detailed discussion on the selection of c in Section 2.5.\nTo fit the regression function g in (6), we use a linear model as given in (1). The form of \u03b2 [l] is unknown, and is dependent upon applications. To gain flexibility, one may use a spline basis expansion with penalization (Marx and Eilers, 1999; Cardot et al., 2003) . Such penalized basis approach can provide more flexible control over the shape of \u03b2, with the shape being determined by data. For instance, when the relation between X and Y is linear over the subset X [l] ,\u03b2 [l] will reflect the relation by choosing a large value for the smoothing parameter as found in our gasoline example, see Figure 8 in the supplementary material. Using a complex form for fitting \u03b2 [l] would not cause a serious problem, when the sample size is large enough. However, when it is small, compared to the dimension of the data, and brain image data is a typical example of such data set, a complex form is likely to result in over-fitting, and hence unstable estimation and poor prediction. Indeed, results in our 1D simulation study in Table 6 and Table 7 the supplementary material reveal that when n << p, using penalized splines methods for fitting for each \u03b2 [l] can lead to over-fitting as indicated by large values of prediction error."}, {"section_title": "Selecting the stable subdomain via subsampling", "text": "The proposed method involves two tuning parameters: 1) \u03c1 regulates the complexity of the segmentation; and 2) c determines the point of termination of the selection procedure. They are interrelated and have a joint effect on the amount of selection. When selecting the best subset, the major concern is to determine whether there exists a pair (\u03c1, c) that identifies the true subset with high probability, and the aim is to choose such a pair. Data-driven methods such as cross-validated approaches may provide the simplest tool for the selection. However, the best model chosen by cross-validation in the lasso, for instance, tends to include too many variables (Meinshausen and B\u00fchlmann, 2006; Leng et al., 2006) , and in our analysis of brain data we observed that the selection can be specific to a dataset. We attempt to avoid the situation by combining our selection algorithm with a generic subsampling scheme. Specifically, instead of choosing a single set of tuning parameters to determine the best subset, we perturb the data many times and select regions that appear in selected subsets with high probability. The spirit is that the stable subdomain should be consistently identified on similar sets of data.\nMotivated by Meinshausen and B\u00fchlmann (2010) , we define the selection probability of each subdomain and the stable subdomain as follows. The selection probability of any subdomain X \u2286 X is the probability of being in X \u03c1,c J * , where X \u03c1,c J * is the subdomain of X associated with the selected segment index set J * , given \u03c1 and c. Recall that we introduced two stages of estimation for X \u03c1,c J * in previous subsections. Let S be a random subsample of {1, ..., n} of size n/2 , drawn without replacement, and we use the subsample to obtain X \u03c1,c J * . Here, the selected set is implicitly a function of S, so we incorporate this dependence by writing X \u03c1,c J * (S). As introduced in Section 2.4, our selection algorithm is based on minimizing a 5-fold cross validation, which displays an additional source of randomness to the selected set. So, we write X \u03c1,c J * (S, I), where I is a 5-fold random split of the subsample S. Mathematically, the selection probability of the subdomain X given \u03c1 and c is defined as \u03a6 \u03c1,c\nwhere the probability P is in terms of two sources of randomness S and I. The estimate of the above probability can be naturally estimated by repeating the subsampling procedure a large number of times, and computing the relative frequency for X \u2286 X \u03c1,c\nX , we define stable subdomain as\nwhere \u03c0 is an user-defined cut-off probability and where \u03c1 (resp. c) belongs to some given grid of values. Having used the stability selection procedure, the problem has shifted from the choice of \u03c1 and c to the choice of \u03c0 and a two-dimensional grid B for both tuning parameters \u03c1 and c. Choosing the optimal pair of (\u03c1, c) is an extremely difficult problem in high dimensional settings, while subsampling can provide a more straightforward and general framework for the problem as choosing fewer subregions or increasing \u03c0 will reduce the expected rate of falsely selected subregion. A major advantage of stability selection would be that the choice of B does not have a large effect on the result, as long as it is varied within reasonable limits (Meinshausen and B\u00fchlmann, 2010) . We find a similar effect as discussed below. Given \u03c1, decreasing c tends to select the subset in an incremental manner, i.e., X \u03c1,c1\nJ * , for c 2 \u2264 c 1 . Of course, as our selection procedure is not nested in nature, the above incremental relation would not hold theoretically, but we found that when c is fairly small, say c \u2264 0.05, the above relation tends to be satisfied. Considering this issue, so-called pointwise control (Meinshausen and B\u00fchlmann, 2010) , we only consider a single value for c, i.e., c = 0.01, in such a way that some over-fitting occurs, so each selected subset X \u03c1,c J * would contain the true subset with high probability. Unlike c, as \u03c1 does not explicitly exhibit the above incremental relation, we consider the minimum and maximum number of segments, and vary values for \u03c1, in a way thatL (the estimated number of segments) changes within this range.\nWe now give guideline on \u03c0. Unlike B, the choice of \u03c0 is more directly related to the selection results. As decreasing \u03c0 generally increases the size ofX \u03c0 stable , it is likely to include the true subset with high probability. Choosing a small \u03c0 however would increase the expected rate of falsely selected subregions. One possible way of choosing \u03c0 would be evaluating predictive performance of eachX \u03c0 stable , and selecting \u03c0 that yields the smallest prediction error. Another possibility would be monitoring the maximized selection probability for each subdomain X , and searching if there is any clear threshold for the choice. For instance, in the analysis of gasoline data in the supplementary material, we find that two subregions clearly stand out with selection probability higher than 0.8, see a red line in Figure S8 (a). A sensible choice between the two possible approaches would depend on the aim of analysis. We find that the best \u03c0, in the sense that it attains the best predictive result, tends to get smaller as the sample size n gets smaller, or the size of observational error on Y becomes larger. These two quantities interplay, but n seems to have a stronger effect. We shall give more detailed discussion on the selection of \u03c0 in Section 3 and in the supplementary material.\nStability selection has a very attractive theoretical property that, under some assumptions and model settings, a certain bound on the expected number of false selections is guaranteed (Meinshausen and B\u00fchlmann, 2010) . In this article, we do not investigate its theoretical properties, rather we only highlight the two practical advantages offered by the scheme. First, it increases the selection probability at boundaries of the true segments. Due to the nature of the proposed method, where segments are predetermined without considering its relation to Y , the selection probability at the true boundaries can be low, if the estimated segments do not coincide with the true segments. Using stability selection can add flexibility to the segmentation procedure, so that the estimated boundaries can move around at each repetition of subsampling. Our empirical results show that this procedure indeed overcomes the boundary issue. The second benefit is that it offers a nice tool for stabilizing the selected subset, reducing the rate of falsely selected subregion without compromising the predictive power. Our simulation study presented in the supplementary material reveals that FuDoS can yield comparatively good prediction performance for a range of \u03c0. Additionally, the rate of falsely selected subregion seems to be reasonably low. We shall emphasize the second advantage in Section 3 as well as in the supplementary material."}, {"section_title": "Building predictive models based on stable subdomain", "text": "One of the most important and popular goals in brain image studies is prediction of a disease, or a clinical outcome using brain image data. For this goal, we attempt to develop predictive models based on selected stable subregions as explained below. Assume that we have a sequence of the selected stable subdomains for different values of \u03c0, and X \u03c01 stable \u2286 X \u03c02 stable , for \u03c0 2 \u2264 \u03c0 1 . And let T \u03c0 stable be the domain on which X \u03c0 stable is defined. Then, for each \u03c0, a predictive model is built\nThroughout the paper, we consider linear models for fitting g, so (10) becomes\nwhere \u03b2 X \u03c0 stable is a regression coefficient with flat region, i.e., \u03b2 X \u03c0 stable = 0, for t / \u2208 T \u03c0 stable , and \u03b2 X \u03c0 stable = 0, for t \u2208 T \u03c0 stable . Under this setting, the fitted curve, or image of \u03b2 is zero over the region where t / \u2208 T \u03c0 stable . To fit \u03b2 X \u03c0 stable , the same model as used to find X \u03c0 stable is considered. For instance, when a penalized splines fitting criterion is used as in our 1D numerical study, a equi-spaced sequence of knots is placed over the subregion T \u03c0 stable , and the roughness of \u03b2(t) is controlled by a smoothing parameter. While when piecewise constant basis is used as in our 3D numerical study, one must determine the size of each piece, which amounts to dividing T \u03c0 stable into several parts, over each part a constant function is fitted. We divide T \u03c0 stable into several pieces using a density based clustering algorithm for spatial data (Ester et al., 1996; Sander et al., 1998) . The algorithm groups together points that are closely located, and marks points as outliers, when they locate alone in low density regions. However, as our aim here is not to identify outlying points among the selected points, but split them into several groups, merely based on their locations, we consider the outlying points forming groups with low density. Under our selection framework, although the density is low, the outlying points (whose nearest neighbors are far apart) in the selected stable subset would have predictive power with high probability, and their mean effects on Y would be quite different from a big cluster of points, if they are far apart. The density based clustering algorithm provides appropriate tools for our problem. First, it does not require one to specify the number of clusters a priori, as opposed to K-means clustering (Hartigan and Wong, 1979) . Moreover, the algorithm works well when the shape of clusters is arbitrary. It is efficient to implement and almost deterministic. For the implementation, we used the R-function dbscan in R-package dbscan (Hahsler et al., 2015) . As explained, when prediction of Y is the purpose of analysis, \u03c0 can be selected by evaluating the predictive performance of each M \u03c0 , and selecting \u03c0 that yields the smallest prediction error. We find that the predictive performance varies little for a range of \u03c0. We give more detailed discussion on the selection in Section 3 and in the supplementary material."}, {"section_title": "Extensions to 3D", "text": "We treat each individual's brain image as a 3D functional object and therefore extend the proposed methodology to 3D functional data. Essentially, for the selection, the same algorithm is used, regardless of the dimensionality. However, segmenting a multi-dimensional functional object is more complicated and computationally challenging. Unlike the 1D case, where segments are given as non-overlapping intervals (and are in some sense totally ordered), segments of the brain are non-overlapping 3D volumes in various shapes. To simplify the problem we assume that the complete 6-dimensional covariance function of the brain admits a separable form. The separability largely reduces the set space, over which we explore to find the optimal segmentation, leading to a significant increase in computational speed. For example, without the separability, the dimension of the set space that has to be explored in the ADNI's PET data is approximately 2.5 million (\u2248 160 \u00d7 160 \u00d7 96). While when the separability is assumed, the size of the set space shrinks approximately to 410 (\u2248 160 + 160 + 96). Of course, we gain computational efficiency at a price of precision. The use of separable functions for brain image data is not new, we refer the reader for instance to Aston et al. (2012) .\nWe now detail the separability. Denote X i (h, v, z) the ith sample of X(h, v, z), where h \u2208 H, v \u2208 V and z \u2208 Z represent voxel location in the brain, with H, V and Z being compact sets. We can translate the 3D brain into a 1D functional object as X(t) \u2261 X(h, v, z), where t \u2261 (h, v, z). We assume that X is centred and denote the spatial full covariance function of v, z X h , v , z ] . Suppose that G has the following separable form\nwhere\nTo obtain G H , G V and G Z we follow the calculation, as used in Aston et al. (2012) . To obtain G H , we integrate out G with respect to V and Z as\nand its estimate\u011c H (h, h ) is computed by replacing G h, v, z , h , v , z with its sample alternative a\u015d\nand then\u011c H (h, h ) found by marginalising over v, z. Once\u011c H ,\u011c V and\u011c Z are computed using the above forms, we perform the segmentation for each direction of H, V and Z using the procedure as introduced in Section 2.3. This requires choosing \u03c1 for each direction, so now we have to choose a 3-dimensional grid\nOnce we obtain boundary points for each direction of the coordinate, we create 3D segments in the following way. Assume that we obtain a sequence of boundary points in H-direction as\nThe segments produced in this way will have a cuboid form. The brain has folded appearance and is round in shape. So the issue of approximation error can arise when trying to divide it into 3D cuboids. To avoid this issue, we set the size of each segment of the brain fairly small. As long as an element of brain images, e.g., voxel, displays high resolution, approximation error caused by separability would be minimal."}, {"section_title": "Computational issues", "text": "We save computational cost of the segmentation procedure via separability. Further, as addressed in Section 2.4, we reduce the cost by adopting and modifying the idea as used in NOVAS (Ferraty and Hall, 2015) . Specifically, unlike NOVAS, where q = p is assumed, we build potential subsets based on segments, so it would be more natural to set q = L, where L is the total number of segments. In this way, only O(\nnumber of potential subsets are explored in each step of the selection procedure. In our analysis of brain image data, however, the estimated L is a few of thousands, and so O(L) is still quite large for the upper bound to the capability of computational resources. Instead, we use q = L/2 and save the cost even further. For instance, when L = 4000, with q = L, the number of explored subsets at each step of the selection is 64, while with q = L/2, it is 45. The computational gain does not seem very large for single run of our subsampling scheme, however, as we repeat the selection procedure on subsamples 100 times, and each repetition involves |B| number of estimation where B is the considered set of tuning parameters, we can save 100 \u00d7 |B| \u00d7 (64 \u2212 45) number of computational operations. Our method is computationally intensive. So we speed up computation by parallelization of the procedure. For instance, using a desktop with a 4-core, 3.4 GHz processor with 16GB RAM, the run time for the analysis of ADNI's FDG PET data presented in Section 3.2 was less than 19 hours, where the total number of voxels involved was 1,408,000."}, {"section_title": "3D numerical study", "text": "We have conducted simulations with 3D functional data, but unlike the 1D case in the supplementary material, comparison with other methods is not made as they were developed only in the context of 1D functional data, and are not easily extended. We also apply the FuDoS methodology to the ADNI's PET brain image data."}, {"section_title": "3D simulation", "text": "To realistically imitate brain images, we generate datasets based on ADNI's PET brain images. The details of acquisition and preprocessing of the images will be given in the supplementary material.\n\u2022 The original ADNI's PET brain image, X i (h, v, z), i = 1, ..., n t , with n t = 1403, lower index t here means total, displays a grid of size (160 \u00d7 160 \u00d7 96). To facilitate the computational time, we reduce the size to (120 \u00d7 120 \u00d7 10), taking axial slices located at z = 51, ..., 60 (focusing on central part of the brain) in the coordinate space, and eliminating some voxels outside of the brain.\n\u2022 We define the coefficient image as piecewise constant:\nwhere (h, v, z) means voxel location in the brain. Figure 1 illustrates the true \u03b2(h, v, z) overlaid on a randomly chosen individual's PET brain image. The number on top of each plot is z = 2(z \u2212 48), where z is an axial slice number of the brain.\n\u2022 Based on X i (h, v, z) and \u03b2(h, v, z), we generate\nwhere i \u223c N (0, \u03c3 2 ), with \u03c3 2 controlled by signal-to-noise ratio (SN R), i.e., SN R = var(\nWe consider whether the FuDoS methodology is able to identify true subdomain of X that have an association with Y under various levels of error and sample size. To address this, we consider four values of SN R = {2.5, 5, 10, 20} and two values of n = {200, 1000}, producing eight simulations in total. In each case, n brain images (either 200 or 1000) are randomly drawn from the total n t = 1403 brain images, and Y i are generated using the above form with different values of SN R. For a random subsample of the data with size n/2 , we select the most predictive subset for each pair of (\u03c1, c) \u2208 B, returning X \u03c1,c\nand c \u2208 C, we employ A h = A v = A z = {0.01, 0.03} and C = {0.01}, so in total 8 pairs of (\u03c1, c) are involved. The subsampling procedure is repeated 100 times to estimate the selection probability as in (9), and the stable subdomain, n=1000;SNR=20 n=200;SNR=20 n=1000;SNR=10 n=200;SNR=10 n=1000;SNR=5 n=200;SNR=5 n=1000;SNR=2.5 n=200;SNR=2.5 see (8), is determined when the maximum selection exceeds the cut-off \u03c0. For fitting \u03b2 [l] , see (1), a piecewise constant basis is used. We evaluate the selection performance using P 1 and P 2 as follows. Denote the true segment, where \u03b2(t) = 0, by X * , and write the estimated stable subdomain byX \u03c0 stable . Denoting |A| the size of any set A, we compute\nThe size ofX \u03c0 stable tends to get larger as \u03c0 decreases, so that the selected stable subregion would include the true set with high probability as \u03c0 becomes smaller. When trying to recover the true set X * , a natural goal would be to include as few false segments as possible. To penalize the rate of falsely identified subregion, we also measure\nThe predictive ability of each method is also investigated. We sample n v = 200 pairs of test observations, where the lower index v means validation, except for the samples included in the training set, carry out prediction and calculate the root mean squared error (RMSE) on the test samples, defined as (\n, where\u0176 i is the predicted value of ith test sample, Y i . As explained in Section 2.6, we build a predictive model based on X \u03c0 stable for each \u03c0, so prediction RMSE is measured for each \u03c0. Each simulation is iterated 100 times and the average and the standard deviation of P 1 , P 2 and RMSE are reported in Table 1 . The minimum of RMSE and the maximum of P 1 and P 2 in each case of simulations are written in bold. When n and SN R get smaller, it becomes more difficult to indentify the true set with high probability. Therefore, one should select a small value for \u03c0 to guarantee that the selected stable subregion encompasses the true set. Indeed, the best range of \u03c0 achieving fairly small values of RMSE is 0.35 < \u03c0 < 0.65, and it tends to decrease as n and SN R become smaller. In the supplementary material, it is seen that the optimal range of \u03c0 in the 1D functional case was higher, i.e., it was 0.55 < \u03c0 < 0.85. This is not surprising as the problem of n << p is more severe in case of 3D functional data, i.e., p = 128 and n = (50, 800) versus p = 144, 000 and n = (200, 1000). The average of P 1 and P 2 for different values of \u03c0 is plotted in Figure 2 . The value of P 1 approaches to 1 as \u03c0 approaches to 0. Both n and SN R have an impact on the selection result, but the effect of n seems to be stronger. An estimated stable subdomain with \u03c0 = 0.4 for a simulation in each case of simulation setting is shown in Figure 3 . The colour indicates maximum value of selection probability."}, {"section_title": "Analysis of ADNI's FDG PET", "text": "The ADNI's PET data used in this analysis consists of n = 1302 individuals, including participants from all of the ADNI's study phases: 402 individuals from ADNI-1, 127 from ADNI-GO and 773 from ADNI-2. Recall that a total number of available ADNI's PET brain images used in our 3D simulation study was n t = 1403, as it included brain images from the same subject acquired at different time of visit. More detailed demographic features of the involved individuals are summarized in Table 2 . Acquisition and preprocessing parameters of the data set are explained in the supplementary material.\nUsing the preprocessed ADNI's PET brain images, the goal is to identify subregions of the brain associated with cognitive deficit, and we use the subjects' mini-mental state examination (MMSE) scores as a measure of cognitive ability. Typically, the range of the MMSE is 0 \u2212 30, and it tends to decline as AD progresses as seen in Table 2 . Assuming that only relatively few areas of the brain are truly related to cognitive ability, we applied the proposed methodology to the data set. In some situations, brain images may have predictive Figure 4 : Selected subregions of the ADNI's PET brain associated with cognitive decline with selection probability higher than 0.2. Colour indicates estimated maximum selection probability. The first row shows axial view of the brain, the second is sagittal and the third is coronal.\npower for a clinical outcome as the images are related to one or more demographic characteristics, as a form of confounder, that drive the relation. Investigating the presence of confounding effects would be useful in practice because scalar covariates are generally much simpler than images to acquire (Reiss et al., 2015) . To test whether the subjects' demographic variables have an association with cognitive decline, the MMSE was linearly regressed on gender, age and years of education. The achieved R-square was 0.06, so these scalar covariates seem to be unrelated. In comparison, the predictive R-square achieved by the brain images using the proposed method is larger than 0.3, as discussed later in the section, therefore, we decided not to adjust MMSE scores for demographic variables. To reduce memory burden and facilitate computational time, we decrease the size of the brain to (120 \u00d7 120 \u00d7 55), taking axial slices located at z = 16, ..., 70, and eliminating voxels outside of the brain. When we fitted the same model involving the complete brain slices using participants in the ADNI 1 study phrase only (in this case n = 402), clusters of voxels in these discarded brain slices appeared to be irrelevant, so we dropped the seemingly redundant brain slices from the analysis. The size of the original PET brain image data with n = 1302 subjects was approximately 13 GB, but after the reduction, the size has decreased to 8.2 GB. First, we identify subregions of the brain, associated with the MMSE scores, selected by the FuDoS method with high probability. For each random subsample of the data set with size n/2 , n = 1302, we fit FuDoS to the subsamples, gaining X \u03c1,c\n.01, 0.03} and C = {0.01}. The size of each segment is set to be 3 3 \u2212 7 3 cubes, and with the considered values in A, the estimated number of segmentsL is approximately ranges from 2100 to 7500. When the full brain was used, the same segmentation rule led to approximately 5100 <L < 11, 100 . Note that the total number of voxels involved in the analysis is 792,000.\nFor fitting \u03b2 [l] , piecewise constant basis is used. Thus, a total number of regression parameters to be estimated is the same as the number of segments in the set J , except an intercept. Because ADNI's PET data is high resolution, i.e., 1.5 mm of voxel size, and the size of each segment is quite small, i.e., each segment includes relatively small number of voxels, using a constant form for \u03b2 [l] is not be too restrictive. In other words, influences of neighbouring voxels in the same segment are likely to be similar. The above subsampling procedure is repeated 100 times, and for each (\u03c1, c), the probability of each voxel being included in X \u03c1,c J * is estimated. We take maximum of the probability over B, and determine stable subregions X \u03c0 stable for each \u03c0 as in (9). Table 3 reports the number of selected stable voxels for different values of \u03c0.\nThe brain is composed of three parts: the brainstem, cerebellum and cerebrum (the largest part of the brain), with the surface of the cerebrum called the cortex. The cortex has a folded appearance, and each fold (gyrus) a groove between a sulcus. Cerebral cortex contains most of the brain's neuronal cell bodies (grey matter), and includes regions of the brain involved in sensory perception such as seeing and hearing, memory, emotions, speech, decision making. In the analysis of AD, grey matter is most of interest as it has been clinically proven that measures of semantic (fact-based) and short-term memory have a significant positive correlation with grey matter volume in older people (Resnick et al., 2003; Buckner, 2004) . In contrast, no association was found between white matter volume and variability in cognitive functions. The cerebrum is divided into four lobes: frontal, parietal, temporal and occipital, and each lobe is composed of several areas. Each of the brain areas serves specific functions. They do not function alone, but often, work together having complex relationships with each other.\nIn AD patients, there is an overall shrinkage of brain tissue. The sulci are noticeably widened, and big shrinkage in the gyri is often found. Moreover, in many brain imaging studies of early AD, including Gusnard and Raichle (2001) , decreased metabolism has been found predominantly in the posterior cingulate cortex (associated with yellow clusters in Figure 4 ), medial temporal lobe (associated with orange/red clusters in Figure 4 ) and inferior parietal lobe. Figure 4 presents the estimated selection probability for the selected stable subregions. To aid visualization, we view the brain in three different anatomical planes: axial plane (top row), coronal plane (middle row) and sagittal plane (bottom row). We note that the clusters of voxels identified in our analysis agrees well with the two expected anatomical brain regions. Firstly, the big yellow/orange clusters, in axial planes of 14 and 28, in coronal planes of 50 \u2212 66, and in sagittal plane of 83, are associated with the posterior cingulate cortex. The posterior cingulate cortex (PCC) is the posterior part of the cingulate cortex, situated in the upper part of the limbic lobe, surrounded by the precuneus and the retrosplenial cortex. The PCC is known to have memory-related functions, and many previous studies have found abnormal patterns of the brain in this region in AD patients (Foster et al., 1984; Minoshima et al., 1995 Minoshima et al., , 1997 Huang et al., 2002) . For example, Minoshima et al. (1995) proposed a fully automated approach to discriminating probable AD patients from normal control (NC) subjects, and a statistically significant reduction in glucose metabolism was found in potential AD patients in most of cortical areas, including the parietal, temporal and frontal cortex. The profound brain abnormalities in AD patients in the posterior parietal lobes were also demonstrated in Foster et al. (1984) , see their Fig 1. Similar to our analysis, a regression approach was used in earlier studies to reveal the brain abnormalities. Minoshima et al. (1997) linearly regressed the MMSE on each voxel of the brain in cortical areas, and the results indicate a marked metabolic reduction in the PCC in patients who are at the very early stage of AD, see their Fig 1 and 2 . Functional brain imaging techniques also have been widely used for the analysis for AD as AD is closely related to the changes in the functional connectivity among different brain regions (Fransson and Marrelec, 2008) . Based on functional MRI (fMRI) and diffusion tensor inmaging (DTI) data, Zhou et al. (2008) investigated the functional connectivity maps of representative of NC, mild cognitive impairment (MCI) and early AD subjects, and claimed a significant reduction of fiber bundles in the PCC in the groups of early AD and MCI, compared with the NC group, see their Fig 1 and 2 . Moreover, according to Huang et al. (2002) , a reduction in relative blood flow of the posterior cingulate gyrus could be found, at least two years before the patients are clinically diagnosed as AD. The second expected brain region identified from our analysis is hippocampus and medial temporal lobe, related to the red clusters of voxels, in axial locations between -36 and -14, in coronal locations between 60 and 80, and in sagittal locations between 68 and 72 and 93 and 116. Many studies have identified the anatomical components of the brain system that govern memory function in the medial temporal lobe, and this neural system consists of the hippocampus and adjacent, including entorhinal, perirhinal, and parahippocampal cortex (Squire and Zola-Morgan, 1991) . Considering the well-known medial temporal lobe memory system, it is not surprising that early symptoms of AD are associated with pathological change and loss of neurons in this lobe (Jobst et al., 1994; Jack et al., 1997; Visser et al., 1999; Dickerson and Sperling, 2008) .\nWe now investigate mean effects of the selected stable subregions on the MMSE scores. In this analysis, the used cut-off value is \u03c0 = 0.3, which results in a total number of selected voxels of 1466, as given in Table 3 . We used a density based clustering algorithm for spatial data (Ester et al., 1996; Sander et al., 1998) to group the selected voxels as explained in Section 2.6, resulting in four subregions (four clusters of voxels). To obtain the sampling distributions of the mean effects, bootstrapping is used. The obtained sampling distributions from 100 iterated bootstrapping are reported in Table 4 , and the mean of each effect is displayed in Figure 5 . The lower bound of the bootstrap-based confidence intervals of the subregion 1 and 3, associated with the voxels in hippocampus is negative. This might indicate that glucose consumption of the brain cells in hippocampal area has a negative relation with the cognitive ability, and the effects are statistically significant. While, the mean effects on the cognitive ability at the subregion 2 and 4, related to voxels in the PCC and some parts of the medial temporal lobe, is positive and statistically significant.\nFinally, we exploit a 10-fold cross validation to perform prediction. Similar to the analysis of gasoline data in the supplementary material, we leave out 10% of observations as a validation set, use the rest to train the model, including the identification of the stable subregions through 100 repeated subsampling, and perform the prediction on the data points that have been left out. Repeating this procedure for each sample fold, we aggregate the predicted values, and compute the predictive RMSE and predictive R-square, defined\nis the predictive value of Y i , based on the model fitted from training samples except samples in jth sample fold. As introduced in Section 2.6, predictive models, denoted by M \u03c0 , were built for each value of \u03c0, and the results of each predictive model are reported in Table 5 . The achieved predictive R-square with \u03c0 < 0.3 is 0.35. We note that Wang et al. (2014) also carried out 10-fold cross validated prediction of MMSE scores based on the ADNI's PET brain images, including n = 403 number of subjects, and the produced predictive R-square using their method was 0.26."}, {"section_title": "Conclusion", "text": "In this article, we have introduced a domain selection method which, in the context of functional data analysis, effectively identifies subregions of the brain associated with a clinical outcome of interest. The methodology is general, so it can be applied to any kind of data, where a predictor X is functional and a response Y is scalar. The methodology is composed of two stages of estimation. We first segment X into several small parts based on the correlation structure. Then, potential subsets are built using the obtained segments and their predictive performance are evaluated to select the best subset. To account for functional features of X, two functional regression approaches, either penalized splines, or piecewise constant basis, are considered for fitting the regression function. We used a subsampling scheme, i.e., stability selection criterion to stabilize the selected subset and we found that this selection scheme provides several advantages to the proposed method. It increases the selection probability at boundaries of the true segments. Moreover, it reduces the rate of falsely selected subregion. The proposed method also has a practical advantage. Due to the induced sparseness, the results naturally provide more interpretable information about the relations between the regions and the outcome. We also investigated predictive ability of selected stable subregions. Our 1-D numerical results given in the supplementary material suggest that the selected stable sets can be used for building predictive models, and they can outperform competing methods in prediction for a range of \u03c0.\nOne possible criticism of the proposed method would be that the current segmentation procedure does not account for the response Y . So, one can imagine to obtain a more relevant segmentation using a conditional covariance function of X given Y , or using an approximated covariance function based on functional partial least squares decomposition. However, due to the high-dimensional nature of the brain image data, the extension is not trivial, and at least the numerical cases considered in this article may not suggest the need of a more complex approach for the segmentation as the current methodology can select the true subset with high probability. Another possible criticism would be that functional linear regression can be too restrictive in some situations, and so one can expect to achieve better predictive performance by replacing the functional linear model with more flexible models, such as multiple functional index models, functional projection pursuit models, or pure nonparametric models. However, these flexible models are computationally very intensive, therefore they might be practically impossible to apply to the brain image data. Also, at least, in the analysis considered in this article, a linearity assumption on the regression function does not seem to be restrictive. A Some simulation results of segmentation New participants were recruited during each phase of the study, and they are followed and reassessed over time."}, {"section_title": "C Acquisition and preprocessing of FDG PET images", "text": "Before scanning, the tracer FDG (fludeoxyglucoser), an analogue of glucose is injected through a vein, and the tracer travels through blood and collects in brain tissues. The injected dose of FDG was 5.0 \u00b1 0.5mCi, and the subjects were scanned from 30 to 60 minutes post-injection. This procedure generates either six fiveminute frames (for ADNI-1), or four five-minute frames (for ADNI-GO and ADNI-2). All subject underwent neurological examinations within three months of the scanning, involving the memory, effective memory and mini-mental state examination (MMSE). The MMSE invented by Folstein et al. (1975) examines various cognitive abilities, including orientation to time and place, immediate and delayed recall of three words, attention and calculation, language and visuo-constructural functions. It is often used by clinicians alongside patients' medical history, symptoms, physical exams and the results of other tests, including brain images to diagnose dementia and assess progression and severity of the disease. Typically, the range of the MMSE is 0 \u2212 30, and it tends to decline as AD progresses as seen in Table 2 . The PET brain image scans were preprocessed by the following steps. Each frame was coregistered to the first frame of the raw image file. Six or four coregistered frames were averaged to create a single PET image. Each subject's coregistered and averaged PET image from the baseline PET scan was reoriented into a standard grid (160 \u00d7 160 \u00d7 96), displaying cubic voxels of size 1.5 mm, and the anterior-posterior axis of each subject is parallel to a AP-CP line. Finally, each image set is filtered with a scanner-specific filter function to produce images of a uniform isotropic resolution of 8 mm FWHM. The detailed description of the PET preprocessing is described in http://adni.loni.usc.edu/methods/pet-analysis/pre-processing/."}, {"section_title": "D 1D numerical study", "text": "As a proof of concept, in addition to analyzing three dimensional data, we investigate the properties on 1-D functional data. This allows comparison with other 1-D methods in the literature. We assess the performance of the proposed methodology on simulated 1D functional data, and compare the results with FLiRTI (James et al., 2009) and WLasso (Zhao et al., 2012) . We also demonstrate the methods on the gasoline data set (Crainiceanu et al., 2013) ."}, {"section_title": "D.1 1D simulation", "text": "Two forms of X(t) are considered and the details of the simulation settings are following.\n\u2022 Case 1: X i (t), i = 1, ..., n, t \u2208 [0, 1], are generated from ARM A(2, 2) and discretized at equi-spaced points:\nwhere e i (t j ) \u223c N (0, 1).\n\u2022 Case 2: X i (t), i = 1, ..., n, t \u2208 [0, 1], are a linear combination of cubic B-splines with interior knots placed at 1/16, ..., 15/16 and coefficients, that is,\nwhere c ij \u223c N (0, 4), and \u03c6 j (t) are B-spline basis functions.\n\u2022 In both cases we use a disconnected smooth function for \u03b2(t) with flat region. Specifically,\nwhere the range of t is given as approximated values. Because of the discretization, each segment can be expressed as a set of consecutive design point t j with an index j as\n\u2022 Based on X i (t) and \u03b2(t), we simulate\nwhere i \u223c N (0, \u03c3 2 ) is an observational noise, with \u03c3 2 determined by signal-to-noise ratio (SN R), i.e., SN R = var(\u1ef8 i )/\u03c3 2 with\u1ef8 i = X i , \u03b2 . In approximation, we can write\u1ef8\nTo examine the performance of the proposed method under various simulation settings, we vary the sample size as n = {50, 800}, and the size of noise as SN R = {2.5, 5, 10, 20}, producing eight simulations for each case of X. To determine the stable subdomains, we randomly draw subsamples of size n/2 a hundred times, returning 100 selected subsets X \u03c1,c J * , for each pair of (\u03c1, c) \u2208 B. Then, we estimate the selection probability of each subdomain X as in (8), and take its maximum over B. When choosing B = (A \u00d7 C) with \u03c1 \u2208 A and c \u2208 C, we consider a single set for C as C = {0.01}, and consider A = {.02, .035, .04, .05, .06} in case 1 of X, and A = {0, .03, .04, .06, .08} in case 2 of X. The set A is chosen to achieve two goals: 1) the size of each segment is such that it encompasses at least 5 equispaced t j 's and 20 at most; and 2) different values of \u03c1 result in different levels of segmentationL, whereL is the estimated number of segments. In case 1 of X, A = {.02, .035, .04, .05, .06} approximately results inL = {19, 17, 15, 12, 8} on average, and in case 2 of X, A = {0, .03, .04, .06, .08} leads toL = {18, 14, 12, 10, 8}. For fitting \u03b2 [l] as in (1), penalized B-splines (Marx and Eilers, 1999; Cardot et al., 2003) , with the smoothing parameter selected by generalized cross validation (Craven and Wahba, 1979 ) is used. To fit FLiRTI (James et al., 2009) , one must choose three values of tuning parameters: 1) the penalty parameter to adjust the level of sparsity; 2) the weight to be placed on the zeroth derivative relative to the higher order derivative; and 3) the derivative order to assume sparsity in. We use the default choice of the zeroth and the second derivative order to impose sparsity, and select the penalty and weight parameter using a 5-fold CV, which is the default setting of the R-code provided by the authors. WLasso (Zhao et al., 2012) involves three tuning parameters: 1) the number of coefficients retained for prediction; 2) the penalty parameter associated with the regularizing term; and 3) the min-scale adjusting the coarseness level of the wavelet decomposition, and the optimal values of these three parameters are chosen by a 5-fold CV. We evaluate the selection performance using P 1 as in (15) and P 2 as in (16). The prediction ability of FuDoS is also investigated. The predictive ability of each method is also investigated. We generate 1000 pairs of test samples, carry out prediction and calculate the RMSE on test samples. As explained in Section 2.6, we build a predictive model based on X \u03c0 stable for each \u03c0, so prediction RMSE is measured for each \u03c0.\nEach simulation setting is repeated 100 times, and the average and the standard deviation of RMSE, P 1 and P 2 are reported in Table 6 (for case 1 of X) and in Table 7 (for case 2 of X). In each simulation scenario, the minimum of RMSE and the maximum of P 1 and P 2 are highlighted in bold. In all simulation scenarios, FuDoS outperforms the competing methods in prediction for a range of \u03c0. The best range of \u03c0, in the sense that it produces comparably small values of RMSE tends to fall as the sample size n and SN R becomes smaller. Specifically, when n = 800 >> p = 128, the range of 0.35 \u2264 \u03c0 \u2264 0.85 results in very similar prediction results, and the smallest RMSE is achieved when 0.65 \u2264 \u03c0 \u2264 0.85. While when n = 50 << p, \u03c0 > 0.75 generates poor prediction results, and the smallest RMSE is produced where 0.55 \u2264 \u03c0 \u2264 0.65.\nWe now discuss the selection results. Although WLasso is designed for producing sparse results, it does not exactly allow \u03b2(t) = 0, so we did not measure P 1 and P 2 for WLasso. FLiRTI seems to perform well in selection as the produced P 1 (true positive rate) is very close to 1 in all simulation cases. However, the selected subset from the FLiRTI method tends to include many false segments as indicated by its fairly small values of P 2 . The FuDoS method tends to identify the true subset more often than FLiRTI method as seen from its higher values of P 2 . For instance, in case 2 of X, with n = 800 and SN R = 20, FuDoS produces P 1 = .99 and P 2 = .44, with \u03c0 = .45, but FLiRTI yields P 2 = 0.32, with the similar value of P 1 = .99. There is only one case (case 1 of X with n = 800 and SN R = 10) that FuDoS emcompasses more false segments. To effectively visualize the selection performance of FuDoS, we plot the average of P 1 and P 2 for different values of \u03c0 in Figure 7 . It is not surprising that P 1 approaches to 1 as \u03c0 approaches to 0. Selection performance is related to both n and SN R, but the effect of n seems to be stronger. (0) 100 (0) 100 (0) 100 (0) 100 (0) 100 (0) 99 n=800;SNR=20 n=50;SNR=20 n=800;SNR=10 n=50;SNR=10 n=800;SNR=5 n=50;SNR=5 n=800;SNR=2.5 n=50;SNR=2.5 C = {0.01}. Through subsampling, we estimate maximum of selection probability and display it in Figure  8 (a) in a dashed red line. The regression coefficient \u03b2 [l] is estimated using penalized B-splines methods, with the smoothing parameter selected by GCV, and the smoothing parameters are assumed to be the same for all l = \u03ba 1 , ..., \u03ba K . Figure 8 (a) reveals that FuDoS identifies two parts of the spectra, roughly 1150-1250 nm and 1320-1370 nm, being related to the octane number with high probability. We now estimate \u03b2(t) using the estimated stable subdomainX Figure 8 (c) in a dashed red line, where the involved two tuning parameters were chosen via a 5-fold CV, and the used two derivative orders were d = 0, 3. It is shown that the octane number is negatively related to the spectra between 1200-1270 nm, but has a positive association near 1350 nm, which is consistent with the result from FuDoS. There is one part in 8(c) not identified by FuDoS, i.e., wavelengths between 1500-1550 nm are selected by FuDoS with probability less than 0.2."}, {"section_title": "D.2 Analysis of gasoline data", "text": "Next, we exploit 10-fold cross validation to test the predictive ability of the two methods. Specifically, each sample fold of 10% observations is left out as a validation set, the rest is used to train the model, and the prediction is performed on the observations that have been left out. Repeating this procedure for each sample fold, we aggregate the predicted values, and compute the predictive R-square, defined as 1 \u2212 (Y i \u2212\u0176 \u2212j,i ) 2 / (Y i \u2212\u0232 ) 2 , where\u0176 \u2212j,i is the predictive value of Y i , based on the model fitted from training samples except samples in jth sample fold. The predictive R-square produced by FuDoS is 0.97-0.98 for a range of 0.1 < \u03c0 < 0.9, and the R-square yielded from FLiRTI is 0.98. The plots of the original versus predicted octane number are shown in Figure 9 (a). "}]