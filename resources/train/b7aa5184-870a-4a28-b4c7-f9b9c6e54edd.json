[{"section_title": "", "text": "In a time of limited government resources, demonstrating program performance is essential. For environmental and conservation programs in the U.S. Department of Agriculture (USDA), accurately forecasting program performance requires consideration of climatic, economic, and other highly variable factors. An analyst developing a forecast over a specific horizon must handle the uncertainty caused by these variables, while at the same time providing scientific rigor. How do you separate the impacts of programs authorized by Congress from the acts of nature? Forecasting the effect of millions of individual conservation actions (such as buffer strips, nutrient management plans, etc.) must be reconciled with measurable water quality and other environmental outcomes-such as nitrogen concentrations in the Chesapeake Bay or the size of the Gulf of Mexico hypoxic zone (which are dependent on precipitation and other variables). The effects of individual conservation actions are not easily observable and must be modeled. But the outcomes-such as wildlife populations and water quality-are readily observable. Papers and Proceedings"}, {"section_title": "Forecasting and Assessing Environmental Performance in a Non-Market Economy", "text": "Joy Harwood and Skip Hyberg U.S. Department of Agriculture/Farm Service Agency The United States has experienced considerable success over the past 30 years in reducing point source pollution (from industries and water treatment plants) through the successes of the Clean Water Act. Increasingly, the focus is turning to the broader goals of improving water quality through reducing nonpoint sources of sediment, nitrogen, and phosphorus levels in the Mississippi River, the Chesapeake Bay, and other watersheds (from agriculture and urban runoff). Nitrogen and phosphorus from the Mississippi River, for example, contribute to the \"hypoxic zone\" in the Gulf of Mexico, where depleted oxygen levels reduce the health of aquatic life. Many factors contribute to hypoxic zones, including fertilizer from production agriculture, atmospheric deposition, manure from livestock farms, and sewage effluent. This paper focuses on the scientific and economic forecasting challenges associated with water quality and agriculture (particular, crop production). It reviews pending issues that analysts are currently grappling with, such as measuring the effects of conservation practices on water quality given the significant effects of weather, and issues of reconciling model results with real-world data. The paper also discusses the interrelationships between science and economics, as well as the similar issues that forecasters face in both of these fields. Note that the paper does not discuss water quality markets and pay-for-performance, which are emerging, longer-term issues. Various types of conservation practices are used in crop production. These include buffer strips (areas near cropland planted to close-growing crops and designed to intercept and thereby reduce sediment, nutrient, and chemical runoff into waterways), no-till cultivation, crop rotations, and nutrient management plans. Forecasting the benefits of such conservation practices on an expanded scale-and the impact of doing so on watersheds-poses a significant challenge to analysts. Forecasting is difficult because individual conservation efforts are unobservable, requiring the use of process models to estimate current and future benefits. Further, the effects of practices on water quality are lagged due to \"acts of God\" (such as heavy rainfall) which are random and can significantly affect water quality, confounding our ability to observe the effectiveness of conservation (or \"Acts of Congress\"). Last but not least, landscapes are multidimensional in their complexity, and the location of conservation practices matters. Scientific rigor regarding these issues is critical. Both scientific and economic forecasters face several critical questions. Some of the more important include: How are projections best made to determine whether we are likely to meet water quality targets established by the Environmental Protection Agency (EPA)? How do we ensure that our models are properly specified? How do we know that we are establishing the right indicators to measure progress? How can we achieve these goal in the least-cost manner-both regarding producers and Federal, state, and local governments? How can the performance of conservation programs be best measured and demonstrated to policymakers in this time of increased budget scrutiny?"}, {"section_title": "Complexities Associated with Scientific Modeling", "text": "Knowledge of what factors are important to water quality is not fully developed and is still evolving. For example, scientists have identified that sediment, nitrogen, and phosphorus are key to assessing water quality, but views on the appropriate measurement of these variables has changed over time. Rather than measuring total nitrogen and phosphorus, some scientists now argue that soluble nitrogen and phosphorus are better linked to the development and growth of hypoxic zones. Divergences in thinking about the effectiveness on nutrient reduction are reflected in differences in how total nitrogen is defined in predictive models: some such as the Conservation Effects Assessment Project (CEAP) model (Lund), use total nitrogen, while others (such as Greene, et al, Turner et al., and Goolsby and Battaglin 2000) use nitrate ( NO 3 ) or the sum of NO 2 and NO 3 . In addition, researchers initially believed that reducing nitrogen was the critical catalyst decreasing hypoxic zones, and deemphasized phosphorus. Now, scientists have identified the important role that phosphorus plays in contributing to hypoxia (U.S. EPA). The lagged effects of conservation practices are also a critical aspect of modeling and predicting performance. This is because nutrient loadings in watersheds do not immediately respond to changes in conservation practices. Many conservation practices-such as the establishment of long-term ground cover-take years to reach maturity, and thus have a gradual impact on intercepting surface water as well as subsurface water flow affecting nutrient levels. In addition, as the impacts of individual conservation practices are not directly observable, the use of aggregate models (which, for example, estimate nutrient loads in the Mississippi) and prediction validation (involving measurement of nutrient loads at the mouth of the Mississippi) are essential. Researchers are continuing to develop a better understanding of the link between specific types of conservation practices, landscape composition (such as the existence of tile drainage), and water quality. Soluble nitrogen is transported from crop land primarily in subsurface drainage, especially in the Corn Belt where nitrogen-based fertilizer and tile drainage use are common (Crumpton, et al.). David, Drinkwater, and McIsaac have found that incorporating the use of tile drainage systems in their models greatly improved the predictive power associated with Gulf of Mexico nitrogen loadings-an occurrence that had been hypothesized prior by numerous analysts. Jacobson, David, and Drinkwater found that phosphorus loads to the Gulf of Mexico were explained using variables including cropland within the area, fertilizer (phosphorus) inputs, soil variables (such as bulk density), and the effects of human population. Manure was not found to have a significant effect. Undoubtedly, researchers will find more missing variables and better specifications of those variables as we try to answer complex questions about the landscape and the resulting downstream impacts regarding water quality. Our understanding of the behavior of watersheds is also evolving, which complicates our forecasting efforts. More specifically, enhanced understanding of stream hydrological dynamics is bringing greater attention to the role of stream sediment and bank erosion on river sediment transport. Stream dynamics involve a continuous process of acquiring, suspending, transporting, and depositing sediment. Studies show that seventy-five to eighty percent of the suspended sediment in rivers can be from stream bank erosion and scouring of the channel (Gellis and Landwehr). Considerable nitrogen and phosphorus is attached to this sediment and becomes included in total nitrogen and total phosphorus measures as this sediment is used by rivers in the process of reaching \"equilibrium.\" How do we best deal with these time lags and the dynamic, multi-dimensional nature of these complex systems? If we assume that we have the perfect model and perfect information, it still takes time to install conservation systems and observe changes in water quality. For example, several years of large coastal storms in May and June and the associated precipitation can dwarf the effects of conservation practices on water quality. Figure 1 illustrates the considerable year-to-year midsummer hypoxia levels for the Gulf of Mexico, in comparison with the long-term average and the EPA goal. Figures 2-3, which are related to the hypoxia chart, indicate the impacts of streamflow on nitrogen and phosphorus flux. It is clear that nutrient fluxes are not the sole explanatory variables for the extent of the hypoxia zone. A challenge for forecasters is their confidence in their models, their ability to communicate why water quality is not improving when such \"acts of God\" are quite variable, and to buy time to continue to \"do the right thing\" while waiting for water quality to catch up. Given that there is no perfect model, we need to be both rigorous and honest in our assessments, reflecting the uncertainty we know to exist. In addition to the effects of precipitation, what other factors introduce this uncertainty? The impact of summing thousands of conservation practices across the landscape introduces considerable uncertainty. So, too, do the dynamics of the transformation of multiple forms of nitrogen and phosphorus that exist, the movement of these nutrients over a variable landscape, and many other factors. Once measures are agreed upon, models must be as accurate as possible given our understanding of the landscape and the availability of data. This is both an art and a science, and data availability is a critical factor. For example, multiple models are used to measure performance in the Chesapeake Bay watershed. EPA's model, used to set required total maximum daily load (TMDL) levels, focuses on different patterns of land use, but does not account for the impact of alternative conservation practices. In contrast, the Department of Agriculture's (USDA's) model focuses on agricultural land use and assesses the impacts of changes in conservation practices. Both models forecast changes in water quality, using different approaches to do so. Further, neither the EPA nor USDA models take into account the voluntary actions of producers outside of existing conservation programs. Several private groups have challenged that private actions taken by producers should be taken into account, as they can have significant effects on water quality. Others cite the example of publicly owned water treatment facilities, such as in the Delaware River Basin, where municipal biosolids are composted and in some cases transported into the Chesapeake Watershed (after, for example, being applied to fields as fertilizer) (Hewitt). Conversely, nitrogen from the Chesapeake Bay watershed volitilizes and is deposited in other watersheds. Accounting mechanisms for actively assessing inter-watershed transport of these nutrients do not exist currently. A thorough nutrient accounting matrix of the Chesapeake Bay is needed, and significant \"missing\" variables should be ideally included in modeling systems. Issues have also arisen with regard to the linkage between model estimates and real-world data about changes in water quality. In a well-publicized example, a 2005 Government Accountability Office report criticized claims as to improvement in the health of the Chesapeake Bay. Prior reports and statements about Bay health were cited as relying largely on predictive models, which tended to indicate more progress was being made than did actual monitoring data. Further, the report emphasized the difficulties associated with defining an overall measure of Bay health and water quality. For example, measures such as dissolved oxygen, water clarity, and \"chlorophyll a\" were used, but no method had been developed to combine these measures into an overall composite indicator of Bay health."}, {"section_title": "Complexities Associated with Economic Modeling and Producer Response", "text": "From an economist's perspective, the issues of understanding science and economics are closely intertwined. For example, in terms of economics, what are the most cost-effective methods of reducing nitrogen and phosphorus? Hatfield, et al. examined nitrate patterns in the Raccoon River in Iowa. This study found that widespread removal of hay and small grains from crop rotations to an almost exclusive use of corn/bean rotations altered seasonal water use patterns and, in particular, increased nitrate loss in the early spring. The study concluded that focusing only on changing fertilizer rates or timing is inadequate, that other cropping practices can affect water quality, and that the complete agricultural production system must be the focus. At this point, economics plays an important role in gauging the trade-offs between science, economics, and the practical aspects of farming. For example, corn and soybeans are significantly more profitable crops in many parts of the country than small grains. Iowa State University publishes annual crop budgets that project net returns for various rotations and cropping practices (Duffy). Planting corn in Iowa in 2011 (assuming soybeans were planted the prior year) results in an expected net return over variables costs of $463 per acre (and $158 per acre when compared to all costs). In contrast, planting oats is projected to produce a net return over variables costs of -$24 per acre (and -$193 per acre when compared to all costs). These projections indicate why corn and soybeans are by far the predominant crops in the Corn Belt, and also why corn and soybeans-which now have shorter-day length and more drought-resistant varieties availableare increasingly prevalent in the Dakotas and other western areas, which have traditionally been small grain territory. Needless to say, creating incentives for change must take into account the costs and benefits to individual producers, and not only society as a whole. A significant role for forecasting models is identifying the cost-effective policies and practices for reducing non-point source pollution. By integrating environmental and economic variables and testing alternative approaches, models can provide cost effective direction for developing policy, designing incentives, targeting resources, and implementing programs. Numerous state programs have been successful at creating changes in behavior, both through regulation and incentive systems. For example, farmers historically have often applied nitrogen in the fall only to see significant moisture over the winter wash much of it away. This is often done as \"insurance\" against the time needed for soil preparation activities in the spring and the potential planting delays (which can result in lower yields), to distribute labor use over time, to avoid soil compaction in the case of wet springs, and to take advantage of price incentives. The state of Maryland has, through its \"Nutrient Management Planning\" program, as well as the Water Quality Improvement Act, created incentives for delaying input applications until the spring (Maryland Department of Agriculture). Under this program, licensed private sector nutrient management consultants help with soil tests, work with farmers on yield goals, and estimate residual nitrogen to generate field-by-field recommendations. By doing so, the program helps protect water quality in the Bay and its tributaries, and also helps control soil erosion. An example of an emerging state program is the state of Florida's, in response to a 2008 lawsuit and EPA's resulting actions (Obreza, et al.). In 2009, EPA determined that Florida's \"narrative\" criterion was insufficient to protect water quality and that numeric standards needed to be put in place. (Florida's narrative criterion had stated: \"In no case shall nutrient concentrations of a body of water be altered so as to cause an imbalance in natural populations of aquatic flora or fauna.\") The University of Florida indicates that the impact of this change will likely not have a great effect on agricultural producers who are using (or adopt) best management practices with regard to fertilizer and chemical use, and other activities. Over the longer term, however, more aggressive and expensive practices may be required to meet the new numeric standards. For analysts, numeric standards provide a way to more easily assess the effectiveness of existing, as well as new, conservation activities."}, {"section_title": "Examples Using the Conservation Reserve Program", "text": "As economists, we can not only estimate the least cost methods of moving the U.S. toward certain goals, but also forecast the nonmonetary benefits of doing so. At the national level, the Conservation Reserve Program (CRP) provides a good example where such projections are developed. CRP is a voluntary program whereby recipients receive annual rental payments and cost-share assistance to establish long-term (10-15 year), resource conserving covers on eligible farmland (USDA Fact Sheet). USDA pays about $2 billion annually in rental payments to CRP participants. An annual CRP \"monitoring and evaluation\" effort funds cooperative research across government agencies and with colleagues at universities to estimate the benefits of the program (USDA FSA Annual Summary). For example, this research has estimated that, since the beginning of the program in 1985, more than 8 billion tons of soil have been prevented from eroding, including an estimated 325 million tons in 2010. On fields enrolled in CRP, nitrogen and phosphorus losses were reduced by an estimated 600 million pounds and 100 million pounds, respectively, in 2010. In addition, CRP acreage reduces the impacts of downstream flood events and recharges groundwater aquifers. Like the scientific examples used earlier, our understanding of the complex processes involving the CRP and its potential benefits are evolving over time. While the CRP has focused much attention on grass covers in the Great Plains, there is an increasing focus on constructed wetlands in the Mississippi River basin, particularly in the form of a CRP initiative in Iowa. These constructed wetlands reduce nitrogen loadings in watersheds dominated by tile-drained cropland, and consist of a treatment pool and grass buffer (ranging from 20-70 acres in total). Monitoring data from the Iowa project indicate that these wetlands remove 40-90 percent of the nitrate flowing into the wetlands. The cost to reduce nitrogen load by a pound in such situations is projected to be less than $1.38 per year, for 50 years-which compares favorably cost-wise to other approaches (Iovanna, et al.). Crumpton, et al. took this Iowa project a step further, estimating the extent to which constructed wetlands would have to be established in the upper Mississippi and Ohio River basins to meet the goal of reducing nitrogen discharged into the Gulf of Mexico by 30 percent. Their simulation framework indicated that approximately 520,000 to 1.1 million acres of strategically placed constructed wetlands-reducing by 40-60 percent the nitrogen loads entering them-could achieve the 30 percent goal. The authors estimated the associated cost at about $1 billion annually."}, {"section_title": "Thoughts for the Future", "text": "As can be seen from this discussion, the efforts of forecasters are critical to understanding and assessing many environmental issues in agriculture, such as those relating to water quality. How do we proceed from here? Performance measures must be chosen that make a very real connection to the landscape. They must be science-based and communicate to a broad audience of policymakers and the public. They need to accurately separate the impacts of various weather conditions and other factors from the effects of agricultural conservation practices on water quality. These issues speak to the need for greater collaboration across Federal, state, and local governments-as well as the critical incorporation of key variables in making accurate forecasts. To date, analysis and forecasting has been focused largely within farming, and have not taken into account the costs and benefits much beyond agriculture-such as the impacts of increased population and the impacts on transportation, municipal waste plants, and other activities. Understanding the linkages between agriculture and other sources that affect water quality would provide greater insights into practices that have the greatest impact on environmental concerns. Further, forecasting environmental quality requires performance estimates that reflect changes in expected future outcomes from current conservation actions. Ideally, these measures need to be posted alongside direct measures of the outcome. Regarding water quality, estimated nitrogen and phosphorus reductions from conservation would be published--along with actual measured nitrogen and phosphorus concentrations--regarding the targeted waters. Making both the performance measures and the outcome measures available assures that projects are validated by program managers, policy makers, and the public. The focus of this paper has been on water quality. In a broader sense, conservation activities in reality provide multiple benefits. Cost-benefit analyses should capture the full set of environmental benefits. A common error made in examining conservation initiatives has been to focus on a single environmental benefit resulting from such conservation activities, and attributing all costs to that single factor. Doing so does not, however, accurately provide a composite picture of the trade-offs between benefits and costs. The author develops revised long term forecasts of Medicare Part A expenditures. The revisions reflect corrections to the treatment of multifactor productivity that affect the official 2010 Medicare Trustees' long term projection, and an alternative 2010 projection prepared by the Centers for Medicare and Medicaid Services, Office of the Actuary. In particular, the revision to the official Trustees' methodology raises present value Part A projected expenditures from $17.1 trillion to $23.3 trillion. The author concludes with recommendations for improving current Medicare long term projection methods."}, {"section_title": "INTRODUCTION", "text": "The changing landscape of the population living in the United States over the past several decades can be seen in many areas throughout the country. Whether it is a road sign written in Chinese or a Spanish-language television station, one can see that the language diversity in the United States is rapidly changing. In 2009, 57.1 million people (20 percent of the population 5 years and older) spoke a language other than English (LOTE) at home. In 1980, there were 23.1 million (11 percent of the population 5 years and older) LOTE speakers ( Table 1). The overall 148 percent increase from 1980 to 2009 in the number of LOTE speakers was not evenly distributed among languages. Polish, German, and Italian actually had fewer speakers in 2009 compared to 1980. Other languages, such as Spanish, Vietnamese, and Russian, however, had considerable increases in their use. This paper presents national-level projections of what the LOTE population might look like in 2020, with a focus on the methodology that is used to produce these projections.\nThe recent financial crisis and ensuing Great Recession have highlighted the importance and pervasiveness of international linkages in the world economy-and the importance of capturing those linkages in empirical macroeconomic models that are used for economic analysis, forecasting, and policy analysis. Pesaran, Schuermann, and Weiner (2004) propose and implement global vector autoregressions (or GVARs) as an ingenious approach for capturing international linkages between country-or region-specific error correction models. D\u00e9es, di Mauro, Pesaran, and Smith (2007) (hereafter DdPS) extend that work to a larger number of countries and regions; and Pesaran, Schuermann, and Smith (2009) assess the forecasting properties of the GVAR implemented in DdPS. The GVAR methodology has several attractive features: \uf0b7 a versatile structure for characterizing international macroeconomic and financial linkages though multiple channels, \uf0b7 a standardized economically appealing choice of variables (both domestic and foreign) for each country or region, \uf0b7 a systematic treatment of long-run properties through cointegration analysis, and \uf0b7 flexible dynamic specification through vector error correction modeling. These features are very appealing, and they balance naturally the roles of data and economic theory in empirical modeling. The GVAR explicitly aims to capture international economic linkages, especially linkages between the macroeconomic and financial sides of economies. Weak exogeneity plays an important role through allowing conditional subsystem analysis on a country-by-country basis. Data aggregationempirically implemented but based on economic theory-achieves a high degree of parsimony in the estimated models. The current paper re-examines some of the empirical underpinnings for global vector autoregressions, focusing on parameter constancy because of the intimate connections between it and forecast performance. To test parameter constancy, this paper uses impulse indicator saturation, which is a recent generic approach to evaluating constancy. The empirical results indicate substantial room for an improved, more robust specification of DdPS's GVAR; and some tests are suggestive of how to achieve such improvements. See Clements and Hendry (1998, 2002 and Hendry (2006) for discussions on the relationships between parameter constancy, forecast performance, and forecast failure. In related work, Ericsson (2011) discusses the theory of reduction and exogeneity in the context of GVARs, thereby providing the background for tests of parameter constancy, data aggregation, and weak exogeneity in GVARs. Using those tests, Ericsson (2011) then evaluates the equations for the United States, the euro area, the United Kingdom, and China in DdPS's GVAR. Ericsson and Reisman (2011) provide parallel results for equations for all 26 countries in DdPS's GVAR. This paper is organized as follows. Section 2 describes a prototypical GVAR and, in the context of that prototypical GVAR, summarizes the current approach taken to modeling GVARs, as developed in Pesaran, Schuermann, and Weiner (2004) and DdPS inter alia. Section 3 reviews the procedure for testing parameter constancy called impulse indicator saturation, which utilizes the computer-automated model selection algorithm in Autometrics. Section 4 empirically evaluates DdPS's GVAR for parameter constancy, using impulse indicator saturation. Section 5 concludes."}, {"section_title": "BACKGROUND", "text": "The United States has always been a country noted for its linguistic diversity. Information on language use and proficiency collected from decennial censuses shows that there have been striking changes in the linguistic landscape. These changes have been driven in large part by a shift in the origins of immigration to the United States. During the late 19 th and early 20 th centuries, the majority of U.S. immigrants spoke either English or a European language such as German, Polish, or Italian (Stevens, 1999). Beginning in the middle of the 20 th century, patterns of immigration shifted to countries in Latin America, the Caribbean, and Asia (Bean and Stevens, 2005). As a result, the use of Spanish and Asian or Pacific Island languages began to grow. By 2000, over 70 percent of the population speaking a LOTE spoke Spanish, Chinese, Japanese, Korean, Vietnamese, or Tagalog (Shin and Bruno, 2003). Since 1980, the percentage of the population who reported speaking a language other than English at home rose from 23.1 million speakers to 57.1 million speakers in 2009 (Table 2). The largest numeric increase in the population speaking a language other than English at home was for Spanish speakers (increased by 24.4 million speakers) whereas the largest percent increase was for Vietnamese speakers (533 percent increase). Language use is an indicator of cultural assimilation (Rumbaut, 1997), which is measured by shifts to English as the language usually spoken by U.S. immigrants and their descendants (Stevens, 1994). For most U.S. immigrant groups, the shift to English monolingualism takes place within a few generations (Hurtado and Vega, 2004). There are many incentives to learn and use English in the American society. Economists have argued that the impetus for language acquisition was for human capital (Chiswick and Miller, 2001) or that potential earnings could be affected by not having a strong command of the English language and, therefore, motivate immigrants to learn English and increase potential earnings (Cohen-Goldner and Eckstein, 2008). Others have argued that the economic view overlooks the social and cultural aspects of learning English in the United States (Espenshade and Fu, 1997;Mouw and Xie, 1999;Stevens, 1992) such as communication within and outside of one's language group. The U.S. Census Bureau has collected information about the language characteristics of U.S. residents in every decennial census from 1890 through 2000, with the exception of the 1950 census. Information was collected on English proficiency, mother tongue, and language spoken. The development of a consistent time series of data for the period between 1890 and 1980 is hindered by the considerable variation across censuses in terms of question wording, coding of responses, and the subsets of the population that were asked these questions (Stevens, 1999). Beginning in 1980, a series of three questions were introduced to gather data on language use and English speaking ability. These questions were developed to satisfy the legislative mandate of the minority language assistance provision of Section 203 in the Voting Rights Act of 1965 and, along with a few other variables, are used to determine which jurisdictions must provide voting rights materials in minority languages. 1 These same three questions were asked in the 1980, 1990, and 2000 censuses, providing a consistent time series with which to study changes in language use and English-speaking ability among U.S. residents over time. Since 2001, the language questions, along with all of the other social, economic, and housing questions that were asked in the Census 2000 long-form census questionnaire, are now asked yearly in the American Community Survey. This change allows for these characteristics to be gathered yearly instead of every 10 years. Having the same three questions asked for the last 3 decades gives a good metric for comparing the relative growth or decline of individual languages. The three questions were asked of the population 5 years and over. The first question asked \"Does this person speak a language other than English at home?\" If the respondent answered \"Yes\" to this question, they were then asked \"What is this language?\" with a write-in field for the answer and then asked \"How well does this person speak English?\" with the following four answer categories: \"Very well,\" \"Well,\" \"Not well,\" and \"Not at all.\" 1 For more information on the Voting Rights Act and how the language questions are used to satisfy the legislative mandate, see the Federal Register at <http://www.census.gov/rdo/pdf/FRN_VotingRightsDet erminations.pdf>. The language data collected are obtained from the second language question that asks \"What is this language?\" The languages written in this box are put through a coding procedure that assigns a language code for individual languages or groups of languages. There are 382 language codes and from this list, a standard classification of 39 detailed language groups is available. These 39 languages are further collapsed into four major language groups; Spanish, Other Indo-European languages, Asian and Pacific Island languages, and all other languages. Table 1 shows data from the 2009 American Community Survey for the four-and 39-language groups by English-speaking ability."}, {"section_title": "DATA AND METHODS", "text": "This paper presents a series of national-level language projections developed using data on the language spoken at home from the American Community Survey and the Census Bureau's 2008 and2009 National Population Projections. The paper discusses the language-projection results using the 2008 National Population Projections numbers only. The results using the 2009 projections are available upon request."}, {"section_title": "American Community Survey Data", "text": "The American Community Survey (ACS) collects data on social, housing, and economic characteristics for demographic groups in the United States. This paper uses the 2006, 2007, 2008, and 2009 ACS files. Data on language use and English-speaking ability historically collected in the decennial censuses, are now captured every year in the ACS. The ACS was conducted on a test basis from 2000 through 2004 and expanded to full sample size for housing units in 2005 and for group quarters in 2006. To have a complete sample, comparable to Census 2000, we chose to use the ACS data files from 2006 through 2009. 2"}, {"section_title": "National Population Projections Data", "text": "The U.S. Census Bureau's 2008 and National Population Projections were created using the cohortcomponent method and provide projections of the resident population of the United States and demographic components of change (births, deaths, and net international migration). 3"}, {"section_title": "Language Projection Methodology", "text": "These projections are based on Census 2000 data. These data are provided by age, sex, race and Hispanic origin for each year from July 1, 2000 to July 1, 2050. The projection series released in 2009 provided four supplemental series of projections with results for different international migration assumptions. The supplemental series included: (1) high migration, (2) low migration, (3) constant migration, and (4) zero migration. Assumptions about future rates of mortality and fertility are the same in all five series. This paper uses data for the years 2010 through 2020 from the 2008 series (U.S. Census Bureau, 2008) and the high, low, and constant series from the 2009 release (U.S. Census Bureau, 2009). We produce projections of both the total number of people speaking a language other than English at home (LOTE speakers) and the number of speakers for individual languages with at least 500,000 speakers in 2009. The 13 languages that meet this condition are: Spanish, French, Italian, Portuguese, German, Russian, Polish, Hindi, Chinese, Korean, Vietnamese, Tagalog, and Arabic. These are the most commonly spoken non-English languages and for some, such as Vietnamese and Russian, there has been tremendous growth in the number of speakers in the last few decades. The projections are produced by projecting future LOTE use based on trends in the ACS data and then applying the projected distribution of LOTE speakers to the projected population from the Census Bureau's 2008 and National Population Projections. The distributions of LOTE speakers are projected by demographic characteristics. For projections of the overall population speaking a LOTE and the population speaking Spanish, we project by age (single years 5-49 and 50 years and over) and Hispanic origin, resulting in a total of 92 groups for which we project the percent speaking a LOTE and Spanish. Projections of the individual languages other than Spanish are developed by age, resulting in a total of 46 groups for which we project the percent speaking other individual languages. We have developed three series of language projections, based on assumptions of constant, linear, and logistic change. The first assumption we make is the most basic and simplistic. We held LOTE use constant at currently observed levels. To do this, we held the percentage of LOTE speakers constant for each age and Hispanic origin group we project for at the level reported in the 2009 ACS. This is represented in equation 1, where P represents the percent speaking a LOTE in a given year.  The constant model assumes that future LOTE use will remain constant at recently estimated levels, and consequently there would be no change in the distribution of LOTE speakers within age and Hispanic origin groups. In this model, changes in the number of speakers will be driven by changes in the population projections. The percentage of LOTE speakers remains the same through 2020, but we apply these percentages to a population that is changing over time. If the size of a group increases over time, so will the number of speakers. The other two models we use are a linear model and a logistic model, which are based on the assumption that language use can change over time and are based on trends in LOTE use observed in the four years of ACS data (2006)(2007)(2008)(2009). The linear model assumes that language use in the future will change by the same amount as in the past and is represented by equation 2, where P t represents the percent speaking a LOTE at time t, a is the estimated intercept, b is the estimated slope, and t is the year of data being projected. P a b t t = + ( ) [2] The third series, based on an assumption of logistic growth, is also based on trends in LOTE use from the 2006 through 2009 ACS. In contrast to the assumption of linear growth, the logistic model assumes that growth is constrained by an upper and lower bound. The logistic model is represented by equation 3, where P t represents the percent speaking a LOTE at time t; a, b, and c are estimated parameters, and t is the year of data being projected."}, {"section_title": "P a b e ct", "text": "The linear model has the potential to exceed the bounds of the percent distribution, rising above 100 percent or falling below zero, whereas the logistic model will constrain growth as it approaches the upper and lower asymptotes of the distribution. In contrast to the constant model, where changes in the number of speakers will be driven by the population projections, for the linear and logistic models, changes in the number of speakers will be driven by both changes in the projected percentages of LOTE speakers within each group and by changes in the population projections."}, {"section_title": "Comparison of Language Projection Models", "text": "Figures 1 and 2 provide two examples of what each projection model looks like, based on ACS data for two age and Hispanic origin groups. These groups illustrate two trends we observed in the ACS data. One group shows an increase in the number of LOTE speakers whereas the other group shows a decrease in the number speaking a LOTE. Figure 1 shows the observed and projected percent speaking a LOTE at home for 36-year old non-Hispanics. This group showed an increase in LOTE use from 2006 to 2009, represented by the blue line in the figure. The red, green, and purple lines show what the projected percent of LOTE speakers will be for each of our three models. The constant series, represented by the red line, sets the projected percent of LOTE speakers for this group to equal the value observed in 2009, which was 13.8 percent. When this projected percent of LOTE speakers is applied to the projected population for this group, we would expect to see an increase in LOTE speakers so long as the projected population for this group increases over time. The green and purple lines show what the projected percent of LOTE speakers would be based on trends in the ACS data. These lines are very close to each other, illustrating that the linear and logistic models produce very similar results. When the percent projected to speak a LOTE is applied to the projected population for this group, we would expect to see an increase in the number of speakers. This increase would be larger than what would result from the constant model. Figure 2 shows the observed and projected percent speaking a LOTE at home for 19-year old non-Hispanics. This group showed a slight decrease in LOTE use from 2006 to 2009, represented by the blue line in the figure. The projected percent of LOTE speakers for each of our three models is represented by the red, green, and purple lines in the figure. The constant series, represented by the red line in the figure, sets the projected percent of LOTE speakers for this group to equal 9.1 percent, which was the value observed in 2009. When this projected percent of LOTE speakers is applied to the projected population for this group, we would expect to see an increase in the number of LOTE speakers as long as the projected population for this group increases over time. The green and purple lines show what the projected percent of LOTE speakers would be based on trends in the ACS data. As was the case in the first example, the linear and logistic models produce very similar results. When the percent projected to speak a LOTE is applied to the projected population for this group, we would expect to see a decrease in the number of LOTE speakers. The trend in this example is the trend that we found for a majority of the groups we projected for. As a result, the projected number of LOTE speakers in the constant model will increase over time as long as the population increases, while the linear and logistic models will show either small increases or in some cases a decrease in the number projected to speak a LOTE."}, {"section_title": "RESULTS", "text": "The results are presented in three sections. The first will address the overall use of a language other than English, followed by results for Spanish speakers, and finally the results for the other twelve individual languages we projected. The discussion presented in the paper is for the language projections based on the 2008 National Population Projections. Appendix Table  1 provides the results using the 2008 series. The results for the language projections using the 2009 National Population Projections are provided in appendix tables 2 through 4."}, {"section_title": "Language Other than English Use", "text": "The overall number speaking a LOTE is projected to increase in all three projection models (see Figure 3). We see the largest increase in the constant model, which is based on the simplistic assumption that the percent speaking a LOTE within the age and Hispanic origin groups we project would remain constant. When applying the constant proportions, we see a large amount of growth in the number of LOTE speakers. For the linear and logistic models, where a majority of groups actually showed decreases in the percent speaking a LOTE from 2006 to 2009, the projected increases in LOTE use are much smaller. While the population for these groups is projected to grow, the projected percent speaking a LOTE actually goes down. This results in a smaller increase in the overall number projected to speak a LOTE. The distribution of the population by language spoken is presented in Figure 4. This figure shows the percent distribution of the population that is projected to speak a LOTE and those that are projected to speak only English in 2010, 2015, and 2020. In each of the three models, there is a small increase in the percent that is projected to speak a LOTE. For all three models, English is projected to remain the only language spoken by a majority of U.S. residents. The constant model does show a slightly larger increase in LOTE use compared to the linear and logistic models. This finding is expected given that the assumption of the constant model is that the percent speaking a LOTE will remain constant at the levels observed in 2009, rather than to decrease over time as projected for several groups in the linear and logistic models."}, {"section_title": "Spanish Use", "text": "The number of Spanish speakers is projected to increase in all of the projection models (see Figure 5). As was the case for the overall number of LOTE speakers, the largest increase in the number of Spanish speakers occurs in the constant model, whereas for the linear and logistic models, which follow the trends in the ACS, the projected percent of the population speaking Spanish increases, but by a smaller amount. This is to be expected, since a majority of the age and Hispanic origin groups we projected showed a decrease in the percent speaking Spanish. While the projected population increases over time, the percentage speaking Spanish decreased for many groups. This resulted in smaller increases in the overall number projected to speak Spanish in the linear and logistic models, compared to results for the constant assumption. Figure 6 presents the percent of the total population five years and older that is projected to speak Spanish in 2010, 2015, and 2020. The percent speaking Spanish is projected to increase slightly over the next decade. In 2009, just over 12 percent of the population spoke Spanish at home. Under the assumptions that use of Spanish would remain constant over the next ten years, nearly 16 percent of the population 5 years or older is projected to speak Spanish. The linear and logistic models project a smaller increase, to just over 13 percent in 2020. Spanish is projected to remain the language spoken by a majority of LOTE speakers (see Figure 7). In 2009, 63 percent of LOTE speakers reported speaking Spanish at home. This increased to almost 68 percent in the constant series, while the percent projected to speak Spanish held steady at just over 62 percent in the linear and logistic models in 2020."}, {"section_title": "Use of Other Languages", "text": "The projected change between 2010 and 2020 in the population speaking French, Italian, Portuguese, German, Russian, and Polish is presented in Figure 8. The constant model shows an increase in the number of speakers for all languages. This is expected because the driver of change for this model is the population projections. In the linear and logistic models, which are based on observed trends, the population speaking French, Italian, German, and Polish is projected to decline. The decline in the number of speakers for these languages is also consistent with longer term trends observed in the 1980, 1990, and 2000 Census data ( Table 2). The population speaking Portuguese and Russian is projected to increase in the linear and logistic models, and the increases are higher than what was projected in the constant model, indicating that trends in the ACS data show growth in the use of these languages. Figure 9 shows the projected change in the population that speaks Hindi, Chinese, Korean, Vietnamese, Tagalog, and Arabic. With the exception of Korean, use of the non-European languages is projected to increase over the next ten years in all three models. While the number of Korean speakers increased from 1980 to 2000, trends in ACS data show that the use of Korean has decline in recent years. As a result, Korean is projected to decline in the linear and logistic models. Figures 10, 11, and 12 present the distribution of LOTE speakers by the language spoken for the constant, linear, and logistic models, respectively. Spanish, which was presented in Figure 7, and Chinese are the most commonly spoken languages in all three projections series, followed by French and Tagalog. Polish is the least spoken language among the thirteen languages we projected. In the constant model, all languages, except Spanish, are projected to decrease slightly as a percent of overall LOTE use (see Figures 7 and 10). In the linear model, Russian, Hindi, Tagalog, and Arabic increased slightly as a percent of overall LOTE use, while the other languages were either maintained at levels projected for 2010 or decreased slightly (see Figures 7 and 11). For the logistic model, Hindi, Chinese, Vietnamese, Tagalog, and Arabic all increased slightly as a percent of overall LOTE use, while the other languages were either maintained at current levels, or decreased slightly (see Figures 7 and 12)."}, {"section_title": "CONCLUSIONS", "text": "This research suggests that the United States will continue to be a linguistically diverse nation in the coming years. The projections we produced show that the use of LOTE is projected to increase over the next ten years, though English is expected to continue to be the only language spoken by a substantial majority of all U.S. residents 5 years and older. The population speaking Spanish, as well as the populations speaking Portuguese, Russian, Hindi, Chinese, Vietnamese, Tagalog, and Arabic are projected to increase. Spanish is projected to remain the most commonly spoken non-English language. The linear and logistic models suggest that the populations speaking French, Italian, German, Polish, and Korean can be expected to decrease over the next decade. The assumption of constant growth is likely overly simplistic, as it results in an increase in LOTE use for all languages, even those that are shown to decline in Census and in ACS data. The linear and logistic assumptions are perhaps more realistic, following observed trends, and provide results that are very similar. Since the logistic assumption is constrained within upper and lower bounds, and cannot produce projected percentages below zero or above 100, we may consider adopting the logistic model for use in future work. As we move forward with this research, we plan to add 2010 ACS data to the time series that provides the basis for these projections, extending the time series to five years. We will also use the 2010-Census based population projections when they become available. Increasing the sample size could reduce variation resulting from sampling variability and improve the robustness of our results. In an effort to increase the sample size of the age and Hispanic origin groups we project, we will consider projecting by age groups instead of single years of age or using three-year ACS files instead of single year files to form the basis of the time series. We will also consider projecting by birth cohorts instead of by age. A cohort approach will entail following cohorts of individuals as they grow older, instead of comparing language use of the population of the same age at different points in time. Studies have shown that language use can shift and change over the life course (Lutz, 2006;Ortman and Stevens, 2008;Portes and Rumbaut, 2001), which supports the adoption of a cohort approach to projecting language use into the future. We did not project language use by nativity or generational status. Research shows that the use of non-English languages is strongly linked to immigration and is most frequent among first generation residents (Alba et al., 2002;Rumbaut et al., 2006;Stevens, 1992). The Census Bureau's population projections do not currently separate the population by foreign and native-born status. Should projections by nativity become available, we could further develop our methodology to project by nativity status, which could inform and improve the accuracy of the language projections. Papers and Proceedings     \nA global vector autoregression is an ingenious structure for capturing international linkages between country-or region-specific error correction models. A GVAR is a versatile structure for characterizing international macroeconomic and financial linkages though multiple channels; it embodies a standardized economically appealing choice of variables for each country or region; it treats long-run properties through cointegration analysis in a systematic fashion; and it permits flexible dynamic specification through vector error correction modeling. The current paper re-examines the empirical underpinnings for GVARs, focusing on tests of parameter constancy that use impulse indicator saturation. Recent developments in computer-automated model selection allow implementation of impulse indicator saturation, even though historically IIS would have been viewed as infeasible. Empirical results from impulse indicator saturation show scope for improving the GVAR in DdPS and suggest directions to pursue for doing so."}, {"section_title": "I. INTRODUCTION", "text": "In the aftermath of the recent financial and economic crisis, rapidly increasing government debt around the world has generated increasing worries about economic growth. In 2010, the United States' total federal government debt outstanding was 92 percent of Gross Domestic Product (GDP), a share that has not been reached since World War II. Recent projections from the Congressional Budget Office (CBO) and the Office of Management and Budget (OMB) both predict government debt will rise above 100 percent of GDP in the near future. These projections have prompted concerns that the United States' debt burden will become unsustainable and have increased concerns about raising the debt limit. 1 However, upon closer examination there are considerable differences between the available debt forecasts. The intense focus on the United States' debt makes it increasingly important to understand how well the debt can be forecast. Furthermore, given the ongoing debate over the debt, it is important to know which forecast more closely tracks the trajectory of the debt. This paper aims at answering these questions. Using a time series of the United States' federal debt, this paper compares how well one-step-ahead debt forecasts from the CBO and the OMB have performed since 1984. The previous literature has extensively compared CBO and OMB forecasts. Overall these studies have mixed results. While some studies find that CBO forecasts are significantly better than OMB forecasts, others find that OMB forecasts are on par with CBO forecasts and even in a few cases the OMB forecasts perform better than the CBO forecasts. These findings tend to vary depending on the time period examined, the variable being forecast and the forecast horizon. This analysis adds to the collection in several ways. First and foremost, it extends forecast comparisons of the CBO and OMB to the federal debt, which has never before been examined. Second, it compares CBO and OMB forecasts against one another individually, with the Analysis of the President's Budget, and with averages of the agency forecasts. Finally, the analysis makes use of both the root mean square forecasting errors (RMSFE) as well as forecast encompassing to compare the forecasts. This allows for the determination of whether certain forecasts or combination of forecasts can outperform other forecasts. The analysis finds that all of the agency forecasts perform well except during recessions. Furthermore, it shows that a simple average of the agencies' forecasts performs better than either agency individually. The paper is structured as follows. Section II reviews the previous literature on OMB and CBO forecast comparisons. Section III provides a background to the forecast encompassing test used. Section III describes the data and some initial comparisons of the forecasts. Section IV presents the empirical findings and analysis. Section V concludes."}, {"section_title": "II. LITERATURE REVIEW", "text": "There is a considerable body of literature that compares CBO and OMB forecasts. These studies can roughly be broken into two different types. The more popular type typically uses the mean square forecasting error (MSFE), the mean absolute error (MAE), or the mean absolute percent error (MAPE) to compare forecasts between the two agencies. The second type uses forecast encompassing tests to compare the forecasts. Both of these types of studies help to compare forecasts from the two agencies in different ways. Using forecast summary statistics, the first group of studies compares forecasts from the two agencies and come up with a variety of findings. Kamlet, Mowery, and Su (1987) compare one-step and multi-step ahead forecasts from CBO, OMB, their ARIMA model, and the ASA/NBER model for the real growth rate, inflation rate, and unemployment from 1976 to 1984. They find that for short-term forecasts both agencies are \"accurate and unbiased\" and that neither of the forecasts \"outperforms the other in forecasting accuracy\". However, for forecasts extending beyond three years, they find OMB forecasts are \"more biased than those of CBO\" but are not \"less accurate than CBO projections\". Plesko (1988) examines the CBO and OMB forecasts of nominal GNP, current receipts, current outlays, and the deficit from 1974 to 1988 and finds similar results for the short-term forecasts. McNees (1995) compares forecasts from the Federal Reserve Board (FRB), the CBO, the Council of Economic Advisors (CEA) 2 , and private forecasters for inflation, GNP, and unemployment from 1976 to 1994. McNees finds similar results for long term forecasts, where the CEA forecasts were more biased than the CBO, FRB and private forecasts. Frendreis and Tatalovich (2000) compare CBO, OMB, and FRB onestep-ahead forecasts of GNP growth, inflation, and unemployment from 1979 to 1997. While all three agencies' forecasts tend to be close, they find the CBO forecasts to be the best, followed by the FRB, and then the OMB. The CBO conducts a semi-annual comparison of its forecasts with the OMB and private forecasts. The most recent update is CBO (2010), which compares two-year forecasts and five-year forecasts for output, inflation, three month Treasury rates, long-term interest rates, and wage and salary disbursements from 1980 to 2008. Similar to the previous studies, it finds that the CBO's two-year forecasts are as accurate as the OMB and private forecasts. The second type of study in the literature uses different types of forecast encompassing tests to compare forecasts and also has somewhat mixed results. Howard (1987) compares the CBO and OMB forecasts of the real GNP growth rate, GNP deflator, consumer price index, unemployment rate, and the three-month Treasury bill rate from 1976 to 1985. By regressing the residuals of the OMB forecasts on a constant and the residuals of the CBO forecasts, Howard finds that while errors for both forecasts are strongly correlated, the OMB forecasts are biased. Belongia (1988) compares the Council of Economic Advisors (CEA), the CBO, and private one-step-ahead forecasts of real GNP growth, the GNP deflator, and unemployment from 1976 to 1987. By regressing the actual growth rate of each variable on a constant and different pairs of predicted growth rates, Belongia finds that in general the private forecasts perform better than either the CBO or CEA while neither CBO nor CEA outperform one another. These results suggest that CBO and CEA forecasts may be encompassed by private forecasts of the same variables, but do not encompass one another. Cohen and Follette (2003) compare CBO, OMB and FRB one-step-ahead forecasts of the budget from 1977 to 2003. They regress the actual outcomes on OMB and CBO forecasts over different periods and find that for most samples, CBO forecasts encompass OMB forecasts. Douglas and Krause (2005) also compare CBO, OMB and FRB one-step-ahead forecasts of real and nominal GDP, inflation, unemployment, tax revenues, government outlays, and the budget deficit from 1976 to 2001. They use a variety of encompassing tests, and find that, with the exception of unemployment and tax revenues, the forecasts are not statistically distinguishable from one another. They find that the FRB forecasts perform better than either the CBO or OMB in terms of unemployment, while the CBO forecasts perform worse than either the OMB or the FRB in terms of tax revenues. Corder (2005) examines forecasts of GDP, inflation, unemployment, and interest rates from the Social Security Administration (SSA), the CBO, and the OMB between 1976 and 2003. Using two different tests to check for bias and efficiency, he finds that the CBO forecasts encompass OMB forecasts in terms of GDP, OMB forecasts encompass CBO forecasts in terms of unemployment and inflation, and neither encompasses the other for interest rates. As a result, he concludes that both agencies could improve their forecasts if they incorporated information from the other agency. Overall both types of studies comparing forecasts from the OMB and CBO have mixed results. While some studies find that CBO forecasts are significantly better than OMB forecasts, others find that OMB forecasts are on par with CBO forecasts and even in a few cases the OMB forecasts perform better than the CBO forecasts. For a summary of the previous studies see Table 1. Both the studies that use standard forecast summary statistics and those that rely on forecast encompassing tests have their limitations. Ericsson (1992) shows that while the MSFE is a necessary condition for ascertaining which forecast is better, it is not sufficient in determining whether one forecast can explain another forecast's errors (i.e., encompass it). On the other hand, CBO (2010) cautions against using statistical tests with such small sample sizes because, \"particular errors can have an unduly large influence on the measures\". Thus, rather than relying on one test or another, this analysis uses a forecast encompassing test along with the root mean squared forecast errors to compare government forecasts of the debt. By doing so, the risk of choosing a less powerful test is spread over a range of tests while allowing for a comparison of the results across tests."}, {"section_title": "III. FORECAST-ENCOMPASSING TESTS", "text": "The analysis in this paper relies on the root mean squared forecast errors to compare forecasts. It also uses the concept of forecast encompassing developed by Chong and Hendry (1986). They lay out a simple forecast encompassing test to see whether one forecast can better explain the dependent variable than another. Their basic framework is: where b 0 is a constant, y t denotes the actual value of the variable being forecast, x t denotes the one-step-ahead forecasts from the first agency and z t denotes the onestep-ahead forecasts from the second agency. Using this approach the null hypothesis tested is {b 0 =0, b 1 =1, b 2 =0} which suggests that the first agency's forecast provides complete explanation of the dependent variable over the second agency's forecast and is unbiased. The forecasts can also be rearranged to test whether the second agency's forecasts \"encompass\" the first agency's forecasts. The forecast encompassing test used in this analysis is a very general version of the forecast encompassing framework. Ericsson (1992), Ericsson and Marquez (1993), and Ericsson (1993) provide further variants of this framework. These alternative forecast encompassing tests accommodate different forecast properties and test more specific hypotheses. For a more detailed description of these variants see Martinez (2011)."}, {"section_title": "IV. DATA SOURCES AND DESCRIPTIONS", "text": "This section describes the data used in the analysis and provides some visual comparisons of the forecasts. The primary variable of concern is the log of total gross federal debt outstanding held by the public and the government in billions of dollars from 1984 to 2010 (LDEBTB). This data is published by the U.S. Department of Treasury's Financial Management Service and is measured on a fiscal year basis ending on September 30 th . 3 The federal debt is often overlooked because the change in public debt is usually thought to be equal to the deficit. However, changes in the public debt also include other off budget items that are not included in the deficit; a recent example of this is the Troubled Asset Relief Program (TARP). Furthermore, the federal debt includes debt held by the public, debt held by the government, and agency issued debt. Therefore, it is important to look at the debt since looking only at the deficit would miss other changes to the federal debt. The remaining variables in this analysis span from 1984 to 2010 and come from the annual releases of the CBO's Budget and Economic Outlook, the OMB's Budget of the United States Government, and the CBO's Analysis of the President's Budget (hereafter APB). These forecasts are typically released at the beginning of the year, usually between January and March, and contain forecasts through to the end of the fiscal year. It is important to note that these forecasts are constrained by different conditions. The CBO forecasts rely on the assumption that current law will be unchanged over the forecast horizon. On the other hand, the OMB and APB forecasts assume that the policy changes proposed in the president's budget will be implemented. Therefore, these different assumptions may lead to some of the differences between the forecasts. For more information on the forecasts and their release dates see Table 2 in Martinez (2011). The primary variables of interest from the CBO, OMB, and APB are the log levels of the one-step-ahead federal debt forecasts in billions of dollars (LCBODF1, LOMBDF1, and LAPBDF1). Figure 1 plots these forecasts, together with the actual debt to provide an initial view of how they perform. Figure 2 plots the forecast errors, which are generated by subtracting the forecasts from the actual debt. The largest forecast errors for all three forecasts were in 1990, 2001, 2002, 2008, and 2009. These errors make intuitive sense given that during each of these years, the United States was either entering or in the midst of a major recession. 4 For further comparison, the analysis also includes an average of the CBO and OMB forecasts (Average 1) and an average of the OMB and APB forecasts (Average 2). Comparing the individual agency forecasts with the averages provides an additional test of whether a combination of the forecasts could improve upon the individual agency forecasts. For more information on each of the data series and their sources see Table 2."}, {"section_title": "V. ANALYSIS AND RESULTS", "text": "Insight can be gained by comparing the bias, error variance, and the root mean square forecasting errors (RMSFE) for the different forecasts with one another and with forecasts from a random walk model (Table  3), and over different subsamples. For the period 1984-2010, all of the agency forecasts and their averages perform better than the random walk model. When the analysis is restricted to only the CBO, the OMB, and the average of these two forecasts (Average 1), the OMB forecasts have the smallest RMSFE for the subsample 1984-2008 (1.46), followed by the average forecasts (1.54), while the CBO forecasts have the highest RMSFE (1.73). 5 For the samples 1984-2009 and 1984-2010, the Average 1 forecasts have the smallest RMSFE, followed by the CBO forecasts. When this analysis is extended to include the APB forecasts and the average of the APB and OMB forecasts (Average 2), the results change. Regardless of which sample is chosen for the analysis, the APB forecasts outperform all of the other forecasts with the lowest error variance and RMSFE. The next lowest RMSFE is Average 2 even though Average 1 always has the lowest overall bias. Thus, the RMSFEs suggest that the APB forecast performs best, followed by the averages of the two forecast combination pairs. Forecast encompassing tests provide an alternative method of comparing the same forecasts. The analysis initially compares the OMB and CBO forecasts. Then it examines the OMB and APB forecasts. Following the form of equation 1, the regression (2) results when the actual debt is regressed on a constant and the OMB and CBO forecasts through 2010: 4 For exact recession dates see NBER: http://www.nber.org/cycles/cyclesmain.html 5 The average forecast has a bias that is slightly closer to zero than the OMB forecast. In this case negative indicates a tendency to over project the debt while positive indicates a tendency to under project the debt. where the estimated standard errors are in parentheses. The null hypothesis of {b 0 =0, b OMB =1, and b CBO =0} tests whether the OMB forecasts completely explain the debt while testing that the CBO forecasts do not provide any explanation and that there is no bias. If the null hypothesis is not rejected, then it suggests that the OMB forecasts encompass the CBO forecasts. In this case the null hypothesis is rejected, as seen by the coefficients and their error terms. The coefficients indicate that the CBO forecasts explain around two thirds of the debt while the OMB forecasts explain about one third of the debt. This suggests that both forecasts provide unique albeit partial explanations of the actual debt with neither agency's forecasts completely encompassing the other (For the other equations used, see Table 4). Equation 2can also be used to test whether the CBO forecasts encompass the OMB forecasts, which corresponds to a null hypothesis of {b 0 =0, b OMB =0, and b CBO =1}. Table 5 shows the results of these tests, along with the tests that compare the average of CBO and OMB forecasts with the OMB and CBO forecasts individually. The results illustrate that the CBO forecasts also fail to explain the debt completely without the help of the OMB forecasts. The F-test is 7.887 with a p-value less than 1% in the first comparison and 3.237 and at 5% respectively for encompassing of the OMB by the CBO. On the other hand, the results suggest that the average of the two agency forecasts does encompass either of the individual forecasts. This illustrates that, through 2010, an equal combination of the OMB and CBO forecasts captures the pertinent information from both of the agency forecasts. The second equation from Table 4 can be used to compare the OMB and APB forecasts, in the same way as was done for the OMB and CBO forecasts. The additional results from Table 5 suggest that the OMB forecasts do not encompass the APB forecasts. Similarly, the evidence rejects the possibility that the APB forecasts could encompass the OMB forecasts (albeit at a weaker level of confidence). Furthermore, the results show that the average of the APB and OMB does not encompass the individual agency forecasts. Therefore, when comparing the OMB and APB, neither forecast strongly distinguishes itself from the other in a way that would allow one to encompass the other or an average of the two forecasts, as was seen with the OMB and the CBO. The average of the CBO and the OMB forecasts is better at forecasting than the individual agency forecasts, suggesting that the average is more robust to changes in the economy than the individual agency forecasts. This is supported by Clements and Hendry (2004), who show that pooling forecasts can add value when individual forecasting models are differentially mis-specified. Furthermore, Hendry and Mizon (2005) illustrate that there may be a need to pool across forecasting and policy models when there are structural breaks or policy regime shifts. As a result, individual forecasts' weaknesses can be ameliorated by combining them. Based on the forecast encompassing test, the APB forecasts do not perform significantly better than the OMB forecasts. However, the APB forecasts have the smallest RMSFE. The APB forecasts could also been seen as a quasi forecast combination, in that they are created by the CBO while using the OMB's assumptions. Therefore, the APB forecasts could also be seen as an average of sorts and its performance could benefit from this combination. There are several possible explanations for why the individual forecasts may be differentially mis-specified. One possible reason stems from the different forecast release dates, which allows the later forecast (typically OMB) to incorporate newer information. Even a small difference in release dates could have a significant impact during a recession or in the midst of large economic and policy changes. Another explanation could be the different assumptions, mentioned above, that the agencies make when producing their forecasts. These assumptions, especially when policies are changing significantly, could lead to large differences in the forecasts. While it is unclear exactly which characteristics of the individual OMB and CBO forecasts lead to their divergence, it is clear that a combination of the forecasts can limit their susceptibility to change. For more information on differences between the OMB and CBO forecasts see Martinez (2011)."}, {"section_title": "VI. CONCLUSION", "text": "This paper compares one-step-ahead debt forecasts from the Congressional Budget Office and the Office of Management and Budget over the past 27 years. While the analysis only looks at the root mean squared forecast errors and one variant of the forecast encompassing test, the results are consistent with the results of other variants of forecast encompassing test, using different samples, and when accounting for potential structural breaks and policy shifts. For these and other results see Martinez (2011). Overall, the agency forecasts all perform better than a simple benchmark forecast model in terms of the RMSEs. While the CBO forecasts outperform the OMB forecasts through 2010 in terms of the root mean square forecast errors, they fail to completely encompass the information available in the OMB forecasts. Furthermore, the average of the agency forecasts have a lower RMSFE and encompass both the CBO and OMB forecasts individually. While the APB forecasts have the lowest overall RMSFE, there is little evidence to suggest that they can fully explain the debt without the help of the OMB forecasts. In conclusion, while both the Congressional Budget Office and the Office of Management and Budget's forecasts are relatively successful in forecasting the debt, each agency's forecast remains incomplete and could benefit from further information that the other agency takes into account. When only one of the agency's forecasts is used, there is an incomplete and potentially distorted picture of the future levels of the government debt. The evidence is mixed as to whether the Analysis of the President's Budget can help to improve upon the other two agency forecasts. While the APB forecast is effectively a quasi combination of the two forecasts in that it includes information from both agencies in its forecasts, the evidence suggests that it does not clearly outperform the OMB forecasts. On the other hand, while this analysis did not directly compare the APB forecast with the average of the CBO and OMB forecasts, the root mean square forecast errors indicate that the APB performs best. Even so, the APB's forecast is released up to several months after the CBO and the OMB forecasts, which reduces its effectiveness for policy making despite the improved information content. Therefore, an average of the two agency forecasts produces the best and most timely forecast of the debt. As a result, it is important that information from both agency forecasts of the debt are taken into consideration to better forecast the future levels of the United States' gross federal debt.   Notes: 1. The three entries within a given block of numbers in the last five columns are: the approximate F statistics for testing the null hypothesis, the tail probability associated with that value of the F statistic (in square brackets), and the degrees of freedom for the F statistic (in parentheses). 2. Asterisks * and ** denote rejection at the 5% and 1% critical values.  "}, {"section_title": "VIII. TABLES", "text": ""}, {"section_title": "IX. FIGURES", "text": ""}, {"section_title": "I. Introduction", "text": "The Statement of Actuarial Opinion in the 2010 Medicare Trustees' Report to Congress cautioned that: \"...the financial projections shown in this report for Medicare do not represent a reasonable expectation for actual program operations in either the short range...or the long range...because of the strong likelihood that the statutory reductions in price updates for most categories of Medicare provider services will not be viable.\" Specifically, the official 2010 Medicare projections included an adjustment to reflect annual price updates that will reduce payments for most non-physician services by the growth rate of economy-wide multifactor productivity under a provision of the Patient Protection and Affordable Care Act (PPACA). The Actuarial Opinion concluded that health care providers would not be able to achieve improvements in productivity in line with the annual price adjustments mandated by PPACA, and hence that the official Trustees' projections \"are not reasonable as an indication of actual future costs\". At least with respect to the long term, however, the 2010 Medicare projections and the Actuarial Opinion were based on a methodological error. Trustees' projections made prior to 2010 already included an assumption equivalent to the PPACA productivity adjustment. In fact, the long run parity of medical sector and economy-wide multifactor productivity is explicit in the economic model used to generate the Medicare long term growth pattern. 1 To explain the source of confusion in the 2010 projections this paper: (1) Gives a brief history of U.S. medical spending growth (section II). (2) Describes the Medicare long term projection methodology and explains its underlying productivity growth assumptions (section III). (3) Explains the economic model used to develop Medicare's long run expenditure growth pattern, and constructs corrected projections of Medicare Part A expenditures (Section IV)."}, {"section_title": "II. Historical U.S. Medical Spending Growth", "text": "The practice of medicine, like many other technologydriven fields, has experienced tremendous advances over the last century. Unlike other industries, however, medical technological change has been accompanied by significantly higher costs and an increasing share of total final demand. In 1961 NHE accounted for about 5 percent of GDP, twenty years later it approached 10 percent, and by 2009 it was approximately 17 percent. In fact, medical care is virtually unique among major categories of consumption in that over many decades real medical consumption has grown faster than total consumption, Between 1960 and 2009, health spending growth has exceeded GDP growth by about 2.5 percent per year, and Medicare growth (since the inception of the program in 1965) has exceeded total economic growth by 4.3 percent. 1 The calculations shown in this paper apply to the 2010 Medicare Trustees' Report to Congress. However, the basic arguments apply to the 2011 Report, and corrections to the 2011 Medicare projections would be of the same approximate magnitude (in percent terms) as those presented here. 2 1960 is the first year for which an uninterrupted time series of National Health Expenditures is available. Source: Centers for Medicare and Medicaid Services. even though medical prices have been rising faster than other prices. 3 The factors that explain the steadily rising medical share of total final demand can be classified in two general groups: price effects and quantity effects. The former refers to the phenomenon that, since medical prices have historically increased much faster than nonmedical prices, the current dollar medical expenditure share has increased (independent of the increase in constant dollar medical care consumption). Figure 2 shows the ratio of medical spending growth (as measured by the Bureau of Labor Statistics medical price index) and All-Urban Consumer Price Index (CPI) growth. Since 1985 medical prices have grown about 2 percent faster per year than the general level of inflation. Quantity effects refer to factors that have increased the demand for medical services. Broadly, there are three factors that have historically driven the medical spending quantity effect. The first factor is the expansion of health insurance, both as a result of government insurance programs and in the private sector (primarily through employer-provided coverage). The second factor is real income since as income rises consumers tend to increase the consumption of everything, including medical care. The third factor is medical technological innovation. In principle, medical innovation can have both expenditure-decreasing and expenditure-increasing effects. Expenditure-decreasing innovations reduce treatment costs, e.g., the development of a polio vaccine virtually eliminated the costs associated with treating a life-long crippling disease. Expenditure-increasing innovations raise costs either by introducing therapies for previously untreatable conditions or by providing more expensive alternatives to existing therapies (with presumably greater health benefits), e.g., a new procedure that expands surgical intervention to a larger population of patients. There is almost universal agreement among health economists that the dominant effect is expenditureincreasing. Estimates of the technology effect suggest that 25-75 percent of the historical increase in medical spending can be ascribed to technological change alone. proposition that the bulk of the increasing medical share of final demand can be ascribed to medical technological innovation. 5"}, {"section_title": "III. Medicare Projection Methodology", "text": "The Medicare Projections can be divided into short (1 -10 years into the future), intermediate (11 -24 years), and long term (25 -75 years) timeframes. In the short term, projections for Medicare Parts A (hospital insurance), B (medical insurance), and D (prescription drug insurance) are developed separately based on information about various categories of health spending and assumptions about relative medical price inflation and rates of utilization. Therefore, short term forecasted \"excess cost growth\"--that is, the degree to which per capita cost growth, net demographic effects, exceeds per capita GDP growth--are different for each of Parts A, B, and D. In the intermediate timeframe the separate excess cost growth forecasts are gradually adjusted so that by the 25th year of the projection, they are merged to a single common path that continues into the long run. (Caldis, 2008). In 2000, the Medicare Trustees convened a Technical Review Panel to study ways to improve the forecast process. A key Panel recommendation was that projected per capita medical spending, net age and gender effects, should grow one percent faster than per capita GDP over the long run time horizon. The growth rule, often referred to as \"GDP + 1\", was adopted in 2001 and is the current underlying long term growth assumption. It is important to understand that GDP + 1 reflects the impact of medical technological innovation. Specifically, the 2000 Panel estimated that one-half of real medical expenditure growth was due to technological innovation. \"Attributing 50 percent of the median 4.4-percent growth in real per-capita NHE\u2026 to progress in medical technology, the Panel estimates a 2.2-percent growth in real per-capita NHE in the future due to medical technology alone. Subtracting\u2026real percapita GDP yields a differential of 1 percent.\" The Panel was quick to point out, however, that GDP + 1 was at the \"lower end of the reasonable range because it assumes that non-technological factors will not continue to contribute to health care expenditure growth.\" In the context of the error in the 2010 Medicare Projections, medical sector price inflation is a particularly important non-technology factor. Long run price inflation, in turn, is related to the relative levels of productivity growth. If medical productivity increases at a slower pace relative to the rest of the economy, then medical care will be more expensive relative to other goods and services, and, as we will demonstrate in simulations below, the medical share of GDP will tend to be higher, ceteris paribus. Overall, the 2000 Panel concluded that long run productivity growth parity was a reasonable assumption, remarking that \"better control of prices by public and private payers, along with properly measured price indices, will result in future increases in health care prices that approximate wage and price growth in the overall economy\u2026[c]urrently, the use of conventional health care price indices gives an overly pessimistic view of productivity gains in medical care.\" To recapitulate, the 2000 Medicare Technical Review panel made the following recommendations (which were later adopted by the Medicare Trustees): (1) The long term medical spending (and, hence, Medicare) growth assumption, net of age and gender effects should be the rate of real per-capita GDP growth plus one percent (\"GDP + 1\"). This reflected the historical record that suggested that about one-half of historical medical spending growth was due to technological innovation. Multiplying observed real medical spending growth by 50% and then subtracting real GDP growth yielded the one percent differential. 6"}, {"section_title": "IV. Alternative Medicare Spending Projections", "text": "(2) In the long run, medical sector productivity growth should approach economywide productivity growth. Therefore, medical price inflation should be comparable to economy-wide inflation (at least, in the long term future). (3) Other factors, such as the expansion of health insurance coverage, would not continue into the future, and therefore would not be a source of future medical spending growth in excess of GDP growth. While GDP + 1 was a significant improvement over previous assumptions, it had three weaknesses. First, GDP + 1 itself is inconsistent with historical health-GDP growth patterns. For example, the 2000 Panel noted that the average annualized difference between real National Health Expenditure (NHE) and real GDP growth was 2.7 percent for the post-World War II period. Second, if extrapolated out far enough, the rule implies that virtually the entire economy would consist of health care. Third, the simple growth rule is not linked to explicit theories of consumer behavior and economic production. The basic problem with GDP + 1 is that it is a mechanical growth rule-there is nothing in the rule to slow future spending growth once the initial trajectory is established. This is illustrated in Figure 3, which is taken from the 2000 Medicare Technical Review Panel report. For example, under \"GDP + 1\", the NHE share of GDP is a reasonable 38 percent in 2075. However, under \"GDP + 2\", the NHE share is an implausible 80 percent of GDP by 2075."}, {"section_title": "Borger, Rutherford, Won General Equilibrium Model", "text": "In 2006 the Medicare Trustees' and the Centers for Medicare and Medicaid Services, Office of the Actuary (CMS OACT) adopted a dynamic computable general equilibrium model developed by Borger, Rutherford and Won (BRW, 2008) that partially addressed these problems. (Borger, et al., 2008) The BRW model introduced a formal framework to explain how new technologies affect the demand for medical care. Key was the development of calibration methods that allowed model parameters to be reconciled with empirical results from previous health economics research. Although technological change is difficult to directly measure in the aggregate, the model equations allow one to infer what the demand effect of technological innovation must have been in order to explain the observed historical pattern of medical spending growth given income and price elasticity estimates derived econometrically and from the literature. For the purposes of the official Medicare projections the model is constrained in three ways. First, overall economic growth is targeted to the Social Security Trustees' edlong term real GDP growth projection. Second, medical sector productivity growth is equal to economy-wide productivity growth in the long term. Third, the BRW medical spending projection is computed so that it is consistent with the same Medicare Part A actuarial balance as GDP + 1. Figure 4 shows a BRW model simulation of historical and projected real GDP prepared for this paper. The simulation results below focus on the effect of our corrections on the financial projections of Medicare Part A. Typically, the status of the program is characterized by the actuarial balance, defined as the difference between average income and cost rates for a given period. In the case of an actuarial deficit, this difference can be interpreted as the number of percentage points by which the payroll tax rate must be raised (or cost rates must be lowered) in order to resolve the program imbalance over the valuation period. In summary, the long term projection process consists of the following steps: Step 1: Formulate the long run excess cost growth pattern, by running the BRW model under the three constraints: Social Security Trustees' GDP projection, medical / non-medical productivity growth parity, and Medicare Part A actuarial balance. Step 2: Incorporate demographic effects. Step 3: In the intermediate timeframe gradually transition the short term projections to the long term growth pattern. Step 4: As discussed above, the 2010 Trustees' Report included an additional adjustment to account for the multifactor productivity price update under PPACA. Thus, the Medicare Part A growth rate assumptions were determined as described in steps 1 through 3, \"minus the full amount of the 10-year average productivity increase [estimated to average 1.1 percent].\" 7 At this point, however, it should be clear that the underlying actuarial methodology prior to PPACA already assumed that there would be no long term differential between medical sector and economy-wide productivity growth, and that this assumption was specifically coded into the BRW model. \"GDP + 1\" is a constant dollar assumption that does not factor in relative medical price inflation. (Shatto and Clemens, 2010) "}, {"section_title": "Corrected 2010 Medicare Part A Projection", "text": "Under the corrected 2010 Part A projection, we assume that PPACA is successful in restraining Medicare cost inflation, and that, in the long term, medical sector productivity growth is equal to that of the rest of the economy. As noted above, this assumption was already explicitly coded into the CMS OACT version of the BRW model prior to 2010, therefore no further adjustment was necessary. The corrected 2010 projection, then, simply removes the 1.1 percent adjustment in the long term timeframe. After correction, Medicare Part A expenditures for 2010 through2084 are estimated to be $23.3 trillion at present value, about 36 percent greater than the 2010 official projection of $17.1 trillion. The corrected actuarial balance, then, is -2.28 percent compared to the official projection of -0.66 percent. Although the corrected figures are significantly higher than the original 2010 Trustees expenditure projections, they are still much lower than the official 2009 forecast."}, {"section_title": "Corrected CMS OACT Alternative Projection", "text": "Based on CMS OACT research, the 2010 Trustees' Report Actuarial Opinion indicated that it was unlikely that the price adjustments under PPACA could be sustained in the long run. The Opinion pointed readers 7 Prior to PPACA, most Medicare prices were based on input price indices reflecting the costs of factors of production. In principle, then, it would be logical to apply a productivity adjustment to approximate the increase in output prices required by providers to maintain their margins. Under PPACA, the input-based Medicare price update is adjusted to reflect economy wide productivity gains. Based on historical trends, the Medicare Actuaries projection assumes that this adjustment would reduce prices by 1.1 percent per year. The actuaries' projected adjustment, however, only makes sense if the pre-PPACA projections reflected a productivity-gap between Medicare price inflation and economy-wide inflation of at least 1.1 percent. to an alternative \"Illustrative Scenario\" projection, in which the PPACA multifactor productivity adjustments are applied fully through 2019, but then phased out over the 15 years beginning in 2020. Following the CMS OACT rationale, our corrected alternative scenario posits that productivity growth parity is an implausible long term assumption. Therefore, in our corrected CMS OACT illustrative scenario alternative, we re-run the BRW model and incorporate a 1.1 percent differential between medical sector multifactor productivity growth and productivity growth in the rest of the economy. The productivity differential affects medical expenditures by increasing the relative cost of medical care. ( Figure 5 shows the BRW model productivity differential used to generate the results below.) The CMS OACT alternative scenario does not account for this effect, therefore our corrected forecast results in Part A expenditures that are approximately $3 trillion higher than CMS OACT, at present value. Figure 6 compares Medicare Part A costs as a percent of taxable payroll in the official 2010 Trustees' Report and the CMS OACT illustrative scenario alternative."}, {"section_title": "Revised 2009 Trustees' Medicare Projection", "text": "If productivity growth parity is an unreasonable assumption, then it follows that Trustees' projections prior to 2010 are also incorrect. In the third set of simulations, we apply recomputed BRW model results to the 2009 projections, again assuming a 1.1 percent average productivity growth differential. Accounting for lagging medical sector productivity would have increased the Trustees' 2009 Medicare Part A expenditure projection by approximately 6 percent; $27.3 trillion in our revised forecast versus $25.8 trillion in the official forecast. Our revised 2009 actuarial balance forecast is -4.31 percent, relative to the official 2009 forecast of -3.88 percent. The Table summarizes our results, comparing the original cost and actuarial balance projections to our corrected forecasts."}, {"section_title": "Conclusions and Future Directions for the Medicare Projections", "text": "The errors in the Medicare Trustees' 2010 projection and the CMS OACT alternative highlight the urgent need to upgrade current Medicare long term forecasting procedures to include: expanded use of economic modeling, explicit identification of key forecast assumptions, and regular re-evaluation of those assumptions by economic, actuarial and health industry experts. An important benefit of general equilibrium modeling, in particular, is that it enforces consistency from a macroeconomic accounting perspective that is absent in GDP + 1. Formal modeling, in turn, would permit more systematic evaluations of the long term effects of Medicare policy options.        Using the Carlson-Parkin framework and employing the Pesaran-Timmerman Predictive Failure statistic, this paper evaluates several CES/IFO consensus forecasts. Examining issues related to interpreting qualitative survey responses, this paper defines what an \"about the same\" response implies across different economic variables, the value of agreement across the forecast panel, and how to maximize the signal value provided by the survey. This paper finds that survey respondents provide statistically significant directional forecasts, that forecaster agreement does not aid in determining the magnitude of shifts, and that the survey respondents as a group tend to miss downward turning points."}, {"section_title": "Consensus and Survey Forecasts", "text": ""}, {"section_title": "Measuring Green Economic Activity Ricardo Limes, Bureau of Labor Statistics", "text": "There has been much interest lately in the green economy and green jobs both inside and outside of the United States. Several State government-sponsored surveys have been conducted over the past couple of years. While all of the green surveys have measured green activity in the economy, they have used a variety of definitions and measures. The Bureau of Labor Statistics (BLS) is currently collecting and producing information on the occupational employment and wages of green jobs. After researching previous survey efforts BLS settled on a definition of green jobs and green activities. BLS is using two approaches to measuring green jobs: the output approach, which identifies establishments that produce green goods and services, and the process approach, which identifies establishments that use environmentally friendly production processes and practices. Two establishment surveys are underway: a recurring industry survey -the Green Goods and Services Survey as part of the output approach, and as part of the process approach, a special topic survey on green process jobs called the Green Technologies and Practices (GTP) Survey. The GTP Survey will measure jobs in which workers' duties involve making their establishment's production processes more environmentally friendly or use fewer natural resources. This presentation will cover previous green survey efforts in the US and abroad, the measures used by BLS for their green surveys, and the development work that BLS has done for the GTP survey. The consensus of private forecasters has previously been shown to consistently beat na\u00efve rules in predicting economic indicator releases. However, the race is neck-and-neck when the consensus competes against real time regression models that are presumably similar to what forecasters use. Despite a finding that forecasters' predictions are biased by over cautiousness, prediction errors are smaller when the consensus is added to the regression models, suggesting a combination approach to forecasting. This result could be interpreted as: the value-added of forecaster judgment, the ability of the consensus to incorporate a wider range of information, or simply as remaining model misspecification. "}, {"section_title": "Forecasters vs. Models: A Horse Race on Monthly Indicator Releases", "text": ""}, {"section_title": "Introduction", "text": "Economic data releases can move financial markets, to varying degrees. Therefore, many private-sector economists attempt to predict a release's headline number ahead of time. Surveys collect these predictions and publish a consensus. For monthly data releases, some companies which do this are Dow Jones News, Bloomberg, Thomson Reuters, Market News International, and Money Market Services/Action Economics. Previous studies of these consensus forecasts have examined their rationality using tests of unbiasedness and autocorrelation [Aggarwal, Mohanty, and Song (AMS) 1995;Moersch 2001;Schirm 2003;Campbell and Sharpe 2009;Pandl 2010]. If the consensus is rational, then the difference between the consensus and the data release can be said to accurately reflect the new information that the markets should be reacting to. Non-rational forecasts theoretically ought to be able to be improved using existing information. Section I. below tests the rationality of the consensus forecasts for five economic data releases, and compares the results to these earlier studies. Section II. compares the accuracy of the consensus to various mechanistic models."}, {"section_title": "I. Tests of Unbiasedness and Efficiency", "text": "Forecasts which are rational ought to be both unbiased and efficient. A test of unbiasedness is to run the regression (1) A t = \u03b1 + \u03b2F t + \u03b5 t where A t is the reported outcome and F t is the consensus forecast of the data release. If the forecast is unbiased, then it will be distributed randomly around the actual, and so we should see \u03b1 = 0 and \u03b2 = 1. If \u03b2 > 1, this is could be an indication of forecaster over cautiousness. If the forecast is efficient, then \u03b5 t should be white noise, so that it is not affected by any previous errors. Thus, in the regression (2) \u03b5 t = a 0 + a 1 \u03b5 t-1 + \u03c5 t we should see a 0 = a 1 = 0. The presence of autocorrelation is usually an indication of an omitted variables problem. In this case, a finding of autocorrelation suggests that additional variables need to be added to (1). This paper uses the Dow Jones News consensus forecasts, and along with Pandl, finds that forecast unbiasedness and efficiency are rejected for most of the data series tested, whereas Schirm and Moersch do not. Table 1 shows that unbiasedness is rejected in time periods when volatility (as measured by the standard deviation) is higher. There is also some evidence that forecasting is easier and more rational when the autoregressive correlation in the data series is high and positive. For example, for the series in Table 2, the \u03b2 value climbs when the correlation falls, even though the standard deviations are much lower in the Schirm and Moersch time periods."}, {"section_title": "II. A Better Test of Forecaster Value-added", "text": "Moersch and Pandl test the accuracy of the forecasters' consensus by comparing their performance against a random walk and a three or six-month moving average, and find that the consensus forecast beats them handily. Although it may be difficult for long run forecasts to beat a random walk, it is doubtful this is the case for the very short run because of the availability of relevant market information. In fact, this paper also finds that forecasters win easily against na\u00efve models when making monthly predictions for five economic data releases. A tougher test for forecasters might therefore be in order. In Table 3, the consensus is tested against a range of five mostly mechanistic models: a random walk, a six-month moving average, a five-year moving average, an ARMA model, and a \"learning\" regression model where explanatory variables are added to the regression only as they become significant. All of these are evaluated with only real time data 1 So, the forecasters win easily against all models except for the learning regressions, where the two finish neckand-neck and split the results. Interestingly, column 7shows that when the forecasters' consensus is included as a variable in the learning regression, the prediction error improves so as to beat both the learning regression and forecasters separately for four of the five data series. In column (8), the ex-post regression by itself beat the forecasters in four out of the five series. This is not greatly surprising, given the fact that a model that is formulated after-the-fact is estimated to conform to the entire sample period of the existing data. Once the consensus forecast is added as a variable in this regression in column (9), the improvement is even greater. The fact that forecasters add value even to the ex-post regressions suggests that they are contributing something that these specifications do not reflect. This suggests that the best predictions may be made by combining the consensus with a regression model for most data series. in order to make the competition fair. It is also instructive to see which explanatory variables drop out of the regression when the consensus is added. (See the appendix for a list of all the explanatory variables used in each regression along with their tstatistics.) This may be indicative of which variables forecasters are paying attention to and which they may be ignoring. For example, in the retail sales regression, once the consensus forecast is added, the change in unit motor vehicle sales in the current month becomes insignificant. This is not surprising, as motor vehicles are a large part of retail sales and this data is easily available before the release. However, what is surprising is that the change in unit motor vehicle sales from the previous month is still significant. This may indicate that forecasters are not considering this data to be important. Overall, it is also surprising how many explanatory variables are still significant once the consensus forecast is included. For many of them, the estimated coefficient is less in absolute value, but the fact that they are still significant may represent information that forecasters are not fully taking into account. In further confirmation of section I, when the data series in Table 3 are broken into sub-periods, it is seen that forecaster errors across time periods are related positively to series standard deviation, and inversely to series correlation (table not shown)."}, {"section_title": "Conclusion", "text": "We have seen that forecasting is easier, both in terms of reduced errors and reduced bias when the series being forecasted are less volatile and more autoregressive. We have also seen that although a regression parameterized after the fact (almost) always beats the consensus forecast, the consensus still wins or closely follows a regression or any other model when the need for learning is taken into account. This implies that just because a regression model can be formulated today which beats the consensus in the past doesn't mean that it will continue to do so in the future. It is surprising, however, how many explanatory variables remain significant in the regressions even after the consensus forecast is added. Perhaps this represents information that forecasters are not fully taking into account, or evidence of caution bias. But the fact that a regression does better with the consensus forecast than without it suggests that forecasters may have information not represented solely by these regression variables. This is another way of saying that the regressions are still misspecified in some way, but it may not be the case that this misspecification can be easily corrected. That is why the best forecasts may come from a regression combining the consensus forecast with other explanatory variables. This is the horse that seems likely to win on average in the future.  Unbiasedness is rejected at the 5% significance level for bolded results above. Notes: \"Learning\" regression is a model where intuitive explanatory variables are only added as they become statistically significant. In the \"ex-post\" regression, all variables that either start or become significant at some point in the sample period are included in the model specification from the beginning. The challenges of the twenty-first century are the exhaustion of nonrenewable resources and the growth of national debt. There are two national policy alternatives: 1) Attempt to ignore the inevitable and keep promoting growth, and 2) Ease the transition to a steady state economy. Option 1 will fail because resources will become more scarce and their prices will rise. Option 2 consists of supporting research on obtaining both materials and energy from limited renewable resources. Catastrophes can be avoided by recognizing problems before they develop into crises. Politicians, economists, business people, and editorial writers are pondering the end of the most recent economic downturn, the \"Great Recession,\" and asking when vigorous economic growth will resume. And while it is true that the recession has ended, there has been no strong spurt of growth that brings the GDP back to the level of its long-term trend. See Figures 1 and 2.\nWe study the effects of increasing electricity price increases and an increase in temperature in the US mountain region on electricity consumption and CO2 emissions. Using cointegration and a general-to-specific modeling approach we developed a long-run electricity demand model. One cointegrating vector was found and signs for the long-run elasticities held according to economic theory. Using these elasticities, we were able to run two simulations. The simulations examine the possible effects from the reference case in EIA's NEMS projections in 2030. The first simulation was based on 10% increase in the price of electricity. Our results suggested that this would reduce consumption in 2030 by almost 2% and CO2 emissions by slightly more than 2%. The second was based on a two degree Fahrenheit warming. This offset the projected gains in efficiency of electricity consumption per household. Moreover, the growth in CO2 emission doubled from the NEMS reference case. These simulations showed how changes in prices or weather affect the consumption behavior of consumers, which in turn affect the emissions produced by electricity consumption. The simulations were performed using the Reference case model run for NEMS 2010. Aeo2010r1118a.xls. We obtained date from tables: Table 3. Energy Prices by Sector and Source, (2008 dollars per million Btu, unless otherwise noted) Table 4. Residential Sector Key Indicators and Consumption,(quadrillion Btu, unless otherwise noted) Table 8. Energy Consumption by Sector and Source,(quadrillion Btu, unless otherwise noted)      a For a variable x, the ADF (1981) statistic, ADF(k) is the t ratio on \u03c0 from the regression where k is the number lags on the dependent variable; \u03b1 0 is the intercept; \u03a0, {\u03b3 t }, \u03b1 1 through \u03b1 12 are coefficients; the {s it } are centered seasonal dummies; t is a trend; and e t is an error term. For a given variable and null order of I(1), two values are reported --the 12th-order (k=12) ADF statistic and in parentheses the estimated coefficient on the lagged variable x t-1 --for variables in the levels. That coefficient should be 0 under the null hypothesis that x is I(1). For a given variable and null order of I(2), two values are reported for the first difference of the variables. A rejection implies that the first difference of the series is a stationary process. b The sample for the ADF test was from 1992(3) -2009(12) and all variables are in natural logarithms.   7  12  1  ,  7  11  2  ,  6  10  1  ,  6  9  2  ,  5  8  1  ,  5  7  2  ,  4  6   1  ,  4  5  2  ,  3  4  1  ,  3  3  1  ,  2  2  1  ,  1  1         The community-based reorganization of the globalizing agro-food system into more localized food systems has been discussed and implemented over the last few decades as a means toward environmental and economic sustainability for consumers and producers, as well as a means toward greater food security and safety (Delind and Ferguson, 1999;La Trobe and Acott 2000;Norberg-Hodge 2002). Direct marketing strategies (DMS) have emerged and grown through outlets such as community supported agriculture farms (CSA) and buying clubs, farmers markets, on-farm and roadside stands, you-pick operations, consumer cooperatives, locally branded commodities as well as direct marketing to grocers and retailers (Kohls and Uhl, 1998;Buhr, 2004). USDA's \"Know Your Farmer, Know Your Food\" (KNF 2 ) initiative builds on the 2008 Farm Act to strengthen Federal programs promoting local foods and includes plans to enhance direct marketing and farmers' promotion programs, to support local farmers and community food groups, to strengthen rural communities and to promote local eating. The KNF 2 website (http://www.usda.gov/knowyourfarmer) lists opportunities for farm loan programs such as direct and guaranteed ownership loans for beginning farmers and socially disadvantaged groups, farm storage facility loans, value-added producer grants, beginning farmer and rancher development programs, and technical assistance and marketing services for farmers engaged in local selling. Whole Foods, the world=s largest natural foods retailer, features its Locally Grown promise with a commitment to buying from local producers, particularly those who farm organically and are dedicated to environmentally sustainable agriculture. Produce that has traveled 7 or fewer hours by car or truck from the farm to the retail outlet can be labeled \"locally grown.\" Wal-Mart offers local food vendors marketing assistance to supply its flagship stores and Sam's Clubs and presents training sessions to teach prospective suppliers how to win favor with Wal-Mart purchasers. According to the 2007 Census of Agriculture, 136,817 farms implemented a form of a DMS an increase of 17 percent from 2002 (Detre, et al. 2010). Over the same period, farmers saw the value of direct marketing sales increase by 49 percent. DMS allow producers to receive a better price by directly selling their products to consumers who have increasing demand for fresh and \"local\" food due to the growing concern for a healthier diet (Govindasamy et al. 1999, Morgan and Alipoe 2001, Uva 2002. Even lacking a clear definition of \"local food\" (Hand and Martinez, 2010;Martinez et al. 2010), some consumers are willing to pay more for locally grown products even after controlling for freshness (Darby et al. 2008). The initiative to create a sustainable food supply chain is another important driving force in the implementation of a DMS by farm operators (Ilbery and Maye 2005). Finally, since the majority of the food products sold through direct marketing channels is typically sourced locally instead of transported from national or international sources, direct marketing potentially mitigates the impact on the environment by reducing the carbon footprint in the food supply chain. There is a plethora of literature on direct marketing strategies examining characteristics of consumers who buy directly from producers (Eastwood et al. 1987, Gallons et al. 1997, Govindasamy and Nayga 1997, Kezis et al. 1998, Kuches et al. 1999, Ladzinski and Toensmeyer 1983, Lehman et al. 1998, Schatzer et al. 1989, Thilmany and Watson 2004, Wolf 1997. Fewer studies exist that are focused on the production side (Brown et al. 2006, Detre et al. 2010, Govindasamy et al. 1999, Monson et al. 2008, examining producer behavior regarding DMS and how participation in DMS affects farm business income. A review of literature reveals two overlooked aspects in the current literature on DMS. First, most studies are limited to a regional or state-level analysis. A broad motivation of this study is, therefore, to provide a comprehensive picture of DMS used in U.S. farming. In particular, we investigate the factors affecting choices of direct sales by farmers in (1)direct to consumer outlets (DTC), (2) intermediated outlets (IMO), and (3) both DTC and IMO outlets. A secondary objective of this study is to assess the impact of choice of direct sales on the earnings intensity of the business. By examining the influence of choice of direct sales on earnings, the study can provide significant information to U.S. policy makers on factors influencing participation in local sales, especially whether the composition and use of the Internet influences farmer participation in direct sales. The analysis is conducted on a national farm-level basis with the unique feature of a large sample, comprising farms of different economic sizes, and in different regions of the United States. The empirical approach is based on a discrete choice model where producers select a set of marketing channels to agricultural output. McFadden (1986) developed the economic choice theory underlying the multinomial logit model and highlighted its value in linking discrete choice behavior (choice of market outlet) with continuous decisions (sales revenue in each outlet). Ofek and Srinivasan (2002) demonstrated how market valuation of improved product attributes that account for competition from other brands, potential market expansion, and heterogeneous consumer preferences can be derived from the multinomial logit framework. We account for selectivity bias in the observed earnings from a marketing outlet, recognizing that producers choose from a set of marketing options to obtain the highest returns."}, {"section_title": "Concurrent Sessions II Risk Forecasting", "text": ""}, {"section_title": "Transition to a Steady State Economy", "text": "Now is a good time to ask whether growth can continue and for how long. The first part is easy. Exponential growth is not sustainable. To determine this one does not need any esoteric mathematics or a state-of-the-art supercomputer. The compound interest formulas (middle-school math) suffice. Constant rate (exponential) growth causes something to double in size in fixed time intervals. Of course, growth rates vary, especially in national economies, so GDPs often double in roughly equal time intervals. There is no mathematical reason that something cannot double forever. There is no limit to the number of numbers. But there is to the size of tangible objects, the ultimate being collapse into a black hole. Those who lived through the Great Depression will appreciate this analogy. For 10 doublings, the growth factor is g(10) = 2 10 =1024 (1) So for 20 it is g(20) = 2 20 = 1,048,576 a bit over 1,000,000. There are few things which might grow by a factor of 1000 and almost none by 1,000,000. One exception has been computing power, where the technology used involves making things smaller and smaller, namely electronic components. Moore's law observes that computing power doubles every two years or so. Establishing that there are limits to growth, whether of economic activity, population, or anything else is trivial. On the other hand, coming up with numerical estimates of what they are is difficult to do with any precision. For one thing, there are tradeoffs. One country may choose to have a small population with a high standard of living. Another, a large population living in poverty. National policy almost everywhere seems to be an exponentially expanding population and an economy expanding exponentially at a faster rate than the population, so that the level of consumption may rise. The best way to categorize this is as a belief that \"you can have your cake and eat it too.\" The Rev. Thomas Robert Malthus, FRS (1766-1834), was among the first to come to the conclusion that population could not keep growing and that economic production also was limited. He has often been derided for this basic comprehension of reality. While he attributed it to divine plan, it follows from a little basic math and science. The rapid technological progress of the nineteenth and twentieth centuries obscured the issue of limits, but set into motion processes that would make the end of growth happen sooner and be potentially much more dangerous. Resources, both renewable and nonrenewable, have been consumed with increasing rapidity (Hardin, 1968). In many cases the capacity to produce renewable resources is being diminished. The then new technology of large, mainframe computers was used to study economic growth in the 1960s. Large-scale dynamical models were constructed using ODEs (ordinary differential equations) solved by numerical means, since that was the only way possible. This work was part of a larger effort called System Dynamics (Forrester, 1961;Randers, 1980). A serious challenge is that even small systems of nonlinear ODEs can have extremely unstable solutions, a phenomenon known as chaos (Gleick, 1987;Morrison, 2008). These studies were sponsored by a think tank called the Club of Rome. The effort is well summarized thusly: The originality of their approach soon became clear. In 1972 the campaigning of this growing group of likeminded individuals gained a new worldwide reputation with the first report to the Club of Rome: \"The Limits to Growth\", commissioned by the Club from a group of systems scientists at the Massachusetts Institute of Technology. The Report explored a number of scenarios and stressed the choices open to society to reconcile sustainable progress within environmental constraints. (http://www.clubofrome.org/eng/about/4/) The work continued after this report to the Club of Rome and the results are reported by Meadows, et al., (2004). A newer and simpler methodology has been developed by the Global Footprint Network based in Oakland, CA. It is basically an accounting methodology that evaluates what resources are available and how much of them are being consumed (Wackernagel, et al., 2002). The latest estimate, for the year 2007, is 150%. There are annual values for all these years and quarterly ones starting with 1947. The data for the years before 1929 are GNP, not GDP, and they were given in 1929 and 1958 dollars, not \"chained\" 2005 dollars. These data were adjusted by a log-linear regression to fit the 1929 and subsequent GDP data in 2005 dollars. The regression formulas were then extrapolated back to 1889 so that the pre-1929 data were converted to GDP in \"chained\" 2005 dollars. The results were then averaged, with three data sets covering the period 1909-1928 and two for 1889-1908. The results are given in Table 1. The US GDP graph in Figure 1 includes a trend model derived from a 20-point ramp filter (Morrison and Morrison, 1997) applied to the logarithms of the data. Figure 2 shows a trend model generated by fitting a second degree polynomial to the logarithms of the data. A back extrapolation from 1889 was done so as not to lose the first 19 years. The data show that the growth factor for GDP was The average growth rate was 3.63%/year, which equates to a doubling time of about 19.5 years. So the US economy doubled more than 6 times in the period of 121 years, 1889-2010. Is it possible that it will do the same by the year 2131? 3. A Simplified \"System Dynamics\" Model The System Dynamics models worked rather well, considering the fact that nonlinear dynamic systems can be chaotic and virtually unpredictable, like the weather and the long-term consequences of global warming. However, once one realizes that exponential growth is not sustainable and, hence, not an issue, it is possible to begin with a very simple model. A good way to start is by aggregating all resources into the categories of a. Renewable b. Nonrenewable c. Nonrenewable, but partially recycled Fresh water is a renewable resource, but one that is becoming scarce in many areas due to increased demand. The supply is being diminished by pollution, with the extraction of shale oil and gas being a new and potentially devastating threat. Fossil fuels are nonrenewable, unlike wood or solar power. Uranium and other fuels for nuclear reactors are nonrenewable, but can be created with breeder reactors, a technology adopted by France. Nuclear waste is a major problem for the USA and parts of the former USSR, but it could be reprocessed. Most metals can be recycled and many now are, even that most common of all, iron and its alloy called steel. Many plastics are being recycled, with a large part of the motivation being that landfills are overflowing with yesterday's trash. The simplified model assumes a fixed amount of nonrenewables and a fixed rate of recycling for some part of them. Renewables are replenished at a constant rate, which may be a bit optimistic, considering the growing threats to water resources. The economic model consists of a difference equation that attempts to grow exponentially by consuming more and more resources. A difference equation, whether linear or nonlinear, is very easy to solve with an Excel spreadsheet. The level of accuracy of predictions that might be expected from any sort of model of this type and scope does not justify the complexity of differential equations. Forecasting methodologies use difference equations almost universally, with the basic dynamical model being a noise-driven linear system. The caveat is that the linear system must have no eigenvalues z with a modulus greater than or equal to 1, i.e., \u2551z\u2551 < 1 This assures that the linear system decays exponentially, perhaps with sinusoidal swings, rather than growing exponentially (Morrison, 2008, pp. 175-210). Figure 3 shows a graph of a projection of US GDP as nonrenewable resources are exhausted and the consumption of renewable ones is saturated. Needless to say, this is a highly simplified model, though it may be as accurate a forecast as can be made. In reality, not all nonrenewables will be exhausted at the same time and not all renewables will be saturated simultaneously. Substitution of one resource for another will certainly occur, as it does now when commodity prices vary. The model does not contain components for the business cycle, which certainly will not go away. However, the business cycle cannot be forecast very well for even one of its variable periods. The equations could be supplemented by terms for the business cycle and the complex interactions of markets, governments, and consumers, but these would be in the category of emulations, not forecasts. What this means is that the qualitative behavior might be captured, but that the oscillations in the projections will not likely correspond with what will happen. The amplitudes might be good approximations, but the phases would be a lost cause. A property of complex dynamic systems that exist in the real world, whether that of nature or of human actions or the interactions between the two, is that they exhibit some sort of global stability, but are almost chaotic on the local scale. For example, the weather cannot be forecast very well in most locations, but the climate has been rather stable over periods of years and even decades. Life on Earth has survived for billions of years. Determining whether global warming could destroy this overall stability is a challenge for science and addressing the possibility is an even bigger challenge for governments, industries, other institutions, and the various peoples of the world."}, {"section_title": "What Is the Business Cycle Telling Us Now?", "text": "Our phase plane model of the business cycle was indicating that a recession might well begin in 2007 or 2008 and this indeed did come to pass. What we did not anticipate was the scope or depth of the contraction. While it is true that the growth of subprime mortgages and the securitization of these and other forms of highrisk debt as high-grade securities was being reported in the financial press, nobody seemed aware of the scale of the risks. Eventually everybody found out the hard way. The methodology of the business cycle model is presented in detail in Morrison and Morrison (1997, 2001, 2002. Our most recent plots of the business cycle model indicate that a recovery is occurring, though GDP growth is modest and not spiking up to return it to the level of the long-term trend. See Figures 1 and 2. There is no indication that resource shortages caused the \"Great Recession,\" even though the consumption of renewables may now exceed the sustainable supply by a factor of 150%. However, commodity prices are rising, blamed by some economists on the increasing demand from developing nations (Evans, 2011). The one event that might well trigger a sudden halt to economic growth is the decline of petroleum production. The giant Ghawar field in Saudi Arabia is nearing the end of its useful life and the new fields being opened there will probably not match its capacity (Chazan, 2008;Day, et al., 2009;Hall and Day, 2009). Rising prices of commodities may be caused by currency depreciation (inflation) as well as declining supplies. The Federal Reserve's target for inflation is 2%/year, which is small enough that it does not trigger defensive action by either the public or businesses, such as hoarding gold. The rise of gold to the $1400/Troy ounce level has been due to anxieties over a number of things. Few people, not even economists or corporate CEOs, will do the math and observe that this mild inflation rate cuts the buying power of the dollar by 50% every 35 years and by 75% in 70 years. The question remains whether we are now seeing a flight from the dollar and, perhaps, from all currencies."}, {"section_title": "The Race between Climate Change and Resource Depletion", "text": "The debate about climate change and its possible consequences has taken on an ideological dimension. While it is true that everything in science is probabilistic (except perhaps some parts of mathematics) rather than absolute truth, skepticism is always in order. However, the evidence that global warming could have drastic or even catastrophic results is strong. But the evidence that resource depletion will terminate economic growth is even stronger. So, will economic stagnation arrest climate change? Not necessarily. As the supply of petroleum declines, the use of coal and natural gas will likely increase to make up for some of the oil. Natural gas and petroleum derived from shale and other nonconventional sources may cause severe environmental damage, including the destruction of fresh water resources. The basic problem is that neither governments nor industries are prepared for the inevitable. The result may be that either populations or economic activities will overshoot what is sustainable, and perhaps both. Rare earth metals are already in short supply. These exotic elements, with atomic numbers ranging from 57 (La -Lanthanum) to 71 (Lu -Lutetium) were once scientific curiosities, but they have found many critical uses in modern industry (Hodgman, 1955). China controls much of the supply and is trying to retain these metals for its own industries. Not all nonrenewable resources will be exhausted simultaneously, of course. Some things can be substituted for others, but only up to a point. Various technologies will have to be abandoned and, to the extent possible, replaced by others. In some cases it may be possible to replace rare metals with organic molecules. Science has its job cut out for it."}, {"section_title": "The Transition to a Steady State Economy", "text": "There will be a time lag between the end of the possibility for growth and the acceptance of this inevitability. The culture of endless growth and increasing prosperity has spread from what is called \"the West,\" and especially the USA, to being a global phenomenon. The nations of Asia, with the singular exception of Japan, have been slow to embrace the promises of endless technological progress and the resulting universal prosperity. But now they have joined in a process that is coming to an end. The first part of the twenty-first century will be a classic example of \"generals fighting the last war over again.\" Except that the leaders will be economists, industrialists, and politicians all around the world. The \"war\" will be one of economic competition and, of course, this may produce actual warfare as it has so often in the past. Some of these wars may already be underway. Eventually the world's leaders will conclude that the trade wars and military operations are not solving the problems, but making them worse. The solution is to develop new technologies based on renewable resources and new economies not dependent on endless growth."}, {"section_title": "Smoothing the Transition", "text": "A critical role for government in easing the transition to a steady state economy is supporting appropriate research. This has been underway for a number of years, but not much has been accomplished. For example, it is not clear whether the ethanol mixed into gasoline uses for its production more or less petroleum than is saved. The problem of where and how to store nuclear waste is still unresolved, despite the fact that nuclear power might ease the transition away from fossil fuels. Numerous social problems will arise; some are already here. Addressing these will be difficult and controversial. After all, everybody can agree that getting energy from nonpolluting sources benefits everybody. Efforts to stabilize populations, however, will be an almost impossible challenge due to religions, ethical issues, ideologies, ethnic rivalries, and many other factors. Governmental finances are a major problem already, with nations around the world spending much more than can be collected in tax revenues. More sovereign defaults will occur as others follow in the steps of Argentina. The US dollar will no longer be the global reserve currency and it is unlikely that anything will replace it. Trade will be done with currency swaps, a practice now being used by China, or barter, that was much used in the former Warsaw Pact bloc (of Soviet Satellites). Gold certificates may enjoy a revival, but these will be issued only by private banks or other financial institutions that are strictly regulated. President Nixon ended the era of any national currency being a reliable store of value when he closed the gold window in 1971."}, {"section_title": "Conclusions", "text": "The age of economic and population growth is coming to an end. Economic and political ideologies must yield to the laws of physics. Nations and even regions will face a tradeoff of population size vs. standard of living. This was less apparent as recently as the 1960s (and still is not acknowledged by most people), in part because wars and disease kept populations in check. Now there are more humane ways to control population size, but numerous cultures reject some or even all of them. The adaptation to a steady state economy will be more difficult for some nations than others.  Figure 3. A simplified dynamical model of transition to a steady state economy. Of course, the actual transition will almost certainly be much more complex, but any attempt to forecast to that level of precision would be futile.  The business cycle model is a phase plane plot of a weighted mean of the detrended leading and detrended lagging indicators as x-coordinate and detrended coincident indicator as y-coordinate. Normal cycles follow a counterclockwise roughly circular path with occasional stalls and reversals. Time is indicated along the cycle path. The data have a 1-month lag. Expansions occur between 0\u00b0 and 90\u00b0 and recessions between 180\u00b0 and 270\u00b0. Other angles denote transition (90\u00b0-180\u00b0) and recovery (270\u00b0-360\u00b0=0\u00b0) periods. An \"official\" (NBER) beginning of a recession is indicated by a label \"B\" and an end by \"E\". The Producer Price Index currently emphasizes its stage of processing (SOP) system as the key structure for analyzing producer prices. The SOP system aggregates manufacturers' selling prices for crude, intermediate, and finished goods. Over the past 20 years, however, PPI coverage has expanded to include price indexes for many service and construction activities. PPI has developed an experimental index aggregation system that incorporates these additional indexes and is currently seeking feedback from data users. This paper presents the theoretical background underling the aggregation system and presents preliminary index data from the system.\nIn the era of a global economy, farmers face increasing pressure in developing a portfolio of various marketing channels and in bargaining competitively with increasingly sophisticated marketing participants in the supply chain of agricultural products in local and regional markets. Many farmers begin selling directly through farmers' markets, roadside stands, community supported agriculture. Regional distributors, state branding programs, direct sales to grocery stores, local restaurants, and other retailers represent another set of available outlets. This research assists producers by examining direct marketing strategies and identifies specific farm and demographic factors that are associated with the choice of direct marketing outlets. Results from the discrete model (multinomial logit model) highlight variables that may influence the choice of direct marketing outlets by farmers in the U.S. Extension agents, crop consultants, and marketing analysts can adapt this information to predict the type of marketing outlet that a given farmer might use and provide better information for farmers. Getting an Internet connection and using the Internet for farm commerce, increases the likelihood that a farmer uses intermediated marketing outlets (IMO). On the other hand, using the Internet for farm commerce and growing a diversified selection of products (more enterprises) increases the likelihood that a farmer uses direct to consumer marketing outlets (DTC). Using the Internet for farm related news decreases the likelihood of participation in direct to consumer marketing outlets (DTC). Finally, farming operations purchasing a higher number of their farming inputs near the farm are less likely to use IMO and both DTC and IMO as a choice of direct marketing outlets.     The main topic of this paper is to determine whether data revisions could influence forecasting because the composition of GDP varies substantially from one vintage of data to another. We introduce a number of new statistics for measuring the magnitude of these compositional changes. We further specifically investigate the potential role of changes in the state of the economy for these compositional changes. Our analysis shows that the early data generally reflected the composition of the changes in GDP that was observed in the later data. Thus, under most circumstances, an analyst could use the early data to obtain a realistic picture of what had happened in the economy in the previous quarter. However, the differences in the composition of the vectors of the two vintages were larger during recessions than in expansions. Unfortunately, it is in those periods when accurate information is most vital for forecasting."}, {"section_title": "GDP TREND", "text": ""}, {"section_title": "Topics in Forecasting", "text": ""}, {"section_title": "Modelling and Simulating Long-Run Residential Electricity Consumption in the U.S. Mountain Region Jason Jorgenson and Frederick Joutz, The George Washington University (Paper following)", "text": "The U.S. Mountain Region has experienced significant economic and demographic growth since 1990. There have been structural changes as a result of population growth and electricity deregulation. In addition, there has been growing access to natural gas for residential consumers. This research examines the short-run and long-run dynamics of residential electricity and natural gas consumption. We compare our model and forecasts with those of the U.S. Energy Information Administration's (EIA's) Short Term Energy Outlook. We examine the short-run forecasting properties of our model and again can compare them with the EIA counterpart. Before doing so, the forecasting variables are tested for strong exogeneity. Then we will conduct a forecasting evaluation between our models and the EIA models for the period January 2007 through December 2009. Finally, we use the price and weather responses to perform simulations looking at the impact of higher electricity prices and warmer weather on carbon emissions."}, {"section_title": "Direct Marketing Strategies and Internet Connectivity", "text": "Timothy Park and Shawn Wozniak, U.S. Department of Agriculture, Economic Research Service (Paper following) Initiatives to create a sustainable food supply chain are an important driving force in the growth of local food sales by farm operators yet there is very little analysis that has examined how participation in these markets affects farm business income. This study uses data from the 2008 Agricultural Resource Management Survey (ARMS) on direct marketing strategies used by farmers. We account for selectivity bias in the observed earnings from a marketing outlet, recognizing that producers choose from a set of marketing options to obtain the highest returns. The results will provide significant information on whether direct sales should be part of a farm business management plan, contingent on the type and location of the operation."}, {"section_title": "Modelling and Simulating Long-Run Residential Electricity Consumption in the U.S. Mountain Region", "text": "Prepared for Federal Forecasters Conference, April 2011 Jason Jorgensen and Frederick Joutz"}, {"section_title": "Department of Economics, The George Washington University", "text": "We analyze residential electricity demand for the Mountain Region of the U.S. The region has experienced significant economic and demographic growth since 1990. There have been structural changes as a result of population growth and electricity deregulation. In addition, there has been growing access to natural gas for residential consumers. We estimate a long-run model of electricity demand. Then we perform two simulations. The first looks at the impact of a 10% increase in the electricity price on consumption and resulting greenhouse emissions. The second simulation addresses the effect of an increase of a two degree Fahrenheit on cooling needs, electricity consumption, and greenhouse gas emissions. We develop a long-run model using cointegration techniques. Its properties include: white noise residuals, stable, consistent with economic intuition, explains previous results. The long-run relationship yields a negative and inelastic own price elasticity, positive income elasticity, and cross price elasticity with natural gas that is positive, but not linear homogenous. Our paper is organized as follows. The next section presents a brief literature review of energy demand studies. Section three provides an overview of the formation of the general models used in energy demand. The fourth section provides an overview of the data used in the estimation of the model. The fifth section reviews the econometric issues and testing procedures used in cointegration and the ECM, presenting and evaluating the results of the paper. The sixth section looks at the simulations of long run electricity consumption. The seventh section offers conclusions."}, {"section_title": "Brief Literature Review", "text": "Electricity demand studies have been used for numerous reasons over the past four decades. Dahl (1993) suggests that the demand for energy and energy products have been studied more than any other good or factor. The importance of these studies hinges on extrapolating precise economic information such as price and income elasticities. These elasticities illustrate impact of economic activity and energy prices on energy demand. This information can then be used to make difficult decisions in policy and provide forecasts of future electricity demand, which would allow for planning. Erdogdu (2007) discusses how energy demand studies have used two different approaches to modeling: the first being the \"reduced form model\" and the second being the \"structural form model\". Cointegration is becoming a more visible technique in electricity demand, Erdogdu (2007), Holtedahl and Joutz (2004), and Joutz and Silk (1997). Cointegration analysis begins with a reduced form model and then tests for the existence of \"structural\" relationships. Over the years numerous price and income elasticities have been reported from electricity demand estimation (Dahl, 1993). These elasticities have covered different time periods and different parts of the world. Espey and Espey (2004) gathered price and income elasticities from 36 peer reviewed studies published between 1971 and 2000. These results come from studies using various estimation techniques, most prominently, reduced form estimations, running OLS. They report that short-run price elasticities in the literature range from 0.076 to -2.01 and in the long-run they range from -0.07 to -2.5. They find short-run income elasticities ranging from 0.04 to 3.48 and long-run elasticities of 0.02 to 5.74. In addition report that the average means for U.S. regional price and income elasticities are, in the short run, -0.64 and 0.50, respectively and in the long run, -0.74 and 0.75, respectively. Do to the limited use of cointegration, the authors only report one average long-run price elasticity of -0.10, which falls into category of \"other lag\". What becomes apparent is that the reported variation in elasticities is large and the use of cointegration and error correction modeling is minimal."}, {"section_title": "Formulation of a General Model", "text": "The most basic residential electricity consumption function is a static reduced form function, modeling electricity demand as a function of socio-economic and weather factors, X t , and the stock of electrical equipment, K t . kWh t = F(X t , K t (X t )) (1) Short-run demand for electricity will fix the stock of electricity-using appliances and only allow demand to fluctuate as utilization of these fixed appliances fluctuates. Long-run demand for electricity allows for the stock of electricity-using appliances to fluctuate as well as the utilization rates. This allows for changes in relative prices and income. The capital stock of energy-using appliances can be thought of as two unique types. The first represents demand for daily energy use such as lighting, refrigeration, cleaning and entertainment. The second represents the seasonal weather patterns that affect the amount of air conditioning and heating required. The general model for residential electricity demand will be formulated accordingly: ED = f (PE/kWh, Income, Population, Price of Natural Gas, Weather, Budget Share) (2) where ED is the dependent variable and represents residential demand for electricity per million kilowatthours per day. PE/kWh represents the price of electricity per kilowatt hour. Income represents real disposable income. The price of natural gas represents a substitute for electricity. Weather represents the heating and cooling degree days that drive electricity usage. Budget share takes into account the relative share of income spent on the consumption of electricity."}, {"section_title": "Data: Sources and Description", "text": "All data for this analysis is monthly from the year 1991 to 2009. We obtained the data from the Short Term Energy Outlook Tables (2010) available from the website of the Energy Information Administration. We specifically took data that applied to the mountain region and the residential sector of the U.S. Electricity demand is measured in million kilowatthours per day. The quantity of electricity demanded shows a strong seasonal pattern with the typical two spikes per year, in the summer and winter. There appears to be a general increase in electricity demand over the sample period. From 1991 to 2000, the spikes for the summer and winter periods appear to maintain a consistent magnitude. From 2000 onward, the summer spikes increase considerably in magnitude whereas the winter spikes remain the same. The change in electricity demand from May 1999, the lowest summer demand, to August 1999, the highest summer demand, is 76 million kilowatt hours per day. At its peak in 2007, the change in electricity demand from April 2007, the lowest spring demand, to August 2007, the highest summer demand, is 166 million kilowatt hours per day. This represents an increase in summer electricity demand of 118.42 percent over these years. From 2008 onward, there is a decline in electricity demand as the US entered into a recession. Figure 1 shows the demand for electricity per household. The increase in summer peak demand is driven by population/household growth and the increased penetration of air conditioning usage. The price of electricity is measured in cents per kilowatt-hour. The nominal prices were deflated 1 Real and nominal monthly natural gas prices are measured in dollars per thousand cubic feet. The nominal prices are deflated in the same manner as real electricity prices using the price index for personal consumption expenditure from the Bureau of Economic Analysis. Figure 2 shows the patterns of real natural gas prices. Real personal income per household is measured in billions chained to 2005 dollars and is SAAR (Seasonal Adjusted Annual Rate). It shows a strong trend of increasing growth at a decreasing rate throughout the sample. In Figure 1, real personal income per household grows rapidly from 1991 until 2001. It falls from 2001 to 2003 due the recession and the impact on housing From 2003 until the end of 2007, income growth returns to the same trend as during the 1990's. Finally, due to the late 2000's recession, income declines in 2008 and 2009."}, {"section_title": "Econometric Issues and Hypothesis Tests", "text": "The general-to-specific approach is used as described by Hendry (1986) and Juselius (2000, 2001). Campos, Ericsson, and Hendry (2005) describe generalto-specific modeling as \"the practical embodiment of reduction.\" General-to-specific modeling attempts to uncover the local data generating process (DGP). Using theory and the existing empirical background, a general unrestricted model is constructed to approximate the local DGP. Using diagnostic tests, the unrestricted model is simplified to a \"parsimonious congruent representation\" that can be used for empirical analysis. Discovering the order of integration of each of the endogenous variable is the first step required. It is useful to determine the order of integration for the data because if multiple data are integrated of the same order, through cointegration, long-run or equilibrium relationships can be determined. Table 1 provides the 12 th -order augmented Dickey-Fuller (1981) statistics for the variables: demand for electricity (ed), demand for electricity per household (edhh), real price of electricity (rpe), real price of natural gas (rpng), real personal income (rpi), and real personal income per household (rpihh). All of the variables are in natural logarithms and were tested in their levels and then in first differences for the order of integration. The results from the tests are shown in Table 1. The deviation from unity of the estimated largest root appears in parentheses below each Dickey-Fuller statistic. This deviation should be approximately zero if the series has a unit root. The first two rows test the null hypothesis that the series may contain a unit root or is non-stationary. The second two rows test the null hypothesis that the first difference of the series contain a unit root or is non-stationary. We find that for all the series in Table 1, the null hypothesis of a unit root fails to be rejected in the levels. Looking at all the series in the differences, we find that the null hypothesis of a unit root can be rejected. This implies that the first differences of the series are stationary, giving strong evidence that each of the series contain unit roots and are integrated of order 1, I(1). Next, due to the obvious seasonal patterns observed in the series for electricity demand, price of electricity, and price of natural gas, we follow Franses (1991), Franses and Hobijn (1997) and test for seasonal unit roots in monthly time series. The importance of this test determines whether the seasonal pattern is constant and is characterized as deterministic or if the seasonal pattern varies and should be characterized as stochastic. Deterministic seasonality can be modeled simply by applying seasonal dummy variables. Stochastic seasonal patterns require the use of seasonal differencing to be modeled. If seasonal differencing can be avoided, we can avoid the potential undesirable effects produced by differencing, such as the loss of information in the smoothed series. We use the Franses and Hobijn auxiliary regression tests for seasonal unit roots. In Table 2, we provide the results. To test the null hypothesis that \u03c0 1 = 0 and \u03c0 2 = 0, a one-sided t-test is performed and the critical values are from t-tables based upon Monte Carlos replications from Frances (1990). To test the null hypothesis that \u03c0 3 = \u03c0 4 = ... = \u03c0 11 = \u03c0 12 = 0, the coefficients need to be tested as pairs because these pairs of complex unit roots are conjugates, therefore seasonal unit roots are only present when \u03c0 3 = \u03c0 4 = 0, \u03c0 5 = \u03c0 6 = 0, and so on. When the null hypothesis \u03c0 1 = 0 cannot be rejected it indicates the presence of a unit root similar to the Dickey-Fuller test done above. When the null hypothesis that pairs of \u03c0's are equal to zero cannot be rejected, it indicates that seasonal unit roots exist. Overall, the Franses and Hobijn tests indicate no sign of seasonal unit roots. For the demand of electricity, there does appear to be some sign of seasonal unit roots for the pairs 7 and 8, but these results are close and as the test is preformed for the joint test \u03c0 3 = \u03c0 4 = ... = \u03c0 11 = \u03c0 12 = 0, we can easily reject the null. Therefore, we will conclude that this data does not show significant signs of seasonal unit roots. The other interesting result shows the rejection of the null \u03c0 1 = 0 for the real price of electricity, this might indicate the electricity price series shows no signs of a unit root. When compared to the critical value, this result is borderline significant and taking into account the results from the augmented Dickey-Fuller tests, we still will conclude that the real price of electricity is non-stationary of order 1. This now implies that modeling with seasonal dummy variables is appropriate and the model will not suffer from the loss of information by the differencing of seasonal data. The VAR is specified as a four variable system with the sample period ranging from 1992(2) to 2009(12). The variables in the VAR include demand for electricity per household, price of electricity, real disposable income per household, and the price of natural gas. Prices are lagged one period due to the household response to pricing on electricity demand. The model also includes an intercept, seasonal dummies, and variables for the number of heating-degree days and cooling-degree days as exogenous variables. The lag length of the system is not know upfront, but through tests using the loglikelihood statistic, the AIC, and the SC, reducing the VAR to a reasonable length will increase the power of the Johansen procedure. A 12th-order VAR was estimated first and tests on the lag length were conducted. Residual diagnostics suggested that there is no evidence of serial correlation of the residuals. Recursive analysis was performed on the system and it was found to be relatively stable. The residual density and histogram appeared normally distributed. We concluded that a lag length of six is the appropriate length following an analysis of the residuals, AIC, and Schwartz Criterion. We use the Johansen Test, to look for an equilibrium or long-run relation. Table 3 shows the results of the cointegration analysis. Results of the \u03bb max and \u03bb trace test statistics in the column labeled r=0 indicate a clear presence of one cointegrating vector. To find the implied cointegrating vector, the first column under edhh of the standardized eigenvectors can be evaluated. The results are as follows edhh = -0.200 rpe + 0.197 rpihh + 0.054 rpng All coefficients have the expected signs. The numeric magnitudes follow reasonable estimates of long-run elasticities. They are consistent with previous studies. The rest of Table 3 provides three other tests of weak exogeneity, stationarity, and significance. Tests of weak exogeneity identify whether a given \u03b1 is zero. This is done to identify feedback of the cointegrating vector. If identified, weak exogeneity allows for simplification of the model and inference can be made using the simple model without loss of information. From Table 3, there is strong evidence of weak exogeneity among all variables but electricity demand, indicating, according to theory, that all feedback enters through the electricity demand equation. A joint test of weak exogeneity, setting all \u03b1's associated with rpe, rpihh, and rpng to zero, gives \u03c7 2 (3) = 4.75. Therefore, the model can be simplified and the corresponding cointegrating vector is: edhh = -0.192 rpe + 0.213 rpihh + 0.053rpng (4) Equation (4) is very similar to (3). The feedback coefficient for edhh is -0.93 when the restrictions for (4) are applied. This is a good indication that the results of this model are robust and a single equation will allow for inference without much loss of information. Completing these tests, equation 4identifies the cointegrating vector and long-run relationship. This indicates that the long-run elasticities are, for the price of electricity -0.192, for income 0.213 and for the price of natural gas 0.053. These results fall within the bounds established by Espey and Espey's (2004) analysis."}, {"section_title": "Simulations", "text": "We combine the long-run electricity consumption equation with projections from the EIA National Energy Modeling System and Residential Energy Consumption Survey (RECS). Then, we examine the sensitivity of consumption to two scenarios. Aggregate energy spending and consumption patterns by households for the mountain region by usage for both electricity and natural gas are available from the RECS(2011). In Figure 4 we show that the average growth per year for total households using any fuel is 2.3%. The average growth per year of households using any fuel for air conditioning is 8.0%. The percent of household using fuel for air conditioning increased from 46.77% in 1997 to 72.15% in 2009. Since RECS reports electricity as the only fuel being used for air conditioning, these figures show how the housing boom of the 2000's has led to an increase in the percent of households using air conditioning and therefore an increase in electricity consumption for the average household in the mountain region. The rest of the RECS figures confirm this result as total electricity consumption increased from 6 billion kWh in 1997 to 18 billion kWh in 2005. Total expenditure on electricity increased 72.4% from 4.46 billion dollars in 1997 to 7.60 billion dollars in 2005. As reliance on electricity gets larger, due to increased use of air conditioning, appliances, lighting, and population growth, the potential economic policy questions of how price changes or weather changes might affect electricity consumption become potentially more important. Beyond this, questions of how these changes in consumption behavior might affect greenhouse gas emissions should be analyzed. We examine two simulations, based on the congruent longrun electricity demand model we have estimated. To begin the simulation, we took data from the National Energy Modeling System (NEMS, 2010). NEMS provides projections out to 2030. Table 4 provides summary statistics of the NEMS data and projections. Residential consumption growth of electricity is projected to increase 27% between 2010 and 2030. Household growth is projected to be 40%. This indicates that NEMS projects residential electricity consumption per household to decrease by 9.5% over the next 20 years. This decrease could be attributed to a more efficient capital stock of electrical equipment and shell efficiency of homes. In the first simulation, we assume a 10% increase in the price of electricity 2 2 Given that this is a partial equilibrium or demand driven model, we are implicitly assuming electricity production is perfectly elastic or that capacity will exist for any change in the production of electricity. due to an exogenous change, perhaps rationalizing the cost of consuming fossil fuels. Our model estimates own price elasticity to be ] ) kWh  Using these two pieces of information along with the NEMS projections we calculate the new consumption path for the residential sector. Table 5 provides these results. Over the next 20 years, NEMS originally projected consumption to increase by 27.2%. Simulating the price change, we see residential consumption now increases by 24.7%. Consumption per household now decreases by 11.4% instead of 9.6%. To calculate the change in emissions, NEMS provides projections of the different fuels used in electricity consumption which allow us to calculate how the various fuels breakout by share of total consumption. NEMS also provides projections of carbon dioxide emissions be fuel type and power generation by fuel type in kilowatt-hours. Using this data we follow formula (5): (5) to calculate changes in emissions. We only use coal and natural gas in the calculation; there a very small amount (less than 1%) of distillate electricity generation. Coal and natural gas account for approximately 78% of the input share of electricity produced for consumption. This number holds steady throughout the 20 years of NEMS projections. But since we are interested in emissions, coal and natural gas account for over 99% of the emissions in electricity consumption, which again, holds throughout the 20 years of NEMS projections. Table 5 shows the baseline emissions projections and how these projections change based upon a 10% increase in price. The growth in emissions decreases from 11.78% to 9.54% or from 91.30 million metric tons of CO2 to 89.47 million metric tons of CO2 in 2030. The second simulation addresses the effect of an increase in temperature on cooling needs, electricity consumption, and greenhouse gas emissions 3 3 The temperature sensitivity is based on the error correction model associated with the long-run relationship. There we incorporate cooling degree day effects. These estimates are available upon request. . We assume a two degree Fahrenheit increase in cooling-degree day needs. Assuming a base of 72 degrees this represents an increase in cooling-degree days of 2.77%. Using estimates of cooling-degree day elasticities from our error correction model, suggest a 1% increase in cooling-degree days leads to a 3.9% monthly increase in the consumption of residential electricity. We calculate that the response to a two degree Fahrenheit warming on consumption of electricity is equal to 10%. Table 6 shows how residential electricity consumption increases from 27% to 40% based on these assumptions. Consumption per household now increases in 2020 and returns to 2010 levels in 2030 Thus the warming effect will effectively counteract the efficiency gains in the electrical stock projected in the reference case of the NEMS. Finally, emissions grow at 23%, approximately double the current projections."}, {"section_title": "Econometric Model of Choice of Sales Outlets and Earnings in Chosen Outlet", "text": "Producers choose their marketing plans and assess outside options that are available before participating in any marketing channel. The 2008 ARMS surveys queried farm operators on choices of sales (marketing) outlets and income earned when producers choose different market outlets to sell commodities. Based on this information, a set of three marketing outlets was identified. The marketing outlets included (1) DTC outlets only, (2) IMO only, and (3) both DTC and IMO outlets. The producer's choice of a marketing strategy is based on utility maximization among M alternatives, where utility depends on features of the outlets and the producer's marketing expertise. The marketing strategies include the choice to market through any one outlet, any two outlets, all the outlets, or none of the outlets (no direct sales). For producer i and marketing strategy j, the observed utility U ij in the additive random utility model is composed of a deterministic component, V ij and an unobserved random component e ij : where V ij will depend on a set of explanatory variables Z and estimated parameters g. Marketing strategy m is observed if alternative m has the highest utility across all the alternative strategies: As Cameron and Trivedi (2009) noted, g j is set to zero for one of the categories and coefficients are interpreted with respect to the omitted or base category. The MNL model offers a framework for dealing with selectivity effects in discrete choice models and has distinct theoretical and empirical advantages. Basuroy and Nguyen (1998) show that the MNL framework is appropriate for establishing equilibrium in market shares and assessing the impact of optimal firm responses to entry and potential market expansion. Choice models based on the MNL formulation are commonly used in marketing science applications and yield optimal pricing policies, which align with observed sales and pricing strategies of firms (Cattani, Dahan, and Schmidt, 2010). The parameters of the MNL model can be estimated by maximum likelihood techniques."}, {"section_title": "Data", "text": "The study employs data obtained from the nationwide 2008 Agricultural Resource Management Survey (ARMS) collected by the Economic Research Service (ERS) and the National Agricultural Statistics Service (NASS). The ARMS provides information about the relationships between agricultural production, resources, and the environment as well as about the characteristics and financial conditions of farm households, management strategies, and off-farm income. Data are collected from the senior farm operator per farm, who makes most of the day-to-day management decisions. For the purpose of this study, we excluded operator households organized as nonfamily corporations or cooperatives and farms run by hired managers. Operators associated with farm businesses representing agricultural production in the 48 contiguous states make up the target population of the survey. USDA defines a farm as an establishment that sold or normally would have sold at least $1,000 of agricultural products during the year. Farms may be organized as sole proprietorships, partnerships, or family corporations. In addition to farm economic data, the 2008 ARMS also collected information on farm households. It contains detailed information on off-farm hours worked by spouses and farm operators, the amount of income received from off-farm work, net cash income from operating another farm/ranch, net cash income from operating another business, and net income from share renting. The 2008 ARMS queried farm operators on choices of sales (marketing) outlets and income earned when producers choose different market outlets to sell commodities. The survey instrument contains specific questions pertaining to the use of direct marketing strategy by farmers. Specifically, the survey queried farmers whether they have used the following direct marketing outlets: (1) roadside stand or on-farm facility, (2) on-farm stores, (3) farmers' markets, (4) community supported agriculture (CSA), (5) regional distributors, (6) state branding programs, and (7) direct sales to local grocery stores, restaurants and other retailers. Based on this information, a set of three marketing outlets was identified. Choices 1 through 4 were grouped into DTC and include ten percent of producers. Choices 5 through 7 were grouped into IMO and account for seven percent of producers. The third group includes farmer who used both DTC and IMO outlets, and includes four percent of the producers. Farms with no direct sales outlets were used as the base group and comprise 79 percent of the farms in the 2008 ARMS dataset. Figures 1 and 2 show violin plots of total value of farm sales by participation in direct marketing and by DMS choice. Violin plots combine box plots and density traces in one diagram. The box plots display the center, the spread, asymmetry and outliers in data while the density traces reveal the distribution of the data, with its valleys, peaks and bumps. The mean for farmers participating in any form of direct sales is just under $500,000 for net value of farm sales, while the mean for those not participating is somewhat higher. The plot demonstrates that there are a larger number of small farms participating compared to large farms, but large farms participating show higher net value sales overall. There is a large standard deviation for participants and non-participants, suggesting some volatility in participation. Farmers participating in DTC are skewed in the plot mostly toward the left, with the median being under $100,000 for net farm sales. There are three nodes, heavy in the lower group, and slight for farms in the middle and higher net value of sales areas. For IMO only, there are two almost symmetrical nodes, showing almost equal participation by small and large farmers, while those having mixed strategies mostly skew toward farmers with lower net value of farm sales. Table 1 reports descriptive statistics of the independent variables. Table 2 reports parameter estimates of the choice of direct marketing model used by farmers in the US. Note that the base group for comparison is farmers with no direct marketing sales. We focus our discussion on a few key explanatory variables."}, {"section_title": "Results and Discussion", "text": "The ARMS provides information about the locations used by farmers to connect to the Internet including farm residence or office, off-farm residence or office, in the field, a public access site, or elsewhere. Farmers marketing only through DTC outlets report the fewest Internet connectivity options and rely most heavily on connections at the residence or office. The coefficient of Internet connectivity options (connectoptns) is positive and significant in the case of intermediated marketing outlets (IMO), suggesting that, in comparison to farmers with no direct marketing outlets, farmers who have Internet connection are more likely to adopt IMO. This result suggests that having an Internet connection may be proving beneficial to farmers in searching for information on additional markets, by providing the farmer with additional marketing outlets that are more profitable and easier in application, and increasing demand for the products over what would be found in the traditional market place. The model incorporates linear and quadratic terms for the number of hours per week that farmers access the Internet for farm-related news and for farm-related commerce. Farmers who marketed only through DTC channels report the lowest usage of Internet for farm news (about 3.08 hours per week), 32 percent lower than farmers using intermediated outlets only and 47 lowers than farmers with diversified marketing channels. A similar pattern is observed in hours per week using the Internet for commerce. Farmers who marketed only through DTC channels again have the lowest levels of Internet use (about 2.39 hours per week). We considered a model that combined the Internet use variables into a single variable measuring the amount of time the farmer accessed the Internet. The model that failed to distinguish between time spent in searching for marketing information and time spent in commercial activities was not supported. We performed a likelihood ratio test that the number of hours per week that farmers access the Internet for farm-related commerce has no correlation with the probability of choosing any of the marketing strategies. The null hypothesis was decisively rejected as the calculated \u03a0 2 value of 36.89 exceeded the critical value of 10.65 at the 90 percent confidence level. A test that the farmer's marketing strategy is not related to Internet use for farm news yielded a calculated \u03a0 2 value of 10.40. The farmer's use of the Internet for farm-related news (the joint impact of the intntfrmnew variable in linear and quadratic form), shows a significant negatively relationship with the DTC marketing choice. Results suggest that an additional hour spent on the Internet for farm-related news is associated with a decrease in the probability that a farmer uses the DTC marketing choice. A possible explanation is that farmers using Internet for farm-related news may be growing commodity program crops and may show little propensity to participate in direct selling initiatives. These farmers may also make more use of extensive information related to government programs and information regarding farming techniques, machinery, fertilizer, and services provided by University Extension and private sector firms. An interesting finding in table 2 is the positive and significant overall relationship between Internet access use for farm-related commerce (the intntcommc variables) and each of the marketing strategies. This result is consistent with the findings of Mishra, Williams, and Detre (2009) who conclude that farmers with Internet connections are more likely to explore additional marketing outlets for their farm products. Results in table 1 indicate that farming operations purchasing a higher number of their farming inputs near the farm (farminpTWN) are less likely to use IMO and both DTC and IMO as a choice of direct marketing outlets. It is likely that farms purchasing most of their inputs near the farm are likely to be smaller farms located in rural areas where access to IMO might be more limited. The model includes a measure of the diversification of the farm across commodities. The Herfindahl index is based on the share of the ith product in sales accruing to the farm, with product categories for major grains, other grains, vegetables, fruits and nuts, and hay and other crops. The index is calculated as where s i is the share of the ith product in total sales. The Herfindahl index can be decomposed as The first component varies directly with the number of commodities (n) that the farmer produces for sale so that an expanded portfolio of products indicates a higher level of diversification for the farm operation. This component is termed the number component (NUMherf). The second component is the distribution component. When the farmer grows n products that comprise equal shares of total sales, then the distribution component is zero indicating an equal distribution across the products. Higher negative values reveal that the distribution of sales shares across products is increasingly unequal and the farm operation is less diversified. The distribution component is identified as DISTherf. Results in table 2 indicate a positive and significant association between the number of crops grown by the farm and choice of direct marketing outlets. In particular, the coefficient of NUMherf is positive and significant for DTC and both DTC and IMO marketing outlets, when compared to the base group (no direct marketing sales). A higher Herfindahl index indicates a diversified farm and it can be argued that diversified farms are seeking several marketing outlets, including direct sales (DTC) and intermediated outlets (IMO). The Herfindahl index results when the number component and the distribution component are combined. The decomposition that is used in the multinomial logit model provides more information to assess how diversification across agricultural products is related to choice of marketing strategy. The marketing strategy choice is more closely related to the producer's decision on the portfolio of crops to grow and market rather than the magnitude of the sales shares that are claimed by each crop. A model that combines the number and distribution components of the Herfindahl index into a single variable is rejected as the calculated \u03a0 2 value of 26.96 exceeds the critical value of 4.60 at the 90 percent confidence level. Beginning farmers (begfarmer), those who began farming after 1997, are more likely to choose DTC as their choice of direct marketing strategy. The finding here suggests that entrants in farming may be more educated and are likely to engage in off-farm work (Mishra et al. 2002). The new entrants are more likely to operate small and diversified farms, located near metroareas, where the demand for local food items and fresh produce is greater than for farms located in more sparely populated areas."}, {"section_title": "Papers and Proceedings", "text": ""}, {"section_title": "Assessing Global Vector Auto-Regressions for Forecasting Neil R. Ericsson and Erica L. Reisman; Federal Reserve Board (Paper following)", "text": "Global vector auto-regressions (GVARs) have several attractive features: a standardized economically appealing choice of variables for each country or region examined a systematic treatment of long-run properties, and flexible dynamic specification. Pesaran, Schuermann, and Smith (2009) generate and evaluate forecasts from a paradigm GVAR with 26 countries. The current paper empirically assesses that GVAR with impulse indicator saturation, a new generic procedure for evaluating parameter constancy---a central element in model-based forecasting. The results indicate substantial room for an improved, more robust specification of that GVAR, with some tests suggestive of how to achieve such improvements."}, {"section_title": "THE GVAR", "text": "To motivate the use of GVARs in practice, this section describes a prototypical GVAR (Section 2.1) and relates it to the GVAR in DdPS (Section 2.2). The current approach to modeling GVARs has been developed in Pesaran, Schuermann, and Weiner (2004) and DdPS inter alia. For further research on GVARs, see Garratt, Lee, Pesaran, and Shin (2006); Pesaran and Smith (2006); D\u00e9es, Holly, Pesaran, and Smith (2007); Pesaran, Smith, and Smith (2007); Hieberta and Vansteenkiste (2009); Pesaran, Schuermann, and Smith (2009); Castr\u00e9n, D\u00e9es, and Zaher (2010); Chudik and Pesaran (2011); and the comments and rejoinders to Pesaran, Schuermann, and Weiner (2004) and Pesaran, Schuermann, and Smith (2009). Juselius (1992) provides a conceptual precursor to GVARs in her sector-by-sector analysis of the Danish economy to obtain multiple longrun feedbacks entering an equation for domestic inflation. Smith and Galesi (2010) have designed and documented an easy-to-use Excel-based interface that accesses Matlab procedures to implement GVARs."}, {"section_title": "A Prototypical GVAR", "text": "This subsection describes a prototypical GVAR that has three countries, with two variables per country and a single lag on each variable in the underlying vector autoregression (VAR). For ease of exposition, global variables (such as oil prices) and deterministic variables (such as an intercept and trend) are ignored. This prototypical GVAR highlights key features that are important to the remainder of this paper. In the exposition below, this prototypical GVAR is considered first in its generic form, then in its error correction representation, then on a country-by-country basis, and finally on a variable-by-variable basis for each country. While the prototypical GVAR may well be unrealistically simple for empirical use, it conveys important aspects of the GVAR without undue algebraic complication, and it allows (in Section 2.2) a straightforward description of the GVAR in DdPS. Ericsson (2011) provides a more complete description of the structure of GVARs, the notation used, and the underlying assumptions. The underlying VAR for the prototypical GVAR is: (1) , for 0,1,2, and 1,2, \u2026 , , where is the country index, is the time index, is the number of observations, is the vector of domestic variables for country at time , is the vector of corresponding foreign aggregates (i.e., foreign relative to country ) at time , is the matrix of coefficients on the lagged domestic variables, and are the matrices of coefficients on the contemporaneous and lagged foreign aggregates, and is the error term induced by having conditioned on those foreign variables. Empirically, one interesting triplet of countries is as follows: the United States ( 0), the euro area ( 1), and China ( 2). Each subsystem in (1) is also a VARX* model-that is, a VAR model that conditions on a set of (assumed) exogenous variables and their lags. In error correction representation, the prototypical GVAR in (1) is: (2) \u0394 \u0393 \u0394 : , for 0,1,2, and 1,2, \u2026 , , where \u0394 is the difference operator, \u0393 is the matrix of coefficients on the change in contemporaneous foreign aggregates, and and are the matrices of adjustment coefficients and cointegrating vectors for country . The matrices \u0393 , , and in (2) are functions of the matrices , , and in (1). The explicit country-by-country structure of the GVAR in equation 2is as follows: \uf0b7 Real equity prices ( ), \uf0b7 Real exchange rate ( ), \uf0b7 Short-term interest rate ( ), and \uf0b7 Long-term interest rate ( ). DdPS focus on 25 countries plus one region (the euro area); see Table 2 in Section 4.2 for a list of the countries. For convenience, both countries and regions are referred to as \"countries\" below. The countryspecific aggregated foreign variables ( ) are constructed from the full set of domestic variables across all countries, using fixed trade weights. The VARX* for each country initially has two lags on domestic variables and on the foreign aggregates. In some instances, however, shorter lags are selected, based on standard information criteria. Also, the VARX* includes a global variable (oil prices) and deterministic variables (an intercept and trend). Cointegration in the VARX* is tested, following the procedure in Harbo, Johansen, Nielsen, and Rahbek (1998) and using critical values from MacKinnon, Haug, and Michelis (1999); see also Johansen (1992Johansen ( , 1995 and Juselius (2006). The number of cointegrating vectors may differ from country to country. In the conditional error correction model, the country's cointegrating vectors are written in their reduced form, i.e., with beginning with an identity matrix. The data are quarterly, taken from the IMF's International Financial Statistics (except for Singapore's data, which are from Datastream). Estimation is typically over 1979Q4-2003Q4 ( 97). This GVAR from DdPS provides the empirical illustration examined in Section 4."}, {"section_title": "IMPULSE INDICATOR SATURATION", "text": "This section describes the procedure called impulse indicator saturation, which Section 4 uses to test parameter constancy of the GVAR in DdPS. Impulse indicator saturation (IIS) uses zero-one impulse indicator dummies to analyze properties of a model. There are such dummies, one for each observation in the sample. While inclusion of all dummies in an estimated model is infeasible, blocks of dummies can be included; and that insight provides the basis for IIS. A simple example with two equal-sized blocks motivates the generic approach in IIS. See Hendry, Johansen, and Santos (2008), Johansen and Nielsen (2009), and Hendry and Santos (2010) for further discussion and recent developments. Imagine estimating a model specification such as the GVAR in (4) in three steps. First, estimate that model, including impulse indicator dummies for the first half of the sample. That estimation is equivalent to estimating the model over the second half of the sample, ignoring the first half. Drop all statistically insignificant impulse indicator dummies and retain the statistically significant dummies. Second, repeat this process, but start by including impulse indicator dummies for the second half of the sample; and retain the statistically significant ones. Third, re-estimate the original model, including all dummies retained in the two block searches; and select the statistically significant dummies from that combined set. Hendry, Johansen, and Santos (2008) and Johansen and Nielsen (2009) have shown that, under the null hypothesis of correct specification, the fraction of impulse indicator dummies retained is roughly , where is the target size. For instance, if 100 and the target size is 1%, then (on average) only one impulse indicator dummy is retained when the model is correctly specified. If the model is mis-specified such that its implied coefficients are nonconstant over time, IIS has power to detect that nonconstancy. See Hendry and Santos (2010, Section 4) for an example. Interestingly, the residuals of the estimated model without any impulse indicator dummies need not lie outside their estimated 95% confidence region, even with a statistically and economically large break in the underlying parameters of the data generation process. Also, the IIS procedure can have high power to detect the break, even though the nature of the break was not utilized in the procedure itself. In practice, IIS in the Autometrics routine of Doornik and Hendry's (2009) OxMetrics utilizes many blocks, and the partitioning of the sample into blocks may vary over iterations of searches; see also Hendry and Krolzig (1999, 2001, 2005, Perez (1999, 2004), and Krolzig and Hendry (2001). IIS is a statistically valid procedure for integrated, cointegrated data; see Johansen and Nielsen (2009). IIS can also serve as a diagnostic statistic for many forms of mis-specification. Many existing procedures can be interpreted as \"special cases\" of IIS in that they represent particular algorithmic implementations of IIS. Such special cases include recursive estimation, rolling regression, the Chow (1960) predictive failure statistic (including the 1-step, breakpoint, and forecast versions implemented in OxMetrics), the Andrews (1993) unknown breakpoint test, the Bai and Perron (1998) multiple breakpoint test, intercept correction (in forecasting), and robust estimation. IIS thus provides a general and generic procedure for analyzing a model's constancy, allowing for an unknown number of structural breaks occurring at unknown times with unknown duration anywhere in the sample. Algorithmically, IIS also solves the problem of having more regressors than observations by testing and selecting over blocks of variables. That block approach permits testing the aggregation assumption implied by the use of foreign aggregates in the GVAR; see Ericsson (2011) for a discussion of the underlying theory and for implementation in Autometrics."}, {"section_title": "EVALUATION OF DdPS's GVAR WITH IIS", "text": "This section implements the parameter constancy test associated with impulse indicator saturation, using the GVAR in DdPS to illustrate. Tests on individual equations and on country-specific subsystems are both feasible. Specifically, the subsystem for a given country is unrestricted (either as an unrestricted VARX*, or as an unrestricted cointegrated VARX* conditional on the subsystem estimate of ), so OLS estimation equation by equation is maximum likelihood estimation of the subsystem VARX* model. Valid omitted-variables test statistics can be calculated on either the VARX* as a subsystem, or on the individual equations of the VARX*. These two approaches may imply different alternative hypotheses, even while the null hypothesis is the same. This section discusses these test statistics for the equation-by-equation approach for the cointegrated VARX*; Ericsson (2011) and Ericsson and Reisman (2011) examine inter alia the IIS test statistics for the subsystem approach. Section 4.1 discusses the IIS test results in detail for selected equations for the United States, the euro area, the United Kingdom, and China. Section 4.2 summarizes the results for all countries and equations. Section 4.3 compares these results with those reported in DdPS, and it discusses various implications and extensions."}, {"section_title": "Selected Highlights", "text": "To illustrate the use of impulse indicator saturation, Table 1 reports results from IIS at the 0.1% target level for four selected equations: for US equity prices, for euro-area inflation, for the UK real exchange rate, and for the Chinese short-term interest rate. The table lists the -statistics for the significance of the retained impulse indicator dummies, the associated -values, and the dates of the retained impulse indicator dummies. Notes. The four entries within a given block of numbers are (i) the -statistic for the significance of the retained impulse indicator dummies, (ii) the tail probability associated with that value of the -statistic (in square brackets), (iii) the degrees of freedom (d.f.) for the -statistic (in parentheses), and (iv) the dates of the retained impulse indicator dummies or (for China) the number of retained impulse indicator dummies. The two superscript asterisks ** on the -statistics denote significance at the 1% level. Parameter constancy is rejected by IIS in all four equations. In the first three equations, the retained impulse indicator dummies reflect known historical events associated with major changes in the behavior of the variable being modeled. In the equation for US equity prices, IIS retains an impulse indicator dummy for 1987Q4, reflecting the fall of US stock prices by over 20% on Black Monday (October 19). In the equation for euro-area inflation, IIS retains impulse indicator dummies for periods when euro-area inflation markedly increased or decreased: 1981Q1, and 1986Q3-1986Q4 respectively. In the equation for the UK real exchange rate, IIS retains an impulse indicator dummy for 1992Q4, which captures the substantial devaluation of the pound sterling near the end of the previous quarter on Black Wednesday (September 16), when the UK government was forced to withdraw the pound sterling from the European Exchange Rate Mechanism (the ERM). IIS for the fourth equation-of the Chinese short-term interest rate-is particularly revealing. IIS detects 18 dummies, with an infinite -statistic for the significance of those dummies. As seen in Figure 1, the Chinese short-term interest rate appears to be an administered rate, with stretches of several quarters when it is constant. Hence, the series for \u0394 is a series of zeros, interspersed with (nonzero) impulses whose values reflect the magnitude of the change in the level of the interest rate when that level changes. Because the estimated equation is in its error correction form, the dependent variable is \u0394 . IIS detects all of the impulses in the series for \u0394 ; and IIS sets all other coefficients in the equation for \u0394 to zero, giving a perfect fit. It is encouraging that IIS demonstrates the ability to detect this plethora of scattered impulses: IIS favors detecting impulses when the impulses are all within a single block; but no single block in IIS for the equation for \u0394 includes all of the time periods for which \u0394 0. Economically and statistically, IIS for this equation implies that a VARX* model is inadequate to capture the essential features of the Chinese short-term interest rate. Table 2 reports the number of impulse indicator dummies found at the 0.1% target level in each equation for each of the 26 countries in DdPS's GVAR. Even at such a stringent target level as 0.1%, statistically and economically significant impulse indicator dummies are detected frequently."}, {"section_title": "Results for All Countries and Equations", "text": "For the short-term interest rate equation, all countries but Switzerland have at least one retained impulse indicator dummy; and five countries have ten or more. That result is particularly impressive because the target level is 0.1% and the sample size is 97, implying an only one-inten probability of retaining an impulse indicator dummy in a given equation by chance. IIS of the equations for the short-term interest rate results in the most retained impulse indicator dummies on average for any equation (six impulse indicator dummies per equation, on average), as might be expected for a variable that could be strongly influenced by shifts in monetary policy. Table 2. The number of retained impulse indicator dummies for IIS at a 0.1% target size, by equation in each country, for DdPS's GVAR. equations in which no impulse indicator dummies are retained. For example, six countries have at least three equations with no retained impulse indicator dummies: the euro area, Japan, Australia, Canada, Singapore, and Switzerland. All but Singapore are considered developed countries, and Singapore itself has many of the characteristics of a developed country. While parameter constancy is rejected for most equations in most countries at even the 0.1% level, rejections are particularly frequent and compelling in equations for emerging market economies. That said, parameter nonconstancy is detected in at least two equations for every country, regardless of the country."}, {"section_title": "Remarks", "text": "Several implications follow directly from the rejections in Tables 1 and 2. First, DdPS (Table V) evaluate their GVAR using tests for structural breaks and find little evidence of mis-specification. DdPS's results contrast with the evidence in Tables 1 and 2 above. The explanation of these differences lies in the tests employed. For evaluating parameter constancy, impulse indicator saturation may have more power than the structural break tests in DdPS for the range of relevant alternatives, particularly for breaks near the ends of the sample. Hence, the results in Tables 1 and 2 represent new information about the GVAR's properties and need not be related to other diagnostic tests such as those for residual autocorrelation. Equally, the rejections in Tables 1 and 2 are unsurprising in that IIS was not incorporated as a design criterion in the building of DdPS's GVAR; see Hendry (1987) on the role of design criteria in model construction. Second, rejection of a given null hypothesis does not imply the alternative hypothesis. For instance, the IIS test of parameter constancy may reject because of omitted-variable bias due to improper data aggregation and changing data moments. More generally, the presence of retained impulse indicator dummies may have any of a number of possible implications for modeling. It may be appropriate to simply include the retained impulse indicator dummies, as in the equation for US equity prices, where the dummy captures information beyond the scope of the model's structure. Or, one might add economic variables that the impulse indicator dummies proxy, as perhaps is the case for the euro-area inflation equation. Or, one might treat the presence of the impulse indicator dummies as evidence for a much more fundamental sort of mis-specification being present in the model, as with the equation for the Chinese short-term interest rate. Third, while the large number of rejections in Table 2 may be discouraging at first blush, they are also encouraging because they imply substantial potential for model improvement; and they may provide some guidance in finding a better-specified model. Some of the test statistics above indicate clear directions for model redesign, as with impulse indicator saturation of the equation for \u0394 detecting 1992Q4. This modeling approach is consistent with a progressive research strategy; see Hendry (1987), White (1990), and Doornik (2008) inter alia. Fourth, and relatedly, IIS in conjunction with automatic model selection may be used constructively in model building. In particular, Castle, Fawcett, and Hendry (2009), Choi and Varian (2009), and Castle and Hendry (2010) show how automatic model selection among a plethora of variables can improve nowcasting of important economic time series. Fifth, inclusion of impulse indicator dummies can and does have significant consequences for the rest of the model's coefficients-economically, as well as statistically and numerically; see Ericsson (2011). Sixth, impulse indicator saturation can be applied to any empirical model to assess parameter constancy and model specification of that model. Given the central and substantive roles of invariance and constancy in economic model interpretation, forecasting, and policy analysis, IIS would be of particular interest to apply to dynamic stochastic general equilibrium (DSGE) models, new Keynesian Phillips curve models, Markov switching models, and statistical time series models inter alia. For DSGE models in particular, see the analysis in Edge and G\u00fcrkaynak (2010); and note Erceg, Guerrieri, and Gust (2006), Smets and Wouters (2007), and Erceg and Lind\u00e9 (2010). Finally, discovering test rejections for a given equation has no implications for the properties of a well-specified equation. For instance, the latter may have constant parameters, even though the former does not."}]