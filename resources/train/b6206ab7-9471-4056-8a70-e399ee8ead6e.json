[{"section_title": "Abstract", "text": "Abstract-In this paper a custom classification algorithm based on linear discriminant analysis and probability-based weights is implemented and applied to the hippocampus measurements of structural magnetic resonance images from healthy subjects "}, {"section_title": "I. INTRODUCTION", "text": "Alzheimer's Disease (AD) is a chronic neurodegenerative disease which is responsible for 60% to 70% cases of de mentia and affects around 6% of the population who are aged 65 and above; the percentage affected by the disease increases with age [1] . Symptoms of AD based on the DSM IV TR criteria include: memory impairment; aphasia (language disturbance); apraxia (impaired motor functionality); agnosia (failure to recognise objects) and a disturbance in executive functioning (the management of cognitive processes) [2] . Early detection of AD is challenging as there have been no biological markers found to definitively diagnose it at this stage. Current diagnosis of AD involves clinical approaches which are a set of neuropsychological tests to assess the patient [3] , thus AD can only be diagnosed once it has had a significant effect on the patient's lifestyle. While there is no cure for an AD sufferer, symptomatic treatments exist to help patients reduce the symptoms they are suffering from. Clinical trials are still running to develop new treatments aimed to lower the chance of developing AD or delaying the onset and progression of it [4] .\nAutomated classification of AD is the process where a subject is classified as a healthy control (HC) or suffering from AD. Most research in this area tends to use either neuropsychological tests as data; or more commonly, magnetic resonance images (MRIs). The classifier developed in this paper uses MRI data, thus MRI data will be the focus when investigating previous research. There are multiple types of MRIs, two of these types are structural MRIs and functional MRIs. Structural MRIs are the 3D image of a subject's brain recording the intensity at each point that is measured (the intensity refers to whether the part of the brain at the measured location is white matter, grey matter or cerebral spinal fluid. Functional MRIs include the structural MRI of the brain as well as measuring brain activity from changes in blood flow in the brain. There exist multiple challenges to the classification of MRI data, the first problem which occurs is artifacts produced by the MRI scanner, these include ghosting which is caused by the subject moving potentially due to respiratory motion and heartbeats [5] . Fixing these artifacts is neccessary else the MRIs are likely to be of bad quality and may produce data which will lead to the creation of an inadequate classifier for AD. Another challenge is the large dimensionality of the data which leads to the curse of dimensionality where as the dimensions increase the data becomes more sparse and harder to find patterns.\n[6] investigates feature selection methods (these methods are: recursive feature elimination, correlation filters, random forest filter) for SVMs applied to extracted data from the Freesurfer processed MRIs. One-versus-one SVMs are com bined with a voting mechanism to create a three-class classifier capable of classifying between HC, MCI and AD. Intra cranial volume normalisation (ICV) is also tested on the subjects to determine whether it is beneficial to the accuracy of the classifier or not. The best accuracy is achieved by the SVM which uses no ICV normalisation and the random forest filter. [7] also used extracted Freesurfer features from MRI where features were selected based on a priori knowledge, and features were also combined with each other to reduce the dimensionality; a high classification accuracy of 73% was achieved. Previous work which uses Bayesian modelling in application to AD diagnosis includes [8] which uses Bayes' Theorem to calculate likelihood ratios for multiple tests which are used to diagnose a patient with AD. These likelihood ratios are then combined to find the tests which have the best joint predictive power on the diagnosis of the subjects. Authors in [9] propose a sparse Bayesian multi-task learning algorithm on MRI data extracted by Freesurfer to deduce relationships between neuroimaging measures and cognitive scores to show how changes in the structure of a brain can affect the cognitive condition of it.\n[10] uses a Bayesian classifier to distinguish between HC and AD based on the clusters of voxel data of structural MRI scans."}, {"section_title": "II. DATA PRE-PROCESSING", "text": "The data used in this paper was created using 287 MRI scans downloaded from the ADNI database and processed with Freesurfer version 5.3 [11] with the optional conunand to segment the hippocampal subfields as well as perform the reconstruction of the entire brain. The hippocampal subfields consist of 16 volumetric measurements: the entire left hip pocampus, the entire right hippocampus, the left and right presubiculum, the left and right subiculum, the left and right comus ammonis (CA) 1, the left and right CA 2-3, the left and right dentate gyrus, the left and right fimbria, and the left and right hippocampal fissure. The criteria used to refine the search to find these 287 subjects was: that they were the baseline scan for each subject -this is the initial scan taken and initial diagnosis given to the subject; the slice thickness of scan was I.Simn and it was weighted in T 1; and that Freesurfer was able to analyse the MRI (in rare cases Freesurfer crashes during its execution if it is unable to process the MRI). A breakdown of the information of the subjects used in this study can be found in Figure I , which shows the distribution of attributes of the subjects such as diagnosis, age and gender.\nICV normalisation is a technique commonly used in classifi cation of dementia from structural MRI; it involves transform ing the volume measurements to account for different head sizes and total brain size as age and gender will affect the brain size which will in turn affect the volumes of the regions inside the brain. There are multiple methods to calculate ICV described in [12] , the simplest of which is the proportional method In the data used in this paper there is not a linear relation between the hippocampal volumes and the ICV of the patient, thus using ICV normalisation may cause loss of relationships between variables as two of the three methods previously described require a linear relationship between the volumes and ICY. Also ICV normalisation has been shown to lead to bias in volume measurements based on the age or gender of the subjects [13] . While there are multiple arguments against the usage of ICV normalisation on the data used in this paper, it will be verified as it may improve the accuracy of the method regardless of the potential downsides. The proportional method will be used, V i = lJi:V' to compute the ICV normalised volumes."}, {"section_title": "III. PROBABILITY-BASED CL ASSIFIER", "text": "A. Linear Discriminant Analysis to Partition the Feature"}, {"section_title": "Space of a Single Region", "text": "Linear Discriminant Analysis (LDA) is a method to find the optimal linear hyperplane to separate two classes with the least margin of error. In this case it will be applied to single region to determine an optimal threshold where the values that lie below the threshold belong to one class, and the values that lie above the threshold belong to another class. Figure 1 shows the probability density functions for each of the hippocampal volumes for HC and AD patients and also the threshold for each attribute generated by LDA. From this it can be seen that generally the volumes for HC subjects are greater than the volumes than AD patients except in the left and right hippocampal fissures; however, the fissures are measurements of a gap rather than the volume of a mass in the hippocampus. It also shows the thresholds created when LDA is used on the attributes to classify the diagnosis of the subject. As the hippocampal fissure measurements do not provide any predictive power for the diagnosis of a subject these measurements will be ignored and only the other 14 attributes will be used.\nUsing the hyperplanes found during the LDA process, each subject's hippocampal attributes are tested to verify whether the value lies in the HC-sized side of the hyperplane or the AD-sized side. A value belongs to the HC-sized side if it lies above the threshold, and AD-sized otherwise. If the subject lies in the HC-sized side of the hyperplane then a -1 is recorded for that attribute, and for the AD-sized side, a 1 is recorded; this generates a binary feature vector for each subject as the attributes have been transformed from being a continuous value to having one of two possible values. An example of this process on a subject is shown in Table II . Intuitively if a higher count of 1 is found it should mean the subject is more likely to have AD as more of the hippocampal attributes indicate a likelihood that AD is present; and vice versa -if a higher count of -1 was found it means there is a likelihood of HC This is shown in Figure 2 , where the bar chart showing the distribution of subjects with the number of positive attributes and their actual diagnosis. This bar chart shows a correlation between the number of positive attributes and a diagnosis of AD; and also a correlation between the number of negative attributes and HC subjects. However, there are subjects with a Based on the number of positive attributes for a subject, a classification can be made as to whether the subject is HC or AD, as for an AD patient, the number of positive regions found would be expected to be greater than the number of negative regions found, this algorithm will be referred to as the Binary Region Classification algorithm (BRC). BRC works by summing the binary feature vector (which is comprised of -1 and 1 rather than 0 and 1) and if the value is above 0 a classification of HC is made; if it's 0 or below then an AD classification is made. BRC is shown in Equation 2.\n(1)\ni =l\nOne downside to the BRC is that each region is given the same weight as every other region (lor -1) -it assumes all regions have the same predictive power. This is not the case, some attributes will be better at identifying AD in a subject given the attribute is positive. This probability can be written as P(AD I Xi :s; ti). The probability of a patient being HC given the attribute is negative is given by P(HC I Xn > ti)'\nThe predictive powers for each volume measurement are shown for HC and AD in Table III and they can be used to weight the attributes: rather than each attribute having an integer value of -lor 1. Table III also shows some additional interesting information: the top four predictors for both AD and HC are the same (though in a slightly different order of ability for each diagnosis): Left Subiculum, Right Subiculum, Left Presubiculum and Right Presubiculum; this matches with the findings of [14] where it was found that atrophy of the subiculum and presubiculum were the best hippocampal markers for detection of AD. Using this weighting system which will be referred to as Probability-Weight Classifcation (PWC), two variants of the algorithm are proposed and tested. PWC1 (X) and PWC2(X) are the variants and they adopt two equations which use the summation of the probability weights, and the resultant value is used to classify the subject as HC if it is less than 0 and AD otherwise. The variants, PWC1(X) and PWC2(X) , are shown in Equations 3 and 4 respectively.\nBuilt-in feature selection can be implemented for these algorithms by having a threshold where if the predictive ability of a region falls below this threshold then it is discarded. The reasoning behind this is that regions which are worse at predicting HC or AD in a subject and thus the information they provide may lead to more misclassifications. Therefore if a thresholds PT is provided such that all regions with a predic tive power lower than PT in the context of P(HC I Xi > ti) and P(AD I Xi :s: ti) are ignored (note that the minimum number of features that can remain are 1 no matter the value of PT)."}, {"section_title": "A. ICV Normalisation", "text": "Two methods of ICV normalisation will be tested, the proportional method discussed earlier where V i = I \ufffdVv and a custom method which involves only normalising the volumes which lie outside the mean of all the subjects' values of that volume plus or minus the standard deviation of the values for the volumes. This is shown in Equation 5 , where sd(ICV)\nis the standard deviation of the volume values and will be referred to as ICV normalisation SD. The ICV normalisation SD method was designed because in the initial testing of the algorithm, many of the false positives (HC misclassified as AD) and false negatives (AD misclassified as HC) were from patients who had a very high or very low ICV in comparison to the other patients (such that their ICV was outside the range of the mean \u00b1 the standard deviation of the ICVs of all subjects)."}, {"section_title": "B. Naive Bayes and Support Vector Machine Classification", "text": "This paper describes a custom method being used to classify the data, to evaluate the performance of this method it will be compared to two state-of-the-art classification methods: a Naive Bayes classifier and a Support Ve ctor Machine (SVM). The Naive Bayes classifier is a probabalistic classifier which applies Bayes' theorem on a set of independent features, in this case the independent features are the hippocampal volume measurements for each subject. An SVM is a non linear classifier using a kernel to transform data into a higher dimensional feature space and then classify the data in the new feature space. The kernels which will be tested in this paper are the linear kernel k(x, x') = (x, x') and the Gaussian radial basis filter (RBF) kernel k(x, x') = exp ( -allx -x'11 2 )."}, {"section_title": "V. EXPERIMENTAL SETUP", "text": "After the initial pre-processing of the data with Freesurfer, it was imported into the data mining software KNIME [15] with the use of the library K-Surfer [16] . This allowed the statistical data output from Freesurfer to be merged into a single table rather than multiple text-based files. Using the KNIME the structural MRI measurements were merged with the corresponding patient's details -their age, gender and diagnosis; the merged data was then saved in a single comma separated file. The algorithm was implemented and tested using the statistical programming language R [17] using the data output from KNIME as the input. External libraries for R provided the implementation for the LDA classifier [18] ; and also the SVM [19] and the Naive Bayes [20] classifier which were used to compare the algorithm proposed in this paper. Figures 3 and 4 show that the optimal probability thresholds are found between 0.7 and 0.8 ( Figure  3) , so those set the range for the probability thresholds which will be tested further. The results for the BRC algorithm are found in Table IV where BRC is used on data where ICV normalisation has not been applied; Table V shows BRC  applied to ICV normalised data and Table VI is BRC applied to data which uses the ICV normalisation SD method. The results for the PWC algorithm are found in Table VII where PWC1 and PWC2 are used on data which hasn't been ICV normalised; PWC1 and PWC2 are applied to data which have been ICV normalised and the results are in Table VIII;  and in Table IX , PWC1 and PWC2 have been applied to data (4) with the ICV normalisation SD method used. A Naive Bayes classifer has been tested on the data in Table XI as it, like PWC, also uses probability to classify data so they both share a similarity. SVMs were also applied to the data in Table X because out of various classifiers applied to the data, SVMs outperformed them all thus it would be useful to compare the classifiers developed in this paper to what might be one of the highest accuracies achievable on the data with commonly used classifiers. For all the results lO-fold cross validation was repeated ten times using a different set of folds for each repetition. All of the classifiers tested used the same sets of folds so they were training and testing on the same data sets. lO-fold cross validation is a form of k-fold cross validation [21] and works by splitting the data set into k folds where each fold contains an equal (or near equal) number of samples. Next, k iterations of training and testing are performed where k -1 folds are used for training and the leftover fold is used for testing. Eventually all folds will have been tested (using a different k -1 folds for training at each iteration) and then an accuracy can be computed between the actual class of all of the samples of the data and the predicted class for all the data samples. The lO-fold cross validation is repeated ten times for all classifiers in order to try and eliminate any bias towards the data set being tested."}, {"section_title": "VI. RESULTS", "text": ""}, {"section_title": "Initial testing in", "text": "dim(x) ( \u00a2( ) 1 \u00a2( ) 1 ) PWC1(X') = \ufffd X i 2 -(P(HClxn > ti)-P(ADlxn > ti)) + X i 2 + (P(HCl X n::; ti)-P(ADlxn::; ti)) dim(x) PWC2(X') = L ( \u00a2( X i \ufffd -1 P(HC I X n > ti) -\u00a2( X i \ufffd + 1 P(AD I X i ::; ti)) i=l"}, {"section_title": "VII. DISCUSSION", "text": "The highest accuracy achieved was by BRC with an ac curacy of 85.8% using a probability threshold of 0.78 on ICV normalised data. This outperformed even the SVM which obtained an accuracy of 84.9% using a linear kernel on ICV normalised data and also outperformed the Naive Bayes classifier with a maximum accuracy of 84.7%. The difference between BRC and PWC was negligible with PWC having a maximum accuracy of 85.6% (compared to BRe's 85.8%) thus showing that there is no need to complicate the method using the probability-based weights that is used by PWC and instead use the binary weights from BRe. PWC1 always achieved a higher maximum accuracy than PWC2, however, at lower probability thresholds (0.7 -0.75), PWC2 performs better than PWC1 so it is better able to cope with less relevant features.\nICV normalised data far outperformed non-ICV normalised data; in BRC and PWC using ICV normalisation improved the maximum accuracy by over 5% in both cases, and all the accuracies at each probability threshold used were improved. With the Naive Bayes classifier and the SVM, ICV normal isation also improved the results, with the SVM being the classifier able to cope with the non-ICV normalised data the best by achieved the highest accuracy of 82.3% on it. The ICV normalisation SD method described in this paper in all cases par one performs worse than standard ICV normalisation, and it always outperforms ICV normalisation. For the Naive Bayes classifier however, ICV normalisation SD outperfonns ICV normalisation by 1.5%, though this is only a marginal amount and since ICV normalisation SD was only better than standard ICV normalisation in one case, the SD method isn't worth using. The descriptive model produced by this algorithm is shown in Table II , each region of a subject's hippocampus is given a binary feature of 1 or -1 depending on the value of the volume measurement for that region of the hippocampus. If a subject has more negative attributes than positive attributes then a diagnosis of HC is made; and more positive attributes than negative attributes then an AD diagnosis is made. There is a case which could occur for certain subjects where the number of negative attributes is equal to the number of positive attributes; in this case, a classification of AD will made -the reasoning behind this is that it would be better (regarding the patient's interest) for a healthy patient to receive treatment for AD rather than an AD patient to go ignored. An alternative would be to not make a decision and leave the diagnosis unknown which could potentially lead to doctor's monitoring the development of the patient's brain over the foreseeable future."}, {"section_title": "VIII. CONCLUSION", "text": "The work in this paper has created a classifier which outperforms an SVM and a Naive Bayes classifier at accuracy in diagnosing a patient as HC or AD based on a baseline structural MRI scan. As well as attaining a higher accuracy it can extract a easily understood description of why a diagnosis was made, whereas an SVM doesn't have this advantage as an SVM works by transforming the data into a higher dimensional space where descriptive information about the data is lost. The descriptiveness of this model could aid a doctor by pointing to regions in the hippocampus that the doctor should look at -if we let all negative features be a warning sign of AD, then say the left presubiculum is negative, the classifier could advise the doctor to manually check the left presubiculum of the subject's MRI to see if there is anything about that region that could correlate with AD. This work has also shown that the best features in the hippocampus are the left presubiculum, right presubiculum, left subiculum and right subiculum matching with current literature on this topic [14] .\nThere is also potential for the algorithm to have a parallel execution in both the training and predicting parts of it. The training of the algorithm could be parallelised by: creating the thresholds for the decision boundary in parallel and evaluat ing different probability thresholds in parallel. Regarding the prediction part of the algorithm: the proposed methods in this paper are based on the summation of weights and each of the weights are independent of the other weights, this means that the weights can be computed in parallel.\nFuture work involves applying this algorithm to the three class HC, AD and also Mild Cognitive Impairment sufferers and seeing if it performs as well. After the thresholds are created, region are given the diagnosis of negative or positive which converts a continuous variable into a binary variable, rule mining could be applied to the binary variables to generate rule associations to further the understanding of AD in the hippocampus. In this paper only the hippocampal subfields are used, the classifier's performance when applied to the cortical and subcortical fields of the brain could improve so it is worth seeing how well it works applied to the additional features. Another idea is to use subspace clustering to find a subspace where the subjects can be grouped into clusters and see if rules can be deduced from the clusters that are able to be merged with the rules found by this classifier to create a better classifier."}]