[{"section_title": "Abstract", "text": "Recent evidence has shown that structural magnetic resonance imaging (MRI) is an effective tool for Alzheimer's disease (AD) prediction and diagnosis. While traditional MRI-based diagnosis uses images acquired at a single time point, a longitudinal study is more sensitive and accurate in detecting early pathological changes of the AD. Two main difficulties arise in longitudinal MRI-based diagnosis: (1) the inconsistent longitudinal scans among subjects (i.e., different scanning time and different total number of scans); (2) the heterogeneous progressions of high-dimensional regions of interest (ROIs) in MRI. In this work, we propose a novel feature selection and estimation method which can be applied to extract features from the heterogeneous longitudinal MRI. A key ingredient of our method is the combination of smoothing splines and the l 1 -penalty. We perform experiments on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. The results corroborate the advantages of the proposed method for AD prediction in longitudinal studies."}, {"section_title": "Introduction", "text": "Alzheimer's Disease (AD) is the most common cause of dementia in the aged population [12] . In order to prevent disease progression and take therapeutic treatment in the earliest stage, it is vital to identify AD-related pathological biomarkers of progression and diagnose early-stage AD. A considerable amount of research has been devoted to the use of structured magnetic resonance imaging (MRI) for early-stage AD diagnosis; e.g., [6, 7] . The structural MRI provides measures of cerebral atrophy and it is shown to be most closely coupled with clinical symptoms in AD [5] . Most work in the literature focus on the cross-sectional study with MRI collected at one single time point; see, e.g., [1, 8, 20] . However, the cross-sectional study could be insensitive to early pathological changes. As an alternative, longitudinal analysis of structural abnormalities has recently attracted attentions [2, 24, 26] . Most of these existing longitudinal studies focus on the atrophy of a few well-known biomarkers such as the hippocampus, entorhinal cortex, and ventricular cortex. However, these prespecified ROIs may be insufficient to capture the full morphological abnormality pattern of the brain MRI. Besides it, a few other issues remain as challenges in the longitudinal analysis. First, longitudinal scans across subjects are usually inconsistent. For example, subjects could have different scanning time and different total number of scans. Second, the total number of ROIs in the brain is large compared with the number of subjects, which poses a challenge to select AD-rated longitudinal biomarkers from the whole brain. Third, the rates of longitudinal change in different ROIs are different and this heterogeneity should be accounted by the modeling of progression.\nThe goal of this paper is to identify important AD-related ROIs in the whole brain MRI with longitudinal MRI data and use the selected ROIs for AD prediction. Specifically, we use the varying coefficient model [3] to characterize the heterogeneous changes of different ROIs in structural MRI. This model also allows a nonlinear functional modeling between MRI and clinical cognition functions. We propose a novel feature selection method by combining the smoothing splines and a l 1 -penalty, which can simultaneously select and estimate AD-related ROIs. We provide an efficient algorithm to implement the proposed feature selection method. Then the prediction is performed based on the selected longitudinal features and estimated varying coefficients. Our method is robust to the inconsistency among longitudinal scans and is adaptive to the heterogeneity of changes in different ROIs. The use of varying coefficient models is motivated by the hypothetical AD dynamic biomarkers curves proposed by [6, 7] , where their principle is that the rates of change over time for MRI and clinical cognition functions are in a temporally ordered manner. Hence, the functional relationship between the atrophy of MRI and the change in clinical cognition functions must be nonlinear in time.\nTo evaluate our method, we perform experiments using data from the Alzheimer's Dis-ease Neuroimaging Initiative (ADNI). We predict future clinical changes of mild cognitive impairment (MCI) subjects with brain MRI data. The MCI is a prodromal stage of AD.\nThe prediction of clinical changes help to determine whether a MCI subject will convert into AD at a future time point, which is vital for early diagnosis of AD.\nMain differences between this paper and existing longitudinal studies in [2, 24, 26] are as follows.\n\u2022 Different feature representations. We use the varying coefficient model to characterize nonlinear and smooth progression of longitudinal features, which is motivated by clinical findings and the dynamic biomarker curve in [6, 7] . On the contrary, [2, 24, 26] use linear representations for features.\n\u2022 Different scalability to heterogenous longitudinal scans. Different from [2, 24, 26] , our method does not require same scanning times and a same number of scans across samples.\n\u2022 Different feature selections. We proposed a novel feature selection method by combining smoothing splines with a l 1 -penalty, which allows to simultaneously select and estimate features. This is different from the two-step method in [26] by doing the selection and estimation separately and [2, 24] by only using pre-selected features.\nThe rest of the paper is organized as follows. We introduce our method in Section 2. We give experiment results in Section 3. The conclusion remarks and discussions are given in The varying coefficient model [3] can describe time-dependent covariate effects on the responses. Given scaled time t \u2208 [0, 1], the response functional Y (\u00b7) is related to covariates\nwhere the centered noise process \u03b5(\u00b7) is independent of X j (\u00b7)s. The model (2.1) allows a nonlinear relationship between X j (\u00b7)s and Y (\u00b7) be letting the coefficients \u03b2 j (\u00b7)s vary on t. On the other hand, (2.1) has an additive structure on covariates X j (\u00b7)s, which enables efficient estimations of coefficients \u03b2 j (\u00b7)s.\nIn practice, data are obtained for subject i = 1, . . . , n at time t i\u03bd , where \u03bd = 1, 2, . . . , m i ,\nNote that m i and t i\u03bd s are allowed to be different for different subjects i. Denote X j (t ij ) = x ij and let y i\u03bd be the response for subject i at time t i\u03bd , then (2.1) implies\nThe structure of heterogenous longitudinal data is illustrated in Figure 1 , where some subjects could have missing feature values at certain time point. The number of covariates p in (2.2) can be larger than the sample size n, and then (2.2) becomes a high-dimensional model. Since some covariates might be irrelevant with the response, we want to select important covariates X j (\u00b7)s based on data (2.2) and use the selected covariates for prediction.\nWe propose a new method to simultaneously select covariates and estimate their corresponding varying coefficients as follows. Assume that varying coefficients\nreside in a reproducing kernel Hilbert space (RKHS) (H K , \u00b7 H K ) with the reproducing\nwhere N = n i=1 m i and \u00b7 H K is the RKHS norm. The first term in (2.3) measures the goodness of data fitting and the second term merits the selection property by the l 1 -like penalty p j=1 \u03b2 j H K . We first provide the following theorem to justify the existence of minimizer for (2.3).\nTheorem 2.1. There exists a minimizer of (2.3) that is in the domain\nThe proof of this theorem is given in Appendix C. The variable selection method (2.3) is new in the literature and (2.3) is efficient for optimization since it is convex in \u03b2 j (\u00b7)s and it has only one tuning parameter \u03bb. We provide an algorithm in Appendix D.\nThe following theorem gives further insights into (2.3) that it is indeed a combination of the smoothing splines [22] and the Lasso [17] . Theorem 2.2. Consider the following optimization problem. Find\nwhere \u03c4 0 is a constant and \u03c4 1 is a tuning parameter. Let \u03c4 1 = \u03bb 4 /(4\u03c4 0 ). The following equivalence holds."}, {"section_title": "minimizes (2.3).", "text": "We give the proof of this theorem in Appendix E. Note that (2.4) is a combination of the smoothing splines and the Lasso since the first two terms:\nis actually the same as the smoothing splines in nonparametric statistics [22] , and the last term\nis actually the same as the Lasso penalty [17] for the weights \u03b8 j s.\nand \u03b2 j 1 , \u03b2 j 2 , . . . , \u03b2 js be the corresponding estimated varying coefficients by (2.3) . Then the prediction model for a new subject with features X * j 1"}, {"section_title": "Experiment Results", "text": "In this section, we predict future clinical changes of MCI subjects with real data from the ADNI database. A detailed description of the ADNI database is relegated to Appendix A.\nThe MCI is a prodromal stage of AD. Generally, some MCI subjects will convert into AD after certain time (i.e., MCI converters, MCI-C for short), while others will not convert (i.e., MCI non-converters, MCI-NC for short) [26] . The prediction of clinical change in a MCI subject help to determine whether the subject will convert into AD at a future time point, which is a central task for the early diagnosis of AD. We summarize the baseline demographic information of ADNI subjects studied here in Table 1 .\nThe preprocessing steps for brain MR imaging used here are described in Appendix B.\nSpecifically, we have total 324 ROIs for each imaging. For MCI subjects, MRI scans were performed at baseline (bl), 6 months (M06), one year (M12), 18 months (M18), two years (M24), three years (M36), and four years (M48). However, some subjects may miss a few visit times and hence they do not have MRI scans at these time points. We choose n = 172\nMCI subjects who have M48 imaging data. Table 2 lists the distributions of visit times for these 172 MCI subjects, where, e.g., 6 of MCI-C subjects make at most 3 visits among the scheduled six times (bl, M06, M12, M18, M24, M36) such that they have at most 3 longitudinal MRI scans.\nOur goal is to use longitudinal information (from bl up to M36) to predict the clinical changes of MCI subjects at M48. Since the empirical evidences suggest that the rates of change over time for structural MRI and clinical cognition functions are in a temporally ordered manner (see, e.g., [6, 7] ), a nonlinear modeling for the functional relationship between the atrophy of MRI and the change in clinical cognition functions is necessary. Hence, the and education years. The index t in (2.1) should be identifiable and we let t be the scaled time relative to subjects enter the ADNI study. Figure 2 gives the flowchart of our method.\nWe build six models by using six different levels of longitudinal information:\n\u2022 Model 1: bl.\n\u2022 Model 2: bl+M06 (including subjects have missings at bl).\n\u2022 Model 3: bl+M06+M12 (including subjects have missings at bl or M06).\n\u2022 Model 4: bl+M06+M12+M18 (including subjects have missings at bl, M06 or M12). \u2022 Model 5: bl+M06+M12+M18+M24 (including subjects have missings at bl, M06, M12 or M18).\n\u2022 Model 6: bl+M06+M12+M18+M24+M36 (including subjects have missings at bl, M06, M12, M18, or M24).\nFollowing the flowchart in Figure 2 , we first perform the feature selection method in The prediction comparisons of our method using six levels of longitudinal data.\nreplicated for 100 times. We summarized the prediction results in Figure 3 . It is clear that the longitudinal data can significantly improve the prediction results compared with only using baseline information. And the more longitudinal data included, the better prediction will be obtained. We also observe that the prediction results for MCI-NC are slightly better compared with MCI-C, which can be explained by the fact that MCI-NC subjects have more stable clinical status and less varied clinical scores.\nWe give examples of selected feature in Figure 4 . These are four ROIs that consistently selected in Model 6 for 100 experiments. \u2022 The longitudinal analysis in [2] which only uses the hippocampal volume shrinkage rate as the feature.\n\u2022 The longitudinal analysis in [26] which use linear feature representations and a group Lasso for variable selection (e.g., [25] ).\nSince the methods in [2, 26] require same scanning times and a same number of scans across samples, we perform Model 1-6 for AD prediction with samples having no missing visits.\nIn each experiment, we randomly leave out half of samples in both MCI-C and MCI-NC for prediction. For the training of each model, a 10-fold cross validation is performed to select the tuning parameters in (2.3) and in [2, 26] . The experiments are replicated for 100 times. The prediction comparison results for MCI-C are summarized in Figure 5 and the prediction comparison results for MCI-NC are summarized in Figure 6 . It is clear that our proposed method consistently achieves better prediction performances for both MCI-C and MCI-NC. The reason of the superior performance of our method is due to the modeling of nonlinear progression of longitudinal features and selecting important features from the whole brain instead of only using a prespecified feature for prediction."}, {"section_title": "Discussion", "text": "We study a framework to integrate longitudinal features from the structural MR images for AD prediction based on varying coefficient models. We propose a novel variable selection method by combining smoothing splines and Lasso, which enables simultaneous selection and estimation and is adaptive to heterogeneous longitudinal data. To illustrate the effec- (NINCDS/ADRDA) criteria for probable AD in [11] . The mild cognitive impairment subjects (MCI) should meet defined criteria for MCI but do not meet the criteria in [11] and the MCI subjects should have largely intact general cognition as well as functional performance. Study subjects should have given written informed consent at the time of enrollment for imaging and genetic sample collection and completed questionnaires approved by each participating sites Institutional Review Board (IRB). "}, {"section_title": "B Preprocessing of the brain MRI used here", "text": "The structural MRI used in this study are cortical gray matter volumes processed using\nFreeSurfer software version 4.4 longitudinal image processing framework (https://surfer. nmr.mgh.harvard.edu/) (\"ucsffsl\" file). This dataset has been used in, for example, [9, 18, 19] . Specifically, subjects with a 1.5-T MRI were included in the dataset where the scans were preprocessed by certain correction methods including gradwarp, B1 calibration, N3\ncorrection, and skull-stripping (see, e.g., [4] for detail), and the FreeSurfer 4.4 implements the symmetric registration [13] and unbiased robust template estimation [14] . Only MRIs which passed the quality control for all the areas were included in our study. There are \nSince \u2126 is closed, convex, and bounded set, there exists a minimizer for (2.3) in \u2126. Denote the minimizer by\u03b2 0 ,\u03b2 1 (\u00b7), . . . ,\u03b2 p (\u00b7). Then, A(\u03b2 0 ,\u03b2 1 (\u00b7), . . . ,\u03b2 p (\u00b7)) \u2264 A(0, 0, . . . , 0) < \u03c1. On the other hand, for any\n. . , \u03b2 p (\u00b7)) \u2264 \u03c1 and |b| > \u03c1 1/2 + (c K c x + 1)\u03c1, (C.1) implies that for any i = 1, . . . , n,\nHence, A(b, \u03b2 1 (\u00b7), . . . , \u03b2 p (\u00b7)) > \u03c1. Therefore, for any b, \u03b2 1 (\u00b7), . . . , \u03b2 p (\u00b7) \u2208 \u2126, we have that\nis the minimizer of (2.3). This completes the proof."}, {"section_title": "D Algorithm", "text": "This algorithm is based on Theorem 2.2 whose proof is given later in Appendix E. Consider for any fixed \u03b8 1 , . . . , \u03b8 p \u2265 0. If \u03b8 j = 0 for some j, then \u03b2 j = 0 in the optimization (2.4).\nWithout less of generality, let \u03b8 1 , . . . , \u03b8 p > 0 and (2.4) is equivalent to the smoothing spline type problem: find b \u2208 R, \u03b2 1 (\u00b7), . . . , \u03b2 p (\u00b7) \u2208 H K to minimize\nBy the representer lemma [22] , \u03b2 1 (\u00b7), . . . , \u03b2 p (\u00b7) have a closed form expression:\n. . .\nLet the unknown coefficient vector c j be\nWrite the response vector y as\nLet 1 N be the column vector consisting of N 1's. Then (D.1) becomes\nwhich has the unique solution given as follows:\nNote that when \u03b8 1 , . . . , \u03b8 p are fixed, (2.4) is equivalent to find b \u2208 R, c \u2208 R N p to minimize\nThe minimizer of (D. We choose the best M in Step 3 according to the fivefold cross-validation. In the simulations we find that when \u03c4 0 is fixed according to step 2, the optimal M seems to be close to the number of important components. This gives a range to determine the tuning for M ."}, {"section_title": "E Proof of Theorem 2.2", "text": "Recall that A(b, \u03b2 1 (\u00b7), . . . , \u03b2 p (\u00b7)) denotes the functional in (2. "}]