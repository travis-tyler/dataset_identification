[{"section_title": "Abstract", "text": "The purpose of the present study was to analyze an international large-scale data set using a cognitive assessment approach. Although some researchers question the usefulness of international large-scale assessments (e.g., TIMSS), participating countries have continued to use the results from these large-scale assessments to improve their curricula and teaching methods. Despite the common reporting practice-single-score-in these large scale assessments gives useful insights about students' overall performances, they still lack diagnostic information. Cognitive diagnosis models (CDMs) were developed to provide more feedback on students' cognitive strengths and weaknesses. This study retrofitted the TIMSS 2011 eighth grade mathematics assessment by applying a specific CDM called the DINA (the deterministic, inputs, noisy, \"and\" gate) model to data from South Korea and Turkey. Results of the DINA model were used to make a detailed comparison between students of these two countries."}, {"section_title": "INTRODUCTION", "text": "Since the first administration of the Trends in International Mathematics and Science Study (TIMSS) in 1995, the comparison of the relative performances of participating countries has become very helpful for finding out country-level success relative to other countries. Although some researchers question the relationship between student-level (or country-level) achievement and comparison studies based on such international large-scale assessments (Holliday & Holliday, 2003; Wang, 2001 ), In order to provide an example to show how diagnostic feedback can be given using real data, this study analyzes TIMSS 2011 data from a cognitive diagnostic assessment (CDA; Leighton & Gierl, 2007) perspective. Over the last two decades, the interest in CDA has increased in order to obtain more information about students' performances on a measurement. This type of assessment classifies students based on their degrees of mastery of specific skills. Thus, examiners and instructors can obtain more information relevant to classroom teaching and learning. Unlike a single-overall test score, CDA-based reports simply show what students know (master) and what they do not know (master) rather than how much they know.\nThe main purpose of this study is to examine Turkish eight graders' strengths and weaknesses on topics that were covered on the TIMSS 2011 mathematics achievement test. In order to do so, in this study, the relative performances of Turkish students in comparison with South Korean (Korea hereafter) students were assessed. Hence, this CDA-based study examines the following research questions:\n1. How do Turkish and Korean eight graders' relative TIMSS 2011 mathematics performances differ? 2. What are the Turkish eight grade students' weaknesses and strengths on TIMSS 2011's mathematics topics in comparison to the Korean eight graders?"}, {"section_title": "Literature Review", "text": "Several recent studies (e.g., Dogan & Tatsuoka, 2008; Im & Park, 2010; Lee et al., 2011; Toker & Green, 2012; Lee et al., 2013) have been conducted to compare students' achievements on international large-scale assessments (e.g., TIMSS, PIRLS) using DCMs. These studies have provided useful feedback on the students' performance and skills, the linkage between teachers' instruction and students' performances, and the countries' educational systems and their curricular instructions. For instance, Dogan and Tatsuoka (2008) compared Turkish and American eight-grade students' mathematics performances on the TIMMS-R 1999. Their results indicated that Turkish students were weak in algebra and probability/statistics in comparison to their American peers, and they also \"demonstrated poor profiles in skills such as applying rules in algebra, approximation/estimation, solving open-ended problems, recognizing patterns and relationships, and quantitative reading\" (Dogan & Tatsuoka, 2008, p. 263) . Similarly, Im and Park (2010) "}, {"section_title": "240", "text": "differences in the performances of Korean and American students, especially in \"problem restructuring and reasoning, measurement, and geometry\" (p. 287). Their results suggested that encouraging students' independent problem solving was the most useful instructional strategy for both Korean and American students. Moreover, American students benefitted from reviewing, re-teaching, and clarifying as well. In addition to the above studies, Lee et al. (2011) Lee et al. (2011) also provided fine-grained diagnostic information on students' performances, which they suggest could be exactly applied to classroom instruction. For example, by analyzing item parameter estimates (e.g., slipping and guessing) they offered curricular suggestions to the classroom teachers on how to improve students' performances.\nIn this study, Korea was chosen as a reference country, because Korean eight graders have been regularly placed in the top three in TIMSS mathematics performance. As stated by Mullis, Martin, Foy, and Arrora (2012) , 42 countries and 14 benchmarking entities participated in TIMSS 2011. In that assessment, the international TIMSS scale average was set to 500. Among 42 countries, Turkish students had an average score of 452 and were ranked in 24 th place. Korean students had an average score of 613 and were ranked in first place on the TIMSS 2011. As explained by Im and Park (2010) , several studies investigated which characteristics of Korean education have been contributing to such tremendous performance in mathematics. According to Im and Park (2010) , the results of those studies pointed out that factors contributing to Korean students' high achievement could be grouped under social and instructional factors. Social factors included \"competitive examination and selection, a regular and metric number system, the serious attitudes of students towards tests, meaningful repetitive learning, and the competence of mathematics teachers (Kim et al., 2008; Park, 2004) \" (Im & Park, 2010, p. 288) , and instructional factors included \"cooperative learning activities (Chung & Son, 2000; House, 2009) , the use of constructivist strategies (Fisher & Kim, 1999) , and teachers' guidance (Oh, 2005) \" (Im & Park, 2010, p. 288) . These social and instructional factors also affected our decision to select Korea as the reference country."}, {"section_title": "Diagnostic Classification Models", "text": "A number of cognitive diagnosis models (CDMs), also known as diagnostic classification models (DCMs), have been developed (Rupp, Templin, & Henson, 2010) to apply the CDA approach. For an overview of DCMs, the reader is referred to DiBello, Roussos, and Stout (2007) , Fu and Li (2007) , Rupp and Templin (2008a) , and Rupp et al. (2010) . However, it should be noted de la Torre (2011) classified these psychometric models as either general or a specific type based on their characteristics. Specific DCMs include: deterministic inputs, noisy \"and\" gate (DINA; Haertel, 1989; de la Torre, 2009; Junker & Sijtsma, 2001 ), deterministic inputs, noisy \"or\" gate (DINO; Templin & Henson, 2006), noisy-input, deterministic \"and\" gate (NIDA; Junker & Sijtsma, 2001) , and the reduced reparameterized unified model (R-RUM; Hartz, 2002; Roussos et al., 2007) . General DCMs include the log-linear cognitive diagnostic model (LCDM; Henson, Templin, & Willse, 2009) , the general diagnostic model (GDM; von Davier, 2005) , and the generalized DINA (G-DINA; de la Torre, 2011) model. This study focused on the DINA model. Thus, a brief description of the DINA model is presented below."}, {"section_title": "The DINA Model", "text": "The DINA model is a non-compensatory model with a conjunctive rule (Rupp et al., 2010) . Based on the conjunctive nature of the DINA model, a respondent has to master all of the measured attributes of an item in order to get full credit for this item. Respondents get zero credit for an item if they did not master at least one of the measured attributes of this item. Thus, the DINA model divides respondents into two groups for each item: those who mastered all attributes and those who did not master all attributes. This is done with the conjunctive kernel of the DINA model, which is presented as a latent response vector ( below (Equation 1). Let be the response of examinee r to item i, and let "}, {"section_title": "241", "text": "be the examinee's binary attributes vector, which is coded as 1 for presence or mastery of attribute k on the kth element and zero otherwise. Like most of the CDMs, the DINA model requires a Q-matrix (Tatsuoka, 1985) that shows the relationship among items (i,...,I) and attributes (k,...,K). A value of 1 for the Q-matrix entry (i.e., ) indicates that attribute k is measured for item i. For example, suppose we measure four attributes in an arithmetic test. Let addition, subtraction, division, and multiplication be four attributes coded as Attribute 1, Attribute 2, Attribute 3, and Attribute 4, respectively. Based on this attribute list and the DINA model specification, students have to master both Attribute 1 (addition) and Attribute 3 (division) in order to get full credit ( ) for an item such as . A student with mastery of addition or division cannot get full credit ( ), as he/she would miss one of the required attributes for this item. The conjunctive kernel of the DINA model can be presented as below:\n, where is the latent variable which is coded as zero or one for respondent r and item i, and is the Q-matrix entry described above.\nrepresents the latent attribute variable indicating whether respondent r has mastered attribute k ( ) or not ( ). Thus, the latent response vector ( can have a value of 1 if respondent r masters all the attributes required for item i and a value of 0 if the respondent did not master at least one of the measured attributes for item i. It is possible that respondents who have mastered all attributes can give a wrong answer to item i, while respondents who have missed one of the required attributes can correctly answer item i. The former refers to slipping, and the latter refers to a guessing situation in the DINA model specifications. Thus, two parameters are obtained for each item in the DINA model regardless of the number of attributes. Item slipping ( ) and guessing ( ) parameters do not change across attributes, because they are itemspecific. In the DINA model, these two item parameters are defined as follows: , After defining slipping and guessing parameters, the probability of the correct response of a respondent in latent class c for item i can be computed as below:\nAccording to Equation 4, respondents need to master all attributes measured by an item in order to answer this item correctly. DINA model was used in this study, because the DINA model requires an estimation of two parameters for each item, and the number of attributes does not affect the number of estimated parameters in the DINA model. The DINA model is also an appropriate model for equally important items like TIMSS items. The DINA model has been used in analyses of the TIMSS data by several authors, including Lee et al., (2011) and Choi, Lee, and Park (2015) ."}, {"section_title": "METHOD", "text": ""}, {"section_title": "Subjects and Data", "text": "Data sets from the students of two countries (i.e., Korea and Turkey) were compared in this study. Table 1 . Each booklet showed different distributions for content domains. The administration of Booklet 2 to Korean and Turkish students was selected for the DINA model analyses in this study due to the following reasons: (a) there were relatively more topics-13-in Booklet 2; (b) the subject areas of the items were distributed evenly-nine items for Numbers, nine items for Algebra, seven items for Data and Chance, and seven items for Geometry; and (c) the cognitive domains among the items were also distributed evenly-10 items required knowing, 13 items required applying, and nine items required reasoning. Booklet 2 was composed of Block 2 and Block 3 with 32 items, including 15 multiple choice and 17 constructed response items. There were 368 Korean students and 488 Turkish students who had taken Booklet 2. "}, {"section_title": "Construction of Q-Matrix", "text": "Attributes, which are used to define skills required to solve a specific item, were adopted from the Common Core State Standards for Mathematics (CCSSM; Common Core State Standards Initiative, 2010). The CCSSM was developed as a result of recognizing the need for a more focused and coherent mathematics curriculum in the United States to improve the quality of mathematics education and to increase mathematics achievement to the level of high-performing countries (Common Core State Standards Initiative, 2010). Therefore, standards from high-performing countries played a significant role in the development of the CCSSM (Common Core State Standards Initiative, 2014). Thus, in this study, the CCSSM was used to determine our attributes. By means of carefully examining TIMSS items and the standards, a list of 13 attributes (see Table 2 ) was created. In order to generate attributes that cover all possible skills, some of the two related standards were combined and separated with semi-colons. Using the attribute list in Table 2 , 32 items were coded independently by four doctoral students with advance degrees in mathematics education at one large public university in the Southeast. An attribute was included in our Q-matrix if at least two coders agreed that an item measured that attribute (see Table 3 ).\nThe attributes in the Q-matrix are independently generated by considering the required steps to solve each item. For example, in Item 6, students were given a picture of a rectangular garden that had a (x + 4)-meter width and an x-meter height (see Figure 1 ). The garden consisted of two small rectangular gardens and one rectangular path. The path was 1 meter wide and was between the two small gardens. Students were asked to calculate the total area of the two small rectangular gardens, which were shaded, in . In order to solve this problem, students need to master three attributes (Attributes 4, 5, and 11). First, they must understand the concept of area and relate area to multiplication-Attribute 11. Second, they need to multiply width and height for the big rectangular garden and for the rectangular path to calculate their areas. These two multiplication operations involve using algebraic expressions and require applying previous knowledge of arithmetic to algebra-Attribute 4. Third, they must know the distribution property, which also requires applying previous knowledge of arithmetic to algebra, and understand that the equivalent expressions of and are and x-Attribute 5. In the last step, they can obtain the area of the shaded garden as ( ) by subtracting x from . This last step also requires mastery of Attribute 4, since students who master Attribute 4 can apply arithmetic operations to algebraic equations. A student can solve this problem also by subtracting 1-meter from (x + 4)-meter and multiplying (x + 3)-meter by xmeter. Students also need to master Attributes 4, 5, and 11 to use this method. Note that one item "}, {"section_title": "243", "text": "(Item M052503A) was dropped when constructing the Q-matrix, because Item M052503A and Item M052503B were identical in the original 32-item list. Thus, only 31 items were used to create our Qmatrix (see Table 3 ). 3 A13-Investigates chance processes and develops, uses, and evaluates probability models."}, {"section_title": "4", "text": ""}, {"section_title": "Data Analysis", "text": "As outlined in the TIMSS 2011 assessment framework, the TIMSS items were assessed using a three-parameter logistic item response theory (3PL IRT) model. This comparative study attempted to analyze TIMSS data sets for Korea and Turkey using a DINA model in order to present an application of a CDA-based analysis. As de la Torre and Lee (2008) showed, the results of the DINA model are consistent with that of the IRT models for the same data. Item  Item ID  1  2  3  4  5  6  7  8  9  10  11  12  13  1  M052216  1  1  0 In addition to responses from Korean and Turkish students to the TIMSS eight grade mathematics assessment, the attributes (see Table 2 ) and Q-matrix (see Table 3 ) were also inputted into a DINA model. Since the TIMSS mathematics items included multiple choice and constructed responses, we dichotomized (0 = wrong answer, 1 = correct answer) those items for use with the dichotomous DINA model in this study. The DINA model parameters were estimated using maximum likelihood estimation with an expectation-maximization (EM) algorithm. All analyses were conducted using an object-oriented software package called OxEdit (Doornik, 2003) in order to obtain DINA model estimations using expectation-maximization (EM) algorithm. This program was chosen for analyses because it was a free software unlike other commercial software packages. The codes for the DINA model were requested from de la Torre (personal communication, February, 2014) . The results of the two countries were compared in order to identify the weaknesses and strengths of the students of each country. Item parameter estimates and attribute mastery prevalence estimates are presented in the Results section. In addition, 3PL IRT model estimations were obtained using maximum likelihood estimation method for comparison purpose."}, {"section_title": "Journal of Measurement and Evaluation in Education and", "text": ""}, {"section_title": "RESULTS", "text": "As presented above, the DINA model provides one slipping and one guessing parameter per item. These two parameters are equal across attributes. The DINA-based discrimination index (de la Torre, 2008) can also be calculated using slipping and guessing parameters for each item (i.e., \u03b4 = 1 \u2212 g \u2212 s). The item discrimination index refers to the probability of correctly solving an item without the effect of guessing and slipping parameters. Put differently, it is the difference in probabilities of a correct response between and . Slipping, guessing and discrimination parameter estimates for Korean and Turkish samples are presented in Table 6 . Sixty-two item parameter estimates (31"}, {"section_title": "\u015een, S., Ar\u0131can, M. / A Diagnostic Comparison of Turkish and Korean Students' Mathematics Performances on the TIMSS 2011 Assessment", "text": ""}, {"section_title": "___________________________________________________________________________________", "text": "\n"}, {"section_title": "___________________________________________________________________________________________________________________", "text": "\n"}, {"section_title": "ISSN: 1309 -6575 E\u011fitimde ve Psikolojide \u00d6l\u00e7me ve De\u011ferlendirme Dergisi Journal of Measurement and Evaluation in Education and Psychology", "text": "\n"}, {"section_title": "245", "text": "guessing and 31 slipping parameters) were obtained for each sample. In addition to item parameters, in total 2 13 = 8,192 attribute profile parameters were estimated for the 13 attributes listed in Table 2 . Fit statistics for DINA model analyses are presented in Table 4 . Since IEA (The International Association for the Evaluation of Educational Achievement) used 3PL IRT model for TIMSS analyses, results of 3PL IRT model were also provided for two samples before presenting main DINA model results (see Table 5 ). Table 6 presents item parameter estimates for slipping, guessing and the discrimination index for both countries. The small slipping and guessing parameter estimates indicate that examinees who master the measured attributes are able to apply the attributes correctly. As shown in Table 6 , Items 4, 24, and 25 (the three with the lowest guessing and slipping parameter estimates) are the most informative items for Korean and Turkish samples. For example, for a Korean respondent who mastered Attribute 1, there is less than a 1% chance (s 4 = .009) that Item 4 is answered incorrectly. In contrast, a respondent who has not mastered Attribute 1 has no chance (g 4 = .000) of answering this item correctly. On the other hand, a Korean student has a 93% chance of answering Item 15 correctly even if he/she lacks at least one of two attributes (i.e., Attribute 1 or Attribute 2). It is desirable for a DINA model to have small guessing and slipping parameter estimates for a good model-data fit (Rupp et al., 2010) . Higher values of item guessing and slipping parameters could be an indication of item-specific model misfit (Rupp & Templin, 2008b) . DINA model item parameter estimates with high guessing values can be an indication item-specific misfit for Items 1, 7, 9, 15, 19, 29 and 31 in Korean data set while high slipping parameter estimates indicates possible misfits for Items 12, 14 and 21 in Turkey data set."}, {"section_title": "Item Parameters", "text": "The mean values for item guessing, slipping parameters and the discrimination index are presented in the last row of Table 4 . As can be seen in Table 6 , Korean students had higher guessing parameter estimates and lower slipping parameter estimates than Turkish students for most of the items. The mean item discrimination index for the Korean sample was lower ( = .525) than that ( = .619) for the Turkish sample (see Table 6 ). Both samples had high discrimination indices for most of the items. A high discrimination index indicates a greater difference of probabilities of correct responses between and . For most of the items, the item discrimination index was lower for the Korean sample than for the Turkish sample due to the higher guessing parameter estimates for the Korean sample. Among the 31 items, Items 1 (requires Attributes 1 and 2; Numbers), and 15 (requires Attributes 1 and 2; Numbers) had the lowest discrimination indices for the Korean sample due to their high guessing and low slipping parameter estimates. It should be noted that the item discrimination index for Item 24 was found to be very high (.999) for both the Korean and Turkish samples, indicating that Item 24 was very informative. This item appeared to discriminate probabilities of correct responses between and very well."}, {"section_title": "Attribute Probability and Attribute Prevalence", "text": "In addition to item parameter estimates, the DINA model provides respondent parameters estimates (attribute probability and attribute prevalence). Attribute probability assigns respondents to any of the C (2 A where A denotes the number of attributes) latent classes. As mentioned above, 8,192 classes exist for 13 attributes in our TIMSS example. The attribute prevalence estimate is obtained by summing the probabilities across all latent classes requiring that specific attribute. Attribute prevalence estimates are presented in Table 7 for the Korean and Turkish samples. For all of the 13 attributes, the"}, {"section_title": "Journal of Measurement and Evaluation in Education and Psychology", "text": ""}, {"section_title": "246", "text": "Korean sample had a higher attribute prevalence than the Turkish sample (see Table 7 ). These results indicate that Korean students are more likely to master all of the attributes. The probability of Turkish students mastering some attributes is also high (e.g., Attributes 3 and 11). Attribute 6 had the lowest probability value for the Turkish sample (.320) and the Korean sample (.609). Thus, Attribute 6, analyzes and solves linear equations and pairs of simultaneous linear equations, was difficult to master by eighth grade students. Besides Attribute 6, Turkish students also had difficulty in mastering Attributes 13, 7, 4, and 1. By means of examining the attribute prevalence estimates (see Table 7 ), it was concluded that Turkish students were particularly weak in mastering Attributes 1, 8, and 13 when compared to their Korean peers. These three attributes had the highest prevalence estimate differences for Korea and Turkey. Hence, while most of the Korean students mastered these three attributes, many Turkish students had "}, {"section_title": "247", "text": "difficulty in mastering them. On the contrary, Attributes 3, 11, and 5 had the lowest prevalence estimate differences for Korea and Turkey. That means the probability of Turkish students' mastery of those three attributes were close enough to the probability of Korean students' mastery. However, it should be noted that these three lowest prevalence estimate differences mainly occurred because of the increments on the probability of the Turkish students' mastery on those attributes, not because of the decrements on the probability of the Korean students' mastery. The top five highest attribute class profiles with the highest probability estimates are presented in Table 8 . These classes with highest probabilities were selected from 8,192 possible latent classes. The probability estimates listed in Table 8 can be interpreted as percentages, as the sum of probabilities for 8,192 different latent class profiles are equal to unity. For example, a probability value of .013 for a "}, {"section_title": "248", "text": "latent class profile indicates that only 13% of the respondents were assigned to this specific latent class. As shown in Table 8 , 44% of Korean students mastered all of the attributes (attribute class of 1111111111111), while only almost 13% of Turkish students mastered all of the attributes. Other latent class profiles with the highest probability values showed that Attributes 5, 6, 7, and 12 were difficult to master for Korean students (see bolded zeros in Table 8 ). Less than one percent (p = .0016) of Korean respondents appeared to master none of the attributes (attribute class of 0000000000000). The second largest latent class for Turkish students was the mastery of all attributes except for Attributes 5 and 6. The posterior probability of this latent class profile was .026. Therefore, Attribute 6 appeared to be difficult to master by Turkish students as it was not mastered by most of the Turkish students (see bolded zeros for Attribute 6 in Table 8 ). Furthermore, 1.0% of the Turkish students could not master any of the attributes, while another 1.0% of Turkish sample only mastered Attribute 3 (understands ratio concepts and uses ratio reasoning to solve problems; finds a percent of a quantity as a rate per 100). "}, {"section_title": "DISCUSSION", "text": "This study showed the application of a CDA-based assessment for a large-scale test data set, which has been originally analyzed with a traditional IRT model (3PL). CDM approach was selected, because it is possible to report a more detailed evaluation of students' performances on specific skills. Korea (the top performing country) and Turkey (the focus of the study) were selected for analyses in this study to show how a DINA model can be used to obtain fine-grained information about the performances of the students from these two countries. There are several advantages of the DINA model over traditional IRT models. For example, analyses based on IRT models provide a single overall score based on invariant item and ability parameters. Unlike IRT models, CDMs (e.g., the DINA model) are used to obtain qualitative information in addition to quantitative information. The qualitative part of the CDMs comes from a latent class based structure. Using this property, it was tried to show which "}, {"section_title": "249", "text": "skill profiles both Korean and Turkish students were assigned. This specific respondent information could be very useful for instructors and educational policy makers for demonstrating the mastery of each student on each attribute, which is important feedback for instructors. In addition to attribute masteries, a number of item parameter estimates can be estimated with DCMs, like item guessing, slipping, and discrimination parameters.\nThe results of the DINA model for the Korea and Turkey data sets provided different patterns for the strengths and weaknesses of the two countries. As in the original 3PL IRT analysis, the Korean sample showed a higher performance than the Turkish sample in this study. As expected, the posterior probability of mastering all of the attributes (i.e., 1111111111111) for Korean students was higher than that of Turkish students. Additionally, one percent of the Turkish sample mastered none of the attributes, while this percentage was less than one percent for the Korean sample. In addition, six percent of Turkish respondents mastered only one of the thirteen attributes. These findings were very crucial for diagnosing the most problematic attributes (or skills) for the Turkish sample. Another attribute related finding showed that attribute prevalence estimates were higher than .70 for all items except for Attribute 6 in the Korean sample. However, all of the attribute prevalence estimates were less than .70 for the Turkish sample.\nAs a result of examining the estimates provided in Table 7 , it was decided that Turkish students had difficulties mastering Attributes 4, 6, and 7. Because these three attributes were classified in the Algebra content domain, it was suggested that Turkish educators should pay more attention to eight graders' understanding of Algebra topics. They should especially focus on students' understanding of analyzing and solving linear equations and applying previous understandings of arithmetic to algebraic expressions. This result was consistent with the findings from Dogan and Tatsuoka (2008) who also stated Turkish students' weaknesses in algebra content domain when compared to American students. Furthermore, when compared to their Korean peers, Turkish students were particularly weak in mastering Attributes 1, 8, and 13. These three attributes had the highest prevalence estimate differences for Korea and Turkey. Hence, while most of the Korean students mastered these three attributes, many Turkish students had difficulties in mastering them. The items in which the mastery of Attribute 1 was required were all fractions and decimals items. Therefore, the results indicate Turkish students' weaknesses in the fractions and decimals subject area-especially with understanding fraction equivalence and ordering-compared to their Korean peers. Similarly, the mastery of Attribute 8 was required in solving geometry items; so, compared to their Korean peers, Turkish students did not perform well on the geometry items that involved drawing, constructing, and describing geometrical figures and the relationships between them. Additionally, except for one item, the mastery of Attribute 13 was necessitated in solving data and chance items. Hence, in comparison to Korean students, as in the Dogan and Tatsuoka (2008) study, Turkish students also did not perform well on the data and chance problems that investigated the chance process and using and evaluating probability models. On the contrary, the three lowest prevalence estimate differences between Korea and Turkey were obtained for Attributes 3, 11, and 5. Thus, it can be concluded that Turkish students performed relatively well on items that involved understanding ratio concepts and using ratio reasoning; recognizing perimeter and understanding concepts of area; and reasoning about and solving one-variable equations and inequalities.\nIt should be noted that model-data fit and item fit statistics may have an effect on the interpretations of item parameter estimates obtained from a DINA model. More appropriate conclusions can be made based on models with better fit. It is obvious that DINA models in this study did not show perfect fit to two data sets. Assuming that we have enough model-data fit, we can make several conclusions based on DINA model results. Under this condition, item parameter estimates from the DINA model can provide feedback for students from the two countries. Apparently, Korean students were less likely to slip and were more likely to guess correct answers. However, the Turkish sample yielded lower guessing parameter estimates and higher slipping parameter estimates, indicating possible problems with content knowledge or testing strategies. Item parameter estimates can also be used for improving measurement instruments. Results of item parameter estimates showed problems with several items. For example, Items 6 and 8 yielded higher slipping parameter estimates for both samples. Both items "}, {"section_title": "250", "text": "were classified under the algebra content domain, and Item 6 was a multiple choice item, whereas Item 8 was a constructed response item. Turkish students were also most likely to slip on Items 21, 14, 12, and 27. Considering Items 21, 14, and 12 were also constructed response items, it can be concluded that Turkish students were more likely to slip on constructed response items. Dogan and Tatsuoka (2008) also stated a similar weakness of the Turkish students' in their study. They observed that Turkish students did not perform well on the open-ended items and had difficulty constructing answers in comparison to selecting an answer from given alternatives. Therefore, the findings of this study suggest that Turkish educators and policy makers should pay more attention to teaching students how to deal with constructed response items instead of teaching test skills to solve multiple choice items.\nTo accomplish this, teachers should encourage students through verbal and written expressions of their mathematical understandings. In addition, item discrimination indices may also be useful for identifying poor items. For instance, Items 1 and 15 had the highest guessing parameters and lowest discrimination indices for the Korean sample. Hence, these two items were not very informative and required improvements.\nIn sum, various factors might have affected Korean and Turkish eight-grade students' performances on the TIMSS assessment. As previously discussed, Im and Park (2010) attributed Korean students' high achievement to the social and instructional factors. In a similar vein, when compared Chinese Taipei and Turkey on the TIMSS 2007 eight-grade science items, Ozturk and Ucar (2010) found that socioeconomics, parents' education level, and quality of schooling contributed to Turkish students' relatively low academic performance. In this study, our results identify situations for instructors where current curriculum may be improved to help students master some lacking attributes based on CDMbased feedback. As Leighton and Gierl (2007) stated, recent CDM studies have been applied for posthoc analyses and item analyses rather than constructing the tests (Chapter 7). Although, our study demonstrated that retrofitting of a CDM via the DINA model can be very useful for the TIMSS assessment, it is evident that more benefit can be obtained from CDM-based analyses when tests are designed using CDMs in advance."}]