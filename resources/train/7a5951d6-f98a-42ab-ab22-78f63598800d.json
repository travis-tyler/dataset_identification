[{"section_title": "", "text": "1-2 these materials are available under \"Data Products\" on the ECLS-K:2011 website, http://nces.ed.gov/ecls/kindergarten2011.asp. The ECLS-K:2011 is following a nationally representative sample of children from kindergarten through their elementary school years. It is a multisource, multimethod study that focuses on children's early school experiences. It includes interviews with parents, self-administered questionnaires completed by teachers and school administrators, and one-on-one assessments of children. During the kindergarten year, it also included self-administered questionnaires for nonparental before-and afterschool care providers. The ECLS-K:2011 is sponsored by the National Center for Education Statistics (NCES) within the Institute of Education Sciences (IES) of the U.S. Department of Education."}, {"section_title": "Background", "text": "The ECLS-K:2011 is the third and latest study in the Early Childhood Longitudinal Study (ECLS) program, which comprises three longitudinal studies of young children: the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K); the Early Childhood Longitudinal Study, Birth Cohort (ECLS-B); and the ECLS-K:2011. The ECLS program is broad in its scope and coverage of child development, early learning, and school progress. It draws together information from multiple sources, including children, parents, teachers, school administrators, and early care and education providers, to provide data for researchers and policymakers to use to answer questions regarding children's early educational experiences and address important policy questions. The ECLS-K:2011 provides current information about today's elementary school children. Also, coming more than a decade after the inception of the ECLS-K, the ECLS-K:2011 allows for cross-cohort comparisons of two nationally representative kindergarten classes experiencing different policy, educational, and demographic environments. The three studies in the ECLS program provide national data on children's developmental status at birth and at various points thereafter; children's transitions to nonparental care, early education programs, and school; and children's home and school experiences, growth, and learning. The ECLS program also provides data that enable researchers to analyze how a wide range of child, family, school, classroom, nonparental care and education provider, and community characteristics relate to children's development and to their experiences and success in school. Together, these three studies provide the 1-3 range and breadth of data needed to more fully describe and understand children's education experiences, early learning, development, and health in the late 1990s, 2000s, and 2010s. More information about all three of these studies can be found on the ECLS website (http://nces.ed.gov/ecls)."}, {"section_title": "Periods of Data Collection", "text": "The ECLS-K:2011 is following a cohort of children from their kindergarten year (the 2010-11 school year, referred to as the base year) through the 2015-16 school year, when most of the children are expected to be in fifth grade (exhibit 1-1). The sample includes both children who were in kindergarten for the first time and those who were repeating kindergarten during 2010-11. Although the study refers to later rounds of data collection by the grade the majority of children are expected to be in (that is, the modal grade for children who were in kindergarten in the 2010-11 school year), children are being included in subsequent data collections regardless of their grade level. 2 During the 2010-11 school year, when both a fall and a spring data collection were conducted, approximately 18,170 kindergartners from about 1,310 schools 3 and their parents, teachers, school administrators, and before-and after-school care providers participated in the study. Fall and spring data collections were also conducted during the first-grade year. While the fall kindergarten collection included the full ECLS-K:2011 sample, the fall first-grade collection was conducted with children in one-third of the sample of primary sampling units (PSUs) selected for the study. These children are referred to as the fall subsample. The data collection schedule for second grade was similar to the schedule for first grade, with a fall second-grade collection that included the same subsample of children from the fall of first grade and a spring collection that included the entire sample of children who participated in at least one of the two base-year data collection rounds. For third through fifth grade, spring data collections with the entire sample of children who participated in the base year are planned. 4 2 Children may not be in the modal grade due to retention in a grade or promotion to a higher grade ahead of schedule."}, {"section_title": "Overview of the Second-Grade Rounds of Data Collection", "text": "As described in chapter 1 of the base-year User's Manual, the ECLS-K:2011 collects information from children, parents, classroom teachers, special education teachers, and school administrators. In the base year, information was also collected from children's before-and after-school care providers. Data collection instruments for all of these different respondent types were included in the second-grade rounds of data collection, with the exception of the care provider questionnaires. The care provider component was included in the base year to obtain more information about young children's activities outside of school, which is particularly important for understanding differences in the educational environments of children attending full-day kindergarten and of those attending part-day kindergarten. The assessments and instruments used in second grade were largely the same as those used in kindergarten and first grade to allow for longitudinal analysis. However, the first-grade assessments and instruments were revised, as necessary, to make them appropriate for the second-grade data collections. For example, questions in the school administrator questionnaire asking about the school's first-graders were revised to ask about the school's second-graders. More detailed information about the second-grade study instruments, including how they differ from the instruments used in the kindergarten and first-grade rounds, is provided in chapter 2. 1-5"}, {"section_title": "ECLS-K:2011 Kindergarten-Second Grade (K-2) Data File", "text": "The ECLS-K:2011 Kindergarten-Second Grade (K-2) data file includes the base-year, firstgrade, and second-grade data encompassing both the fall and spring rounds of data collection in kindergarten, first, and second grade. The data file includes all students who participated during the kindergarten year even if they did not participate during the first-or second-grade rounds. Second-grade data for students who did not participate are set to \"system missing\" for the second-grade round or rounds in which they are nonrespondents. The K-2 public-use file (PUF) is intended to replace the previously released PUFs; the K-2 PUF includes all of the cases included in prior PUFs and has some important corrections and updates to previously released data, including the child assessment scores. In preparing data files for release, NCES takes steps to minimize the likelihood that individual schools, teachers, parents, or students participating in the study can be identified. Every effort is made to protect the identity of individual respondents. The process of preparing the files for release includes a formal disclosure risk analysis. Small percentages of values are swapped across cases with similar characteristics to make it very difficult to identify a respondent with certainty. The modifications used to reduce the likelihood that any respondent could be identified in the data do not affect the overall data quality. Analysts should be aware that the ECLS-K:2011 data file is provided as a child-level data file containing one record for each child who participated in the base year. The record for each child contains information from each of the study respondents: the child, as well as his or her parent, teacher(s), school administrator and, if applicable, before-or after-school care provider. The ECLS-K:2011 K-2 data are provided in an electronic codebook (ECB) that permits analysts to view the variable frequencies, tag selected variables, and prepare data extract files for analysis with SAS, SPSS, or Stata. The public-use versions of the data are available online."}, {"section_title": "Contents of Manual", "text": "The remainder of this manual contains more detailed information on the second-grade data collection instruments (chapter 2) and the direct and indirect child assessments (chapter 3). It also describes the ECLS-K:2011 sample design and weighting procedures (chapter 4), response rates and bias 1-6 analysis (chapter 5), and data preparation procedures (chapter 6). In addition, this manual describes the structure of the K-2 data file and the composite variables that have been developed for the file (chapter 7). The last chapter of the manual contains a short introduction to the Electronic Codebook (ECB) and how to use it (chapter 8). Additional information about the ECLS-K:2011 study design, methods, and measures can be found in the earlier round user's manuals noted above, as well as in the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011, Kindergarten Psychometric Report (Najarian et al. forthcoming) andthe Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), First-Grade and Second-Grade Psychometric Report (Najarian et al. forthcoming). Also, as noted earlier, additional information about the ECLS program can be found online at http://nces.ed.gov/ecls. This page intentionally left blank. 2-1"}, {"section_title": "DATA COLLECTION INSTRUMENTS AND METHODS", "text": "This chapter describes the data collection instruments used in the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011) second-grade rounds of data collection, including the child assessments, parent interview, school administrator questionnaires, and teacher questionnaires. 1 Differences between the kindergarten, first-grade, and second-grade rounds in the study instruments and data collection procedures are discussed. For more information on the kindergarten and first-grade data collection instruments and methods, consult the user's manuals for those rounds."}, {"section_title": "Data Collection Instruments", "text": "The design of the ECLS-K:2011 and its survey instruments is guided by a conceptual framework of children's development and learning that emphasizes the interaction among the various environments in which children live and learn and the resources within those environments to which children have access. A comprehensive picture of children's environments and experiences is created by combining information from children themselves, their parents, their school administrators, their teachers, and their kindergarten before-and after-school care providers. Exhibit 2-1 presents a listing of the ECLS-K:2011 data collection instruments and the rounds of data collection in which they were used. The instruments for the kindergarten, first-grade, and secondgrade collections are included on the ECLS-K:2011 kindergarten-second grade (K-2) restricted-use DVD and are available online at http://nces.ed.gov/ecls, with the exception of copyrighted materials or items adapted from copyrighted materials that cannot be publicly distributed without copyright holder and NCES permission. Study instruments and items for which copyright permissions are needed are discussed further in section 2.1.6. The information collected in the ECLS-K:2011 instruments can be used to answer a wide variety of research questions about how home, school, and neighborhood factors relate to children's cognitive, social, emotional, and physical development. Sections 2.1.1-2.1.5 describe the major topics covered in each instrument. 1 For ease of presentation, this chapter refers to all students as \"second-grade students\"; however, the reader should keep in mind that some children had been retained in a grade and a very small number of students had been advanced to a higher grade. These children are included in the group being referred to as second-graders."}, {"section_title": "2-2", "text": "Exhibit 2-1. Instruments used in the ECLS-K:2011 kindergarten, first-grade, and second-grade rounds of data collection: School years 2010-11, 2011-12, and 2012-  2-3"}, {"section_title": "Direct Child Assessment", "text": "In the second-grade data collections, children were assessed in reading, mathematics, and science in both the fall and the spring. The majority of the items included in the second-grade assessments had been included in the first-grade assessments. However, to ensure that the assessments adequately measured the knowledge and skills of the children as they progressed through school, new, more difficult items were added to the assessments in second grade, and easier items reflecting lower level first-grade skills were omitted. All children received the assessments designed for the second-grade collections, regardless of their actual grade level. In both the fall and the spring, students' executive function skills were assessed and their height and weight were measured. The assessments were administered directly to the sampled children on an individual basis by trained and certified child assessors. The battery of assessments was designed to be administered within about 60 minutes per child. 2 Child responses were entered by the assessor into a computer-assisted interviewing (CAI) program. In the fall second-grade data collection, a subsample of the children also had their hearing evaluated by specially trained health technicians. The health technicians typically conducted the 15-minute hearing evaluations immediately after each selected child's assessment and height and weight measurement. Two-stage assessment. The second-grade direct cognitive assessment included two-stage assessments for reading, mathematics, and science. For each assessment domain, the first stage of the assessment was a routing section that included items covering a broad range of difficulty. A child's performance on the routing section of a domain determined which one of three second-stage tests (low, middle, or high difficulty) the child was next administered for that domain. The second-stage tests varied by level of difficulty so that a child would be administered questions appropriate for his or her demonstrated level of ability for each of the cognitive domains. The purpose of this adaptive assessment design was to maximize accuracy of measurement while minimizing administration time. Language screener for children whose home language was not English. Unlike in kindergarten and first grade, a language screener was not used in second-grade for children whose home language was not English. By the spring of first grade, nearly all children (99.9 percent) were routed through the assessment in English; therefore the language screener was not administered beyond spring of first grade. 2 Actual assessment time averaged about 72 minutes per child. Together the reading, mathematics, and science assessments took longer than expected, at an average of 58 minutes. The executive function assessments averaged 10 minutes, and measurement of height and weight took about 4 minutes."}, {"section_title": "2-4", "text": "Cognitive domains. The second-grade cognitive assessment focused on four domains: reading (language use and literacy), mathematics, science, and executive function (working memory and cognitive flexibility). For the reading, mathematics, and science assessments, assessors asked the children questions related to images that were presented on a small easel, such as pictures, words, or short sentences for reading or numbers and number problems for mathematics. For the reading assessment, children were also asked questions about short reading selections they were asked to read in a passages booklet developed for the assessment. Certain items required children to point to a response, while others required children to tell the assessor their answers. Children were not required to write their answers or explain their reasoning. The executive function component included a computer-administered card sort task, for which children entered responses in the assessor's laptop computer, and a backward digit span task, for which children provided verbal responses to the assessor. A brief description of each of the cognitive assessment components follows. Reading (language and literacy). The reading assessment included questions measuring basic skills (print familiarity, letter recognition, beginning and ending sounds, rhyming words, and word recognition), vocabulary knowledge, and reading comprehension. Reading comprehension questions asked the child to identify information specifically stated in text (e.g., definitions, facts, supporting details); to make complex inferences within texts; and to consider the text objectively and judge its appropriateness and quality. The reading assessment began with a set of 29 routing items, with the children's score on these items determining which second-stage form (low, middle, or high difficulty) the child received. Mathematics. The mathematics assessment was designed to measure skills in conceptual knowledge, procedural knowledge, and problem solving. The assessment consisted of questions on number sense, properties, and operations; measurement; geometry and spatial sense; data analysis, statistics, and probability; and patterns, algebra, and functions. A set of 20 routing items was administered to all children, and the score on these items determined which second-stage test (low, middle, or high difficulty) a child received. Most of the text that the children could see on the easel pages, for example, question text for word problems or graph labels, was read to them by the assessor to reduce the likelihood that the children's reading ability would affect their mathematics assessment performance. 3 Paper and pencil were offered to the children for use during the mathematics assessment, and children were periodically reminded of the availability of paper and pencil as part of the assessment protocol. Wooden cubes were also provided on certain assessment items as an aid to children. 3 Numbers were read to the child only when the question text referenced the number."}, {"section_title": "2-5", "text": "Science. The science assessment domain included questions about physical sciences, life sciences, environmental sciences, and scientific inquiry. The science assessment included 19 routing items that all children received, followed by one of three second-stage forms (low, middle, or high difficulty). As with reading and mathematics, the second-stage form children received depended on their responses to the routing items. The questions, response options, and any text the children could see on the easel pages (for example, graph labels) were read to the children to reduce the likelihood that their reading ability would affect their science assessment score. To measure cognitive flexibility, children were administered the Dimensional Change Card Sort (DCCS) (Zelazo 2006). Different versions of the DCCS were used in different rounds of data collection because there was no single task that was age appropriate across all rounds of data collection when the study began. During the kindergarten and first-grade rounds, the hard-copy, tabletop version of the DCCS, as described in Zelazo 2006, was administered using physical cards that children were asked to sort into piles. Because the tabletop version of the DCCS would have been too easy for the majority of the study children during the second-grade rounds, in both the fall and the spring children were administered a new, age-appropriate, computerized version of the DCCS in which the \"cards\" are presented on a computer screen and children sort them into \"piles\" on the computer screen using keys on the keyboard to indicate where to place each card. The computerized task was developed as part of the National Institutes of Health (NIH) Toolbox for the Assessment of Neurological and Behavioral Function and is appropriate for ages 3-85 (Zelazo et al. 2013). The Toolbox DCCS has two versions that differ based on the age of the child: one version for children 7 years and younger and one for children 8 years and older. The task had been under development during the kindergarten and first-grade rounds of data collection but became available in time to be incorporated into the second-grade data collections. The ECLS-K:2011 used the version for children 8 years and older. Although the construct assessed in the tabletop and the computer versions is the same, the scoring and the way in which the construct is assessed differ across the two tasks (for information on scoring, see section 3.2.1). Like the hard-copy version of the DCCS administered in kindergarten and first grade, the computerized version asks children to sort cards either by shape or color. However, rather than administer the cards in sections with a consistent sorting rule (with cards first sorted only by color, then only by shape, and finally by color or shape depending on whether a card had a black border), in the computerized DCCS the sorting rules are intermixed across the 30 trials of the task. One rule is more common than the 2-6 other to build a response tendency (i.e., a response that is \"preferred\" because it happens more frequently, resulting in a predisposition to respond in that manner). Also, whereas performance on the tabletop version is measured by sorting accuracy, performance on the computerized version is measured as a function of both accuracy and reaction time. Reaction time is calculated based on reaction time only for trials using the sorting rule that is presented less often and only when there is a correct response. The reaction time of the less frequent trials or \"switch trials\" is of most interest because when a child is predisposed to respond in a particular way it is harder and takes more time to inhibit that response tendency and switch the response to maintain accuracy. As children get older, it is important to incorporate reaction time into the DCCS score because older children and adults tend to slow down in order to respond accurately. Younger children do not tend to show a speed/accuracy trade-off, and therefore accuracy is a better metric of performance for young children (Davidson et al. 2006). Performance on the computerized version of the DCCS is derived from a formula that takes into consideration both accuracy and reaction time (Zelazo et al. 2013;Slotkin et al. 2012). After the card sort, children were administered the Numbers Reversed task. In this task, they were asked to repeat strings of orally presented numbers in reverse order. The sequence of numbers became increasingly longer, up to a maximum of eight numbers, until the child got three consecutive number sequences incorrect (or completed all number sequences). The task was ended when children responded incorrectly to a certain number of items in a row so that they would not be asked to continue at a level that was too difficult. Height and weight measurement. In addition to the cognitive domains described above, children's height and weight were measured during each data collection. A Shorr board (a tall wooden stand with a ruled edge used for measuring height) and a digital scale were used to obtain the measurements. 4 Assessors recorded the children's height (in inches to the nearest one-quarter inch) and weight (in pounds to one decimal place) on a height and weight recording form and then entered the measurements into a laptop computer. Each measurement was taken and recorded twice to ensure reliable measurement. Hearing evaluations. In the fall second-grade data collection, a subsample of the children also had their hearing evaluated by specially trained health technicians. Study protocol called for the health technicians to conduct the 15-minute hearing evaluations immediately after each selected child's 4 The Shorr board is manufactured by Weigh and Measure, LLC, and is model ICA. The digital scale was Seca Bella model 840."}, {"section_title": "2-7", "text": "assessment and height and weight measurement. 5 First, the health technician asked the child a few questions about his or her hearing and recent experiences that could affect the results of the evaluation, including whether the child had an earache or recent cold or had recently heard any loud noises. Next, the child's ears were visually examined to see if there was any blockage that could affect the evaluation. The child's responses to the questions and the results of the visual examination were entered into a laptop computer. Then the health technician used a tympanometer to measure inner-ear functioning. Finally, the child listened to short tones of various pitches and decibel levels that were presented through headphones connected to an audiometer in order to determine hearing thresholds (the softest sounds the child could hear) for each ear. The data collected from the tympanometer and audiometer were automatically transferred from the hearing equipment and saved to the health technician's laptop."}, {"section_title": "Parent Interview", "text": "As in kindergarten and first grade, a parent interview was conducted during the fall and spring of second grade. The average length of the parent interview was approximately 12 minutes in the fall of second grade and 31 minutes in the spring of second grade. The spring second-grade parent interview was shorter than the spring kindergarten and spring first-grade parent interviews, but captured much of the same information. Similar to the fall first-grade parent interview, the fall second-grade parent interview was relatively short and focused on children's experiences during the summer. In both the fall first-and second-grade parent interviews parents provided information about various educational and enrichment activities children participated in during the previous summer, including educational activities in the home, use of a computer for educational purposes, reading books from summer book lists provided by the school, going to the library or bookstore, playing outside, outings with family members, camps, summer school, tutoring, therapy services or special education programs, hours spent watching television and playing video games, and nonparental child care. In addition, information about children's demographic characteristics was collected if it had not been collected in kindergarten or first grade. In the fall of second grade, parents also provided information about children's ear infections, earaches, and hearing. The spring second-grade parent interview included many of the same questions that were included in the kindergarten and first-grade rounds of the study, for example, questions about parent 2-8 involvement in the child's school, homework, time children spent playing video games, children's participation in out-of-school activities, whether there had been a change in the relationship of one of the parent figures to the child (e.g., adoption), and child health and well-being. Questions about attending a new school, confirmation of the language(s) spoken in the home, nonresident parents' country of origin, parent-child communication, peer victimization, and the education levels of the respondents' parents were asked for the first time in the spring second-grade parent interview. Exhibit 2-2 shows the content areas included in the parent interview in the fall and spring of kindergarten, first grade, and second grade by data collection round. While many of the same topics were addressed in multiple rounds, there were differences in the specific questions asked for each topic. For example, questions about home activities in the fall of second grade included questions about reading to the child during a typical week of the previous summer, participation in camps, and attendance at summer school, whereas questions in that section in the spring of second grade asked about current reading to the child in a typical week, home activities, and the child's extracurricular activities outside of school hours. The respondent to the parent interview, which was conducted by telephone for most cases, was usually a parent or guardian in the household who identified himself or herself as the person who knew the most about the child's care, education, and health. During the fall and spring second-grade data collection rounds, interviewers attempted to complete the parent interview with the same respondent who completed the parent interview in the previous rounds. Another parent or guardian in the household who knew about the child's care, education, and health was selected if the previous respondent was not available. The parent interview was fully translated into Spanish before data collection began and was administered by bilingual interviewers if parent respondents preferred to speak in Spanish. The parent interview was not translated into other languages because it was cost prohibitive to do so. However, interviews were completed with parents who spoke other languages by using an interpreter who translated the English version during the interview."}, {"section_title": "2-9", "text": "Exhibit 2-2. Parent interview topics, by round of data collection in the ECLS-K:2011: School years 2010-11, 2011-12, and 2012-13 Parent interview content Parental discipline, warmth, and emotional supportiveness X X X Time father/other adult male spends with child X Welfare and other public transfers X X X X 1 In the fall of kindergarten, questions were asked about current child care and child care in the year before kindergarten. In the spring of kindergarten, questions about child care in the year before kindergarten were asked if information had not been collected in the fall. In the fall of first and second grades, questions were about child care during the previous summer. In the spring of first and second grades, questions asked about current child care. 2 Questions about child demographic characteristics were asked in the fall and spring of kindergarten and then asked in later rounds of the study if the information was missing from a previous round. 3 Questions in the fall first-and second-grade interviews were about services for special needs or participation in a special education program over the previous summer. Questions about disabilities and services in other rounds of the study were not limited to the past summer. 4 Asked if information had not been collected in a previous round. 5 Questions in the fall first-and second-grade interviews were about home activities, outings with family members, camps, and summer school during the previous summer. Questions in other rounds of the study were not limited to the summer. 6 In the spring of kindergarten, this was asked of respondents who did not have a fall kindergarten parent interview. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2010, spring 2011, fall 2011, spring 2012, fall 2012, and spring 2013."}, {"section_title": "General Classroom Teacher Questionnaires", "text": "During the second-grade year, the general classroom teachers of children in the study completed a self-administered hard-copy questionnaire about themselves and their classrooms as well as child-level questionnaires for each child in their classrooms who was participating in the ECLS-K:2011. The purpose of the teacher-level questionnaire was to collect information about the classroom 2-11 environment and experiences that may relate to children's academic and social development. It included questions about the classroom and student characteristics, class materials, instructional practices and curricula, evaluation practices, and parent involvement. It also included questions on the teacher's background, teaching experience, staff development and support activities, and attitudes about teaching and the school climate. The purpose of the child-level questionnaires was to collect information specifically about each study child's experiences and performance in the classroom. Information was collected in the child-level questionnaires about the children's academic and cognitive abilities, behavior, social skills, and achievement group placement in mathematics and reading, if applicable. In the fall of the second-grade year, teachers were asked to complete only a short child-level teacher questionnaire; there was no fall teacher-level questionnaire. The fall second-grade version of the child-level questionnaire contained some of the same items as the fall kindergarten and fall first-grade versions, namely a small set of indicators that were useful to measure early in the school year. Also, the fall first-grade and fall second-grade questionnaires included items about summer assignments. The differences between the fall first-grade and fall second-grade child-level questionnaires pertained to the Academic Rating Scale. The fall second-grade Academic Rating Scale for language and literacy skills was updated to reflect appropriate skills for second grade and contained four items rather than two. Similar to the spring first-grade collection, both a teacher-level questionnaire and a childlevel questionnaire were included in the spring second-grade data collection. The teacher-level questionnaire used in the spring of the second-grade year was very similar in content and length to those that were used in the spring kindergarten and spring first-grade collections. There were no new constructs added for second grade. The items in the section on \"Instructional Activities and Curricular Focus\" covering skills and activities for reading/language arts and mathematics were revised to reflect secondgrade standards in the Common Core State Standards, although some items related to first-grade standards that had been included in the first-grade teacher-level questionnaire were retained. The spring child-level questionnaire was very similar to the one used in the first-grade data collection for children in first grade. A few items were omitted for second grade. Unlike the spring kindergarten and first-grade child-level questionnaires, the spring second-grade questionnaire contained no Academic Rating Scales. However, a single multi-part item rating the child's overall academic skills in specific subject areas was retained. New items were added to the child-level questionnaire in second grade to capture whether the student had been victimized by other students and whether the student had victimized others."}, {"section_title": "2-12", "text": "Exhibits 2-3 and 2-4 show the topics addressed in the kindergarten, first-grade, and secondgrade teacher-level questionnaires and child-level questionnaires, respectively, by data collection round. Exhibit 2-3. General classroom teacher teacher-level questionnaire topics, by round of data collection in the ECLS-K:2011: School year 2010-11, spring 2012, and spring 2013 Teacher's views on teaching, school climate, and environment X X X X X Teacher's experience, education, and background X X 1 X X X 1 In the spring of kindergarten, teachers new to the study were asked to complete a supplemental teacher-level questionnaire in order to collect information on their experience, education, and background that had been collected from other teachers in the fall. Teachers who provided information in the fall were not asked the same questions again in the spring. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2010, spring 2011, spring 2012, and spring 2013."}, {"section_title": "2-13", "text": "Exhibit 2-4. General classroom teacher child-level questionnaire topics, by round of data collection in the ECLS-K:2011: School years 2010-11, 2011-12, and 2012-13 Child-level questionnaire content "}, {"section_title": "Special Education Teacher Questionnaires", "text": "As was done in the kindergarten and first-grade years, a set of special education teacher questionnaires was completed in the spring of the second-grade year for each participating child with an"}, {"section_title": "2-14", "text": "Individualized Education Program (IEP) or equivalent program on record with the school. The respondent to the questionnaire could have been a staff member identified as the child's special education teacher, a related service provider if the child was not taught by a special education teacher, or the child's general classroom teacher if that teacher provided all of the child's education and services required by an IEP. Similar to the model used for the general classroom teacher questionnaires, two self-administered hardcopy instruments were used, a teacher-level questionnaire and a child-level questionnaire. The special education teacher-level questionnaire used in second grade was almost identical to the questionnaire used in prior rounds of data collection. It collected information on the special education teacher's background, education, teaching experience, teaching position, and caseload. The special education teacher child-level questionnaire addressed the following topics: current services received through an IEP, child's disabilities (primary and all those for which the child received services), IEP goals and meeting those goals, classroom placement, expectations regarding general education goals, the special education teacher's communication with other teachers and the child's parents, grade placement, and participation in assessments. Exhibit 2-5 shows the topics addressed in the kindergarten, first-grade, and second-grade special education teacher-level and child-level questionnaires by data collection round. "}, {"section_title": "School Administrator Questionnaires", "text": "As in first grade, there were two versions of the school administrator questionnaire (SAQ) used in second grade: (1) a version for schools that were new to the study or for which a completed school administrator questionnaire was not received in a prior data collection, and (2) a shorter version for schools for which a school administrator questionnaire had been completed in the kindergarten or firstgrade year. To reduce respondent burden, the shorter version did not include questions for which the responses were not expected to change significantly from year to year, for example, grades offered by the school, type of school (public, private, magnet, charter), adequacy of facilities, and grade retention policies."}, {"section_title": "2-16", "text": "The school administrator questionnaires were hard-copy paper questionnaires completed by the school principal/administrator and/or his or her designee during the spring data collection round of the second-grade year. The school administrator questionnaires addressed the following topics: school characteristics, facilities, and resources; school-family-community connections; school policies and practices; implementation of Response to Intervention programs; school programs for particular populations (language minority children and children with special needs); federal programs; staffing and teacher characteristics; and school administrator characteristics and background. The school administrator questionnaires for the second grade-both those for new schools and those for returning schools-were very similar to those in the first-grade year. An item was added on the length of the school day, and items relating to the characteristics of the school breakfast and lunch programs were omitted. Items that related specifically to first grade were reworded to refer to second grade. Exhibit 2-6 shows the topics addressed in the kindergarten, first-grade, and second-grade school administrator questionnaires by data collection round.  (returning  schools) School characteristics, facilities, and resources X X X X X School-family-community connections X X X X X School policies and practices X X X X X Response to Intervention programs X X X X School programs for particular populations (language minority children and children with special needs) X X X X X Federal programs X X X X X Staffing and teacher characteristics X X X X X School administrator characteristics and background X X X X X SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011): spring 2011, spring 2012, and spring 2013."}, {"section_title": "ECLS-K:2011 DIRECT AND INDIRECT ASSESSMENT DATA", "text": "This chapter provides information primarily about the direct and indirect assessment data from the second-grade collections of the ECLS-K:2011. The chapter begins with a description of the direct cognitive assessments, providing information about the scores available in the data file. The chapter then presents information on the executive function assessments. Finally, the chapter closes with information on teacher assessments of children's cognitive and socioemotional knowledge and skills. This chapter includes some information about assessment data from the kindergarten and first-grade rounds of data collection in three instances: when those data have been changed since their release on previous files, when new data from those rounds have been added to the kindergarten through second-grade (K-2) data file, and when necessary to illustrate how second-grade data related to a particular measure or construct differ from data related to the same measure or construct released for the earlier rounds. Information about assessments that were used in prior rounds but not in second grade, for example the Spanish Early Reading Skills (SERS) assessment, and about scores that were produced only for earlier rounds, such as raw number-right scores, can be found in the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), User's Manual for the ECLS-K:2011 Kindergarten Data File and Electronic Codebook, Public Version (NCES 2015-074) (Tourangeau et al. 2015), hereinafter referred to as the base-year User's Manual, andthe Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), User's Manual for the ECLS-K:2011 Kindergarten-First Grade Data File and Electronic Codebook, Public Version (NCES 2015-078) (Tourangeau et al. 2015)."}, {"section_title": "Direct Cognitive Assessment: Reading, Mathematics, and Science", "text": "The kindergarten, first-grade, and second-grade direct cognitive assessments measured children's knowledge and skills in reading, mathematics, and science. This section presents information about the assessment scores available in the data file. More detailed information about the development of the scores, including a more complete discussion of item response theory (IRT) procedures, can be found in the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011, Kindergarten Psychometric Report (Najarian et al. forthcoming) and in the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011, First-Grade and Second-Grade Psychometric Report (Najarian et al. forthcoming). A description of the administration of the direct assessments is provided in chapter 2. It must be emphasized that the assessment scores described below are not directly comparable with those developed for the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K). Although the IRT procedures used in the analysis of data were similar in the ECLS-K and in the ECLS-K:2011, each study incorporated different items and the resulting scales are different."}, {"section_title": "IRT-Based Scores Developed for the ECLS-K:2011", "text": "Broad-based scores using the full set of items administered in the kindergarten, first-grade, and second-grade assessments in reading, mathematics, and science were calculated using IRT procedures. IRT is a method for modeling assessment data that makes it possible to calculate an overall score for each domain measured for each child that can be compared to scores of other children regardless of which specific items a child is administered. This method was used to calculate scores for the ECLS-K:2011 because, as discussed in chapter 2, the study employed a two-stage assessment (in reading and mathematics in kindergarten and in reading, mathematics, and science in first and second grades) in which children were administered a set of items appropriate for their demonstrated ability level, rather than all the items in the assessment. Although this procedure resulted in children being administered different sets of items, there was a subset of items that all children received (the items in the routing tests, plus a set of items common across the different second-stage forms). These common items were used to calculate scores for all children on the same scale. IRT also was used to calculate scores for all children on the same scale for the science assessment fielded in the spring of kindergarten even though that assessment was not two-stage. In that assessment, the assortment of items a child received was not dependent upon routing to a second stage, but instead on omissions by the child or the discontinuation of the administration of the assessment. In those cases, IRT was used to estimate the probability that a child would have provided a correct response when no response was available."}, {"section_title": "3-2", "text": "IRT uses the pattern of right and wrong responses to the items actually administered in an assessment and the difficulty, discriminating ability, 1 and \"guess-ability\" of each item to estimate each child's ability on the same continuous scale. IRT has several advantages over raw number-right scoring. By using the overall pattern of right and wrong responses and the characteristics of each item to estimate ability, IRT can adjust for the possibility of a low-ability child guessing several difficult items correctly. If answers on several easy items are wrong, the probability of a correct answer on a difficult item would be quite low. Omitted items are also less likely to cause distortion of scores, as long as enough items have been answered to establish a consistent pattern of right and wrong answers. Unlike raw number-right scoring, which treats omitted items as if they had been answered incorrectly, IRT procedures use the pattern of responses to estimate the probability of a child providing a correct response for each assessment question. Finally, IRT scoring makes possible longitudinal measurement of gain in achievement, even when the assessments that are administered to a child are not identical at each point, for example, when a child was administered different levels of the second-stage form in the fall and spring data collections within one year or different sets of items across grades."}, {"section_title": "Theta and the Standard Error of Measurement (SEM) of Theta", "text": "A theta score is provided in the ECLS-K:2011 data file for each child who participated in the direct cognitive assessment for each cognitive domain assessed and for each data collection in which the assessment was administered. The theta score 2 is an estimate of a child's ability in a particular domain (e.g., reading, mathematics, or science) based on his or her performance on the items he or she was actually administered. The theta scores are reported on a metric ranging from -6 to 6, with lower scores indicating lower ability and higher scores indicating higher ability. Theta scores tend to be normally distributed because they represent a child's latent ability and are not dependent on the difficulty of the items included within a specific test. The standard error of theta provides a measure of uncertainty of the theta score estimate for each child. Adding and subtracting twice the standard error from the theta score estimates provides an approximate 95 percent confidence interval or range of values that is likely to include the true theta score. Unlike classical item theory, in which the precision of the scores is consistent across all examinees, IRT allows the standard error to vary. Larger standard errors of measurement can be the result of estimations 1 The discriminating ability describes how well changes in ability level predict changes in the probability of answering the item correctly at a particular ability level. 2 Theta is iteratively estimated and re-estimated and the theta score is derived from the means of the posterior distribution of the theta estimate."}, {"section_title": "3-3", "text": "of thetas in the extremes of the distribution (very low or very high ability) or for children who responded to a limited number of items (i.e., children who responded to all items administered generally have lower standard errors of measurement than those children responding to fewer items because more information about their actual performance is available, thereby making estimates of their ability more precise). Tables 3-1 and 3-2 list the names of the variables pertaining to the reading, mathematics, and science IRT theta scores and standard errors of measurement available in the data file, along with the variable descriptions, value ranges, weighted means, and standard deviations. 3 As can be seen in the tables, theta scores are available for all data collection rounds for reading and mathematics. For science, theta scores are available for all rounds except the fall of kindergarten; the science assessment was not included in that first round of data collection. The variable names and descriptions end with K2, indicating these are scores released on the kindergarten-second grade (K-2) longitudinal data file. The method used to compute the theta scores allows for the calculation of theta for a given round that will not change based on later administrations of the assessments (which is not true for the scale scores, as described in the next section). Therefore, for any given child, the kindergarten, first-grade, and second-grade theta scores provided in subsequent data files will be the same as theta scores released in earlier data files, with one exception: the reading thetas provided in the base-year data file. After the kindergarten-year data collection, the methodology used to calibrate and compute reading scores changed; therefore, the reading thetas reported in the base-year file are not the same as the kindergarten reading thetas provided in the files with later-round data. Any analysis involving kindergarten reading theta scores and reading theta scores from later rounds, for example an analysis looking at growth in reading knowledge and skills between the spring of kindergarten and the spring of first grade, should use the kindergarten reading theta scores from a data file released after the base year. The reading theta scores released in the kindergarten-year data file are appropriate for analyses involving only the kindergartenround data; analyses conducted with only data released in the base-year file are not incorrect, since those analyses do not compare kindergarten scores to scores in later rounds that were computed differently. However, now that the recomputed kindergarten theta scores are available in the kindergarten through first-grade and kindergarten through second-grade data files, it is recommended that researchers conduct any new analyses with the recomputed kindergarten reading theta scores. For more information on the 3 The name and description for each variable in the tables begin with an \"X,\" indicating that it is a derived/calculated variable, and a data collection round number (1 for the fall kindergarten round, 2 for the spring kindergarten round, 3 for the fall first-grade round, 4 for the spring first-grade round, 5 for the fall second-grade round, and 6 for the spring second-grade round). These variable naming conventions are used for all the variables mentioned in this chapter. More information about variable naming conventions can be found in chapter 7. 3-4 methods used to calculate theta scores, see the ECLS-K:2011 First-Grade and Second-Grade Psychometric Report (Najarian et al. forthcoming).  "}, {"section_title": "Scale Scores", "text": "The IRT-based overall scale score for each content domain is an estimate of the number of items a child would have answered correctly in each data collection round if he or she had been administered all of the questions for that domain that were included in the kindergarten, first-grade, and second-grade assessments (that is, all of the 120 unique questions in the router and the three second-stage reading forms administered in kindergarten, first grade, and second grade; all of the 113 unique questions in the router and the three second-stage mathematics forms administered in kindergarten, first grade, and second grade; and all of the 64 unique items administered in the router and three second-stage science forms in first and second grade and in the single-stage kindergarten science form)."}, {"section_title": "3-6", "text": "To calculate the IRT-based overall scale score for each domain, a child's theta is used to predict a probability for each assessment item that the child would have gotten that item correct. Then, the probabilities for all the items fielded as part of the domain in every round are summed to create the overall scale score. Because the computed scale scores are sums of probabilities, the scores are not integers. Gain scores in each domain may be obtained by subtracting the IRT scale scores at an earlier round from the IRT scale scores at a later round. For example, subtracting the fall kindergarten mathematics score from the spring kindergarten mathematics score would result in a score indicating gain across the kindergarten year. Similarly, a gain score from kindergarten entry to the end of second grade would be obtained by subtracting the fall kindergarten mathematics score from the spring second-grade mathematics score. However, the scale scores are only comparable across rounds within a single data file. In other words, the scale scores for a given domain in the K-2 data file are all comparable to one other, but they are not comparable to the scale scores for that domain reported in the base-year file or in subsequent files. Although the thetas remain the same for a given domain across rounds, the scale scores are recomputed for each file because the scale scores represent the estimated number correct for all items across all assessments administered; the total number of items in the pool expands each year as more difficult items are added to the assessments. Scores for different subject areas are not comparable to each other because they are based on different numbers of questions and content that is not necessarily equivalent in difficulty. For example, if a child's IRT scale score in reading is higher than in mathematics, it would not be appropriate to interpret that to mean the child is doing better in reading than in mathematics. 3-7 Table 3-3 provides the names of the variables pertaining to the IRT scale scores available in the data file, along with the variable descriptions, value ranges, weighted means, and standard deviations. "}, {"section_title": "Variables Indicating Exclusion from the Direct Assessment Due to Disability", "text": "The variables X1EXDIS, X2EXDIS, X3EXDIS, X4EXDIS, X5EXDIS, and X6EXDIS can be used to identify children who were excluded from the assessment because they needed an accommodation the study did not provide or because they had an Individualized Education Program (IEP) that indicated they could not take part in standardized assessments. These variables are coded 1, Excluded from assessment due to disability, for children who were excluded from the assessment for these reasons. All other children are coded 0 for variables X1EXDIS, X2EXDIS, X4EXDIS, and X6EXDIS. For the variables pertaining to the fall first-grade and fall second-grade data collections (X3EXDIS and X5EXDIS), children who were part of the subsample and not excluded from the assessments are coded 0 3-8 and children who were not part of the subsample (and, therefore, not eligible for the assessments in these rounds) are coded as system missing. 4 "}, {"section_title": "Choosing the Appropriate Score for Analysis", "text": "When choosing scores to use in analysis, researchers should consider the nature of their research questions, the type of statistical analysis to be conducted, the population of interest, and the audience. The sections below discuss the general suitability of the different types of scores for different analyses."}, {"section_title": "\uf06e", "text": "The IRT-based theta scores are overall measures of ability. They are appropriate for both cross-sectional and longitudinal analyses. They are useful in examining differences in overall achievement among subgroups of children in a given data collection round or across rounds, as well as in analysis of correlations between achievement and child, family, and school characteristics. The fall kindergarten, spring kindergarten, fall first-grade, spring first-grade, fall second-grade, and springsecond grade theta scores included in the K-2 data file are on the same metric. Therefore, an analyst looking at growth across the kindergarten year could subtract the fall kindergarten score from the spring kindergarten score to compute a gain score. Or when looking at growth from kindergarten entry to the end of second grade, an analyst could subtract the fall kindergarten score from the spring second-grade score to compute a gain score. The theta scores may be more desirable than the scale scores for use in a multivariate analysis because their distribution generally tends to be more normal than the distribution of the scale scores. 5 However, for a broader audience of readers unfamiliar with IRT modeling techniques, the metric of the theta scores (from -6 to 6) may be less readily interpretable than the metric of the scale scores. Researchers should consider their analysis and the audience for their research when selecting between the theta and the scale score.\nThe IRT-based scale scores also are overall measures of achievement. They are appropriate for both cross-sectional and longitudinal analyses. They are useful in examining differences in overall achievement among subgroups of children in a given data collection round or in different rounds, as well as in analysis looking at correlations between achievement and child, family, and school characteristics. The fall kindergarten, spring kindergarten, fall first-grade, spring first-grade, fall secondgrade, and spring second-grade scale scores included in the K-2 data file are on the same metric. Therefore, an analyst looking at growth across the kindergarten year could subtract the fall kindergarten score from the spring kindergarten score to compute a gain score. Or when looking at growth from kindergarten entry to the end of second grade, an analyst could subtract the fall kindergarten score from the spring second-grade score to compute a gain score. Results expressed in terms of scale score points, scale score gains, or an average scale score may be more easily interpretable by a wider audience than results based on the theta scores.\nThere is 1 case (CHILDID=10005679) that has a household member (person 4) who was not in the spring 2012 household roster, but was in the spring 2013 household roster. This person was not in the spring 2012 household roster because of problems in the base year when case 10014103 was confused by an interviewer with case 10005679. \uf06e There is 1 case (CHILDID=10005750) that had a spring 2012 error documented in the K-1 User's Manual data anomalies section as a case not to use in analysis. The spring 2012 data indicated that the father was the respondent, but the mother was the respondent and the father should have been identified as having left the household in spring 2012. The spring 2013 household roster data are fixed for this case, with spring 2013 P6CUR_3 = 2 (leaving) and P6REASL3 = 6 (roster error). \uf06e There are some cases for which household members' relationships to the study child were changed in editing and do not correspond to values for parent interview variables about whether there was a change in relationship to the child. There are 11 cases (CHILDID = 10000272, 10006212, 10008758, 10009608, 10011516, 10014834, 10015383, 10016212, 10002767, 10015594, 10016196) where the questions about whether there has been a change in the relationship to the child (P6CHGRESPREL or P6CHGSPSPREL) are set to 2 (no); however, the relationships to the child were changed in editing based on information obtained after the interview (confirmation from recorded sections of the interview and/or interviewer comments). The values of P6CHGRESPREL and P6CHGSPSPREL have been left as collected.\nThere is 1 case (CHILDID= 10009577) where the questions about whether there has been a change in the relationship to the child (P6CHGRESPREL and P6CHGSPSPREL) are set to -9 (not ascertained) because the relationships of household members to the child were edited based on information obtained after the interview. P6CHGRESPREL and P6CHGSPSPREL were not asked during the interview, so they are set to -9 (not ascertained).\nThere is 1 case (CHILDID=10005319) where a change in relationship to the child was reported (P6CHGRESPREL= 1); however, the change was to correct the relationship to the child from a grandfather to a grandmother. The value of P6CHGRESPREL was left as collected. \uf06e There are 4 cases (CHILDID=10008603, 10013281, 10017796, 10015704) where the respondents reported that their relationship to the child had changed (P6CHGRESPREL= 1), but the relationship was not updated due to a problem in CAPI.\nFor case 10000918, the respondent reported that child received child care on a regular basis by his grandmother and was coded as having care from a relative. The respondent also indicated that the grandmother ran a day care center in her home, which this child attended. The child was coded as receiving both \"care by relative\" (P6RELNOW=1) and \"care in daycare or before/after school program\" A-3 (P6CTRNOW=1). Specific questions about relative care (P6RELNUM, P6RPLACE, P6RBEFOR, P6RAFTER, P6RWKEND, P6RWEEK, P6RDAYS, P6RHRS) and center care (P6CTRNUM, P6CTRLOK, P6CBEFOR, P6CFTER, P6CWKEND, P6CWEEK, P6CDAYS, P6CHRS) were also answered. Users may want to code this case as only having center-based care or only having relative care, but not both.\nThere are some cases that have a disability diagnosis for the focal child and have follow-up questions about that diagnosis recorded in variables other than those used for the child's specific diagnosis. In the parent interview, respondents were asked to provide the diagnosis of the child's disability, if applicable, in question CHQ125 (P6LRNDIS-P6OTHDIA). If a diagnosis did not fit one of the categories in the parent interview specifications, the diagnosis was entered as \"other.\" Follow-up questions about age at diagnosis and medication taken for a particular diagnosis (CHQ130-CHQ173) were asked about the diagnosis entered as \"other.\" During data editing and review of \"other\" responses conducted after the parent interview was completed, it was determined that some answers in the \"other\" category fit within existing codes that were available in the interview and were assigned codes for those existing categories. For example, in a situation in which the parent-report was initially coded as an \"other\" diagnosis in CHQ125 but was later determined to be depression, the diagnosis was recategorized from \"other\" to depression (P6DEPRESS=1), but the information collected in follow-up questions about age at diagnosis and medication taken for a particular diagnosis (CHQ130-CHQ173) remain in the variables pertaining to the \"other\" category. Parent Interview: Spring 2013 Errors in the CAI Programming \uf06e There were 4 parent interview cases (10017985, 10010070, 10014762, 10011919) for which the relationship or race/ethnicity of the spouse/partner was not asked after it was determined in question FSQ122 (P6CHGSPSPREL) that there had been a change in the relationship of the spouse/partner to the child (P6CHGSPSPREL=1). If a spouse or partner has a change in relationship to the child (P6CHGSPSREL= 1), the respondent should be asked what the new relationship to the child is and the race/ethnicity of the spouse or partner (if race/ethnicity is missing and the new relationship to the child fits the criteria in the interview for asking race/ethnicity). There was a problem in CAPI for these 4 cases and the relationship to the child was obtained from audio records. Race and ethnicity were set to -9 (not ascertained) for these cases.\nThere were errors in the preloaded data used to determine which questions about languages used in the household should be asked, given information collected in priorround interviews. As a result, there are cases for which the data collected are inconsistent with the skip patterns documented in the interview specifications. There were 453 cases for which language information was collected in the base year that should have had the language spoken in the home verified in P6SINGLL rather than asked again. There were 511 cases for which language information was collected in the base year that should have had primary language verified in P6PRIMRL rather than asked again. A-4 \uf06e There were errors in the preloaded data used to determine which questions about parent employment should be asked, given information collected in prior-round interviews. As a result, there are cases for which the data collected are inconsistent with the skip patterns documented in the interview specifications. There are 463 cases for which one or both parents in the household had employment information collected in the fall of 2010 that should have been asked the employment change question, P6EMPCHG_#, but were not. This affected 343 cases for parent 1 employment data and 286 cases for parent 2 employment data (both parent 1 and parent 2 employment data were affected for some cases). There were also 139 parents who were the same as a parent in the household in spring 2012 and had employment data collected or confirmed in spring 2012, but were not asked P6EMPCH_#. Rather than verify their previously reported employment data, these parents were asked for their employment data again using the full set of employment questions. There were also 28 cases that were asked P6EMPCHG_# based on information collected in the spring 2012 parent interview but should not have been, because the data from 2012 were not for the same parent. P6EMPCHG_# was set to -1 for these cases. Parent Interview: Data Considerations \uf06e Some households have an uncommon combination of parent/guardians. Cases with CHILDID= 10012353, 10013049, and 10003018 were reviewed, and it was either confirmed that the parent/guardians listed are as reported during the parent interview or no information was provided to indicate the parent/guardians listed were incorrect.\nThe nonresident parent section of the parent interview (NRQ) is designed to ask about biological and adoptive parents who are not in the household. If there is one adoptive parent in the household, questions are asked about contact the child may have with another adoptive parent who is not in the household. Questions in this section are asked about a nonresident adoptive parent who is the opposite sex of the adoptive parent in the household. Questions are not asked about a nonresident adoptive parent who is the same sex as the other adoptive parent in the household.\nRange checks include logical soft checks for continuous variables.\nConsistency checks include logical soft comparisons between related variables within a form to check for inconsistencies. When data were identified during quality control (QC) processes as possibly in error, the original questionnaire returned by the respondent was reviewed to determine whether the response was A-5 incorrectly captured during the questionnaire scanning process. For those cases listed in this section as having anomalous data, data reviewers confirmed that the data matched the response provided on the questionnaire and a reasonable correction(s) could not be determined. Therefore, the data were left as reported. School Administrator Questionnaire: Spring 2013 \uf06e In the spring 2013 data collection, two versions of the school administrator questionnaire were used. Version A (SAQ-A) was given to schools that were new to the study or did not have a completed SAQ from a prior round, and version B (SAQ-B) was given to schools that had completed an SAQ in a prior round. SAQ-B was given in error to 24 schools that should have been asked to complete SAQ-A. The following 24 schools completed and returned the spring 2013 SAQ-B fielded in error: 1111; 1360; 1687; 1955; 2010; 2220; 2415; 2423; 2435; 2519; 2626; 2646; 2683; 2697; 2790; 2889; 2938; 2943; 3000; 3048; 3076; 3146; 3192; and 3395. Teacher Questionnaires: Spring 2013 \uf06e For 11 cases, the spring 2013 child-level teacher questionnaire completion dates (T6COMPMM, T6COMPDD) were changed from the date written on the questionnaire to the date the questionnaire was receipted, as the date reported by teachers was inconsistent with the time period in which the questionnaire was fielded.\nMost of the changes were needed due to an issue with the SAS truncation function in the code for the month of the school year that resulted in months being rounded conventionally. As a result, the grade fraction (indicating the child assessment date expressed as a fraction (month 1 = 0.1, month 2 = 0.2, etc.) that is added to the child's grade) used for the score look-up tables was slightly elevated for about half the children (i.e., those rounded up appeared to have spent additional time in school). This reduced the scores relative to the developer's scoring rules and norms. The scores were recalculated without rounding up, and there were only modest changes resulting from the correction of the truncation of the month of the school year. The maximum discrepancy is one month for this rounding error (e.g., 1.755 was rounded to 1.8 when it should have been truncated to 1.7), but it affects many participants in each round.\nThe algorithm used to set the portion of the school year that had been completed did not limit the maximum grade fraction to 0.9, so some children had 1.0 or 1.1 added to A-7 their grade when assessed at or after the end of the school year. These were recoded as 0.9, per the developer's rules.\nTwo of the flags related to the Numbers Reversed scores, X1NRGEST and X2NRGEST, have an error. The error is related to the assignment of a missing code \"-9: Not ascertained\" versus coding the flag variable as \"0: False.\" The variable X*NRGEST is a true/false data flag variable that indicates whether the child's grade in decimal form (with the decimal indicating the number of months the child has been in the identified grade in school) at the time of the assessment was estimated for the Numbers Reversed grade-normed scores. When school start or end dates are not available, an estimate of the assessment point is required. If the child's grade in school in decimal form at the time of assessment was estimated, the flag should be set to \"1: True.\" Otherwise the flag should be set to \"0: False.\" For X1NRGEST and X2NRGEST there are cases that were erroneously set to \"-9: Not ascertained.\" Researchers should treat these cases as \"0: False.\"\nVariables with too few cases and/or a sparse distribution are suppressed in the K-2 PUF. The values for these variables are set to -2 and labeled \"suppressed\" in the ECB. The value -2 means that the data for this variable are suppressed to protect the respondent's confidentiality.\nVariables that provide a particularly identifying characteristic, such as a specific disability, or information that could be matched against external data sources to obtain a specific identifying characteristic, such as exact date of marriage or divorce, are also suppressed. The values for these variables are set to -2.\nThe variables from kindergarten and first grade are masked identically in the K-2 PUF as they were in the K-1 PUF. To the greatest extent possible, second-grade variables have been masked to be consistent with the masking for similar kindergarten and firstgrade variables. There is a comment field in the variable frequency distribution view screen of the ECB that displays a comment for each masked variable indicating whether the variable from the restricted-use file has been recoded or suppressed in the K-2 PUF. Exhibits 1 to 9 present the lists of masked variables for second grade. The exhibits display the variable name, variable label, and a comment indicating whether the variable was recoded or suppressed, and the reason for suppression. See section 7.1 of this manual for the variable naming conventions. All variables from the special education teacher questionnaire part A (i.e., all variables with the prefix D6) and from the special education teacher questionnaire part B (i.e., all variables with the prefix E6) are suppressed on the K-2 PUF. For brevity, these variables are not included in the exhibits."}, {"section_title": "Analytic Considerations for Measuring Gains in the ECLS-K:2011", "text": "An important issue to be considered when analyzing achievement scores and gains is assessment timing: children's age at assessment, the date of assessment, and the time interval between assessments. Most sampled children were born throughout the second half of 2004 and first half of 2005, but their birth dates were not related to testing dates. As a result, children were tested at different developmental and chronological ages. Assessment dates ranged from August to December for the fall data collections, and from March to June for the spring data collections. Children assessed later in a data collection period in a particular grade level, for example in December during a fall collection, may be expected to have an advantage over children assessed earlier in the data collection period, for example in the first days or weeks of school, because they had more exposure to educational content before being assessed. Substantial differences in the intervals between assessments may also affect analysis of gain scores. Children assessed in September for the fall data collection and June for the spring data collection have more time to learn knowledge skills than do children assessed first in November and then again in March. These differences in interval may or may not have a significant impact on analysis results. In designing an analysis plan, it is important to consider whether and how differences in age, assessment date, and interval may affect the results; to look at relationships between these factors and other variables of interest; and to adjust for differences, if necessary. When using the IRT scale scores as longitudinal measures of overall growth, analysts should keep in mind that gains made at different points on the scale have qualitatively different interpretations. Children who made gains toward the lower end of the scale, for example, in skills such as identifying letters and associating letters with sounds, are learning different skills than children who made gains at the higher end of the scale, for example, those who have gone from reading sentences to reading passages, although their gains in number of scale score points may be the same. Comparison of gains in scale score points is most meaningful for groups that started with similar initial status. One way to account for children's initial status is to include a prior round assessment score as a control variable in an analytic model. For example, the fall kindergarten scale score could be included in a model using the spring kindergarten scale score as the outcome."}, {"section_title": "3.1.5", "text": ""}, {"section_title": "Reliability of the ECLS-K:2011 Scores", "text": "Reliability statistics assess consistency of measurement, or the extent to which test items in a set are related to each other and to the score scale as a whole. For tests of equal length, reliability estimates can be expected to be higher for sets of items that are closely related to the underlying construct than for tests with more diversity of content. Conversely, for tests with similar levels of diversity in content, reliabilities tend to be higher for longer tests compared to shorter tests. Reliabilities range from 0 to 1. Table 3-4 presents the reliability statistics computed for the IRT-based scores for each subject area for the fall and spring of kindergarten, the fall and spring of first grade, and the fall and spring of second grade. The reliability of the overall ability estimate, theta, is based on the variance of repeated estimates of theta for each individual child compared with total sample variance. The reliabilities calculated for theta also apply to the scores derived from the theta estimate, namely, the IRT scale scores. The reliabilities are relatively high, ranging from .75 to .95. Science, the domain with the most diverse content and the smallest number of items, had lower reliability coefficients than reading and mathematics. 6 "}, {"section_title": "3.1.6", "text": ""}, {"section_title": "Validity of the ECLS-K:2011 Scores", "text": "Evidence for the validity of the direct cognitive assessments was derived from several sources. A review of national and state performance standards, comparison with state and commercial assessments, and the judgments of curriculum experts all informed the development of the test specifications. For the kindergarten, first-grade, and second-grade assessments, national and state performance standards in each of the domains were examined. The reading specifications are based on the NAEP Reading Frameworks for 2009, with the addition of basic reading skills and vocabulary categories suitable for the earlier grades. Although the NAEP assessments are administered starting in fourth grade, the reading specifications were extrapolated down to kindergarten, based on current curriculum standards from Texas, California, New Jersey, Florida, and Virginia. The mathematics test specifications are based on the framework developed for the ECLS-K assessments for kindergarten, first grade, and third grade, which were based on the 1996 NAEP mathematics frameworks and extended down to earlier grades. In science, the 2009 standards of six states (Arizona, California, Florida, New Mexico, Texas, and Virginia) were reviewed to find a commonality of topics that are taught in kindergarten, first grade, and/or second grade. Pools of potential assessment items were developed for each content domain based on the framework or standards pertinent to the domain. An expert panel of school educators, including curriculum specialists in the subject areas, then examined the pool of items for content and framework strand design, accuracy, nonambiguity of response options, and appropriate formatting. The items were included in a field test and better performing items were selected for the final assessment battery."}, {"section_title": "Direct Cognitive Assessment: Executive Function", "text": "Executive functions are interdependent processes that work together to regulate and orchestrate cognition, emotion, and behavior and that help a child to learn in the classroom. Two measures of executive function were included in the kindergarten, first-grade, and second-grade direct child assessment battery: the Dimensional Change Card Sort (DCCS) (Zelazo 2006 (Woodcock, McGrew, and Mather 2001), assessing working memory. The same versions of the DCCS and the Numbers Reversed tasks were administered in fall and spring of the kindergarten year and fall and spring of first grade. In second grade, the DCCS was changed"}, {"section_title": "3-12", "text": "to computerized administration to remain age appropriate, while the version of the Numbers Reversed task remained the same as the version used in the earlier data collection rounds."}, {"section_title": "Dimensional Change Card Sort", "text": "The Dimensional Change Card Sort (DCCS) (Zelazo 2006) (Zelazo et al. 2013) is used to collect information on children's cognitive flexibility. In the kindergarten and first-grade data collections, the DCCS was administered as a physical, table-top card sort with the items administered by a trained assessor. In the second-grade data collections, a computerized version of the DCCS developed for the National Institutes of Health Toolbox for the Assessment of Neurological and Behavioral Function (NIH Toolbox) was administered. As noted above, this task was administered differently in the second-grade collections than it was in the earlier collections to remain age-appropriate. This section describes both administration methods and the types of data generated by each method. In the version of this task used in kindergarten and first grade, children were asked to sort a series of 22 picture cards according to different rules. Each card had a picture of either a red rabbit or a blue boat. The children were asked to sort each card into one of two trays depending on the sorting rule they had been told to use. One tray had a picture of a red boat and the other had a picture of a blue rabbit. For the first set of items, the Color Game (each set is referred to as a game), the rule was to sort the cards by color (i.e., red or blue). For example, a blue boat card would be sorted into the blue rabbit tray. The Color Game trials are referred to as the \"pre-switch\" trials because they occur first, before the child is asked to change or \"switch\" sorting rules. In the second game, the Shape Game, the child was asked to change sorting rules and sort the cards by shape (i.e., rabbit or boat). For example, a red rabbit card would be sorted into the blue rabbit tray. The Shape Game trials are referred to as the \"post-switch\" trials because they occur after the change or \"switch\" from sorting by color to sorting by shape. If the child correctly sorted four of the six cards in the Shape Game, then he or she moved on to the third game: the Border Game. In the Border Game, the sorting rule (by color or by shape) depended on whether the card had a black border around the edges. If the card had a border, the child was to sort by color; if there was no border on the card, the child was to sort by shape. Kindergarten and first-grade item-level data for this version of the DCCS are provided in the ECLS-K:2011 kindergarten-second grade data file. There are six variables with item-level data for the Color Game, six variables with item-level data for the Shape Game, and six variables with item-level 3-13 data for the Border Game. There were four practice items administered to children, but the item-level data from these practice items are not included in the data file. The item-level data for the Color, Shape, and Border Games are scored \"correct\" (i.e., the card was sorted into the correct tray according to the sorting rule) or \"incorrect\" (i.e., the card was sorted into the incorrect tray). There is a third score provided for the Border Game, \"not administered\"; this code indicates that the child was not administered the item because he or she did not answer enough items correctly in the Color or Shape Game to advance to this set of items in the assessment. The \"not administered\" code is different than a system missing code in that only those children who were administered the DCCS can have a \"not administered\" code. If a child was not administered the DCCS at all, his or her data for the DCCS scores are coded as missing. Variable names for the item-level data from the fall kindergarten assessments begin with \"C1,\" and the variable names for the item-level data from the spring kindergarten assessments begin with \"C2.\" Similarly, variable names for item-level data from the fall and spring first-grade assessments begin with \"C3\" and \"C4,\" respectively. Using scoring rules provided by the developers, four scores were developed from the DCCS data for the fall and spring kindergarten and the fall and spring first-grade rounds of data collection: the pre-switch score, the post-switch score, the Border Game score, and a total score. The pre-switch score is the number of cards the child correctly sorted by color during the first phase of the assessment. The postswitch score is the number of cards the child correctly sorted by shape after switching from sorting by color to sorting by shape. The Border Game score is the number of cards the child correctly sorted when the sorting rule was determined by the presence (or absence) of a border around the card. 7 A final combined scale score reflects the total accuracy for the three tasks (i.e., the total number of cards sorted correctly in the Color, Shape, and Border Games), which results in a maximum score of 18 correct. The developer of the DCCS recommends using the overall accuracy score to assess performance. The total DCCS scores for kindergarten and first grade included in the kindergarten-second grade data file are calculated differently than what was recommended for calculation of the total DCCS score in the base-year user's manual. The User's Manual for the ECLS-K:2011 Kindergarten Data File andElectronic Codebook, Public Version (Tourangeau et al. 2015) indicated that researchers could create a single DCCS composite score by summing the post-switch score and the Border Game score and use that combined score in analyses. This composite score does not include information from the pre-switch score. After the release of the kindergarten-year file, further consideration was given to the fall and spring kindergarten data, where 1,038 cases scored 0 on the post-switch score in the fall of kindergarten and 457 cases scored 0 on the post-switch score in the spring of kindergarten. Following consultation with the DCCS developer, it was subsequently decided to include the pre-switch scores in the calculation of the total score in order to better capture variability at the lower ability levels. Therefore, the total scores for kindergarten and first grade (X*DCCSTOT) 8 included in the kindergarten-first grade and kindergartensecond grade data files reflect children's performance across all 18 trials (i.e., the pre-switch or color trials, the post switch or shape trails, and the Border Game trials). 9 In kindergarten and first grade, the DCCS was administered in Spanish for children routed through the Spanish assessment. Data from the English and Spanish administrations are combined into the same item-level variables and into the same score variables. Researchers who want to account for language of administration in their analyses can use the data flag provided in the data file for each round (X*FLSCRN) to identify which cases were administered the DCCS in English and which cases were administered it in Spanish. All children were administered the assessments in English in the second-grade collections. Therefore, the second-grade DCCS scores for all children are based on an English administration of the assessment, and data flags to indicate language of administration in second grade (rounds 5 and 6) are not provided in the data file. 10 Because the version of the DCCS used in kindergarten and first grade would have been too easy for the majority of the study children during the second-grade rounds, children were administered a new, age-appropriate, computerized version of the DCCS in which the \"cards\" are presented on a computer screen and children sort them into virtual \"piles\" on the screen using keys on the keyboard to indicate where to place each card. The ECLS-K:2011 Executive Function Content Review Panel reviewed the study plans and supported transitioning to a computerized version of the DCCS. The Content Review Panel members indicated that the results of the tabletop and computerized versions should be comparable across rounds and suggested that standardized scores would facilitate comparisons. The computerized task was developed as part of the National Institutes of Health Toolbox for the Assessment of Neurological and Behavioral Function (NIH Toolbox) and is appropriate for ages 3-85 11 (Zelazo et al. 2013). The task had been under development and became available in time to be incorporated into the second-grade data collections. The NIH Toolbox Dimensional Change Card Sort Test (NIH Toolbox DCCS) is a task is that is used across the 3 through 85 age range, but it has two different start points based on the age of the child in order to limit administration time. The NIH Toolbox DCCS consists of 40 trials, including 5 pre-switch trials (where children are asked to sort by one dimension, e.g., color), 5 post-switch trials (where children are asked to sort by a different dimension, e.g., shape), and 30 mixed-block trials (in which the sorting dimension, either color or shape, varies by trial). Testing conducted in the development of the NIH Toolbox DCCS indicated that 8-year-olds typically scored at ceiling on the pre-switch and post-switch trials. Consequently, children under age 8 begin with the pre-switch trials, and children age 8 and above begin with the mixed-block trials and are given credit in the scoring for completing the pre-switch and post-switch trials accurately. It is important to note that all ECLS-K:2011 children were administered the version of the NIH Toolbox DCCS for ages 8 years and older, regardless of their age at the time of assessment. Approximately 90 percent of the ECLS-K:2011 children who have a score on the DCCS in the fall subsample for second grade (round 5) and approximately 40 percent of children with a DCCS score in the spring of second grade (round 6) were not yet 8 years old when the DCCS was administered. The decision to administer the same version of the DCCS from second grade forward, regardless of whether the child was age 8 in the second-grade administration, was made so that all study children would receive the same version of the DCCS task in the fall and spring of second grade and in later rounds of data collection. Use of the same measure allows for a longitudinal analysis of performance on the DCCS from second grade into later rounds of data collection. 12 The construct assessed in the physical version and the computerized version of the DCCS is the same-cognitive flexibility. However, the way the construct is assessed and the scoring differ across the versions. One key difference between the two versions is that the computerized version captures data on the amount of time in milliseconds that it takes the child to complete any given item; it is not possible to accurately measure reaction time at the necessary level of precision in the physical version. Therefore, the computerized version supports the use of both accuracy of sorting and reaction time to assess overall performance while the physical card sort assesses performance by accuracy alone. Differences between scoring methods for the physical card sort and the computerized version of the DCCS can be thought of as a logical progression of the scoring, rather than a comparison of two unlike scores, because of developmental differences in how younger versus older children approach this task. Younger children tend to respond rapidly with lower accuracy; since there is sufficient variability in performance based on accuracy alone, the physical card sort is generally appropriate for assessing cognitive flexibility in younger children. In contrast, older children and adults typically learn to slow down while sorting the cards in order to avoid making mistakes, which results in their sorting with high accuracy. It becomes important to factor in reaction time once the child has mastered the speedaccuracy tradeoff and is able to slow responding enough to maintain high accuracy, because there is less variability in sorting accuracy and sorting accurately more quickly is an indicator of higher cognitive flexibility once relatively high accuracy is achieved. In each of the 30 mixed-block trials administered to children in the ECLS-K:2011, the children were presented with a stimulus picture of a ball or truck that was either yellow or blue. A prerecorded female voice announced the sorting rule to be used for that trial (\"color\" or \"shape\") as the appropriate word \"color\" or \"shape\" was briefly displayed in the center of screen. Next, the stimulus picture was displayed in the center of screen, where the word had just appeared. Children then selected one of two pictures at the bottom of the screen (a blue ball on the left or a yellow truck on the right) that was either the same shape or the same color as the stimulus picture, depending on whether the shape or color sorting rule was in effect for the trial. Children indicated their choice of picture by pressing the arrow key on the laptop keyboard that was associated with the picture; the left arrow key was used to select the picture on the left side of the screen and the right arrow key was used to select the picture on the right side of the screen. Children were instructed to use just one pointer finger to press the arrow keys. They were asked to return their pointer finger to the button in between the left and right arrow keys (marked with a fuzzy sticker, and so identified as the \"fuzzy button\") in between trials to standardize the start location for every child's finger, with the goal of maximizing accuracy in the measurement of response time. Both reaction time to sort the card and accuracy of its placement according to the sorting rule in effect for the trial were recorded by the computer program. The sorting rules (i.e., to either sort by shape or color) were intermixed across the trials, and one rule was more common than the other. The shape rule was used for 23 trials while the color rule was used in 7 trials. For example, the child may be asked to sort by shape for 4 trials in a row, then to sort by color on trial 5, and then to sort by shape on trials 6 and 7. One sorting rule was presented more frequently in order to build a response tendency (i.e., a response that is \"preferred\" because it happens more frequently, resulting in a predisposition to respond in that manner). A predisposition to sort by the dominant rule (i.e., shape) can result in either more errors or a slower reaction or response time on nondominant trials because it is necessary to inhibit the dominant response (i.e., sorting by shape) in order to shift to the less frequent sorting rule (i.e., color). The \"cost\" associated with the shift from a more frequent rule (the \"dominant\" rule) to a less frequent rule (the \"nondominant\" rule) tends to differ by the age of the participant (Davidson et al. 2006). The \"cost\" to younger children is that they tend to make more errors on the nondominant rule trials; that is, they do not demonstrate the cognitive flexibility to make the switch between rules even when prompted. Younger children do not tend to slow themselves down in favor of higher accuracy and, therefore, accuracy is a better metric of performance for young children (Zelazo et al. 2013). In contrast, older children and adults tend to demonstrate a speed/accuracy tradeoff; they slow down the pace at which they respond in order to maintain accuracy. Thus, the \"cost\" to older children and adults is seen in reaction time on the nondominant rule trials. The formula used to produce scores from the data collected by the computerized DCCS factors in reaction time on the infrequent or nondominant trials when a child demonstrates sufficiently accurate performance across all the test trials, defined as being accurate on more than 80 percent of the trials (Zelazo et al. 2013). Thus, the computerized DCCS provides a measure of performance through this developmental shift to learning to trade speed for accuracy. More information on scoring is provided below. The 30 test trials were administered only to children who successfully completed the practice portion of the DCCS. The practice consisted of a minimum of 8 trials and a maximum of 24 trials, depending upon how quickly the child demonstrated that he or she understood the task. For the first set of practice trials, the assessor instructed the child how to sort by shape using text automatically presented on the DCCS screen that was read by the assessor along with additional standardized instructions presented by the assessor. Following the instructions, the computer administered four practice trials asking the child to sort by shape. If the child sorted at least three of the 4 items correctly by shape, he or she progressed to the color practice. If the child sorted more than one item in the set of 4 incorrectly, he or she was presented with a second set of 4 practice items. If the child failed to sort three of 4 items correctly by shape in the second set of practice items, he or she was presented a third set; failure of this third set ended the DCCS program before any actual scored trials were presented."}, {"section_title": "3-18", "text": "Once a child passed the shape practice trials, the assessor instructed on how to sort by color, and the computer presented 4 to 12 practice trials asking to sort by color. Like the shape practice trials, up to three sets of 4 items could be presented before the DCCS advanced to the scored trials. If the child was not able to pass the color practice, the DCCS program ended after the third set of color practice items, again before any actual scored trials were presented. In contrast with the scored trials, the practice trials maintained one sorting rule for all items presented in succession until practice for the rule was complete. An additional difference between the practice and scored trials was that the stimulus pictures in the practice trials were white or brown rabbits and boats. Item-level data for the 30 test trials are included in the data file. They are provided in three blocks of 30 items for each participant that indicate: (1) correct versus incorrect responses, (2) the type of trial, reported as dominant (most frequently presented but not included in reaction time scores; shape is the dominant sorting rule) or non-dominant (less frequently presented and used to calculate reaction time scores; color is the non-dominant sorting rule), and (3) reaction times reported in milliseconds. Variable names for the item-level data begin with \"C5\" for fall second grade and \"C6\" for spring second grade. The overall computed score reported for the second-grade DCCS is derived using a formula provided by the task developer and follows the scoring algorithm used for this task in the NIH Toolbox (see the NIH Toolbox Scoring and Interpretation Guide, Slotkin et al. 2012, for additional information on scoring). Scores range from 0 to 10, with weight given to accuracy (0 to 5 units) and reaction time (0 to 5 units) in the computation of the scores. Accuracy is considered first. If the child's accuracy rate is less than or equal to 80 percent, the child's overall computed score is based entirely on accuracy. If the child's accuracy rate is more than 80 percent, the child's overall computed score is based on a combination of accuracy and reaction time. The accuracy score factored into the computation of the overall score can range from 0 to 5. There are a total of 40 accuracy points that are scaled down to a maximum score of 5: for each correct response, the child earns a score of .125 (5 points divided by 40 trials). Because all children used the start point of the DCCS for children 8 years and older, each child was administered the 30 mixed-block trials, and each child who successfully passed the practice items was automatically given 10 accuracy points for the 5 pre-switch and the 5 post-switch trials of the DCCS that were not administered. Therefore, the accuracy component of the overall computed DCCS score is calculated as follows:"}, {"section_title": "3-19", "text": "DCCS accuracy score = 0.125 * number of correct responses 13 If the child's accuracy rate is higher than 80 percent, a reaction time score is added to the child's accuracy score. 14 Like the accuracy score, the reaction time score ranges from 0 to 5 points. The reaction time component of the overall computed score for the computerized DCCS is computed using the child's median reaction time to correct nondominant trials (i.e., the trials with the less frequently used sorting rule, color), following the same scoring algorithm outlined in the scoring manual for the NIH Toolbox (Slotkin et al. 2012). First, for those children with greater than 80 percent accuracy on the 40 trials, the median reaction time is calculated based on reaction times for correct nondominant trials with reaction times greater than or equal to 100 milliseconds (msec) and within plus or minus three standard deviations from the child's mean reaction time on the correct nondominant trials. The minimum median reaction time allowed is 500 msec; the maximum median reaction time is 3,000 msec. If the child's median reaction time falls outside this range, the child's median reaction is set to the minimum or maximum allowable range: reaction times between 100 msec and 500 msec were set to 500 msec and reaction times between 3,000 msec and 10,000 msec (the maximum trial duration) are set to 3,000 msec. A log (base 10) transformation is applied to the median reaction times to create a more normal distribution. The log values are then algebraically rescaled to a 0 to 5 range and then reversed such that faster (better) reaction times have higher values and slower reaction times have lower values. The formula for rescaling the median reaction times is the following: Reaction time score = 5 \u2212 \ufffd5 * \ufffd log \u2212 log (500) log(3000) -log (500) \ufffd\ufffd where RT is the median reaction time on nondominant trials within set outer limits. 15 13 The number of correct responses = 10 + the number of correct trials out of the 30 mixed block trials. Once the child has passed the practice trials and advanced into the scored portion of the assessment, 10 accuracy points are automatically awarded due to the chosen start point for the task. For this reason, it is not possible for ECLS-K:2011 children to get an accuracy score of 0. Therefore, the minimum possible value for the DCCS accuracy score is 1.25 and the maximum possible DCCS accuracy score is 5. 14 The criterion of greater than 80 percent accuracy is calculated based on all 40 trials (30 administered trials plus the 10 trials not administered). That is, 80 percent of 40 trials is 32 items. However, this can also be thought of in terms of how many items out of the 30 administered trials are required. If the criterion is 80 percent of the 40 trials, this translates to 23 of the 30 administered trials. For example, if a child responds accurately on 23 of the 30 mixed block trials, the child's accuracy rate equals 82.5 percent (10 points automatically awarded for the pre-switch and postswitch trials plus the 23 correct mixed block trials divided by 40; 33/40 = .825). In this example, the child's accuracy score would be [(10 + 23) * .125] = 4.125. Because the accuracy rate is greater than 80 percent, the child's reaction time score would be added to this accuracy score to obtain the overall computed score for the DCCS. Alternatively, if the child responded accurately on 22 of the 30 mixed-block trials, the child's accuracy rate would equal 80 percent and, therefore, the child's accuracy is not greater than 80 percent and the child's overall score would be based solely on accuracy (overall computed score = [(10 + 22) * .125] = 4). 15 The median reaction time (RT) used to calculate the reaction time score falls within the range of 500 msec through 3,000 msec. Calculation of the median score requires a minimum of at least one correct nondominant trial reaction time that is greater than 100 msec. When the child reached the accuracy threshold for including the reaction time component in the scoring but did not have any within-range reaction times on correct nondominant trials, the child's overall computed score on the DCCS was set equal to the child's accuracy score, and reaction time was not factored into the child's score."}, {"section_title": "3-20", "text": "To summarize, the overall computed score on the computerized DCCS is equal to the child's accuracy score if the child's accuracy rate is less than or equal to 80 percent. If the child's accuracy rate is greater than 80 percent, the child's overall computed score is equal to the child's accuracy score plus the child's reaction time score, which is derived from the child's reaction time on correct nondominant trials as described above. Additional details on the calculation of the computed score are available in the NIH Toolbox Scoring and Interpretation Guide (Slotkin et al. 2012) and the NIH Toolbox Technical Manual (Slotkin et al. 2012). The overall computed score on the computerized DCCS cannot be directly compared with the kindergarten and first-grade total scores (X1DCCSTOT, X2DCCSTOT, X3DCCSTOT, and X4DCCSTOT), which are based on sorting physical cards and are computed differently. The kindergarten and first-grade overall scores indicate the number of items a child sorted accurately and have a potential range of 0 to 18. The second-grade computed scores (X5DCCSSCR and X6DCCSSCR) range from 0 to 10, with weight given to accuracy (0 to 5 units) and reaction time (0 to 5 units) in the computation of the scores. The overall computed scores for the physical card sort can be used to examine change across rounds that use the physical cards (e.g., performance in the fall of kindergarten can be directly compared to performance in the spring of first grade). The overall computed scores for the computerized DCCS can be used to examine change across rounds that use the computerized DCCS (i.e., performance in the fall of second grade can be directly compared to performance in the spring of second grade). It is important for researchers using the DCCS data to be aware of the characteristics of the overall DCCS scores and determine how best to use these scores in their analyses. As noted above, the NIH-developed scoring model computes scores differently depending on sorting accuracy. The use of this scoring model with the data collected from children in the ECLS-K:2011 resulted in a non-normal distribution, with approximately 14 percent of children in the fall second-grade data collection and 10 percent of children in the spring second-grade data collection who have a computed overall score failing to achieve greater than 80 percent accuracy and, therefore, having their score calculated based solely on accuracy. The remaining children have scores calculated based on both accuracy and reaction time. The non-normal distribution may be problematic for statistical analyses. For this reason, users may want to run analyses that do not use the overall score as is with the full sample. For example, users could conduct their analyses separately for the two groups of children so that each analysis only includes children with scores calculated in the same way, or they may decide to limit their analyses to only one group. Another option is for users to analyze all children using the score indicating accuracy alone, recognizing that this score is highly skewed, as most children were able to sort the cards with at least 80 percent accuracy."}, {"section_title": "3-21", "text": "Users may also want to consider investigating alternative scoring models using the item-level accuracy and reaction time data available on the data file. The decision about how best to use the DCCS overall score in analysis is left to the user, given the research questions being addressed.  (Tourangeau et al. 2015). The following scores based on the second-grade computerized administration are presented on the data file: overall score for fall second grade and spring second grade (range: 0-10); accuracy score for fall second grade and spring second grade (range: 0-5); and reaction time score for fall second grade and spring second grade (range: Researchers should note that the reaction time score was only computed for cases for which the accuracy score was greater than 80 percent. If the accuracy score was not greater than 80 percent, then the reaction time score was set to -9 (not ascertained).  \"correct\" (i.e., the child correctly repeated the number sequence in reversed order), \"incorrect\" (i.e., the child did not correctly repeat the number sequence in reversed order), or \"not administered\" (i.e., the child was not administered the item because he or she did not answer enough items correctly to advance to this item). The \"not administered\" code is different than a system missing code in that only those children who were administered the Numbers Reversed subtask could have a \"not administered\" code. If a child was not administered the Numbers Reversed subtask at all, his or her case would have a missing code for the Numbers Reversed scores. Variable names for the item-level data from the fall kindergarten assessments begin with \"C1,\" and variable names for the item-level data from the spring kindergarten assessments begin with \"C2.\" Similarly, variable names for item-level data from the fall and spring firstgrade assessments begin with \"C3\" and \"C4,\" while those for fall and spring second grade begin with \"C5\" and \"C6,\" respectively. Variable descriptions for these items indicate the length of the digit sequence (e.g., C1 Numbers Reversed Two-digit sequence #1). In addition to the item-level data, five scores developed using guidelines from the publisher's scoring materials are included in the data file for Numbers Reversed: the W-ability 16 score, the age standard score, the grade standard score, the age percentile score, and the grade percentile score. The 16 The W-ability score is a W score that represents the individual's level of ability on the task presented."}, {"section_title": "3-25", "text": "grade standard score and the grade percentile score are additional scores for all rounds of data collection being released for the first time on the kindergarten-second grade data file. Before analyzing the Numbers Reversed data, it is important that researchers understand the characteristics of these scores and how these characteristics may affect the analysis and interpretation of the Numbers Reversed data in the context of the ECLS-K:2011. Depending on the research question and analysis being conducted, one of the scores may be more preferable than another. For example, the W score may be best for a longitudinal analysis, whereas the age or grade percentile rank and/or age or grade standardized score may be better suited for an analysis focusing on one point in time. The descriptions below provide more information about which score may be better suited for a given analysis. 17 The W score, a type of standardized score, is a special transformation of the Rasch ability scale and provides a common scale of equal intervals that represents both a child's ability and the task difficulty. The W scale is particularly useful for the measurement of growth and can be considered a growth scale. Typically, the W scale has a mean of 500 and standard deviation of 100. Furthermore, the publisher of the WJ III has set the mean to the average of performance for a child of 10 years, 0 months. This means that it would be expected that most children younger than 10 years, 0 months would obtain W scores lower than the mean of 500, and most older children would be expected to have scores above the mean of 500. Also, as a child develops with age, it would be expected that the child's W score would increase to reflect growth. For example, when a child's W-ability score increases from 420 to 440, this indicates growth, and this would be the same amount of growth in the measured ability as any other student who gained 20 W points elsewhere on the measurement scale. As mentioned above, the W score is an equal-interval scale, suited for analyses such as correlations and regressions. Higher W scores indicate that a child provided more correct responses and generally indicate that a child was able to correctly respond to at least some longer number sequences. The W score accounts for only the total number of administered sequences answered correctly and does not reflect the pattern of responses, meaning the W score does not indicate how many of each length number sequence the child answered correctly. As noted above, the data file includes item-level data that can be used to examine patterns of response. The W score for each child in the ECLS-K:2011 was determined using norming data provided by the publisher. More specifically, a sample child was assigned the W score from the publisher norming data that was associated with the child's raw number-right score, the child's age (in months), and the language of administration. In kindergarten and first grade, the Numbers Reversed subtask was administered in both 19 Normative data for the WJ III were gathered from 8,818 subjects in more than 100 geographically diverse U.S. communities (McGrew and Woodcock, 2001). The kindergarten through 12th grade sample was composed of 4,783 subjects. The norming sample was selected to be representative of the U.S. population from age 24 months to age 90 years and older. Subjects were randomly selected within a stratified sampling design that controlled for the following 10 specific community and subject variables: census region (Northeast, Midwest, South, West); community size (city and urban, larger community, smaller community, rural area); sex; race (White, Black, American Indian, Asian and Pacific Islander); Hispanic or non-Hispanic; type of school (elementary, secondary, public, private, home); type of college/university (2-year, 4-year, public, private); education of adults; occupational status of adults; occupation of adults in the labor force. into the school year the child had been in the grade at the time of the assessment (e.g., 0.1 = 1 month; 0.2 = 2 months, etc.; 0.9 = 9 months, including time in the summer prior to the start of the next grade level). When school year start and end dates were not available, it was necessary to estimate the decimal representing the proportion of the school year completed when the assessment occurred. X5NRGEST and X6NRGEST are flags that indicate whether the number of months completed in the grade was estimated."}, {"section_title": "Indirect Cognitive Assessment, the Academic Rating Scale", "text": "The Academic Rating Scale was developed for the ECLS-K to obtain teachers' evaluations of children's academic achievement in three domains: language and literacy, science, and mathematical thinking. The ECLS-K:2011 fielded the Academic Rating Scale developed for the ECLS-K with some modifications to the item text. Teachers rated the child's skills, knowledge, and behaviors on a scale from \"not yet\" to \"proficient\" (exhibit 3-2). If a skill, knowledge, or behavior had not been introduced in the classroom yet, the teacher was instructed to mark that item as NA (not applicable or skill not yet taught). The number and content of the items vary across round of data collection. While some items are repeated across grade levels, other items change across rounds because the focus of instruction changes as children progress through school. Language and literacy items were administered in fall of kindergarten (9 items), spring of kindergarten (9 items), fall of first grade (2 items for children in first grade and 9 items for children repeating kindergarten), spring of first grade (9 items), and fall of second grade (4 items). Mathematical thinking items were administered in the fall and spring of kindergarten (8 items) and in the spring of first grade (8 items). Science items were administered in the fall of kindergarten (8 items) and in the spring of first grade (8 items)."}, {"section_title": "3-32", "text": "3-33"}, {"section_title": "Teacher-Reported Social Skills", "text": "In the fall and spring data collections in kindergarten, first grade, and second grade, teachers reported how often their ECLS-K:2011 children exhibited certain social skills and behaviors using a fouroption frequency scale ranging from \"never\" to \"very often.\" Teachers also had the option of indicating that they had not had an opportunity to observe the described behavior for the child being asked about.  20 The Social Skills Rating System is a copyrighted instrument (1990 NCS Pearson) and has been adapted with permission. 21 For children who were in first grade during the first-grade data collections (rounds 3 and 4) and for all children in the second-grade data collections (rounds 5 and 6), the externalizing problem behaviors composite is based on 6 items. This is different from how the composite was created for the kindergarten rounds (rounds 1 and 2). One additional item was included at the end of the \"Social Skills\" section of the questionnaire in first and second grade. The item asked about the child's tendency to talk at times when the child was not supposed to be talking. The item was added because it had been included in the first-grade round of the ECLS-K and was factored into the calculation of that study's first-grade composite score. 22 Two versions of the teacher-level and child-level teacher questionnaires were used in the spring of first grade: one version for students who were in first grade or higher during the data collection period and one for students who had been retained in kindergarten for the 2011-12 school year. Details of the differences in these questionnaires are presented in chapter 2 of the User's Manual for the ECLS-K:2011 Kindergarten-First Grade Data File andElectronic Codebook, Public Version (NCES 2015-078) (Tourangeau et al. 2015). In the tables below, the variables applicable to first-grade students begin with X4, while those for children retained in kindergarten begin with X4K.   "}, {"section_title": "Teacher-Reported Approaches to Learning Items and Scale", "text": "The child-level teacher questionnaire fielded in every round of data collection from the fall of kindergarten to the spring of second grade included seven items, referred to as \"Approaches to Learning\" items, that asked the teachers to report how often their ECLS-K:2011 children exhibited a selected set of learning behaviors (keeps belongings organized; shows eagerness to learn new things; works independently; easily adapts to changes in routine; persists in completing tasks; pays attention well; and follows classroom rules). 23 These items were presented in the same item set as the social skills items adapted from the Social Skills Rating System (described above in section 3.4), and teachers used the same frequency scale to report how often each child demonstrated the behaviors described. The Approaches to Learning scale score is the mean rating on the seven items included in the scale. A score was computed when the respondent provided a rating on at least 4 of the 7 items that composed the scale. Higher scale scores indicate that the child exhibited positive learning behaviors more often. The itemlevel data for the teacher-reported Approaches to Learning items are included in the data file along with the other child-level teacher questionnaire data. Variable names for the item-level data from the fall firstgrade child-level teacher questionnaire begin with \"T3.\" Those for the item-level data from the spring first-grade child-level teacher questionnaire for children in first grade begin with \"T4,\" while those for children held back in kindergarten begin with \"T4K.\" Variable names for the fall of second grade begin with \"T5,\" and those for the spring of second grade begin with \"T6.\" The variable names, descriptions, value ranges, weighted means, and standard deviations for the teacher-reported Approaches to Learning scale scores are shown in table 3-10. The Approaches to Learning scale has a reliability estimate of .91 for each round of data collection, as measured by Cronbach's alpha. Additionally, the item-level data for the teacher-reported Approaches to Learning items are included in the data file along with the other childlevel teacher questionnaire data. "}, {"section_title": "Children's Behavior Questionnaire (CBQ) and Temperament in Middle Childhood", "text": ""}, {"section_title": "Questionnaire (TMCQ)", "text": "The fall kindergarten, spring kindergarten, and spring first-grade child-level teacher questionnaires (both the version for students in first grade and the version for students in kindergarten) included 12 items from the Short Form of the Children's Behavior Questionnaire (Putnam and Rothbart 2006) 24 asking teachers to indicate how often their ECLS-K:2011 children exhibited certain social skills and behaviors related to inhibitory control and attentional focusing, two indicators related to executive functioning. Rothbart describes inhibitory control as the \"capacity to plan and to suppress inappropriate approach responses under instructions or in novel or uncertain situations\" (Rothbart et al. 2001(Rothbart et al. , p. 1406. Teachers were presented with statements about how the children might have reacted to a number of situations in the past 6 months and were asked to indicate how \"true\" or \"untrue\" those statements were about that child on a 7-point scale ranging from \"extremely untrue\" to \"extremely true,\" with a middle option of \"neither true nor untrue.\" If a statement or situation did not apply to that child, the teacher could indicate \"not applicable.\" The CBQ is appropriate for assessment of children ages 3 through 7 years, so it could not be used past the first-grade rounds of data collection. To remain age appropriate, the CBQ was replaced with the Temperament in Middle Childhood Questionnaire (TMCQ 25 ; Simonds and Rothbart 2004) in the spring of second grade. The TMCQ was designed as an upward age-extension of the CBQ and is appropriate for children ages 7 through 10 years. While many of the items from the TMCQ are different from the items on the CBQ, the items are believed to assess the same or similar constructs in an ageappropriate way. Teachers received the same instructions for the CBQ and TMCQ items, although the TMCQ items were rated on a 5-point scale instead of the 7-point scale used for the CBQ items. For the TMCQ items, teachers used a 5-point scale ranging from \"almost always untrue\" to \"almost always true,\" with a middle option of \"sometimes true, sometimes untrue.\" Like the CBQ, there was a \"not applicable\" option that the teacher could select if the statement or situation did not apply to the child. Item-level data for the items that make up the Attentional Focusing and Inhibitory Control scales are provided on the kindergarten-second grade data file. Variable names for the item-level data from the fall and spring kindergarten child-level teacher questionnaire begin with \"T1\" and \"T2,\" respectively. Variable names for the item-level data from the spring first-grade child-level teacher questionnaire for children in first grade begin with \"T4,\" while variable names for children held back in kindergarten begin with \"T4K.\" Variable names for the spring of second grade begin with \"T6.\" The data file includes two scale scores for each round of data collection in which each measure was included: (1) attentional focus and (2) inhibitory control. In kindergarten and first grade, these scores are derived from the CBQ, and in second grade these scores are derived primarily from the TMCQ, as explained further below. The scale scores were developed using guidelines from the developers of both the CBQ and TMCQ. In kindergarten and first grade, the ECLS-K:2011 fielded all 6 items from the Attentional Focusing subscale and all 6 items from the Inhibitory Control subscale of the CBQ Short Form. As such, the kindergarten and first-grade attentional focus and inhibitory control scores are each based on all 6 items in the relevant Short Form subscale. Because the CBQ was initially designed as a parent-report measure, the item wording for 3 of the items from the CBQ Inhibitory Control subscale was modified slightly for use in the ECLS-K:2011 to make them more appropriate for a school setting. In second grade, the ECLS-K:2011 fielded 6 of the 7 items from the original TMCQ Attentional Focusing subscale. For the inhibitory control dimension, the ECLS-K:2011 fielded 6 of the 8 items from the TMCQ Inhibitory Control subscale and one item from the CBQ Inhibitory Control subscale. Therefore, the second-grade attentional focusing scale score reflects the 6 items fielded by the ECLS-K:2011, not the full set of items in the original TMCQ scale. The second-grade inhibitory control scale score reflects the 7 items fielded by the ECLS-K:2011 (six from the TMCQ and one from the CBQ), again not the full set of items in the original TMCQ scale. Because the TMCQ was designed as a parentreport measure, the item wording on one item from the TMCQ Attentional Focusing subscale was modified slightly to make it more appropriate for a school setting and, similarly, one item on the TMCQ Inhibitory Control subscale was modified. For the kindergarten, first-grade, and second-grade Attentional Focusing and Inhibitory Control scales, the score on each scale is the mean rating on the items included in the scale. A score was computed when the respondent provided a rating on at least 4 of the 6 or 7 items that made up the scale. Higher scale scores on the Attentional Focus scale indicate that the child exhibited more behaviors that demonstrate the ability to focus attention on cues in the environment that are relevant to the task. Higher scale scores on the Inhibitory Control scale indicate that the child exhibited more behaviors that demonstrate the ability to hold back or suppress a behavior as necessary for a particular situation. The variable names, descriptions, value ranges, weighted means, and standard deviations for these scales are shown in tables 3-11 and 3-12. Table 3-13 presents the internal consistency reliability coefficients (Cronbach's alpha) for the teacher-reported Attentional Focus and Inhibitory Control scales for kindergarten through second grades. The Attentional Focus scale for the fall and spring kindergarten data collections (X1ATTNFS, X2ATTNFS) has an internal consistency reliability of .87, and the Inhibitory Control scale for the fall and spring kindergarten data collections (X1INBCNT, X2INBCNT) has a reliability estimate of .87. For the spring of first grade, the Attentional Focus scale (X4ATTNFS) has an internal consistency reliability coefficient (Cronbach's alpha) of .83 for children in first grade and .86 for children retained in kindergarten, and the Inhibitory Control scale (X4INBCNT) has an internal consistency reliability coefficient (Cronbach's alpha) of .86 for both children in first grade and those retained in kindergarten. For the spring of second grade, the Attentional Focus scale (X6ATTMCQ 26 ) has an internal consistency reliability coefficient (Cronbach's alpha) of .96, and the Inhibitory Control scale (X6INTMCQ 27 ) has an internal consistency reliability coefficient (Cronbach's alpha) of .87. Recently, the study received copyright permission to include item-level data from both the CBQ and the TMCQ in the ECLS-K:2011 data files. Therefore, these data have been included in the kindergarten through second-grade data file with the teacher child-level data (which have variable names that begin with \"T\").   "}, {"section_title": "Student-Teacher Relationship Scale", "text": "The Student-Teacher Relationship Scale (STRS) (Pianta 2001) is a 15-item, teacher-reported measure of closeness and conflict between the teacher and child. As part of the spring kindergarten, spring first-grade, and spring-second grade child-level teacher questionnaire, the teacher was presented with 15 descriptive statements about his or her relationship with the ECLS-K:2011 child and asked to indicate the degree to which each statement applied to their relationship using a 5-point scale ranging from \"definitely does not apply\" to \"definitely applies.\""}, {"section_title": "3-42", "text": "Two scales were developed based on guidelines from the developer of the scale: Closeness and Conflict. The Closeness scale score is the average rating on the 7 items included in the STRS, while the Conflict scale score is the average rating on the eight items included in the STRS. A score was computed when the respondent provided a rating on at least 5 of the 7 or 8 items that composed the scales. The Closeness scale is a measure of the affection, warmth, and open communication that the teacher experiences with the student. The Conflict scale is a measure of the teacher's perception of the negative and conflictual aspects of the teacher's relationship with the student. Higher scores on the Closeness scale indicate that the teacher perceived he or she had a closer relationship with the child. Higher scores on the Conflict scale indicate that the teacher perceived his or her relationship with the child to be characterized by more conflict. The variable names, descriptions, value ranges, weighted means, and standard deviations for the STRS scales are shown in table 3-14. Table 3-15 presents the internal consistency reliability coefficients (Cronbach's alpha) for the teacher-reported STRS closeness and conflict scores for kindergarten, first grade, and second grade. In the springs of kindergarten, first grade and second grade, the closeness scale (X2CLSNSS, X4CLSNSS, X4KCLSNSS, and X6CLSNSS) has a reliability estimate that ranges from .86 to .89, and the Conflict scale (X2CNFLCT, X4CNFLCT, X4KCNFLCT, and X6CNFLCT) has an internal consistency reliability estimate that ranges from .88 to .90, as measured by Cronbach's alpha. Recently, the study received copyright permission to include item-level data from the STRS on the ECLS-K:2011 restricted-use data files. While item-level data from the STRS are not in the kindergarten through first-grade data file, they are included in the kindergarten through second-grade restricted-use data file along with the other child-level teacher questionnaire data. 28 Variable names for the item-level data from the spring of kindergarten, the spring of first grade, and the spring of second grade begin with \"T2,\" \"T4\" and \"T6,\" respectively. Variable names that begin with \"T4K\" are for itemlevel data from the spring of first grade for students retained in kindergarten.  "}, {"section_title": "3-44", "text": "This page intentionally left blank."}, {"section_title": "3-45", "text": "4-1"}, {"section_title": "SAMPLE DESIGN AND SAMPLING WEIGHTS", "text": "The Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011) is providing national data on children's characteristics as they progress from kindergarten through the 2015-16 school year, when most of the children will be in fifth grade. In the 2010-11 school year, the ECLS-K:2011 collected data from a nationally representative sample of 18,174 children enrolled in 968 schools. 1 This chapter summarizes the process used to select the sample for the study in the base year (i.e., kindergarten), describes how the sample design changed for the first-and second-grade years, and provides information necessary to properly analyze the data that were collected."}, {"section_title": "Sample Design", "text": "The optimal sample design for collecting data to produce national child-level estimates is to sample children with probabilities that are approximately the same for each child. In most studies, this is achieved using a multi-stage sampling design that involves sampling primary sampling units (PSUs) and schools with probabilities proportional to the targeted number of children attending the school and selecting a fixed number of children per school. Such a sampling procedure was used for the ECLS-K:2011. Additionally, a clustered design was used to minimize data collection costs, which are strongly related to the dispersion of the children in the sample. Restricting data collection to a limited number of geographic areas and to as few schools as possible helps to minimize costs while still achieving an acceptable level of precision in the estimates produced with the data. The sample for the ECLS-K:2011 was selected using a three-stage process. In the first stage of sampling, the country was divided into primary sampling units (PSUs), or geographic areas that are counties or groups of contiguous counties, and 90 PSUs were sampled for inclusion in the study. In the second stage, samples of public and private schools with kindergarten programs or that educated children of kindergarten age (i.e., 5-year-old children) in ungraded settings were selected within the sampled PSUs. Both PSUs and schools were selected with probability proportional to measures of size (defined as the population of 5-year-old children) that took into account a desired oversampling of Asians, Native  (Tourangeau et al. 2015), hereinafter referred to as the base-year User's Manual."}, {"section_title": "ECLS-K:2011 School Sample", "text": "A total of 1,221 clusters of schools 3 were originally selected for the ECLS-K:2011, of which 1,003 were clusters of public schools and 218 were clusters of private schools. This resulted in 1,036 sampled public schools and 283 sampled private schools, for a total of 1,319 sampled schools. The sample frames used to select schools were the 2006-07 Common Core of Data (CCD) and the 2007-08 Private School Survey (PSS), which were the most recent CCD and PSS data available at the time of sampling. Because the 2006-07 CCD and the 2007-08 PSS school frames were several years old, additional schools were sampled from supplemental frames that included newly opened schools and existing schools that added a kindergarten program after the 2006-07 CCD and the 2007-08 PSS data were collected. These additional schools were added to the original school sample. In total, 33 new schools were added, of which 16 were public, 4 were Catholic, and 13 were non-Catholic private schools. The total number of sampled schools after updating was 1,352 (1,052 public schools and 300 private schools). For a detailed discussion of the supplemental school sample, see section 4.1.2.7 of the base-year User's Manual. Early in the process of recruiting schools that had been sampled for the study, it was determined that the rate at which public schools were agreeing to participate was lower than expected and it would be difficult to meet the target number of participating schools by the end of the recruitment Manual, which shows characteristics for the originally sampled schools before substitution.  Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: Data for these school characteristics are taken from the original school sampling frame. Therefore, the table estimates for these characteristics cannot be replicated with variables on the released data file. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2010 and spring 2011."}, {"section_title": "The Sample of Children", "text": "The goal of the sample design was to obtain an approximately self-weighting sample of children, with the exception of Asians, Native Hawaiians, and Other Pacific Islanders (API) who needed to be oversampled to meet sample size goals. Table 4-2 shows the distribution of the eligible children sampled for the ECLS-K:2011, by selected characteristics. Table 4-3 shows the distribution of the children who were respondents in the base year, by selected characteristics. To be considered a base-year respondent, a student had to have child assessment data (defined as having at least one set of scoreable mathematics/reading/science data OR a height or weight measurement, or having been excluded from the assessment due to lack of accommodation for a disability) or parent interview data from the fall or spring data collection, or both, in the base year. Later rounds of data collection were conducted only with baseyear respondents. Sampled students who did not participate in the base year were not recontacted for later rounds of data collection, and no new students were added to the study sample after the base year. 1 Data for this school characteristic are taken from the original school sampling frame. Therefore, the table estimates for this characteristic cannot be replicated with variables on the released data file. 2 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 3 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. 4 Race/ethnicity information was obtained from schools at the time of sampling. As mentioned in the base-year User's Manual, operational problems prevented the study from conducting data collection activities in some areas of the country where Asian, Native Hawaiian/Other Pacific Islander, and American Indian/Alaska Native students sampled for the study resided. For this reason, base-year response rates for these groups of students were lower than response rates for students of other racial/ethnic backgrounds. As a result, a relatively small number of ECLS-K:2011 sample children in the Native Hawaiian/Other Pacific Islander group reside in Hawaii."}, {"section_title": "4-5", "text": "Additionally, nonresponse on the child assessment, parent interview, or both, leads to some of these sampled cases not being included in weighted analyses depending on the weight used. Also, none of the ECLS-K:2011 sample children in the American Indian/Alaska Native group resided in Alaska at the time of sampling. Users are encouraged to consider these sample characteristics when making statements about children in these two racial groups. As a reminder, however, the study was not designed to be representative at the state level or for subgroups within any specific racial or ethnic group."}, {"section_title": "4.2", "text": "Sample Design for the First-and Second-Grade Years"}, {"section_title": "Fall First Grade and Fall Second Grade", "text": "A subsample of students was selected for the fall first-grade and second-grade data collections from the full study sample described above via a three-step procedure. This subsample was designed to be representative of the full sample. In the first step, 30 PSUs were sampled from the 90 PSUs selected for the base year. Within the 30 subsampled PSUs, the 10 self-representing PSUs are large in population size and were included in the fall first-grade sample with certainty. The remaining 20 PSUs were selected from the 80 non-self-representing PSUs in 40 strata. To select the 20 non-self-representing PSUs, 20 strata were sampled with equal probability, and then one PSU was sampled within each stratum also with equal probabilities. This is equivalent to selection with probability proportional to size since the original PSU sample was selected with probability proportional to size. In the second step, all schools within the 30 subsampled PSUs that were eligible for the base-year collection were included in the fall subsample for both first and second grade. However, data collection was not conducted in the subsampled schools in which no children participated in the base year because the study did not try and recruit base-year nonrespondents for later round of data collections.  Table 4-5 shows the characteristics for the subsampled schools with base-year respondents; these are the schools in which data collection was conducted. Transfer schools (those schools that children moved into after the fall of kindergarten) are not included in this table. Of the 346 original sampled schools at the start of the fall data collections, 306 schools still cooperated in fall second grade. In the third step of sampling, students attending the subsampled schools who were respondents in the base year and who had not moved outside of the United States or died before the day assessments began in their school for the fall first-grade data collection were included as part of the fall sample for the first-grade data collection. This sample formed the base sample for the fall second-grade data collection as well, though subsampled children who had moved outside of the United States before the day assessments began in their school for the fall second-grade data collection were excluded. Table 4-6 shows the characteristics of base-year respondents in the fall subsample who were selected in the third sampling step. Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 2 Locale information was taken from the school sampling frame for most schools. For a very small number of schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. NOTE: Data for these school characteristics are taken from the original school sampling frame. Therefore, the table estimates for these characteristics cannot be replicated with variables on the released data file. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2011 and fall 2012. Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 2 Locale information was taken from the school sampling frame for most schools. For a very small number of schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. NOTE: Data for these school characteristics are taken from the original school sampling frame. Therefore, the table estimates for these characteristics cannot be replicated with variables on the released data file. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2011 and fall 2012. Data for this school characteristic are taken from the original school sampling frame. Therefore, the table estimates for this characteristic cannot be replicated with variables on the released data file. 2 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 3 Locale information was taken from the school sampling frame for most schools. For a very small number of schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. 4 Race/ethnicity is from the second-grade race/ethnicity composite X_RACETH_R. NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2011 and fall 2012."}, {"section_title": "4-10", "text": ""}, {"section_title": "4-11", "text": ""}, {"section_title": "4-12", "text": "Tables 4-7 and 4-8 show the characteristics of base-year respondents in the fall samples, by whether the students were still in the original sampled schools or had transferred to other schools by the end of first grade and second grade, respectively. Table 4-7 shows that 81 percent of students were still attending their original sampled schools in the fall of first grade. Table 4-8 shows that 70 percent of students were still attending their original sampled schools in the fall of second grade. In the fall of first grade, the lowest percentages of students who were still attending their original sample schools are for students in non-Catholic private schools, students in the West, students in the suburbs, and Black students. The same is true for the fall of second grade with the percentage of students in non-Catholic private schools even lower than in first grade. 5 5 Significance tests were not conducted for the comparisons in this chapter because the differences discussed were based on the same sample of base-year respondents. 1 Because this table includes transfer schools that were not in the original school frame, school frame data could not be used for school characteristics. Data for school census region and school locale are taken from the first-grade composite variables X3REGION and X3LOCALE. There was no school administrator questionnaire in the fall of first grade, therefore, the composite for school type, X3SCTYP, was constructed specially for the User's Manual and not included in the data file. 2 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 3 Locale information was taken from the school sampling frame for most schools. For a very small number of schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. 4 Race/ethnicity is from the race/ethnicity composite X_RACETH_R. NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2011. Because this table includes transfer schools that were not in the original school frame, school frame data could not be used for school characteristics. Data for school census region and school locale are taken from the second-grade composite variables X5REGION and X5LOCALE. There was no school administrator questionnaire in the fall of second grade; therefore, the composite for school type, X5SCTYP, was constructed specially for the User's Manual and not included in the data file. 2 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 3 Locale information was taken from the school sampling frame for most schools. For a very small number of schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. 4 Race/ethnicity is from the race/ethnicity composite X_RACETH_R. NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2012."}, {"section_title": "4-13", "text": ""}, {"section_title": "4-14", "text": ""}, {"section_title": "4-15", "text": ""}, {"section_title": "Spring First Grade and Spring Second Grade", "text": "All base-year respondents were statistically eligible for the spring first-grade and spring second-grade data collections, with the exception of those who moved outside the United States or died before the assessments began in their school.  Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 2 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. NOTE: Data for these school characteristics are taken from the original school sampling frame. Therefore, the table estimates for these characteristics cannot be replicated with variables on the released data file. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2012 and spring 2013."}, {"section_title": "4-16", "text": "The characteristics of base-year respondents who were eligible for the spring first-and second-grade data collections are those presented above in table 4-3; since there was no subsampling for the spring rounds of data collection, all base-year respondents were initially eligible for data collection if they had not moved outside the United States or died prior to data collection. By the end of the secondgrade data collections, 127 base-year respondents had moved out the country and 4 had died. Table 4-10 and table 4-11 show the characteristics of base-year respondents in the spring samples, by whether the students were still in their original sampled schools or had transferred to other schools. In the spring of first grade, 78 percent of base-year respondents were still attending their original sampled schools. This percent is 68 for the spring of second grade. As is seen with the fall subsample, the lowest percentages of students who were still attending their original sample schools in the spring of first grade are for students in non-Catholic private schools, students in the West, students in the suburbs, and Black students. For the spring of second grade, the pattern is the same except that students in different types of private schools moved at about the same rate, while students in public schools moved at a higher rate.  4 Race/ethnicity is from the race/ethnicity composite X_RACETH_R. NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2012.  4 Race/ethnicity is from the second-grade race/ethnicity composite X_RACETH_R. NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. SOURCE: U.S. In order to control data collection costs, there are some students who are part of the statistical samples for the first-and second-grade data collections but were excluded from actual data collection. These students, while statistically eligible for the study, were operationally ineligible."}, {"section_title": "4-17", "text": ""}, {"section_title": "4-18", "text": "Specifically, not all students who moved away from their original base-year schools after the spring baseyear data collection (known as \"movers\") were followed into their new schools. While some movers were followed with certainty, some subsampling of other movers occurred, as described below. Although information was not collected from all students in every round, the study sampling procedures, combined with the use of sampling weights that include mover subsampling adjustments (described below in section 4.3.2.2) in data analysis, result in the collected data being representative of the students in the kindergarten class of 2010-11 who remain living in the United States. Homeschooled children ( i.e., those who were enrolled in a school at the time of sampling in the base year but left school to become homeschooled) were followed with certainty; they were assessed in their home if there was parental consent to do so. Destination schools. When four or more students moved from an original sampled school into the same transfer school, all those movers were followed into the new school, which is referred to as a destination school. This type of movement occurred for children who attended sampled schools that ended at a particular grade, which are referred to as terminal schools. For example, study students who attended an original sample school that ended with first grade would move as a group to a new school for second grade. In some cases, an original sampled school did not terminate in a particular grade, but for some reason four or more students from that school moved together into the same transfer school for the second-grade data collections. For example, this would happen if an original sample school closed after the spring first-grade data collection. More than one destination school may be identified for an original school if separate clusters of four or more students move into different transfer schools."}, {"section_title": "4-20", "text": "Language minority (LM) students, students with an Individualized Education Program (IEP), and students who had an Individualized Family Service Plan (IFSP). Students who were identified as language minority (LM) based on parent report of home language in the base year, as well as students identified as currently having an Individualized Education Program (IEP), were followed at a rate of 100 percent. The IEP status of the child was obtained during the pre-assessment call when the team leader asked the school coordinator whether the child had an IEP or equivalent program on record with the school. The school records also may have indicated that a child had an IFSP when he or she was younger, even if the child did not have an IEP at the time of data collection, which the team leader could have noted during the call. Additionally, information about whether a child had had an IFSP prior to kindergarten was collected in the base-year parent interview. Approximately 92 percent of children who had an Individualized Family Service Plan (IFSP) before starting kindergarten, according to parent report, were followed through the spring second-grade data collection. 6 General procedures for all other movers. Fifty percent of students who did not meet one of the criteria described above (i.e., did not move to a destination school, were not LM, and did not have an IEP) were sampled with equal probability to be flagged as \"follow\" if they moved from their original sample school. If a student was flagged as \"do not follow,\" no data were collected for him or her. Students flagged as \"do not follow\" were not sought for participation in any further data collection unless they were part of the fall subsample, as explained further below. If a student was flagged as \"follow,\" and 6 The study intended to follow children whose parents indicated they had had an IFSP at a rate of 100 percent. However, due to an identification error, these children were not flagged to be followed with certainty, and, therefore, not all of them were followed when they moved from their originally sampled school. Despite this lack of sample protection, the vast majority of students who had an IFSP, according to parent report, were followed into second grade, either because they did not change schools, they had an IEP and became part of the protected group as a result of the IEP, or because they were part of the mover subsample that was followed at a rate of 50 percent. There are some differences between the group of IFSP children who were followed and those who were not. However, some of these differences appear to be related to the likelihood that a child had an IEP (and, therefore, whether the child became part of the protected group as a result of the IEP). For example, compared to those IFSP children who were not followed, a higher percentage of IFSP children who were followed attended public schools, which are required to provide disability services through an IEP. The subsampling process itself should not have introduced bias into the sample of IFSP children who were followed, because cases were randomly flagged to be followed. Additionally, the sampling weights developed for use with second-grade data account for this random subsampling. A comparison of key weighted estimates (such as school type, region of residence, school locale, percent of students in the school who were races other than White, and student race/ethnicity, gender, and year of birth) between kindergarten and first grade generally suggests the loss of those children who were not followed has little impact on the overall estimates for children who had IFSPs before age 3. Where slight differences between the kindergarten and first-grade estimates were noticed (for example, on the percent of students of race other than White in a school), the pattern with the sample of IFSP children is reflective of differences seen in the full ECLS-K:2011 sample. Also, it should be kept in mind that identifying a child to be followed with certainty does not necessarily mean that the child would have participated in the round(s) in which he or she was followed. Due to general sample attrition, the IFSP students who were not flagged to be followed with certainty constitute only about half of all IFSP students who did not participate in first grade and second grade. It is unlikely that differences in weighted estimates for the entire group of IFSP children (about 680) are due solely to the absence of the approximately 60 IFSP cases that were not followed neither in first grade nor in second grade. Nonparticipation of IFSP children in later rounds of the study for any reason does reduce the IFSP sample available for analysis. As is the case for analysis of any small subgroup, users should consider the size of their analytic sample and whether there is enough power in the data to make generalizations about the groups being examined."}, {"section_title": "4-21", "text": "1. the student moved into a school in a study PSU: the student was included in all aspects of data collection (child assessment, parent interview, school administrator questionnaire, and teacher questionnaires); 2. the student moved into a school outside a study PSU: only a parent interview was attempted; 3. the student moved into a school outside the country: the student was out of scope and considered ineligible for continuation in the study. Procedures for students in the fall subsample. Fifty percent of all students in the subsample had their follow flag set to \"follow\" after the base-year data collection. Children were sampled with equal probability to be flagged as \"follow,\" meaning that if they transferred to a new school they would be followed into that new school for the fall first-and second-grade data collections. As explained in detail below, all students who were subsampled in the fall, regardless of their mover status, were followed in the spring data collections. Procedures for students in the spring main sample. Fifty percent of the schools in the main sample were subsampled with equal probability to have follow flags (i.e., all students in the 50 percent subsample of schools have flags set to \"follow\") applicable for the spring data collections. All fall schools in the 30 sampled PSUs were included in the \"mover follow\" sample for the spring of first and second grade. An additional sample of schools that were not part of the fall subsample was selected to arrive at 50 percent of the entire sample of schools being included in the \"mover follow\" subsample in the spring first-and second-grade data collections. In this way, students who were originally sampled for fall data collections were included in the spring data collections with certainty. These fall subsample cases were followed for the spring data collections even if they were movers in the fall and had their fall mover flag set to \"not follow\" or they were nonrespondents in the fall. Also, this method allows fall subsample movers to continue to be followed in each subsequent round of data collection, as well as more clustering of the movers to be followed, thus cutting down on field costs."}, {"section_title": "Calculation and Use of Sample Weights", "text": "The ECLS-K:2011 data should be weighted to account for differential probabilities of selection at each sampling stage and to adjust for the effect nonresponse can have on the estimates. For the base year, weights were provided at the child and school levels. Estimates produced using the base- year. Estimates produced using the base-year school-level weight are representative of schools with kindergarten programs or schools that educate children of kindergarten age in an ungraded setting. For the first-and second-grade data collections, weights are provided only at the child level, to produce estimates for the kindergarten cohort during the 2011-12 school year and the 2012-13 school year, respectively. "}, {"section_title": "Types of Sample Weights", "text": "Main sampling weights designed for use with data from a complex sample survey serve two primary purposes. When used in analyses, the main sampling weight weights the sample size up to the population total of interest. In the ECLS-K:2011, weighting produces national-level estimates. Also, the main sampling weight adjusts for differential nonresponse patterns that can lead to bias in the estimates. If people with certain characteristics are systematically less likely than others to respond to a survey, the collected data may not accurately reflect the characteristics and experiences of the nonrespondents, which can lead to bias. To adjust for this, respondents are assigned weights that, when applied, result in respondents representing their own characteristics and experiences as well as those of nonrespondents with similar attributes. A sample weight could be produced for use with data from every component of the study (e.g., data from the fall second-grade parent interview, the spring second-grade child assessment, the and for every combination of components for the study (e.g., data from the spring second-grade child assessment with data from the spring second-grade school administrator questionnaire, or data from the spring kindergarten child assessment with data from the fall second-grade child assessment and the fall second-grade parent interview). However, creating all possible weights for a study with as many components as the ECLS-K:2011 would be impractical, especially as the study progresses and the number of possible weights increases. In order to determine which weights would be most useful for researchers analyzing data from second grade, completion rates for each fall second-grade and spring second-grade component (e.g., response to the child assessment, the parent interview, various parts of the teacher questionnaire) were reviewed in combination with completion rates from the kindergarten and first-grade years, and consideration was given to how analysts are likely to use the data. The best approach to choosing a sample weight for a given analysis is to select one that maximizes the number of sources of data included in the analyses for which nonresponse adjustments are made, which in turn minimizes bias in estimates, while maintaining as large an unweighted sample size as Child base weight adjusted for nonresponse associated with child assessment data from fall kindergarten and spring kindergarten, spring first grade, and spring second grade, parent data from fall kindergarten or spring kindergarten, parent data from spring first grade, parent data from spring second grade, and either teacher/classroom or child-level teacher data from fall and spring kindergarten (from a core or supplemental teacher questionnaire), spring first grade (from a first-grade or a kindergarten teacher questionnaire), and spring second grade Child base weight adjusted for nonresponse associated with child assessment data from spring kindergarten, fall first grade, spring first grade, and fall second grade, parent data from fall kindergarten or spring kindergarten, parent data from fall first grade, and parent data from fall second grade (C2)(C3)(C4)(C5)(P1_P2)(P3)(P5)"}, {"section_title": "W6C_6P_6TZ0", "text": "Child base weight adjusted for nonresponse associated with child assessment data from fall kindergarten or spring kindergarten, child assessment data from spring second grade, parent data from fall kindergarten or spring kindergarten, parent data from spring second grade, either teacher/classroom or child-level teacher data from fall kindergarten or spring kindergarten (from a core or supplemental teacher questionnaire), and teacher/classroom or child-level teacher data from spring second grade. This weight is positive for the sample of children who have child, parent, and teacher data as defined above. This weight also includes an adjustment for unknown eligibility and nonresponse associated with the beforeor after-school care (BASC) questionnaires from spring kindergarten. (C1_C2)(C6)(P1_P2)(P6)(T1_T2)(T6)(Z2) NOTE: Having child assessment data includes (1) having reading and/or mathematics and/or science scores, (2) having at least one executive function score, (3) having a height or weight measurement, or (4) being excluded from assessment due to lack of accommodation for a disability. The weight designations (C1, C2, etc.) use the same prefixes that are used for other variables in the kindergarten-second grade data file. The prefixes are listed in exhibit 7-1. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), kindergarten-second grade (K-2) data file."}, {"section_title": "4-26", "text": "Exhibit 4-2, which presents the same information as exhibit 4-1 in matrix format, was developed to further assist researchers in deciding which weight to use for analyses. In exhibit 4-2, the components for which nonresponse adjustments are made for each weight are noted with a \"Yes.\" Researchers should choose a weight that has a \"Yes\" in the column(s) for the source(s) of data they are using in their analyses. The best weight would have a \"Yes\" for each and every source used and only those sources. For example, if a researcher is conducting an analysis that includes fall second-grade and spring second-grade child assessment data, and fall kindergarten or spring kindergarten parent interview data, the weight W6CF6P_2A0 should be used since it adjusts for nonresponse on all of those components (i.e., exhibit 4-2 shows a \"Yes\" in the fall kindergarten and spring kindergarten parent columns and the fall second-grade and spring-second grade child assessment columns; the italicized Yes indicates an \"or\" condition). However, for many analyses, there will be no weight that adjusts for nonresponse to all the sources of data that are included and for only those sources. When no weight corresponds exactly to the combination of components included in the desired analysis, researchers might prefer to use a weight that includes nonresponse adjustments for more components than they are using in their analysis (i.e., a weight with \"Yes\" in columns corresponding to components that are not included in their analyses) if that weight also includes nonresponse adjustments for the components they are using. Although such a weight may result in a smaller analytic sample than would be available when using a weight that corresponds exactly to the components from which the analyst is using data, it will adjust for the potential differential nonresponse associated with the components. If researchers instead choose a weight with nonresponse adjustments for fewer components than they are using in their analysis, missing data should be examined for potential bias. Main sampling weights (indicated by the suffix 0) and replicate weights (indicated by the suffixes 1 to 40 or 1 to 80) were computed and included in the data file. In the sections that follow, only the main sampling weight is discussed, but any adjustment done to the main sampling weight was done to the replicate weights as well."}, {"section_title": "Student Base Weights", "text": "Only base-year respondents were eligible to participate in the second-grade rounds of data collection. For the fall of second grade, when only a subsample of students was included in data collection, the second-grade student base weight is the product of the base-year student base weight 8 adjusted for base-year nonresponse and the inverse of the selection probabilities for the primary sampling units (PSUs) for the fall subsample. For the spring of second grade, when the full sample of students was included in data collection, the second-grade student base weight is the base-year student base weight 4-29 adjusted for base-year nonresponse. The adjustment factor for base-year nonresponse is the sum of the base weights of the eligible students in the base year divided by the sum of the base weights of the baseyear respondents within nonresponse adjustment classes. 9 For a description of the computation of the base-year student base weights, see section 4.2.2.3.1 of the base-year User's Manual."}, {"section_title": "Student Weights Adjusted for Mover Subsampling", "text": "The student base weight described in section 4.3.2.1 was adjusted to reflect the subsampling of movers described in section 4.2.3. For every student who is a base-year respondent, a \"follow\" flag was assigned a value of 0 (do not follow if student moves) or 1 (follow if student moves). A moversubsampling adjustment factor was set to 1 if the student was not a mover, 2 if the student was a mover and was followed into his or her new school, and 0 if the student was a mover and was not followed. The mover-subsampling adjusted weight is the product of the base weight described in section 4.3.2.1 and this mover-subsampling adjustment factor. Note that child assessments were not conducted and school staff questionnaires were not fielded for students who moved into nonsampled PSUs even if their flag was set to \"follow\"; such students are counted as nonrespondents in the adjustment for nonresponse on weights involving child assessment or teacher data. 10 However, an attempt was made to complete a parent interview for students who moved into nonsampled PSUs if their flag was set to \"follow\"; therefore, their parents would be counted as respondents in the adjustment for parent nonresponse if a parent interview was completed and as nonrespondents if a parent interview was not completed."}, {"section_title": "Student Nonresponse-Adjusted Weights", "text": "The mover-subsampling adjusted weight described in section 4.3.2.2 was adjusted for nonresponse to produce each of the student-level weights described in exhibit 4-1. For each weight, a response status was defined based on the presence of data for the particular component(s) and round(s) covered by the weight."}, {"section_title": "4-30", "text": "For example, for the weight W6CF6P_2A0, an eligible respondent is a base-year respondent who satisfies both of these criteria: (1) the student has child assessment data 11 from fall second grade and spring second grade, and (2) the student has parent interview data from either the fall or spring of kindergarten. An ineligible student is one who moved out of the country or is deceased or moved to another school and was not assigned to be followed. A student of unknown eligibility is one who could not be located. The remaining students are eligible nonrespondents. Nonresponse adjustment was done in two steps: (1) adjustment for children whose eligibility was not determined (i.e., those who could not be located, or those who moved to another sampled PSU and who did not have parent interview data because the parent could not be contacted), and (2) adjustmentfor eligible nonrespondents. In the first step, a portion of cases with unknown eligibility was assumed to be ineligible. This proportion varied between 1 and 2 percent, depending on the weight. The proportion was closer to 2 percent for those weights that adjusted for teacher nonresponse. Nonresponse classes were created using school and child characteristics and used in adjustments for both unknown eligibility and nonresponse. Note that the weight involving BASC data 12 (W6C_6P_6TZ0) is not computed only for children with BASC data or who were eligible for the BASC component. It is computed for the entire sample and includes additional adjustments for BASC unknown eligibility and BASC nonresponse. Weight W6C_6P_6TZ0 is nonzero for children with child assessment data from the fall or spring of kindergarten, child assessment data from the spring of second grade, parent data from the fall or spring of kindergarten, parent data from the spring of second grade, teacher data (teacher/classroom or child-level) from the fall or spring of kindergarten, and teacher data (teacher/classroom or child-level) from the spring of second grade. It includes adjustments for nonresponse associated with these components as well as adjustments for BASC unknown eligibility and nonresponse, and is, therefore, appropriate for analyses that include BASC data along with data from these other components."}, {"section_title": "Raking to Sample Control Totals", "text": "To reduce the variability due to the subsampling of movers and to ensure that the final weights continue to sum to the base-year population total, the student nonresponse-adjusted weights were 4-31 raked to sample-based control totals using the second-grade student base weights. Raking is a calibration estimator that is closely related to poststratification. The poststratification adjustment procedure involves applying a ratio adjustment to the weights. Respondents are partitioned into groups, known as poststrata cells, and a single ratio adjustment factor is applied to the weights of all units in a given poststratification cell. The numerator of the ratio is a \"control total\" usually obtained from a secondary source; the denominator is a weighted total for the survey data. Therefore at the poststratum level, estimates obtained using the poststratified survey weights will correspond to the control totals used. If either the cell-level population counts are not available for all cells or the majority of the cell sample sizes are too small, raking is used to adjust the survey estimates to the known marginal totals of several categorical variables. Raking is essentially a multivariate poststratification. In the ECLS-K:2011, multiple background characteristics from schools, students, and parents were combined to create raking cells. The student records included in the file used for computing the control totals are records of base-year eligible children. The sum of the base weights from this file is the estimated number of children who were in kindergarten in 2010-11. Raking was done within raking cells (also known as raking dimensions). The raking dimensions were based on single characteristics (e.g., locale) or a combination of characteristics (e.g., age and race/ethnicity). Chi-Square Automatic Interaction Detector (CHAID) analysis was used to determine the best set of raking cells. The final weight is the product of the raking factor and the student nonresponse-adjusted weight. The raking factor was computed as the ratio of the base-year sample control total for a raking cell over the sum of the nonresponse-adjusted first-grade weights in that raking cell."}, {"section_title": "Characteristics of Sample Weights", "text": "The statistical characteristics of the sample weights are presented in table 4-12. For each weight, the number of cases with a nonzero weight is presented along with the mean weight, the standard deviation, the coefficient of variation (i.e., the standard deviation as a percentage of the mean weight), the minimum weight, the maximum weight, the skewness, the kurtosis, and the sum of weights. The procedure for raking to control totals included respondents and ineligible cases. Afterwards, weights of ineligible cases were set to zero. Because a portion of children of unknown eligibility was assumed to be ineligible (as discussed in section 4.3.2.3) and this adjustment for unknown eligibility was done within adjustment cells, there are small differences in the sums of weights. "}, {"section_title": "4-32", "text": ""}, {"section_title": "Variance Estimation", "text": "The precision of the sample estimates derived from a survey can be evaluated by estimating the variances of these estimates. For a complex sample design such as the one employed in the ECLS-K:2011, replication and Taylor Series methods have been developed to correctly estimate variance. These methods take into account the clustered, multistage sampling design and the use of differential sampling rates to oversample targeted subpopulations. For the ECLS-K:2011, in which the first-stage self-representing sampling units (i.e., PSUs) were selected with certainty and the first-stage non-selfrepresenting sampling units were selected with two units per stratum, the paired jackknife replication method (JK2) is recommended. This section describes the JK2 and the Taylor series methods, which can be used to compute correct standard errors for any analysis."}, {"section_title": "Jackknife Method", "text": "The final main sampling and replicate weights can be used to compute estimates of variance for survey estimates using the jackknife method with two PSUs per stratum (JK2) using several software 4-33 packages, including WesVar, AM, SUDAAN, SAS, Stata, and R. In the jackknife method, each survey estimate of interest is calculated for the full sample as well as for each of the g replicates, where g is 80 for the spring weights, and 40 for the fall weights. The variation of the replicate estimates around the fullsample estimate is used to estimate the variance for the full sample. The variance estimator is computed as the sum of squared deviations of the replicate estimates from the full sample estimate: where \u03b8 is the survey estimate of interest, \u03b8 is the estimate of \u03b8 based on the full sample, G is the number of replicates, and \u03b8 ( ) is the g th replicate estimate of \u03b8 based on the observations included in the g th replicate. Each main sampling weight that does not include adjustments for nonresponse to components from the fall second-grade data collection has 80 corresponding replicate weights for use with the JK2 method. The replicate weights begin with the same characters as the main sampling weight and end with the numbers 1 to 80. For example, the replicate weights corresponding to weight W6C6P_60 are W6C6P_61 through W6C6P_680. For weights that include nonresponse adjustments for components from the fall second-grade data collection, there are 40 replicate weights. For example, weight W6CF6P_2A0 has W6CF6P_2A1 through W6CF6P_2A40 as replicate weights."}, {"section_title": "Taylor Series Method", "text": "Variance stratum and variance unit (first-stage sample unit [i.e., PSU]) identifiers were also created to be used in statistical software that computes variance estimates based on the Taylor series method (for example, AM, SUDAAN, SAS, SPSS, and Stata). In this method, a linear approximation of a statistic is formed and then substituted into the formula for calculating the variance of a linear estimate appropriate for the sample design."}, {"section_title": "If", "text": "is the corresponding vector of estimators based on a sample s of size n(s), \u03b8 = ( ) is the population parameter of interest, and \u03b8 = (\u0302) is an estimator of \u03b8, then where \u03b8 is the estimate of \u03b8 based on the full sample, \u03b8 is the survey estimate of interest, Y is a p-dimensional vector of population parameters, \u0302 is a p-dimensional vector of estimators, y is an element of the vector Y, and ( ) is an estimator of \u03b8. The Taylor series method relies on a simplified procedure for estimating the variance for a linear statistic even with a complex sample design and is valid when analyzing data from large samples in which the first-stage units are sampled with replacement. 13 The stratum and first-stage unit identifiers needed to use the Taylor series method were assigned as follows: all independent sampling strata were numbered sequentially from 1 to h; within each sampling stratum, first-stage sampling units were numbered from 1 to n h . Care was taken to ensure that there were at least two responding units in each stratum. For instances in which a stratum did not have at least two responding units, the stratum was combined with an adjacent stratum. Stratum and first-stage unit identifiers are provided in the data file. Each main sampling weight has corresponding stratum and PSU identifiers for use with the Taylor series method. The stratum and PSU identifiers begin with the same characters as the main sampling weight and end with either STR or PSU. For example, the stratum and PSU identifiers corresponding to weight W6C6P_60 are W6C6P_6STR and W6C6P_6PSU, respectively."}, {"section_title": "Specifications for Computing Standard Errors", "text": "For the jackknife replication method, the main sampling weight, the replicate weights, and the method of replication must be specified. All analyses of the ECLS-K:2011 data using the replication method should be done using JK2. As an example, an analyst using the main sample weight W6CF6P_2A0 to compute child-level estimates of mean reading scores for the fall of second grade would 13 For the ECLS-K:2011, the sample of PSUs was selected using the Durbin method. In this method, two PSUs were selected per stratum without replacement with probability proportional to size and known joint probability of inclusion in such a way to allow variances to be estimated as if the units had been selected with replacement."}, {"section_title": "4-35", "text": "need to specify W6CF6P_2A0 as the main sampling weight, W6CF6P_2A1 to W6CF6P_2A40 as the replicate weights, and JK2 as the method of replication. Note that there are 40 replicate weights for each weight that involves the fall second-grade data collection and 80 replicate weights for each weight not involving the fall second-grade data collection. For the Taylor series method, the main sampling weight, the sample design, the nesting stratum, and PSU variables must be specified. As an example, an analyst using the main sample weight W6CF6P_2A0 to compute child-level estimates of mean reading scores for the fall of second grade must specify the main sampling weight (W6CF6P_2A0), the stratum variable (W6CF6P_2ASTR0), and the PSU variable (W6CF6P_2APSU). The \"with replacement\" sample design option, WR, must also be specified if using SUDAAN."}, {"section_title": "Use of Design Effects", "text": "An important analytic device is to compare the statistical efficiency of survey estimates from a complex sample survey such as the ECLS-K:2011 with what would have been obtained in a hypothetical and usually impractical simple random sample (SRS) of the same size. In a stratified clustered design, stratification generally leads to a gain in efficiency over simple random sampling, but clustering has the opposite effect because of the positive intracluster correlation of the units in the cluster. The basic measure of the relative efficiency of the sample is the design effect (DEFF), defined as the ratio, for a given statistic, of the variance estimate under the actual sample design to the variance estimate that would be obtained with an SRS of the same sample size: The root design effect (DEFT) is the square root of the design effect: where SE is the standard error of the estimate. As discussed above, jackknife replication and Taylor Series can be used to compute more precise standard errors for data from complex surveys. If statistical analyses are conducted using software . . . . . ."}, {"section_title": "4-36", "text": "packages that assume the data were collected using simple random sampling (i.e., adjustments are not made using jackknife replication or the Taylor series method), the standard errors will be calculated under this assumption and will be incorrect. They can be adjusted using the average DEFT, although this method is less precise than JK or Taylor series. 14 The standard error of an estimate under the actual sample design can be approximated as the product of the DEFT and the standard error assuming simple random sampling. In the ECLS-K:2011, a large number of data items were collected from children, parents, teachers, school administrators, and before-and after-school care providers. Each item has its own design effect that can be estimated from the survey data. Standard errors and design effects are presented in the tables below for selected items from the study to allow analysts to see the range of standard errors and design effects for the study variables. They were computed using the paired jackknife replication method in the statistical software package WesVar. However, as discussed in section 4.3.4, not all statistical analysis software packages have procedures to compute the variance estimate or standard error using the replication method, and some analysts may not have access to software packages that do have such procedures. In such situations the correct variance estimate or standard error can be approximated using the design effect or the root design effect. As the first step in the approximation of a standard error, the analyst should normalize the overall sample weights for packages that use the weighted population size (N) in the calculation of standard errors (SPSS but not SAS). The normalized weight will sum to the sample size (n) and is calculated as where n is the sample size (i.e., the number of cases with a valid main sampling weight) and N is the sum of weights. See table 4-12 for the sample size n and the sum of weights N. As the second step in the approximation, the standard errors produced by the statistical software, the test statistics, or the sample weight used in analysis can be adjusted to reflect the actual 14 Common procedures in SAS, SPSS, and Stata assume simple random sampling. Data analysts should use the SURVEY procedure (SAS), the Complex Samples module (SPSS), or the SVY command (Stata) to account for complex samples."}, {"section_title": "4-37", "text": "complex design of the study. To adjust the standard error of an estimate, the analyst should multiply the standard error produced by the statistical software by the square root of the DEFF or the DEFT as follows: A standard statistical analysis package can be used to obtain VAR SRS and SE SRS . The DEFF and DEFT used to make adjustments can be calculated for specific estimates, can be the median DEFF and DEFT across a number of variables, or can be the median DEFF and DEFT for a specific subgroup in the population. Adjusted standard errors can then be used in hypothesis testing, for example, when calculating t and F statistics. A second option is to adjust the t and F statistics produced by statistical software packages using unadjusted (i.e., SRS) standard errors. To do this, first conduct the desired analysis weighted by the normalized weight and then divide a t statistic by the DEFT or divide an F statistic by the DEFF. A third alternative is to create a new analytic weight variable in the data file by dividing the normalized analytic weight by the DEFF and using the adjusted weight in the analyses. proportions selected from the spring second-grade data collection. In general, design effects for the fall of second grade are larger than design effects for the spring of second grade for similar items. This is due to the larger variability in the weights as a result of the smaller sample size in the fall due to subsampling. Similarly, design effects are larger when an estimate calculated for one round of data collection applies only to a small sample of children. As was the case in earlier years, design effects for the teacher-level data and the school-level data are quite large compared to the rest because the intraclass correlation is 100 percent for children in the same class with the same teacher and for children in the same school. 1 Estimates of variables whose names start with X5 or P5 were computed using weight W6CF6P_2A0, except for those with names starting with X5T. 2 Estimates of variables whose names start with X5T or T5 were computed using weight W6CF6P_2T0. NOTE: SE is the standard error based on the sample design. SEsrs is the standard error assuming simple random sampling. DEFT is the root design effect. DEFF is the design effect. Estimates produced with the restricted-use file. Due to top-and bottom-coding, the same estimates may not be obtained from the public-use file. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2012.   1 Estimates of assessment scores, age, height, weight and BMI were computed using weight W6CS6P_20. 2 Estimates of variables from the teacher and school administrator questionnaires were computed using weight W6CS6P_6TA0. 3 Estimates of variables from the parent interview were computed using weight W6CS6P_6A0. NOTE: SE is the standard error based on the sample design. SEsrs is the standard error assuming simple random sampling. DEFT is the root design effect. DEFF is the design effect. Estimates produced with the restricted-use file. Due to top-and bottom-coding, the same estimates may not be obtained from the public-use file. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2013. This page intentionally left blank."}, {"section_title": "4-38", "text": ""}, {"section_title": "4-39", "text": ""}, {"section_title": "4-40", "text": ""}, {"section_title": "4-42", "text": "5-1"}, {"section_title": "RESPONSE RATES", "text": "This chapter presents unit response rates and overall response rates for the different instruments included in the second-grade data collections of the ECLS-K:2011. A unit response rate is the ratio of the number of units with a completed interview, questionnaire, or assessment (for example, the units are students with a completed assessment) to the number of units sampled and eligible for the interview, questionnaire, or assessment. Unit response rates are used to describe the outcomes of data collection activities and to measure the quality of the study. The overall response rate indicates the percentage of eligible units with a completed interview, questionnaire, or assessment, taking all survey stages into account."}, {"section_title": "Study Instruments", "text": "For the ECLS-K:2011 second-grade data collections, there were several survey instruments, as shown in exhibit 5-1. Exhibit 5-1 also indicates how much information had to be collected for each instrument for it to be considered \"complete\" and, therefore, for a case to be considered a respondent to that instrument for the purpose of calculating response rates. Response rates are presented in section 5.2 for all of these instruments, separately for each round of data collection in which the instrument was included and, for selected instruments, for combinations of rounds of data collection."}, {"section_title": "5-2", "text": "Exhibit In the spring second grade data collection, there were two versions of the school administrator questionnaire. SAQ-A was given to administrators in schools for which there were neither school administrator data from the spring of first grade nor the spring of kindergarten, and SAQ-B was given to administrators in schools for which there were school administrator data from the spring of first grade or spring of kindergarten. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), kindergarten-second grade (K-2) data file."}, {"section_title": "Unit Response Rates and Overall Response Rates", "text": "The tables in this section present both weighted and unweighted response rates for the different components of data collection shown above in exhibit 5-1 (the child assessment, parent interview, general classroom teacher questionnaires, school administrator questionnaire (SAQ), and special education teacher questionnaires) computed at the student level. Response rates for all students and response rates by selected school and student background characteristics are provided. Only weighted rates are discussed in this section. The unweighted rate provides a useful description of the success of the operational aspects of the survey. The weighted rate gives a better description of the success of the survey with respect to the population sampled since the weights allow for inference of the sample data (including response status) to the population level. Both rates are usually similar unless the probabilities of selection and the unit response rates in the categories with different selection probabilities vary considerably. All of the unit response rates discussed in this chapter are weighted unless noted specifically in the text, since the main purpose of this chapter is to describe the success of the survey with respect to the survey population. The weights used in the computation of the student-level unit response rates are the second-grade student base weights. For a description of these weights, see chapter 4. In order to compute response rates by different characteristics, the selected characteristics must be known for both respondents and nonrespondents. Multiple sources were used to obtain information on school characteristics in order to have data that were as complete as possible for the calculation of response rates. For respondents, data for school census region, school locale, school type, and school enrollment come from the composite variables derived for the data file. 1 For nonrespondents, school characteristic variables were computed for use in the response rate calculations using the same process that was used to compute the data file composite variables. 2 The tables presenting response rates for the spring second-grade collection use the spring second-grade version of the composites. Fall second-second grade response rate tables use the spring second grade version of the composite for cases where the child was in the same school in both fall and spring of second grade. If the child was not in the same school in the fall and spring of second grade, then the fall second-grade version of the composites were used. Information on the child characteristics presented in the tables comes from the second-grade data collection. Information on student sex comes from the composite variable X_CHSEX_R (described 1 A composite variable for school type is only available in the data file for the spring second-grade collection. The school type variable for the fall second-grade collection was computed specifically to produce these response rate tables and is not available on the data file. 2 Information on the derivation of variables indicating school region (X*REGION) and school locale (X*LOCALE) is provided in section 7.5.4.8. Information on the derivation of variables indicating school type (X*SCTYP) is provided in section 7.5.4.1. Information on the derivation of variables indicating school enrollment (X*ENRLS) is provided in section 7.5.4.3. Information on the derivation of variables indicating percent minority enrollment (X*RCETH) is provided in section 7.5.4.4."}, {"section_title": "5-4", "text": "in section 7.5.1.3). Information on student race/ethnicity comes from the composite variable X_RACETH_R (described in section 7.5.1.4). Information on student year of birth comes from the composite variable X_DOBYY_R (described in section 7.5.1.1). These composites were derived for all base-year respondents; therefore they exist for second-grade respondents as well as nonrespondents. As noted in chapter 4, the fall second-grade data collection was conducted with a subsample of students attending schools that had participated in the base year and were located within the subsample of 30 PSUs selected for the fall collection. While all students attending the subsample schools who had been originally sampled for the study are considered to be part of the fall subsample (7,019 children in 346 schools), only those students who were base-year respondents 3 were followed for participation in the fall second-grade data collection. Of those 6,109 base-year respondents, about 60 became ineligible for the fall data collection because they had moved out of the country sometime between the base year and the start of the fall second-grade data collection, 2 died (and thus became ineligible), and about 630 were not included in the fall data collection because they were movers who were subsampled out of the study (see section 4.2.3 for information on mover subsampling). After these exclusions for ineligibility and subsampling, the number of children followed for data collection in the fall of second-grade was about 5,410. This number is the denominator used to calculate the unweighted fall parent interview response rate. Students who were excluded from the assessment because the study did not provide needed accommodations for a disability, such as an assessment in Braille, are not included in the calculation of response rates for the child assessment. The denominator used to calculate the unweighted fall child assessment response rate is about 5,390. For the teacher response rates, the denominator is 4,797. 4 This denominator is lower than the ones used to calculate response rates for the child assessment and parent interview because it excludes students who were not eligible for the teacher questionnaire component: homeschooled children 5 and children who did not have either a complete child assessment score or parent interview (per the definition of complete provided in exhibit 5-1) for the fall second-grade collection. The parent and teacher response rates are computed at the student level, meaning they indicate the percentages of students for whom a parent interview was completed and for whom a teacher questionnaire was received, respectively. When necessary, comparisons in this chapter were examined to ensure that the differences discussed were statistically significant at the 95 percent level of confidence. For example, this was done 5-5 for tables in sections 5.3 when comparing characteristics of the data using different weights, or when comparing data from different years. Significance tests were not conducted for statements related to response rates in section 5.2 because the base weights were used to produce all rates, which are calculated over the same sample of eligible cases. included in a specific school characteristics category. This may have an impact on the calculation of the response rates by school characteristics that should be considered. Specifically, including these unlocatable cases in a separate category likely results in response rates by different school characteristics being higher than they would be if the unlocatable cases were included as nonrespodents when calculating response rates for the different school characteristic categories. This is true for all response rates presented in tables 5-1 to 5-4. For the fall child assessment, the weighted student-level response rate was 84 percent. Almost all of the response rates by the selected school characteristics exceed 90 percent. The exceptions are for students in the \"Unknown\" categories and in non-Catholic private schools. The highest response rates were for students in towns and rural areas (96.4 percent for both) and in the South census region (96.2 percent). The lowest response rates were for found for students in non-Catholic private schools (68.9 percent). For the fall parent interview, the weighted response rate was 80.5 percent. The highest response rates were for students in schools in the lowest percent minority group (91.1 percent), and in Catholic schools (88.3 percent). As with the child assessment, the lowest response rate was for students in non-Catholic private schools (74.4 percent).  1 Student had scoreable reading or mathematics or science data, or at least one executive function score, or a height or weight measurement. 2 Parent answered all applicable items in the time use section of the questionnaire (TUQ). 3 Because the school administrator questionnaire (SAQ) was not administered in the fall of second grade, school characteristics were taken from the spring second-grade SAQ responses for fall second-grade students who attended the same school in both rounds, where available. When spring second-grade SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). The versions of the school characteristics variables used to produce this table were specially derived for the User's Manual and are not included in the data file. 4 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total. 5   percent, respectively) and Black students (71 percent). 1 Student had scoreable reading or mathematics or science data, or at least one executive function score, or a height or weight measurement. 2 Parent answered all applicable items in the time use section of the questionnaire (TUQ). 3 Sample sizes have been rounded to the nearest 10. Therefore, detail may not sum to total. NOTE: The fall second-grade data collection included only 30 percent of the sampled study PSUs. The weighted response rates were calculated using the fall second-grade student base weight. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2012."}, {"section_title": "5-6", "text": ""}, {"section_title": "5-8", "text": ""}, {"section_title": "5-9", "text": "Table 5-3 presents the weighted and unweighted response rates for the general classroom teacher student-level questionnaire in the fall second-grade data collection, by selected school characteristics. The weighted response rate for all students was 95.8 percent. The highest response rates were for students in the South census region (98.5 percent) and for students in towns (98 percent). Aside from the \"Unknown\" categories, which had very low response rates due to the composition of the category, the lowest response rate was for students in non-Catholic private schools (86 percent). 1 Because the school administrator questionnaire (SAQ) was not administered in the fall of second grade, school characteristics were taken from the spring second-grade SAQ responses for fall second-grade students who attended the same school in both rounds, where available. When spring second-grade SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). The versions of the school characteristics variables used to produce this table were specially derived for the User's Manual and are not included in the data file. 2 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total. 3 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. NOTE: The fall second-grade data collection included only 30 percent of the sampled study PSUs. A respondent is defined as a child for whom a teacher questionnaire was returned and the questionnaire had at least one response. The weighted response rates were calculated using the fall second-grade student base weight. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2012. Table 5-4 presents the weighted and unweighted response rates for the general classroom teacher student-level questionnaire in the fall second-grade data collection, by selected student characteristics. Overall, the response rates for students with different characteristics were fairly consistent, ranging between 91 and 97 percent. No subgroups had a notably low response rate. The subgroup with the highest response rates are students born in 2004 (96.9 percent). 1 This category includes children who are more than one race (non-Hispanic) and children whose race/ethnicity is unknown. 2 Sample sizes have been rounded to the nearest 10. Therefore, detail may not sum to total. NOTE: The fall second-grade data collection included only 30 percent of the sampled study PSUs. A respondent is defined as a child for whom a teacher questionnaire was returned and the questionnaire had at least one response. The weighted response rates were calculated using the fall second-grade student base weight. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2012."}, {"section_title": "5-11", "text": ""}, {"section_title": "5-12", "text": "The overall response rate indicates the percentage of possible interviews, questionnaires, or assessments completed, taking all survey stages into account. In the base-year data collection, children were identified for assessment in a two-stage process. The first stage involved the recruitment of sampled schools to participate in the study. Assessments were then conducted for the sampled children whose parents consented the children's participation. In the fall of second grade, children in the subsampled schools were contacted for follow-up unless they (1) became ineligible for the study because they had moved out of the country or had died, or (2) were movers who were not sampled for follow-up and, therefore, were excluded from data collection. The response rate for the child assessment is the percentage of sampled and eligible children not subsampled out as an unfollowed mover who completed the assessment. The overall weighted response rate is the product of the base-year before-substitution school response rate for all schools (62.7 percent), and the second-grade weighted child assessment response rate. The overall unweighted response rate is the product of the unweighted base-year before-substitution response rate for all schools (61.3 percent) and the second-grade unweighted child assessment response rate. In the overall response rate tables, the response rates by characteristic are also a product of the second-grade response rate by the corresponding (weighted or unweighted) overall base-year rate. The overall weighted and unweighted response rates for the child assessment, the parent interview, and the student-level teacher questionnaire in the fall second-grade data collection are presented in tables 5-5 and 5-6. Not all the schools in the fall subsample responded to the fall data collection; of the 995 original and transfer schools that were initially identified as eligible for the fall second-grade data collection, 888 schools agreed to participate, 31 schools refused, and 76 schools became ineligible because all ECLS-K:2011 students in the school had moved to other schools. Because children were sampled in the base year and school participation after the base year was not required for the children to stay in the study, the school response rates used to calculate the student-level response rates in these tables are those from the base year (the base-year response rates are presented in  (Tourangeau et al. 2015), hereinafter referred to as the base-year User's Manual). The overall response rate for the fall child assessment (the product of the base-year school response rate and the fall child assessment rate) was 52.7 percent. The overall response rate for the fall parent interview was 50.5 percent and for the teacher questionnaire was 60.1 percent. Because the driving factor of the overall response rate is the base-year school response rate for all schools, the pattern of the overall response rates by subgroups is the same as the pattern of the fall second-grade response rates. 1 Student had scoreable reading or mathematics or science data, or at least one executive function score, or a height or weight measurement. 2 Parent answered all applicable items in the time use section of the questionnaire (TUQ). 3 Because the school administrator questionnaire (SAQ) was not administered in the fall of second grade, school characteristics were taken from the spring second-grade SAQ responses for fall second-grade students who attended the same school in both rounds, where available. When spring second-grade SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). The versions of the school characteristics variables used to produce this table were specially derived for the User's Manual and are not included in the data file. 4 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total. 5 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. NOTE: The fall second-grade data collection included only 30 percent of the sampled study PSUs. The weighted overall response rate was calculated using the school base weight for the school response rate component and the student base weight for the student response rate component. The counts of students by subgroups do not sum to the total because homeschooled students and students with unknown school characteristics are not included in this  1 Because the school administrator questionnaire (SAQ) was not administered in the fall of second grade, school characteristics were taken from the spring second-grade SAQ responses for fall second-grade students who attended the same school in both rounds, where available. When spring second-grade SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). The versions of the school characteristics variables used to produce this table were specially derived for the User's Manual and are not included in the data file. 2 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total. 3 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. NOTE: The fall second-grade data collection included only 30 percent of the sampled study PSUs. A respondent is defined as a child for whom a teacher questionnaire was returned and the questionnaire had at least one response. The weighted overall response rate was calculated using the school base weight for the school response rate component and the student base weight for the student response rate component. The counts of students by subgroups do not sum to the total because homeschooled students and students with unknown school characteristics are not included in this "}, {"section_title": "5-13", "text": ""}, {"section_title": "5-14", "text": ""}, {"section_title": "5-15", "text": "In the spring second-grade data collection, the 18,174 base-year respondents were part of the sample. Of these, about 130 became ineligible for the spring data collection because they had moved out of the country sometime between the base year and the start of the spring second-grade data collection and 4 had died. An additional 2,110 students were not included in the spring data collection because they were movers who were subsampled out of the study (see section 4.2.3 for information on mover subsampling). After these exclusions for ineligibility and subsampling, the number of children followed for data collection in the spring of second-grade was about 15,940. This number is the denominator used to calculate the unweighted spring parent interview response rate. Students who were excluded from the assessment because the study did not provide needed accommodations for a disability, such as an assessment in Braille, are not included in the calculation of response rates for the child assessment. The denominator used to calculate the unweighted child assessment response rate is about 15,870. The denominator used to calculate the teacher and the school administrator response rates is 14,404. This denominator is lower than the ones used to calculate response rates for the child assessment and parent interview because it excludes students who were not eligible for the teacher and administrator questionnaire components: homeschooled children and children who did not have either a complete child assessment score or parent interview (per the definition of complete provided in Exhibit 5-1) for the spring second-grade collection. As with the fall response rates, the parent and teacher rates are computed at the student level, meaning they indicate the percentages of students for whom a parent interview was completed or for whom a teacher questionnaire was received. The school administrator rate is also computed at the student level and indicates the percentage of students whose school administrator completed a questionnaire. There were two versions of the administrator questionnaire but response rates are not calculated separately for each version since a student would only have data for one version. Table 5-7 presents weighted and unweighted response rates for the child assessment and the parent interview in the spring second-grade data collection, by selected school characteristics. Researchers should note that the \"unknown/homeschool group\" has a low response rate, in large part because this group includes unlocatable cases who are, by default, nonrespondents. This unknown/homeschool group (approximately 1,430 cases) is about 8 percent of the overall sample of cases. Because their school characteristics are unknown, cases in this group cannot be included in a specific school characteristics category. This may have an impact on the calculation of the response rates by school characteristics that should be considered. Specifically, including these unlocatable cases in a separate category likely results in response rates by different school characteristics being higher than they would be if the unlocatable cases were included as nonrespodents when calculating response rates for the different school characteristic categories. The lowest response rates were for students in non-Catholic private schools (86.5 percent) while"}, {"section_title": "5-16", "text": "in all other subgroups, it is at least 90 percent. This is true for all response rates presented in tables 5-7 to 5-10.  1 Student had scoreable reading or mathematics or science data, or at least one executive function score, or a height or weight measurement. 2 Parent answered all applicable items in the family structure section of the questionnaire (FSQ) through item FSQ200 on current marital status. 3 School characteristics were taken from the spring second-grade school administrator questionnaire (SAQ) when available. When spring secondgrade SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). The versions of the school characteristics variables used to produce this table were specially derived for the User's Manual and are not included in the data file. 4 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total. 5   (62.7 percent). 1 Student had scoreable reading or mathematics or science data, or at least one executive function score, or a height or weight measurement. 2 Parent answered all applicable items in the family structure section of the questionnaire (FSQ) through item FSQ200 on current marital status. 3 Sample sizes have been rounded to the nearest 10. Therefore, detail may not sum to total. NOTE: The weighted response rates were calculated using the spring second-grade student base weight. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2013. students in Catholic schools (97.1 percent). Among categories other than \"Unknown,\" which has a low response rate in part due to the definition of the category, the lowest rates were found for students in schools with at least 86 percent of students who were racial/ethnic minorities (84.8 percent), in the West (87.1 percent), and in cities (86.7 percent). For the student-level teacher questionnaires, the weighted response rate was 86.8 percent. The response rates by subgroup are similar to the rates observed for the teacher-level questionnaire. 1 School characteristics were taken from the spring second-grade school administrator questionnaire (SAQ) when available. When spring second-grade SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). The versions of the school characteristics variables used to produce this table were specially derived for the User's Manual and are not included in the data file. 2 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total. 3 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. NOTE: A respondent is defined as a child for whom a teacher questionnaire was returned and the questionnaire had at least one response. The weighted response rates were calculated using the spring second-grade student base weight. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2013. Table 5-10 presents weighted and unweighted response rates for the teacher questionnaires in the spring second-grade data collection, by selected student characteristics. For the teacher-level questionnaire, the highest subgroup rates were observed for children born in 2003 (89.2 percent), those born in 2004 (88.1 percent), and White students (87.9 percent). The subgroups with the lowest rates were students born in 2006 (65.7 percent) and Asian students (77.0 percent), though the sample size for the former group was very small. Response rates by subgroup for the student-level teacher questionnaire show similar patterns as those for the teacher-level questionnaire.  132 became ineligible because all ECLS-K:2011 students in the school had moved to other schools. The school response rates used in the overall rates are from the base year because children were sampled in the base year and are eligible to stay in the study regardless of school participation after the base year. The overall weighted response rate is the product of the base-year before-substitution school response rate for all schools (62.7 percent) and the spring second-grade weighted child assessment response rate. The overall unweighted response rate is the product of the unweighted base-year before-substitution response rate for all schools (61.3 percent) and the spring second-grade unweighted child assessment response rate. In the overall response rate tables, the response rates by characteristic are also a product of the spring second-grade response rate by the corresponding (weighted or unweighted) overall base-year rate."}, {"section_title": "5-18", "text": ""}, {"section_title": "5-21", "text": "The overall response rate for the spring child assessment was 52.3 percent. For the parent interview, the overall weighted response rate for the spring data collection was 46.6 percent. Because the driving factor of the overall response rate is the base-year school response rate for all schools, the pattern of overall response rates by subgroups is the same as the pattern of spring second-grade response rates.  1 Student had scoreable reading and/or mathematics and/or science data, or executive function scores, or student had height and/or weight measurement. 2 Parent answered all applicable items in the family structure section of the questionnaire (FSQ) through item FSQ200 on current marital status. 3 School characteristics were taken from the spring second-grade school administrator questionnaire (SAQ) when available. When spring second-grade SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). The versions of the school characteristics variables used to produce this table were specially derived for the User's Manual and are not included in the data file. 4 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total. 5 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. NOTE: The weighted overall response rates were calculated using the school base weight for the school response rate component and the student base weight for the student response rate component. The counts of students by subgroups do not sum to the total because homeschooled students and students with unknown school characteristics are not included in this  Table 5-12 presents weighted and unweighted overall response rates for the general classroom teacher questionnaires in the spring second-grade data collection, by selected school characteristics. The overall response rate for the teacher-level questionnaire was 54 percent. The overall response rate for the student-level teacher questionnaire was 54.4 percent. The response rates by subgroup follow the same pattern as for the spring second-grade teacher response rates. 1 School characteristics were taken from the spring second-grade school administrator questionnaire (SAQ) when available. When spring second-grade SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). The versions of the school characteristics variables used to produce this table were specially derived for the User's Manual and are not included in the data file. 2 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total. 3 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. NOTE: A respondent is defined as a child for whom a teacher questionnaire was returned and the questionnaire had at least one response. The weighted overall response rates were calculated using the school base weight for the school response rate component and the spring second-grade student base weight for the student response rate component. The counts of students by subgroups do not sum to the total because homeschooled students and students with unknown school characteristics are not included in this  Table 5-13 presents the response rates for the two special education teacher questionnaires."}, {"section_title": "5-24", "text": ""}, {"section_title": "5-25", "text": "Response rates are not presented by subgroup for the special education teacher questionnaires because of the relatively small number of students eligible for this component. The denominator for the special education teacher rates is 1,093. This denominator excludes children who did not have either a complete child assessment score or parent interview for the spring second-grade collection, even if they had special education teacher data. The two special education teacher questionnaires had similar response rates. Tables 5-14 and 5-15 present response rates for the school administrator questionnaire (SAQ) included in the spring second-grade data collection. In the base year, the school sample was representative of schools educating kindergartners and kindergarten-aged children, so the base-year User's Manual presented response rates at the school level. After the base year, the school sample is the set of schools attended by children in the ECLS-K:2011 and is no longer a nationally representative sample of schools. For this reason, response rates for the SAQ are presented only at the student level. Table 5-14 presents the weighted and unweighted response rates for the school administrator questionnaire, by selected school characteristics. They are rates for students who were not homeschooled and are respondents in the spring second-grade data collection. 6 The weighted response rate for the school administrator questionnaire was 90.1 percent. The highest response rates for this questionnaire were for students in schools with zero to 15 percent of students who were racial/ethnic minorities (95.3 percent) and in towns (95.0 percent). Aside from the \"Unknown\" categories, which had very low response rates due to their composition, the lowest response rates were for students in schools with at least 86 percent of students who were racial/ethnic minorities (85.0 percent) and in the Northeast (86.7 percent). In this table, 6 A spring second-grade respondent has child data (scoreable reading or mathematics or science data, or at least one executive function score, or a height or weight measurement, or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from the spring second-grade round of data collection."}, {"section_title": "5-26", "text": "the \"unknown\" school characteristics are for a small number of students with SAQ data, but the locale, school size and minority enrollment data are missing.  1 School characteristics were taken from the spring second-grade school administrator questionnaire (SAQ) when available. When spring second-grade SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). The versions of the school characteristics variables used to produce this table were specially derived for the User's Manual and are not included in the data file. 2 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total. 3 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. NOTE: A respondent is defined as an eligible student for whom the school was eligible for the school administrator questionnaire, the questionnaire was returned, and there was at least one response. The weighted response rates were calculated using the spring second-grade student base weight. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2013. Table 5-15 presents the weighted and unweighted response rates for the school administrator questionnaire, by selected student characteristics. The highest weighted response rate for the school administrator questionnaire was for White students (92.3 percent). Excluding subgroups with small numbers of sampled students, the lowest response rates were for, Black (86.3 percent) students and Hispanic students (87.6 percent). 1 Sample sizes have been rounded to the nearest 10. Therefore, detail may not sum to total. NOTE: A respondent is defined as an eligible student for whom the school was eligible for the school administrator questionnaire, the questionnaire was returned, and there was at least one response. The weighted response rates were calculated using the spring second-grade student base weight. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2013. Table 5-16 shows the overall response rates for the school administrator questionnaire. The overall weighted response rate was 56.5 percent. As with other overall response rates, the overall rates by subgroups have the same pattern as the spring second-grade response rates because the base-year school response rate is for all schools and, thus, the same for all subgroups. 1 School characteristics were taken from the spring second-grade school administrator questionnaire (SAQ) when available. When spring second-grade SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). The versions of the school characteristics variables used to produce this table were specially derived for the User's Manual and are not included in the data file. 2 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total. 3 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. NOTE: A respondent is defined as an eligible student for whom the school was eligible for the school administrator questionnaire, the questionnaire was returned, and there was at least one response. The weighted overall response rates were calculated using the school base weight for the school response rate component and the spring second-grade student base weight for the student response rate component. The counts of students by subgroups do not sum to the total because students with unknown school characteristics are not included in this "}, {"section_title": "5-28", "text": ""}, {"section_title": "5-29", "text": ""}, {"section_title": "5-30", "text": "Tables 5-17 through 5-19 present response rates that reflect response across the fall and spring second-grade collections combined. These rates are referred to as longitudinal response rates. Response rates are for cases with a response for a given component in both the fall and the spring. The denominators for the unweighted response rates in these tables include students who were part of the fall second-grade subsample who remained eligible in the spring of second grade. The weight used to compute estimates for tables 5-17 through [5][6][7][8][9][10][11][12][13][14][15][16][17][18][19] showing longitudinal response rates is the fall secondgrade student base weight, which includes adjustments for fall and mover subsampling but not for unknown eligibility and nonresponse. Variables for school and child characteristics are derived in the same manner as those presented in tables 5-1 through 5-4, because these tables present information on children in the fall second-grade sample who have data in both the fall and spring rounds. Table 5-17 presents the weighted and unweighted response rates for students who completed a child assessment in both the fall and spring second-grade data collections, and for students who have a complete parent interview in both the fall and spring second-grade data collections, by selected school characteristics. The denominator used to calculate the unweighted child assessment longitudinal response rate is 5,373. The denominator used to calculate the unweighted parent interview longitudinal response rate is 5,402. The weighted response rate for students with assessments in both fall and spring is 82.3 percent. The highest response rates were for students in schools with less than 16 percent of students who were racial/ethnic minorities (95.8 percent) and in schools with at least 86 percent of students who were racial/ethnic minorities (95.4 percent). With the exception of the \"Unknown\" categories, the lowest rate was found among students in non-Catholic private schools (66.1 percent). The weighted response rate for students whose parent completed an interview in both the fall and the spring was 73.3 percent. The highest rates were found for students in schools with zero to 15 percent of their students in a racial/ethnic minority group (87.5 percent) and for students in Catholic schools (86.4 percent). The lowest rates were for students in non-Catholic private schools (65.8 percent), in schools in cities (73.3 percent), and in schools with between 46 and 85 percent of students who were racial/ethnic minorities (73.4 percent).  1 Student had scoreable reading or mathematics or science data, or at least one executive function score, or a height or weight measurement, in both fall and spring second grade. 2 Parent answered all applicable items in the time use section of the questionnaire (TUQ) in fall, and the family structure questions (FSQ) in spring through item FSQ200 on current marital status. 3 Because the school administrator questionnaire (SAQ) was not administered in the fall of second grade, school characteristics were taken from the spring second-grade SAQ responses for fall second-grade students who attended the same school in both rounds, where available. When spring second-grade SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). The versions of the school characteristics variables used to produce this table were specially derived for the User's Manual and are not included in the data file. 4 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total. 5 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. NOTE: The fall second-grade data collection included only 30 percent of the sampled study PSUs. The weighted response rates were calculated using the fall second-grade student base weight. The school characteristics are the same as for the fall second-grade tables. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2012 and spring 2013. response rates were for students born in 2006 (38.6 percent) and for Black students (59.9 percent). Again, the sample size for the group of students born in 2006 is very small. 2 Parent answered all applicable items in the time use section of the questionnaire (TUQ) in fall, and the family structure questions (FSQ) in spring through item FSQ200 on current marital status. 3 Sample sizes have been rounded to the nearest 10. Therefore, detail may not sum to total. NOTE: The fall second-grade data collection included only 30 percent of the sampled study PSUs. The weighted response rates were calculated using the fall second-grade student base weight. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2012 and spring 2013. Table 5-19 presents overall weighted and unweighted response rates for students who completed a child assessment in both the fall and spring second-grade data collections, and for students who have a complete parent interview in both the fall and spring second-grade data collections, by selected school characteristics. The overall weighted response rate for students with assessments in both fall and spring was 51.6 percent. The overall weighted response rate for students with a complete parent interview in both the fall and spring was 46 percent.  2 Parent answered all applicable items in the time use section of the questionnaire (TUQ) in fall, and the family structure questions (FSQ) in spring through item FSQ200 on current marital status. 3 Because the school administrator questionnaire (SAQ) was not administered in the fall of second grade, school characteristics were taken from the spring second-grade SAQ responses for fall second-grade students who attended the same school in both rounds, where available. When spring second-grade SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). The versions of the school characteristics variables used to produce this table were specially derived for the User's Manual and are not included in the data file. 4 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total. 5 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. NOTE: The fall second-grade data collection included only 30 percent of the sampled study PSUs. The weighted overall response rate was calculated using the school base weight for the school response rate component and the student base weight for the student response rate component. The counts of students by subgroups do not sum to the total because homeschooled students and students with unknown school characteristics are not included in this 5-36"}, {"section_title": "5-31", "text": ""}, {"section_title": "5-34", "text": ""}, {"section_title": "Nonresponse Bias Analysis", "text": "NCES statistical standards require that any survey instrument with a weighted unit response rate less than 85 percent be evaluated for potential nonresponse bias. For the second-grade rounds of data collection, four study components had weighted response rates lower than 85 percent: the fall child assessment (84 percent, weighted, and 87.9 percent, unweighted), the spring child assessment (83.4 percent, weighted, and 87.3 percent, unweighted), the fall parent interview (80. 5 percent, weighted, and 80.7 percent, unweighted), and the spring parent interview (74.3 percent, weighted, and 75.9 percent, unweighted). The effect of nonresponse is examined in two ways. Sections 5.3.1 and 5.3.2 discuss the effect of nonresponse on estimates produced from the child assessment data and the parent interview data, respectively. Section 5.3.3 compares estimates of selected base-year characteristics between base-year respondents and spring second-grade respondents. 7 A comparison of the study estimate to frame estimates cannot be done because the sample of students is no longer representative of second-graders. After the base year, students in the ECLS-K:2011 can only be referred to as the cohort of children who attended kindergarten or of kindergarten age in the 2010-11 school year. For a comparison to frame estimates, see chapter 5 of the base-year User's Manual."}, {"section_title": "Effect of Nonresponse on Child Assessment Data", "text": "Estimates weighted by the nonresponse-adjusted weights are compared with estimates weighted by the base weights (which are referred to as unadjusted estimates). Large differences between the estimates weighted by the nonresponse-adjusted weights and the unadjusted weights may indicate the potential for bias in the unadjusted estimates. If the differences are small, then either there is very small bias in the estimates, or the characteristics used in the adjustment process are not related to the survey estimates and therefore the adjustments do not introduce changes in the estimates. The unadjusted base weight only takes into account the selection probabilities of the sampling units and the subsampling of movers to be followed. The nonresponse-adjusted weights are the weights used to analyze ECLS-K:2011 data. The nonresponse adjusted weights used in this analysis are W6CF6P_2A0, which is adjusted for nonresponse to the fall child assessment, and W6CS6P_20, which is 7 A base-year respondent has child data (scoreable assessment data or height or weight measurements or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. A spring second-grade respondent has child data (scoreable assessment data or height or weight measurements or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from the spring second-grade round of data collection."}, {"section_title": "5-37", "text": "adjusted for nonresponse to the spring child assessment. For a discussion of how the weights were constructed, see chapter 4. In the ECLS-K:2011, chi-square analyses were used to identify characteristics that are most related to nonresponse, and these characteristics were used in the adjustment. Therefore, the likelihood that the weighted estimates are biased as a result of nonresponse would be lower than if nonresponse adjustment was not implemented. This method of examining nonresponse bias provides an indication of the degree to which nonresponse adjustments are needed and how effective the adjustments are. Table 5-20 shows estimates of selected items from the fall child assessment. Table 5-21 shows the differences between unweighted and weighted estimates, and between estimates produced using base weights (unadjusted estimates) and estimates produced using nonresponse adjusted weights. The differences are shown in absolute value and as a percent (relative difference), together with their p value (\u03b1 = 0.05). For example, for the differences between unweighted and unadjusted estimates, the difference is the absolute value of the unweighted estimate minus the unadjusted estimate, and the percent is the difference divided by the unweighted estimate. A p value of less than 0.05 means that there is a statistically significant difference between the two estimates. Tables 5-22 "}, {"section_title": "5-38", "text": "In terms of interpreting percent difference (relative difference), it should be noted that percent difference is sensitive to sample size but also to the prevalence of a particular characteristic. Large relative differences can be a function of small sample sizes. For example, as seen in table 5-23 for students who attended school in a town, there is an absolute difference in the nonresponse-adjusted and unadjusted estimates of 0.44 and a relative difference of 4.14. For students who attended school in the South, there is an absolute difference in the nonresponse-adjusted and unadjusted estimates of 0.47 and a relative difference of 1.26. Proportionately there are fewer students who attended school in a town than students attended school in the South; therefore, the relative difference is higher for students who went to school in a town even though the absolute difference is somewhat similar. The differences found in the analyses show that there is some potential for nonresponse bias in the unweighted assessment data, but the weights used to produce estimates were adjusted for nonresponse and, thus, reduce that potential for bias. 1 Unadjusted estimates are produced using the student base weight. 2 Adjusted estimates are produced using weight W6CF6P_2A0. 3 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. Sample sizes rounded to the nearest 10. NOTE: SE = standard error. The sample sizes are the number of cases with a nonzero fall second-grade base weight and a nonmissing value for the characteristic or group of characteristics. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2012. 1 Unadjusted estimates are produced using the student base weight. 2 Adjusted estimates are produced using weight W6CF6P_2A0. 3 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. Sample sizes rounded to the nearest 10. NOTE: SE = standard error. The sample sizes are the number of cases with a nonzero fall second-grade base weight and a nonmissing value for the characteristic or group of characteristics. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2012. 1 Unadjusted estimates are produced using the student base weight. 2 Adjusted estimates are produced using weight W6CS6P_20. 3 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. Sample sizes rounded to the nearest 10. NOTE: The sample sizes are the number of cases with a nonzero spring second-grade base weight and a nonmissing value for the characteristic or group of characteristics. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2012."}, {"section_title": "5-39", "text": ""}, {"section_title": "5-40", "text": ""}, {"section_title": "5-42", "text": ""}, {"section_title": "5-45", "text": ""}, {"section_title": "Effect of Nonresponse on Parent Interview Data", "text": "The adjusted weights used in this analysis are W6CF6P_2A0, adjusted for nonresponse to the fall child assessment, 8 and W6CS6P_6A0, adjusted for nonresponse to the spring parent interview. For a discussion of how the weights were constructed, see chapter 4. Table 5-24 shows estimates of selected items from the fall parent interview. Table 5-25 shows the differences between unweighted and weighted estimates, and between estimates produced using base weights (unadjusted estimates) and estimates produced using nonresponse adjusted weights. Tables 5-26  In spring, the rage of absolute differences is 0 to 4.2, and the average is 1.1. Again, because of the smaller fall sample, in general the relative differences are larger in fall than in spring. The discussion of how to interpret the relative difference applies to the parent interview data as well. It was noted before that the percent difference is sensitive to sample size but also to the prevalence of a particular characteristic. The example is from table 5-27. The percent of students who used non-English at home is 17.6; the absolute difference in the nonresponse-adjusted and unadjusted estimate is 0.68 and the relative difference is 4.66. The percent of students with at least one parent working more than 35 hours a week is 89.5 with an absolute difference of 0.87 and a relative difference of 0.98. The relative difference is smaller for the groups of students with the higher prevalence in the characteristic examined. As with the assessment data, the weights used to produce estimates from the parent interview data were adjusted for nonresponse and helped to reduce the potential for bias. The differences found in the analyses show that there is some potential for nonresponse bias in the unweighted assessment data, but the weights used to produce estimates were adjusted for nonresponse and, thus, reduce that potential for bias. 8 Weight W6CF6P_2A0 is used instead of weight W5CF5PF_50 because weight W5CF5PF_50 is adjusted for nonresponse to the fall secondgrade parent interview but also nonresponse to other components such as nonresponse to both child assessment and parent interview in fall first grade, thus more restrictive and yielding a much smaller sample size for analysis. 1 Unadjusted estimates are produced using the student base weight. 2 Adjusted estimates are produced using weight W6CF6P_2A0. 3 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. Sample sizes rounded to the nearest 10. NOTE: The sample sizes are the number of cases with a nonzero fall second-grade base weight and a nonmissing value for the characteristic or group of characteristics. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2012. "}, {"section_title": "5-46", "text": ""}, {"section_title": "5-53", "text": ""}, {"section_title": "Effect of Nonresponse on Characteristics from the Base Year", "text": "In this section, the effect of nonresponse is explored by comparing estimates of selected base-year characteristics between base-year respondents and spring second-grade respondents. 9 The estimates are unadjusted estimates (i.e., they are weighted by the base weights). Base-year estimates are weighted by the base-year base weight that takes into account only the selection probabilities of the sampling units. Spring second-grade estimates are weighted by the spring-second grade base weight that takes into account the selection probabilities and the subsampling of movers to be followed. Native/Pacific Islander and the American Indian/Alaska Native groups with very small sample sizes). Since race/ethnicity is one of the characteristics used to construct nonresponse cells for nonresponse adjustments, any potential bias would be reduced in estimates produced using weights adjusted for nonresponse. 9 A base-year respondent has child data (scoreable assessment data or height or weight measurements or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. A spring second-grade respondent has child data (scoreable assessment data or height or weight measurements or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from the spring second-grade round of data collection. 1 Unadjusted estimates are produced using the kindergarten base weight for kindergarten and the second-grade base weight for second grade. 2 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. Sample sizes rounded to the nearest 10. NOTE: The sample sizes for kindergarten are the number of base-year respondents with a nonmissing value for the kindergarten characteristic or group of characteristics. The sample sizes for second grade are the number of spring second-grade respondents with a nonmissing value for the kindergarten characteristic or group of characteristics. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2011 and spring 2013.   1 Adjusted estimates are produced using weight W1_2P0 for kindergarten and weight W6CS6P_6A0 for second grade. 2 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. Sample sizes rounded to the nearest 10. NOTE: The sample sizes for kindergarten are the number of cases with a nonzero kindergarten final weight and a nonmissing value for the kindergarten characteristic or group of characteristics. The sample sizes for second grade are the number of cases with a nonzero spring second-grade final weight and a nonmissing value for the kindergarten characteristic or group of characteristics. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2011 and spring 2013."}, {"section_title": "5-55", "text": ""}, {"section_title": "5-56", "text": "This page intentionally left blank. 6-1"}, {"section_title": "DATA PREPARATION", "text": "In the second-grade rounds, two types of data collection instruments were again used for the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011): computer-assisted interviews and assessments (CAI) and self-administered paper forms (hard-copy questionnaires). As in kindergarten (i.e., the base year) and first grade, once data were collected, they were reviewed and prepared for release to analysts. The approaches used to prepare the data differed with the mode of data collection. The direct child assessments and parent interviews were conducted using CAI. Editing specifications were built into the CAI programs used by assessors or interviewers collecting these data. The teacher and school administrator hard-copy questionnaires were self-administered. When these hardcopy questionnaires were returned to the data collector's home office, staff recorded the receipt of these forms into a project-specific form tracking system. Data from the hard-copy questionnaires were then captured by scanning the completed forms. Before scanning, coders reviewed the questionnaires to ensure that responses were legible and had been written in appropriate response fields for transfer into an electronic format. After the data were scanned and reviewed for range and logical consistency, coding of open-ended 1 \"other, specify\" text responses into existing or new categories was implemented. The following sections briefly describe the data preparation activities for both modes of data collection, focusing on the second-grade activities. More detailed information on these data preparation activities can be found in the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), User's Manual for the ECLS-K:2011 Kindergarten Data File andElectronic Codebook, Public Version (NCES 2015-074) (Tourangeau et al. 2015), hereinafter referred to as the base-year User's Manual."}, {"section_title": "Coding Text Responses", "text": "Additional coding was required for some of the items asked in the CAI parent interview once the data had been collected. These items included \"other, specify\" text responses and responses to questions asking about parent or guardian occupation, which interviewers had entered into the CAI system verbatim. Review of \"other, specify\" items. As in kindergarten and first grade, for second grade trained data preparation staff reviewed respondents' verbatim \"other, specify\" text responses. There were a small number of items in the parent interview for which additional categories were added to categorize \"other, specify\" text responses that occurred with sufficient frequency. For example, a sufficient number of parents provided an \"other, specify\" response to the question about how far the school is from home, reporting that the child was homeschooled. A new response category was added to classify these responses. Text responses that did not fit into any preexisting category and were not common enough to be coded into new categories were left coded as \"other\" in the data. New categories added as a result of this review of \"other, specify\" responses are noted as such in appendix A. There were no \"other, specify\" items in the child assessments. Text responses that could not be coded using the autocoding system were coded manually using a customized computer program designed for coding occupations. The customized coding computer program provided a text string with occupation information to coders, who then determined and assigned the most appropriate occupation code by reviewing occupation text descriptions in the coding manuals. In addition to the occupation text strings, the coders used other information collected from respondents, such as main duties at work, industry, and name of the employer, to ensure that the occupation code assigned to each case was appropriate. Over half the occupations (52.7 percent) were manually coded for second grade. Every manually coded occupation text response was coded at least twice. Two coders assigned codes independently, without knowledge of each other's codes (i.e., using a double-blind coding process). A coding supervisor adjudicated all reported occupations for which the codes assigned independently by each coder differed. Of all the occupations that were assigned a code for second grade, 22.3 percent (1,434) required adjudication, either because the autocode and manually assigned code differed (for the autocoded occupations) or because the two manually assigned codes differed (for the manually coded occupations). Of the 3,043 reported occupations that were autocoded, 453 occupations (14.9 percent) required adjudication because the coder disagreed with the autocoding. Of the 3,392 reported occupations that were manually coded, 981 (28.9 percent) required adjudication because the two human coders disagreed. Following the adjudication process, the coding supervisor conducted a review of all occupation codes that were assigned manually. There were an additional 110 manually coded occupations (1.7 percent of all codes) for which the two coders assigned the same code but the supervisor disagreed with the original manually assigned code and assigned a new occupation code. For second grade, most of the assigned coding staff were experienced, having coded occupations for the ECLS-K:2011 in the base year, first grade, or both. The occupation coding supervisor also was experienced with the NCES coding scheme and had been involved in the project's occupation coding activity since the base year. In instances in which the two coders manually assigned the same code, but the supervisor disagreed with the code, the case was subject to additional examination, and together the supervisor and coders considered the merits of the proposed code before a final code was assigned."}, {"section_title": "Household Roster Review", "text": "The fall second-grade parent interview was much shorter than the parent interview fielded in the spring data collection round and did not include a household roster in which information on household composition was collected. Therefore no household roster review was required for the fall data collection. The spring second-grade parent interview did include a household roster. Following protocols established during the previous rounds, three general types of checks were run on the spring household roster information to identify missing or inaccurate information that would require editing. \uf06e First, the relationship of an individual living in the household to the study child was compared to the individual's listed age and sex. Inconsistencies, such as a male mother, and unusual combinations of characteristics, such as a biological mother over age 65, were examined further. Information was corrected when the interview contained sufficient information to support a change. \uf06e Second, while it is possible to have more than one mother or more than one father in a household, households with more than one mother or more than one father were reviewed to ensure they were not cases of data entry error. Corrections were made whenever clear errors were identified and a clear resolution existed. \uf06e Third, the relationships of an individual in the household to both the study child and the respondent were examined, as there were cases in which the relationship of an individual to the study child conflicted with his or her status as the spouse/partner of the respondent. For example, in a household containing a child's grandparents but not his or her parents, the grandmother may be designated the \"mother\" figure, and the grandfather thus becomes the \"father\" figure for the purposes of some questions in the interview by virtue of his marriage to the grandmother. In this example, these cases would have been examined but left unchanged. Both the original-and correct (grandfather)-relationship data and the new \"parent-figure\" designation (father) that had been constructed were retained. In other situations, discrepancies in the reported relationships indicated an error, and the data were edited. For example, in a household containing two mothers, if a review of the audio recording from the interview indicated the relationship of the second mother was documented incorrectly by the interviewer-that the second female identified as a mother was not actually a mother to the focal child-the relationship of the second female would have been edited (corrected) to something other than mother."}, {"section_title": "6-5", "text": "A flag on the data file (X6EDIT) identifies cases that were reviewed or edited for any of the reasons described above; the flag is set to 1 if the case was identified for review for any of these household roster checks. Note that a code of 1 does not necessarily indicate that the data were changed; if the data were reviewed and found to be as reported by the respondent or there was no clear error to be fixed, the reviewed data were left as is. There were 1,100 cases (9 percent) identified for review of the household roster from the spring of second grade."}, {"section_title": "Partially Complete Parent Interviews", "text": "Parents did not have to complete an entire interview for the data collected from them to be included on the data file. However, parent interviews did have to be completed through a specified section of the interview for those data to be included. For the shorter parent interview in the fall second-grade round, the respondent had to answer all applicable questions in the section on time use (TUQ) for the parent interview data to be included on the data file. There were nine partially completed fall parent interviews for which the respondent answered all applicable questions in the TUQ section but did not finish the entire interview. For the spring second-grade round, the respondent had to answer all applicable questions through the majority of the section on family structure (FSQ). There were 542 partially completed spring parent interviews for which the respondent answered applicable questions in the FSQ section but did not complete the entire interview. 2 All data derived from questions asked after the interview termination point for these partially completed interviews are set to -9 for \"not ascertained.\""}, {"section_title": "6.2", "text": "Receipt, Coding, and Editing of Hard-Copy Questionnaires"}, {"section_title": "Receipt Control", "text": "Receipt control was managed in the same manner for second grade as it had been in the earlier rounds of the ECLS-K:2011. Refer to the base-year User's Manual for details."}, {"section_title": "Scanning of Hard-Copy Questionnaires", "text": "Scanning of hard-copy questionnaires was managed in the same manner for second grade as it had been in the earlier rounds of the ECLS-K:2011. Refer to the base-year User's Manual for details."}, {"section_title": "Coding for Hard-Copy Questionnaires", "text": "Similar to the process described for the parent interview and identical to procedures used in earlier rounds, \"other, specify\" text responses were reviewed by the data editing staff and coded at the instrument level. In the second-grade rounds, there were no items in the hard-copy questionnaires that required new categories to be added due to \"other, specify\" text responses that occurred with sufficient frequency. However, in some instances categories that had been added during the coding process in earlier rounds that did not appear on the questionnaires themselves were again added during the coding process for second grade. For example, in first grade a sufficient number of teachers provided \"other, specify\" responses to the question about which languages other than English are spoken by one or more children in the class. Several new response categories were added to classify these responses, including African languages, Creole, French, German, Polish, Portuguese, Russian, and Hmong. Text responses that did not fit into any preexisting category and were not common enough to be coded into new categories were left coded as \"other\" in the data."}, {"section_title": "Data Editing", "text": "The data editing process for hard-copy questionnaires was managed in the same manner for second grade as it had been in the earlier rounds of the ECLS-K:2011. The base-year User's Manual has more detail related to editing. As part of the editing process in second grade as well as in earlier rounds of the ECLS-K:2011, skip patterns were enforced. In cases in which respondents did not follow the skip instructions and proceeded to answer the questions that were supposed to be skipped, responses for the inapplicable dependent questions generally were deleted and the data were set to -1, the inapplicable code. There are two check boxes (shown below), one on the school administrator questionnaire and one on the teacher-level teacher questionnaire, that were part of skip patterns. When respondents marked these check boxes, they were directed to skip ahead in the questionnaire because a subset of subsequent, dependent questions were not applicable to them. In some cases, it was clear to the data editors that the check box was marked in error by the respondent and the responses to the dependent questions were valid, usable data. In such cases, the check box was edited (corrected) in order to retain responses to dependent questions in the data. Consequently, data for the two check boxes listed may not reflect the actual responses provided by the person completing the questionnaire. This page intentionally left blank. 7-1"}, {"section_title": "DATA FILE CONTENT AND COMPOSITE VARIABLES", "text": "This chapter describes the contents of the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011) kindergarten through second-grade (K-2) restricted-use data file. The data are accessible through software called the electronic codebook (ECB). The ECB allows data users to view variable frequencies, tag variables for extraction, and create the SAS, SPSS for Windows, or Stata code needed to create an extract file for analysis. The child data file on the ECB is referred to as a \"child catalog.\" Instructions for installing the ECB are provided in chapter 8. A help file with further information about using the ECB is included on the DVD. The raw data are provided in ASCII data files: the restricted-use file is named childK2.dat and the public-use file is named childK2p.dat. To develop data files for statistical analyses, analysts should use the ECB software or the file record layout located in appendix B of the DVD. The ECB writes syntax files that must be run within a statistical software package to generate customized data files. Users should not access the ASCII data file directly, as any changes made to that file will alter the raw data obtained during data collection. This chapter focuses primarily on the composite variables that were created from information obtained during the second-grade data collections. Most of the variables have been computed in the same way as those that were created using information collected in the base year (i.e., kindergarten) and first grade. However, a small number of them differ slightly either because the same exact information available in the base year or first grade was not available in second grade or because it was determined there was a better way to compute the composite after release of a previous data file. These differences are noted in the descriptions of the variables. To the extent feasible, the composite variables have also been computed in the same way as those created for the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K). This results in consistency between the two studies and facilitates comparisons between the two cohorts. However, some composites were created differently in the ECLS-K:2011 than in the ECLS-K. Documentation for both studies should be consulted before conducting cross-cohort analyses using composites. The user's manuals for kindergarten and first grade should be consulted for detailed descriptions of the composite variables computed for rounds 1 through 4. The Combined User's Manual for the ECLS-K Eighth-Grade and K-8 Full Sample Data Files and Electronic Codebooks (NCES 2009-004) is available on the National Center for Education Statistics website (http://nces.ed.gov/pubsearch/pubsinfo.asp?pubid=2009004), as are the round-specific manuals for each round of ECLS-K data collection (http://nces.ed.gov/pubsearch/getpubcats.asp?sid=024). As discussed in Appendix B, the public-use file is derived from the restricted-use file and is identical in format. However, masking techniques such as re-categorization and top-and bottom-coding have been applied to some data to make them suitable for public release. As a result of masking, some variables in the public-use file may not contain the exact same categories and values described in this chapter. Please see Appendix B for information on which variables are modified in the public-use file and see the public-use codebook for the exact categories and values provided in the public data. The chapter is divided into several sections. Sections 7.1 through 7.4 describe variable naming conventions, identification variables, missing values, and data flags. Section 7.5 provides details about the creation of composite variables, and section 7.6 focuses on the methodological variables. Section 7.7 discusses variables that can be used to identify children who changed teachers between the fall and spring data collections. Finally, section 7.8 discusses variables related to summer school and vacation. 7-3"}, {"section_title": "Variable Naming Conventions", "text": "Variables are named according to the data source (e.g., parent interview, teacher questionnaire) and the data collection round to which they pertain. With the exception of the identification variables described in section 7.2, the first two or three characters of each variable (referred to as the variable prefix) include (1) a letter designating the source and (2) a number indicating the data collection round. For example, the number 5 is used for the data collection that took place in the fall of 2012 and the number 6 is used for the data collection that took place in the spring of 2013. Composite variables derived from data collected in both the fall and spring include both 5 and 6 in their names. These variable naming conventions are used consistently in the data file. The prefixes used for second-grade variables in the kindergarten-second grade data file are listed in exhibit 7-1. Some variable names end with a suffix denoting a particular feature of the variable of which users should be aware. The suffix \"_R\" indicates that the variable has been updated or revised since its release in a prior data file. The suffix \"2\" is used for composites that are based on new questions or have new categories. The suffix \"_I\" indicates that missing data for the variable have been imputed or, in the 7-4 case of a composite variable, that it is computed from imputed source variables. Imputation is discussed in sections 7.5.2.5, 7.5.2.6, and 7.5.4.6."}, {"section_title": "Identification Variables", "text": "The kindergarten through second-grade data file contains a child identification (ID) variable (CHILDID) that uniquely identifies each record. For children who have a twin who also participated in the study, TWIN_ID is the child identification number of the focal child's twin. The file also contains an ID for the parent (PARENTID). The parent ID number (PARENTID) is the same number as the child ID. Unlike in the ECLS-K, CHILDID is randomly generated, so it cannot be used to group children into classrooms or schools (that is, there is no commonality among IDs for children within the same school or classroom). The K-2 restricted-use data file does contain IDs for the child's general classroom teacher in each round, special education teacher (if applicable) in each round, school in each round, and before-and after-school care provider in the kindergarten year (if the child was in before-or after-school care with one provider at least 5 hours per week). Users who wish to conduct hierarchicallevel analyses with the school or classroom as additional levels can use these ID variables to group children within schools and classrooms. However, it should be noted that children change schools and classrooms over time, and this should be taken into account in any analysis of school or classroom effects. Additionally, as children change schools and classrooms over time, cluster sizes may become too small to support hierarchical analyses. The IDs available on the restricted-use file are listed in exhibit 7-2. Children's general classroom teachers in the 2012-13 school year are identified in the restricted-use file with the ID variables T5_ID, the fall 2012 teacher identification number, and T6_ID, the spring 2013 teacher identification number. Study children who share the same general classroom teacher with other study children have the same value for the general classroom teacher ID. If a child was part of the fall 2012 subsample and had the same teacher for the entire school year, T5_ID and T6_ID are identical. For children who had an Individualized Education Program (IEP) on record with the school that was identified as part of the process for determining accommodations for the child assessment, D6T_ID provides the identification number for their special education teacher or related service provider. For some students, the general classroom teacher was also the student's special education teacher. However, D6T_ID does not match T6_ID for these students. The ID variables S5_ID and S6_ID indicate the school the child attended at the time of the fall 2012 and spring 2013 data collections, respectively. As with the general classroom teacher ID variables, children attending the same school share the same school ID, and a child in the fall 2012 subsample who attended the same school for the entire school year has the same value for S5_ID and S6_ID. Each child has a school identification number for the two kindergarten data collections, the spring first-grade data collection, and the spring second-grade data collection. Children selected for the fall subsamples also have school identification numbers for the fall 2011 and fall 2012 data collections. Not all identification numbers represent specific schools. Instead, certain identification numbers have been designated to identify children who were homeschoolers (9100), moved to a nonsampled county (9997), were unlocatable (9995), moved outside the United States (9993), were movers who were not subsampled to be followed into their new schools (9998), were deceased (9994), or whose parents asked for them to be removed from the study (9999). If a child did not have an IEP on record with the school that was identified as part of the process for determining accommodations for the child assessment, there is no special education teacher or related services provider associated with that child, and D6T_ID is missing. The D6T_ID would also be missing if the school records indicated that a child had an Individualized Family Service Plan (IFSP) when he or she was younger, but did not have an IEP at the time of data collection. In most cases, if a child had an IEP identified as part of the process for determining accommodations for the child assessment and, therefore, a special education teacher associated with him or her, there is an ID provided in D6T_ID whether or not the special education teacher responded to the spring 2013 special education teacher questionnaires. For both general classroom and special education teachers, there could be missing data for the child's teacher-level or child-level questionnaire (for example, if the general or special education teacher replied to only one of the two teacher questionnaires for the general or special education teacher, respectively, or did not fully complete the questionnaires) even though there is an assigned teacher ID. It is left to users to determine how they would like to set \"not applicable\" versus \"not ascertained\" codes when data for T5_ID, T6_ID, or D6T_ID are missing. Note that if a teacher did not complete a teacherlevel questionnaire, completed a child-level questionnaire for one child, and did not complete another child-level questionnaire for a child to whom the teacher was also linked, both children would have the same teacher identification number (e.g., T6_ID for the general classroom teacher or D6T_ID for the special education teacher). However, only the child for whom the teacher completed the child-level questionnaire would have data for those variables. 7-7"}, {"section_title": "Missing Values", "text": "Variables on the ECLS-K:2011 data file use a standard scheme for identifying missing data. Missing value codes are used to indicate item nonresponse (when a question is not answered within an otherwise completed interview or questionnaire), legitimate skips (when a question was not asked or skipped because it did not pertain to the respondent), and unit nonresponse (when a respondent did not complete any portion of an interview or questionnaire) (see exhibit 7-3). Exhibit 7-3. Missing value codes used in the ECLS-K:2011 data file Not applicable, including legitimate skips -2 Data suppressed (public-use data file only) -4 Data suppressed due to administration error -5 Item not asked in school administrator questionnaire form B (SAQ-B) -7 Refused (a type of item nonresponse) -8 Don't know (a type of item nonresponse) -9 Not ascertained (a type of item nonresponse) (blank) System missing (unit nonresponse) SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K: 2011), kindergarten-second grade (K-2) data file. The -1 (not applicable) code is used to indicate that a respondent did not answer a question due to skip instructions within the instrument. In the parent interview, \"not applicable\" is coded for questions that were not asked of the respondent because a previous answer made the question inapplicable to the particular respondent. For example, a question about a child's sibling's age is not asked when the respondent has indicated that the child has no siblings. For the teacher and school administrator selfadministered instruments, \"not applicable\" is coded for questions that the respondent left blank because the written directions instructed him or her to skip the question due to a certain response on a previous question that made the question inapplicable to the particular respondent. One example of the use of \"not applicable\" is found in the spring 2013 school administrator questionnaire version A (SAQ-A) questions F2 and F3a-h. Question F1 asks whether the school received Title I funds for this school year. If the answer to question F1 is \"yes,\" the respondent is directed to continue to question F2 asking if the school was operating a Title I targeted assistance or schoolwide program, and then to F3a-h, which ask how the Title I funds are used. If the answer to question F1 is \"no,\" the respondent is supposed to skip to question F4 and questions F2 and F3a-h would be coded as -1 (not applicable). If questions F1, F2, and F3a-h are left blank by the respondent, and the respondent did not indicate that it is a private school (S6PRVSCH = 0), data for these questions are coded -9 (not ascertained), meaning the questions should have been 7-8 answered but were not. If the respondent indicated that the school is private (S6PRVSCH = 1) and questions F1, F2, and F3a-h are left blank, data for these questions are coded -1 (not applicable) because they were supposed to be left blank given the school's designation as private. There are some exceptions to the standard use of -1 to indicate data are inapplicable for specific cases. For questions about the hours and minutes that the child spends watching television and playing video games, the questions about the number of minutes (P5TVMIN, P6TVMIN, P5VIDMIN,   P6VIDMIN) can be entered by interviewers as \"0\" or skipped if there are no minutes. If the questions about the number of minutes are skipped, they are coded -1 (not applicable). Another exception to the standard used of -1 is that for several round 5 and round 6 variables (X5RTHETK2, X6RTHETK2, X5MTHETK2, X6MTHETK2, X5STHETK2, X6STHETK2), -1 is a valid value and should not be identified as missing data. In order to protect the confidentiality of study participants, some data are suppressed in the public-use data file. The code -2 indicates the suppression of data for confidentiality. The suppression code -4 is used in rare instances in which there was a problem in the administration of an item that led to a high proportion of cases having missing or flawed data on the affected item, such that the data that were collected were not useful. Although the administration error typically did not affect all cases, the -4 missing data code is assigned to all cases, including those not specifically affected by the error. Information about a number of school characteristics that was collected in the SAQ-A (the school administrator questionnaire given to schools that were new to the study or had not previously completed an SAQ) was not collected in the SAQ-B (the school administrator questionnaire given to schools that had previously completed an SAQ). This data collection approach reduced respondent burden by eliminating questions about school characteristics that were unlikely to change from year to year, such as public/private control and the grade levels taught at the school. The code -5 is a special \"not applicable\" code indicating that a child does not have a value for the given school characteristic variable because it was not included in the abbreviated SAQ-B. The -7 (refused) code indicates that the respondent specifically told the interviewer that he or she would not answer the question. This, along with the -8 (don't know) code and the -9 (not ascertained) code, indicate item nonresponse. The -7 (refused) code is not used in the school or teacher data."}, {"section_title": "7-9", "text": "The -8 (don't know) code indicates that the respondent specifically told the interviewer that he or she did not know the answer to the question. The -8 (don't know) code is not used in the school or teacher data. For questions where \"don't know\" is one of the options explicitly provided, a -8 is not coded for those who choose this option; instead the \"don't know\" response is coded as indicated in the value label information for the variable associated with that question. The -9 (not ascertained) code indicates that the respondent left a question blank that he or she should have answered (or for which it is uncertain whether the item should have been answered or legitimately skipped because the respondent also left a preceding item blank). However, if a gate question 2 was left blank, but valid responses are provided to follow-up questions, the valid responses are included in the data file. For example, in the spring 2013 school administrator questionnaire version A (SAQ-A), question E1 asks, \"Do any of the children in this school come from a home where a language other than English is spoken?\" If the school administrator left E1 blank (i.e., unanswered), but then provided a valid response for question E2 which asks, \"What percentage of children in this school and in second grade are English language learners (ELL)?,\" E1 is coded -9 and the information from E2 is included in the data file as reported. If a gate question and its follow-up questions were left blank, all of the questions (gate and follow-up) are coded as -9 (not ascertained). For data that are not collected using the self-administered questionnaires (e.g., direct assessment scores), a -9 means that a value was not ascertained or could not be calculated due to nonresponse. The -9 (not ascertained) code is also used in the parent interview data when the interview ended before all applicable questions were asked. In these cases, the code of -9 is used for all variables associated with interview questions that came after the point at which the parent ended the interview. One exception to this coding scheme is the pointer variables. 3 Pointer variables are not set to -9 when the interview ended before all applicable questions were asked; instead they are set to the value corresponding to the household's parent figure(s). The -9 code is also used in the parent interview for questions that were edited 4 or inadvertently skipped in computer-assisted interviewing (CAI) programming. After editing, for complete interviews, the data for all questions that should have been asked but were not are coded as -9 (not ascertained), while the data for other skipped questions are coded as -1 (not applicable); codes -7 and -8 are 2 A gate question is the first question in a series with skips to one or more follow-up questions. 3 Pointer variables indicate the household roster number of a person in the household who was the subject of questions about a parent figure. 4 Edits to household composition data that result in the addition or deletion of a parent or parent figure in the child's household sometimes result in -9 (not ascertained) codes being used for variables in multiple sections of the parent interview that have questions that are asked depending on the presence of specific parents or parent figures. The affected sections in the spring 2013 parent interview include FSQ (Family Structure), DWQ (Discipline, Warmth, and Emotional Supportiveness), NRQ (Nonresident Parents), COQ (Country of Origin for Nonresident Biological Parents), PPQ (Parent's Psychological Well-Being and Health), PEQ (Parent Education and Human Capital), and EMQ (Parent Employment). The -9 (not ascertained) code is used for both questions that are asked about specific parent/parent figures as well as those that are based on skips from those questions."}, {"section_title": "7-10", "text": "used only when respondents stated a response of \"don't know\" or \"refused,\" and not as a result of editing or inadvertently skipping a question as a result of CAI programming. Missing values (-1, -7, -8, or -9) in questions that allow for more than one response are coded the same for all coding categories used for the question. For example, in the spring 2013 parent interview, if the question about languages spoken in the home (PLQ040) has the answer of -8 (don't know), then all the language variables associated with that question (e.g., Arabic, French, Korean, and any categories that were added based on \"other, specify\" upcoding) are also coded as -8 (don't know). The \"system missing\" code appears as a blank when viewing codebook frequencies and in the ASCII data file. System missing codes (blanks) indicate that data for an entire instrument or assessment are missing due to unit nonresponse. For example, when a child's parent does not participate in the parent interview, all of the data associated with questions from the parent interview are coded \"system missing\" (blank) for that child. These blanks may be converted to another value when the data are extracted into specific processing packages. For instance, SAS converts these blanks into periods (\".\") for numeric variables. Codes used to identify missing values (-1, -7, -8, -9, or system missing) are not all identified as missing values by default in the data analysis software. Users will need to define these as missing values in the software they are using to analyze the data. Depending on the research question being addressed, in some instances users may want to assign a valid value to cases with missing values. For example, a teacher who reported that he or she did not have any English language learners in his or her classroom in the spring 2013 teacher-level questionnaire (Q A21) skipped the next question (Q A22) asking how many English language learners were in his or her classroom. An analyst interested in knowing the average number of English language learners in the classrooms of children in the ECLS-K:2011 may want to recode a value of -1 (not applicable) on the variable associated with Q A22 to a value of 0 (thereby indicating no English language learners in the classroom) in those instances where a teacher indicated in Q A21 that there were no English language learners in the classroom. It is advised that users crosstabulate all gate questions and follow-up questions before proceeding with any recodes or use of the data. Additionally, data users are encouraged to closely examine the distribution of their data and value labels to determine if values that appear to be missing value codes are valid data prior to any recoding. Composite variables may be derived using data from one or more instrument(s) in one round of data collection, from instrument data across multiple rounds, or from both instrument data and data 7-11 from administrative records in one or more rounds. If a particular composite is inapplicable for a certain case, for example, as school composite variables are for children who are homeschooled, the variable is given a value of -1 (not applicable) for that case. In instances where a variable is applicable, but complete information required to construct the composite is not available, the composite is given a value of -9 (not ascertained). The -7 (refused) code is not used for any of the composites except for the height and weight composites. The -8 (don't know) code is not used for any of the composites. There is variation in the use of system missing for composite variables. Some child demographic variables (date of birth, sex, and race/ethnicity) are considered applicable to all 18,174 children who participated in the base year and are not assigned a value of system missing for any case. For composite variables using data from both a survey instrument and other administrative or school data sources, only nonparticipants in a given round of data collection are assigned values of system missing. For composite variables using data from only one instrument, (e.g., X6LANGST, primary household language, is derived from the spring 2013 parent interview), a value of system missing is assigned if the instrument on which they are based was not completed; if the instrument was completed and an item used in the composite derivation was missing, the composite is assigned a value of -9 as described above. There are many flags on the data file that indicate the presence or absence of child assessment data across rounds 5 and 6. X5RDGFLG and X6RDGFLG denote whether a child had scoreable reading assessment data in fall 2012 and spring 2013, respectively; X5MTHFLG and X6MTHFLG denote whether a child had scoreable mathematics assessment data in fall 2012 and spring 2013, respectively; X5SCIFLG and X6SCIFLG denote whether a child had scoreable science assessment 7-12 data in fall 2012 and spring 2013, respectively. 5 If a child answered fewer than 10 questions in any direct cognitive assessment domain (reading, mathematics, or science), the assessment was not considered scoreable. Only items actually attempted by the child counted toward the scoreability threshold. 6 A flag value of 1 indicates that the child responded to 10 or more questions in the assessment for that domain, and thus has the associated scores. A flag value of 0 indicates the child had fewer than 10 responses and does not have a score. X5HGTFLG, X6HGTFLG, X5WGTFLG, and X6WGTFLG indicate the presence of data for height and weight in fall 2012 and spring 2013, respectively. As was done for the Numbers Reversed task and DCCS, flags indicating the presence of height (X1HGTFLG, X2HGTFLG) and weight (X1WGTFLG, X2WGTFLG) data in rounds 1 and 2 were added to the kindergarten through second grade data file. 5 For earlier rounds of data collection, these reading and mathematics flags took into account both the English and Spanish administrations of the assessments. (The science assessment was administered only in English). In the fall 2012 and spring 2013 data collections, all children received the reading and mathematics assessments in English so no language of administration is specified here. For more information on the language of administration, see section 2.1.1. 6 See chapter 3 for a complete discussion of assessment scoreability."}, {"section_title": "7-13", "text": "The child's assessment status for the fall of 2012 and spring of 2013 is indicated by the composites X5ASMTST and X6ASMTST, respectively. The valid values include 1 for children who have any assessment data in the data file; 7 2 for those children who were excluded due to disability (and, therefore, do not have assessment data in the data file); and 3 for children who do not have assessment data in the data file and were not excluded due to disability. Note that those excluded due to disability (code 2) are considered to be participants in the data collection round even if they do not have any parent interview data either. In addition, two composite variables use FMS data to indicate whether the child was excluded from the assessment due to a disability: X5EXDIS and X6EXDIS. Study team leaders obtained information from school staff in the fall of 2012 and spring of 2013 about whether a child had an IEP on file and if any information in a child's IEP indicated that he or she would need Braille, large print, or sign language. It was also determined whether the IEP specifically prohibited the child from participating in standardized assessments such as those conducted in the ECLS-K:2011. If so, the child was not assessed, and XnEXDIS was coded 1 (child was excluded from the assessment due to a disability). Otherwise, XnEXDIS was coded 0 (child was not excluded from the assessment due to a disability). Students could have been excluded from taking the assessment for other reasons (e.g., lack of parental consent); these children are also coded 0 on XnEXDIS. The number of cases with system missing values varies across the six XnEXDIS variables, due to the sample for each round. The cases that are system missing on X1EXDIS are cases that were added to the sample in the spring of the base year and thus were not members of the sample in round 1. The cases that are system missing on X3EXDIS and X5EXDIS are those that were not selected for the fall subsample. There are no cases coded system missing on these variables in rounds 2, 4 and 6."}, {"section_title": "Parent Data Flags (X5PARDAT, X6PARDAT, X6EDIT, X5BRKFNL, X6BRKFNL)", "text": "There are two flags that indicate the presence of parent interview data. X5PARDAT is coded as 1 if there was a fully completed or partially completed interview in fall 2012. A partially completed parent interview in fall 2012 was one that ended before all applicable questions were answered, but that had answers to questions through section TUQ (time use). 8 X6PARDAT is coded as 1 if there was a fully completed or partially completed interview in spring 2013. A partially completed interview in spring 2013 was one that ended before all applicable questions were answered, but that had answers to questions through FSQ200 (variable P6CURMAR) in section FSQ (family structure). The flag X6EDIT indicates whether, for a given case, household matrix data were reviewed or edited. It is coded as 1 if a parent interview household matrix was edited (e.g., if the age of a household member was reported incorrectly and had to be updated, or a person who was added to the household in error needed to be deleted from the household) or reviewed for editing even if no data were changed (e.g., if there were data that suggested a possible problem, but after examining the case the data were left as they were reported). This flag is included to make users aware that data cleaning or review of household matrix data was necessary for a particular case. If something about the household composition or characteristics of the household members seems unusual (e.g., the child is identified as having a 34-yearold brother in the household) and this flag is set to 1, this is an indication that the unusual data were reviewed and either edited to appear as they do in the data file or left as is because it was confirmed the data were accurate or there was no additional information indicating how the data could be edited accurately. The composite variables X5BRKFNL and X6BRKFNL indicate a final breakoff from the round 5 and round 6 parent interviews, respectively. A final breakoff occurs when a respondent stops in the middle of the interview before answering all applicable questions. These composites identify the variable associated with the last question answered by the parent. The breakoff point is provided only for those parent interviews with a status of partially complete. Cases for which a parent completed the interview have a value of -1, indicating that the case was not a breakoff."}, {"section_title": "Teacher Flags (X5TQCDAT, X6TQCDAT, X6TQTDAT, X6SETQA, X6SETQC)", "text": "Two types of data were collected from teachers using two different questionnaires, a teacher- The data file contains flag variables that can be used to determine whether data were obtained from a teacher. 9 There are separate flag variables corresponding to each of the teacher questionnaires (teacher-level and child-level) given to the specific teacher in the fall and spring data collections (X6TQTDAT for the teacher-level questionnaire (as noted above, a teacher-level questionnaire was not fielded in the fall); X5TQCDAT and X6TQCDAT for the child-level questionnaire in fall and spring, respectively). Two flags indicate the presence of data from each of the two special education teacher questionnaires for spring 2013 (X6SETQA for the teacher-level questionnaire and X6SETQC for the child-level questionnaire). No special education teacher questionnaires were fielded in the fall collection. Cases linked to a special education teacher who did not complete a questionnaire and cases that were not linked to a special education teacher have a value of 0 on these flags. Users interested in information about whether special education teacher questionnaires were requested, regardless of whether special education questionnaires were completed in the spring of 2013, can use the composite variable X6SPECS, which is based on information from the FMS rather than the special education questionnaires. X6SPECS can be used with the flags for the presence of data for special education teacher questionnaires, X6SETQA and X6SETQC, to indicate whether special education questionnaires were requested and received. For example, if X6SETQA=0 and X6SPECS=1, this indicates that the case was linked to a special education teacher who did not complete a teacher-level special education questionnaire, but special education questionnaires were requested. If X6SETQA=0 and X6SPECS=2, this indicates that the case was not linked to a special education teacher and special education questionnaires were not requested. X6SPECS is described further below in section 7.5.1.11. 9 An identification number is provided in the teacher ID variables T5_ID and T6_ID as long as a child was linked to a general classroom teacher, even if the teacher did not complete any questionnaires. 7-16"}, {"section_title": "School Administrator Data Flag (X6INSAQ)", "text": "There is a flag for the school administrator questionnaire (X6INSAQ) that is coded 1 if there are data from either version of the spring 2013 school administrator questionnaire (SAQ) and 0 if there are no data from the SAQ."}, {"section_title": "Other Child Status Flags (X5DEST, X6DEST, X5FALLSMP)", "text": "Three additional child status flags are included in the data file. The variable X5DEST is nonmissing for respondents in the fall round and indicates whether the child was in a destination school in the fall of 2012. Destination schools are schools for which it was determined that at least four ECLS-K:2011 children moved into them; this typically happened when children attended a school that ended with a particular grade (e.g., a school that only provided education through first grade) or a school that closed. This variable is coded 1 if the school a child attended was identified as a destination school; otherwise, it is coded 0. It is set to system missing for children in the fall 2012 subsample who did not participate in fall 2012 and for children who were not in the fall 2012 subsample. X6DEST is nonmissing for respondents in the spring round and is coded 1 if the child attended a destination school in the spring of 2013, and 0 otherwise. The identification variable X5FALLSMP indicates whether a child was selected to participate in the round 5 fall subsample. A value of 1 indicates the child was selected and either participated in the fall 2012 child assessment or had a parent complete the fall parent interview, while 2 indicates the child was selected but does not have a complete child assessment or parent interview. A value of 3 indicates the child was not selected for the fall subsample."}, {"section_title": "Composite Variables", "text": "To facilitate analysis of the survey data, composite variables were derived and included in the data file. This section identifies the source variables and provides other details for the composite variables. Most composite variables were created using two or more variables that are also available in the data file, each of which is named in the text that explains the composite variable. Other composites, for example, X_CHSEX_R, were created using data from the Field Management System (FMS) and the sampling frame, which are not available in the data file. Note that some of these variables have been 7-17 updated or revised since their release on the base-year data file. Such variables have an \"_R\" suffix in their name."}, {"section_title": "Child Composite Variables", "text": "There are many child-level composite variables in the child catalog. The nonassessment variables are described in further detail here. The child-level composites for the direct and indirect child assessment are described in chapter 3."}, {"section_title": "Child's Date of Birth (X_DOBYY_R and X_DOBMM_R)", "text": "Information about child's date of birth was collected from schools at the time of sampling and stored in the FMS, collected from parents in the fall kindergarten parent interview, and then collected or confirmed by parents in the spring kindergarten parent interview (parents confirmed the parent report from the fall or FMS data if the fall parent report was not obtained). Questions to collect date of birth information were only asked in the fall 2011, spring 2012, fall 2012, or spring 2013 interviews if data from the parent interview about the child's date of birth were missing due to unit or item nonresponse. In these rounds of the study, the parent was only asked child's date of birth if the parent had not confirmed FMS-reported data (or had not reported date of birth if there were no FMS data) in a prior interview. The child's date of birth composite variable was derived from information collected in the parent interview (P6CHDOBM, P6CHDOBY, P5CHDOBM, and P5CHDOBY) and the composite date of birth variable released in the kindergarten through first grade (K-1) longitudinal data file (X_DOBMM_R, X_DOBDD_R, 10 and X_DOBYY_R).Spring 2013 data for the child's date of birth were given priority for creating the composite, followed by the fall 2012 data. In creating the composite, the spring 2013 data were given priority over data from other rounds because they were collected in the most recent interview and any data that were missing from the parent interview due to unit or item nonresponse had the potential to be updated in spring 2013. If the data about the child's date of birth were not collected in these rounds because information about date of birth was collected in a prior parent interview, then the data in this composite are the same as the data in the composite released in the K-1 longitudinal data file. Information on date of birth is based on information collected from parents in the fall 2012 or spring 2013 7-18 parent interviews for just a small number of children. In such instances, the data for the composite in the K-2 longitudinal data file are different than the data for the composite in the K-1 longitudinal data file. Variables indicating the date of assessment (day, month, and year) in rounds 1 through 6 are also included in the kindergarten through second grade data file. Variables for the day of assessment (X1ASMTDD, X2ASMTDD, X3ASMTDD, X4ASMTDD, X5ASMTDD, X6ASMTDD) provide a range of days in a month that the child was assessed and are coded 1 (days 1 through 7); 2 (days 8 through 15); 3 (days 16 through 22); 4 (day 23 or later); or -9 (not ascertained). The exact day of the month is not provided for reasons related to confidentiality. Variables for the month of assessment (X1ASMTMM, X2ASMTMM, X3ASMTMM, X4ASMTMM, X5ASMTMM, X6ASMTMM) indicate the month that the child was assessed, and variables for the year of assessment (X1ASMTYY, X2ASMTYY, X3ASMTYY, X4ASMTYY, X5ASMTYY, X6ASMTYY) indicate the year that the child was assessed."}, {"section_title": "Child's Sex (X_CHSEX_R)", "text": "Information about child's sex was collected from schools at the time of sampling and stored in the FMS, collected from parents in the fall kindergarten parent interview, and then collected or confirmed by parents in the spring kindergarten parent interview (parents confirmed the parent report from the fall or FMS data if the fall parent report was not obtained). Questions to collect information on the child's sex were only asked in the fall 2011, spring 2012, fall 2012, or spring 2013 interviews if data from the parent interview about the child's sex were missing due to unit or item nonresponse. In these rounds of the study, the parent was only asked the child's sex if the parent had not confirmed FMS reported data (or had not reported the child's sex if there were no FMS data) in a prior interview. The composite variable indicating the child's sex was derived using data collected in the parent interview (P5CHSEX and P6CHSEX) and the composite child sex variable released in the K-1 longitudinal data file (X_CHSEX_R). Spring 2013 data for the child's sex were given priority for creating the composite, followed by the fall 2012 data. In creating the composite, the spring 2013 data were given priority over data from other rounds because they were collected in the most recent interview and any data that were missing from the parent interview due to unit or item nonresponse had the potential to be updated in spring 2013. If the data about the child's sex were not collected in these rounds because information about child sex was collected in a prior parent interview, then the data in this composite are the same as the data in the composite released in the K-1 longitudinal data file. Information on child sex is based on information collected from parents in the fall 2012 or spring 2013 parent interviews for fewer than 10 cases. In such instances, the data for the composite in the K-2 longitudinal data file are different than the data for the composite in the K-1 longitudinal data file. information was updated in these composite variables for about 300 cases based on information collected from parents in the spring 2013 parent interviews. Parents were asked about the child's ethnicity in the spring of 2013 if ethnicity in the parent interview items for the child were missing due to unit or item nonresponse. Specifically, parents were asked whether or not their child was Hispanic or Latino. Parents were also asked about the child's race in spring 2013 only if parent interview race data for the child were missing. Parents were asked to indicate to which of five race categories (White, Black or African American, Asian, Native Hawaiian or other Pacific Islander, American Indian or Alaska Native) their child belonged, and they were allowed to indicate more than one. From these responses, a series of five dichotomous race variables were created that indicate separately whether the child belonged to each of the five specified race groups. In addition, one additional dichotomous variable was created to identify those who had indicated that their child belonged to more than one race category. 13 The seven dichotomous ethnicity and race variables (X_HISP_R, X_AMINAN_R, X_ASIAN_R, X_HAWPI_R, X_BLACK_R, X_WHITE_R, X_MULTR_R) were created using parent data from spring 2013, or if those data were not asked in spring 2013 because they were asked in a previous round of the study, the dichotomous composites were set to the values of the first-grade dichotomous race composites that used parent data from the first grade and base year (X_HISP_R, X_AMINAN_R, X_ASIAN_R, X_HAWPI_R, X_BLACK_R, X_WHITE_R, X_MULTR_R). Otherwise, the dichotomous ethnicity and race composites were set to -9 (not ascertained). Using the six dichotomous race variables and the Hispanic ethnicity variable, the race/ethnicity composite variables for the child (X_RACETHP_R, X_RACETH_R) were created. The categories for these variables are: White, non-Hispanic; Black or African American, non-Hispanic; Hispanic, race specified; Hispanic, no race specified; Asian, non-Hispanic; Native Hawaiian or other and American Indian or Alaska Native, non-Hispanic; More than one Race, non-Hispanic) are coded according to the child's reported race. If the report about whether the child was Hispanic or Latino was -7 7-21 (refused) or -8 (don't know), or if the child is not Hispanic/Latino and parent reported race is missing, X_RACETHP_R is coded -9 (not ascertained); if the report about whether the child was Hispanic or Latino is also missing from the FMS, or if the child is not Hispanic/Latino and race is also missing from the FMS, X_RACETH_R is coded -9 (not ascertained). The difference between X_RACETHP_R and X_RACETH_R is that if race or ethnicity data are missing from the spring 2013 parent interview, X_RACETH_R is set to the value used for the first-grade composite, also called X_RACETH_R, which uses both parent data and FMS data, while only parent-report data were used for the variable X_RACETHP_R. Thus, there are more missing data for X_RACETHP_R than for X_RACETH_R. About 210 cases have a value for X_RACETHP_R that is different in the K-2 longitudinal file than in the K-1 longitudinal file due to the collection of child race/ethnicity data in the spring 2013 parent interview. Most of these cases changed value from -9 (not ascertained) to a valid value, but about 85 cases changed from code 4, Hispanic-no race reported, to code 3, Hispanic-race reported. About 150 cases have a changed value for X_RACETH_R due to the collection of child race/ethnicity data in the spring 2013 parent interview. Most of these cases, about 120, changed from code 4, Hispanic-no race reported, to code 3, Hispanic-race reported. The categories for X_RACETHP_R and X_RACETH_R are mutually exclusive, meaning that a child is coded as just one race/ethnicity. Users interested in the specific races of children who are identified as multiracial, or who are interested in identifying the race(s) of children who are identified as Hispanic, should use the dichotomous race variables discussed above."}, {"section_title": "Child's Height (X5HEIGHT, X6HEIGHT)", "text": "To obtain accurate measurements, each child's height was measured twice in each data collection round. The height measurements were entered into the computer program used for the assessment, with a lower limit set at 35 inches and an upper limit set at 72 inches. For the height composites, if the two height measurements obtained within a round (i.e., C5HGT1 and C5HGT2 for fall 2012 and C6HGT1 and C6HGT2 for spring 2013) were less than 2 inches apart, the average of the two height values was computed and used as the composite value. If the two measurements were 2 inches or more apart, for X5HEIGHT (the child's height in fall 2012), the measurement that was closest to 49.46 inches for boys and 49.17 inches for girls was used as the composite value. This is the 50th percentile height for children who were 7 and a half years old (91.14 7-22 months for boys and 90.45 months for girls: the average age at assessment in fall 2012 using the composite X5AGE). If the two spring measurements were 2 inches or more apart, for X6HEIGHT (the child's height in spring 2013), the measurement that was closest to 50.63 inches for boys and 50.51 inches for girls was used as the composite value. This is the 50th percentile height for children who were 8 years old (97.74 months for boys and 97.12 months for girls: the average age at assessment in spring 2013 using the composite X6AGE). The height averages come from the 2000 Centers for Disease Control and Prevention (CDC) Growth Charts (www.cdc.gov/growthcharts/html_charts/statage.htm). 14 The two height measurements were 2 or more inches apart for 3 cases for X5HEIGHT and 9 cases for X6HEIGHT. If one value for height was missing, the other value was used for the composite. If both the first and second measurements of height were coded as -8 (don't know), then the height composite was coded as -9 (not ascertained). If both the first and second measurements of height were coded as -7 (refused), then the height composite was coded as -7 (refused). If both the first and second measurements of height were coded as -9 (not ascertained) because height data were missing as the result of a breakoff in the child assessment or the measurements had different missing values (e.g., one was -8 and the other was -9), then the height composite was coded as -9 (not ascertained). For 117 cases, the child's height in the spring of 2013 (X6HEIGHT) was shorter than in the fall of 2012 (X5HEIGHT). A difference of 1 inch or less (54 children) could be a function of things such as slouching versus standing upright or differences in shoes, hairstyle, thickness of socks, or a combination of these factors. However, 63 children were recorded as being more than 1 inch shorter in the spring than in the fall, and 45 of those were recorded as being more than 2 inches shorter. In addition, 145 children were recorded as having a shorter height in the spring of 2012 than in the spring of 2011. Of these children, 49 were recorded as having a height difference of 1 inch or less; 96 were recorded as having a height difference of more than 1 inch; and 65 were recorded as having a height difference of more than 2 inches. These discrepancies may result from measurement error or recording error. Analysts should use their own judgment in how to use these cases in their analysis."}, {"section_title": "Child's Weight (X5WEIGHT, X6WEIGHT)", "text": "To obtain accurate measurements, each child's weight was measured twice in each data collection round. The weight measurements were entered into the computer program used for the assessment, with a lower limit set at 20 pounds and an upper limit set at 200 pounds. Values outside the range that were documented in assessor comments as being valid measurements were included in the data file. For the weight composites, if the two weight measurements obtained within a round (i.e., C6WGT1 and C6WGT2 for spring 2013 and C5WGT1 and C5WGT2 for fall 2012) were less than 5 pounds apart, the average of the two weight values was computed and used as the composite value. If the two measurements were 5 or more pounds apart, for X5WEIGHT the measurement that was closest to 54.33 pounds for boys or 53.48 pounds for girls was used as the composite value. These are the median weights for children who were 7 and a half years old (91.14 months for boys and 90.45 months for girls: the average age at assessment in fall 2012 using the composite X5AGE). If the two measurements were 5 or more pounds apart, for X6WEIGHT the measurement that was closest to 57.28 pounds for boys or 57.36 pounds for girls was used as the composite value. These are the median weights for children who were 8 years old (97.74 months for boys and 97.12 months for girls: the average ages at assessment in spring 2013 using the composite X6AGE). The weight averages come from the 2000 CDC Growth Charts (see www.cdc.gov/growthcharts/html_charts/wtage.htm). 15 The two weight measurements were 5 or more pounds apart in 5 cases for X5WEIGHT and 8 cases for X6WEIGHT. If one value for weight was missing, the other value was used for the composite. If both the first and second measurements of weight were coded as -8 (don't know), the weight composite was coded as -9 (not ascertained). If both the first and second measurement of weight in the child assessment were coded as -7 (refused), then the weight composite was coded as -7 (refused). If both the first and second measurements of weight in the child assessment were coded as -9 because weight data were missing as the result of a breakoff in the child assessment or the measurements had different missing values (e.g., one was -8 and the other was -9), then the weight composite was coded as -9 (not ascertained). There are 16 children whose round 6 weights are more than 10 pounds lower than their round 5 weights; of these, four changes are in the range of 23 pounds to 74.1 pounds. There are 69 15 For calculating the median weight, the composites X5AGE and X6AGE were used to determine children's average age at assessment. The average age at assessment in fall 2012 was 91.14 months for boys and 90.45 months for girls using the composite X5AGE. The closest value on the CDC Growth Chart was 91.5 for boys and 90.5 for girls. The average age at assessment in spring 2013 was 97.74 months for boys and 97.12 months for girls using the composite X6AGE. The closest value on the CDC Growth Chart was 97.5."}, {"section_title": "7-24", "text": "children whose round 6 weights are more than 15 pounds higher than their round 5 weights; of these, nine changes are in the range of 30.1 to 54.2. It is possible that some of these changes result from measurement error. Analysts may wish to review such cases and determine how to account for these weight changes in their analysis."}, {"section_title": "Child's Body Mass Index (X5BMI, X6BMI)", "text": "Composite body mass index (BMI) was calculated by multiplying the composite weight in pounds by 703.0696261393 and dividing by the square of the child's composite height in inches (Keys et al. 1972;Mei et al. 2002). Unrounded values of height and weight were used in the calculation of BMI. If either the height or weight composite was coded as -9 (not ascertained), -7 (refused), or -8 (don't know), the BMI composite was coded as not ascertained (-9)."}, {"section_title": "Child's Disability Status (X6DISABL2, X6DISABL)", "text": "Two composite variables based on information obtained in the parent interview were created to indicate whether a child had a disability diagnosed by a professional. Note that these variables indicate either diagnosed disabilities that were identified for the first time in the round 6 parent interview or diagnoses reported in a previous interview for which the child also had a diagnosis reported in round 6. The variables must be used in conjunction with the disability composites from earlier rounds to identify the entire group of children who have ever had a disability diagnosed by a professional. Also, these two variables differ in how missing data were treated during their creation, as described below. Questions in the spring 2013 parent interview asked about the child's ability to be independent and take care of himself or herself, ability to pay attention and learn, coordination in moving arms and legs, overall activity level, overall behavior and ability to relate to adults and children, emotional or psychological difficulties, ability to communicate, difficulty in hearing and understanding speech, and eyesight. If parents indicated that their child had any issues or difficulties in response to these questions, follow-up questions asked whether the child had been evaluated by a professional for that particular issue and whether a diagnosis of a problem was obtained by a professional (CHQ120, CHQ125, CHQ215, CHQ245, CHQ246, CHQ300, CHQ301). A question was also asked about current receipt of therapy services or participation in a program for children with disabilities (CHQ340)."}, {"section_title": "7-25", "text": "The composite variable X6DISABL is coded 1 (yes) if the parent answered \"yes\" to at least one of the questions about diagnosis (indicating a diagnosis of a problem was obtained) or therapy services (indicating the child received services) (CHQ120, CHQ215, CHQ245, CHQ300, CHQ340) and the questions about the specific diagnoses (CHQ125, CHQ246, CHQ301) were not coded -7 (refused,) -8 (don't know), or -9 (not ascertained); or in the case of the vision diagnosis (CHQ301), the question was not coded as only nearsightedness (myopia), farsightedness (hyperopia), color blindness or deficiency, or astigmatism; or in the case of a hearing diagnosis (CHQ246), the question was not coded as only external ear canal ear wax. Using these criteria to calculate X6DISABL, a child could be coded as having a disability even if data for some of the questions about diagnoses or therapy services (CHQ120, CHQ215, CHQ245, CHQ300, CHQ340) were missing. This is because a child is coded as not having a disability if there are data for at least one of the questions about diagnoses or therapy services (CHQ120, CHQ215, CHQ245, CHQ300, CHQ340), and the response was either 2 (no) or the item was -1 (inapplicable) (because the child did not have issues that indicated a question should be asked), even if data for some of these questions were missing. In addition to having \"no\" answers or \"inapplicable\" codes for the diagnoses or therapy services questions, if the child had a diagnosis, but the specific diagnosis was not reported (was refused, don't know, or not ascertained), X6DISABL was also coded 2 (no) because there was no reported disability. The composite was coded as missing only if all of the data for the questions about diagnoses or therapy services (CHQ120, CHQ215, CHQ245, CHQ300, CHQ340) were -7 (refused), -8 (don't know), or -9 (not ascertained), or if the items that skipped to these items were -7 (refused), -8 (don't know), or -9 (not ascertained). A more conservative approach when coding cases that had incomplete data for the diagnoses and services variables was used to derive the variable X6DISABL2. Whereas X6DISABL codes cases with missing data as \"no\" as long as all the information that was collected indicates the child does not have a diagnosed disability or receive services for a diagnosed disability, X6DISABL2 is coded -9 (not ascertained) when any of the questions about diagnoses or therapy services (CHQ120, CHQ215, CHQ245, CHQ300, CHQ340) are -7 (refused), -8 (don't know), or -9 (not ascertained), or the items that skipped to these items are -7 (refused), -8 (don't know), or -9 (not ascertained). For X6DISABL2, if there are no \"yes\" answers for a disability, but any of the evaluation (CHQ115, CHQ210, CHQ235, CHQ290), diagnoses (CHQ120, CHQ215, CHQ245, CHQ300), or therapy questions (CHQ340) are -7 (refused), -8 7-26 (don't know), or -9 (not ascertained), 16 or if any of the evaluation, diagnosis, or therapy questions were not asked (were -1 for inapplicable) because of missing data for questions that skipped to those questions (and thus it is not known if they should have been asked), X6DISABL2 is coded -9 (not ascertained). In addition, if the parents indicated that a diagnosis had been obtained, but the specific diagnosis was coded as refused, don't know, or not ascertained, X6DISABL2 is coded as -9 (not ascertained). This approach is more conservative because it does not assume that the response for unanswered questions was \"no.\" Due to these differences in coding, the number of cases identified as not having a diagnosed disability is higher for X6DISABL than it is for X6DISABL2."}, {"section_title": "Primary Language in the Child's Home (X6LANGST)", "text": "A composite variable was created to indicate whether English was a primary language spoken in the home or whether a non-English language was the primary language spoken in the spring of 2013. If there were data from a parent interview conducted prior to the spring of 2013 about what languages were spoken in the household, parents were asked to verify, if applicable, that English was the only language spoken or, if applicable, that a language other than English was spoken (P6SINGLL). If there was a primary language reported in a previous interview, parents were also asked to verify the primary language (P6PRIMRL). If there was no information about language from a previous interview or if the parent indicated that the language recorded in a previous interview was not currently the language spoken (P6SINGLL = 2), parents were asked if any language other than English was regularly spoken in their home (P6ANYLNG). If a language other than English was not spoken in the home (P6ANYLNG = 2 or (P6SINGLL = 1 and English was the only language spoken)), or if a language other than English was spoken in the home but the primary language of the household (P6PRMLNG) was English (P6PRMLNG = 0 or P6PRIMRL = 1 and the previous round composite X*LANGST = 2 for English), the composite is coded as 2 (English language). If both English and another language were spoken in the home, and the respondent reported that two or more languages were spoken equally or they could not choose a primary language, the composite is coded 3 (cannot choose primary language or two languages equally). Otherwise, if a language other than English was spoken (P6ANYLNG = 1), either solely (P6ENGTOO) or primarily in the home (P6PRMLNG has a nonmissing value other than 0 for English, or P6PRIMRL = 1 and the 16 If CHQ340 was -9 (not ascertained) because the interview broke off after CHQ330, but all answers in CHQ330 and questions prior to CHQ330 indicated that CHQ340 would not have been applicable, X6DISABL2 and X2DISABL2 were coded 2 (no disability) because that question would not have been asked for those children."}, {"section_title": "7-27", "text": "primary language verified was a non-English language as indicated in the previous round composite X*LANGST), the composite is coded as 1 (non-English language)."}, {"section_title": "Student Grade Level (X5GRDLVL, X6GRDLVL)", "text": "The X5GRDLVL composite indicates the child's grade level in the fall of 2012 as reported by the teacher or recorded in the FMS. This composite has valid values for the 4,981 cases that are respondents for round 5, that is the cases that have either child assessment or parent interview data. It is constructed using T5GRADE (child's grade level from the fall TQC) and F5CLASS2 (child's grade in fall 2012 from the FMS). The values include 1 for kindergarten (either full or part day), 2 for first grade, 3 for second grade, 4 for third grade or higher, and 5 when the child is in an ungraded setting. In all other cases the value is set to -9 for not ascertained. The X6GRDLVL composite indicates the child's grade level in the spring of 2013 as reported by the teacher or recorded in the FMS. This composite has valid values for the 14,447 cases that are respondents for round 6, that is the cases that have either child assessment or parent interview data. It is constructed using T6GRADE (child's grade level from the spring TQC) and F6CLASS2 (child's grade in spring 2013 from the FMS). The values include 1 for kindergarten, 2 for first grade, 3 for second grade, 4 for third grade or higher, and 5 when the child is in an ungraded setting. In all other cases the value is set to -9 for not ascertained. Note that grade level (F5CLASS2 and F6CLASS2) was obtained for homeschooled children. Prior to the spring of 2013, parents were asked for the child's grade level during the child assessment in the home. In the spring of 2013, parents were not asked for this information. The grade level from the spring of 2012 was increased by one grade for the fall of 2012. If a parent volunteered new information about grade level and it contradicted the constructed grade level, the parent-reported information was used to code grade level on the composite."}, {"section_title": "Child Linked to a Special Education Teacher (X6SPECS)", "text": "The composite variable X6SPECS indicates whether or not children were linked to a special education teacher and special education questionnaires were requested from teachers in the spring of 2013, based on the presence or absence of a link to a special education teacher or related service provider 7-28 in the FMS. The value is 1 if special education questionnaires were requested and 2 if special education questionnaires were not requested. Study team leaders asked school staff if any accommodations were required for the study children to be assessed. During that discussion about assessment accommodations, team leaders were also supposed to record whether the child had an Individualized Education Program (IEP) on file with the school but did not require any accommodations for the study assessments. The link to a special education teacher was established automatically when information indicating a child needed an accommodation or had an IEP but did not require an accommodation was entered in the FMS by study team leaders. There are a few cases of a mismatch between X6SPECS and special education teacher reports about an IEP. In about 40 cases, there were FMS data indicating the child had an IEP on record at the school (and thus a special education teacher questionnaire was requested from the teacher and X6SPECS = 1), but the special education teacher indicated in the child-level questionnaire that the child did not have an IEP (E6RECSPE=2). "}, {"section_title": "Family and Household Composite Variables", "text": ""}, {"section_title": "Household Counts (X6HTOTAL, X6NUMSIB, X6LESS18, X6OVER18)", "text": "The composite variable X6HTOTAL provides a count of the total number of household members in the spring of 2013. For households for which household roster information had been collected in a prior round, this count is the number of household members who were previously rostered and reported to still be in the household plus any new persons added after the last interview in which roster information was collected. For a small number of households that did not participate in any of the prior parent interviews in which household composition information was collected (fall 2010, spring 2011, or spring 2012), X6HTOTAL is a count of the total number of persons identified by the respondent as household members in the spring 2013 parent interview. Two composite variables take the ages of the household members into account to indicate the total numbers of (1) adults and (2) children in the household in the spring of 2013. Information about household members' ages was collected in the household matrix, or roster, section of the parent interview. X6LESS18 indicates the total number of people in the household under age 18, including the study child, siblings, and other children, and X6OVER18 indicates the total number of people in the household age 18 or older. All household members who were 18 years old or older, as well as anyone identified as a parent  or grandparent of the focal child whose age is missing, are counted in the total for X6OVER18. Households with members with missing age information who are not identified as a parent or grandparent are coded as -9 (not ascertained) on X6OVER18 and X6LESS18. X6LESS18 is created by subtracting X6OVER18 from X6HTOTAL. The composite X6NUMSIB indicates the total number of siblings (biological, step-, adoptive, or foster) living in the household with the study child. Siblings were identified by questions in the FSQ section of the parent interview that asked about the relationship of each household member to the study child. X6NUMSIB does not count children of the parent's boyfriend or girlfriend (identified by the code 5 in the variables associated with question FSQ180) as siblings."}, {"section_title": "Household Rosters", "text": "The ECLS-K:2011 data file includes rosters of the household members as collected in the parent interviews. The roster information appears as part of the block of Family Structure Questions (FSQ) for each round in which the FSQ section was included in the parent interview. Variable names begin with P1 for round 1 (fall kindergarten), P2 for round 2 (spring kindergarten), P4 for round 4 (spring 2012, when most children were in first grade), and P6 for round 6 (spring 2013 when most children were in second grade). No FSQ section was included in the brief round 3 or round 5 parent interviews. For each household member in each round, roster variables include the following, where * is the round number (1, 2, 4 or 6) and # is the household roster number (1 through 25):"}, {"section_title": "7-32", "text": "In rare cases, there are roster positions for which all values are system missing or -1 across all rounds but P4CUR_# = 2 or P6CUR_# = 2 (not a current household member). This may occur because a new household member was the respondent for round 3 or 5, when there was no roster completion or confirmation in the parent interview, and that person had left the household before the next parent interview in which complete household composition information was collected. 18 Specific cases for which these circumstances occurred are described in the appendix of this manual. Determining household membership in a given round. In round 1, respondents were not asked if persons were currently household members, because that was the first household enumeration for the study and all enumerated persons were household members at that time. For rounds 2, 4, and 6, analysts can determine the current household membership at the time of the parent interview for the round by examining the variables P2CUR_#, P4CUR_#, and P6CUR_#, respectively. Analysts should not look for the first \"empty\" position in the roster series to determine the last person with roster data in the household, since, as noted above, all persons retain their household positions permanently; if person 3 leaves the household, then person 4 still remains in position 4."}, {"section_title": "Parent Identifiers and Type in the Household (X6IDP1, X6IDP2, X6HPAR1, X6HPAR2, X6HPARNT)", "text": "X6IDP1 and X6IDP2 indicate the positions in the household roster of the sampled child's residential parent/parent figure(s) in the spring of 2013. 19 The construction of parent identifiers and the household composition variables from the parent interview data was a multistep process. First, it was determined from household roster variables whether there was a mother (biological, adoptive, step-, or foster) and/or a father (biological, adoptive, step-, or foster) in the household. Using this information, the method described below was used to create X6IDP1 and X6IDP2 for the spring."}, {"section_title": "1.", "text": "If there was only one mother (of any type, including unknown type) and only one father (of any type, including unknown type) in the household, the mother was identified as parent 1 (X6IDP1) and the father was identified as parent 2 (X6IDP2)."}, {"section_title": "2.", "text": "If there was only one mother (of any type, including unknown type) in the household and no other parent figure (of any type), the mother was identified as parent 1 and parent 2 is coded -1 (not applicable). If there was a mother and she had a male spouse/partner in the household who was not identified as a father (of any type, including unknown type), the spouse/partner was identified as parent 2."}, {"section_title": "3.", "text": "If there was only one father (of any type, including unknown type) in the household and no other parent figure (of any type), the father was identified as parent 1 and parent 2 is coded -1 (not applicable). If there was a father and he had a female spouse/partner in the household who was not identified as a mother (of any type), the spouse/partner was identified as parent 1 and the father was identified as parent 2."}, {"section_title": "4.", "text": "If there were two mothers (or a mother and female spouse/partner) in the household, an order of preference was used to identify one mother to be parent 1, with the order specified as biological, adoptive, step-, foster mother or female guardian, then other female parent or guardian. 20 The other mother was identified as parent 2. If there were two mothers of the same type (e.g., two adoptive mothers) or there were two mothers and the type for both was -7 (refused) or -8 (don't know), the mother with the lowest household roster number was identified as parent 1 and the other mother was identified as parent 2."}, {"section_title": "5.", "text": "If there were two fathers in the household (or a father and male spouse/partner), an order of preference was used to identify one father to be parent 1, with the order specified as biological, adoptive, step-, foster father or male guardian, then other male parent or guardian. The other father was identified as parent 2. If there were two fathers of the same type (e.g., two adoptive fathers) or there were two fathers and the type for both was -7 (refused) or -8 (don't know), the father with the lowest household roster number was identified as parent 1 and the other father was identified as parent 2."}, {"section_title": "6.", "text": "If there was no one in the household identified as a mother or father, then a female respondent or the female spouse or partner of a male respondent was identified as parent 1. If the female parent figure had a male spouse or partner, the spouse/partner was identified as parent 2. If the respondent was male and had a female spouse or partner, she was designed as parent 1 and he was designated as parent 2. For example, if a child lived with his grandmother (the respondent) and grandfather, and neither his mother nor father lived in the household, then the grandmother was identified as parent 1 and the grandfather was identified as parent 2. If the grandfather lived in the household, but no grandmother or parents lived there, the grandfather respondent would be parent 1 and parent 2 would be coded -1. Demographic information such as age, race, and education was collected for these \"parent figures.\" Once parents/parent figures were identified, X6HPAR1 and X6HPAR2 were created to identify the specific relationship of parent 1 and parent 2 to the study child. 21 It should be noted, however, that for households in which the child lived with parent figures other than his or her mother and/or father, the parent figures identified in X6IDP1 and X6IDP2 were not defined as parents (meaning biological, 7-34 step-, adoptive, or foster) for the construction of X6HPAR1 and X6HPAR2. For example, if there are a grandmother and grandfather and there are no parents listed in the household, X6HPAR1 and X6HPAR2 would be coded as category 15 (no resident parent). X6HPARNT indicates the type(s) of parents living in the household with the study child. The values for the X6HPARNT composite are as follows: When study children are living with parent figures (e.g., grandmother and grandfather), rather than biological, adoptive, step-, or foster parents, X6HPARNT is coded 4. The composite parent identifier variables X6IDP1 and X6IDP2 are used to determine which composite variables correspond to parent 1 and parent 2, respectively. These \"pointer\" variables indicate the household roster number of the person who was the subject of the questions being asked. All parent composite variables that include \"PAR\" and the number 1 in the variable name are associated with the person designated in X6IDP1, who is parent 1. All parent composite variables that include \"PAR\" and the number 2 in the variable name are associated with the person designated in X6IDP2, who is parent 2. In the second-grade parent interview, there are two sets of questions that were first asked about parent 1 and then asked about parent 2 if the household contained two parents. The first set of questions asks where parent 1 and parent 2 were born (P6PARCT1 and P6PARCT2) and when, if applicable, they moved to the United States (P6PAREM1 and P6PAREM2). The second set of questions asks about parent employment. There is a second set of \"pointer\" variables that hold the household roster number of the person who was the subject of the employment questions (P6EMPP1 and P6EMPP2). However, since the employment questions were asked only of parent(s) or parent figure(s) in the household, the value of employment pointer variables is the same as the value for the composite parent identifier variables. That is, the household roster number indicated in the pointer variable for the employment data for parent 1, P6EMPP1, is always equal to the household roster number indicated in the composite parent identifier variable of parent 1, X6IDP1. Similarly, the household roster number indicated in the pointer variable for the employment data for parent 2, P6EMPP2, is always equal to the household roster number indicated in the composite parent identifier variable of parent 2, X6IDP2 (where applicable)."}, {"section_title": "7-35", "text": "To illustrate how the pointer variables work, suppose there is a household with both a mother and a father who were listed as the third and fourth individuals in the household roster. According to the rules outlined above, household member #3, the mother, becomes parent 1 and X6IDP1 equals 3. All applicable pointer variables for parent 1 will subsequently take on the value 3. Similarly, household member #4, the father, becomes parent 2 and X6IDP2 equals 4. All applicable pointer variables for parent 2 will subsequently take on the value 4. Table 7-1 identifies the EMQ section pointer variables included in the data file along with the interview items and variables associated with those pointer variables. The pointer variables are necessary to determine which parent should be assigned the answers to items about employment. Returning to the example above, the answers to the employment questions for the mother are stored in variables that end with the suffix \"1\" since the mother was identified as parent 1, and her household roster number is the value in X6IDP1. For example, P6PAY_1_I and P6VAC_1_I indicate whether the mother had a paid job in the last week and whether the mother was on leave in the past week, respectively.. The answers to the employment questions for the father are stored in variables that end with the suffix \"2\" since the father was identified as parent 2, and his household roster number is the value in XIDP2. For example, P6PAY_2_I and P6VAC_2_I indicate whether the father had a paid job in the last week and whether the father was on leave in the past week, respectively.  "}, {"section_title": "7-36", "text": ""}, {"section_title": "Parent Demographic Variables (X6PAR1AGE, X6PAR2AGE, X6PAR1RAC, X6PAR2RAC)", "text": "X6PAR1AGE is a composite variable for the age of parent 1 from the household roster (the person whose roster number is indicated in X6IDP1) and X6PAR2AGE is the composite variable for the age of parent 2 from the household roster (the person whose roster number is indicated in X6IDP2). 22 The ages of all household members (other than the child) who had their ages collected in the fall of 2010 or spring of 2011 were automatically incremented by two years for the spring 2013 parent interview. Age was incremented by one year for household members who were living in the household in the spring of 2012 and had age information collected in that interview but who were not in the household in the fall of 2010 or the spring of 2011. For information about how the first and second parents were selected for these and other parent variables, see section 7.5.2.3 above. The composite variables for race/ethnicity for the parent/guardians were derived in the same way as those for the child, except that there are no variables that supplement parent-reported race/ethnicity with FMS data as was done for children. All data on parent race/ethnicity come from the (checked with a private employment agency); P6DO3_1_I =1 (checked with an employer directly or sent a resume to an employer); P6DO4_1_I =1 (checked with friends or relatives); P6DO5_1_I =1 (placed or answered ads/sent a resume related to an ad); P6DO6_1_I (contacted school/university employment center); or P6DO7_1_I (checked a union register or professional register)), 25 then X6PAR1EMP_I is coded 3 (looking for work). If parent 1 was not working for pay, not on vacation, and not looking for work (P6PAY_1=2 and P6VAC_1_I =2 and P6LOK_1_I =2), or if parent 1 was looking for work (P6LOK_1_I =1) and the variables for the seven activities indicating the parent was actively looking for work were all coded 2 (no), X6PAR1EMP_I is coded 4 (not in the labor force). 26 X6PAR2EMP_I (parent 2 employment status) is created the same way as X6PAR1EMP_I, but uses the data linked to parent 2. guardian in any round (P*REL_* = 1 or 2 or P*UNR = 3 or 4), those who were a respondent in any round (P*PER_* = 1), and persons who were spouse/partners of respondent parents in any round. 25 P6DO6_1_I (contacted school/university employment center) and P6DO7_1_I (checked a union register or professional register) are new variables that were added as activities indicating the respondent was looking for work and used for coding the employment composite in the spring of 2013. 26 Because some persons were not looking for work according to the five categories described above, even though it was reported that a parent was looking for work (P6LOK_1_I = 1), the parent is coded as not in the labor force (X6PAR1EMP_I = 4) rather than as looking for work (X6PAR1EMP_I = 3). If a parent was reported as looking for work (P6LOK_1_I =1), the questions about the parent's last occupation were asked. There are 149 cases with occupation data that are categorized as X6PAR1EMP = 4 (not in the labor force) because they indicated that all they were doing to look for work was looking at or reading want ads or some \"other\" activity that did not qualify them to be classified as looking for work according to the U.S. Bureau of Labor Statistics (2014); there are 44 cases with occupation data where X6PAR2EMP = 4."}, {"section_title": "7-40", "text": "Imputation was performed on the variables (e.g., P6HRS_1_I, P6DO1_I) that were used to create the X6PAR1EMP_I and X6PAR2EMP_I composite variables. Variables that were imputed were those associated with questions about whether employment had changed since the spring of 2012, whether the parent had worked for pay in the last week or was on leave or vacation, hours worked in a typical week, whether the parent was looking for work and if, so, what the parent was doing to find work. Each variable has a separate imputation flag (e.g., IFP6PAY_1 is the imputation flag for P6PAY_1_I, the variable for whether parent 1 had paid job last week) indicating whether data were imputed for each case in the data file. "}, {"section_title": "7-41", "text": "Exhibit 7-4. Industry and occupation codes used in the ECLS-K:2011"}, {"section_title": "Executive, Administrative, and Managerial Occupations", "text": "This category includes senior-level and middle management occupations and occupations that directly support management. Senior-level managers are persons concerned with policymaking, planning, staffing, directing, and/or controlling activities. Middle managers include persons who plan, organize, or direct and/or control activities at the operational level. Workers in this category are not directly concerned with the fabrication of products or with the provision of services. Other officials and administrators include consultants, library directors, custom house builders, and location managers. Legislators are also included in this category."}, {"section_title": "Engineers, Surveyors, and Architects", "text": "This category includes occupations concerned with applying principles of architecture and engineering in the design and construction of buildings, equipment and processing systems, highways and roads, and land utilization."}, {"section_title": "Natural Scientists and Mathematicians", "text": "This category includes those engaged primarily in the application of scientific principles to research and development. Natural scientists are those in the physical sciences (e.g., chemistry, physics) and the life sciences (e.g., biology, agriculture, medicine). In addition, this category includes those in computer science, mathematics (including statistics), and operations research."}, {"section_title": "Social Scientists, Social Workers, Religious Workers, and Lawyers", "text": "This category includes occupations concerned with the social needs of people and with basic and applied research in the social sciences."}, {"section_title": "Teachers: College, University, and Other Postsecondary Institution; Counselors, Librarians, and Archivists", "text": "This category includes those who teach at higher education institutions and at other postsecondary (after high school) institutions, such as vocational institutes. In addition, vocational and educational counselors, librarians, and archivists are included here."}, {"section_title": "Teachers, Except Postsecondary Institution", "text": "This category includes prekindergarten and kindergarten teachers, elementary and secondary teachers, special education teachers, instructional coordinators, and adult education teachers (outside postsecondary education)."}, {"section_title": "Physicians, Dentists, and Veterinarians", "text": "This category includes health care professionals who diagnose and treat patients. In addition to physicians, dentists, and veterinarians, this category includes optometrists, podiatrists, and other diagnosing and treating professionals, such as chiropractors, hypnotherapists, and acupuncturists. Once occupations were classified in X6PAR1OCC_I and X6PAR2OCC_I, they were assigned the average of the 1989 General Social Survey (GSS) prestige scores, which is reported in variables X6PAR1SCR_I and X6PAR2SCR_I. If the parent's occupation was 22 (Unemployed, Retired, Unclassifiable), the prestige score was set to -9 (not ascertained). If the parent's occupation was -1 (No Occupation) on X6PAR1OCC_I or X6PAR2OCC_I, the prestige score was also coded -1. Although the GSS prestige scores are from 1989, they are still being used by the current GSS survey and matched to 1980 census codes. 27 Because these prestige scores were also used for the ECLS-K 1998-99 cohort, they allow for comparisons to the ECLS-K. Table 7-2 provides the prestige score values for each occupation category. Occupations were imputed if such information was not collected in the parent interview. Missing data for individual items related to parent employment were imputed first, and then those imputed data were used to compute the occupation composite variables if necessary (i.e., cases missing employment status that were imputed to be working or on leave from a job also had their occupation imputed and a prestige score assigned to the imputed occupation; cases missing data for the variables about looking for work and that were imputed to be actively looking for work (defined by EMQ070 answers 1-7) also had occupation imputed). Table 7-3 shows the amount of missing data for parent occupation. 27 New technology jobs that came into existence since 1989 were appropriately coded. For example, \"website developer\" was included in the \"Technologists and Technicians, Except Health\" category; \"website sales\" was included in the \"Marketing and Sales Occupations\" category; and \"run web printer\" was included in the \"Production Working Occupations\" category. leave from a job or unemployed and actively looking for work, he or she was asked the occupation questions. Category 22 was used only if a respondent reported an occupation that could not be classified in the coding scheme, \"unemployed \"or \"retired.\") Because these occupations could not be classified, the prestige score is coded -9 (not ascertained) -1 (No occupation) When occupation is -1, the prestige score is also -1.  "}, {"section_title": "7-45", "text": ""}, {"section_title": "7-46", "text": "The first stage of imputation was to use longitudinal imputation, where possible. In longitudinal imputation, values from a prior interview are carried forward when data from the round of interest are missing, provided the parent figure for whom data are being imputed was the same in both Where longitudinal imputation was not possible, missing values were imputed using a hot deck method in which similar respondents and nonrespondents are grouped or assigned to \"imputation cells,\" and a respondent's value is randomly \"donated\" to a nonrespondent within the same cell. Cells are defined by characteristics such as geographic region, school locale, school type, household type, age, race, education, and income. When information was missing for any of the variables used to define the imputation cells in second grade, information was used from first grade or the base year, where available. After imputation was completed, the average of the General Social Survey (GSS) prestige scores was assigned to each occupation code. The imputation flag variables IFX6PAR1OCC and IFX6PAR1SCR indicate whether the occupation (X6PAR1OCC_I) and occupational prestige score (X6PAR1SCR_I) for parent 1 were imputed. These flags match in value because the prestige score (e.g., X6PAR1SCR_I) is coded directly from occupation (e.g., X6PAR1OCC_I). Similarly, the flags IFX6PAR2OCC and IFX6PAR2SCR indicate whether the occupation (X6PAR2OCC_I) and occupational prestige score (X6PAR2SCR_I) for parent 2 were imputed."}, {"section_title": "Household Income and Poverty (X6INCCAT_I, X6POVTY_I)", "text": "Household income data were collected in the spring 2013 parent interview. Parents were asked to report income by broad range ($25,000 or less or more than $25,000) and by detailed range as shown in table 7-4. 28 The composite X6INCCAT_I was created using the detailed income range information. X6INCCAT_I was set to the value of P6INCLOW_I (detailed income range for those who 28 Starting at category 9 of the detailed income range, the categories for the income variable in the ECLS-K:2011 are different from those used in the ECLS-K. More narrow ranges of income were used at higher income levels in the ECLS-K:2011 in order to determine whether household income was near 200 percent of the federal poverty threshold given household size. If so, follow-up questions about exact income were asked."}, {"section_title": "7-47", "text": "reported the broad income range in P6HILOW_I as $25,000 or less) or P6INCHIG (detailed income range for those who reported the broad income range in P6HILOW_I as more than $25,000). When data for the broad range variable (P6HILOW_I) or one of the detailed range variables (P6INCLOW_I, P6INCHIG_I) were missing (i.e., coded -7 (refused), -8 (don't know), or -9 (not ascertained)), income information was imputed.   Where longitudinal imputation was not possible, missing values were imputed using the hot deck method described in section 7.5.2.5. Cells were defined by characteristics such as geographic region, school locale, school type, household type, age, and race. When information used to define the imputation cells was missing for any of these variables in second grade, information was used from first grade or the base year, where available. Imputation flag values for IFP6HILOW, IFP6INCLOW, and IFP6INCHIG identify cases for which longitudinal or hot deck imputation was conducted. Reported income was used to determine household poverty status in the spring of 2013, which is provided in variable X6POVTY_I. For some households, more detailed information about household income than the ranges described above was collected. Specifically, when parent respondents reported a detailed household income range suggesting the household income was close to or lower than 200 percent of the U.S. Census Bureau poverty threshold for a household of its size, the respondents were asked to report household income to the nearest $1,000 (referred to as exact income) in order to determine household poverty status more accurately. Table 7-6 shows the reported detailed income categories for households of a given size for which respondents were asked the exact income question. For example, a respondent in a household with two people would have been asked to provide an exact income if the respondent had indicated that the household income was less than or equal to $30,000. When information about exact household income was available (P6TINCTH_I), it was used in conjunction with household size (X6HTOTAL) to calculate the poverty composite. When exact income was not available because the exact income question was not asked, the midpoint of the detailed income category (X6INCCAT_I) was used in conjunction with household size (X6HTOTAL). 30"}, {"section_title": "7-48", "text": "29 The CAPI program used to conduct the parent interview was programmed to only ask for exact income when parent respondents reported a detailed household income range suggesting the household income was close to or lower than 200 percent of the U.S. Census Bureau poverty threshold for a household of its size. Although the parent interview in which this information was collected was conducted in the spring of 2013, the 2011 poverty thresholds were used for instrument programming because they were the most recent thresholds available when programming was done. The question about exact income was asked for the following conditions: (NUMBER IN HH = 1 AND PAQ. . 30 Because exact income information was not collected from all parents, the ECLS-K:2011 provides an approximate but not exact measure of poverty. Years Old, retrieved 5/6/2014 from http://www.census.gov/hhes/www/poverty/data/threshld/index.html. 2 The 2012 weighted poverty thresholds were used for the poverty composite because respondents in the spring of 2013 were asked about household income in the past year. At the time that the spring 2013 parent interview was finalized, the most updated poverty thresholds available were the weighted 2011 poverty thresholds. Poverty thresholds for 2012 were similar to the poverty thresholds for 2011. However, because of differences in one category, exact income should have been asked for one narrow range of incomes according to the 2012 thresholds, but it was not asked because the 2011 thresholds were used. Using the 2012 poverty thresholds rather than the 2011 poverty thresholds, cases with five household members and an income between $55,001 and $55,654 were not asked exact income when they should have been. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2013."}, {"section_title": "7-49", "text": "Household poverty status in the spring of 2013 was determined by comparing total household income reported in the parent interview to the weighted 2012 poverty thresholds from the U.S. Census Bureau (shown in table 7-7), which vary by household size. Although the parent interview was conducted in the spring of 2013, the 2012 weighted poverty thresholds were used in the derivation of the poverty composite because respondents were asked about household income in the past year. Exact income (P6TINCTH_I) was asked in the parent interview or imputed for all persons in categories 1 and 2 of the poverty composite. Imputation of exact income was conducted according to thresholds in the parent interview. Households with an exact income that fell below the appropriate threshold were classified as category 1, \"below the poverty threshold,\" in the composite variable. Households with an exact income that was at or above the poverty threshold but below 200 percent of the poverty threshold were classified as category 2, \"at or above the poverty threshold, but below 200 percent of the poverty threshold,\" in the composite variable. Households with a total income (either exact or the income representing the midpoint of the detailed range reported by the composite) that was at or above 200 percent of the poverty threshold were classified as category 3, \"at or above 200 percent of the poverty threshold,\" in the composite variable. 31 For example, if a household contained two members and the household income was lower than $14,937, the household was considered to be below the poverty threshold and would have a value of 1 for 31 In the ECLS-K:2011, there are three categories in the poverty composite rather than two categories for \"below poverty threshold\" and \"at or above poverty threshold\" as there were in the ECLS-K. The ECLS-K:2011 categories 2 and 3 can be combined to create a poverty composite variable comparable to the ECLS-K poverty composite variable."}, {"section_title": "7-50", "text": "the composite. If a household with two members had an income of $14,937 or more, but less than $29,874 (200 percent of the poverty threshold for a household of two), the composite would have a value of 2. If a household with two members had an income of $29,874 or more, the composite would have a value of 3. x is the natural log of the midpoint of the detailed income range. The weight used to compute the z score would be the spring 2013 child base weight. The spring 2013 base weight,W6CI0, should be merged by CHILDID into the data file from file K2BASEWTS in appendix D of the ECB. The SES variable for the i-th household would then be computed as , where m is the number of components. Note that for households with only one parent present and for parents who were retired or not currently in the labor force, not all the components would be defined. In these cases, the SES would be the average of the z scores of the available components. "}, {"section_title": "X6RESREL2)", "text": "The respondent to the parent interview was a person identified as the household member who knew the most about the child's care, education, and health. X6RESID indicates the household roster number of the spring 2013 parent interview respondent. The relationship variables (P6REL_1-P6REL_25, P6MOM_1-P6MOM_25, P6DAD_1-P6DAD_25, and P6UNR_1-P6UNR_25) associated with the respondent's household roster number were used to code X6RESREL2. If the respondent was a biological mother or father, X6RESREL2 is coded as 1 (biological mother) or 4 (biological father), respectively. If the respondent was an adoptive, step-, or foster mother or father, or other female or male guardian, X6RESREL2 is coded as 2 (other mother type) or 5 (other father type), respectively. If the respondent was a mother or father but the type of mother (P6MOM_#) or father (P6DAD_#) was coded as -7 (refused), -8 (don't know), or -9 (not ascertained), X6RESREL2 is coded as 3 (mother of unknown type) or 6 (father of unknown type). 32 If the respondent was a grandparent, aunt, uncle, cousin, sibling, or other 32 Categories for mothers and fathers of unknown type were new for the spring 2012 composite. Mothers and fathers of unknown type were included in the categories \"other mother type\" and \"other father type\" for the fall 2010 and spring 2011 composites, X1RESREL and X2RESREL."}, {"section_title": "7-52", "text": "relative, X6RESREL2 is coded as 7 (nonparent relative). If the respondent was a girlfriend or boyfriend of the child's parent or guardian; a daughter or son of the child's parent's partner; other relative of the child's parent's partner; or another nonrelative, X6RESREL2 is coded as 8 (nonrelative). Otherwise, X6RESREL2 is coded as -9 (not ascertained). Because the interviewer initially asked to speak with the previous round respondent at the beginning of the spring 2013 parent interview, the respondent for previous interviews (X*RESID) was the same person for many cases. Although household roster information was not collected in the fall 2011 and fall 2012 parent interviews, the variables X3RESID and X5RESID indicate the household roster number of the parent interview respondent in these rounds when it was possible to identify that fall respondent in the roster from a different round. For cases that indicated that the respondent was the same person as the respondent in the most recent interview with family roster data, X3RESID and X5RESID were created by assigning the number for that respondent in a previously collected roster. If the respondent was new to the household in the fall of 2011 or the fall of 2012 and, therefore, was not included in a previously collected household roster, X3RESID and X5RESID were created by assigning the roster number for that respondent from a subsequent interview in which information on that respondent was added to the roster. In some cases, the fall 2011 or fall 2012 parent respondent was not a household member in a previous interview and was no longer a household member in the subsequent spring interview, so no roster information was ever collected for that respondent. In these instances, X3RESID and X5RESID are coded -9 (not ascertained)."}, {"section_title": "Teacher Composite Variables", "text": "In addition to the teacher data flags discussed in section 7.4.3 above, there are several composite variables on the file that use data from teachers. There is a composite variable (X56CHGTCH) discussed below in section 7.7 indicating whether the child changed teachers between the fall and spring data collections. There are also composite variables about the child's closeness and conflict with the teacher (X6CLSNSS, X6CNFLCT). These two variables are described in chapter 3, along with other variables derived from teacher reports of children's social skills. Other variables that use teacher data are about the child's grade level (e.g., X6GRDLVL) and are discussed above in section 7.5.1 about the child composites. 7-53"}, {"section_title": "School Composite Variables", "text": "Variables describing children's school characteristics were constructed using data from the teacher, the school administrator, and the sample frame. Details on how these variables were created are provided below."}, {"section_title": "School Type (X6SCTYP)", "text": "In the spring of 2013, the questionnaire given to administrators in schools that did not have previous round data (SAQ-A) contained a question on school type that was used in the creation of the spring school type composite (X6SCTYP). The questionnaire given to administrators in schools that had provided data in previous rounds (SAQ-B) did not contain the question used to create the school type composite; therefore, for these schools data from the spring 2012 composite, X4SCTYP, or base-year data from the round 2 composite, X2SCTYP, were used for the composite X6SCTYP when such data were available. X6SCTYP was created as follows when SAQ-A was given to school administrators: If question A6 in the SAQ-A (\"Which of the following characterizes your school?\") was answered as \"a regular public school (not including magnet school or school of choice)\" (S6REGPSK); \"a public magnet school\" (S6MAGSKL); or \"a charter school\" (S6CHRSKL); the school was coded as \"public.\" If the question was answered as \"a Catholic school\" of any type (S6CATHOL, S6DIOCSK, S6PARSKL, or S6PRVORS), the school was coded as \"Catholic.\" If the question was answered as \"other private school, religious affiliation\" (S6OTHREL), the school was coded as \"other religious.\" Otherwise, if the question was answered as \"private school, no religious affiliation\" (S6OTNAIS, S6OTHRNO), then the school was coded as \"other private.\" When questionnaire SAQ-B was given to school administrators, or if questionnaire SAQ-A was given to school administrators and there were missing data for school type, X6SCTYP was set to the value for X4SCTYP if there were data from round 4. Otherwise, if there were data from the base year for X2SCTYP, X6SCTYP was set to the value for X2SCTYP."}, {"section_title": "7-54", "text": "If data from SAQ-A or the previous round composites (X4SCTYP, X2SCTYP) were missing, information about school type from the school master file (which included FMS and frame data) were used. Homeschooled children have a code of -1 (not applicable) on X6SCTYP. 33 Children who changed schools and were not followed and children who were not located in the spring of 2013 have a code of -9 (not ascertained) for X6SCTYP. The variable X6SCTYP is set to system missing for children who were not participants in the spring 2013 round. In addition, nonparticipants have a value of 990000000 on the variable F6CCDLEA."}, {"section_title": "Public or Private School (X5PUBPRI, X6PUBPRI)", "text": "X5PUBPRI and X6PUBPRI are broad indicators of school type with only two categoriespublic and private. X5PUBPRI, which is derived from fall 2012 school frame information, has valid values for the 4,981 cases that are respondents for round 5; that is, the cases that have either child assessment or parent interview data. X6PUBPRI, which is derived from the more detailed school type variable X6SCTYP described above, has valid values for the 14,447 cases that have either child assessment or parent interview data in round 6. These composites were created as follows: X5PUBPRI and X6PUBPRI are coded 1 (public) if school type indicated in the fall 2012 frame data or X6SCTYP, respectively, is 4 (public). X5PUBPRI and X6PUBPRI are coded 2 (private) if school type indicated in the fall 2012 frame data or X6SCTYP, respectively, is 1, 2, or 3 (Catholic, other religious, or other private). If school frame data for fall 2012 or X6SCTYP data are missing, X5PUBPRI and X6PUBPRI are coded according to the composite from the most recent previous round of the study with nonmissing data (X4PUBPRI and X2PUBPRI are used for both X5PUBPRI and X6PUBPRI; X3PUBPRI is used for X5PUBPRI). If the school identification number for fall 2012 indicated that the child was homeschooled, then X5PUBPRI is coded -1 (not applicable). If the school identification number for spring 2013 indicated that the child was homeschooled, then X6PUBPRI is coded -1 (not applicable). X5PUBPRI and X6PUBPRI are coded -9 (not ascertained) if data on school type are not available in the fall 2012 school frame and X6SCTYP, respectively. X5PUBPRI is set to system missing for children who did not participate in round 5. Similarly, X6PUBPRI is set to system missing for those who did not participate in round 6. 33 These children were enrolled in a school at the time of sampling in the base year, but were homeschooled during the spring of 2013. 7-55"}, {"section_title": "School Enrollment (X6ENRLS)", "text": "There is a composite variable in the data file (X6ENRLS) that indicates total school enrollment on October 1, 2012 (or the date nearest to that date for which the school administrator had data available). This total school enrollment composite was created using the school enrollment variable from the school administrator questionnaire (S6ANUMCH). If school administrator data on total school enrollment were missing, enrollment data were obtained from the 2011-12 Private School Universe Survey (PSS) for private schools and from the 2011-12 Common Core of Data (CCD) public school universe data for public schools. If enrollment data were also missing on the PSS or CCD, longitudinal imputation using data from a prior round of the ECLS-K:2011 was conducted for cases in which children were still attending the same study school in round 6 as in the prior round for which data are available. If the spring 2012 enrollment composite variable had a valid value, then X6ENRLS was set to the value of X4KENRLS; otherwise, if the spring 2011 enrollment composite variable had a valid value, the value of X6ENRLS was set to the value of X2KENRLS. In all other cases the variable is coded -9 (not ascertained)."}, {"section_title": "Percent Non-White Students in the School (X6RCETH)", "text": "The composite variable X6RCETH indicates the percentage of the student population that was not White in the spring of 2013. 34 The composite is derived from a question in the school administrator questionnaire (question A9 in SAQ-A, and question A6 in SAQ-B) that asked the number or percentage of students in the school who were the following race/ethnicities: Hispanic/Latino of any "}, {"section_title": "7-56", "text": "School administrators were allowed to report their answers to the student racial/ethnic composition questions as either numbers or percentages. All answers provided as numbers were converted to percentages using the total enrollment variable S6TOTENR as the denominator before computing the composite variable. 35 The sum of the calculated percentages for each race/ethnicity category was allowed to be within +/-5 percent of 100 percent to allow for minor reporting errors of numbers that did not add to the reported total or percentages that did not add to 100 percent. In a few cases, this procedure resulted in a total sum of percentages that was slightly over 100 percent. Totals greater than 100 percent are topcoded to 100 percent. A flag for each individual race/ethnicity variable indicating whether the school administrator reported the information as a number or a percent is included in the data file. 36 Because the composite is calculated as a percent, these flags will not be needed by users unless they are interested in examining how answers were reported. If the flag (S6ASIAFL S6HISPFL, S6BLACFL, S6WHITFL, S6AIANFL, S6HAWPFL, and S6MULTFL) for each of the race/ethnicity variables (S6ASIAPT, S6HISPPT, S6BLACPT, S6WHITPT, S6AIANPT, S6HAWPPT, and S6MULTPT) is equal to 1, that indicates the information was reported by the school administrator as a percentage. If the flag (S6ASIAFL S6HISPFL, S6BLACFL, S6WHITFL, S6AIANFL, S6HAWPFL, and S6MULTFL) for each of the race/ethnicity variables (S6ASIAPT, S6HISPPT, S6BLACPT, S6WHITPT, S6AIANPT, S6HAWPPT, and S6MULTPT) is equal to 2, that indicates the information was reported by the school administrator as a number."}, {"section_title": "7-57", "text": "In some cases, the composite could not be derived from the school administrator questionnaire responses because some data used to compute it were missing or the data collected from administrators appeared to be in error. If the composite could not be derived from the data, the percentage of non-White students in the school was obtained from the 2011-12 CCD (for public schools) or the 2011-12 PSS (for private schools). If these data were also missing on the CCD or PSS, the composite is coded based on the spring first-grade composite X4RCETH if the child attended the same school. If those data were also missing, the composite is coded based on the spring kindergarten composite X2KRCETH. If those data were also missing, X6RCETH is coded -9 (not ascertained). If the study child was homeschooled in the spring of 2013, X6RCETH is coded -1 (not applicable)."}, {"section_title": "Highest and Lowest Grade at the School (X6LOWGRD, X6HIGGRD)", "text": "Composite variables indicate the lowest grade taught at the school (X6LOWGRD) and the highest grade taught at the school (X6HIGGRD). They are derived from information collected from the school administrator during the spring 2013 data collection (for administrators in schools for which previous data were not available, who received questionnaire SAQ-A) or from the spring of 2012 or the spring of 2011 (for administrators in schools for which previous data were available and who received questionnaire SAQ-B). For administrators who received questionnaire SAQ-A, both variables are created by first coding answers of \"ungraded\" in question A5 (\"Mark all grade levels included in your school\") as category 15 (ungraded) and then coding the lowest grade in the school and the highest grade in the school, respectively. The grade level for children in transitional kindergarten, kindergarten, or pre-first grade is coded as category 2 (kindergarten). For administrators who received questionnaire SAQ-B, or those who received questionnaire SAQ-A and had missing data for school grade levels, the composites X6HIGGRD and X6LOWGRD were set to the composite values for X4HIGGRD and X4LOWGRD, respectively, if those data were available. If round 4 data were not available, X6HIGGRD and X6LOWGRD were set to the values forX2HIGGRD and X2LOWGRD, respectively. Data from the school frame were used if information about the highest and lowest grade at the school was not collected from the SAQ-A or was not in the previous round composites (X4HIGGRD and X4LOWGRD, X2HIGGRD, and X2LOWGRD). 7-58"}, {"section_title": "Students Eligible for Free or Reduced-Price School Meals (X6FRMEAL_I)", "text": "A new composite, X6FRMEAL_I, indicates the percent of students in the school who were approved for free or reduced-price school meals (X6FRMEAL_I). This composite has valid values for the 14,447 cases that have either child assessment or parent interview data in round 6. This composite differs from the school meal composites created in for the spring of 2011 and the spring of 2012 (X2FLCH2_I, X2RLCH2_I, X4FMEAL_I, and X4RMEAL_I) because the spring 2013 school administrator questionnaire did not include questions on USDA program participation or the numbers of students eligible for free and reduced priced meals (breakfast or lunch) that were used as the sources of the composite variables for previous rounds. However, in the spring of 2013 and in previous rounds of the study, school administrators were asked for the percentage of children eligible for free or reduced-price lunch. This question and several other sources of information were used to create X6FRMEAL_I. Specifically, X6FRMEAL_I is derived from the percentage of children eligible for free or reduced-price lunch reported by the school administrator during the spring 2013 data collection, or imputed if the item was missing, using information collected from school administrators in the spring of 2012 or the spring of 2011, school master file variables, or composite variables from the spring of 2012 and the spring of 2011. 37 X6FRMEAL_I, based on school administrator data about children eligible for free or reducedprice lunch, was imputed with information from previous rounds about students eligible for free or reduced-price meals because children are approved for free or reduced-price meals generally, not just for lunch. Children who were homeschooled have X6FRMEAL_I set to -1. The percent of children reported by school administrators in spring 2013 to be eligible for free or reduced-price lunch (S6PCTFLN_I) was used as the first source of data for X6FRMEAL_I. 38 S6PCTFLN_I was imputed for all cases that had child assessment or parent interview data in the spring 2013 round and a completed SAQ, but for which the administrator did not provide free and reduced-price lunch information. Table 7-8 shows the level of missing data for the school administrator variable for the percent of children who were eligible for free or reduced-price lunch (S6PCTFLN) among the schools that had at least one child or parent respondent in the spring 2013 data collection. 37 Both public schools and nonprofit private schools are eligible for the National School Lunch Program. 38 X6FRMEAL_I was top-coded to 100 percent, if necessary. The imputation flag IFS6PCTFLN indicates whether the school administrator questionnaire variable S6PCTFLN_I was longitudinally imputed using spring 2011 or spring 2012 data, was filled with data from the CCD, was imputed using the hot deck method, or was not imputed. For cases with missing data on S6PCTFLN, longitudinal imputation was used first, if possible, taking a value from school administrator data in a previous round for the same school in spring 2012 (S4PCTFLN) or spring 2011 (S2LUNCH). If historical survey data were not available, then data from the 2011-12 CCD were used to impute for these missing S6PCTFLN_I values for public schools. The PSS does not have data on school meals that can be used to compute an imputed value for S6PCTFLN_I. If CCD data were not available,"}, {"section_title": "7-59", "text": "then the values of the meal composites from previous rounds were used to compute an imputed value for S6PCTFLN_I, where available, with the imputed value computed as the sum of X4FMEAL_I and X4RMEAL_I if these were available, and otherwise the sum of X2FLCH2_I and X2RLCH2_I, if available. If S6PCTFLN_I was still missing after data from previous rounds and the CCD were used, it was imputed using the hot deck method described above in section 7.5.2.5. Hot-deck imputation was done at the school level and the imputed value was then assigned to each child in the school. In hot-deck imputation, a school with a non-missing value for a component has this value assigned or \"donated\" to a similar school with a missing value for the component. Schools are similar if they belong in the same imputation cell. Imputation cells were created using district poverty category (created from the district poverty variable X6DISTPOV described in section 7.5.7), census region, school type, the percentage of students in minority ethnic groups, and whether the school received Title I funding. Cases that did not have any data from the school administrator questionnaire in the spring of 2013 did not have a value for S6PCTFLN_I to set the value of the composite X6FRMEAL_I, so other sources were used to assign a value for the composite. X6FRMEAL_I was set to the percentage of students in the child's current school eligible for free or reduced-price lunch reported by the school 7-60 administrator in the spring of 2012 (S4PCTFLN), if those data were available. If spring 2012 data were not available but data from the spring of 2011 (S2LUNCH) were, the 2011 data were used. Otherwise, if the school master file had data for the school's total enrollment, the number of children approved for free meals, and the number of children approved for reduced-price meals, X6FRMEAL_I was set to the percentage of children approved for free meals plus the percentage of children approved for reduced-price meals. Otherwise, if the child attended the same school in spring 2012 or spring 2011, X6FRMEAL_I was set to the value of the spring 2012 free-meal composite plus the value of the spring 2012 reducedprice meal composite (X4FMEAL_I plus X4RMEAL_I), or the value of the spring 2011 free meal composite plus the value of the spring 2011 reduced-price meal composite (X2FLCH2_I plus X2RLCH2_I), respectively. Finally, if X6FRMEAL_I did not have an assigned value following each of the above steps, the remaining missing values were imputed using hot deck imputation at the composite level. The imputation flag IFX6FRMEAL indicates whether X6FRMEAL_I was imputed longitudinally, was imputed using the hot deck method, or was not imputed. Imputed values at the composite level can occur because a value of imputed composites from round 4 (X4FMEAL_I and X4RMEAL_I) or from round 2 (X2FLCH2_I and X2RLCH2_I) were used to assign a value to X6FRMEAL_I, or because X6FRMEAL_I was directly imputed at the composite level using hot deck procedures. In some cases, the children's schools are unknown because the child was unlocatable or the child moved to a nonsampled county and was not followed into his/her new school, but a parent interview was completed. In such cases, data were not imputed for X6FRMEAL_I because no information about the school was available (e.g., public or private control, school size, or even if the child was enrolled in a school). X6FRMEAL_I is coded as -9 for these cases."}, {"section_title": "School Year Start and End Dates (X6SCHBDD, X6SCHBMM, X6SCHBYY, X6SCHEDD, X6SCHEMM, X6SCHEYY)", "text": "The composite variables indicating school year start and end dates, which are listed below, were derived from question A2 in the school administrator questionnaires (S6SYRSMM, S6SYRSDD, S6SYRSYY, S6SYREMM, S6SYREDD, S6SYREYY). If the school administrator did not answer that question, data for these variables come from information contained in the FMS.  X5REGION and X6REGION are coded -9 (not ascertained) for children who were unlocatable or moved out of a sampled county and were not followed to new schools in the fall of 2012 or spring of 2013, respectively, but for whom there are parent interview data. Children who were homeschooled in the fall of 2012 have a code of -1 on X5REGION, and those who were homeschooled in the spring of 2013 have a code of -1 on X6REGION. X5REGION and X6REGION are set to system missing for those who did not participate in round 5 or 6, respectively."}, {"section_title": "7-62", "text": "For the fall 2012 and spring 2013 school locality variables, X5LOCALE and X6LOCALE, the categories correspond to the NCES system for coding locale (http://nces.ed.gov/ccd/rural_locales.asp). If data are not available for the child's school from the PSS or CCD, and locale data were available from an earlier round, the composites were set to the value from the most recent round (X4LOCALE, X2LOCALE, or X1LOCALE). Otherwise, the composites are coded -9 (not ascertained). Some -9 (not ascertained) values for X5LOCALE and X6LOCALE are associated with cases in which children who moved were unlocatable or moved out of a sampled county and were not followed to new schools in fall 2012 or spring 2013, respectively, but for whom there are parent interview data. Children who were homeschooled in the fall of 2012 are coded as -1 on X5LOCALE, and those who were homeschooled in spring 2013 are coded as -1 on X6LOCALE. X5LOCALE and X6LOCALE are set to system missing for those who did not participate in round 5 or 6, respectively. Values for X5LOCALE and X6LOCALE are the following: 42 -Rural, Distant: Census-defined rural territory that is more than 5 miles but less than or equal to 25 miles from an urbanized area, as well as rural territory that is more than 2.5 miles but less than or equal to 10 miles from an urban cluster; and 43 -Rural, Remote: Census-defined rural territory that is more than 25 miles from an urbanized area and is also more than 10 miles from an urban cluster. Some schools have different values for X*LOCALE between the base year and subsequent rounds. The differences in values reflect changes in the PSS or CCD source data. The classification of locale has undergone some changes since the ECLS-K study conducted with children in the kindergarten class of 1998-99. Information on these changes is available on the NCES website at http://nces.ed.gov/ccd/rural_locales.asp."}, {"section_title": "Field Management System (FMS) Composite Variables", "text": "Several composite variables were created from data stored in the FMS, which were obtained from frame data as well as by field staff during visits to the schools and discussions with school staff."}, {"section_title": "Year-Round Schools (X6YRRND)", "text": "The year-round school composite variable is based on information obtained from the school staff member who helps coordinate the data collection activities in the school (referred to as the school coordinator) about whether a school is a year-round school. It does not incorporate information collected from the school administrator in the school administrator questionnaire. This composite has valid values for the 14,447 cases that have child assessment or parent interview data in round 6. The values for this composite variable are 1 (year-round school) and 0 (not year-round school). If the child was homeschooled in the spring of 2013, the composite is coded as -1 (not applicable). If these data were not obtained in the spring of 2013 but information about being a year-round school was collected in an earlier round, the composite was set to the value from the most recent round (X4YRRND or X12YRRND). 7-64 . There are 55 ECLS-K:2011 public schools with a missing value for X6DISTPOV because the values were missing in the SAIPE source data."}, {"section_title": "Methodological Variables", "text": "To facilitate methodological research, 11 variables pertaining to aspects of the data collection work were extracted from the FMS and included in the data file. These include identifiers for parent interview work area (F5PWKARE, F6PWKARE), parent interviewer identification number (F5PINTVR, F6PINTVR), child assessment work area (F5CWKARE, F6CWKARE), and child assessor identification number (F5CASSOR, F6CASSOR). A \"work area\" is the group of schools that each team leader was assigned. Team leaders managed a group of 2 to 4 other individuals who worked as child assessors and parent interviewers for the sampled cases in the work area."}, {"section_title": "Children Who Changed Teachers Between Rounds (X56CHGTCH)", "text": "Teacher identification numbers (T5_ID, T6_ID) and school identification numbers (S5_ID, S6_ID) were used to determine whether children changed teachers between the fall of 2012 and the spring of 2013. The variable X56CHGTCH is only valid for cases that participated in the fall 2011 data collection. If the fall and spring teacher identification numbers are not missing and are equal to each other, then X56CHGTCH is coded 0 (no change), with a few exceptions noted below. If the fall and spring teacher identification numbers are not missing and they are not equal to each other, then X56CHGTCH is coded as 1 (changed teachers). If a teacher identification number is missing in either the fall of 2012 or the spring of 2013 but the school identification numbers are not missing and do not match, then X56CHGTCH is coded as 1 (changed teachers), because a child who changed schools would have 7-65 changed teachers. Some districts or schools did not allow the study to identify and contact teachers, and a general \"refused teacher\" code was assigned. In the 136 cases with this circumstance, the fall and spring teacher IDs match but represent refused teachers, and X56CHGTCH is coded as -9 (not ascertained). Children who were homeschooled in the spring of 2013 have a code of -1, \"schooled at home,\" for X56CHGTCH. 39 In all other cases, X56CHGTCH is coded as -9 (not ascertained) for children that participated in the fall 2011 data collection. X56CHGTCH is coded as system missing for children who were not part of the fall subsample."}, {"section_title": "Summer School and Vacation (X5SUMSH, X5SUMVD)", "text": "One composite variable provides the number of hours a child spent in summer school during the summer of 2012 (X5SUMSH) and another indicates the length of a child's summer vacation (X5SUMVD). X5SUMSH is derived from the fall 2011 parent interview questions on whether the child attended summer school, the length of the summer school session itself (days, weeks, or months), and the amount of time, in days and hours per day, of attendance (P5SUMSCH, P5SMSCNUM, P5SMSCUN, P5NDYPRM, and P5NHRPRM). If the child did not attend summer school, then X5SUMSH is set to 0. If the variables indicating (1) that the child attended summer school or (2) the amount of time in summer school were -7 (refused), -8 (don't know), or -9 (not ascertained), then X5SUMSH is set to -9 (not ascertained). It is coded as system missing for children who were not part of the fall subsample. X5SUMVD indicates the length of a child's summer vacation in days. Though length of summer vacation could be calculated for all children in the study, X5SUMVD is valid only for children who were part of the fall subsample. It is calculated as the length of time between the last day of school for the 2011-12 school year and the first day of school for the 2012-13 school year. The ending date for the school the child attended in the spring of 2012 (X4SCHEMM, X4SCHEDD) is subtracted from the beginning date of the school the child attended in the fall of 2012. The beginning date for the 2012-13 school year was only obtained in the spring of 2013 (round 6), so this variable is computed only for children who attended the same school in both the fall of 2012 and the spring of 2013 using the spring 2013 variables (X6SCHBMM, X6SCHBDD). Beginning dates in the school master file are used if the composite school beginning dates are not available because the child participated in the fall of 2011 but not the spring of 2012. If the child was homeschooled in the fall of 2012, X5SUMVD is set to -1 (not 39 Some children who were participants in round 5 were not participants in round 6 (112 children). Of this group, 90 have no round 6 teacher ID, and 82 have an unidentified school (S6_ID in the 99** series). It was assumed that those moving from a known school to a school in the 99** series changed schools and, therefore, teachers. the fall 2012 parent interview, which did not include a household roster, who had left the household by the time of the spring 2013 parent interview."}, {"section_title": "Hard-Copy Questionnaires", "text": "For the hard-copy instruments (school administrator questionnaires, teacher-level teacher questionnaire, and teacher child-level questionnaire), both range and consistency checks were performed."}, {"section_title": "Composite Variable Anomalies, Errata, and Considerations", "text": "Chapter 7 of this manual provides detailed information about the composite variables that were created and included in the data file. In this section, several data considerations related to the composite variables are described. Analysts are encouraged to carefully review the descriptions of the composite measures of interest to them in chapter 7. \uf06e There are some cases that had issues associated with the verification of primary language data that were collected in a previous parent interview. There is 1 case (10016545) that had the wrong language confirmed in the question associated with variable P6PRIMRL (PLQ010) as the primary language in the household. In the spring of 2013, primary language was confirmed based on previous interview data if they were available. For this case, although other language questions were answered in the spring 2012 parent interview, the question about primary language (question PLQ060, variable P4PRMLNG) was coded -9 (not ascertained). In the fall of 2010, the primary language was reported to be English. Although persons in this household spoke a non-English language and it was confirmed that a non-English language was the primary language in the spring of 2013, because English was reported in the only round that asked for rather than confirmed primary language, the spring 2013 composite for primary language, X6LANGST, is coded as 2 (English). There are several cases that had \"other, specify\" responses reported as the primary language in a previous round of the study that had these languages displayed in the question associated with variable P6PRIMRL (PLQ010) to confirm the primary language. This correctly follows the documented skip instructions in the parent interview. However, in a few cases (10009622,10005865,10014888,10014962,10017790) the \"other, specify\" text did not mention only one language, so one primary language was not actually verified in the spring of 2013. In all such cases, language information from previous rounds was reviewed and X6LANGST was set to the most appropriate value given prior-round information. \uf06e X56CHGTCH was coded -1 (not applicable) for children who were homeschooled in the spring of 2013. Children who were homeschooled in the fall of 2012, but not in the spring of 2013 were coded as X56CHGTCH= 1 (changed teachers) because they changed schools from homeschools to public or private schools. There are 2 children who were homeschooled in the fall of 2012 and moved out of the sampled county in the spring of 2013. These cases are also coded as X56CHGTCH= 1 (changed teachers); however, it is not known whether the children were still homeschooled in their new location or enrolled in school. \uf06e X5SUMVD was coded -1 (not applicable) for children who were homeschooled in the fall of 2012. There was 1 case who was homeschooled in spring of 2012 and not in the fall of 2012. X5SUMVD is -9 (not ascertained) for this case. \uf06e For a small number of cases, values for X2KRCETH, X4RCETH, X2FLCH2_I, X2RLCH2_I, X4FMEAL_I, and X4RMEAL_I were corrected due to errors in imputation present in these variables on previously released data files."}, {"section_title": "Numbers Reversed Grade-Normed Scores", "text": "There were multiple changes to the Numbers Reversed grade-normed scores (XnNRSSGR, XnNRPEGR) for all rounds (R1 to R6)."}, {"section_title": "\"Other, Specify\" Variables", "text": "As discussed in chapter 6, there were times when a sufficient number of cases provided the same \"other, specify\" response to warrant the addition of a new category to the response options. The categories added after data collection ended, during review of the data, are listed in exhibit A-1. Users should keep in mind that if these new categories had been offered as response options to all respondents during data collection, it is possible that more respondents would have chosen them."}, {"section_title": "A-8", "text": "Exhibit A-1. to hereinafter as the K-2 PUF, which includes data from the base-year (kindergarten), first-grade, and second-grade data collections. This guide is a supplemental document that describes the edits made to the restricted-use file in order to produce the public-use fie. The K-2 PUF is derived from the K-2 restricted-use file, or RUF, and is identical in format. All the variables from the K-2 restricted-use file are included in the same order on the K-2 public-use file. Like the RUF, the PUF is a child-level file that contains assessment data and parent, teacher, and school information collected for all 18,174 study children who are considered base-year respondents. Data masking techniques were applied to variables in the K-2 RUF to make it suitable for release to researchers without a restricted-use license. These masking techniques, which are described further in the next section, include suppression of sensitive data or variables that apply to only a small subset of study participants, collapsing variable categories, top-or bottom-coding values that are unusually low or unusually high, and converting continuous variables to categorical variables. These techniques are applied to the data to minimize the risk that any study participant can be identified using the information provided in the data file about them."}, {"section_title": "Masked Variables", "text": "As noted above, the masking techniques used to produce the ECLS-K:2011 public-use data file include variable recoding and suppression. The purpose of masking is to provide data in a format that minimizes the potential for a respondent to be identified because of that respondent's characteristics or a unique combination of characteristics. For example, there is potential for the principal of a school to be identified if the ZIP code of that school, the number of students in the school, and the age and race/ethnicity of that principal are all provided in the data file. To guard against this potential disclosure, B-2 ZIP code and principal race/ethnicity are suppressed (i.e., not provided) in the PUF, and the number of students in the school and principal age are provided in categories rather than as exact values. There are several types of modifications to variables in the K-2 PUF, as described below. \uf06e Outliers (that is, unusually high or unusually low values) are top-or bottom-coded to prevent identification of unique schools, teachers, parents, and children without affecting overall data quality. The category value labels for variables that are top-and bottom-coded in the PUF are edited versions of the RUF category labels and reflect the new highest and lowest categories. \uf06e Some continuous variables are converted into categorical variables, and some categorical variables have their categories collapsed in the K-2 PUF. Category value labels are provided for continuous variables that are converted into categorical variables."}]