[{"section_title": "List of Tables", "text": ""}, {"section_title": "List of Figures", "text": ""}, {"section_title": "List of Exhibits", "text": "Exhibit Page"}, {"section_title": "1.", "text": "Illustration of hierarchical structure for TIMSS 2015 grade 4 mathematics framework that specifies the subject's content areas and, for each content area, the topics that are covered ............. 2"}, {"section_title": "Section 1: Introduction", "text": "What is the value of reporting results below the content area level? Large-scale assessment programs, such as the Trends in International Mathematics and Science Study (TIMSS) and the Progress in International Reading Literacy Study (PIRLS), employ item response theory (IRT) and marginal estimation methods to estimate student proficiency in specific subjects such as mathematics, science, or reading. Each of these subjects is defined by a framework that specifies the subject's content areas and, within each content area, the topics that are covered. In TIMSS mathematics, the framework specifies three content areas at grade 4 broken down into six topics, as illustrated in exhibit 1. TIMSS reports on student proficiency in a subject in two ways: with overall average scale scores and with the percentage of students who reach defined performance benchmarks (sometimes referred to as achievement levels in other large-scale assessments). TIMSS also reports on student performance for content areas within subjects. However, no estimate of student achievement is reported for any level below the content area. The purpose of this report is to introduce a method for analyzing student achievement at the topic level using data from TIMSS assessments. Reporting topic-level scores in large-scale assessments, such as TIMSS, would be tremendously valuable from both a policy and pedagogical perspective. Topic-level scores could allow one to pinpoint areas in which students are excelling or struggling more specifically than is possible when one looks at just content area scores. Analysis of topic-level scores could reveal the strengths and weaknesses of students in relation to the intended and actualized curricula. 1 Trends in topiclevel scores across years and between countries could also help more fully realize the promise of monitoring system-level performance in an \"actionable\" way. In other words, instead of learning, for example, that U.S. fourth-graders consistently lag behind Korean students in mathematics, or even that they are weaker in geometry compared to their international peers, topic-level scores would allow one to identify the particular topic(s) within geometry in which U.S. students are comparatively weaker and stronger. For countries with a single national curriculum that has remained the same or comparable, such comparisons would open up the possibility of examining trends in student performance in relation to the curriculum over time. Comparisons between all participating countries could also become more meaningful because instead of researchers comparing how mathematics is taught in general, researchers could focus their efforts on comparing specific topic areas of mathematics instruction-for example, looking at how \"fractions and decimals\" or \"patterns and relationships\" are taught-with data on student performance in that specific topic to ground their analysis. Moreover, although these examples and this report have been based on TIMSS, the method introduced here for estimating student proficiency at a \"topic\" level could be applied to other large-scale trend assessments that are curricular-based, such as PIRLS or the National Assessment of Educational Progress (NAEP)."}, {"section_title": "Exhibit 1. Illustration of hierarchical structure for TIMSS 2015 grade 4 mathematics framework", "text": "that specifies the subject's content areas and, for each content area, the topics that are covered"}, {"section_title": "A new approach for estimating student achievement at the topic level", "text": "Researchers have long recognized the value of estimating proficiency at the topic level but have not had a reliable or viable method to do so for large-scale assessments. Among potential methods for estimating student proficiency at the topic level, the simplest approach is to calculate the proportion of items correctly answered in each topic. Using this approach, one can divide the number of items per topic that students answered correctly by the total number of items per topic, and thereby determine the percentage of correct responses per topic. While viable when dealing with a very large number of items, this simple method lacks reliability at the topic level for large-scale assessments because the number of items measuring a topic is relatively small. As Haladyna and Roid (1983) have noted, computing an observed percentage correct score for a test with fewer than 20 items can lead to unreasonable estimates. In addition, with the simple number/proportion correct method, the topic-level scores would need to be computed one test form (or \"booklet\") at a time. Most large-scale assessments feature multiple forms, and these are not equated on the number/proportion correct metric. Therefore, observed performance in a topic on a given form would not necessarily reflect what the performance would be on a larger and more representative set of items. Given the limitations of the percentage correct scoring approach, IRT is the next logical method to consider. In IRT-based \"domain scoring\" 2 2 Domain score estimates refer to the estimates of student proficiency across the domain of interest, which may be the subject domain (e.g., mathematics) or a subsidiary level of the subject domain (e.g., the topics that make up the mathematics domain). (Skorupski 2008;Bock, Thissen, and Zimowski 1997;Pommerich 2006), the probability of a correct answer (or weighted sum of probability of correct and partially correct answers in the case of polytomous items, which are items that have more than two possible scores) on a set of items is estimated based on the student's \"ability\" and item parameter estimates. This probability is known as model-based item scores (also known as expected item scores in the IRT literature). Summing these probabilities across a \"domain\" of interest provides a domain score. When expressed as a percentage, the domain score can be thought of as \"the index of the proportion of the domain mastered\" (Bock, Thissen, and Zimowski 1997, p. 197). The meaningfulness of a domain score evidently depends on how large and representative of the domain the items in the given domain are. In the case of TIMSS, in any given assessment year, there are only between 12 and 30 items per topic. Furthermore, not all students get all the items in each topic area. Because of booklet spiraling, students may get only a few items for a given topic in the particular test booklet they receive. Thus, with only a very small pool of items completed by each student in each topic in each administration of a large-scale assessment, estimating student performance at the topic level with a domain score approach might not yield reliable and meaningful scores. The tacit assumption behind this conclusion, however, is that topic-level performance has to be calculated within cross-sectional data from a single administration. This is not correct. One of the strengths of IRT is that it allows estimation of the probability of a correct answer on an item for a student with an ability estimate, even if the test form the student took did not include the given item, as long as the parameter estimates of the item are on the same metric as the items that made up the form that the student did take (Lord 1980). Therefore, domain scores at the topic level can be estimated using the larger pool of items accumulated across years and booklets. In other words, just as IRT allows item information gathered from across all administered booklets within a single administration to be used to estimate student performance on items that were not in a student's test form, IRT allows estimation of student performance on items that were not administered in the year they received the assessment. Topic-level scores have never been reported in TIMSS because there have been too few items at the topic level in any single administration to yield meaningful scores. However, the total number of TIMSS items in each topic has increased with each administration of TIMSS, that is, every 4 years. This study's new approach takes advantage of this now extant larger item pool (with its deeper breadth of topic-level items). In this study, probabilities of a correct answer are computed for the U.S. fourth-grade sample that participated in the 2011 grade 4 TIMSS mathematics assessments, with their expected item scores computed for all items in the 2007 and 2011 grade 4 TIMSS mathematics assessments combined. Per Bock, Thissen, and Zimowski (1997), when these expected item scores are summed across items within a topic, the resulting score can be thought of as a student's degree of mastery of the topic. There are several technical conditions that need to be met in order to generate an estimated domain score this way. As discussed in Bock, Thissen, and Zimowski (1997), these conditions include the following: \u2022 The relative weights of items in the item pool in defining the domain of interest are known. \u2022 Item parameters have been estimated in large samples of examinees from a specified population. 3 3 It is important to keep in mind that the condition of a \"specified population\" for IRT may encompass more than demographic parameters for a population. For example, items administered in different years of TIMSS may have been under different educational conditions or curricular requirements, which would effectively mean that responses from different cycles were collected from nonequivalent populations for the purposes of policy analysis. \u2022 The item response models show satisfactory item and model fit. \u2022 The test is composed of items from an item pool that cover the domain of interest. All of these conditions are in fact met in the case of TIMSS data because these are the same conditions that permit the conventional reporting of TIMSS scores by the TIMSS International Study Center (ISC). The relative weight of items in the TIMSS item pool at the topic level can easily be computed using the number of points associated with each subtopic under the given topic. The size of the sample used in item parameter estimation well exceeds the minimum 500 recommended by Bock, Thissen, and Zimowski (1997). TIMSS 2011 item parameters were estimated from a total student sample of over 300,000 at grade 4 (with more than 42,000 student responses per item) (Mullis et al. 2012). Model and item fit are routinely checked in the scaling of the TIMSS student achievement data by comparing the item response function curves generated using the item parameters estimated from the data with the empirical item response functions (Foy, Brossman, and Galia 2013). Finally, since a relatively large number of items, accumulated across two administrations, 4 are used in analyzing achievement in each topic, we assume that the fourth condition listed above is also satisfied in this study. Besides these technical reasons that warrant using an IRT-based domain score approach to estimate student proficiency at the topic level, another compelling reason for using this approach is that for secondary users using standard statistical software, replicating the process of generating IRT-based domain scores at the topic level is relatively straightforward: these users only need item weights 5 TIMSS routinely releases all item parameters as part of its technical reports. In addition, public-use TIMSS data files include, for each student, five plausible values per content area that can easily be converted to a theta metric using the linear transformation coefficients that TIMSS uses in generating the plausible values, which also are available in TIMSS technical reports. Moreover, using an IRT-based domain score approach allows one to generate scores that are relatively simple to interpret, since they can be expressed in terms of the number of points earned in the given domain (e.g., 20 out 25 points) or as a percentage of points earned in the given domain (e.g., 80 percent). \"Domain scores offer the possibility of facilitating interpretation and evaluation of performance, provided the domain has been well defined\" (Pommerich, Nicewander, and Hanson 1999, p. 199). Bock, Thissen, and Zimowski (1997) argue that \"the meaning of a reported percent of domain attainment requires little further explanation\" (p. 208) if the content of the domain has been adequately described. In the case of TIMSS, the framework and the released items help define the domain, satisfying this condition."}, {"section_title": "Section 2: TIMSS Grade 4 Mathematics Assessments", "text": "To understand the method presented in this report, it is important to understand how TIMSS mathematics assessment items are classified, scored, and benchmarked in general. This section explains this key background information."}, {"section_title": "Items and their classification", "text": "The TIMSS mathematics framework describes the mathematics content to be assessed at the target grades by classifying the content into specific categories. For example, exhibit 1 displays how fourth-grade mathematics content is organized in a hierarchical manner based on the most current version of the framework: the TIMSS 2015 mathematics framework. In TIMSS, items are administered using a matrix sampling design so that each student receives 1 of 14 test booklets. Each booklet contains two mathematics item blocks as well as two science blocks. Across the 14 test booklets in the TIMSS 2011 grade 4 mathematics assessment, there were a total of 14 mathematics item blocks. Six of these 14 item blocks were composed of new operational items for 2011; the remaining 8 blocks were composed of items carried forward from the previous TIMSS assessment cycles, called trend items. For each cycle of TIMSS, trend items are the basis for linking the TIMSS assessments and maintaining trends in performance measures over time (Foy, Brossman, and Galia 2013). Table 1 shows the number of mathematics items used in this study, i.e., all items in the 2007 and 2011 TIMSS grade 4 mathematics assessments that have published item difficulty estimates that were calibrated to a common metric. As displayed in the table, the total number of score points 7 7 Dichotomously scored items feature one point. Polytomously scored items can feature two or more points, depending on the rubric used in scoring these items. in the TIMSS 2007 and 2011 grade 4 mathematics assessments ranged from 23 in Points, Lines, and Angles (G1) to 72 in Whole Numbers (N1) at the topic level (see gray columns). The TIMSS mathematics framework is modified slightly for every new administration in order to keep the framework educationally relevant and current. During such updates, some topics and/or objectives may be modified, moved, or replaced. As a result of such changes between 2007 and 2011, TIMSS reclassified some of the 2007 items into the new schema of topics and objectives so that they could be used in the 2011 cycle. However, items that were released in 2007 did not get reclassified by TIMSS because they were no longer being administered in 2011. In this study, all items-released, trend, and new 2011 items-were classified according to the most recent TIMSS framework (2015) by two mathematics experts so that they could be readily analyzed in the future with the TIMSS 2015 items when they are released. 8 8 See appendix A for full details on the process of reclassification. For the purposes of topic-level analyses, all items were reclassified because even the items released after 2007 are of value as they have a theta, or measure of student performance, on a topic that was tested in a TIMSS cycle. Reclassifying these items, in the same way that trend items are reclassified by the ISC, allows these 2007-released items to provide data to support estimates of subscores."}, {"section_title": "Current reporting in TIMSS TIMSS mathematics scale and international benchmarks", "text": "Overall scores and content area scores in TIMSS mathematics assessments are reported on a 0 to 1,000 scale. Because TIMSS uses a matrix-sampling booklet design where each student is administered only a subset of the entire item pool, TIMSS does not report scores at the individual student level. Student achievement is estimated at the group level by the use of IRT scaling and marginal estimation in the following four steps (Mullis et al. 2012): 1. Student responses to individual items are scored, and all items are calibrated and placed onto a common theta (\u03b8) scale. 2. Student achievement distribution is estimated on the theta (\u03b8) metric for the overall mathematics scale and the content areas using the item parameter estimates and the conditioning variables. Design variables (e.g., sampling weights) are incorporated in this estimation."}, {"section_title": "Multiple plausible values 9", "text": "9 For a detailed discussion of plausible values, see Mislevy (1991) and von Davier, Gonzalez, and Mislevy (2009). are drawn from student achievement distributions. 4. The plausible values on the theta (\u03b8) metric are linearly transformed to the reporting metric and recorded in databases for secondary users of TIMSS data. A different set of transformation constants is used for each of the five plausible values (table 2) such that, for student i and draw p, PVip = Ap + Bp * \u03b8ip. In addition to estimating the scale scores, TIMSS also estimates and reports the percentage of students at or above four international benchmarks, or achievement levels: Advanced, High, Intermediate, and Low."}, {"section_title": "Section 3: An Illustration of an IRT-Based Domain Scoring Approach for Estimating Topic-Level Achievement", "text": ""}, {"section_title": "Steps for estimating topic-level achievement", "text": "Once the items in each topic are properly classified (as described in appendix A), the next task is to compute a statistic to summarize student performance for each topic. As mentioned earlier, this is accomplished by estimation of the probability of a correct (and partially correct for polytomous items) answer on all items in a given topic and summing these across items to generate a topic-level score. The following steps, originally proposed by Bock, Thissen, and Zimowski (1997), explain the process in more detail: 1. Compute the weight of each item, in terms of its contribution to the topic-level score, according to the objective and topic it belongs to. 2. Convert each of the five plausible values for each content area for each student to the theta metric (\u03b8ip) using the linear transformation coefficients displayed in table 2. 3. For each item, compute an expected item score using a. IRT parameter estimates for the given item; and b. the theta (\u03b8ip) equivalent of each of the five plausible values obtained in step 2, above. 4. Multiply each of these five expected item scores by the weight computed in step 1 for the respective item. 5. For each topic, sum each of the five weighted expected item scores, one at a time, from the previous step across all items within the topic. This summation produces five model-based topic scores ( ). Dividing each by the sum of item weights for the given topic and multiplying that product by 100 produces model-based topic percent scores ( ). After computing five MTS and five MTPS for each student for each topic, the standard plausible values methodology is applied to these to compute average MTS and average MTPS for the desired student group. A detailed description of these five steps follows."}, {"section_title": "Step 1: Computing item weights", "text": "As discussed earlier, the TIMSS 2007 and 2011 fourth-grade mathematics assessments include three content areas (Number, Geometric Shapes and Measures, and Data Display). Each content area includes multiple topics, and each topic is composed of multiple objectives. To compute the weight of an item that measures a specific objective, the first step is to calculate the ratio of the total points in the item's respective objective to the total points in its respective topic in 2011. For example, of the 19 points available in topic G1 (table 3), 4 were from objective G1.1, corresponding to a ratio of 4/19 = .21 for this objective. We kept this ratio constant even after items from 2007 were brought into the G1 item pool. Then, the weight of each item in a given objective (wj) within a given topic was computed such where n(year) is the number of points in the given objective for the given administration year(s), and N(year) is the total number of points in the given topic that the objective belongs to, for the given administration year(s). All items that measure the same objective receive the same weight. Table 3 shows the weights for items within topic G1 to illustrate this computation. Step"}, {"section_title": "2: Conversion of plausible values to the theta (\u03b8) metric", "text": "In this second step, each plausible value for each content area for each student is converted to the theta (\u03b8) metric by solving for in the following equation: Where is the plausible value p (p = 1, \u2026, 5) for student i in a given content area, and Ap and Bp are the published linear transformation coefficients (table 2) for each plausible value. This results in a total of 15 for each student, 5 from each of the three content areas. Step 3: Computing expected item scores The third step is to compute an expected item score, for each student i for each item j within each topic, based on the item parameter estimates, and the item response model for each item: where h denotes the score category (0 or 1 in dichotomous items and 0, 1, or 2 in polytomous items 10 10 Polytomous items can theoretically assume higher values; however, no polytomous item in our data assumed a higher value than 2. ), is the maximum score attainable for item j, and is the probability of a score of h on item j, given . Note that since ranges from 0 to 2 for polytomous items, these items contribute twice as much to the model-based topic score compared to a dichotomous item. In computing the expected item scores, we used the same item response model that TIMSS used in calibrating the item: \u2022 a three-parameter logistic model (3PL) was used with dichotomously scored multiple-choice items; \u2022 a two-parameter logistic model (2PL) was used with dichotomously scored constructed-response items; and \u2022 a partial credit model was used with polytomous constructed-response items. Note that the item parameter estimates used in the study were estimated and published by TIMSS as part of their operational work. In order to maintain trend, TIMSS estimates the item parameters for the items in the current assessment through a concurrent calibration of the data from the current assessment and from the previous assessment. In 2011, TIMSS concurrently calibrated the 2011 and 2007 items, putting them on the same metric (Foy, Brossman, and Galia 2013). Step 4: Weighting each expected item score In the fourth step, each expected item score is simply multiplied by its weight from step 1. Step"}, {"section_title": "5: Computing model-based topic scores (MTS) and model-based topic percent scores (MTPS)", "text": "In the last step, the products from step 4 are summed across all items within a topic. This summation produces five model-based topic scores (MTSip) (p = 1, \u2026, 5) within each topic: where denotes the weight of item j in the given topic, is the number of items in the given topic, and is the expected item score for student i for item j, given . Dividing each MTSip by the sum of item weights for the given topic and multiplying that product by 100 produces five model-based topic percent scores (MTSPip) (p = 1, \u2026, 5): In computing mean MTS and mean MTPS for the desired student group for a given topic, the standard plausible values methodology was applied. The mean of interest was computed five times, using each of the five MTSip and MTSPip values. The average of these five means produces the mean MTS and mean MTPS, respectively. The variance of this statistic has two components: sampling variance and imputation variance. 11 The standard error of the estimate for the mean is the square root of the sum of these two variance components. The same procedure was followed in computing the mean MTPS for the desired student group for a given topic and its associated standard error."}, {"section_title": "Results", "text": "Because the model-based topic scores are based on item parameters in each topic, we first examined the distribution of the difficulty parameter (b) estimates by topic. Mean estimates for item difficulty parameters ranged from -0.38 (Reading, Interpreting, and Representing) to 0.42 (Fractions and Decimals), with an overall mean of 0.03 and a standard deviation of 0.71. Figure 1 displays the range of the topic-level parameter estimates. An omnibus test across the six topics revealed statistically significant differences (F5, 249 = 4.87, p < .05). In addition, the mean estimate for Fractions and Decimals was higher than the mean estimates for Whole Numbers (t90, 2.98, p < .05) and for Expressions, Simple Equations, and Relationships (t59, 2.69, p < .05). These differences indicated that it is reasonable to expect differences in topic-level performance using the MTS and MTPS metrics. "}, {"section_title": "Achievement profile of U.S. students in terms of topic-level performance", "text": "The mean MTS and MTPS were computed for the U.S. sample that took the 2011 assessment (table 4). 12 12 All design variables, including sampling weights, were incorporated in these analyses. A total of 249 items, corresponding to 261 points in the combined item pool (i.e., all items from TIMSS 2007 and TIMSS 2011), were used to compute these scores. MTS are additive across topics since each item belongs to a single topic. The average MTS across all topics add up to 155.9, indicating that the 2011 U.S. sample would have earned, on average, 59.7 percent of 261 points had they been given the entire pool of 249 items. The mean MTPS add up to 70.3, 59.1, and 57.0 in Data, Geometry, and Numbers, respectively, when aggregated within each content area. At the topic level, the mean MTPS ranged from 52.4 in Fractions and Decimals to 70.3 in Reading, Interpreting, and Representing, showing considerable variation across topics. The mean MTPS were also compared by gender (table B-1) and race/ethnicity (table B-2). The largest difference between the two groups in mean MTPS was observed in Fractions and Decimals (N2), where the means were 53.7 and 50.8 for male and female students, respectively, corresponding to an effect size of 0.09, which is small according to Cohen's (1988) convention for interpreting effect sizes. 13 13 Effect size is computed as the difference in mean MTPS divided by the square root of the pooled variance of the MTPS. Note that statistical significance tests (i.e., t tests) were not conducted for group differences on the MTPS because the subgroups' MTS and MTPS distributions were found to be highly nonnormal, which could be skewed, multimodal, or uniform. Attempts to transform these scores to more normal distributions were not successful. Due to the serious violation of distributional assumption, significance tests were not carried out on the group differences. When analyzed by race/ethnicity, the MTPS add up to a maximum of 70.2 (Asian) and a minimum of 46.6 (Black) across all six topics (table B-2). As with gender, the largest differences in MTPS means were in the topic of Fractions and Decimals, where the difference in means between White and Asian students was 8.9 in favor of Asian students, with a small effect size of 0.29, and the difference in means between White and Black students was 19.1 in favor of White students with a medium effect size of 0.66."}, {"section_title": "Section 4: Conclusion", "text": "Topic-level scores for an assessment such as TIMSS can be generated by estimating how a particular cohort would have performed on items pooled across multiple administrations of the assessment if (a) the items are based on the same framework, (b) the populations across years are equivalent, and (c) the items are calibrated to the same scale. Such analysis opens up the possibility of using IRT-based domain scoring to estimate topic-level scores and to consider the relationships between specific areas of achievement and what is taught (or \"opportunity to learn\") across subgroups as well as across different school settings. An advantage offered by this method is that it is relatively straightforward for a secondary user employing standard statistical software to use it to replicate and produce scores that are relatively simple to interpret, given that they can be expressed in terms of MTS (e.g., 20 out 25 points) or MTPS (e.g., 80 percent). This method also enhances the interpretation of group differences in terms of achievement in more specific terms. For instance, in the case of differences between White and Black students, several studies, including TIMSS, have demonstrated significant gaps. However, we know now, for the first time, where that gap is largest (Fractions and Decimals) and how wide that gap is: 57 percent versus 38 percent mastery of the topic, corresponding to a medium effect size of 0.66. The method explained in this paper has been illustrated with TIMSS 2011 grade 4 mathematics data; however, the same method can be applied to other grades, subjects, and assessments that meet the conditions described in section 1. An inherent limitation to the approach introduced in the study is that domain scores are correlated to the same degree that the theta estimates used to produce them are correlated. This does not, however, change the fact that students' level of mastery of each topic is different.  NOTE: The \"Other\" category consisted of the small numbers of students indicating that they were \"American Indian or Alaska Native\" or \"Native Hawaiian or Other Pacific Islander.\" Race categories exclude students of Hispanic ethnicity. The unweighted sample size for White was 6,137; for Black, 1,408; for Hispanic, 3,325; for Asian, 524; for Two or more races, 482; and for Other, 452. The population size represented by these samples was 1,851,583 for White; 396,028 for Black; 851,257 for Hispanic; 136,398 for Asian; 142,871 for Two or more races; and 130,481 for Other. All design variables, including sampling weights, were incorporated in estimating mean scores. SOURCE: International Association for the Evaluation of Educational Achievement (IEA), Trends in International Mathematics and Science Study (TIMSS), 2007 and 2011."}]