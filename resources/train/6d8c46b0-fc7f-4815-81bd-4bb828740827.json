[{"section_title": "Abstract", "text": "Campbell, M., Congalton, R.G., Hartter, J., Ducey, M. Optimal land cover mapping and change analysis in northeastern oregon using landsat imagery. (2015 "}, {"section_title": "Introduction", "text": "Remote sensing technologies are unparalleled in their ability to monitor and analyze Earth's natural resources rapidly, cost-effectively, and with ever-increasing levels of precision and accuracy (Jensen, 2005) . Although a number of high spatial resolution imagery platforms have emerged in recent years (e.g., Ikonos, QuickBird), the Landsat program has greatly benefited the remote sensing community by providing consistently high quality, medium spatial resolution imagery since 1972 (Green, 2006) . Landsat-5 Thematic Mapper (TM) has proven particularly valuable, having contributed almost 30 years worth of essentially uninterrupted data (well beyond its expected life span of three years) at a bi-monthly temporal resolution (Chander and Markham, 2003) . With Landsat data now freely available, the potential for remote sensing studies of all kinds has exploded as indicated by a 60-fold increase in data downloads since January, 2009 (NASA) .\nCentral to the study of natural resource management is the ability to monitor changes in the landscape over time. The remote sensing community is constantly seeking newer and better ways to accomplish this very goal. Programs like the National Land Cover Database (NLCD) are extremely valuable in providing a baseline of data which can be utilized in studies spanning an array of disciplines (Homer et al., 2004) . Additionally, the NLCD provides a generalized framework by which similar land cover assessments can be accomplished, including a tried-and-true methodology for land cover change analysis (Xian et al., 2009) . Similarly, the National Oceanic and Atmospheric Administration's (NOAA) Coastal Change Analysis Program (C-CAP) has informed this study and others by suggesting a number of standardized techniques by which land cover change can be monitored (Dobson et al., 1995) .\nTraditionally, land cover mapping and analysis was performed on a pixel basis, i.e., a purely spectral approach wherein reflectance values for each pixel (and derivative information) of an image are the sole basis for classifying the imagery into a map. Within the last decade, object-based image analysis (OBIA, also called GEOBIA) has gained momentum in the remote sensing community (Blaschke, 2010) . OBIA is based on segmenting images (i.e., grouping of pixels) into meaningful areas of spatial and spectral homogeneity called \"objects\" (Jensen, 2005) . There is a great degree of user flexibility in generating these objects, guided by the manipulation of three parameters: scale, shape, and compactness to produce the optimal segmentation (e.g., Moller et al., 2007) . While the results tend to be case-specific, there appears to be general agreement that images can be over-segmented (objects are too small) and under-segmented (objects are too large) (Kim et al., 2008; Holt et al., 2009; Liu and Xia, 2010; MacLean and Congalton, 2011) .\nWhile the majority of OBIA studies tend to focus on feature extraction from high-resolution image data (e.g., Moran, 2010; Alganci et al., 2013) , a few have explored its applications on medium-resolution data sources such as Landsat (e.g., Geneletti and Gorte, 2003; Gamanya, 2009 ). An increasing number of studies are inquiring into the feasibility of using OBIA techniques to analyze land cover change (e.g., Im et al., 2008; Chen et al., 2012 ), but we have found few studies that link object-based land cover change and Landsat-5 TM data; Robertson and King (2011) is a notable exception.\nWhile the remote sensing community has consistently pushed the limits of technical and computational capacity, seeking to develop new and improved methodologies, there is a critical need for the implementation of broad-scale monitoring operations that employ relatively simple, repeatable, and comprehensible processes. The focus of this study is precisely that: to establish an analytical and processing workflow for Michael Campbell, Russell G. Congalton, and Mark Ducey are with the Department of Natural Resources & the Environment, 56 College Road, 114 James Hall, University of New Hampshire, Durham, NH 03824 (russ.congalton@unh.edu). a land cover change assessment upon which future studies can be based. In so doing we compare a number of well-established techniques with some new methods using a two-county area in northeastern Oregon as a case study.\nThe objectives of this study are to (a) evaluate pixel-based versus object-based image analysis for a generalized land cover change assessment of medium resolution data (i.e., Landsat Thematic Mapper) at the landscape level, (b) explore a variety of change analysis techniques including a modified principal component analysis to provide the best change maps of the area, and (c) use the optimal/best change analysis method to conduct an assessment of forest harvesting and regeneration from 1986 to 2011."}, {"section_title": "Study Area", "text": "Union and Baker Counties in northeastern Oregon, USA are large counties (13,267km 2 ) with a combined population of 41,882, as of the 2010 Census (Figure 1 ). The region is characterized by a highly varied topography ranging from very mountainous terrain to expansive valley bottoms. Elevations range from 512 m at the lowest point to 2,915 m in the Wallowa Mountains. This region is relatively dry, receiving less than 50 cm average annual precipitation on the valley floors. Large water bodies are relatively few and far between, with only a few notably-sized lakes and rivers being present throughout the two-county area. As a result, forested environments are found only in the higher elevations, where temperatures remain consistently cool enough and the evapotranspirative balance enables tree growth. Despite this relative aridity, cropland is plentiful on the valley bottoms (hay, alfalfa), benefitting from heavy irrigation and fertile Mount Mazama ash soils. In between these two extremes, there is a dominance of two land cover types: grassland and shrub/scrub. The former tends to fill the elevation transition zone between cropland and forest and is often found in drier patches and south-facing slopes within the forested areas. The latter dominates the middle elevations of the southern portion of the study area, forming vast expanses of rolling hills dominated by sagebrush with little to no undergrowth. Almost 40 percent (5,111 km 2 ) of the land in Union and Baker counties is public land, managed by the USDA Forest Service, 522 km 2 of which falls within the Eagle Cap Wilderness area. For the purposes of this study, elevations above 2,000 m and designated wilderness areas were removed from consideration because they are excluded from active forest management and wildfire suppression. It is believed that land cover changes that occur in these areas are simply the result of differential presence/absence of snow and/or other natural disturbance events (e.g., fire). Of interest to this study are only the anthropogenic effects on regional land cover."}, {"section_title": "Methods", "text": ""}, {"section_title": "Reference Data", "text": "Ground-based land cover reference data were collected between the months of June and August in 2011. Global Positioning System (GPS) data were captured using a Trimble YUMA unit and Esri ArcPad 10 software. Sample units were selected based on a few criteria: (a) the sample unit must be \u226590 m \u00d7 90 m in size (3 \u00d7 3 Landsat pixels) (as per the recommendation of Congalton and Green (2009) ) (most units were significantly larger and then the collection was done at or near the center), (b) the entire area must be visually (and spectrally) homogeneous within the unit, (c) the areas must be heterogeneous between units (capturing maximum variability), and (d) the sampling units must be spatially distributed throughout the entire study area.\nA six-class land cover classification scheme was developed based on local knowledge, high resolution photo interpretation, and preliminary unsupervised classifications. These broad classes were designed to best capture the variability across this vast, heterogeneous landscape and to enable the analysis of generalized cover changes that occur in this region. Table 1 shows the land cover classes and their accompanying sample unit totals. The initial goal of collecting at least 100 sample units per class was realized for four out of the six classes. In order to avoid high sample spatial autocorrelation and to minimize spectral redundancy in land cover classes that were fairly sparsely distributed or were found in units of insufficient size, the goal of 100 sample units was not attained for the classes of water and developed. The reference samples were then randomly divided into two groups; data used to train the classification and data used to assess the thematic accuracy of the classification. between the years of 1986 and 2011. In order to capture the seasonality of the highly moisture-and temperaturedependent land cover classes in this region, two images were used for each year of interest. An \"early summer,\" or growing season image and a \"late summer, \" or senescence image were used in the classification process ( Table 2) . As the late summer images ultimately played a more significant role in the classification process, every effort was made to utilize nearanniversary images at or around the end of August into early September. The exception to this rule was the year of 1986, during which the cloud-free, senescence image availability was limited to October. The time frames of the early summer images were more variable, given the typically higher cloud cover present during the growing season. "}, {"section_title": "Image Preprocessing", "text": "For each image date, six of the seven spectral bands (Bands 1 to 5 and 7) were stacked together and adjacent path-rows were mosaicked together. In order to enhance image comparability between dates and reduce the effects of differential topographic illumination, topographic normalization was performed on these mosaicked images. The C-Correction algorithm (Meyer et al., 1993) was selected as the normalization algorithm of choice, given its demonstrated effectiveness (Ria\u00f1o et al., 2003) . The first step in the C-Correction process is to determine the magnitude of illumination across the entire study area, as defined by:\nwhere \u03b3 i is the solar incidence angle relative to the sloped ground surface, \u03b8 z is the solar zenith angle, \u03b1 s is the slope of the ground surface, \u03b4 a is the solar azimuth angle, and \u03b4 o is the aspect of the ground slope. In order to create an illumination surface, slope and aspect layers were derived from a USGS 30-m Digital Elevation Model (DEM). The solar zenith angle and azimuths for each image date were obtained from their respective Landsat header files. In order to assess the effect of illumination on the Landsat DN values, a random sample of 10,000 points was used to extract the spectral and illumination values at each point. A linear regression was run to determine the relative effect of illumination on the \"brightness\" of the pixel in each spectral band. The purpose of C-Correction (and other non-Lambertian normalization techniques) is to normalize the data such that the presumed positive relationship between illumination and DN value would be reduced to a null effect (Meyer et al., 1993) . In order to do so, the CCorrection algorithm was used:\nwhere DN \u03bbi,h is the DN value of a pixel (i) in a given spectral band (\u03bb) on a horizontal surface (h) (with no influence of solar illumination), DN \u03bbi is the value of that pixel on a sloped surface (subject to illumination influence), and c \u03bb is a band-specific parameter defined by slope (m \u03bb ) and y-intercept (b \u03bb ) of the linear regression line between illumination and DN values, such that:\nTo further enhance image comparability and eliminate the effects of atmospheric interference on image data, atmospheric correction was performed on all images. The COST corrected surface was calculated as follows (Chavez, 1988) :\nwhere d is the sun-earth distance, L min and L max are spectral radiance calibration factors, DN i is the DN value at a given pixel i, DN max is the maximum possible DN value (255 for 8-bit data), DN min is the band-specific minimum DN value found through an exploration of the layer histogram (smallest value with \u2265 1000 pixels), and E sun is the solar spectral irradiance. L min , L max , E sun , and d can all be found in Chander & Markham (2003) .\nIn order to improve the accuracy of resultant classifications, a number of commonly used derivative image layers were generated from the topographically and atmospherically corrected images, including the Normalized Difference Vegetation Index (NDVI) and the Tasseled Cap transformation features (Brightness, Greenness, and Wetness).\nThe ten resulting bands (six raw, four derivatives) were then stacked together into a single image. For each year of interest, the early and late summer ten-band images were then stacked together to form a 20-band image. Finally, given the important link between land cover and topography in this region, slope, aspect, and elevation layers were stacked with the 20-band image to create a 23-band spectral and topographic image."}, {"section_title": "Image Segmentation and Classification", "text": "All subsequent image processing and classification took place using Trimble eCognition \u00ae Developer 8.7. An analysis was performed to determine the optimal segmentation parameters needed to attain the highest land cover classification accuracy. Of interest in the segmentation process were two parameters: (a) scale, and (b) shape. Using the multi-resolution segmentation algorithm, a series of image segmentations were performed on the 2011 23-band image. Assigning equal weights to all 23 spectral, derivative, and topographic bands, the image was segmented at every combination of the following parameter settings:\n\u2022 Scale 2-20, intervals of 2 \u2022 Shape 0.0-0.5, intervals of 0.1. There were a number of considerations that went into the determination of these test ranges. In terms of scale, a visual exploration of images segmented at a variety of scales facilitated the determination of 20 as a suitable high-end extreme. Beyond a scale of 20, the segments became exceedingly large and quickly began to lose their within-segment land cover homogeneity (i.e., at a scale of 30, a single polygon could contain Forest, Shrub/Scrub and Grassland). In terms of shape/color, it was believed that spatial qualities of a segment (shape) should never have a stronger influence on determining the size and shape of the segments than the 23 \"spectral\" bands (color). Accordingly, the high end of shape influence was determined to be 0.5 or 50 percent of the segmentation weight.\nEach of the resulting segmentations was examined closely for the input parameters' effects on segment size, and other spatial and spectral characteristics. Of interest to this study was not only the general effect of scale parameter on segment size, but also the relative variation in segment size that PHOTOGRAMMETRIC ENGINEERING & REMOTE SENSING resulted at each scale level. Accordingly, an analysis was performed to explore the relationship between segment size relative standard deviations (RSD) and the scale parameter. Because the segment sizes at large scale parameters will have significantly larger standard deviations, the normalized or relative standard deviation was deemed an appropriate representation of within scale segment size variation. RSD was calculated as such:\nwhere s ij is the sample standard deviation of segment size (in pixels) at a given scale parameter i and shape parameter j, and \u03bc ij is the mean size at those same parameters. The mean RSDs by scale parameter were then calculated. Each of the image segmentations then underwent a separate land cover classification. Land cover classifications were performed in both a pixel-and object-based environment, using a non-parametric classification algorithm (Classification and Regression Tree (CART)) and a parametric classification algorithm (Bayes -Maximum Likelihood). These two approaches were selected because both are commonly used in land cover mapping. The Bayes-Maximum Likelihood classification technique is by far the most used traditional pixel-based method while CART has gained wide use in the last five years. Taking into account all of the segmentation and classification permutations, 240 classifications of the 2011 imagery were performed (10 scale \u00d7 6 shape \u00d7 2 environments \u00d7 2 algorithms = 240 classifications in total). An important distinction between what was being tested in the pixel-and object-based environments must be made here. For both pixel-and object-based classifications, image segments were intersected with training data sample unit centroids (as created through field reconnaissance and photo interpretation) to determine the segment training units. This approach is not unlike using a regiongrowing algorithm or visually defining a training area boundary to maintain homogeneity in the training data selection. In both cases, the classification algorithm was trained with the resultant image segment sample data. In the object-based environment, this trained model was then applied to the remaining, unclassified image segments. In the pixel-based environment, however, the trained model was then applied to the remaining, unclassified pixels on the image, effectively ignoring the boundaries of the remaining segments. So, in essence, the impact of the segment characteristics has a twofold impact on the resultant classification accuracy (training samples and segment classification) in the object-based environment. In the pixel environment, however, the impact is singular, merely affecting the nature of the training data. Additionally, in the object-based environment, a host of segment features can be used to both train the model and classify the imagery, whereas pixels rely purely on the training data's per-band mean values and variances. The input features for object-based analysis were computed in eCognition as follows:\n\u2022 "}, {"section_title": "Accuracy Assessment", "text": "Error matrices (Congalton et al., 1983) were constructed to determine which combination of segmentation parameters, analytical environment and classification algorithm attained the highest accuracies. Overall accuracies, class-specific user's and producer's accuracies, and Kappa were all calculated for each of the 240 classifications (Congalton and Green, 2009 ). An area-based error matrix (MacLean and Congalton, 2012) was used for the 120 object-based classifications. For each combination of CART versus Bayes and object versus pixel, a mean overall accuracy was computed across each scale and shape parameter. The combination of segmentation parameters, classification type, and classification algorithm that produced the highest overall accuracy for the 2011 land cover classification was selected for use in all subsequent classifications (2006, 2001, 1996, 1991, and 1986) following the change detection process described below. Lastly, each land cover map was filtered to a minimum mapping unit of 4,500 m 2 to remove mostly spurious single pixels remaining in the map."}, {"section_title": "Change Detection", "text": "In order to assess changes in the land cover, an image difference was performed. For each five-year interval of interest a ten-band difference image was created based on a simple pixel-by-pixel subtraction between sequential image dates (i.e., image differencing). Following a methodology introduced by Gong (1993) , a principal components analysis (PCA) was performed on the ten-band difference image to create a single principal component (PC1) that would account for most of the variability (change) found in all ten bands. All ten change bands and PC1 were then used individually as the bases for change-based image segmentations to create 11 separate sets of \"potential change segments\" for comparison. Using two standard deviations from the mean as the base threshold for delineating change areas within each land cover class, segments were classified into change and non-change areas.\nThe 11 different change area delineations were the evaluated for correctness. Using a 15,000 ha heavily-logged area in northern Union County as a reference area, change polygons were manually digitized for the 2006 to 2011 interval at a scale of approximately 1:15 000. This scale was selected because it provided sufficient detail for the change analysis. These reference polygons were then compared to each of the 11 change classifications and an area-based 2 \u00d7 2 change-no change error matrix was produced (Congalton and Green, 2009) . With these error matrices, overall accuracies, user's accuracies (errors of omission) and producer's accuracies (errors of commission) were computed to determine which change image produced the best representation of \"actual\" change. Of interest to this study were change detection algorithms with high overall accuracies, and similar user\u1fbfs and producer\u1fbfs accuracies (in the interest of avoiding vast over-or under-estimation of change). The highest accuracy/best change detection band was then selected for further analysis.\nGiven the relatively high overall omission errors using the two-standard deviation threshold across all bands, an analysis of optimal threshold selection was performed using the most accurate single-band change detection method. Assuming that higher thresholds would only result in greater omission errors, four smaller standard deviation-based thresholds were tested for change detection accuracy: 1 SD, 1.25 SD, 1.5 SD and 1.75 SD. Using the same change detection accuracy methods described above, the highest accuracy threshold was chosen for use in the change detection and subsequent classification process."}, {"section_title": "Change Classification", "text": "With the optimal/best change detection methodology in place, a full change classification was performed using the C-CAP change classification protocol (Dobson et al., 1995) . According to this methodology, each image was classified separately backwards in time using training data from non-change areas. For example, the 2011 classification was created using all of the original training data. However, given the land cover changes that occurred between 2006 and 2011, some of the training data collected in 2011 may no longer be valid because of new forest harvesting or younger trees growing into forests. As such, in order to classify the 2006 image, those data that fell within the change areas were removed and replaced via image interpretation. The new training dataset was then used to classify only those areas where change has occurred. This change area classification was then merged back with the non-change-area 2011 classification to form a wall-to-wall 2006 land cover classification. This process was repeated for each interval of interest.\nAdditionally, the same change detection accuracy assessments were performed on each interval, comparing the automatically-detected change areas to manually digitized areas of similarly high logging activity. Last, all of the land cover classifications were compared by five-year interval to determine the changes that have occurred in the landscape. Change matrices were created to assess the types of change occurring and their magnitudes. These changes were also assessed according to the land ownership type in which they fell, including public lands, private industrial lands, and private non-industrial lands. As the changes in the forested environment are of key importance to this study, the 6 \u00d7 6 land cover change matrices were reduced to simple 2 \u00d7 2 forest-non forest matrices to assess forest harvesting and regeneration trends, both across the entire landscape and across different ownership classes"}, {"section_title": "Results and Discussion", "text": "The scale segmentation parameter has a substantial and direct effect on resultant image segment size. In order to obtain a quantitative estimate of this impact, an analysis was performed using the accuracy assessment sample data. For each segmentation performed at incremental levels of the scale parameter, the accuracy assessment sample data were used to obtain a mean value of segment size (in pixels). Figure 2 shows segment size displayed by scale parameter, with each point representing a different shape parameter input. A power function trend line was fitted to the model and a R 2 value was computed. There is a positive relationship between scale parameter and segment size at least up to a scale parameter of 20 for Landsat TM imagery. Beyond a certain scale parameter value, we anticipate that the distribution of resultant segment sizes will reach an asymptote. Where this leveling off occurs, however, will depend on image spatial extent and resolution, and no evidence of an asymptote is apparent over the range of the scale parameter used here.\nA test was performed to explore the relationship between the scale parameter and segment size variability, as measured by the segment size RSD. The results of this test can be seen in Figure 3 , where two notable trends emerge. The first is a peak RSD at the lowest scale parameter of 2 (RSD = 1.03). This suggests that at a scale of 2, high variability in segment size can be expected. This trend declines to a trough at scale of 8, where segment size was the most consistent. Following this low RSD, a slow steady rise in variability emerges as the segment size increases up to the scale parameter maximum of 20.\nThe manipulation of the shape parameter did not result in a predictable distribution of segment sizes. Instead, the tradeoff between shape and color parameters primarily affected the segments' spatial and spectral characteristics, as would be expected. For every combination of scale and shape parameter segmentations, a classification was performed using all four combinations of CART versus Bayes and pixel-based versus object-based classification. Henceforth, CART object-based = CO, CART pixel-based = CP, Bayes object-based = BO, and Bayes pixel-based = BP. As a result, 240 classifications in all were performed and their thematic accuracies were assessed using the traditional error matrix (Congalton et al., 1983) . The overall accuracies for CO, CP, BO and BP were averaged for each different scale parameter segmentation. The resulting mean accuracies can be seen in Figure 4 . In every case, BP produced the highest classification accuracies, with a peak at a scale parameter of 8 and a mean overall accuracy of 90.68 percent. Interestingly, CP, also pixel-based, although consistently less accurate than BP, shares a similar trend, albeit less smooth, with a peak occurring at or around a scale of 8 and a trough at 18. The two object-based classifications, CO and BO similarly share a generalized trend in accuracy across the range of scale parameters. In both cases, there appears to be a fairly distinct positive relationship between the scale parameter and overall classification accuracy. The relationship is certainly stronger in BO than in "}, {"section_title": "PHOTOGRAMMETRIC ENGINEERING & REMOTE SENSING", "text": "CO, but in BO there is a sharp decrease in accuracy at the very last scale parameter tested, 20. While BP greatly outperformed CP, CO almost exclusively outperformed BO, if only slightly.\nSimilarly, the overall accuracies for CO, CP, BO, and BP were averaged for each of the different shape parameter segmentations. The resulting mean accuracies can be seen in Figure  5 . It is important to note that Figures 4 and 5 should be considered together, rather than in isolation of one another, particularly when comparing between classification method accuracies, because these results tend to be similar across the entire ranges of scale and shape parameters, with the order of descending accuracy being roughly equivalent to BP (best), CP, CO, and BO (worst). That being said, these graphs do function as good indicators of within classification method accuracies. The trend lines of scale versus accuracy themselves are believed to be the most revealing. Accordingly, some important trends emerge in Figure 5 as well. The most accurate method, BP, appears to function almost entirely independent of shape, with functionally equal accuracies across the board. However, the marginally highest mean accuracy was produced at a shape parameter of 0.3 (accuracy of 89.96 percent). Conversely, CP, CO, and BO all appear to have an accuracy peak in the 0.1 to 0.3 ranges and a trough in the 0.4 to 0.5 range, with a slight uptick in accuracy at shape 0.5.\nTaking all of these accuracies into consideration, a selection of segmentation parameters (scale and shape), image analysis environment (pixel versus object) and classification algorithm (CART versus Bayes) was made. The optimal combination was found to be Bayes pixel-based classification with training samples segmented at a scale of 8 and a shape of 0.3 (overall accuracy of 91.48 percent, and Kappa = 0.897). The error matrix with class-specific user's and producer's accuracies can be seen in Table 3 . The final 2011 land cover classification can be seen in Plate 1. Plate 1. 2011 land cover classification of the study area.\nTo determine the optimal change detection technique, the first change interval of interest, 2006 to 2011 was used for analysis. Ten separate difference images and one principal components image were tested to see which produced the best change detection accuracy. The PCA was performed on the ten-band difference image to capture as much change across all of the input bands as possible into a single band (approximately 70 percent of the change variance is captured in PC1). Using the most accurate 2011 land cover classification, a within-class segmentation was performed for each of the 11 change bands of interest (ten difference bands and PC1). From the resultant segments, a distribution of classspecific change values emerged. For each band and class, the change distributions resembled a normal distribution and the class-specific differences visualized in the spread of change magnitudes. In order to determine change thresholds, the class-specific change means and standard deviations were calculated for each band.\nUsing two standard deviations from the mean as a base threshold for change, each band was then tested for its ability to accurately detect change. These class-specific band threshold values were applied to the binary classification of change versus non-change for the 2006 to 2011 interval. As a result, 11 different classifications were performed and assessed for accuracy using an error matrix approach. Band 7 (middle infrared) was determined to be the optimal band for use in the change analysis given the preferential emphasis placed on minimizing errors of omission and highest overall performance. Given that change omission and commission errors can be seen as a direct product of the change threshold used (i.e., a higher standard deviation-based change threshold will likely produce greater omission error and a lower threshold will produce increased errors of commission), band 7 was then further evaluated for a range of standard deviation change thresholds (1 SD to 2 SD, intervals of 0.25 SD). The results show that the best change analysis occurred at 1.75 SD, and this threshold was selected for all further use (Table 4) . Band 7 was used to classify change and non-change areas for each five-year interval of interest iteratively backwards in time starting with 2006 to 2011 and ending with 1986 to 1991. Based on this change analysis, land cover classifications were performed only on the detected change areas for each year. These change area classifications were then merged with the corresponding year's classification to attain wall-to-wall classification. The resulting classifications were intersected to assess class-specific land cover classification changes. Areas were calculated in hectares to determine change magnitude.\nThese change maps were then simplified to forest and non-forest changes in order to further study forest harvesting and regrowth patterns. Four combinations resulted: forest to forest (non-change), forest to non-forest (change), non-forest to non-forest (non-change), and non-forest to forest (change). Forest to non-forest changes were assumed to be the result of harvesting and non-forest to forest changes were assumed to represent forest regeneration. These totals were then intersected with land ownership data to determine owner-specific changes. The forest to non-forest totals and ownership breakdown can be seen in Figure 6 . A few definitive trends emerge. In terms of overall forest harvesting, the first two time intervals (1986 to 1991 and 1991 to 1996) saw very similar total hectares removed at slightly below 8,500 ha each. Following these early highs, a precipitous drop occurred between 1996 and 2001, when only 2,126 ha were removed in total. The final two intervals saw consistently increasing totals with 5,477 ha removed between 2001 and 2006, and 9,227 ha removed in the most recent interval, reaching the highest total of any interval tested. In terms of ownership-specific patterns, some clear trends can be seen as well. A notable decrease in harvesting on public land occurred between 1986 and 2001 (1986 to 1991: 6,242 ha; 1991 to 1996: 3,434 ha; 1996 to 2001: 749 ha) , followed by a less aggressive, steady increase between 2001 and 2011. Harvesting on private industrial land saw significant increases between the 1986 to1991 interval (402 ha removed) and the 2006 to 2011 interval (3,975 ha removed). Private non-industrial land typically saw relatively low harvesting totals, with the one exception being between 1991 and 1996 where 3,603 ha were removed.\nThese results however, should be viewed with the understanding of differential total forest land ownership. For example, in 2011, there were 418,144 ha of forested land throughout the entire study area, 312,284 ha (74.68 percent) is owned by public entities (most of which is USFS), followed by private, non-industrial land owners (77,732 ha, 18.59 percent), and last, private industrial (28,127 ha, 6.73 percent). Accordingly, these removal totals were divided into total forested land ownership to compute the \"normalized\" or percent by ownership removal. The resulting removal percentages can be seen in Figure 7 .\nThe forest and non-forest change classification process not only yields change areas that suggest forest removal, but additionally forest areas that are regenerated (non-forest to forest). From the forest management perspective, this variable is in many ways as valuable, if not more so, than the harvesting totals. Accordingly, forest regeneration totals were calculated across the entire study area and, again, broken down by land ownership class. The results of these analyses can be seen in Figure 8 . The total forest regeneration across all ownership classes does not take on any major trend in the positive or negative direction, with the exception of a steep decline in the 1991 to 1996 interval, which makes sense, given the heavy harvesting that occurred in that year. The ownership-specific trends, however, are of interest. For instance, again with the exception of 1991 to 1996, regeneration on public land has steadily declined. Conversely, both kinds of private land have seen somewhat steady growth in forest regeneration from the 1991 to 1996 interval to 2006 to 2011.\nsuch as this study, however, perhaps the noise reduction caused by grouping of pixels over large areas (OBIA) would produce a more desirable result. This study was not intended to determine outright whether pixel-based analysis or objectbased analysis is preferable. The results depended heavily on the classification algorithm used. Across the entire range of scale and shape parameters, Bayes pixel-based classification significantly outperformed Bayes object-based classification and had the highest overall accuracy. However, the relationship between CART pixel-based and object-based classifications was much more heavily influenced by the segmentation parameters used.\nFinally, detailed, quantitative accuracy assessment formed the basis for not only the individual date land cover maps, but also the land cover change detection analysis and the detailed forest harvesting and regeneration conducted as part of this study. The primary application of interest in this study involved detecting and classifying changes in the forested environments of a two-county area in northeastern Oregon. The results highlight predominant trends in overall and ownership-specific changes in total forested area throughout this region over a 25-year time span at five-year intervals. Three main trends in forest harvesting practices emerge. In terms of overall change, we see that the greatest amount of forest removal occurred in the most recent interval, 2006 to 2011; in total, 9,227 ha of forest were removed. This total decreases to 1996 to 2001 where an estimated 2,127 ha of forest was removed. This total then climbs back up to a plateau for the intervals of 1986 to 1991 and 1991 to 1996 where 8,311 ha and 8,394 ha were removed, respectively. In addition to the overall forest harvesting trends, two ownership-specific trends emerge: (a) an increase in private industrial harvesting, and (b) an initial decrease in public land harvesting followed by a slower increase from 1986 to 2011. These trends are likely the result of a variety of factors. Speculation into the social, economic, and political mechanisms at work that have resulted in this shift from predominantly public land harvesting to primarily private industrial warrants an entire study in and of itself. However, one important geospatial factor that is immediately relevant is that all timberlands are not equally harvestable. The ability to harvest timber from a given location in a forest depends primarily on three factors: (a) accessibility, (b) topography, and (c) rules and regulations. Accessibility is simply the ability for a logger to reach a given area of timber, i.e., a factor that is controlled by the specific locations and densities of the forest road network. Closely related to accessibility is the quality of the terrain, or topography, of the timberlands. Some areas are simply too steep or otherwise impeded by natural, geologic features to harvest timber. And last, there are a variety of legislative and regulatory road blocks to a variety of logging operations, particularly relating to the preservation of wilderness and protection of endangered species. For instance, riparian environments are often protected against logging due to their importance in the preservation of certain fish species that could be harmed by increased runoff and/or other industrial pollutants thought to be caused by logging operations. Taking all of these factors together, a scenario can readily be imagined wherein private industrial timberlands, which tend to be on lower-lying elevations with less dramatic topography, having higher road densities and fewer regulatory impediments, are simply more harvestable than, for example, public lands. Accordingly, this study reveals ownership-specific trends that are related to the degree to which forested areas are harvestable."}, {"section_title": "Conclusions", "text": "This study had a wide-ranging set of objectives, in terms of both remote sensing methods and real world applications; the study utilized a largely exploratory approach to determining the optimal conditions for conducting efficient land cover classification and change detection. In incremental fashion, each procedure in the process was carefully vetted for optimal accuracy. Only when conditions were met to attain an acceptably high analytical accuracy was forward progress made. While the specific results of any remote sensing study are only immediately applicable to that study, certain broader trends can emerge upon which future analyses can be based. The incremental approach used here can function not only as a framework for future investigation, but because the methods were explored using such a wide range of input parameters, a number of the specific results can help inform future research as well.\nOf particular interest in this study is the analysis of pixelbased versus object-based image classification. While OBIA has become often used for high spatial resolution imagery, few studies have documented the utility of using OBIA on medium resolution image datasets such as Landsat-5 TM. This absence is not without justification; Landsat\u1fbfs 30 m pixels are, in many ways, image objects in their own right and have historically been very successful in land cover analyses of all kinds. For a land cover study conducted over a relatively small area with a fairly detailed classification scheme, a 30 m pixel may sufficiently reduce the spectral noise contained within an image to produce accurate, functional ground units, despite their indiscriminant spatial placement. At the regional or landscape scale with more generalized classes "}]