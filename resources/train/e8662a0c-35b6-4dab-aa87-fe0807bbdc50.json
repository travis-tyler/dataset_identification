[{"section_title": "Abstract", "text": "Motivation: The identification of quantitative trait loci (QTL) is critical to the study of causal relationships between genetic variations and disease abnormalities. We focus on identifying the QTLs associated to the brain endophenotypes in imaging genomics study for Alzheimer's Disease (AD). Existing research works mainly depict the association between single nucleotide polymorphisms (SNPs) and the brain endophenotypes via the linear methods, which may introduce high bias due to the simplicity of the models. Since the influence of QTLs on brain endophenotypes is quite complex, it is desired to design the appropriate non-linear models to investigate the associations of genotypes and endophenotypes. Results: In this paper, we propose a new additive model to learn the non-linear associations between SNPs and brain endophenotypes in Alzheimer's disease. Our model can be flexibly employed to explain the non-linear influence of QTLs, thus is more adaptive for the complex distribution of the high-throughput biological data. Meanwhile, as an important computational learning theory contribution, we provide the generalization error analysis for the proposed approach. Unlike most previous theoretical analysis under independent and identically distributed samples assumption, our error bound is based on m-dependent observations, which is more appropriate for the high-throughput and noisy biological data. Experiments on the data from Alzheimer's Disease Neuroimaging Initiative (ADNI) cohort demonstrate the promising performance of our approach for identifying biological meaningful SNPs."}, {"section_title": "Introduction", "text": "Alzheimer's Disease (AD) is the most common form of dementia, which triggers memory, thinking and behavior problems. The genetic causal relationship of AD is complex (Avramopoulos, 2009 ) and therefore presents difficulties in the prevention, diagnosis and treatment of this disease. Recent advances in multimodal neuroimaging and high throughput genotyping and sequencing techniques bring an emerging research field, imaging genomics, which provides exciting new opportunities to ultimately improve our understanding of brain disease, their genetic architecture and their influences on cognition and behavior.\nThe rapid progress in neuroimaging techniques has provided insights into early detection and tracking of neurological disorders (Weiner et al., 2013) . Later research interest in imaging neuroscience has focused on Genome Wide Association Studies (GWAS) to examine the association between genetic markers, called Single Nucleotide Polymorphisms (SNPs), and imaging phenotypes (Cooper-Knock et al., 2014; Waring and Rosenberg, 2008) , with the V C The Author(s) 2018. Published by Oxford University Press. All rights reserved. For permissions, please e-mail: journals.permissions@oup.com i866 Bioinformatics, 34, 2018, i866-i874 doi: 10.1093/bioinformatics/bty557 ECCB 2018\ngoal of finding explanations for the variability observed in brain structures and functions. However, these research works typically study associations between individual SNPs and individual phenotypes and overlook interrelated structures among them. To better understand the genetic causal factors of brain imaging abnormalities, previous works have laid great emphasis on identifying relevant QTL (Ryan et al., 2016; Vounou et al., 2010) , which related high-throughput SNPs to imaging data and enhanced the progress and prosperity of neuroscience research. Several machine learning models were established to depict the relations between SNPs and brain endophenotypes (Huo et al., 2018; Wang et al., 2012a; Wang et al., 2017; Yang et al., 2015; Zhu et al., 2016) . In Wang et al. (2012a) , Zhu et al. (2016) , Wang et al. (2017) and Huo et al. (2018) , the authors used the low-rank learning models or structured sparse learning models to select the imaging features that share common effects in the regression analysis. Yang et al. (2015) applied the LASSO regression model to discover the significant SNPs that are associated with brain imaging features. However, previous works use linear models to predict the relations between genetic biomarkers and brain endophenotypes, which may introduce high bias during the learning process. Since the influence of QTL is complex, it is crucial to design appropriate non-linear model to investigate the genetic biomarkers (due to the limited size of biological data, deep learning models don't work well for our problem). Besides, most previous computational models on genotype and phenotype studies did not provide theoretical analysis on the performance of the models, thus leaves uncertainty in the validity of the models.\nTo tackle with these challenging problems, in this paper, we propose a novel and efficient nonlinear model for the identification of QTL. We apply our model to the QTL identification of Alzheimer's disease (AD), the most common cause of dementia. By means of feedforward neural networks, our model can be flexibly employed to explain the non-linear associations between genetic biomarkers and brain endophenotypes, which is more adaptive for the complicated distribution of the high-throughput biological data. We would like to emphasize the following contributions of our work:\n\u2022 We propose a novel additive model with generalization error analysis. In particular, different from conventional analysis with independent samples, our error bound is under m-dependent observations, which is a more general assumption and more appropriate for the high-throughput complex genotypes and phenotypes.\n\u2022 Our model is efficient in computation. The time complexity of our model is linear to the number of samples and number of features in the data. Experimentally we showed that it only takes a few minutes to run our model on the ADNI data.\n\u2022 Experimental results demonstrate that our model not only identifies several well-established AD-associated genetic variants, but also finds out new potential SNPs.\nNotation:\nThroughout this paper, unless specified otherwise, upper case letters denote matrices, e.g. X, Y. Bold lower case letters denote vectors, e.g. w; b. Plain lower case letters denote scalars, e.g. a, c. w i denotes the i-th element of vector w. w i denotes the i-th row of matrix W. \n. jjWjj 1 denotes the ' 1 norm: "}, {"section_title": "Related work", "text": "In QTL identification of brain imaging abnormalities, the goal is to learn a prediction function which estimates the imaging feature matrix Y \u00bc y 1 ; y 2 ; . . . ; y n \u00bd T 2 R n\u00c2c given the genetic information\nMeanwhile, we want to weigh the importance of each SNP in the prediction according to the learning model. The most straightforward method is least square regression, which learns a weight matrix W 2 R d\u00c2c to study the relations between SNPs and brain endophenotypes. W is an intuitive reflect of the importance of each SNP for the prediction of each endophenotype. Based on least square regression, several models were proposed for QTL identification. In Tibshirani (1996) and Yang et al. (2015) , the authors employed sparse regression models for the discovery of predominant genetic features. In Fazel (2002) and Wang et al. (2012a) , low-rank constraint was imposed to uncover the group structure among SNPs in the association study.\nIn the identification of QTL, previous works mainly use linear models for the prediction. However, according to previous studies, the biological impact of genetic variations is complex (MeyerLindenberg et al., 2006) and the genetic influence on brain structure is complicated (Peper et al., 2007) . Thus, the relations between genetic biomarkers and brain-imaging features may not be necessarily linear and the prediction with linear models is likely to trigger large bias.\nTo depict the non-linear association between genetic variations and endophenotypes, neural networks introduce a convenient and popular framework. Schmidt et al. (1992) proposed feed forward neural networks with random weights (FNNRW), which can be formed as:\nwhere\nin the hidden layer for t-th hidden node, b t 2 R is the corresponding bias term, hv t ; xi \u00bc P d j\u00bc1 v tj x j represents Euclidean inner product, / :\n\u00f0 \u00de is the activation function and a t 2 R is the weight for the t-th hidden node.\nAs is analyzed in Igelnik and Pao (1995) and Rahimi and Recht (2009) , FNNRW enjoys an obvious advantage in computational efficiency over neural nets with back propagation. In Equation (1), v t and b t are randomly and independently chosen before hand, and the randomization in parameter largely relieves the computational burden. FNNRW is aimed at estimating only the weight parameter a t j h t\u00bc1 thus is extremely efficient. Such property makes FNNRW more appropriate for analysis of the high-throughput data in Alzheimer's research. Rahimi and Recht (2009) constructed a classifier using FNNRW where they conduct classification on the featurized data as shown in Equation (1). The classification model can be easily extended to the regression scenario with the objective function formulated as:\nwhere c is the hyper-parameter for the regularization term and a t \u00bc a 1 ; a 2 ; . . . ; a c \u00bd 2 R c is the weight parameter of the t-th hidden node for c different endophenotypes. As discussed above, Problem (2) can be adopted to efficiently estimate the nonlinear associations between genetic variations and brain endophenotypes. However, since the parameters of hidden layer is randomly assigned, traditional FNNRW model makes it hard to evaluate the importance of each feature.\nTo tackle with these problems, we propose a novel additive model in next section, which not only maintains the advantage of computational efficiency of FNNRW but also integrates the flexibility and interpretability of additive models."}, {"section_title": "New additive model for identifying quantitative trait loci of brain endophenotypes", "text": "We propose new Additive Model via Feedforward Neural networks with random weights (FNAM) as:\nwhere we distinguish the contribution of each feature x j and formulate the model in an additive style for the prediction. Similar to that of FNNRW, we propose to optimize the least square loss between the ground truth endophenotype matrix Y and the estimation f a (X) with ' 2 -norm penalization, then we propose the following objective function:\nFor simplicity, if we define A \u00bc a 1 ; a 2 ; . . . ; a h \u00bd T 2 R h\u00c2c as the weight parameter for hidden nodes, and G 2 R n\u00c2h such that\n6 6 6 6 6 6 6 6 6 4 3 7 7 7 7 7 7 7 7 7 5 ;\nthen we could rewrite our objective function Problem (4) as:\nTake derivative w.r.t. A in Problem (6) and set it to 0, we get the closed form solution of A as below:\nAs discussed in the previous section, one obvious advantage of FNAM over FNNRW is that FNAM considers the role of each feature independently in the prediction, thus makes it possible to interpret the importance of each SNP in the identification QTL, which is a fundamental goal of jointly studying genetic and brain imaging features.\nHere, we discuss how to estimate the role of each feature in FNAM. To separate the contribution of each feature, we rewrite Equation (3) as below:\nwhich indicates that the prediction function f a (X) can be regarded as the summation of d terms, where the j-th term P h t\u00bc1 / v tj x j \u00fe b t 1 \u00c0 \u00c1 a t denotes the contribution of the j-th feature.\nNaturally, if we normalize the magnitude of the j-th term with the ' 2 -norm of x j , we could get a good estimation of the significance of the j-th feature. As a consequence, we could define a weight matrix W 2 R d\u00c2c to show the importance of the d SNPs in the prediction of the c imaging features, respectively, such that:"}, {"section_title": "Time complexity analysis", "text": "We summarize the optimization steps of FNAM in Algorithm 1. In Algorithm 1, the time complexity of Step 1 (computing "}, {"section_title": "Generalization ability analysis", "text": "In this section, based on the real situation of biological data, we provide theoretical analysis on the approximation ability of our FNAM model and derive the upper bound of generalization error.\nIn most previous works, theoretical analysis is based on the hypothesis of independent and identically distributed (i.i.d.) samples. However, the i.i.d. sampling is a very restrictive concept that occurs only in the ideal case. As we know, the acquisition of\nof hidden nodes h, parameter c."}, {"section_title": "Output:", "text": "Weight matrix A 2 R h\u00c2c for the hidden nodes. Weight matrix W 2 R d\u00c2c showing the relative importance of the d\nSNPs in the prediction. Initialize the weight matrix V 2 R h\u00c2d randomly according to uniform distribution U 0; 1 \u00f0 \u00de. Initialize the bias term b 2 R h randomly according to uniform distribution U 0; 1 \u00f0 \u00de. 1. Compute G matrix according to the definition in Equation (5). 2. Update A according to the solution in Equation (7) 3. Compute W according to the definition in Equation (9).\nhigh-throughput biological data involves complicated equipments, reagents as well as precise operation of highly trained technicians, which usually introduce variations to the data during the measurement process (Leek et al., 2010) . Thus, the i.i.d. sampling assumption is not appropriate for the high-throughput biological data analysis. In this section, we provide a learning rate estimate of our model in a much general setting, i.e. m-dependent observations (Modha and Masry, 1996) .\nFor simplicity, here we consider the prediction of only one brain endophenotype y \u00bc y 1 ; y 2 ; . . . ; y n \u00bd T 2 R n , which could be easily extended to the case with multiple endophenotypes. Besides, we incorporate the bias term b into the weight matrix V by adding one feature valued 1 for all samples to the data matrix X. For analysis feasibility, we reformulate the general FNAM model as below. Let Z \u00bc X \u00c2 Y, where X is a compact metric space and\n2 Z n and each j 2 1; 2; . .\n. from a distribution l on 0; 1 \u00bd . The FNN with random weights in FNAM can be formulated as the following optimization problem:\nThe predictor of FNAM is:\nTo investigate the generalization error bound of FNAM, we rewrite it from a function approximation viewpoint.\nDefine the hypothesis function space of FNAM as:\nand for any j 2 1; 2; . .\nThen, FNAM can be rewritten as the following optimization problem:\nFor the regression problem, the goal of learning is to find a prediction function f : x ! R such that the expected risk\nis as small as possible. It is well known that the Bayes function\nis the minimizer of E f \u00f0 \u00de over all measurable functions. Therefore, the excess expected risk E f \u00f0 \u00de \u00c0 E f q \u00c0 \u00c1 is used as the measure to evaluate the performance of learning algorithm.\nSince Y & \u00c0k; k \u00bd and kf q k 1 k, we introduce the clipping operation\nto get tight estimate on the excess risk of FNAM. Recall that FNAM in (4) depends on the additive structure and random weighted networks. Indeed, theoretical analysis of standard random weighted networks has been provided in Igelnik and Pao (1995) and Rahimi and Recht (2009) to characterize its generalization error bound. However, the previous works are restricted to the setting of i.i.d. samples, and do not cover the additive models. Hence, it is necessary to establish the upper bound of E p f z \u00f0 \u00de \u00f0 \u00de\u00c0E f q \u00c0 \u00c1 with much general setting, e.g. m-dependent observations (Modha and Masry, 1996; Vidyasagar, 2013) . Now, we introduce some necessary definitions and notations for theoretical analysis.\nbe a stationary random process on a probability space X; A; P \u00f0 \u00de . Denote A \nLet f z be defined in (4) associated with m-dependent\nThere holds \nwhich means the proposed algorithm is consistency. The current result extends the previous theoretical analysis with i.i.d samples (Igelnik and Pao, 1995; Rahimi and Recht, 2009) to the m-dependent observations. Indeed, we can also obtain the error bound for strong mixing samples by the current analysis framework.\nThe following Bernstein inequality for m-dependent observations [Theorem 4.2 in Modha and Masry (1996) ] is used for our theoretical analysis.\nLemma 2:\ni\u00bc1 be a stationary m-dependent process on probability space X; A; P \u00f0 \u00de . Let w : R ! R be some measurable The covering number is introduced to measure the capacity of hypothesis space, which has been studied extensively in (Cucker and Smale, 2001; Cucker and Zhou, 2007; Zou et al., 2009) .\nThe covering number N F; \u00f0 \u00de of a function set F is the minimal integer l such that there exists l disks with radius covering F .\nConsidering the hypothesis space M h in Section 4, we define its subset\nNow we present the uniform concentration estimate for f 2 B R Lemma 3:\nProof:\n. It is easy to verify that jU i j 8k 2 and EU i \u00bc 0. From Lemma 1 we obtain, for any given m-dependent samples z \u00bc x i ; y i \u00f0 \u00de f g n i\u00bc1 2 Z n and measurable function f,\nIt means that\nThis completes the proof. u"}, {"section_title": "Proof of Theorem 1:", "text": "According to the definition of E f \u00f0 \u00de and f q , we deduce that\nwhere\nNow we turn to bound E 1 in terms of Lemma 2. According to the definition of f z , we get\nIt means that f z 2 B R with R \u00bc k ffi ffi Integrating these facts into Lemma 2, we obtain:\nThen, for any g! 64k 2 n m \u00f0 \u00de ,\n:\nFrom this equation, we can deduce that\nHence, On the other hand, the definition f z tells us that\nCombining Equations (10) and (11), we get the desired result in Theorem 1. u"}, {"section_title": "Experimental results", "text": "In this section, we conduct experiments on the ADNI cohort. The goal of QTL identification is to predict brain imaging features given the SNP data. Meanwhile, we expect the model to show the importance of different SNPs, which is fundamental to understanding the role of each genetic variant in Alzheimer's disease."}, {"section_title": "Data description", "text": "The data used in this work were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). One of the goals of ADNI is to test whether serial magnetic resonance imaging (MRI), positron emission tomography (PET), other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment (MCI) and early AD. For the latest information, see www.adni-info.org. The genotype data (Saykin et al., 2010) for all non-Hispanic Caucasian participants from the ADNI Phase 1 cohort were used here. They were genotyped using the Human 610-Quad BeadChip. Among all the SNPs, only SNPs within the boundary of 620K base pairs of the 153 AD candidate genes listed on the AlzGene database (www.alzgene.org) as of April 18, 2011 (Bertram et al., 2007) , were selected after the standard quality control (QC) and imputation steps. The QC criteria for the SNP data include (i) call rate check per subject and per SNP marker, (ii) gender check, (iii) sibling pair identification, (iv) the Hardy-Weinberg equilibrium test, (v) marker removal by the minor allele frequency and (vi) population stratification. As the second pre-processing step, the QC'ed SNPs were imputed using the MaCH software (Li et al., 2010) to estimate the missing genotypes. As a result, our analyses included 3123 SNPs extracted from 153 genes (boundary: 620KB) using the ANNOVAR annotation (http://www.openbioinformatics.org/annovar/). As described previously, two widely employed automated MRI analysis techniques were used to process and extract imaging phenotypes from scans of ADNI participants (Shen et al., 2010) . First, Voxel-Based Morphometry (VBM) (Ashburner and Friston, 2000) was performed to define global gray matter (GM) density maps and extract local GM density values for 90 target regions. Second, automated parcellation via FreeSurfer V4 (Fischl et al., 2002) was conducted to define volumetric and cortical thickness values for 90 regions of interest (ROIs) and to extract total intracranial volume (ICV). Further details are available in (Shen et al., 2010) . All these measures were adjusted for the baseline ICV using the regression weights derived from the healthy control (HC) participants. All 749 participants with no missing MRI measurements were included in this study, including 330 AD samples, and 210 MCI samples and 209 health control (HC) samples. In this study, we focus on a subset of these 90 imaging features which are reported to be related with AD. We extract these QTs from roughly matching regions of interest (ROIs) with VBM and FreeSurfer. Please see (Wang et al., 2012b) for details. We select 26 measures for FreeSurfer, 36 measures for VBM and summarize these measures in Tables 1 and 2 ."}, {"section_title": "Experimental setting", "text": "To evaluate the performance of our FNAM model, we compare with the following related methods: LSR (Least square regression), RR (Ridge regression), Lasso (LSR with ' 1 -norm regularization), Trace (LSR with trace norm regularization) and FNNRW (Feedforward neural network with random weights), where we consider the Frobenius norm loss in the R emp term of Rahimi and Recht (2009) for regression problem. We add a comparing method, FNNRW-Linear (FNNRW using linear activation function), which use linear activation function / x \u00f0 \u00de \u00bc x to illustrate the contribution of the nonlinearity of activation function.\nAs for evaluation metric, we calculate root mean square error (RMSE) and correlation coefficient (CorCoe) between the predicted value and ground truth in out-of-sample prediction. We normalize the RMSE value via Frobenius norm of the ground truth matrix. In comparison, we adopt 5-fold cross validation and report the average performance on these five trials for each method.\nWe tune the hyper-parameter of all models in the range of 10 \u00c04 ; 10 \u00c03:5 ; . . . ; 10 4 \u00c8 \u00c9 via nested 5-fold cross validation on the training data, and report the best parameter w.r.t. RMSE of each method. For methods involving feedforward neural networks, i.e. FNNRW, FNNRW-Linear and FNAM, we set h \u00bc 50. For FNNRW and FNAM, we set / :\n\u00f0 \u00de as the tanh function which maps the input to \u00c01; 1 \u00bd . "}, {"section_title": "Performance comparison on ADNI cohort", "text": "We summarize the RMSE and CorCoe comparison results in Table 3 .\nFrom the results we notice that FNAM outperforms all the counterparts in both FreeSurfer and VBM. Besides, from the comparison between Lasso, Trace and FNAM, we find that the assumptions imposed by Lasso (assumption of sparse structure) and Trace (low-rank assumption) may not be appropriate when the distribution of the real data does not conform to such assumptions. In contrast, FNAM is more flexible and adaptive since FNAM does not make such structure assumption on the data distribution. Moreover, from the comparison between FNNRW, FNNRW-Linear and FNAM, we find that both FNNRW and FNAM outperform FNNRW-Linear, which demonstrates the importance of the nonlinearity introduced by the activation function. FNNRW-Linear only involves linear functions, thus is not able to show the non-linear influence of QTL. As for FNNRW, we deem that the reason for FNAM to perform better than FNNRW lies in the additive mechanism of FNAM. Since FNNRW incorporates all features in each computation, it seems too complex for the prediction thus brings about high variance."}, {"section_title": "Important SNP discovery", "text": "Here, we look into the significant SNPs in the prediction. According to the definition in Equation (9), we calculate the importance of each SNP and select the top 10 SNPs that weigh the most in VBM analysis. We plot the weight map and brain map of the top 10 SNPs in Figure 1 . From the results, we notice that ApoE-rs429358 ranks the first in our prediction. As the major known genetic risk factor of AD, ApoE has been reported to be related with lowered parietal (Small et al., 2000) , temporal (van der Flier et al., 2011) and posterior cingulate cerebral glucose metabolism (Liang et al., 2008) of AD patients. Moreover, we present the LocusZoom plot (Pruim et al., 2010) for the SNPs close to LIPA gene (10M boundary) in Chromosome 10 to show the ADassociated region around LIPA-rs885561 in Figure 2 . Similar to ApoE, LIPA gene is also known to be involved in cholesterol metabolism (Papassotiropoulos et al., 2005) , where elevated cholesterol levels lead to higher risk of developing AD. In addition, we detect other SNPs that are established AD risk factors, e.g. rs1639-PON2 (Shi et al., 2004) and rs2070045-SORL1 (Rogaeva et al., 2007) . Replication of these results demonstrate the validity of our model.\nWe also pick out SNPs with potential risks whose influence on AD has not been clearly revealed in literature. For example, rs727153-LRAT is known to be related with several visual diseases, including early-onset severe retinal dystrophy and Leber congenital amaurosis 14 (Perrault et al., 2004) . LRAT catalyzes the esterification of all-trans-retinol into all-trans-retinyl ester, which is essential for vitamin A metabolism in the visual system (Gollapalli and Rando, 2003) . Clinically, vitamin A have been demonstrated to slow the progression of dementia and there are reports showing an trend of lower vitamin A level in AD patients (Ono et al., 2004) . Thus, it would interesting to look into the molecular role of LRAT in the progression of AD in future study. Such findings may provide insights into the discovery of new ADassociated genetic variations as well as the prevention and therapy of this disease."}, {"section_title": "Performance with varying hidden node number", "text": "In Algorithm 1, we need to predefine the number of hidden nodes h, thus it is crucial to test if the performance of FNAM is stable with different h. In this section, we analyze the stability of FNAM model w.r.t. the choice of hidden node number. Figure 3 display the RMSE and CorCoe comparison results of FNAM when h is set in the range of 10; 20; . . . ; 100 f g . From these results, we can find that our FNAM model performs quite stable w.r.t. the choice of hidden node number. As a consequence, we do not need to make much effort on tuning the number of hidden nodes. This is important to an efficient implementation in practice."}, {"section_title": "Running time analysis", "text": "Here, we present experimental results to analyze the runtime (in seconds) of FNAM with different number of hidden nodes. Our experiments are conducted on a 24-core Intel(R) Xeon(R) E5-2620 v3 CPU @ 2.40 GHz server with 65GB memory. The operating system is Ubuntu 16.04.1 and the software we use is Matlab R2016a (64-bit) 9.0.0. Seen from Figure 4 , it only takes a few minutes to run our model on the ADNI data. The running time is roughly linear to the number of hidden nodes, which is consistent with our theoretical analysis that the time complexity of FNAM is O(ndhc). A novel additive model, called FNAM, was proposed for QTL identification, which can be easily adapted to depict the non-linear associations between SNPs and brain endophenotypes. The experimental results on the ADNI cohort indicated the promising performance of FNAM. In particular, we not only identified some SNPs validated in the previous literature, but also found new SNPs with potential risk for Alzheimer's. These empirical studies validate the effectiveness of our approach, and provide insights into the genetic causal relationships as well as early detection of neurological disorders. We also derived the generalization error bound of FNAM under a general assumption, i.e. m-dependent observations, thus is suitable to many other biological applications."}]