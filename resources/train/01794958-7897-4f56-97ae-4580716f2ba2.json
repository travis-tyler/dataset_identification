[{"section_title": "Abstract", "text": "Disease classification is a crucial element of biomedical research. Recent studies have demonstrated that machine learning techniques, such as Support Vector Machine (SVM) modeling, produce similar or improved predictive capabilities in comparison to the traditional method of Logistic Regression. In addition, it has been found that social network metrics can provide useful predictive information for disease modeling. In this study, we combine simulated social network metrics with SVM to predict diabetes in a sample of data from the Behavioral Risk Factor Surveillance System. In this dataset, Logistic Regression outperformed SVM with ROC index of 81.8 and 81.7 for models with and without graph metrics, respectively. SVM with a polynomial kernel had ROC index of 72.9 and 75.6 for models with and without graph metrics, respectively. Although this did not perform as well as Logistic Regression, the results are consistent with previous studies utilizing SVM to classify diabetes."}, {"section_title": "INTRODUCTION", "text": "Disease classification is a crucial element of biomedical research. Improved disease classification models aim to provide accurate and timely prediction to allow for earlier diagnosis and implementation of preventative measures. For example, in the US, approx. 29.1 million people (9.3%) are affected by diabetes, with 1/3 unaware of their disease status, and 57 million with pre-diabetes [1] . Diabetes and prediabetes are known to increase the risk of heart disease and stroke[1] but these long-term effects can be prevented with lifestyle changes and/or medical intervention [2] . Early screening and predictive risk models built with simple clinical measurements (no lab tests required) are important for deployment of prevention strategies, especially in undiagnosed population [3] .\nTraditionally, biomedical data is modeled using Logistic Regression, a method that relies on fitting data to a pre-determined model. Alternatively, the Support Vector Machine (SVM) algorithm is a supervised machine learning method that is a \"model-free\" method that does not require assumptions of distribution and interdependency of predictor variables. In SVM each data point is represented as a n-dimensional vector and the algorithm constructs an n-1-dimensional separating hyperplane to discriminate 2 classes, with maximized distance between the hyperplane and data points on each side. Non-linear functions, kernels, can also be used to transform data into multidimensional space. Previous research demonstrates that SVM has similar or improved predictive capabilities for disease classification in comparison to Logistic Regression [4] .\nData from 2015 were obtained from Georgia's Behavioral Risk Factor Surveillance System (BRFSS) [7] , landline and cellphone-based survey conducted by the Centers for Disease Control and Prevention (CDC). A binary predictor variable was defined based on survey respondents reporting they had been informed by their physician they had diabetes or pre-diabetes. Once imputing missing data where possible, the analysis dataset included 2066 observations with 401 (19.4%) classified as having diabetes or pre-diabetes. BRFSS includes over 300 variables of various health behaviors and chronic conditions. For this study, I selected potential predictor variables based on literature review and the known conceptual model [8] . Variables considered based on the conceptual model included: sex, age, race, education, income, marital status, BMI, cholesterol, hypertension, arthritis, physical activity, and consumption of fruits and vegetables. Once cleaning the predictors of interest, the analysis dataset included a sample of 1284 people and households in Georgia."}, {"section_title": "LITERATURE REVIEW", "text": ""}, {"section_title": "SVM APPLICATION TO DISEASE CLASSIFICATION", "text": "Yu, et al. [4] used the National Health and Nutrition Examination Survey (NHANES), an ongoing, crosssectional, probability sample of US population, to build SVM and Logistic Regression classification models for 2 classification schemes: persons with diabetes (diagnosed or undiagnosed) vs. persons without diabetes, and persons with undiagnosed diabetes or pre-diabetes vs. persons without diabetes. They used 14 potential predictors commonly associated with diabetes: family history, age, gender, race and ethnicity, weight, height, waist circumference, BMI, hypertension, physical activity, smoking, alcohol use, education, household income [4] . They found that the Radial Basis Function (RBF) kernel, and Linear kernel worked best for classification schemes I and II respectively, and there was no significant difference between Logistic Regression and SVM performance (AUC 0.83 & 0.73 for classification I and II, respectively, with both models) [4] .\nAdditionally, Kumari et al [9] . also found success with the SVM model for classification of diabetes in the Pima India Diabetic Dataset from the UCI Machine Learning Laboratory. In this case, an 8 predictor SVM model, including lab data (plasma glucose concentration, 2-hr serum insulin) was validated with 78% accuracy using the RBF kernel.\nSVM has been used across diverse biomedical classification problems. This includes a patient financial risk model using health claims and clinical encounter data, and a patient response to flu awareness campaign model, both using weighted SVM [10] . A project comparing various machine learning techniques with Logistic Regression for prediction of heart disease also shows no significant difference between Logistic Regression and SVM, with the Linear kernel performing best [11] ."}, {"section_title": "GRAPH THEORY APPLICATION TO DISEASE CLASSIFICATION", "text": "In Kocevar, et al. [6] . they combined graph metrics with SVM to classify various Multiple Sclerosis (MS) clinical profiles. Cortical and sub-cortical gray matter (GM) segmentation was performed on the advanced MRI imaging of 77 MS patients and 26 healthy controls (HC). Figure 1 shows the process of segmentation of the scans to create nodes, and anatomically constrained probabilistic streamline tractography is used to create edges between the segments. Edge weights are determined by a function of the number of fibers connecting the segments. The weakest connections are removed by applying a threshold 0 \u2264 \u03c4 \u2264 1 on the weighted graph, generating an unweighted graph maintaining the \u03c4% strongest connections in the network. Global network metrics calculated for the study on the unweighted graph included: graph density (D), assortativity (r), transitivity (T), global efficiency (Eg), modularity (Q), and characteristic path length (CPL). The study found that global graph metrics were not significantly dependent on patients' age or gender. Overall, significant difference in graph metrics were found when comparing MS patients with HC groups, as well as between different clinical classifications of MS. SVM classification with RBF kernel was then used to predict varying binary classifications of HC groups and clinical courses, with highest classification achieved using all graph metrics as a feature vector in the model at 91.8%. Using only one graph metric, the best in this case being modularity, the study could achieve accuracy of 88.9%.\nIn Khazaee, et al., they found that using changes in brain connections from functional magnetic resonance imaging (MRI) provided strong predictive measures for classifying Alzheimer's Disease (AD) patients from healthy controls (HC). 20 patients with AD and 20 age-matched HC from Alzheimer's disease neuroimaging initiative (ADNI) database were selected for study. MRI images were parcellated into 90 regions and edges were defined as connectivity of all pairs of regions using Pearson's correlation coefficient. As in the previous study, thresholding was used to maintain the strongest connections in the network. Preserving a high proportion of the network results in a dense graph with noisy and less significant edges maintained. However, removing too many edges can result in a disconnected graph where global graph metrics cannot be calculated. From their previous research, this study found that a threshold of 12% was optimal [5] . The study maintained the bridge edges between any disconnected sections that resulted from thresholding, regardless of the edge weight. Figure 2 shows the weighted adjacency matrix for the complete and 12% threshold network. Graph metrics calculated for this project included: functional segregation via clustering coefficient, local efficiency, and normalized local efficiency to measure specialized processing within densely interconnected groups of regions; functional integration via characteristic path length and global efficiency to assess ability of the brain to rapidly combine specialized information from distributed regions; and 3 local measures including degree, participation coefficient, and betweenness centrality to measure properties of the 90 regions. An iterative feature selection algorithm using 7 different methods was then used to filter the most effective graph features for the classification problem. Linear SVM with a tuned C parameter using leave-one-out cross validation was used to perform the final feature classification.\nEnd results found that Fisher Score provided the best feature selection method for the discriminative algorithm. Figure 3 shows performance of the Fisher algorithm with increasing number of selected features and various values of SVM C parameter. The best algorithm found could classify AD patients from HC group with a highest accuracy of 97.5%. METHODOLOGY SAS\u00ae PROC HPSVM was used to build SVM models. PROC HPSVM is a SAS\u00ae Enterprise Miner\u2122 high performance data mining procedure built to take advantage of parallel processing with both single machine and distributed multiple-machine mode. The data were split into 80% training, 20% validation datasets. I ran PROC SVM comparing 3 common kernels: Linear, Polynomial, and RBF. I used 5-fold cross validation for each kernel to determine the best penalty parameter, C. This controls for overfitting of the model by specifying allowable misclassification. SAS\u00ae PROC LOGISTIC was used to build the Logistic Regression model for comparison. The models were compared based on sensitivity, specificity, and area under the receiver operating characteristic (ROC) curve, using Enterprise Miner\u2122."}, {"section_title": "SOCIAL NETWORK SIMULATION", "text": "To test the application of graph theory metrics as potential predictors in a classification model, it was necessary to simulate a social network within the BRFSS dataset. The Watts-Strogatz small world network model was selected to represent the sample social network for this application due to its ability to simulate the interconnected groups (clusters) that exist in real-world networks, as well as the existence of random irregular connectivity patterns [12] . This model was first introduced by Duncan Watts and Steven Strogratz in Nature in 1998Error! Reference source not found..\nWatts-Strogatz is a variation of the lattice network where nodes are connected to their nearest neighbors only. Figure 4 illustrates the adjustment of a lattice network to the Watts-Strogatz network. Watts-Strogatz randomly rewires some of the lattice edges, resulting in high clustering and short paths. This network is undirected. Ideally, the data studied would include some network characteristics, but I did not have access to a public use dataset that includes both demographic and network characteristics. For the purposes of testing the application of graph metrics to a predictive model, a simulated network will suffice. To incorporate an element of the demographic data into the social network simulation, I weighted each edge by the average standardized number of adults in the respondents' household."}, {"section_title": "Figure 4:LATTICE AND WATTS-STROGATZ NETWORK MODELS", "text": "[12]\nThe algorithm for creating a Watts-Strogatz network starts with a lattice network where each node is adjacent to a defined L neighbors. If each node has degree kL, and kL is even, then the global clustering coefficient, C , of the network is\nTo randomly rewire the lattice network, each edge has a defined probability, pw, of being re-wired. Each edge can only have one end re-wired and the edges are replaced so that total number of edges and mean degree is the same as the original lattice [12] . The reason why the Watts-Strogatz model maintains high clustering coefficient, but low average path length, as compared with a random network, is that the global clustering coefficient is based on the average of the local measure; rewiring a small number of connections will only affect the local clustering coefficient of a small number of nodes. However, average path length is a global measure of the average of shortest path length between every combination of nodes. Changing even one edge can create shortcuts between many pairs of nodes, greatly affecting the average path length [12] . In addition, even if we select a rewiring probability of 1, a Watt-Strogatz network will not be the same as a random network with same size and average degree because the WattsStrogatz algorithm does not allow nodes to have degree less than n/2, where the random network does allow this [12] . Figure 5 shows the simulated BRFSS network for 10% of the data."}, {"section_title": "Figure 5:WATTS-STROGATZ SIMULATED NETWORK", "text": "The R igraph package was used to create the Watts-Strogatz network using 1284 nodes, kL = 4 (degree of every node in the initial lattice), and rewiring probability pw 0.5. The resulting edge-list was imported into SAS\u00ae and merged with the household variable in the analysis dataset to create the edge weights as described above. SAS\u00ae PROC OPTGRAPH was used to calculate various graph characteristics to be used in the SVM and Logistic Regression model application, including: local clustering coefficient, degree centrality, closeness centrality, betweenness centrality, and eigenvector centrality."}, {"section_title": "SUPPORT VECTOR MACHINE", "text": "SVM is a supervised learning algorithm that represents instances of data as points in space and then builds a model to assign new instances to one category or another. Each data point is represented as a n-dimensional vector, then SVM constructs an n-1-dimensional separating hyperplane to discriminate 2 classes, with maximized distance between the hyperplane and data points on each side. SVM aims to find the best hyperplane for separation of both classes [11] .\nData are represented as (2) where yi is either 1 or -1, indicating to which class xi belongs. Each xi is p-dimensional vector representing all of the characteristic values (variables) of xi.. The hyperplane that best separates the group of xi vectors where yi = 1 from the group of vectors where yi = -1 is (3) Where is the normal vector to the hyperplane and b is the offset of the hyperplane from the origin. If the data points are linearly separable, the hard margin can be represented as (4) Figure 6 shows a maximum margin separation for linearly separable data. The samples that fall on the margin are known as the support vectors. For data that is not linearly separable we can include a hinge loss function, 'C', to determine the trade-off between increasing the margin and whether an instance of xi lies on the correct side of the margin. In addition, we can implement kernel functions to adjust the inner dot product of the maximum margin hyperplane optimization algorithm. This transforms the data into a higher dimensional space. Figure 7 shows the transformation of features into higher dimension space. [11] SAS\u00ae PROC HPSVM from the Enterprise Miner\u2122 High Performance Procedures was used to build the SVM models. 5-fold cross validation was used to determine best soft margin parameter C in each kernel: Linear, RBF, and Polynomial. Macro programming was used to automatically evaluate several values of RBF kernel parameter, \u03b3, and then choose the best C/\u03b3 combination."}, {"section_title": "LOGISTIC REGRESSION", "text": "Logistic Regression examines the non-linear relationship between a binary outcome and categorical or continuous predictor variables. The logistic model outputs a probability of an event between 0 and 1 as the log of the odds ratio (3) (5) where \u03b2 is the parameter coefficient and x is the value of the independent variable. SAS\u00ae PROC LOGISTIC was used to build the model and stepwise elimination with \u03b1 = 0.05 was used to eliminate redundancy and keep the strongest predictors in the model. 10-fold cross validation was used for model evaluation. Table 1 and Table 2 illustrate the significant effects remaining in the Logistic Regression models, for models with and without network characteristic metrics included. Age, BMI, hypertension, and cholesterol all have increased odds of diabetes outcome, while education has decreased odds. This is consistent with outcomes of previous research and known risk factors for diabetes. In the model with graph metrics included, only closeness centrality remains as significant, in addition to the same demographic variables from the previous model. If this were a real network in the dataset (not simulated), this would indicate that people with shorter total paths to other people in the network would have increased risk of diabetes. Table 3 shows the model performance results for models with and without the social network graph characteristics. The models were evaluated based on the sensitivity, specificity, and ROC index for the validation data set. The logistic model performs best for models with and without graph metrics included, and the SVM model with polynomial kernel is comparable but the ROC index is affected by lower sensitivity. Figure 8 and Figure 9 provide a visual comparison of the area under the ROC curves for models with and without graph metrics, respectively. "}, {"section_title": "RESULTS", "text": ""}, {"section_title": "DISCUSSION & FUTURE RESEARCH", "text": "Support Vector Machines and graph metrics are important tools to be considered for disease classification problems. While SVM did not perform as well as Logistic Regression in this study, it's results were comparable to previous research. SVM is known to be less sensitive to high dimensionality, and sparse datasets, so would likely perform better than Logistic Regression in studies with biomedical data of that nature.\nIncluding graph metrics in the model did improve predictive performance slightly using a simulated network. Ideally, future research will include a dataset with both demographic and network characteristics included. Future improvements to this study will include:\n\uf0b7 Parameter selection using machine learning such as Random Forest \uf0b7 Creating a custom kernel for SVM using deep learning techniques RELEVENT SAS\u00ae CODE "}]