[{"section_title": "Abstract", "text": "Abstract-Clustering explores meaningful patterns in the non-labeled data sets. Cluster Ensemble Selection (CES) is a new approach, which can combine individual clustering results for increasing the performance of the final results. Although CES can achieve better final results in comparison with individual clustering algorithms and cluster ensemble methods, its performance can be dramatically affected by its consensus diversity metric and thresholding procedure. There are two problems in CES: 1) most of the diversity metrics is based on heuristic Shannon's entropy and 2) estimating threshold values are really hard in practice. The main goal of this paper is proposing a robust approach for solving the above mentioned problems. Accordingly, this paper develops a novel framework for clustering problems, which is called Weighted Spectral Cluster Ensemble (WSCE), by exploiting some concepts from community detection arena and graph based clustering. Under this framework, a new version of spectral clustering, which is called Two Kernels Spectral Clustering, is used for generating graphs based individual clustering results. Further, by using modularity, which is a famous metric in the community detection, on the transformed graph representation of individual clustering results, our approach provides an effective diversity estimation for individual clustering results. Moreover, this paper introduces a new approach for combining the evaluated individual clustering results without the procedure of thresholding. Experimental study on varied data sets demonstrates that the prosed approach achieves superior performance to state-of-the-art methods."}, {"section_title": "I. INTRODUCTION", "text": "Clustering, the art of discovering meaningful patterns in the non-labeled data sets, is one of the main tasks in machine learning. Generally, individual clustering algorithms provide different accuracies in a complex data set because they generate the clustering results by optimizing a local or global function instead of natural relations between data points in each data set. [1] , [2] . As a novel solution, cluster ensemble which combines the different clustering results was proposed for achieving a better final result [1] . Cluster Ensemble Selection (CES) is a new solution which combines a selected group of best individual clustering results according to consensus metric(s) from ensemble committee in order to improve the accuracy of final results [3] . The evaluation metric(s), thresholding and selection strategy, and aggregation method are the most important challenges in CES for selecting better partitions of ensemble committee and generating the final result. There are a wide range of ideas for solving mentioned challenges [3] , [4] , [5] , [6] , [7] . Although these methods can improve performance and robustness of final results, using a wide range of threshold values and employing the entropy based metric are two main weak points of this method. Threshold values are different for each data set in the mentioned methods; and it is really hard to find optimum values in real-world applications. Moreover, most of the real-world data sets do not have logarithm behavior. So, there is no prove that entropy based methods, which estimate the diversity based on the logarithm, were the best choice to evaluate the diversity. This paper proposes a novel methodology for solving clustering problems without mentioned weak points.\nAs mentioned before, there are four stages in Cluster Ensemble Selection (CES); i.e. generating individual clustering results, evaluating, selecting and combining them as a final clustering result. Although CES can achieve a better result in comparison with individual clustering algorithms and cluster ensemble methods, the accuracy of CES is fully sensitive to the process of thresholding for selecting individual clustering results, and the consensus metric, which is used for diversity or quality estimation of the results. Unfortunately, it is so hard to find the optimum threshold values in practice; and most of the metrics, which were used for diversity or quality estimation, are heuristic; especially they are based on Shannons entropy. The main goal of this paper is solving mentioned problems. This paper proposes a new method for estimating the diversity of generated individual clustering results by using a redefined version of modularity, which is based on expected value and it is introduced for the community detections applications. Further, this paper introduces a novel approach for combining the evaluated individual clustering results without the process of thresholding.\nOur contribution in this paper can be summarized as follows: Firstly, this study proposes a greedy method based on feedback mechanism [8] which employs the idea of bisecting k-means for generating individual results. After that, this paper introduces the Two Kernels Spectral Clustering (TKSC) for generating individual clustering results. This algorithm generates hybrid individual clustering results, which contains Partitional results and Modular results. Same as simple clustering problems, our method generates Partitional results; and also it generates Modular results, which represented by a graph, as a new alternative for evaluating and combining the individual results. Next, to satisfy the diversity criterion, this study proposes Normalized Modularity, which is a redefined version of Modularity criterion in community detection [9] , for evaluating diversity of individual results in the general clustering problems. Unlike most of the diversity metrics which are based on Shannon's entropy, this metric uses Expected Value in probabilistic theory for evaluating individual clustering results and avoids the undesired logarithm [9] , [10] . Lastly, this paper proposed Weighted Evidence Accumulation Clustering (WEAC) to obtain the final clustering with a weighted combination of all individual results. While the weight of each individual result in WEAC can be estimated with different metrics, the normalized modularity was used in this paper.\nThe rest of this paper is organized as follows: In Section 2, this study first briefly reviews some related works on cluster ensemble selection. Then, it introduces the proposed Weighted Spectral Clustering Ensemble (WSCE) framework in Section 3. Experimental results are reported in Section 4; and finally this paper presents conclusion and pointed out some future works in Section 5."}, {"section_title": "II. RELATED WORKS", "text": "As an unsupervised method, Clustering discovers meaningful patterns in the non-labeled data sets. There is a wide range of studies, which try to increase the performance of clustering algorithms. For instance, Zhang et al. introduced a multi-manifold regularized nonnegative matrix factorization framework (MMNMF) which can preserve the locally geometrical structure of the manifolds for multi-view clustering [11] . Anyway, individual clustering algorithms provide different accuracies in a complex data set because they generate the clustering results by optimizing a local or global function instead of natural relations between data points in each data set [1] , [2] .\nGenerally, a cluster ensemble has two important steps: Firstly, generating individual clustering results by using different algorithms and changing the number of their partitions. Then, combining the primary results and generating the final ensemble. This step is performed by consensus functions (aggregating mechanism) [1] , [12] .\nThe idea that not all partitions are suitable for cooperating to generate the final clustering was proposed in CES [3] . Instead of combing all achieved individual results, CES can combine a selected group of best individual results according to consensus metric(s) from the ensemble committee in order to improve the accuracy of final results [3] , [5] , [8] , [4] , [7] . Fern and Lin developed a method to effectively select individual clustering results for ensemble and the final decision [3] . Azimi et al. proved that diversity maximization is not an effective approach in some real-world applications. They explored that the thresholding procedure must be done based on the complexity and quality of data sets [4] . Jia et al. proposed SIM for diversity measurement, which works based on the Normalized Mutual Information (NMI) [6] . Romano et al. proposed Standardized Mutual Information (SMI) for evaluating clustering results [13] .\nYousefnezhad et al. introduced independency metric instead of quality metric for evaluating the process of solving a problem in the CES [7] . Alizadeh et al. have concluded the disadvantages of NMI as a symmetric criterion. They used the APMM 1 and Maximum (MAX) metrics to measure diversity and stability, respectively, and suggested a new method for building a co-association matrix from a subset of base cluster results [5] , [8] . Alizadeh et al. introduced Wisdom of Crowds Cluster Ensemble (WOCCE), which is a novel method base on a theory in social science [8] . Although, this method can generate high performance and more stable results in comparison with other CES methods, using a wide range of thresholds and employing different types of clustering algorithms for generating individual results are two main problems in this method. Alizadeh et al. used A3, which is based on Shannon's entropy, for diversity evaluation; and Basic Parameter Independency (BPI), which uses initialized values of individual clustering algorithms such as random seeds in the first iterative of k-means, for independency evaluation. In addition, they introduced the feedback mechanism for generating the high-quality results [8] .\nAs a graph based clustering methods, spectral clustering generates high-performance results when it is applied to different applications; i.e. from image segmentation to community detection arena. Kuo et al. introduced a new method for automating the process of Laplacian creation in the medical applications; especially for fMRI segmentation where this method used standard Laplacians perform poorly [14] . Chen et al. proposed a clustering algorithm which is based graph clustering and optimizing an appropriate weighted objective, where larger weights are given to observations (edge or noedge between a pair of nodes) with lower uncertainty [15] . Gao et al. introduced a graph-based consensus maximization (BGCM) method for combining multiple supervised and unsupervised models. This method consolidated a classification solution by maximizing the consensus among both supervised predictions and unsupervised constraints [16] ."}, {"section_title": "III. THE PROPOSED METHOD", "text": "Given a set of high-dimensional data examplesX = {x 1 ,x 2 , . . . ,x n }. The simple average ofX can be denoted\nwhere n is the number of instances in theX; andx i denotes the i \u2212 th instance of the data points. At the beginning, this paper minimized the correlation between features. So, it denotes X as follows:\nwhereX is the data points, andX denotes simple average of X, which calculated by (1). It's clear that X is zero-mean. In other words, the excepted value of X is zero as follows:\nNow, this paper maps Q : X \u2208 R m\u00d7n \u2192 Y \u2208 R m\u00d7n , where m, n denote the number of features and data points, respectively. This mapping just minimizes the correlation between features. This problem can be reformulate as follows:\n, then the correlation of Y will be defined as follows:\nBased on above definition, the expected value of j \u2212 th feature of X denotes as follows:\nwhere q denotes the j \u2212 th index of the Q. In other words, our correlation problem is changed to a variance probe. Now, maximizing the q based on the variance of X will be omitted the correlation between features. Since the scale of data after mapping must be same, we assume following equation:\nFor maximizing the (6), which is denoted by \u03a8(q), our problem will be reformulated as follows:\nwhere the symbol \u03b4q is an abbreviation for 'a small change in q'. We consider (\u03b4q) T \u03b4q \u2248 0, so the above definition denotes as follows:\nBased on (7) and (8), we can assume as follows:\nNow, this paper defines following equation by using (9) and (10):\nwhere \u03bb \u2208 R is a constant. Since (\u03b4q) T = 0, the following equation must be satisfy for minimizing correlation between features:\nwhere R and \u03bb denotes the eigenvectors and eigenvalues, respectively. For all features of X the above equation will be denoted as follows:\nwhich is called eigenstructure equation. In above equation, \u039b is a diagonal matrix. Based on (7), we can define following equation:\nwhere I is identity matrix. Following equation denotes based on (13) and (14):\nwhere m denotes number of features in data X. Now, consider that R is a descending order based on \u039b values.\nFor an optional feature selection we can define the following equation instead of (15):\nwhere d < m is the number of features, which must be selected for generating results. Algorithm 1 shows the mapping function, which can minimized correlation of data set based on above definitions. For generating individual clustering results, the proposed method partitions Y into C l clusters, where k denotes number of clusters in the individual results, and C l is l \u2212 th individual result in the reference set. This paper uses the range of l \u2208 [2 , k + 2 ] for generating individual results, where k is the number of clusters in the final result. This is the same as bisect k-means algorithms but instead of applying the algorithm on generated results in each iterative, our proposed method stores this result on the ensemble committee; and then evaluates and combines these results. In other words, the reference set denotes"}, {"section_title": "Algorithm 1 The Mapping Function", "text": "Input: Data setX \u2208 R m\u00d7n = {x 1 ,x 2 , . . . ,x n }, d as number of features: d = 0 is considered for deactivating the feature selection Output: Mapped data set Y Method:\n1. Calculating simple averageX by using (1). 2. Calculating X by using (2).\n. Calculating \u039b and Q as eigenvalues/vectors of R. 5. Sorting Q based on descending values of \u03bb.\nLike other spectral methods, this paper calculates the non-symmetric distances (adjacency) matrix of Y , which is denoted by A [17] , [18] . In the rest of this paper, our proposed method will be applied to the matrix A for each individual clustering results. Moreover, this paper uses (17) as transform function for converting distances matrix A to similarity matrix S. This transformation can optimize the memory usage [17] , [18] .\nwhere y n denotes the n \u2212 th data point and y i \u2212 y j 2 will be calculated by Euclidean distance. The scaling parameter \u03c6 controls how rapidly affinity S i,j falls off with the distance between the data points. This paper uses Ng et al. method for estimating this value automatically (count non-zero values in each columns of distance matrix A) [17] , [18] . This paper introduces Two Kernels Spectral Clustering (TKSC) algorithm, which can generate all individual results (\u03b6). Unlike normal clustering algorithms, which just generate a partition as the result, the TKSC algorithm generates two independent consequences, which are called Partitional result and Modular result, for each of the individual clustering results by using two kernels (\nis a partitioning of data points same as the result of other clustering methods; and Modular result (M ) is a network of data points, which can be represented by a graph. This paper uses Modular result as a reference for evaluating the diversity of generated partition by using community detection methods [9] , [10] . Furthermore, kernel in the TKSC refers to Laplacian equation in spectral methods because it transforms data points in new environment, especially linear environment for non-linear data sets.\nPartitional Kernel: This paper uses following equation for generating Partitional result:\nwhere I is the identity matrix [17] ; D is the diagonal matrix of S (D = diag(S)); and S will be calculated by (17) . As shows in follows, the eigendecomposition is performed for calculating eigenvectors of L P :\nwhere the matrix V is the eigenvectors of Partitional Kernel.\nThe coefficient W will be defined for normalizing the matrix V :\nwhere V ij shows the i-th row and j-th column of the matrix V ; and is used for omitting the effect of zeros in the matrix W . This paper uses = 10 \u221220 for generating the experimental results. Also, n denotes the number of instances in the data set (W \u2208 R n ). The normalized matrix of eigenvectors will be calculated as follows:\nwhere U ij and V ij denote the i-th row and j-th column of these matrices; and W i is the i-th row of the matrix W which is used for normalization. The Partitional result of TKSC will be calculated by applying the simple k-means [8] on the matrix U as follows:\nwhere K is the number of classes in individual results; and U will be calculated by (21) . Modular Kernel: This paper uses following equation for generating Modular result:\nwhere D is the diagonal matrix of S (D = diag(S)); and S will be calculated by (17) . This paper considers the normalized matrix of L M an adjacency matrix of graph representation of individual result as follows:\nwhere L M is calculated by (23), and the function max finds the biggest value in the matrix L M . Further, all values in the matrix M , which is called Modular result, are between zero and one. Algorithm 2 shows the pseudo code of the TKSC method. Tracing errors can control similarity and repetition of specific answers in clustering problems. There is a wide range of metrics, which are based on Shannon's entropy [8] , [5] , for evaluating the diversity of individual results in the CES methods, such as MI [1] , NMI [12] , APMM [5] , MAX [8] , and SMI [13] . Shannon's entropy uses the logarithm of probability of individual results for (20) and (21). 7. Generate M by applying L M on (24). 8. P l = kmeans(U, l) 9. Return P l and M evaluating the diversity but there is no mathematical prove that all real-world data sets have logarithmic behavior. In community detection arena [9] , [10] , Modularity, which is based on Expected Value, was proposed for solving this problem. Recently, many papers proved that modularity [9] , [10] can estimate the diversity on graph data sets better than entropy based methods. Unfortunately, modularity can measure the diversity only for graph data [9] . This paper proposes TKSC, which can generate a graph based result, called Modular result, for any types of data sets in real-world application. Since modularity was defined for community detection arena, this paper introduces a redefined version of modularity metric for general clustering problems, which is called Normalized Modularity (N M ). It is used for evaluating the diversity of the individual results based on Modular result of the TKSC as follows:\nwhere P l and M are calculated by (22) and (24), respectively; z is sum of all cells in the matrix M (z = M M ij ); and c i and c j are the cluster's numbers of the i-th and j-th instances in the Partitional result P l . Also, \u03c3 i and \u03c3 j show the degree of i-th and j-th nodes in the graph of matrix M (How many rows contains non-zero value in the columns i or j). In addition \u0393 ij and \u0398 (c i , c j ) will be calculated as follows:\nThis diversity evaluation is 0 \u2264 N M \u2264 1. In the rest of this section, we describe how N M will be used for evaluating individual clustering results. Thresholding is used for selecting the evaluated individual results in the CES. Then co-association matrix is generated by using consensus function on the selected results. Lastly, the final result is generated by applying linkage methods on the co-association Figure 1 . In the traditional EAC, the \u03b1 (i,j) represents the number of clusters shared by objects with indices (i, j); and \u03b2 (i,j) is the number of partitions in which this pair of instances (i and j) is simultaneously presented. This method assumes the weights of all individual clustering results (\u03b1 (i,j) ) are the same. This paper proposes Weighted EAC for optimizing this method by using a weight for each individual clustering results instead of just counting their shared clusters. While the weight can have different definitions in the other applications, this paper uses average of Normalized Modularity (NM) of two algorithms as the weight in the WEAC (\u1fb1 (i,j) = \u03b1(i,j) \u03c1 i,j ).\nmatrix. These methods generate the Dendrogram and cut it based on the number of clusters in the result [12] , [8] . In recent years, many papers have used EAC as a highperformance consensus function for combining individual results [12] , [5] , [8] , [4] , [3] . EAC uses the number of clusters shared by objects over the number of partitions in which each selected pair of objects is simultaneously presented for generating each cell of the co-association matrix. Figure 1 illustrates the effect of the EAC equation (c (i, j) = \u03b1(i,j) \u03b2(i,j) ) on the shape of Dendrogram. Where \u03b1 (i,j) represents the number of clusters shared by objects with indices (i, j); and \u03b2 (i,j) is the number of partitions in which this pair of instances (i and j) is simultaneously presented. As a matter of fact; EAC considers that the weights of all algorithms results are the same. Instead of counting these indices, this paper uses following equation, which is called Weighted EAC (WEAC), for generating the co-association matrix.\nwhere \u03b1 (i, j) and \u03b2 (i, j) are same as the EAC equation; Also, \u03c1 i,j is the weight of combining the instances. Although this weight can have different definitions in the other applications, this paper uses average of Normalized Modularity of two algorithms as follows for combining individual results:\nwhere N M i and N M j illustrates the Normalized Modularity of the algorithms, which generate the results for indices i and j. In other words, as a new mechanism, this paper generates the effective results when both algorithms have high NM values; and also the effects of individual results are near of zero when the both algorithms have small values in the NM metric. As a result, this paper just omits the effect of low quality individual results by using mentioned mechanism instead of selecting them by thresholding procedures. Further, the final co-association matrix, which is a symmetric matrix, will be generated by (28) as follows: 1) c(1, 2) . . . c(1, n) c (2, 1) c(2, 2) . . . c(2, n)\n(30) where n is the number of data points; and c(i, j) denotes the final aggregation for i \u2212 th and j \u2212 th instances. Algorithm 3 illustrates the pseudo code of the proposed method. In this algorithm,X is the data set; k is the number of clusters in the final result; P f is the final result partition. The distances are also measured by an Euclidean metric. The TKSC function builds the partitions and modules of individual results; and NM function evaluates these results by using (25). Then, the evaluated results will be added to reference set. The WEAC function generates the co-association matrix, according to (28), by using the Normalized Modularity values and Partitional results. The Average-Linkage function creates the final ensemble according to the average linkage method [5] , [8] ."}, {"section_title": "Algorithm 3 The Weighted Spectral Cluster Ensemble", "text": "Input: Data pointsX, Number of clusters k, Number of features d. Output: final result P f Method: 1. Generate Y by usingX and d on Algorithm 1. 2. Generate matrix A by using Y based on [17] . 3. for l = 2 to k + 2 do 4.\nAdd [P, Q] to \u03b6 as the reference set. 7. end for 8. Generate co-association matrix \u03be = W EAC(\u03b6) 9. P f = Average \u2212 Linkage(\u03be)"}, {"section_title": "IV. EXPERIMENTS", "text": "The empirical studies will be presented in this section. The unsupervised methods are used to find meaningful patterns in non-labeled datasets such as web documents, etc. in real world application. Since the real dataset doesnt have class labels, there is no direct evaluation method for estimating the performance in unsupervised methods. Like many pervious researches [12] , [3] , [5] , [8] , [7] , this paper compares the performance of its proposed method with other individual clustering methods and cluster ensemble (selection) methods by using standard datasets and their real classes. Although this evaluation cannot guarantee that the proposed method generate high performance for all datasets in comparison with other methods, it can be considered as an example for analyzing the probability of predicting good results in the WSCE.\nThe results of the proposed method are compared with individual algorithms k-means [8] and Maximum Likelihood Estimator (MLE) [15] , as well as APMM [5] , WOCCE [8] , SMI [13] , and BGCM [16] which are state-of-the-art cluster ensemble (selection) methods. This paper reported the empirical results of k-means algorithm as one of the classical clustering methods. Furthermore, as a new alternative in the clustering methods, the empirical results of the proposed method are compared with the MLE, SMI, and BGCM methods. Also, this paper uses the unsupervised version of BGCM method (with the null set of supervision information). For representing the effect of Uniformity on the performance of the final results, it compares with two state-of-the-art metrics in diversity evaluation (APMM and SMI). The last but not least, the experimental results of this paper are compared with the WOCCE as another method in the CES, which uses the independency estimation. All of these algorithms are implemented in the MATLAB R2015a (8.5) by authors 2 in order to generate experimental results. All results are reported by averaging the results of 10 independent runs of the algorithms which are used in the experiment. Also, the number of individual clustering results in the reference set of the ensemble is set as 20 for all of mentioned algorithms in all of experiments on a PC with certain specifications 3 ."}, {"section_title": "A. Data Sets", "text": "This paper uses three different groups of data sets for generating experimental results; i.e. image based data sets, document based data sets and others. Table I illustrates the properties of these data sets. This paper uses the USPS digits data set, which is a collection of 16 \u00d7 16 gray-scale images of natural handwritten digits and is available from [19] . Furthermore, this paper uses Alzheimer's Diseases Neuroimaging Initiative (ADNI) data set for 202 subjects as another image based real-world data set. This data set contains MRI and PET images from human Brian in two categories (which are shown by C1 and C2 in the Table I 2 The proposed method is available http://sourceforge.net/projects/myousefnezhad/files/WSCE/and II) for recognizing the Alzheimer diseases. In the first category, this data set partitions subjects to three groups of Health Control (HC), Mild Cognitive Impairment (MCI), and Alzheimer's Diseases (AD). In the second category, there are four groups because the MCI will be partitioned to high and low risk groups (HMCI/LMCI). This paper uses all possible forms of this data set by using only MRI features, only PET features and all of MRI and PET features (FUL) in each of two categorize. More information about ADNI-202 is available in [20] . As a document based data set, the 20 Newsgroups is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. Some of the newsgroups are very closely related to each other, while others are highly unrelated. It has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering. Moreover, the Reuters-21578 is one of the most widely used test collections for text classification research. This data set was collected and labeled by Carnegie Group, Inc. and Reuters, Ltd. We use the 10 largest classes of this data set. The rest of standard data sets are from UCI [21] . The chosen data sets have diversity in their numbers of clusters, features, and samples. Further, their features are normalized to a mean of 0 and variance of 1, i.e. N (0, 1). "}, {"section_title": "B. Performance analysis", "text": "In this section the performance (accuracy metric [8] ) of proposed method will be analyzed. In other words, the final clustering performance was evaluated by re-labeling between obtained clusters and the ground truth labels and then counting the percentage of correctly classified samples [8] . The results of the proposed method are compared with individual algorithms Spectral clustering [17] and MLE [15] , as well as APMM [5] , WOCCE [8] , SMI [13] , and BGCM [16] which are state-of-the-art cluster ensemble (selection) methods. The main reason for comparing the proposed method with Spectral clustering is to show the effect of TKSC framework on the performance of the final results. Furthermore, as a new alternative in the graph based clustering methods, the empirical results of WSCE are compared with the MLE and BGCM methods. This paper uses the unsupervised version of BGCM method (with the null set of supervision information). For representing the effect of Normalized Modularity on the performance of the final results, it compares with three state-of-theart metrics in diversity evaluation (A3, APMM and SMI), which are based on Shannons entropy. This paper doesn't use optional feature selection in this section (d = 0). The experimental results are given in Table II . In this table, the best result which is achieved for each data set is highlighted in bold. As depicted in this table, although individual clustering algorithms (Spectral and MLE) have shown acceptable performance in some data sets, they cannot recognize true patterns in all of them. As mentioned earlier in this paper, in order to solve the clustering problem, each individual algorithm considers a special perspective of a data set which is based on its objective function. The achieved results of individual clustering algorithms, which are depicted in Table II are good evidence for this claim. Furthermore, the results generated by APMM, SMI, and WOCCE show the effect of the aggregation method on improving accuracy in the final results. According to Table II, BGCM and the proposed algorithm (WSCE) have generated better results in comparison with other individual and ensemble algorithms. Even though the proposed method was outperformed by a number of algorithms in four data sets (Iris, SA Hart, and ADNI-MRI-C1/C2), the majority of the results demonstrate the superior accuracy of the proposed method in comparison with other algorithms. In addition, the difference between the performance of proposed method and the best result in those four data sets is lower that 2%."}, {"section_title": "C. Noise and missed-values analysis", "text": "The effect of noise and missed-values on the performance of clustering algorithms will be discussed in this section. The optional feature selection for the proposed method doesn't use in this section (d = 0). In Figure 2 , the effect of noise in the features of data sets will be analyzed on the performance of proposed method. This figure represents the performance of the WSCE, WOCCE, BGCM, SMI, and APMM on the noisy data sets. In this experiment, some features of Arcene and CNAE-9 data sets are randomly changed. This figure shows that proposed method generates more stable results because the Normalized Modularity provides a robust diversity evaluation for selecting most stable individual results. As mentioned before, Shannon's entropy uses the logarithm of probability of individual results for evaluating the diversity but there is no mathematical prove that all realworld data sets have logarithmic behavior. This experiment is the best evidence for this claim. Figure 3 demonstrates the analysis for the effect of missed-values in the data sets on the performance of clustering algorithms. This figure illustrates the performance of the WSCE, WOCCE, BGCM, SMI, and APMM on the data sets with missed-values. In this experiment, some values of attributes of Arcene and CNAE-9 data sets are randomly missed (set null). As you can see in this Figure, the proposed method and BGCM generate more stable results. This is a new advantage of our proposed method in comparison other non-graph based methods. Since, our proposed method uses the TKSC algorithms for generating Partitional and Modular results, it can significantly handle the miss values. In other words, as a local error in the individual results, a missed-value just can destroy an edge in our Modular result, which can be recognized by comparing Modular result with Partitional result in the diversity evaluation by using the NM metric. That is another reason for exploiting the proposed framework in the clustering problems."}, {"section_title": "D. Parameter analysis", "text": "In this section the performance of the proposed method will be analyzed by using the optional features selection (d parameter). This paper employs various data sets, i.e. two low dimension data sets (Wine, Glass), two highdimension data sets (20 Newsgroups, Arcene), and two middle-dimension and also image based data sets (USPS, ADNI) for analyzing the performance of proposed method. Figure 4 illustrates the relationship between the performance of the proposed method based on the percentage of selected features in different data sets. The vertical axis refers to the performance while the horizontal axis refers to the percentage of selected feature in each data set. As you can see in this figure, the optional feature selection can significantly increase the performance of final results on high-dimensional data sets; and also it can dramatically decrease the performance on low-dimensional data sets. Further, it is not more effective on the middle-dimension data sets. This paper offers that the optional features selection will be used only for high-dimensional data sets for handling features-sparsity."}, {"section_title": "V. CONCLUSION", "text": "There are two challenges in Cluster Ensemble Selection (CES); i.e. proposing a robust consensus metric(s) for diversity evaluation and estimating optimum parameters in the thresholding procedure for selecting the evaluated results. This paper introduces a novel solution for solving mentioned challenges. By employing some concepts from community detection arena and graph based clustering, this paper proposes a novel framework for clustering problems, which is called Weighted Spectral Cluster Ensemble (WSCE). Under this framework, a new version of spectral clustering, which is called Two Kernels Spectral Clustering (TKSC), is used for generating graphs based individual clustering results; i.e. Partitional result and Modular result. Instead of entropy based methods in the traditional CES, this paper introduces Normalized Modularity (NM), which is a redefined version of modularity in the community detection arena for general clustering problems. The NM is used on the transformed graph representation of individual clustering results for providing an effective diversity estimation. Moreover, this paper introduces a new solution for combining the evaluated individual clustering results without the procedure of thresholding, which is called Weighted Evidence Accumulation Clustering (WEAC). While the weight of each individual result in WEAC can be estimated with different metrics, the NM was used in this paper. To validate the effectiveness of the proposed approach, an extensive experimental study is performed by comparing with individual clustering methods as well as cluster ensemble (selection) methods on a large number of data sets. Results clearly show the superiority of our approach on both normal data sets and those with noise or missing values. In the future, we plan to develop a new version of normalized modularity for estimating the diversity of Partitional results, directly."}]