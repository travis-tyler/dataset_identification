[{"section_title": "Introduction", "text": "Recent economic research recognizes the importance of the transition to clean technology in controlling and reducing fossil fuel emissions and potentially limiting climate change, 1 while empirical evidence suggests that innovation may switch away from dirty to clean technologies in response to changes in prices and policies. For example, Newell, Ja\u00a4e and Stavins (1999) show that following the oil price hikes, innovation in air-conditioners turned towards producing more energy-e\u00a2 cient units compared to the previous focus on price reduction;  \u2026nds that higher energy prices are associated with a signi\u2026cant increase in energy-saving innovations; Hassler, Krusell and Olovsson (2011) estimate a trend break in factor productivities in the energy-saving direction following the era of higher oil prices; and Aghion et al. (2014) \u2026nd a sizable impact of carbon taxes on the direction of innovation in the automobile industry and further provide evidence that clean innovation has a self-perpetuating nature feeding on its past success. Based on this type of evidence,  suggest that a combination of (temporary) research subsidies and carbon taxes can successfully redirect technological change towards cleaner technologies. Several conceptual and quantitative questions remain, however. The \u2026rst is whether, in the context of a micro-founded quantitative model, reasonable policies can secure a transition to clean technology. The second is whether there is an important role for signi\u2026cant research subsidies conditional on optimally-chosen carbon taxes. The third concerns how rapidly the transition to clean technology should take place under optimal policy. A systematic investigation of these questions necessitates a microeconomic model of innovation and production where clean and dirty technologies compete given the prevailing policies, and the direction of technological change is determined as a function of these policies. 2 It also necessitates a combination of micro data for the modeling of competition in production and innovation with a quantitative model \u2021exible enough to represent realistic dynamics of carbon emissions and potential climate change. This paper is an attempt in this direction. Our \u2026rst contribution is to develop a tractable microeconomic model for this purpose. In our model, which we view as an abstract representation of the energy production and delivery sectors, a continuum of intermediate goods can be produced either using a dirty or clean technology, each of which has a knowledge stock represented by a separate quality ladder. Given production taxes-which di\u00a4er by type of technology, and thus can act as a \"carbon 1 On climate change, see, e.g., Stott et al. (2004) on the contribution of human activity to the European heatwave of 2003; Emanuel (2005) and Landsea (2005) on the increased impact and destructiveness of tropical cyclones and Atlantic hurricanes over the last decades; and Nicholls and Lowe (2006) on sea-level rise. On economic costs of climate change, see Mendelsohn et al. (1994), Pizer (1999), and Weitzman (2009). On economic analyses of climate change, see Golosov et al. (2014), Hassler and Krusell (2012), Krusell and Smith (2009), MacCracken et al. (1999), Nordhaus (1994), Nordhaus and Boyer (2000), Nordhaus (2008), and Stern (2007). On endogenous technology and climate change, see, , Smulders (1995, 1996), Goulder and Mathai (2000), Goulder and Schneider (1999), Grimaud et al. (2011), Hartley et al. (2011), Hassler, Krusell and Olovsson (2011), Popp ( , 2004, and Van der Zwaan et al. (2002). 2   assume that clean and dirty inputs are combined with a constant elasticity of substitution, which allows for a limited form of competition between clean and dirty technologies. tax\"-\u2026nal good producers choose which technology to utilize. Pro\u2026t-maximizing \u2026rms also decide whether to conduct research to improve clean or dirty technologies. Clean research, for example, leads to an improvement over the leading-edge clean technology in one of the product lines, though there is also a small probability of a breakthrough which will build on and surpass the dirty technology when the dirty technology is the frontier in the relevant product line. Research and innovation decisions are impacted by both policies and the current state of technology. For example, when clean technology is far behind, most research directed to that sector will generate incremental innovations that cannot be pro\u2026tably produced (unless there are very high levels of carbon taxes). However, if clean research can be successfully maintained for a while, it gradually becomes self-sustaining as the range of clean technologies that can compete with dirty ones expands as a result of a series of incremental innovations. Our second contribution is to estimate parameters of this model using microdata on R&D expenditures, patents, sales, employment and \u2026rm entry and exit from a sample of US \u2026rms in the energy sector. The data we use for this exercise are from the Census Bureau's Longitudinal Business Database and Economic Censuses, the National Science Foundation's Survey of Industrial Research and Development, and the NBER Patent Database. We design our sample around innovative \u2026rms in the energy sector that are in operation during the 1975-2004 period. 3 We use our sample to estimate two of the key parameters of the model with regression analysis using R&D and patents. We also estimate the initial distribution of productivity gaps between clean and dirty technologies in the economy by allocating the patent stocks of \u2026rms innovating in these technology areas across the three-digit industries in which the \u2026rms are operating. The remaining key parameters are estimated using simulated method of moments. We show that, despite its parsimony, the \u2026t of the model to a rich and diverse set of moments not targeted in the estimation is fairly good. We then combine this structure with a \u2021exible model of the carbon cycle (also used in Golosov et al., 2014). Our \u2026nal and main contribution is to use this estimated quantitative model for the analysis of optimal policy and for a range of counterfactual policy experiments. Though it is intuitive to expect that carbon taxes should do most of the work in the optimal allocation-because they both reduce current emissions and encourage R&D directed to clean technologies-we \u2026nd a major role for both carbon taxes and research subsidies. The research subsidy is initially more aggressive and then declines over time, while we \u2026nd that optimal carbon taxes are backloaded (but also start declining after about 130 years). Despite the di\u00a4erences between the models, the reason for the major role for research subsidies is related to the one emphasized in . 4 Research subsidies are powerful in redirecting technological change, and given this, it is not worth distorting the initial production too much by introducing very high carbon taxes. It is important to emphasize that research subsidies are not used simply to correct a market failure (or an uninternalized externality) in research. In fact, in our model, in the absence of externalities from carbon, or in the special case in which there is only a dirty or a clean sector, the social planner does not have a reason to use research subsidies. This is because a scarce factor, skilled labor, is being used only for research, and thus the social planner cannot increase the growth rate by subsidizing research. 5 The reason why the social planner relies heavily on research subsidies is because when carbon emissions create negative externalities, inducing a transition to clean technology is an e\u00a4ective way of reducing future carbon emissions. In terms of counterfactual policies, we investigate the welfare costs of just relying on carbon taxes and delaying intervention. Delaying optimal policy by 50 years has a welfare cost equivalent to a permanent 1:7% drop in consumption. The costs of relying only on the carbon tax, without any research subsidies, is similar-1:9%. Another useful comparison is to current US policies. We estimate the e\u00a4ective research subsidy from the di\u00a4erential between clean and dirty \u2026rms in our sample in the use of federally funded R&D expenditure. Utilizing this estimate and di\u00a4erent values of e\u00a4ective carbon tax at the moment and its likely values in the future, our estimated optimal policies are quite di\u00a4erent from their US counterparts, and we show that under current US policies, climate change dynamics will be signi\u2026cantly worse. We also consider several variations and robustness checks to show which aspects of the model are important for our main theoretical and quantitative results. In particular, we investigate the implications of using di\u00a4erent discount rates and estimates of the damage of carbon concentration on economic activity, allowing for alternative modelling of emissions, di\u00a4erent degrees of distortions from research subsidies, di\u00a4erent estimates of the microeconomic elasticities in the R&D technology, and di\u00a4erent distributions of initial productivity gaps between clean and dirty technologies. Overall, most of the main qualitative and quantitative features of optimal policy are fairly robust across a range of plausible variations, though a few variations lead to optimal policies that are more aggressive and involve carbon taxes reaching higher levels and remaining high for 200 years or more. Our model combines elements from four di\u00a4erent lines of research, and is thus related to each of these four lines. First, we build on the growing literature on quantitative general equilibrium models of climate change, such as Golosov et al. (2014), Hassler and Krusell (2012), elasticity of substitution formulation of , dirty and clean sectors are not (q-)complements in our model, but explicitly compete in each product line; [4] because of breakthrough innovations, even with fairly aggressive research subsidies and carbon taxes, the dirty sector is eliminated very slowly (in more than 1000 years); and [5] we also allow for research subsidies to create distortions, which plays some role in our quantitative policy conclusions. The third one is one of the most important microeconomic di\u00a4erence, making the production structure more realistic and enabling us to use microdata on innovation and production. 5 The social planner could a\u00a4ect growth by reallocating research activity between incumbents and entrants, but this is neither quantitatively important nor allowed in our analysis (because we restrict subsidies/taxes on the incumbents and entrants to be the same). Adao et al. (2012), Krusell and Smith (2009), Nordhaus (1994), Nordhaus and Boyer (2000), Nordhaus (2008), and Stern (2007). We follow these papers in introducing a simple model of the carbon cycle and the economic costs of carbon emissions in a general equilibrium model, and then characterizing optimal policy. Second, we introduce endogenous and directed technological change along the lines of Acemoglu (1998Acemoglu ( , 2002 in a model where producers have a choice between clean and dirty production methods. In combining these two \u2026rst lines of research, we are following  as well as several other papers listed in footnote 1 above. Third, we develop a tractable but rich model of competition between dirty and clean technologies building on the literature on step-by-step competition as in Harris and Vickers (1995), Aghion et al. (2001), and Acemoglu and Akcigit (2012). Fourth, we model the microeconomics of innovation, employment and output dynamics building on Klette and Kortum (2004), where each \u2026rm consists of a number of products and technologies (di\u00a4erent from other applications, technologies here are di\u00a4erent from products because of the competition between clean and dirty sectors). In estimating a general equilibrium model of \u2026rm-level innovation and employment dynamics, we follow Lentz and Mortensen (2008), Akcigit and Kerr (2010), and Acemoglu et al. (2013). We di\u00a4er from existing work in this area in three important respects, however. First, we combine this type of estimation strategy with a model of clean and dirty technologies and estimate some of the parameters of the R&D technology directly from microdata. Second, rather than focusing on steady-state comparisons, we study non-steady-state dynamics, which is crucial for the question of transitioning to clean technology. Third, we characterize optimal policies in such a framework. The remainder of the paper is organized as follows. Section 2 introduces our model and characterizes the equilibrium. Section 3 describes the dataset we use for estimation and quantitative evaluation, outlines the di\u00a4erent components of our estimation strategy, and presents the estimates of some of the parameters we obtain from our micro data. Section 4 presents the simulated method of moments estimates of our parameters and discusses the \u2026t of the model. Section 5 quantitatively characterizes the structure of optimal environmental policy. In this section, we also conduct a range of counterfactual exercises. Section 6 discusses a range of robustness exercises intended to convey which sorts of assumptions and parameters are important for the qualitative and quantitative results of the paper. Section 7 concludes, while the Online Appendix contains additional details and proofs."}, {"section_title": "Model", "text": "This section presents our baseline model, which is a simple dynamic general equilibrium setup where \u2026nal output combines intermediates produced either using a clean or dirty technology, and the dirty technology also uses an exhaustible resource such as oil. The productivity of the dirty and clean technology for each intermediate is represented by a quality ladder. Production is also subject to taxes, so pro\u2026t-maximizing \u2026nal good producers choose whether to use clean or dirty intermediates as a function of taxes and the productivity gap between the two. Research is conducted both by entrants and incumbent \u2026rms, which already hold a portfolio of products and technologies, and is directed towards clean or dirty technology. Finally, dirty technology contributes to carbon emissions, which create potential economic damage. We next describe each aspect of the model in turn and characterize the equilibrium."}, {"section_title": "Preferences and Endowments", "text": "We model an in\u2026nite-horizon closed economy in continuous time. Since the consumer side is not our focus, we simplify the discussion by modeling it with a representative household with a logarithmic instantaneous utility function and lifetime utility given by where C t is the representative household's consumption at time t and > 0 is the discount rate. The representative household is comprised of unskilled workers, with measure normalized to 1, and \"scientists,\" with measure L s , who will be employed in R&D activities. All workers supply one unit of labor inelastically. The household owns all the \u2026rms in the economy, so it maximizes lifetime utility subject to the following budget constraint and the usual no Ponzi condition. Here t is the total sum of corporate pro\u2026ts net of R&D expenses, w u t and w s t are the wage rates (and thus wage incomes) of the unskilled and R&D workers, and T t is the net lump-sum tax (or transfer) used for balancing the government budget. Because research subsidies may create additional distortions (denoted by D t ), there may be a wedge between consumption and output, so that C t + D t = Y t ."}, {"section_title": "Final Good Technology, Intermediate Production and Pricing", "text": "The \u2026nal good is produced by combining a measure one of intermediates with an elasticity of substitution equal to one. In addition, its production is negatively a\u00a4ected by the amount of atmospheric carbon concentration, which we denote by S t . We follow the formulation suggested by Golosov et al. (2014), which builds on earlier work by Mendelsohn et al. (1994), Nordhaus (1994, 2008 and Stern (2007), and assume where S > 0 is the pre-industrial level of the atmospheric carbon concentration, 0 is a scale parameter, and y i;t is the quantity of intermediate good i. When = 0, (1) gives the standard aggregate production function for combining intermediates to produce a \u2026nal good with unit elasticity of substitution. When > 0, levels of atmospheric carbon concentration above the pre-industrial level reduce productivity with an elasticity of . A feature of (1), which will play a central role in our quantitative exercise, is worth noting: the proportional cost of a unit increase in atmospheric carbon concentration is independent of its current level. Though non-linearities, or even major threshold e\u00a4ects, are likely to be present in the impact of atmospheric concentration on economic activity, this functional form is consistent with assumptions made by other economic approaches to climate change (e.g., Nordhaus, 1994Nordhaus, , 2002Nordhaus, , 2007Nordhaus and Boyer, 2000;Stern, 2007;Golosov et al., 2014). Each intermediate i 2 [0; 1] can be produced with either a dirty or a clean technology, and when it is produced with the clean (dirty) technology, we denote it by y c i;t (y d i;t ). We will sometimes refer to clean and dirty technologies as clean and dirty \"sectors,\" and also use the terms \"intermediates\" and \"product lines\" interchangeably. Firm f can produce intermediate i with either a clean or dirty technology (j 2 fc; dg). The production function for clean technology is where l c i;t (f ) is employment of production workers and q c i;t (f ) is the labor productivity of the technology that this \u2026rm has access to for producing with clean technology in product line i. 6 Focusing on the \u2026rm with the most productive clean technology for this intermediate and suppressing \u2026rm indices, we often write this as y c i;t = q c i;t l c i;t : The production function for dirty technology is similar, except that it also uses an exhaustible resource e i;t : where 2 (0; 1). We assume that the exhaustible resource is owned by a set of competitive \u2026rms, which can extract it with the following technology relying on unskilled labor: where > 0. This implies that the marginal cost of extraction is c e;t = w u t = . The stock of exhaustible resource, R t , evolves according to the law of motion Pro\u2026t maximization of resource owners combined with this law of motion implies that the price of the exhaustible resource, p e;t , follows the Hotelling rule. In particular, this price minus the cost of extraction must grow at the rate of interest (i.e., p e;t w u t grows at the rate r t ). The cost minimization problem of producers using a dirty technology implies Combining this with (2) gives a linear relationship between the output of dirty producers and labor Only \u2026rms with the most advanced technology for intermediate i within the clean or dirty sector will \u2026nd it pro\u2026table to produce it. However, because of taxes j t on sector j at time t, it is not necessarily the most advanced technology between these two sectors that will be active. Given these taxes, the marginal cost of production in line i is if produced with clean technology ( if produced with dirty technology ; where w u t is the wage rate of unskilled workers and is the normalized price of the exhaustible resource at time t. In equilibrium, only the technology with the lower marginal cost inclusive of taxes-or equivalently the one with the higher taxadjusted labor productivity-will produce. Equilibrium production decisions are: We assume that if condition (6) holds with equality, each technology produces with probability 50% at any point in time. 7 Finally, we also assume that at the initial date t = 0, for each leading technology of quality q j i;0 , there also exists an intermediate good of quality q j i;0 = . This ensures that markups in the initial date will be equal to , and this result will hold endogenously in all subsequent dates."}, {"section_title": "Innovation, the Quality Ladder and Dynamics", "text": "Labor productivity for each intermediate (for each technology) evolves as a result of innovation. Research is directed towards clean or dirty technologies. Successful research leads to one of two types of innovation. The \u2026rst is an incremental innovation, which takes place with probability"}, {"section_title": "1", "text": "; and the second is a breakthrough innovation, which takes place with probability (independently of all other events). If research directed to sector j 2 fc; dg leads to an incremental innovation, then the innovator advances by one rung in the quality ladder over the current leading-edge technology of type j in a randomly chosen intermediate. Breakthrough innovations, on the other hand, enable the successful innovator to improve by one rung over the frontier technology, even if this frontier is set by the alternative technology-i.e., a breakthrough clean innovation improves over the dirty technology even if the latter is far ahead of the clean sector, thus allowing the clean sector to leapfrog the dirty one. 8 We assume that each rung in the quality ladder corresponds to a proportional improvement of > 1. Consequently, labor productivity of technology j in intermediate i at time t can be expressed as q j i;t = n j i;t ; where n j i;t 2 Z + is the e\u00a4ective number of steps that this technology has taken since time t = 0 (where the initial levels, q j i;0 's, are set equal to 1 for all i and j without loss of any generality). This also implies that an incremental innovation during an interval of time t leads to a new technology with q j i;t+ t = q j i;t (i.e., an improvement of proportional amount over the leading technology in the same sector), while a breakthrough innovation in sector j that is behind sector j (i.e., with q j i;t < q j i;t ) leads to a new technology with q j i;t+ t = q j i;t , meaning that it builds on the more-advanced technology level of sector j. Given this speci\u2026cation, the relative productivity of dirty to clean technology in intermediate i at time t can be written as where n i;t n d i;t n c i;t 2 Z is de\u2026ned as the technology gap between dirty and clean sectors in product line i at time t. Denoting by z j t the aggregate innovation rate (the sum of incumbents'and entrants'innovation rates) in technology j, the law of motion for the technology gap n i;t can be expressed as where o( t) represent second-order terms that disappear faster than t as t goes to zero. Let us next denote the price-adjusted policy gap by m t , such that whereP e;t is given by (5). The tax-adjusted technology gap can therefore be written as 1 P e;t = n i;t mt : The leading-edge tax-adjusted technology in intermediate i is dirty if n i;t > m t ; the two technologies are neck and neck if n i;t = m t ; and clean is the leading technology otherwise."}, {"section_title": "Firms, R&D and Free Entry", "text": "Following Klette and Kortum (2004), we de\u2026ne a \u2026rm as a collection of leading-edge technologies. Let u j f denote the number of intermediates where \u2026rm f has the leading-edge technology within sector j 2 fc; dg, though these may not be the most advanced technology for the intermediate in question because there is also a competing technology j. We assume that u j f captures the stock of knowledge on which the \u2026rm can build for further innovations with technology j 2 fc; dg. In view of this and without any loss of generality, we simplify the exposition by assuming that each \u2026rm specializes in either clean or dirty technologies. Each \u2026rm can then combine its knowledge stock u j f (in sector/technology j) with scientists (R&D workers) H j f in order to generate a Poisson \u2021ow rate of X j f new innovations according to the production function where 2 (0; 1) is the R&D elasticity with respect to scientists and > 0. The variable cost of generating a \u2021ow rate of X j is therefore w s t uh j , where is the demand for skilled workers (as scientists) given innovation intensity per product line of x j X j =u j and w s t is the wage rate of scientists (we have dropped the \u2026rm index to simplify notation). There is in addition a \u2026xed cost associated with R&D, which is again in terms of skilled workers. In particular, to perform any R&D \u2026rm f will need to hire F I;i scientists per product line, where is an iid draw (across \u2026rms and over time) with mean F I and 2 (0; 1). 9 Hence, the total cost of R&D for \u2026rm i performing R&D directed at technology j 2 fc; dg at time t is C t u; x j = w s t u h j + F I;i;t . Entrants can also undertake R&D directed to either sector by paying a variable cost as in (8) and a \u2026xed cost in terms of F E F I scientists. We denote the endogenously determined mass of entrants performing R&D directed to technology j at time t by E j t . On the policy side, R&D for sector j receives a proportional government subsidy at the rate s j t 2 [0; 1]. Though the government uses lump-sum taxes to \u2026nance its budget shortfall, subsidies to speci\u2026c types of research create various distortionary e\u00a4ects (e.g., because of the ine\u00a2 ciency of picking winners or misdirection of resources). We allow for this by introducing the parameter measuring the extent of such wastage and specifying the aforementioned wedge between output and consumption as D t = S t . This also implies that the government budget at time t is (1 + ) S t = T t :"}, {"section_title": "The Carbon Cycle", "text": "While clean intermediate production y c i;t creates no carbon emission, dirty production y d i;t emits units of carbon per intermediate output. 10 This implies that total amount of carbon emission at time t is where is the total output of the dirty sectors such technology at time t. We follow Archer (2005) and Golosov et al. (2014) in assuming that the atmospheric carbon concentration S t is determined as follows where t = T is the \u2026rst date when emission started and is the amount of carbon emitted l years ago still left in the atmosphere; ' P 2 (0; 1) is the share of emission that remains permanently in the atmosphere; (1 ' P ) ' 0 2 (0; 1) is the fraction of the transitory component that remains in the \u2026rst period; and ' 2 (0; 1) is the rate of decay of carbon concentration over time. As explained in Archer (2005) and Golosov et al. (2014), this speci\u2026cation approximates the complex dynamics of carbon concentration in the atmosphere and provides a good match to the observed dynamics of atmospheric carbon concentration as we show below. 1 0 The assumption that carbon emissions are per unit of intermediate good produced with dirty technology is a plausible baseline. Given our formulation with two types of technological changes, assuming that emissions are directly from the the use of the exhaustible resource would amount to imposing that even dirty technological improvements reduce e\u00a4ective emissions; this is because the same amount of dirty output, which in our empirical work will correspond to energy output, can be obtained with less and less pollution over time-even without any clean innovations. The main motivation for our baseline choice is to avoid this blurring of the distinction between clean and dirty technologies. Alternative emission speci\u2026cations, where it is the use of the exhaustible resource that creates emissions, and where the dirty sector does not use the exhaustible resource, are investigated in Section 6."}, {"section_title": "Prices and Pro\u2026ts", "text": "The aggregate production function (1) implies a unit elastic demand for intermediates which, taking the \u2026nal good as numeraire, can be written as To characterize equilibrium prices, we de\u2026ne the tax-and energy-price-adjusted (for exhaustible resource) qualities b\u1ef9 Therefore, intermediate i will be produced using technology j 2 fc; dg only ifq j i;t q j i;t . Moreover, as explained in the previous section, if the leading technology for intermediate i at time t is q j i;t , another \u2026rm will have access to technology q j i;t = for free, and therefore, equilibrium markups can never exceed . This implies that the price and quantity are given as ; and y j i;t = max Equilibrium pro\u2026ts can then be computed as a function of m t and n as (k) 1 (k) otherwise, and (k) k if k 2 [0; 1] and (k) = if k > 1 summarizes equilibrium markups."}, {"section_title": "Value Functions and Innovation Incentives", "text": "We now characterize innovation incentives. Let us focus on a single \u2026rm, drop the \u2026rm and time indices to simplify notation, and let\u00f1 j h n j 1 ; :::; n j u j i denote the vector of product lines where the \u2026rm in question holds the leading-edge technology of type j 2 fc; dg and n j i is the technology gap compared to technology j within the same product line. Let\u00f1 j i denote the same vector\u00f1 j without its ith element n j i : Then the value of the \u2026rm satis\u2026es the Hamilton-Jacobi-Bellman (HJB) equation: The \u2026rst term on the right-hand side is the pro\u2026ts generated from u j product lines. In addition, at the \u2021ow rate rate z j ; each product line i will be lost to an innovation by another \u2026rm from the same technology j, in which case i is taken out of \u2026rm's portfolio (so that the \u2026rm's portfolio becomes\u00f1 j i ). If instead production line i experiences an innovation from the alternative technology j, which happens at the rate z j , then there are two possibilities. First, if the innovation is incremental (probability 1 ) or technology j is already behind n j i 0 , the technology gap declines by one step to n j i = n j i 1. Second, if the innovation is a breakthrough (probability ) and technology j was leading n j i > 0 , then the \u2026rm falls behind by one step to n j i = 1. Finally, the \u2026rm invests in R&D itself and innovates at the \u2021ow rate X j = u j x j , and the expected return from this R&D (inclusive of costs net of the R&D subsidy for this sector, s j ) is the \u2026nal line of the right-hand side, with the integral accounting for the \u2026xed costs being stochastic. When successful, the \u2026rm adds a new product line so that its portfolio becomes\u00f1 j [ fn j u+1 g. Let us next denote by n;t the fraction of product lines where the dirty lead is exactly equal to n steps at time t. The next lemma provides a convenient re-expression of this HJB equation: wherer t r t g Y;t ; is the e\u00a4 ective interest rate (net of output growth),w s t w s t =Y t is the normalized skilled wage and v j t is the expected per product value of innovation de\u2026ned as Proof of Lemma 1. See the Online Appendix. Intuitively, because there are no technological or product market linkages between the di\u00a4erent product lines in which the \u2026rm has a lead, its value can be written as the sum of the values of each one of its product lines. An important implication of this result is that incumbent innovation rates per product line are independent of their portfolios: where v j n;t is the solution to (15), v j t is given as in (16), andw s t is the normalized skilled wage de\u2026ned in Lemma 1. A number of important qualitative conclusions follow from (17): I. Higher average values of innovation and lower scientist wages increase research e\u00a4ort. II. Subsidies to research increase research e\u00a4ort. This will be important in encouraging clean innovation by means of research subsidies. III. Carbon taxes increase clean research e\u00a4ort (and reduce dirty research e\u00a4ort). This can be seen by considering higher values of m in (13) which, given the distribution of technology gaps n;t , increases c n;t and v c t , because production shifts from dirty to clean technologies (and neck-and-neck sectors shift to positive markups for clean technologies). This shows that carbon taxes alone may be su\u00a2 cient to encourage clean innovation and thus a transition to clean technology. Whether they are optimal is an empirical and quantitative question we consider below. IV. Perhaps most importantly, the nature of innovation is path-dependent in this economy. Consider the fraction of sectors in which clean producers are already making positive pro\u2026ts. The average pro\u2026ts for clean producers is c t X n<m n;t t (m t n) : Since the value function is determined as the discounted sum of these average pro\u2026ts, the sequence f c t g 1 t=0 has a direct impact on innovation incentives through v c t : When there are large technology gaps between dirty and clean, c t is small, discouraging clean innovation and encouraging dirty innovation. But if clean innovation is maintained for a while, then the fraction of sectors with n m increases, raising the probability that clean innovation will improve over a pro\u2026table intermediate. Thus clean innovation can naturally self-reinforce over time, but this is a slow process because the fraction of sectors with n m changes slowly."}, {"section_title": "Free Entry and Labor Market Clearing", "text": "The previous subsection characterized the R&D decisions of incumbents as a function of the state of the economy and policies. The other component of R&D comes from entrants. Using similar reasoning to that for incumbent R&D incentives, the free entry condition for entrants holding as equality if E j t > 0. Inspection of (18) establishes that at time t, there can be positive entry into sector j only if the \"policy-adjusted\"value of innovation is higher in sector j than in sector j. In other words, entrants will direct their R&D to the clean technology if and to the dirty technology if the reverse inequality holds. Conditional on entry, the optimal value of x j E;t is given by equation 17, except that the indicator function now conditions on R&D by entrants in sector j being positive, i.e., I (x j E;t >0) . The labor market clearing condition for skilled workers, combining demand from incum-bents and entrants, is where h( ) is de\u2026ned in (8) above, while x j E;t and x j I;t depend on the normalized skilled wag\u1ebd w s t via (17). This labor market clearing condition thus pins down this normalized skilled wage and indicates that the equilibrium normalized skilled wage will be higher when R&D is more pro\u2026table and is subsidized more heavily. We next characterize labor market clearing for unskilled workers. From the equilibrium production decisions in (12), demand for production workers is Adding the labor used in the extraction of the exhaustible resource, unskilled labor market clearing can be written as This equation shows both the impact of taxes on labor demand (which reduce labor demand and thus wages) and the distribution of technology gaps (because these a\u00a4ect markups). It also shows that if there were only one type of technology (and no extraction of the exhaustible resource), an increase in the tax rate would have no impact on production, just reducing the unskilled wage rate. This is no longer true with two types of technologies, because a tax on dirty technology, for example, would also change the prices of intermediates produced with dirty technology relative to those produced by clean technology, thus impacting production. Next substituting the optimal quantities (12) into the \u2026nal good production function (1), we obtain where Q t exp R lnq it di is the quality index of active tax-adjusted labor productivities, and (n m t ) n;t is an inverse function of equilibrium markups. We can next use (20) to express aggregate output as a function of the quality index of active tax-adjusted labor productivities: where is an adjustment for labor demand coming both from taxes and markups. As noted above, pollution is caused by total dirty intermediate production at time t, which can be expressed as ( 23)where we broke up the quality index by step-size di\u00a4erential n and de\u2026ne Q d with a slight abuse of notation where i 2 n denotes intermediates where the technology gap is n steps). The evolution of the quality indices Q d n;t and Q c n;t are described in the Online Appendix. Intertemporal maximization yields the standard Euler equation, where g C;t is the growth rate of consumption and r t is the interest rate at time t (and in addition we impose the usual transversality condition). Hotelling rule implies that the exhaustible resource price net of the cost of extraction must grow at the rate r t (i.e., at the interest rate), and thus, the exhaustible resource price net of the cost of extraction relative to the unskilled wage, , must grow at the rate r t g w;t . The implied path of the normalized exhaustible resource price is therefore p e;t w u t = p e;0 w u 0 Z t 0 e rs gw;s ds + : An additional boundary condition is given by the requirement that the price of the exhaustible price must satisfy [p e;t w u t ] R t = 0; so that either the entire stock of the exhaustible resource is utilized as t ! 1 or p e;t = w u t for all t 0. Finally, the evolution of the distribution of technology gaps represented by n;t can be derived from the following di\u00a4erential equations (with some initial condition where Intuitively, the fraction of intermediates with the technology gap n changes because of the di\u00a4erence between in \u2021ows and out \u2021ows. There will be in \u2021ows into state n from n 1 when a dirty innovation occurs and from n + 1 when a clean innovation occurs without leapfrogging. Out \u2021ows from the technology gap of n steps will take place due to both clean or dirty innovation (bringing the state into n + 1, n 1, or 1 depending on the innovation type). For n 1, a similar reasoning identi\u2026es We now summarize the dynamic equilibrium path using the equations we have derived in this section. For any given time path of policies satisfy (26) (27) ; [8] total productivity of the sectors with n-step gap Q d n;t evolves according to the innovation rates in (17) and (18); 11 [9] the interest rate satis\u2026es the Euler equation (24);[10] exhaustible resource quantity and price satisfy (4) and (25) ; [11] R t evolves according to the law of motion (3); and [12] S t is given by (9) and (10) with Y d t given by (23)."}, {"section_title": "Empirical Strategy and Data", "text": "Our model has 18 parameters/variables, { , S, , ', ' F E , g, and the initial distribution of technology gaps between clean and dirty technologies, f 0;t g 1 n= 1 , to be determined. As will become clearer below, given f 0;t g 1 n= 1 , the remaining parameters can be estimated without knowledge of taxes and subsidies, and also without any information on S; ; '; ' 0 , and ' P . These parameters become relevant only for our policy analysis. Nevertheless, for completeness we also specify our choices for all these parameters. We proceed in four steps. First, we choose from external sources the parameters of the carbon cycle and the discount rate: ; S; ; '; ' 0 ; ' P ; ; and . Second, we directly estimate L s ; ; and from microdata. Third, we choose the initial distribution of technology gaps to match the distribution of patents between \u2026rms innovating primarily with clean and primarily with dirty technologies as we explain below. Finally, we estimate the remaining parameters ; ; R 0 ; ; F I and F E using simulated method of moments, with moments being selected to model the \u2026rm-level R&D behavior, growth rates, entry/exit rates for the energy sector as we describe below, and from the time path of carbon emissions. The model performs well and is able to replicate these moments reasonably closely. Throughout our focus is on the energy sector, the behavior of which has motivated our theoretical model. The energy sector is de\u2026ned as \u2026rms involved in the sourcing, re\u2026nement and delivery of energy inputs for residential and industrial applications (e.g., oil and gas, electricity), \u2026rms that provide complementary inputs and equipment into this energy-production process (e.g., drilling equipment, power plant technologies), and \u2026rms that interface with the energy inputs for residential and industrial use (e.g., motor manufacturers). As such, the 1576 \u2026rms that make up our sample include oil and gas producers, mining and exploration \u2026rms, engine manufacturers, power companies building upon multiple techniques, energy equipment manufacturers, and similar. The data we use for estimation come from the Census Bureau, and we design our sample around innovative \u2026rms in the energy sector that are in operation during the 1975-2004 period."}, {"section_title": "External Calibration", "text": "We choose the parameters S; ; '; ' 0 ; ' P ; and to link our model to the carbon cycle and its impact on aggregate output following Golosov et al.'s (2014) approach. This approach takes into account the current level of carbon stock and its increase since pre-industrial times; the rate at which new emissions enter the atmosphere, the terrestrial biosphere or shallow oceans, and the deep oceans; how that movement and the various reservoirs of carbon in \u2021uence the earth's temperature; and how higher temperatures and environmental damage hurt the economy. This approach is similar to prior work in environmental economics (e.g., Nordhaus, 2008;Nordhaus and Boyer, 2000), though with some important di\u00a4erences: in particular, it is more \u2021exible in allowing non-linear absorption of atmospheric carbon, but it does not allow any delay on the impact of this carbon content on economic outcomes and temperature changes (e.g., because ocean temperature changes at a di\u00a4erent rate than land temperature) and does not separately keep track of the dynamics of the atmospheric concentration of carbon dioxide (CO 2 ), methane (CH 4 ), and nitrous oxide (N 2 O). The value of the pre-industrial stock of carbon dioxide in the atmosphere S is 581 GtC (gigatons of carbon). To model how emission increases the atmospheric stock of carbon, we de\u2026ne the three parameters '; ' 0 ; and ' P as follows. First, ' P is the portion of new emissions that will remain in the atmosphere for a very long time, likely for thousands of years, and estimates of this parameter from Archer (2005) and IPCC (2007) are about 20%. The other two parameters, ' and ' 0 , govern the short-and medium-term movement of the emitted carbon, which in \u2021uence the earth's temperature over short horizons but are ultimately absorbed into the deep oceans. To identify these parameters, we utilize the 30 year half-life of carbon and match the carbon stock evolution under emissions during the 1900-2008 period. We use the law of motion of the atmospheric carbon concentration S t , given by (10), over the period 1900-2008. 12 The emission data, fK t g 2008 t=1900 , are depicted in Figure 1. 1 2 Namely, St = R t 1900 0 (1 d l ) K t l dl + S1900: As already noted, our damage function also follows Golosov et al. 2014and we choose the same parameter as they do, 5.3 x 10 5 GtC 1 . Because this number may be too low, we investigate the robustness of our results to higher values of in Section 6. In addition, the parameter is chosen to link current world emissions levels to the baseline level of dirty output in the model, Y d as given by (23), so that implied future increases in Y d will translate to correspondingly higher levels of emissions. This formulation assumes that both emissions and technological change in our model can proxy those in the global economy (e.g., because new technologies can be utilized globally). 13 For the elasticity parameter in the dirty production function, we follow Golosov et al. (2014) and set = 0:04. As noted above, the parameter allows for distortionary e\u00a4ects of research subsidies, for example, because it is di\u00a2 cult for the government to know ex ante which projects are most worthy of public support. For instance, the US Department of Energy (DOE) provided $6.3 billion in funding for the support of clean energy research and implementation projects, and there have been some high-pro\u2026le business and technology failures associated with this program (DOE, 2012). A well-known example is that of Solyndra Corporation, a solar panel manufacturer, which received approximately $500 million in support from the DOE, before declaring bankruptcy without achieving large-scale production. Motivated by this, we set our baseline value for to 10% (implying that out of the $6.3 billion, $630 million would be entirely wasted). We will show that our results are robust when is equal to 0% or 20%. The \u2026nal piece of our external calibration is the (private and social) discount rate, which we take as = 1%; close to the 1.5% chosen by Nordhaus. We also show that our results are quite similar, but of course involve more aggressive policies, when the (social) discount rate is 0:1% as assumed by Stern (2007)."}, {"section_title": "Sample Construction and Data Sources", "text": "We use the individual records of patents granted by the USPTO from 1975-2009. We collect patents that are \u2026led by inventors living in the United States at the time of the patent application, and our assigned to industrial \u2026rms. We identify patents related to the energy sector through patent technology codes. The technology codes are the most disaggregated level of the USPTO's classi\u2026cation scheme and number over 150,000. We adopt the prior classi\u2026cations developed by  and . This authoritative prior work is particularly helpful in that they provide classi\u2026cations into various types of energy technologies. We are interested, however, in several technologies (e.g., nuclear power) not considered by   Database, which contains annual employment levels for every private-sector establishment with payroll from 1976 onward. We also employ Economic Censuses that are conducted every \u2026ve years to collect additional plant and \u2026rm operation data (e.g., sales). We match patent data to these operating data using \u2026rm-name and geographic-location matching algorithms. We focus our sample on the years in which Economic Censuses are conducted. This structure matches our data's features and accounts for the lumpiness of innovation outcomes. We thus measure variables using the average of observed values for \u2026rms in \u2026ve-year windows surrounding Economic Census years and have six time periods covering 1975-2004. Our third data source is the NSF's Survey of Industrial Research and Development (R&D Survey) that collects information from all \u2026rms conducting over one million dollars in R&D and subsamples \u2026rms beneath this level. The R&D Survey provides us with information on many \u2026rms'R&D expenditures and employments of science and engineering workers. We use the data, along with the patenting of the \u2026rm, to calculate the innovation production function for the sector (e.g., the and parameters). These calculations only utilize \u2026rm observations for which we always observe reported data. For our broader moments on \u2026rm dynamics, we need a complete distribution that encompasses \u2026rms only subsampled beneath the threshold. sectors, agriculture and transportation, it includes many of the manufacturers of products that are key inputs to these sectors. We de\u2026ne patents as related to dirty technologies if they are connected to the extraction, re\u2026nement or use of fossil-fuel based energy, including oil, coal, natural gas, and shale technologies. We group into clean-energy patents \u2026elds that are related to geothermal, hydroelectric, nuclear, solar, and wind energy. We also include patents for conservation and utilization of energy. Results are robust to reclassi\u2026cations of border group types. Finally, to determine the model's initial conditions, we identify whether \u2026rms are primarily operating in dirty-or clean-energy applications. We \u2026rst classify an observation (a \u2026rm in a given period) as focused on clean energy if 25% or more of its energy-sector patents are devoted to clean-energy \u2026elds; otherwise the \u2026rm is classi\u2026ed as a dirty-energy \u2026rm in the period. We use the 25% threshold as our assignments of clean-energy \u2026elds are conservative compared to dirty-energy \u2026elds. We then describe the \u2026rm overall as a clean-energy \u2026rm if half or more of its time periods achieve this clean-energy focus. The distribution between clean and dirty uses at the \u2026rm level is fairly bimodal-96% of observations have 75% or more of their patents in one technology-making the exact details of these procedures less important. In total, 11% of our 1 4 As the probability that an existing, non-innovative \u2026rm commences R&D or patenting over the ensuing \u2026ve years (conditional on survival) is only about 1%, this exclusion is reasonable. These procedures de\u2026ne the base pool of innovative \u2026rms in the energy sector. 1 5 In the 1992-1997 period, for example, we account for 59% of sales in industries related to coal and oil extraction, re\u2026nement, and shipment; 33% of sales in industries related to electricity production; and 21% of manufacturing sales. Among manufacturing industries, our sample contains higher shares in industries more closely linked with emissions (e.g., 64% in petroleum re\u2026nement, 31% in primary metals). \u2026rms are classi\u2026ed in the clean-energy sub-sector; 14% of energy-sector patents are classi\u2026ed as clean energy."}, {"section_title": "Estimation and Choice of Parameters from Microdata", "text": "We \u2026rst estimate the parameter from our innovation production function, , which can be rewritten as ln(X=u f ) = ln( ) + ln(H=u f ). We measure X by the \u2026rm's count of patents, H by the \u2026rm's R&D expenditures or scientist counts, and u through four proxies of \u2026rm size given below. Our patent count measure is weighted by citations (normalized by the average citations achieved by other patents in the same patent class and application year). To estimate the elasticity as accurately as possible, we use the panel nature of our data and later return to estimating the parameter. As noted earlier, we only use for this exercise \u2026rms that have a full panel of reported R&D data. To focus on higher-quality data for our di\u00a4erenced estimations, we also require that the \u2026rm be present in at least three periods. We \u2026rst estimate a linear regression with year \u2026xed e\u00a4ects t as follows: with standard errors clustered by \u2026rm. We then extend the estimation to allow for \u2026rm \u2026xed e\u00a4ects by \u2026rst-di\u00a4erencing to obtain the following speci\u2026cation: Panel B similarly summarizes eight estimation variants of the \u2026rst-di\u00a4erenced regression (29). The average across these variants is lower at 0.37, with a range of 0.29 to 0.51. We set the baseline value of to 0.5, the mid-point within the range of estimates in Table 1. 16 We next turn to the parameter that describes technology leapfrogging. This process is challenging to model empirically, and we are unfortunately unable to identify exact races between clean and dirty technologies directly within the patent data. This limitation is due to the narrowness of the technology codes that are entirely clean or dirty in application, while patent class divisions are too broad and few in number. We thus identify the rate at which patents with exceptional quality emerge using patent citations and quantify the rate at which patents enter and quickly establish high levels of citations compared to incumbent peers. Speci\u2026cally, we start with our dataset of all energy-sector patents granted to US inventors during the post-1975 period. We calculate among these energy-sector patents the citation count distribution of incumbent patents by year, excluding within-\u2026rm citations. Incumbent patents are de\u2026ned to be those that were applied for 5-10 years before the focal year; we cap at 10 years prior so that we can have a stable window across a time period from 1985 onwards for analysis. Citations are coming from patents being applied for in the focal year. By focusing on patents receiving a citation in a given year, we are e\u00a4ectively looking at technologies that are being actively used, with many incumbent patents dropping out as no one is building on them. We then calculate for new patents the citations they receive by year. We designate a major entrant as any patent that has a citation count that exceeds the 90th percentile of the incumbent distribution in any of its \u2026rst three years. This evaluation approach is designed to keep the incumbent groups (5-10 years earlier) separate from the entrant groups (max of three years earlier). 4.2% of entrants achieve this level of major entrant. We \u2026nd a slightly lower estimate at 4.0% using  de\u2026nitions, and a rate of 4.1% when making the citation distributions speci\u2026c to each patent class. Based upon these \u2026ndings, we set = 4%. Finally, for L s , which is the supply of scientists and engineers involved in R&D-type activities in the model (relative to unskilled workers), we use 5.5%. We calculate this share from Census IPUMS using the 2000 5% sample, focusing on non-group quartered workers aged 20-65, employed in industries closely related to the energy sector. We also require 20 weeks worked within the year and usual hours worked of 20 or more during each week. 5.5% is the share of these workers with college education or more, employed in occupations related to science and engineering. restricting the sample to \u2026rms with energy patents as more than 30% of their patent portfolio yields levels and \u2026rst-di\u00a4erences estimates of 0.744 (0.065) and 0.384 (0.100), respectively. Restricting our sample to \u2026rms identi\u2026ed from  codes yields levels and \u2026rst-di\u00a4erences estimates of 0.704 (0.049) and 0.301 (0.071), respectively. Relaxing our requirement that the \u2026rm be present in three periods yields levels and \u2026rst-di\u00a4erences estimates of 0.614 (0.043) and 0.338 (0.056), respectively. We likewise \u2026nd similar outcomes when incorporating industry-year \u2026xed e\u00a4ects, using unweighted patent counts, or similar adjustments. Finally, the Online Appendix provides similar estimates from Poisson models that allow for zero patenting outcomes."}, {"section_title": "Initial Technology Gaps", "text": "To provide the initial distributions of the model, we develop estimates of the cumulative stock of technologies invented by clean-and dirty-energy \u2026rms using SIC3 industries as approximations of product lines. We develop these distributions in three steps. The \u2026rst step is to calculate the sum of patents by each \u2026rm during the 1975-2004 period and the \u2026rm's distribution of employment across SIC3 industries in these sectors over the same period. We then apportion the \u2026rm's cumulative patent stock across SIC3 industries using the \u2026rm's employment distributions. For each SIC3 industry, we \u2026nally sum the apportioned patents made by clean-and dirty-energy \u2026rms. This sum of patents across all \u2026rms, active or inactive, re \u2021ects the quality ladders structure of our model. These calculations provide us with over 400 estimates of comparative clean-and dirtyenergy stocks. Across these SIC3 industries, clean-energy \u2026rms are estimated to have a higher cumulative patent stock in 13.1% of industries. For data quality and Census Bureau disclosure restrictions, we focus on the upper half of the industry distribution in terms of cumulative clean and dirty patent counts, which has 13.0% of industries being led by the clean-energy stock (within manufacturing and energy production speci\u2026cally, this share is 12.5%). The average gap relative to the frontier in the 13% of cases where clean patents have the lead is 39%, while it is 76% in the 87% of cases where dirty patents have the lead. We convert the patent gaps into the technology gaps in the model using a conversion factor between the patents rates in the data and the innovation rates in the model. In particular, in our model the annual innovation \u2021ow of incumbents is 24:1% per product line (the sum of x c = 4:8% and x d = 19:3%), while in the data the mean of patent \u2021ow is approximately 43 per line. Hence we divide the empirical patent distribution of clean and dirty (which consists of patents registered between 1975-2004) by the conversion factor of 43=0:241, and then round the resulting technology to the nearest integer. This gives us the initial number of improvements n d j;0 and n c j;0 . We then compute the initial productivities as q d j;0 = n d j;0 and q c j;0 = n c j;0 to provide the initialization values. Figure 3 plots the density of the resulting distribution of initial technology gaps between dirty and clean technologies. 17 It shows that in most product lines the dirty technology is only a few steps ahead of clean technology, but there is a long tail of product lines with a large gap between dirty and clean, and a small set in which clean is ahead of dirty. The fraction of product lines with a non-zero gap in terms of step sizes is 90%. Clean energy leads by one or more step sizes in 9% of cases. Dirty energy has a lead of 20 and 50 step sizes or more in 11% and 2% of technologies, respectively. We later consider alternative initial distributions. Figure 3. The distribution of initial productivity gaps between clean and dirty technologies across product lines (a positive number indicates the dirty technology having the lead)."}, {"section_title": "Simulated Method of Moments", "text": "The remaining parameters ; ; F I ; F E ; R 0 and are estimated using the quantitative implications of our model. For the \u2026rst four of these parameters, ; ; F I and F E , we use the simulated method of moments (SMM). This approach chooses the parameter vector so as to minimize the distance between four moments as implied by our model and data, where we index each moment by i: SMM iteratively searches repeatedly across sets of parameter values for ; ; F I and F E until the model's moments are as close as possible to the empirical moments (see Adda and Cooper, 2003, for further details). We also choose the \u2026xed cost heterogeneity parameter, , as 10% and verify that our results are not sensitive to this choice of parameter. We use three moments from the microdata-\u2026rm entry rates, \u2026rm exit rates, and the average R&D/sales ratio of \u2026rms-together with the growth rate of the sector to identify these parameters. The entrant's labor share and exit rates are calculated across the \u2026ve-year intervals of our Census Bureau data and then transformed into annualized net rates of 1:3% and 1:75%, respectively. We match the construction of these entry and exit rate moments in the model. The weighted average R&D-to-sales ratio is 6:56%, using log sales as weights and winsorizing the R&D/sales ratio at the 99th percentile to reduce outliers. The aggregate annual sales growth per worker is 1:23% for the sector across the 1975-2004 period. 18 For the remaining parameters, and R 0 , we use the implied emission path of our model. BP (2010) reports that oil reserves are around 181.7 gigaton, but there is considerable uncertainty around this estimate as well as on coal reserves. Rogner (1997) argues that world coal supplies will be su\u00a2 cient for several hundred years if used at the current rate. This motivates our approach of computing these two parameters to match the level and the growth rate of emissions in 2008 (rather than using a speci\u2026c value of total reserves)."}, {"section_title": "Estimation Results", "text": "In this section, we provide the SMM estimates of the remaining parameters and discuss the \u2026t of our model to non-targeted moments. Finally, we show how atmospheric carbon concentration, temperatures and aggregate output evolve given these parameters in a laissez-faire equilibrium (with no policy intervention) starting from the observed distribution of technology gaps. Table 2 summarizes our parameter estimates. The estimate of = 0:958 implies that a unit of skilled labor on a single product line generates an innovation with probability of approximately 24% per year. Our estimates of the innovation step size, = 1:063, implies a gross pro\u2026t margin of 5:7%, which is reasonable. The model predicts a sizable \u2026xed cost advantage for incumbent \u2026rms: their \u2026xed cost of operation is equal to 5% of the entrants'\u2026xed cost. Finally, our procedure of matching the recent past of emissions delivers the total energy resources as 13; 549 gigatons of carbon equivalent (GtC), which is reasonable given the extent of known reserves.  Table 3 shows the values of the moments used for estimation, which generally match the data quite closely.  Tables 4A 4C. We choose the non-targeted moments to represent aspects of the \u2026rm size distribution and its growth properties, which are quite di\u00a4erent from the moments we targeted in our estimation."}, {"section_title": "Parameter Estimates and Goodness of Fit", "text": "Our \u2026rst non-targeted moment compares the size ratio of the median entrant to the median incumbent \u2026rm. Our targeted moments on entry/exit rates, overall sector growth, and R&D intensity do not directly impose strong constraints on this size distribution of \u2026rms. Table 4A contrasts the size ratios in the model and data with respect to employment, sales, and sales per employee, and shows that our model implications match the data very closely with respect to the latter two metrics, though not as well for employment. 19 We next compare growth distributions. We \u2026rst calculate the unconditional growth rate of employment for each \u2026rm in the model and data de\u2026ned as (Emp t Emp t 1 )=((Emp t + Emp t 1 )=2). As argued by Davis, Haltiwanger and Schuh (1996), this measure of growth has attractive properties like a symmetric treatment of positive and negative growth and bounded values that minimize outliers. We calculate growth over \u2026ve-year intervals. We then calculate the probability that \u2026rms experience substantial movements in either positive or negative directions. Comparing these movements in the model to the data provides insights into how well the innovation step sizes and associated \u2026rm dynamics mirror the sector's true performance. Table 4B shows that the model matches the data quite well on this dimension. The model somewhat overpredicts employment declines of 25% or more, but it closely matches otherwise. Finally, Table 4C turns to a comparison of the model and the data for growth rates of \u2026rms conditional on di\u00a4erent quartiles of the size distribution. For this exercise, we divide \u2026rms into quantiles based upon their initial size in each \u2026ve-year period and then compute the growth rates using the above formula. The employment distribution implied by the model is a little less \u2026ne-grained than the data, as about 50% of our \u2026rms have one product and employment is partially proportional to product counts. Nevertheless, the model's implications match the general patterns in the data, and in particular, the model successfully generates the lack of growth in the top two quantiles compared to the bottom three. Overall, the model performs reasonably well for these diverse tests, providing con\u2026dence in its ability to capture production and innovation dynamics in the energy sector. "}, {"section_title": "Climate Dynamics in the Laissez-faire Economy", "text": "We next describe the implied future equilibrium and atmospheric carbon paths of the model under laissez-faire (meaning no carbon taxes and research subsidies). Given the initial distribution of technology gaps, dirty innovation is more pro\u2026table and with no policy intervention, most R&D is initially targeted to the dirty technology as shown in Figure 4.  Moreover, at these innovation rates, technology gaps and the pro\u2026tability of dirty technologies increase relative to those of clean technologies, and clean R&D rapidly converges to zero. Consequently, in the long run clean technologies disappear completely and dirty technologies take over the entire economy. 20 The obvious implication of this time path of innovations is a steady increase in dirty energy production and carbon emissions. Figure 5 shows an increase in temperature of an additional 11 C in the next 200 years. 21"}, {"section_title": "Policy Analysis", "text": "In this section, we characterize the policies that maximize discounted welfare given our estimated parameters and then consider various counterfactual policy experiments."}, {"section_title": "Optimal Policy", "text": "We start with optimal policy. 22 Throughout, we do not allow the social planner to correct for monopoly distortions, thus limiting ourselves to the policy instruments discussed abovecarbon taxes and subsidies to clean research. 23 In fact, our theoretical analysis makes it clear that what is relevant is the di\u00a4erential tax and subsidy rates for clean vs. dirty energy, motivating us to focus on taxes on dirty production, which we refer to as \"carbon taxes,\"and subsidies to clean innovation. Finally, for computational reasons, we model taxes and subsidies as quartic functions of calendar time. The resulting optimal policies are presented in Figure 6 (with the research subsidy shown on the left axis and the carbon tax on the right axis).  The intuition for why optimal policy relies so much on subsidies to clean research is instructive. The social planner would like to divert R&D from carbon-intensive dirty technologies towards clean technologies. She can do so by choosing a su\u00a2 ciently high carbon tax rate today and 2 1 We compute temperature changes as temperature = ln St ln S = ln 2. 2 2 Because of the non-linear dynamics of atmospheric carbon concentration, optimal policy is not necessarily time-consistent. We ignore this problem by assuming that the social planner is able to commit to the future sequence of taxes and subsidies. 2 3 As mentioned above, in the one-sector version of our model (either with only dirty or only clean technology), taxes or subsidies to research would only a\u00a4ect relative wages of skilled workers (employed in the research sector), and crucially not the aggregate rate of innovation. For this reason, subsidies to clean research or taxes on dirty research are identical in our model. in the future, as this would reduce the pro\u2026tability of production using dirty technologies and secure both a switch to clean production and, on the basis of this, to research directed at clean technologies. However, this is socially costly because given the current state of technology, switching most production to clean technology has a high consumption cost (because the marginal costs of production of clean technologies are initially signi\u2026cantly higher than those of dirty technologies). Hence, the social planner prefers to use the carbon tax to only deal with the carbon emission externality and to rely on the research subsidy to redirect R&D towards clean technologies. Figure 7 shows that the research subsidy is indeed su\u00a2 cient to rapidly switch innovation from the dirty to clean technology.  Figure 6 also shows that while the optimal research subsidy is front-loaded, the optimal carbon tax is hump-shaped. It starts out very low and increases gradually over the \u2026rst 130 years to almost 90%, and then declines back down to zero by about year 180. This is also intuitive: the research subsidy is front-loaded because the social planner would like to switch research towards the clean technology as soon as possible. Once this is achieved and the change in the distribution of technology gaps makes clean research su\u00a2 ciently pro\u2026table, the research subsidy is phased out. Given the research subsidy, which is highly e\u00a4ective in redirecting technological change, the planner initially sets a low carbon tax so as not to excessively distort early consumption. The carbon tax is raised over time, which re \u2021ects two forces; (i) future distortions are less costly to the social planner because of discounting; (ii) as clean technologies improve, the distortionary e\u00a4ects of the carbon tax are reduced. Finally, as most production switches to the clean technology, there is less need for the carbon tax, and it is phased out. 24 Figure 8 depicts the implied path of temperature under the optimal policy. The combination of the research subsidy and the carbon tax keeps the temperature increase below 2 degrees, and in fact, the temperature starts reverting down after about 40 years. This re \u2021ects both the switch of technological change to clean technologies and the role of the carbon tax in discouraging the use of existing dirty technologies."}, {"section_title": "Counterfactual Policy Analysis", "text": "We next investigate the welfare and climatic implications of a range of alternative policies. We \u2026rst focus on two counterfactuals. The \u2026rst concerns the choice of an optimal policy relying only on a carbon tax (i.e., no research subsidy), and the second involves delaying intervention for 50 years and then choosing the optimal policy from that point onwards. The optimal policies are presented in Figure 9. (A) (B) Figure 9. Constrained optimal policies following a 50-year delay and the restriction to only carbon taxes Panel A shows that optimal policy becomes more aggressive when the only policy tool is the carbon tax. The carbon tax in this case starts out higher than in the baseline and continues to increase to above 300%, though it still has a hump-shaped form and declines after about 150 years, coming down to below 200% around year 200 as is visible at the end of the 200-year window shown in the \u2026gure. The much higher level of the carbon tax in this case is because this policy tool is being used not just to reduce current emissions but also to redirect innovation towards clean technologies. An interesting implication of this constrained policy distortion unless it induces some intermediates to switch to clean technology. This means that when there are few intermediates remaining with the dirty technology, the bene\u2026ts from a carbon tax are low, but so are the costs, as we show analytically in the Online Appendix. Nevertheless, along the path induced by that optimal policy, the fraction of product lines using the dirty technology falls below 1% in 100 years, thus signi\u2026cantly reducing the role of carbon taxes thereafter and accounting for the decline of the carbon tax to zero in about 180 years. is that aggregate temperature increases less at long horizons because of the more aggressive carbon tax, but this is at the expense of slower consumption growth, especially early on. As a result, the welfare cost of just relying on carbon tax for optimal policy (in terms of an equivalent loss of initial consumption) is 1:9%. Somewhat paradoxically, delaying the start of optimal policies by 50 years leads to less aggressive policies from that point onwards. This is because the intervening interval has generated a bigger technology gap between the clean and dirty sectors, making a rapid switch from dirty to clean technologies thereafter undesirable (the switch now takes place in about 300 years). Because the economy now generates greater consumption early on, the welfare loss from this 50-year delay is a relatively modest 1:7%. 25 A related counterfactual is to focus on time-invariant policies. Under this restriction, the optimal research subsidy and carbon tax are 63% and 13%; respectively. The welfare loss from using time-invariant policies is also relatively modest, just 1%, which in part re \u2021ects the fact that once the switch to clean technology has taken place, keeping the research subsidy and the carbon tax high only have moderate costs. Finally, we also evaluate climatic and welfare implications of maintaining current US policies (here interpreted for the whole world) relative to adopting the optimal policy. For this purpose, we have tried to estimate the carbon taxes implied by US policies and the current subsidies to clean innovation (relative to dirty R&D) in our sample of \u2026rms. There is much uncertainty about what the carbon tax in the United States will be moving forward. A capand-trade program is likely to be implemented, but it is unclear what the implied carbon tax rate will be. Greenstone et al. (2013) estimate a social cost of carbon equal to about $21 in 2010, expressed in 2007 dollars, and this number is currently being used for cost-bene\u2026t analysis by US agencies. Because future emissions are expected to become more harmful, this social cost estimate is forecasted to increase to $45 in 2050 (in $2007). We therefore use two values for the \"business-as-usual\" carbon tax, 0% consistent with the current situation, and 23% (consistent with a $45 social cost of carbon in 2050). 26 We estimate the current clean research subsidy from our sample as follows: over our full 30-year period, 49% of all R&D expenditures by our clean \u2026rms are federally funded, while the same number is 11% for our dirty \u2026rms. This implies a 43% ((1 0:49) = (1 0:11) ' 1 0:43) subsidy for clean R&D relative to dirty R&D. Because a 43% research subsidy for clean is insu\u00a2 cient to redirect technological change towards clean with no carbon tax, the scenario with a zero carbon tax involves rapid increases 2 5 If we increase to \u2026ve times its baseline value (as we do in our robustness checks below) so that the damages from not reducing carbon emissions become much higher, then policies following a 50-year delay are indeed more aggressive than the baseline and the cost of delaying intervention is signi\u2026cant (about 12.5%). 2 6 In particular, US carbon emissions are 1.58 billion tons in 2002. One metric ton of carbon is equivalent to 3.667 units of carbon dioxide. Our dirty \u2026rms have sales of approximately one trillion dollars in this year. The $45 social cost is $39 in 2002 terms. These numbers imply a real tax rate in 2050 of about 23% ((39 3:667 1:58 10 9 )=10 12 ' 0:23). This carbon tax rate is much less than currently used in Sweden (see Golosov et al. 2014) and also less than the numbers suggested by Nordhaus (2008). in temperature in the \u2026rst several hundred years and leads to close to 100% welfare costs (regardless of the discount rate). Interestingly, however, even with this less than optimal subsidy to clean research, it turns out that the temperature increase can be contained provided that there is a moderate carbon tax of 23%. In fact, because this (suboptimally low) combination of research subsidy and carbon tax is still su\u00a2 cient to induce a rapid switch to clean technology, the welfare loss is only 1:2%."}, {"section_title": "Robustness and Extensions", "text": "In this section, we investigate how our estimation, optimal policy, and counterfactual results are a\u00a4ected by a range of di\u00a4erent approaches, modeling assumptions, and variations on parameter estimates. We economize on space by only reporting the implied optimal policies, even when the variation in question involves reestimation. Overall, we \u2026nd that our main conclusions, especially those concerning the form of optimal policies, are fairly robust across these variations."}, {"section_title": "Three-Step Policy", "text": "The \u2026rst robustness exercise considers policies that take a simple \"step function form\" with three endogenously determined switch points rather than optimal policies modeled as a quartic in time. The optimal policies in this case are depicted in Figure 10, which shows a similar pattern to Figure 6, except that the carbon tax now remains high for a longer interval, coming down only after about 440 years. We therefore conclude that the speci\u2026c computation restrictions we imposed in our baseline analysis are not particularly important for our main conclusions."}, {"section_title": "Figure10", "text": ". Optimal policies restricted to a three-step form."}, {"section_title": "Lower Social Discount Rate", "text": "An important debate in the optimal climate policy literature concerns the social discount rate. Here we follow Stern (2007) and investigate the implications of lower social discount rates, in particular, = 0:5% and 0:1% (while still keeping the private discount rate at 1% so that the implied interest rate is not counterfactually low). Figure 11 shows that with 0.5%, the qualitative features of the optimal policy are very similar to our baseline, though the carbon tax now increases to somewhat higher levels. With the much lower social discount rate of 0.1%, the carbon tax rises much more rapidly to much higher levels so as to cut emissions immediately (since these emissions create persistent costs in the future, which the social planner now cares much more about). But even in this case, the carbon tax has a hump-shaped path, and starts coming down after 150 periods. Overall, we read the patterns in Figure 11 as suggesting that our qualitative, and to some degree quantitative, conclusions are fairly robust to reasonable variations in social discount rate. (A) (B) Figure 11. Optimal policies under a social discount rate of 0.5% and 0.1%."}, {"section_title": "Alternative Speci\u2026cations of Carbon Emissions", "text": "Our model considers emissions that are proportional to the use of dirty output. An alternative is to assume that emissions are proportional to the amount of exhaustible resource used in dirty production as in Golosov et al. (2014). As noted in footnote 10, this amounts to assuming that even technological changes in the dirty sector reduce emissions per unit of energy output. As in Golosov et al. (2014), this would not generate enough emissions to make the climate change implications su\u00a2 ciently costly. We therefore follow their paper in this case and assume that, in addition to having emissions proportional to the use of the exhaustible resource, there is also technological change reducing the cost of extracting the exhaustible resource by 2% a year. Figure 12 shows that the qualitative features of the optimal policy are similar to our baseline in this case, with the only exception that the carbon tax now increases to signi\u2026cantly higher levels and comes down only after about 500 years. Figure 13 shows that if we eliminate the exhaustible resource entirely (so that we have = 0 in the above model and the production function of the dirty sector in (2) simpli\u2026es to , the pattern of the optimal policy in this case is very similar to our baseline.  "}, {"section_title": "Alternative Damage Elasticity", "text": "As noted above, actual damages from atmospheric carbon may be greater than the estimates commonly used in the economics literature. We now show the sensitivity of our results to higher values of these damages, captured by the parameter , in our model. We consider two cases: \u2026rst, where is twice as large as our baseline value of = 5:3 10 5 , and second, where is \u2026ve times as large as the baseline. (A) (B) Figure 14. Optimal policies under alternative economic damage scenarios. Optimal policies are also similar when is twice as large as the baseline, with the only di\u00a4erence being that the carbon tax now increases to a higher level (about 200%) within the \u2026rst 150 years, before declining towards zero again. When is raised to \u2026ve times the baseline, the qualitative pattern is also similar and involves a front-loaded research subsidy and a hump-shaped path for the carbon tax, though the carbon tax now increases to much higher levels before starting to decline. We conclude that the heavy reliance of the optimal policy on research subsidies for the clean sector is robust to a fairly wide range of damage elasticities."}, {"section_title": "Varying the Distortions from Research Subsidies", "text": "We next investigate the robustness of our results to scenarios in which research subsidies create no distortions ( = 0%) or create twice as large distortions as in the baseline model ( = 20%). Without any additional distortions from the use of the research subsidy, optimal policy relies more heavily on this instrument, and the carbon tax increases less rapidly (and because it ramps up slowly, it also starts declining later, after about 540 years). With signi\u2026cantly higher distortions from the use of the research subsidy (20%), the form of the optimal policy is remarkably similar to the baseline, with only slightly higher carbon tax rates. Overall, these results imply that one of our main conclusions, the heavy reliance of optimal policy on research subsidies, is quite robust to reasonable distortions from such subsidies. (A) (B) Figure 15. Optimal policies under di\u00a4erent assumptions on the research subsidy distortions."}, {"section_title": "Alternative R&D Elasticities", "text": "Our baseline results are for = 0:50, which averaged across cross-sectional and \u2026rst-di\u00a4erence estimates. In Figure 16, we reestimate the model using \u2026rst a value of in the ballpark of the cross-sectional estimates ( = 0:65) and then for a value corresponding to the \u2026rst-di\u00a4erence estimates ( = 0:35), and investigate the implications of this for optimal policy. The resulting optimal policies are again similar to the baseline, except that with = 0:35, because research more heavily relies on the past stock of knowledge, redirecting technological change towards the clean sector is somewhat less e\u00a4ective, and the social planner makes heavier use of the carbon tax (which increases to 530% in the \u2026rst 200 years, coming down only after about 390 years). When = 0:65, the optimal policy is very similar to the baseline. "}, {"section_title": "Alternative Leapfrogging Probabilities", "text": "We also investigate the implications of lower and higher leapfrogging rates ( = 0:03 and 0:05). Figure 17 shows that the results are very similar to the baseline, with the only noteworthy di\u00a4erence being that with = 0:03, the slower rate of leapfrogging increases the reliance on the carbon tax slightly, leading to a somewhat higher peak (around 140%) for the optimal carbon tax schedule. "}, {"section_title": "Alternative Initial Technology Distribution", "text": "Finally, we consider a variation on the initial technology distribution. We compute an alternative initial technology gap distribution with three modi\u2026cations: [1] we weight patents by their normalized citation counts; [2] we consider four-digit (SIC4) industries, and [3] we focus on the manufacturing and energy sectors. There are 332 SIC4 industries that are of su\u00a2 cient size to pass disclosure restrictions, and 9.4% are led by the clean technologies. 27 Figure 18 shows that the optimal policy is again very similar to the baseline. Overall, the robustness exercises presented in this section show that the qualitative, and even quantitative, patterns of optimal policy, including the heavy reliance on research subsidies, are fairly robust."}, {"section_title": "Conclusion", "text": "One of the central challenges facing the world economy is reducing carbon emissions, which appears to be feasible only if a successful transition to clean technology is induced. This paper has investigated the nature of a transition to clean technology theoretically and empirically. We developed a microeconomic model where clean and dirty technologies compete in production and innovation. If dirty technologies are more advanced to start with, the potential transition to clean technology can be di\u00a2 cult both because clean research must climb several steps to catch up with dirty technology and because this gap discourages research e\u00a4ort directed towards clean technologies. We estimated our key model parameters from \u2026rm-level microdata in the US energy sector, using regression analysis and SMM. Our model performs fairly well in matching a range of patterns in the data that were not directly targeted in the estimation, giving us con\u2026dence that it is useful for the analysis of the transition to clean technology in the US energy sector. Theoretically, carbon taxes and research subsidies encourage production and innovation in clean technologies. The key question we investigate using our estimated quantitative model is the structure and time path of optimal policies, how rapidly they will be able to secure a transition to clean technology and slow down the potential increases in temperatures, and what the costs of alternative, non-optimal policies are. A natural intuition would be that only carbon taxes should be used because the key externalities in this model are created by carbon (in the absence of these carbon externalities, the social planner would have no reason to interfere with or subsidize research). In contrast to this intuition, we \u2026nd that optimal policy heavily relies on research subsidies, and this result is very robust across a range of variations and for di\u00a4erent damages and social discount rates. We also use the model to evaluate the welfare consequences of a range of alternative policy structures. For example, just relying on carbon taxes or delaying intervention both have signi\u2026cant welfare costs. Though, to the best of our knowledge, this is the \u2026rst attempt to develop a microeconomic model of the transition to clean technology and to quantitatively characterize optimal policy in such a setup, our paper has inevitably left several questions unanswered and taken a number of shortcuts, all of which constitute interesting areas for future research and investigation. We list some of these that we view as particularly important here. First, we have abstracted both from cross-country variation in policies and from the endogenous speed of clean technology transfer across countries, which is likely to be central for the future of climate change and for the structure of optimal policy. Second and related, we have also abstracted from game-theoretic interactions in emissions, policies and technology choice across several countries in the global economy, which become important when multiple countries individually choose their policies (e.g., Harstad, 2012;Dutta and Radner, 2006). Third, we have followed the literature in this area in ignoring non-linear threshold e\u00a4ects in the impact of atmospheric carbon on economic e\u00a2 ciency. Incorporating such non-linearities, together with an explicit approach to uncertainty along the lines of Weitzman (2009), would be an important area for future research. Fourth, as noted above, our optimal policies are characterized under the assumptions of commitment to the policy sequence by the social planner. A major next step is to characterize timeconsistent optimal policy in the absence of perfect commitment. Fifth, another interesting area is to investigate the interactions between international trade, technology and emissions (see . Finally, our framework can also be augmented by considering a richer set of possible technological improvements, including those that enable expansions in the stock of exhaustible resources and technologies, such as carbon sequestration, that reduce the climatic damage from dirty technologies. by a multiplicative factor : Likewise, when there is a new incremental clean innovation at the rate (1 ) z c t in sectors with n + 1; they enter into state n. Finally, there is an out \u2021ow from state n through clean or dirty innovations that occur at the rate z t . The other lines represent the \u2021ow equations for state n 1; n = 0 and n < 0 respectively. Similar \u2021ow equations can be expressed for clean technologies as follows: A-1."}, {"section_title": "Optimal Constant Policy", "text": "To develop an intuition about the nature of optimal policy, we now consider a simpli\u2026ed optimal policy problem. In particular, we abstract from dynamics and from the exhaustible resource, and assume that the dirty output enters linearly into the production function, so that This implies that the objective of the social planners to maximize net output. In the absence of the exhaustible resource, the unskilled labor market clearing condition becomes Let us rank sectors from 0 to 1 with ascending x i , and also for ease of exposition, let us make the following assumptions: 1. x i 's distribution F is continuous and di\u00a4erentiable with density f , 2. all sectors charge a markup of . Production is linear and satis\u2026es y i = q j i l j i , depending on which j 2 fd; cg is used. Furthermore, let us also normalize q d i = 1. We can order the product lines by x i and de\u2026ne the cuto\u00a4 to be , which will be a function of the tax , so that we can rewrite (A-1) as Note that the marginal product line can be de\u2026ned as x = 1 + and = 1 F (1 + ) : In addition, noting that all dirty sectors will use the same labor and all clean sectors will use the same labor, and taking into account the impact of the carbon tax on dirty labor demand, we have Now we can show that Let us assume that Then from (A-3) we get the fraction of dirty sectors as = 1 1 + and the total sum of the dirty output in (23) Now the objective function can be expressed as The \u2026rst-order condition of the maximization problem is This can be expressed as Note that (0) = 0 and lim !1 ( ) = 1: Note also that ( ) is monotonically increasing in : Hence from intermediate value theorem the optimal tax rate ( ) exists and it is unique. Moreover, the optimal tax rate is increasing in the damage parameter :"}, {"section_title": "A-2 Computational and Estimation Algorithms", "text": "A-2.1 Computational Algorithm Our theoretical analysis shows that the key microeconomic decisions are independent of climate dynamics. We therefore solve for value functions, innovation rates, and distributions \u2026rst, then use those to \u2026nd the time path for the carbon stock, temperature, and other variables of interest. The solution algorithm for this model involves \u2026nding the transition dynamics as the \u2026xed point of a forward-backward iteration process, as in Conesa and Krueger (1998). To compute this \u2026xed point, we \u2026rst update the product line distribution ( ) in the forward direction and the value functions (V j ) in the backward direction, using the long-term (clean) steady state as the terminal condition. For the estimation, we simulate a large panel of 16,384 \u2026rms, each with an evolving portfolio of product lines with various technological leads (n). After a lengthy burn-in time, we then sample the targeted statistics in order to \u2026nd the best \u2026t parameters. We use standard optimization routines to compute optimal policies. When focusing on constant policies, we use a straightforward grid search to \u2026nd the optimum. In the time-varying case, we parameterize policies using a quartic carbon tax and a quartic research subsidy. We then search over this space of functions using a combination of a simple simulated annealing algorithm (Kirkpatrick et al., 1983) and a Nelder-Mead (simplex) algorithm (Nelder and Mead, 1965). To solve for the \u2026xed point of the sequence of value functions, we \u2026rst discretize time into N = 2048 steps and set a terminal period T = 2000. Due to the symmetry between technology types inherent in this model, when a single type of technology is dominant-in the sense that the technology gap distribution is heavily skewed to either clean or dirty technology-one can analytically characterize value functions v j n;1 and innovation rates x j 1 and z j 1 . We use these values as terminal conditions, though we do not know in advance which technology (clean or dirty) will be dominant. In addition, we set large upper (100) and lower ( 100) bounds on the step gap distribution space. The algorithm proceeds as follows: 1. Posit an initial guess for the value function at time zero of the form where z represents an estimated rate of creative destruction (we use z = 0:15). Instantiate the technology gap distribution using the patent data with n;t (0) = n;0 8t: 2. Iterate forward in time from t = 0 to t = T by \u2026nding innovation rates x j t and z j t given value function and product distributions guesses at time t + 1, v j n;t+1 (k) and n;t+1 (k). Using these innovation rates, update the time t + 1 product distribution n;t+1 (k + 1) using discrete time versions of the \u2021ow equations in (27). 3. Find the implied dominant technology at the terminal period by determining which technology type has a higher aggregate innovation rate as some late stage period T T P (we use T P = 200). Use the corresponding terminal value functions to update v j n;T (k + 1) = v j n;1 . 4. Iterate backward in time from t = T to t = 0 by updating value function v j n;t 1 (k + 1) using v j n;t (k) and n;t (k) according to a discretized version of the HJB equation in Lemma 1, re-solving for innovation rates x j t and z j t in the process."}, {"section_title": "Repeat steps 2-4 until the convergence criterion", "text": "is met. We use \" = 10 6 . In order to avoid any instability, particularly when one is close to a threshold where the asymptotically dominant technology switches over, we also introduce heterogeneity in incumbent \u2026xed costs as explained in the text. 28 Using up-to-date computer hardware, the equilibrium solver takes anywhere from \u2026ve seconds to two minutes, depending on the speed of convergence. The code is written mostly in Python, with core routines written in C/C++. Finally, in our of policy analysis, we approximate optimal research subsidies and carbon taxes as quartic functions of time. In Section 6, we verify that the qualitative patterns are similar when we approximate these policies by step functions."}, {"section_title": "A-2.2 Estimation Algorithm", "text": "To \u2026nd the moments used in the SMM estimation, 29 we simulate a panel of 16; 384 \u2026rms using equilibrium variables from the above model solving routine. Each \u2026rm has a portfolio of product lines with various technology gap values. We cap the maximum number of product lines a \u2026rm can have at 200. In order to determine the sales and R&D activity of the \u2026rm, we must keep track of both the number of product lines it is currently operating in, as well as the knowledge stock of the \u2026rm, which can in general di\u00a4er. We simulate this panel of \u2026rms for 5 years to replicate the data generating process, and discretize time to have 100 subperiods per year, so that the simulations have 500 periods. 2 8 A similar heterogeneity can be introduced in entrant \u2026xed costs, but because entrants are never in the region where this heterogeneity matters, this makes no di\u00a4erence to our numerical procedures. 2 9 McFadden (1989), Pakes and Pollard (1989) and Gourieroux and Monfort (1996) characterize the statistical properties of the SMM estimator."}, {"section_title": "A-5", "text": ""}, {"section_title": "A-3 Further Details on Data and Sample Construction", "text": "Our patent database was \u2026rst developed by the NBER and was subsequently extended by HBS Research to include patenting in recent years. Each patent record provides information about the invention and the inventors submitting the application. Hall et al. (2001) provide extensive details about these data, and Griliches (1990) surveys the use of patents as economic indicators of technology advancement. Our focus is on industrial assignees with inventors in United States. In a representative year, 1997, this group comprised about 77,000 patents (about 40% of the total USPTO patent count in 1997). We focus on technology codes, that number of 150,000, given the broad nature of patent classes (about 450 groups). This is important as energy-sector patents are spread out over multiple patent classes-two examples related to solar energy are \"Power Plants/utilizing natural heat/Solar\" and \"Stoves and Furnaces/Solar heat collector\". Moreover, we use patenting technologies to classify \u2026rms as being primarily clean-or dirtyenergy \u2026rms. This separation can only be done at the technology level as the patent class level includes both types (e.g., \"Power Plants\" includes technologies for coal-powered plants too). As a representative year, 1997, our energy-related patents comprised 7.6% of the total US patent count. Our classi\u2026cations follow closely  and , and we report results for our key parameters that just use their classi\u2026cation system. When seeking to extend their work, we use three steps. We \u2026rst utilize resources from the OECD's work to identify environment-related technologies. OECD (2011) lists such technologies using the International Patent Classi\u2026cation (IPC) scheme, which some observers believe is better designed to identify and group environment-related technologies than the USPTO classi\u2026cation framework. We use concordances between the IPC and USPTO framework to identify additional technologies to be included. We next use information on energy-related R&D data from the NSF R&D Survey. For the \u2026rms identi\u2026ed in this survey to be conducting energy-related R&D, we list their patent technology codes and frequency. We then manually search the 600 most-frequent codes to identify if they are energy related. In a related \u2026nal step, we also manually search the USPTO database using key words like \"Coal\"and \"Solar\"to determine relevant technologies. This identi\u2026cation process constructs a pool of patents related to the energy sector. 30 We match the patent data to these operating data using \u2026rm-name and geographic-location matching algorithms. The basic concept in these algorithms is to identify Census Bureau \u2026rms that have similar names to USPTO patent assignees and that have establishments in the same geographic area as where inventors of the patents are located. 31 This linkage also accomplishes 3 0 Energy-related patents account for 5%-15% of US patents over our sample period, with a declining share in recent years; in absolute terms, patent counts for the energy sector are stable or growing throughout the period. The declining share is partly due to the sector not growing as fast as others, and partly due to external changes over time to allow for patents to be made in sectors that traditionally did not patent, especially software patents. 3 1 The algorithms are described in detail in an internal Census Bureau report by Ghosh and Kerr (2010). This patent matching builds upon the prior work of Balasubramanian and Sivadasan (2011) and Kerr and Fu (2008). A-6 a related step of aggregating patent assignees to \u2026rms, as some \u2026rms \u2026le patents through multiple patent assignee codes. This aggregation is due to the Census Bureau's establishment-\u2026rm hierarchy, as we observe establishment-level names within multi-unit \u2026rms that help identify subsidiaries and major corporate restructurings like mergers and acquisitions, and through the name-matching process that consolidates slight name variants over patent assignees. We focus our sample on the years in which Economic Censuses are conducted, speci\u2026cally every \u2026ve years starting in 1977 and ending in 2002. We adopt this focus for several reasons: [1] the operating data are often best measured around these years due to heightened Census Bureau resources, [2] some speci\u2026c variables from the Economic Censuses are only available at those \u2026ve-year marks, and [3] our innovation data are most appropriately considered over short time periods. The third rationale is due, for example, to lumpiness in \u2026rm applications for patents; our R&D expenditures data are also often collected biannually. We thus measure variables using the average of observed values for \u2026rms in \u2026ve-year windows surrounding these Economic Census years (e.g., 1985-1989 for the 1987-centered period). The R&D Survey is the US government's primary instrument for surveying the R&D expenditures and innovative e\u00a4orts of US \u2026rms. This is an annual or biannual survey conducted jointly by the Census Bureau and NSF (it is biannual for most of our sample period). The survey includes with certainty all public and private \u2026rms, as well as foreign-owned \u2026rms, undertaking over a minimum threshold of R&D expenditure in the United States. For most of our sample period, this expenditure threshold is one million dollars of R&D within the US. The survey frame also sub-samples \u2026rms conducting less than the certain expenditure threshold. These micro-records begin in 1972 and provide the most detailed statistics available on \u2026rm-level R&D e\u00a4orts. In 1997, 3,741 \u2026rms reported positive R&D expenditures that sum to $158 billion. Foster and Grim (2010) and Akcigit and Kerr (2010) discuss these data in greater detail. When calculating our innovation production function for the sector (e.g., the and parameters), we only utilize \u2026rm observations for which we always observe reported data on R&D expenditures or scientist counts-meaning that these calculations use only \u2026rms that conduct more than the minimum threshold of one million dollars in R&D or are sub-sampled completely. While this might present an issue for sectors like consumer internet start-ups, this is not very restrictive for the supply side of the energy sector given the large amounts of R&D expenditures required by many start-ups. For our broader moments on \u2026rm dynamics, this minimum threshold creates a challenge, however, for the consistent calculation of the entry margin and growth rates. Our model requires that \u2026rms be innovative from the start of their lives, and thus an innovative \u2026rm that falls below threshold value in its \u2026rst period would be inappropriately dropped if we restricted the sample only to \u2026rms for which we always observe R&D expenditure. By contrast, the patent data are universally observed. To ensure a complete distribution, we thus use patents to impute R&D values for \u2026rms that are less than the certainty A-7 threshold and not sub-sampled. 32 As the R&D expenditures in these sub-sampled cases are low (by de\u2026nition), this imputation choice versus treating unsurveyed R&D expenditures as zero expenditures conditional on patenting is not very important. The \u2026rm does not need to conduct R&D or patent in every year of the initial \u2026ve-year window, but the \u2026rm must do one of the two activities at least once. The main text outlines how we focus on \u2026rms that are initially innovative, including those that transition out of innovation, but do not allow non-innovative incumbents to enter. Note that it would have been impossible to build a consistent sample that would also include incumbents switching into innovation. To see why, consider keeping all of the past records for incumbent \u2026rms that start conducting R&D in 1987. In the prior periods, this approach would induce a mismeasurement of exit propensities and growth dynamics because a portion of the sample will include \u2026rms conditioned on survival until 1987. Thus, our compiled dataset includes innovative \u2026rms in the energy sector from 1975-2004. A record in our dataset is a \u2026rm-period observation that aggregates over the \u2026rm's di\u00a4erent establishments. 33 In addition to the statistics reported in the main text, our sample accounts for over a trillion dollars in sales, 3.9 million employees, and 25 billion dollars in R&D expenditures, and the \u2026rms obtain 56,000 patents during 1995-1999. Several additional points are worth noting about the sample and our data approach. First, we generally include technologies that are designed to make fossil fuels cleaner in the dirtyenergy group. In this one regard, we deviate from the classi\u2026cations developed by  where coal liquefaction and gasi\u2026cation are included in alternative energy, for example. When we directly use  technology scheme as a robustness check, we classify the technologies as in their original work. Second, we have not built our sample selection or grouping procedures around technologies related to pollution abatement. We retain all patents for included \u2026rms, and thus they are part of our overall technology description, but we only use energy-directed patents to classify patents and \u2026rms into dirtyor clean-energy groups. Finally, we also use the more detailed information the R&D Survey collects from selected major R&D producers. We speci\u2026cally utilize information collected from about 100 \u2026rms on R&D expenditures related to speci\u2026c energy applications like coal or solar energy. We earlier identi\u2026ed one application of this extra information in that we manually searched the major patenting technology codes from these R&D entities to identify energysector patenting groups that we should be including. A second application is to verify our data development procedures, for example by assigning \u2026rms based upon the types of R&D they conduct rather than observed patents. This group from the R&D Survey also con\u2026rms the bimodal nature of our \u2026rm groupings. While the group of \u2026rms asked these questions is too small and selected to serve as the backbone of our sample, these checks are comforting. While Census Bureau disclosure prevents us from listing \u2026rms, we overlap well with  listed producers as one example. Tables A1 and A2 provide additional results mentioned in the main text:  16% 44% Table A3 summarizes the alternative initial technology distribution considered in the robustness analysis. The average gap to the frontier for dirty-patent stocks in the 9% of cases where clean patents have the lead is 463 patents, or in relative terms, 33% of the total patenting in that line to date. The average gap to the frontier for clean-patent stocks in the 91% of cases where dirty patents have the lead is 624 patents and 82% in relative terms. The distribution graph has a broadly similar shape as Figure 3. The fraction of product lines with a non-zero gap in terms of step sizes is 82%. Clean energy leads by one or more step sizes in 7% of cases. Dirty energy has a lead of 20 and 50 step sizes or more in 8% and 2% of technologies, respectively. "}]