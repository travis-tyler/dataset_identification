[{"section_title": "", "text": "A two-stage sampling methodology was utilized. In the first stage, the institution sample was drawn based on a probability proportional to size (PPS) selection methodology, where each institution was assigned a composite measure of size (MOS) that reflected the number of eligible faculty and instructional staff in each of six strata. A sample of 1,080 postsecondary institutions was selected for participation; 1,070 * of these were eligible. Each institution was asked to provide a list of all of the full-and part-time faculty and instructional staff that the institution employed during the fall 2003 term. Institutions were asked to include all employees with faculty status (both instructional and non-instructional) and all others with instructional responsibilities, regardless of faculty status. A total of 980 institutions provided a list suitable for sampling. In the second stage of sampling, full-and part-time faculty and instructional staff employed by participating institutions as of November 1, 2003 were selected. Sampling was conducted on a flow basis, as lists were received, checked for accuracy, and processed. A total of 35,630 faculty were sampled from participating institutions. Of these, 34,330 were eligible."}, {"section_title": "Instrumentation", "text": "The NSOPF:04 institution questionnaire was designed to be self-administered via the Internet; the NSoFaS:04 website for institutional participation provided secure access to the questionnaire and information about each component of the study. To expedite completion, it could also be administered as a computer-assisted telephone interview (CATI), if necessary. The instrument was divided into major sections that collected information on the number of faculty and instructional staff employed at the target institution, the policies and practices that affected full-time faculty and instructional staff, the policies and practices that affected part-time faculty and instructional staff, and the percentage of undergraduate instruction assigned to various instructional personnel. The NSOPF:04 faculty instrument was also designed as a web-based instrument for selfadministration via the Internet and by CATI for nonresponse follow-up. The faculty website, like the institution website, provided secure access to the self-administered questionnaire as well as additional information about the study. Both instruments were designed to accommodate the mixed-mode data collection approach and to ensure the collection of high-quality data. Design considerations included appropriate question wording for both self-administered and telephone interviews, and checks for out-of-range or inconsistent values. The faculty instrument consisted of the following eight sections grouped by topic: \u2022 employment during the fall 2003 term (including academic rank, tenure status, and field of teaching); \u2022 academic and professional background (including highest degree earned and employment history); \u2022 institutional responsibilities and workload (including instructional activities and other work responsibilities performed in a typical week); \u2022 scholarly activities (including productivity, funding of scholarly activities, and field of research); \u2022 job satisfaction and retirement plans; \u2022 monetary compensation (including income from the institution and other sources, structure of the employment contract, and household income); \u2022 sociodemographic information (including gender, race, date of birth, marital status, number of dependent children, and citizenship); and \u2022 opinions about working conditions at the institution.\nThis section describes the institution and faculty instruments that were developed for the NSOPF:04 full-scale study conducted during the 2003-04 academic year with a national sample of postsecondary institutions and faculty and instructional staff. Data collection for the study was by self-administered questionnaires on the Internet or computer-assisted telephone interviews (CATIs) with web nonrespondents. In contrast to the data collection approach for NSOPF:99, no paper-and-pencil questionnaire options were provided. 9 Facsimiles of the electronic instruments, which provide item wording, response options, and information on respondent groups, are included in appendix C."}, {"section_title": "v Institution Contacting", "text": "Sampled institutions were contacted by mail, e-mail, and telephone beginning in spring 2003 to allow institutions sufficient time to plan for the study and to resolve any potential roadblocks to participation. Institution contacts were designed to verify institutional eligibility, secure timely participation in each survey component, and identify a staff person at each institution-called the Institution Coordinator-to respond to all NSoFaS:04 data requests. The Institution Coordinator was mailed an introductory letter and accompanying information packet, and then contacted by telephone to confirm the institution's intent and ability to participate within schedule constraints. At this time, each coordinator was asked to complete a Coordinator Response Form that confirmed the data items requested for each component of NSoFaS:04 and the projected deadlines for completion of the study. Upon request, project staff prepared additional information packets for Institutional Review Boards (IRBs) and other deliberative bodies within institutions to secure the institution's participation. Beginning in fall 2003, each Institution Coordinator was mailed a binder containing complete specifications for participation. Institution Coordinators were asked to provide electronic lists of all eligible faculty and instructional staff on November 1, 2003, and to complete the institution questionnaire by December 6, 2003. Follow-up activities continued with the Institution Coordinator until all requested data was supplied. Of the 1,070 eligible institutions, 980 (91 percent unweighted and weighted) provided faculty lists, and 920 (86 percent unweighted; 84 percent weighted) completed the institution questionnaire."}, {"section_title": "Help Desk and Interviewer Training", "text": "Training programs were developed for help desk operators who would respond to questions of sample members attempting to complete the web-based survey and for telephone interviewers who would conduct the nonresponse follow-up. Help desk operators received specific training in \"frequently asked questions\" regarding the instrument and technical issues related to completion of the self-administered questionnaire via the Internet. In addition, help desk operators received the same training as telephone interviewers because they were expected to complete the instrument over the telephone if requested by a caller. The telephone interviewer training focused on techniques for successfully locating and interviewing sample members, and covered such topics as administrative procedures required for case management, quality control of interactions with sample members and other contacts, and the organization and operation of the web-based faculty instrument to be used in data collection."}, {"section_title": "Faculty Locating and Survey Completion", "text": "NSOPF:04 data collection procedures were designed to locate sample members, encourage prompt completion of the self-administered questionnaire via the Internet, and conduct telephone interviews with nonrespondents. Upon receipt of faculty lists, contact information for the sampled faculty and instructional staff was reviewed and assessed for completeness. Incomplete information was supplemented by searches of the institution's website for telephone and address information. Intensive tracing was performed when all telephone numbers for a respondent were exhausted. Faculty data collection utilized a mixed-mode approach; sample members could participate either by web-based self-administered questionnaire or by an intervieweradministered telephone interview. The participation of sample members was initially requested in a letter, which provided both instructions for completing the web questionnaire and completing the interview via CATI. Periodic reminder letters and e-mail messages were sent to nonrespondents to encourage their participation. After 4 weeks, interviewers began calling the sample members directly to attempt a CATI interview. An early-response incentive was provided to encourage prompt completion of the instrument. Incentives were also offered to sample members who refused or were unresponsive. Of the 34,330 eligible sample members, 26,110 (76 percent, unweighted and weighted) completed the faculty questionnaire during a field period from January to October of 2004. Seventy-six percent of respondents completed the self-administered web questionnaire, and 24 percent were interviewed by telephone. The average time to complete the survey was 30 minutes."}, {"section_title": "Evaluation of Operations and Data Quality", "text": "Evaluations of operations and procedures focused on the joint institution contacting endeavor, the timeline for data collection from institutions (faculty lists and institution questionnaires) and faculty (CATI and self-administered interviews), tracing and locating procedures, refusal conversion efforts, the effectiveness of incentives, and the length of the faculty interview. Results of the data quality evaluations included the following: \u2022 Eighty-two percent of faculty list counts were within 10 percent of the corresponding institution questionnaire counts. There were greater variances between list counts and IPEDS, which is based on a narrower definition of faculty. Patterns of discrepancies between IPEDS and list data followed expected patterns, with list counts larger than those from IPEDS. \u2022 Item nonresponse was below 15 percent for 87 of the 90 items in the institution questionnaire and for 141 out of the 162 items in the faculty questionnaire. \u2022 Of the 26,550 eligible sample members who started the interview, 570 (2 percent) broke off before completing the interview. Of these, 430 broke off before completing the workload section and were not considered to be partial completes. Of the 140 partial completes, 48 percent broke off in the scholarly activities section; 9 percent broke off in the job satisfaction section; 29 percent in the compensation section; 11 percent in the characteristics section; and 4 percent in the opinions section. \u2022 A new assisted coding system, used to code field of teaching, highest degree field, and principal field of scholarly activity, coded 77 percent of verbatim strings; 23 percent of strings required manual coding. \u2022 A recoding of 10 percent of teaching, research, and highest degree verbatim strings showed 71 percent were coded correctly, 13 percent incorrectly, and the remaining 15 percent were too vague to code. The coding performed by web respondents was more often accepted as correctly coded than that done by CATI interviewers."}, {"section_title": "Background and Purpose of NSOPF:04", "text": "NSOPF:04 was a comprehensive nationwide study of the characteristics, workload, and career paths of postsecondary faculty and instructional staff. 2 The study was based on a nationally representative sample of all full-and part-time faculty and instructional staff at public and private not-for-profit 2-and 4-year degree-granting institutions in the United States. The NSOPF:04 full-scale sample consisted of 35,630 faculty and instructional staff selected from 980 sampled institutions in the 50 states and the District of Columbia. 3 NSOPF:04 comprises the fourth cycle of the National Study of Postsecondary Faculty. Previous studies, conducted in 1988, 1993, and 1999, provided national profiles of faculty and instructional staff in postsecondary institutions, national benchmarks for faculty productivity and workload, and information on institutional policies and practices that affect faculty. The fourth cycle of the National Study of Postsecondary Faculty, NSOPF:04, expanded the information about faculty and instructional staff in two ways: (1) it allowed for comparisons to be made over an extended period of time, and (2) it helped examine emerging issues concerning faculty, such as changes related to increased use of the Internet and distance education. NSOPF:04 was designed to address a variety of policy-relevant issues concerning faculty, instructional staff, and postsecondary institutions. The study included faculty and institution questionnaires covering general policies concerning faculty. Information obtained from these two sources helped address important questions about postsecondary education, such as the following: \u2022 What are the background characteristics of full-and part-time faculty? \u2022 What are their workloads and how is their time allocated between classroom instruction and other activities? \u2022 What are the current teaching practices and uses of technology among postsecondary faculty and instructional staff? \u2022 How satisfied are they with current working conditions and institutional policies? \u2022 How are faculty and instructional staff compensated by their institutions? How important are other sources of income? \u2022 What are the career and retirement plans of faculty and instructional staff? \u2022 What retirement packages are available to faculty and instructional staff? \u2022 Have institutions changed their policies on granting tenure to faculty members? Are changes anticipated in the future? 1.2 Methodological Issues and Changes for NSOPF:04"}, {"section_title": "Combining NSOPF and NPSAS", "text": "NSOPF:04 was, in one respect, unlike any previous cycle of NSOPF, as it was conducted in tandem with another major study, NPSAS:04, under one overarching contract: NSoFaS:04. NCES recognized that, historically, there has been considerable overlap in the institutions selected for participation in NSOPF:04 and NPSAS:04. By combining the two independent studies under one contract, NCES sought to minimize the response burden on institutions and to realize data collection efficiencies. The NSOPF:04 and NPSAS:04 studies retain their separate identities. The purpose of this report is to summarize the methodology of NSOPF:04; sampling and data collection procedures for NPSAS:04 are referred to only as they are combined with, or impact, the parallel procedures for NSOPF:04. The combination of NSOPF:04 and NPSAS:04 into NSoFaS:04 had important implications for the NSOPF:04 institution sample design and institution contacting procedures. Institutions for the NSOPF:04 sample were selected as a subsample of the NPSAS:04 sample institutions. 4 This combination resulted in a somewhat larger sample of institutions for the fullscale study than previous NSOPF cycles (1,070 eligible institutions compared to 960 in 1999) and created a need to balance the design requirements of both studies in all institution-related study procedures."}, {"section_title": "Institution Sampling and List Collection", "text": "Apart from the changes necessitated by combining NSOPF:04 and NPSAS:04, as noted above, the key change in sampling procedures for NSOPF:04 was its use of a customized cost/variance optimization technique. This procedure was designed to identify the allocation that would accommodate all analytical objectives of this survey while minimizing data collection costs. As with the institution-level sampling, a customized cost/variance optimization technique was used to determine the optimal allocation of faculty to the sampling strata. In previous cycles, delays in receiving faculty lists created critical delays in sampling and contacting respondents during the time optimal to reach them (i.e., prior to the close of the regular academic year). Because the perceived burden of NSoFaS:04 would likely be greater than that of the individual studies by themselves, an advance notification and early contacting strategy was developed for this cycle. The purpose of advance notification and early contacting was to provide sufficient time to resolve any roadblocks to participation, allow the Institution Coordinator sufficient time to plan staffing and resources for the study, and to allow sufficient time for the completion of any review process the institution required, thereby facilitating the finish of data collection prior to the deadline. For faculty list collection, procedures were developed that would encourage institutions to provide lists of faculty and complete related documentation (including the institution questionnaire) online. On the NSoFaS:04 website, a secure tool for uploading lists was provided to eliminate the need for institutions to send data files through conventional mail. The institution questionnaire was designed as a single integrated web/computer-assisted telephone interview (CATI) instrument; there was no hardcopy instrument, although a facsimile was provided to allow dissemination of questions to different departments. Table 1 summarizes the data collection schedule for the full-scale study. variance. Further details about faculty sampling may be found in Section 2.1; sample allocation to strata is fully detailed in appendix A.\" Sample size was significantly larger than in the previous cycle: 35,630 faculty were sampled for NSOPF:04; of which, 34,330 were eligible. The final eligible sample for NSOPF:99 was 19,210. Criteria for faculty eligibility are discussed in section 2.1.2. Prior to sampling, faculty counts from all lists provided by participating institutions were checked against both the Integrated Postsecondary Education Data System (IPEDS) and the counts provided by the institution on their institution questionnaire. (In 1999, the IPEDS comparison was used as a quality control check only when institution questionnaire counts were absent). As in NSOPF:99, institutions were contacted to resolve any discrepancies between data sources. As in past cycles, faculty data collection utilized a mixed-mode approach; however, for NSOPF:04, sample members could participate only by a web-based self-administrated questionnaire or by an interviewer-administered telephone interview-there was no hardcopy version of the questionnaire. The participation of sample faculty members was initially requested in a letter that provided both instructions for completing the web questionnaire and calling to complete the interview via CATI. After 4 weeks, interviewers contacted the sample faculty members who had not completed the questionnaire to attempt a telephone interview. An earlyresponse incentive was provided to encourage prompt completion of the instrument. Refusal or nonresponse incentives were also offered to selected sample members. Incentives are discussed in section 3.2.5."}, {"section_title": "NSOPF:04 Products", "text": "Data from the full-scale study will be used by researchers and policymakers to examine a wide range of topics, including who faculty are, what they do, and whether and how they are changing over time. NSOPF:04 provides data on each of these topics. The NCES Data Analysis System (DAS) for public release has been constructed from the data and is available to the public at http://nces.ed.gov/das. Electronically documented, restricted access data files with associated Electronic Codebooks (ECBs) are also available to qualified researchers. The following types of reports are products of NSOPF:04: (1) this methodology report, providing details of sample design and selection procedures, data collection procedures, weighting methodologies, estimation procedures and design effects, and the results of nonresponse analyses; and (2) a series of descriptive statistical reports on key topics of interest. These topics include undergraduate teaching, faculty work activities and compensation, gender and racial/ethnic composition, and characteristics of part-time faculty. NSOPF:04 publications can be accessed electronically through the NCES website at http://nces.ed.gov/pubsearch/getpubcats.asp?sid=011."}, {"section_title": "Special tabulations are available on a limited basis from the National Education Data", "text": "Resource Center (NEDRC) upon request. Use of NEDRC services is most appropriate for well defined questions that are likely to yield a few tables. It is recommended that those requiring more extensive research and in-depth analysis apply for direct access to the restricted access data files. Questions regarding NEDRC services may be directed by e-mail to nedrc@pcci.com or to Aurora D'Amico at aurora.d'amico@ed.gov or (202) 502-7334. The remainder of this report contains the details of various activities. Chapter 2 details the survey design and implementation. Data collection outcomes are reported in chapter 3. Chapter 4 presents evaluations of the quality of data collected from institutions and faculty. Chapter 5 details procedures for data file development and imputation. Chapter 6 reports on procedures for weighting and variance estimation."}, {"section_title": "Chapter 2 Design and Implementation of NSOPF:04", "text": "This chapter provides a detailed summary of the design and implementation of the 2004 National Study of Postsecondary Faculty (NSOPF:04) full-scale study. Sampling of institutions and of faculty and instructional staff is discussed in detail. In addition, instrument design and data collection procedures are described. A Technical Review Panel (TRP) meeting was held on September 8-9, 2003. The panel, comprised of nationally recognized experts in higher education, reviewed the impact of methodological changes in sampling and data collection, including combining NSOPF:04 with NPSAS:04, the elimination of paper instruments, shortening the data collection period, and revisions to the instruments. The list of panel members is provided in appendix B."}, {"section_title": "Sampling Design", "text": "NSOPF:04 employed a two-stage sampling methodology for selection of eligible faculty and instructional staff based on a cost/variance optimization process, details of which are provided in appendix A. In the first step, samples of eligible institutions were selected within the following 10 institutional strata: \u2022 public doctoral; \u2022 public master's; \u2022 public baccalaureate; \u2022 public associate; \u2022 public other/unknown; \u2022 private not-for-profit doctoral; \u2022 private not-for-profit master's; \u2022 private not-for-profit baccalaureate; \u2022 private not-for-profit associate; and \u2022 private not-for-profit other/unknown. In the second step, samples of faculty members were selected within sampled institutions using a stratified systematic sampling where the six strata were defined in the following hierarchical order: \u2022 Hispanic; \u2022 non-Hispanic Black; \u2022 Asian and Pacific Islander; \u2022 full-time female; \u2022 full-time male; and \u2022 all other. The institution frame was comprised of all 3,380 eligible postsecondary institutions, while the faculty frame included all faculty and instructional staff in the corresponding institutions, which was estimated to include approximately 1.1 million individuals (Zimbler 2001). 5 The composition and eligibility definitions for these frames are outlined below."}, {"section_title": "Institution Frame", "text": "The institution frame for the NSOPF:04, like previous NSOPF cycles, consisted of all institutions meeting the following criteria: \u2022 located in the 50 states or the District of Columbia; \u2022 classified as participating in Title IV 6 student aid programs; \u2022 public or private not-for-profit; \u2022 2-or 4-year degree-granting; \u2022 offers educational programs designed for students beyond high school; \u2022 academically, occupationally, or vocationally oriented; and \u2022 makes programs available to the public. The resulting frame was a subset of that used for the National Postsecondary Student Aid Study (NPSAS:04), in that NSOPF:04 did not include private for-profit less-than-2-year nondegree-granting or Puerto Rican institutions that were included in NPSAS:04. The institution frame for NSOPF:04 was constructed from the Winter 2001-02 Integrated Postsecondary Education Data System Data Collection (Winter:02 IPEDS) file. To allow precise survey estimates for sectors of interest to the education community, this set of institutions was stratified based on institution control and level of degree offered. Institution control distinguished between public and private not-for-profit institutions, while level of degree offered was based on the 2000 Carnegie classification system 7 for segmentation of institutions. Table 2 summarizes the number of the eligible institutions for each of the resulting 10 primary institutional strata, based on the Winter:02 IPEDS file.  [53][54][55][56][57][58][59], and unknown 110 620 \u2020 Not applicable. NOTE: For sampling purposes, public baccalaureate, private associate, and other/unknown institutions are collapsed into a single stratum. Definitions of Carnegie codes are available at http://www.carnegiefoundation.org/classification. The institution universe counts include institutions that were added after the sample was selected to account for institutions that became eligible for NSOPF:04 after construction of the institution sampling frame from the Winter:02 IPEDS. Also, the 44 institutions that had an unknown Carnegie code at the time of sample selection have been reassigned to their appropriate strata. Therefore, there are no longer any institutions with unknown Carnegie codes in the sample, but some still remain in the universe. Numbers have been rounded to the nearest 10. Detail may not sum to totals because of rounding. SOURCE: U.S. Department of Education, National Center for Education Statistics, Integrated Postsecondary Education Data System (IPEDS), Fall 2000."}, {"section_title": "Faculty Frame", "text": "The second-stage sampling frame for NSOPF:04 includes faculty and instructional staff in the eligible postsecondary institutions. This includes both instructional faculty and faculty with no instructional responsibilities (e.g., research or administrative faculty) as well as staff with instructional responsibilities regardless of faculty status. In summary, eligible individuals for the NSOPF:04 study included any faculty and instructional staff who \u2022 were permanent, temporary, adjunct, visiting, acting, or postdoctoral appointees; \u2022 were employed full-or part-time by the institution; \u2022 taught credit or noncredit classes; \u2022 were tenured, nontenured but on tenure track, or nontenured and not on tenure track; \u2022 provided individual instruction, served on thesis or dissertation committees, advised, or otherwise interacted with first-professional, graduate, or undergraduate students; \u2022 were in professional schools (e.g., medical, law, dentistry); or \u2022 were on paid sabbatical leave. Ineligible individuals for NSOPF:04 included staff who: \u2022 were graduate or undergraduate teaching or research assistants; \u2022 had instructional duties outside of the United States, unless on sabbatical leave; \u2022 were on leave without pay; \u2022 were not paid by the institution, e.g., those in the military or part of a religious order; \u2022 were supplied by independent contractors; or \u2022 who otherwise volunteer their services."}, {"section_title": "Institution Sample Selection", "text": "The administration of NSOPF:04 consisted of a sample of 35,630 faculty and instructional staff across a sample of 1,080 institutions in the 50 states and the District of Columbia. This section provides details regarding the composition and construction of the institution sampling frame and methods used for selection of the institution sample."}, {"section_title": "Institution frame construction", "text": "The institution sample was selected using Chromy's sequential probability minimum replacement (PMR) sampling algorithm (Chromy 1979) to select institutions with probabilities proportional to a composite measure of size, details of which are provided in appendix A. For this purpose, each institution was assigned a measure of size (MOS) based on the number of eligible faculty and instructional staff and students in the given institution. Specifically, the composite size measure was the sum of cross products of sampling rates and population sizes for the groups, operating as the expected combined sample size at an institution. This measure was designed to ensure that student and faculty in certain minority strata would have a higher chance of selection. For faculty, these minority strata included: \u2022 Hispanic; \u2022 non-Hispanic Black or African American; \u2022 Asian and Pacific Islander; \u2022 female, full-time employee; \u2022 male, full-time employee; and \u2022 all others. It should be noted that the MOS for each institution was calculated to reflect the number of students in the given institutions, since for this administration the institution samples for NPSAS:04 and NSOPF:04 were selected jointly. That is, precision requirements for NSoFaS:04 were considered jointly by reflecting both the faculty and student design objectives. Faculty counts needed for MOS calculations were initially obtained from the Fall Staff Survey component of the Winter:02 IPEDS data collection. However, this source could not provide all information necessary to classify faculty members into one of the above sampling strata. For instance, in a number of institutions faculty counts were not reported, while for others reported counts were not indexed by race and ethnicity. As a result, the missing information was imputed in two steps. In the first step, unreported (missing) faculty counts were imputed, while in the second step, faculty reported as unknown race/ethnicity or nonresident aliens were distributed among the known race categories using a special procedure, details of which are provided in appendix A."}, {"section_title": "Institution sample selection", "text": "The institution sampling frame was constructed from the IPEDS-IC files and was partitioned into institutional strata based on institutional control, highest level of offering, and Carnegie classification. 8 As mentioned earlier, the sample of institutions was selected probability proportional to size (PPS) based on the number of faculty and students at each institution, using Chromy's sampling algorithm. Sample sizes and their corresponding sampling rates were established using a customized cost/variance optimization procedure, which aimed to identify the allocation that would accommodate all analytical objectives of this survey while minimizing data collection costs. Table 3 summarizes the distribution of the resulting sample of institutions for NSOPF:04. Subsequent to selection of the sample, the resulting institutions were contacted and asked to provide lists of eligible faculty and instructional staff for their institutions. 10 620 60 NOTE: The universe and sample counts include institutions that were added after the sample was selected to account for institutions that became eligible for NSOPF:04 since construction of the institution sampling frame from the Winter:02 IPEDS. Also, the 44 sample institutions that had an unknown Carnegie code at the time of sample selection were reassigned to their appropriate strata. Therefore, there are no longer any institutions with unknown Carnegie codes in the sample, but some still remain in the universe. Universe and sample counts are rounded to the nearest 10. Detail may not sum to totals because of rounding. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2004 National Study of Postsecondary Faculty (NSOPF:04)."}, {"section_title": "Faculty Sample Selection", "text": "This section provides an overview of the faculty sample selection procedures, which include methods used for frame construction and the technical details of cost/variance optimization process for selection of the initial sample sizes and calculation of needed sampling rates."}, {"section_title": "Faculty frame construction", "text": "The sampling frames for selection of faculty and instructional staff were constructed institution-by-institution. Each sampled institution was asked to provide a complete listing of eligible full-and part-time faculty and instructional staff. The majority of lists were delivered electronically; however, some of these lists were abstracted from online sources such as institution directories or supplied on paper."}, {"section_title": "Faculty sample selection", "text": "The sample of faculty was selected using an equal probability stratified systematic sampling, within cells indexed by institutional and faculty strata. As detailed in the next section, a customized cost/variance optimization program was utilized.\nFaculty members were sampled as faculty lists were received from participating institutions. Prior to selecting the faculty sample for a given institution, expected sample sizes for each faculty stratum were calculated using the institution-specific faculty list counts and sampling rates. These sampling rates were then modified, as necessary, for the reasons given below. \u2022 Rates were increased across all faculty strata to ensure that at least ten faculty members were selected from each institution, if possible. \u2022 Rates were increased within faculty strata to guarantee that at least one faculty member was selected per stratum within each institution, if possible. \u2022 The sample yield was monitored throughout the months during which faculty lists were received, and the faculty sampling rates were adjusted periodically for institutions for which sample selection had not yet been performed to ensure that the desired faculty sample sizes were achieved. Stratified systematic sampling was used to select faculty members from the faculty lists. Specifically, from each list (institution) sample faculty were selected within each faculty stratum defined by race/ethnicity, gender, and employment status using the corresponding rate for the given institution-faculty stratum, with academic field serving as an implicit sort variable. Whenever a list contained insufficient data to identify faculty strata, a systematic sample of faculty was selected using the overall sampling rate for the institution. For hard copy lists, the resulting sample was then keyed to create an electronic file. The following table 4 provides a summary of the required sample sizes, which were determined based on the cost/variance optimization process and the resulting completed interviews by faculty stratum. The information supplied for each sampled faculty member (e.g., name, academic field, residence) was checked against that of faculty previously selected from other institutions to identify and eliminate respondents sampled twice. Duplicates were eliminated from the sample of the current institution. Once the de-duplication process was complete and the institution's final sample file was created, the institution's final sample file was added to the master dataset. The master dataset contained all sampled faculty members and their relevant sampling information."}, {"section_title": "Determining initial faculty sample sizes and sample allocation", "text": "A special cost/variance optimization program was used to determine the desired allocation of respondents to institution-by-person strata, the goal of which was to secure at least the same level of precision for key estimates as those achieved during the previous administration of the survey. This optimization process, which is detailed in appendix A, consisted of the following steps: \u2022 establishing precision requirements for key estimates; \u2022 constructing a cost model specific to the structure of the NSOPF:04 sample; \u2022 developing a relative variance model; and \u2022 determining the optimum sample allocation."}, {"section_title": "Development of Instrumentation", "text": "Project staff from RTI and MPR Associates were responsible, respectively, for developing and implementing study instrumentation for NSOPF:04 and for ensuring that the instruments, where possible, retained analytic comparability with earlier data collection rounds of the study. Revisions to the institution and faculty/instructional staff instruments built upon the NSOPF:99 instruments, and included the comments and suggestions of the Technical Review Panel (TRP), sample respondents contacted after the study for additional information, and other government officials and postsecondary researchers. (Copies of the NSOPF:99 data collection instruments for postsecondary institutions and faculty/instructional staff are included as appendixes A and B, respectively, in Abraham et al. 2002.) In May 2002, meetings with the TRP were conducted to review the relevance of policy issues examined in NSOPF:99, the importance of emerging issues (such as increased use of the Internet and distance education) not included in the 1999 instruments, and the consequences of adding, revising, or deleting items from the NSOPF:99 instruments. 10 Following contract award for NSOPF:04, project staff developed and tested multiple versions of the institution and faculty/instructional staff instruments. A field test version of the instrumentation was developed at the start of the 2002-03 academic year and closely reviewed by members of the study TRP, government officials, postsecondary researchers, and other interested individuals. Then during the fall and spring terms of 2002-03, field test data collection for NSOPF:04 permitted the evaluation of the revised institution and faculty/staff instrumentation under conditions comparable to those to be employed during the NSOPF:04 fullscale study. 11 Several policy, methodological, and practical concerns guided the development of instrumentation for NSOPF:04. To ensure the comparability of data elements from earlier rounds of the postsecondary faculty study in 1988, 1993, and 1999, one of the primary objectives of instrumentation was to maintain the trend analyses for this national, cross-sectional study. However, this goal was balanced by the importance of adequately considering emerging issues, while at the same time developing instruments that could be completed quickly and efficiently by sample members. For example, almost 70 percent of the institution responses for the 1999 study were obtained via paper-and-pencil questionnaire, and the average time to complete the institution questionnaire was 90 minutes. For the NSOPF:99 faculty questionnaire, over one-half (54 percent) of the respondents completed hardcopy instruments, with an average web and paper questionnaire completion time of 51 minutes; the average CATI completion time was 55 minutes. Based on these considerations, the goals for the NSOPF:04 instrumentation included several elements: \u2022 All data collection would be completed electronically, using web-based selfadministered questionnaires, with telephone interviews for those who did not respond to the web self-administered questionnaires. No paper and pencil instruments would be received. \u2022 All data collection instruments for the study would be shorter than the NSOPF:99 instruments, thus simultaneously increasing response rates while reducing the potential for bias and the need for costly refusal conversion efforts. The targets for average time to complete the instruments were set at 45 minutes for the institution questionnaire and 30 minutes for the faculty/instructional staff questionnaire. \u2022 Consistent with the transition to all-electronic data collection, the NSOPF:04 instrumentation was designed to be easier for sample members to complete, to be easier for the study team to process, and to provide higher quality data. \u2022 Finally, the instrumentation team sought to address emerging issues as well as to maintain comparability with earlier rounds of the study. With these goals established, planning and design for the NSOPF:04 institution and faculty/instructional staff questionnaires began. Specification for both instruments was in RTI's Instrument Development and Documentation System (IDADS), a tool developed specifically for the design of complex electronic data collection instruments (see also section 2.5.1). Using IDADS, instrument designers entered information about each instrument item, including the variable data definition, formatting, and the desired on-screen presentation. 12 For each of the NSOPF:04 instruments, designers specified the variable names and labels, values and value labels, \"applies to\" fields, and variable definitions (e.g., numeric, continuous, maximum and minimum values, field size, etc.)."}, {"section_title": "Instrument Programming", "text": "Despite the different data collection modes for NSOPF:04, the self-administered web instruments for the institution and faculty/instructional staff respondents were identical to their corresponding CATI instruments. Both instruments were web-based products, located on U.S. Department of Education servers. The instruments were developed using Microsoft Corporation's Active Server Pages (ASP) web programming language. 13 This approach resulted in a computer-assisted data collection program that facilitated the preloading of full-screen data entry and editing of \"matrix-type\" responses. The web and CATI system presented interviewers with screens of questions to be completed, with the software guiding the respondent through the interview. Inapplicable questions were skipped automatically based on prior response patterns. On-screen clarification was available for all items. 14 The instrument also provided real-time error checking for inconsistent or out-of-range responses and minimized the potential for inadvertently skipped items."}, {"section_title": "Institution Questionnaire", "text": "Instrumentation activities for the NSOPF:04 institution questionnaire began in May 2002 with revisions to the NSOPF:99 instrument. Project staff began working with a revised version of the NSOPF:99 instrument that incorporated the lessons learned from the NSOPF:99 data collection, including the comments and suggestions for instrumentation provided by both the NSOPF TRP and a small number of study respondents who were contacted for additional information after the completion of NSOPF:99 data collection. This information formed the input for the NSOPF:04 field test institution questionnaire that was administered to a purposive sample of 150 postsecondary institutions during the 2002-03 academic year. The interpretation of responses from the field sample members that completed the instrument (77 percent of the sample of institutions that were eligible to participate), results 12 In addition to instrument development, IDADS also provides a reference system for instrument reviewers and testers and serves as the data documentation system for the data products developed. 13 Active Server Pages (ASP) dynamically produce hypertext markup language (html) pages designed to facilitate information retrieval across the Internet. ASP code includes small embedded programs or scripts that are processed on a web server when accessed by users employing browser programs such as Netscape or Internet Explorer. Before responses are returned to a user, the request typically accesses databases and develops a customized response. 14 Each data collection screen or form for the NSOPF:04 field test faculty instrumentation included a link to a page of \"help text\" prepared specifically for the item and including key definitions, descriptions of respondents to whom the item applied, and other useful information. In an attempt to shorten the administration time for the full-scale instrument, the help text was shortened and appeared on the same form as the question wording and response options. This reduced the need for loading a separate web page for help. A separate help text web page was available for the institution questionnaire for both the field test and full-scale versions of the instrument. of debriefing sessions with institution contact personnel for the field test who were responsible for encouraging response from the institutions, and data collection timing information for the field test also served to inform revisions to the full-scale study institution questionnaire. After careful consideration of this input and examination of the data collected during the 1998-99 academic year-including the patterns of responses and missing data, as well as time to complete estimates-instrument revisions were implemented. Like the NSOPF:99 institution questionnaire, the NSOPF:04 instrument was divided into major sections that collected information on the number of faculty and instructional staff employed at the target institution; the policies and practices that affected, respectively, full-time and part-time faculty and instructional staff; and the percentage of undergraduate instruction assigned to various instructional personnel. Descriptions of the information included in these sections follow (see also the instrument facsimile in appendix C): \u2022 The first section (items 1A and 1B) collected information on the number of faculty and instructional staff employed either full time or part time at the target postsecondary institution during the fall term of the target academic year . For NSOPF:04, institution personnel were requested to provide these counts \"as of November 1, 2003 (or during the fall term of the 2003-04 academic year when your faculty lists are considered complete).\" \u2022 Institution instrument items 2 through 13 defined the second section of the questionnaire and collected information on the employment of the target institution's full-time faculty and instructional staff. After first collecting information on the numbers of these personnel who entered or exited full-time employment during the previous academic year (2002-03 school year), this section examined the characteristics and policies of the target institution's tenure system, employee benefits, union representation (if any), and personnel evaluation, as applied to fulltime faculty and instructional staff. \u2022 The third section of the institution questionnaire (items 14 through 18) examined the employment of the target institution's part-time faculty and instructional staff. This section used items similar to those for full-time faculty and instructional staff in the previous section. These items included the availability of retirement plans to part-time faculty, the availability of and institution-level support for various types of employee benefits, and the characteristics of the institution's personnel evaluation system. \u2022 The fourth instrument section included a single question (19) that collected information on the percentage of the target institution's undergraduate instructional activities assigned to various instructional groups, including full-time faculty and instructional staff, part-time faculty and instructional staff, teaching assistants such as graduate students, and others individuals. \u2022 The last section of the NSOPF:04 institution questionnaire (item 20) collected respondent contact information and feedback on data collection. This section attributed the item responses for the entire institution questionnaire to individual respondents at the institution, which allowed data collection staff to recontact respondents for clarification of responses. These data elements-respondent name, job title, telephone number, and e-mail address-were not maintained after data collection was completed. Appendix D provides a crosswalk of NSOPF:04 institution questionnaire items to the institution questionnaires from NSOPF:88, NSOPF:93, and NSOPF:99. Table 5 notes how the  NSOPF:04 questionnaire differs from the NSOPF:99 questionnaire. As noted in this table, nine items from the NSOPF:99 questionnaire were eliminated from the NSOPF:04 institution questionnaire, 14 items were revised, and three items for NSOPF:99 were repeated without change. Broken into three items; response options revised (Option E, discontinued tenure, asked only of respondents who answered \"no\" to tenure availability) Other actions to reduce tenured faculty Deleted Number of full-time positions sought to hire Unchanged 9 Retirement plans available to full-time staff Deleted Employee benefits available to full-time faculty and instructional staff"}, {"section_title": "Revised", "text": ""}, {"section_title": "10A", "text": ""}, {"section_title": "10B", "text": "Broken into two items, part 10A serves as gate question Response categories for benefits were changed to All, Some, None, Don't know Fully and partially subsidized categories combined Additional employee benefits available to fulltime faculty and instructional staff"}, {"section_title": "Revised 11", "text": "Response categories for benefits changed to All, Some, None, and Don't know; Slight wording change Percentage of salary contributed by institution to benefits Deleted Collective bargaining for full-time faculty and instructional staff"}, {"section_title": "Revised 12", "text": "Percentage of faculty represented by union eliminated Teacher assessment with full-time faculty and instructional staff"}, {"section_title": "Revised 13", "text": "Response options changed to Yes, No, Don't Know; \"Other, specify\" option was eliminated Availability of retirement plans for part-time faculty and instructional staff  "}, {"section_title": "Faculty Questionnaire", "text": "The NSOPF:04 questionnaire for faculty and instructional staff was divided into several sections that described the study and respondents' rights (informed consent); nature of employment; academic and professional background; instructional responsibilities and workload; scholarly activities; job satisfaction; compensation; background characteristics; and opinions. Included within the final section, where applicable, were items that collected address information for sample members who were eligible for response incentives. (See section 3.2.5 for additional information about the early-response and refusal conversion incentives.) Table 6 describes the instrument sections, including the number of forms (or screens) and data elements in each. Like the instrumentation for the study waves in 1988, 1993, and 1999, the NSOPF:04 faculty and instructional staff questionnaire emphasized descriptive and behavioral attributes rather than attitudinal measures. The design of the faculty and instructional staff questionnaire included input from members of the NSOPF:99 TRP and representatives of offices of the U.S. Department of Education, as well as an analysis of the data collected during the 1999 study. Because the NSOPF:99 instrument took 55 minutes to complete, designers made a concerted effort to shorten the instrument and make it more efficient. 15 Several questions were eliminated, and other questions were shortened or otherwise simplified. The instrument was then evaluated in a field test carried out during the 2002-03 academic year under conditions similar to those employed during the full-scale study in 2003-04. Where applicable, these forms also collected address information from sample members qualified for nonresponse incentives. 1 The faculty/instructional staff questionnaire was divided into forms (screens) and items. Each form was structured to include related items. The first number is the number of forms in the section, and the second number is the number of items included on those forms. Forms and items were often skipped based on the responses to earlier forms and items. 2 The number of items in the faculty questionnaire (183) differs from the number of faculty items reported elsewhere in this document (e.g., 162 analysis variables and 144 stochastically imputed variables) because some items were for internal use only (e.g., verbatim text strings used to code field of teaching [see section 4.3.3], school name and city to code IPEDS [see section 4.3.4], and contact information for sending incentives). NOTE: IPEDS = Integrated Postsecondary Education Data System. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2004 National Study of Postsecondary Faculty (NSOPF:04). Following the field test, additional items were modified and eliminated to reach the desired 30-minute interview. The average CATI and web interview for the NSOPF:04 field test took 42 minutes, considerably longer than anticipated. The results of the NSOPF:04 field test reliability reinterview (Heuer et al. 2004), the policy relevance of each instrument item, and the input received from responding sample members, telephone interviewers/help desk staff, and members of the NSOPF:04 TRP were used to identify 27 forms from the full-scale study for elimination. Table 7 describes the NSOPF:04 field test items on these forms that were eliminated following the field test. The table also provides the average time to complete each item during the field test. 16 After adjusting this average time to complete by the proportion of the overall respondent population that reached the item (i.e., complicated and time-consuming items will have little impact on the average time to complete the entire interview if most respondents do not attempt the item), these item reductions were expected to reduce the average time to complete the full-scale instrument by approximately 7 minutes. It should be noted that approximately 7 minutes of the overall time to complete the field test interview were associated with \"transit\" time-in other words, the time involved to transmit information to each respondent, to \"write\" the form and related text onto each sample member's screen, to transmit the responses back for storage, and to begin the transmission of the next item. Interview transit times are dependent on many factors such as server bandwidth, processing efficiency, and instrument content (e.g., other things being equal, the transit time for a form with little text and graphic information will be less than another form with more text and graphics). Notably, transit times are often dependent on factors beyond the control of instrument designers. For example, the type of internet connection used by the sample member (telephone dial-up modem versus direct Ethernet connection with fiber optic lines) and the number of other users on the respondent's internet service provider at the time will affect transit time. Section 3.3.1 describes the interview completion time and transit times for the NSOPF:04 full-scale faculty and instructional staff questionnaire. To reduce transmission time from that experienced in the NSOPF:04 field test, project staff carefully reviewed the code efficiency of the web applications. The project also utilized an outside and independent review of the study procedures and programs. TechSages LLP, a computer consulting firm located in Durham, NC, reviewed the NSOPF:04 field test computer code. The group offered several recommendations for optimizing the code to improve execution speed including changes to database connectivity implementation, code structure, and variable scoping. In addition to the changes in data processing and the reduction in the number of items included in the questionnaire noted above, instrument designers also implemented several other content related changes for the full-scale study. These included the following: \u2022 Instrument designers eliminated the faculty/instructional staff questionnaire's online help and replaced it with more targeted information placed directly on the form containing the question. For the field test, a callable help screen was available for each form of the faculty interview. By selecting a help button at the bottom of each form, the respondent could review a screen of related definitions, examples, and other information about the item. While these help screens provided useful information, accessing them did require the transmission of an additional form and, consequently, an increase in the interview completion time. While adding text, such as definitions or examples, does increase the transit time of a screen, the increase is negligible relative to the increases in interview time that would be obtained by accessing and transmitting a second web page of help text for the item. Adding definitions and examples to the original form of the interview reduced the need for help screens. \u2022 For the full-scale study's questionnaire, project staff also developed an online assisted-coding routine for respondent's academic area or discipline. Assisted coding provided significant time savings over the online coding in the field test, which used two pull-down boxes for each academic discipline. The assisted-coding procedure developed for the full-scale study eliminated pull-down boxes for common disciplines (e.g., mathematics or English), considerably reducing the time each respondent took to code academic field. The pull-down boxes were available for unusual disciplines or when the sample member was not satisfied with the result of the assisted-coding activity. To use the assisted-coding routing, the sample member entered the name of the relevant academic field, and then confirmed or discarded the results of the matches with an assisted-coding dictionary developed from the Classification of Instructional Programs (U.S. Department of Education 2002). \u2022 Instrument developers also improved item wording, and especially screen fills to reduce item wording. \u2022 Finally, the full-scale study instrument combined a number of instrument screens, thus reducing the number of overall forms and the number of data transmissions. (For example, forms Q65 and Q80 in the full-scale study instrument combined previously independent forms.) Table 8 compares and contrasts the faculty and instructional staff instruments used for the NSOPF:04 and NSOPF:99 full-scale studies. As noted in this table, 39 items were eliminated from the 1999 instrument, 51 items were simplified or otherwise revised, 1 item was added, and 3 items were unchanged.     22,25,29,30,33,34,40,42,43,44,45,57,58,59,60,63,76, and 78 in the NSOPF:04 faculty questionnaire were eliminated before data collection, and the instrument was not renumbered. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2004 National Study of Postsecondary Faculty (NSOPF:04)."}, {"section_title": "Institution Data Collection", "text": "The goals of the institution data collection for the NSOPF:04 study were to collect a list of full-and part-time faculty and instructional staff (referred to as a \"faculty list\") from each sampled institution and to obtain a completed questionnaire from each sampled institution. As described in section 2.1.4, the faculty list was used for selecting the faculty sample and also provided the contact information used for faculty data collection activities. The institution questionnaire, detailed in section 2.2.3, collected information on the policies and practices affecting full-and part-time faculty and instructional staff. To facilitate the process of obtaining faculty lists and completing the institution questionnaire, an institution website was developed, and for each sampled institution, the Chief Administrator (CA) was asked to appoint an Institution Coordinator (IC)."}, {"section_title": "Institution Website", "text": "The NSoFaS:04 website served a number of functions for both the NSOPF:04 and NPSAS:04 studies. For institutions, it was a central repository for all study documents and instructions. It allowed for the uploading of electronic lists of faculty and instructional staff. In addition, it housed the institution questionnaire for the Institution Coordinator to complete online. Figure 1 presents the home page of the NSoFaS:04 website. Visitors to the website were provided with the following links (see navigation bar on the left side of the screen): \u2022 Early Contacting provided information about the early institution contacting process for NSoFaS:04 for the initial stage. Section 2.3.2 provides details of early institution contacting. \u2022 About NSOPF (faculty) provided information on the study's mandate and research objectives, with a link to National Center for Education Statistics (NCES) reports from previous study cycles. \u2022 About NPSAS (student) provided comparable information (as noted above) for the student component of NSoFaS:04. \u2022 Instructions provided links that allowed institution staff to view and print copies of various NSOPF:04 and NPSAS:04 forms (in pdf format). \u2022 Endorsements listed the 25 national organizations that endorsed both studies. (The 24NSOPF:04 endorsements are listed in appendix E; one endorsement was applicable only to proprietary schools that were eligible for NPSAS:04 but ineligible for NSOPF:04). \u2022 Frequently Asked Questions (FAQs) included questions and answers concerning all stages of data collection for both components of NSoFaS:04. \u2022 Help provided the help desk toll-free number and e-mail address for contacting project staff, along with instructions for logging in. \u2022 Contact Us contained address information for RTI International. \u2022 Other NCES Sites links to three NCES websites that provided more information about NCES programs and how to order publications. All data entry applications were protected by Secure Sockets Layer (SSL) encryption. Further security was provided by an automatic \"time out\" feature, through which a user was automatically logged out of the NSOPF:04 institution questionnaire if the system was idle for 30 minutes or longer. The system did not use any persistent \"cookies,\" 17 thus adhering to the Department of Education's privacy policy. A status screen, shown in figure 2, indicated which stages of institution data collection were completed (denoted by a check mark) and allowed institutions to select those stages that were not yet completed. Once a stage was completed, it was no longer accessible via the Web. "}, {"section_title": "Institution Contacting", "text": "The eligible institution sample for the NSoFaS:04 consisted of 1,630 institutions, of which 1,070 were sampled for NSOPF:04 as well as NPSAS:04. These 1,070 institutions were recruited to participate in both components of NSoFaS:04 (NSOPF:04 and NPSAS:04). The fielding of NSOPF:04 and NPSAS:04 together as the National Study of Faculty and Students was one of three changes made in the institution contacting procedures for this cycle of NSOPF. The second change was to administer the institution questionnaire as a web or CATI instrument, with no hardcopy equivalent. The third change was to begin recruiting institutions and initiating coordinator contacts in March 2003-a full 8 months prior to the November reference date for the fall term, and roughly 5 to 6 months earlier than the September start dates of previous cycles. This change was prompted by the need to draw a faculty sample and subsequently contact sampled faculty for participation prior to the 2004 summer break. It was hoped that the additional lead time would allow schools to better plan for the staffing and resources required for participation within the study's schedule constraints, allow institutions additional time to initiate and complete any internal review procedures they felt necessary, and also allow the contractor time to work with institutions to resolve any potential roadblocks to their participation. This advance notification was intended both to speed up receipt of faculty lists, and to positively impact the institution response rate. By sampling and contacting faculty earlier in the academic year, it was hoped that a higher faculty response rate could be achieved. Prior to the field test, endorsements from organizations that had previously endorsed NSOPF and/or NPSAS were renewed and extended, as appropriate, to both NSoFaS:04 component studies. An effort was also made to solicit new endorsements from other organizations as well. In all, 25 organizations endorsed both components of NSoFaS:04; 24 of these were relevant to NSOPF:04. 18 These endorsements were featured on all project letterhead and pamphlets and on the NSoFaS website. In addition, several of these organizations continued to promote the study throughout the data collection period in newsletters and other communications with their member institutions. See appendix E for a list of the 24 organizations that endorsed NSOPF:04. For NSOPF:04, data collection proceeded in four stages: \u2022 institution recruitment; \u2022 advance notification of the coordinator; and \u2022 faculty list and institution questionnaire data collection procedures. Procedures for each stage of data collection are outlined below."}, {"section_title": "Verification", "text": "Verification began on January 23,2003, and was completed prior to the start of institution recruitment on March 10, 2003. Institution contactors were trained to contact the institution at their main number, verify address information and confirm the name and contact information for the CA at the institution. They also confirmed that the school was Title IV eligible and open to the general public during the fall 2003 term. Institutions flagged as potentially ineligible-including closed institutions and institutions that indicated they were not Title IV eligible or open to the general public-were forwarded to project staff for review. Project staff also reviewed instances of sampled institutions merging with other institutions (sampled or unsampled), possible changes in mission that could affect the institution's sampling strata, and changes in name or address, to confirm the institution was eligible and correctly identified."}, {"section_title": "Institution recruitment and advance notification of the coordinator", "text": "Institution recruitment began on March 10, 2003. The Chief Administrator (CA) at each institution sampled for NSoFaS:04 was sent the following materials (see appendix F for copies of these letters and pamphlets): \u2022 a cover letter, printed on NCES letterhead, providing background information on NSOPF:04 and NPSAS:04; \u2022 an NSoFaS:04 pamphlet summarizing the objectives of both NPSAS:04 and NSOPF:04, and providing background information and selected findings for each component; \u2022 an NSOPF:04 pamphlet, included to show what had been prepared for mailing to the sampled faculty; \u2022 a NPSAS:04 pamphlet, included to show what had been prepared for mailing to sampled students; and \u2022 a project timeline outlining the flow of activities for both component studies of NSoFaS:04 , and the projected schedule for each. A team of institution contactors followed up with the CA by telephone. The CA was asked to name an Institution Coordinator (IC) by completing the Designation of Coordinator form online, or providing the information over the telephone. Once the IC was identified, they were mailed an identical packet, with a cover letter informing them that they would be mailed complete instructions for their participation in each component in September. During this advance notification stage of data collection, ICs were asked to complete an online Coordinator Response Form (CRF) which could also be administered by CATI (see appendix F). This instrument confirmed that the institution could supply the items requested for the faculty and student lists within the stated schedule constraints. It also contained items designed to expedite collection of student record information for the student component. ICs who indicated that a formal review process (such as an Institutional Review Board [IRB] review) was necessary before their institution would agree to participate were forwarded additional project materials as appropriate. A complete IRB approval packet was prepared for this purpose and mailed to the IC upon request. This packet included copies of instruments, as well as complete descriptions of relevant survey procedures (e.g., confidentiality and informed consent)."}, {"section_title": "Faculty list collection procedures", "text": "Complete instructions for participation in both NSOPF:04 and NPSAS:04 were sent to all designated ICs on September 29, 2003. Binders continued to be mailed to ICs on a flow basis as they were designated. The mailing, which was packaged in a three-ring binder, included the following materials: \u2022 a cover letter describing the study, the institution's password, IPEDS unit ID, 19 and URL (web address) necessary to access the NSoFaS:04 website (a separate letter was created for NPSAS:04-only sampled institutions); \u2022 a copy of the letter that went to the CA, and a facsimile of the Designation of Coordinator form; \u2022 a complete list of endorsements; \u2022 a project timeline outlining the flow of activities for both component studies of NSoFaS:04 , and the projected schedule for each; \u2022 instructions for preparing the list of faculty and instructional staff, including a list of data elements requested, and a suggested file layout; \u2022 complete instructions for participation in each phase of NSoFaS:04; and \u2022 a list of transmittal options for sending faculty lists, by mail, e-mail, and direct upload to the NSoFaS:04 website, together with an express courier packet and label for mailing the lists if required. The instructions directed the ICs to provide a list of full-and part-time faculty and instructional staff, including all personnel who had faculty status or any instructional responsibilities during the fall 2003 term. Institutions were encouraged to submit an electronic list by uploading it to the secure website. The data items requested for each listed faculty or instructional staff member were \u2022 full name; \u2022 academic discipline; \u2022 department/program affiliation; \u2022 full-time/part-time status; \u2022 gender; \u2022 race/ethnicity; \u2022 employee ID number (to eliminate duplicates from sample); and \u2022 contact information (institution and home mailing address, institution and home e-mail address [if available], and home and campus telephone numbers). Follow-up with ICs was conducted by telephone, mail, and e-mail. Telephone prompts to the ICs were made for institutions that had not provided lists. To minimize the number of contacts made to an IC, prompting for NSOPF:04 was combined with prompting for NPSAS:04. E-mail prompts to ICs, keyed to pending project deadlines, were regularly utilized. E-mail prompts focused on timely completion of requested materials and encouraged review of the instructions for participation. As faculty lists were received, they were reviewed for completeness, readability, and accuracy. Additional follow-up to clarify the information provided or retrieve missing information was conducted by the institution contactors as necessary. Counts of full-and part-time faculty were collected in both the institution questionnaire and in the faculty lists. For each institution, the counts of full-and part-time faculty were checked against those provided in the institution questionnaire and against 2001 IPEDS Fall Staff Survey data. IPEDS data were used for discrepancy checks whenever institution questionnaire data were unavailable but also served as an additional check to catch inaccuracies in matching questionnaire/list data that otherwise would not have been discovered. For further details regarding quality control checks, see section 4.1.2. Reimbursement for the time and staff involved in providing the faculty list was offered to institutions indicating a difficulty in complying with the request within schedule constraints. A refusal conversion letter was mailed to institutions that had not responded by November 21, 2003. The letter underscored the offer of reimbursement. Beginning in May 2004 a flat $500 reimbursement was offered to institutions for providing the outstanding faculty and student lists by the end of June. This offer was extended both to explicit refusals and schools which indicated cooperation but had yet to comply. For institutions lacking the resources to provide a complete list of full-and part-time faculty despite the offer of reimbursement, list information was, if possible, abstracted from course catalogs, faculty directories, and other publicly available sources. Those institutions for which usable lists were identified were notified of this sampling procedure; institutions which indicated that they did not want their faculty included in the sample were excluded. Faculty lists abstracted in this fashion were reviewed for completeness against IPEDS before being approved for sampling. Faculty list collection continued through July 11."}, {"section_title": "Institution questionnaire data collection procedures", "text": "Institution Coordinators were asked to complete the institution questionnaire (described in section 2.2.3) using the study's institution website. Institution questionnaire follow-up was conducted simultaneously with follow-up for lists of faculty. If an institution was unable to complete the questionnaire online, efforts were made to collect the information over the telephone. This often involved contacting multiple offices within the institution, as questions about benefits and tenure policies could most frequently be completed by human resources and/or the academic affairs office, while questions about faculty counts and turnover were typically answered by institutional research staff. To expedite data collection, missing questionnaire data was, in some instances, abstracted directly from benefits and policy documentation supplied by the institution, or publicly available on the institution's website. In addition, several large multi-campus systems provided data for their campuses at a system level or indicated that specific policy and benefits information was the same for all related campuses. Refusal conversion efforts for the institution questionnaire were conducted with institutions regardless of whether they supplied a list of faculty. After August, institutions which had not completed the questionnaire were offered a reimbursement of $50 for providing the questionnaire within schedule constraints. Data collection for the institution questionnaire closed on October 22, 2004."}, {"section_title": "Administrative systems and procedures", "text": "To efficiently track all mail and telephone follow-up (both incoming and outgoing) and processing and sampling activities, the study utilized an Institution Contacting System (ICS) specifically designed to meet the needs of the NSoFaS:04 project. The ICS was accessible to contactors, Call Center 20 supervisors, and project staff. The NSoFaS:04 ICS was designed so that a change in status (e.g., a completed Designation of Coordinator form) automatically generated the next step (e.g., a mailout to the IC and an automatic appointment for telephone follow-up). Electronic call notes documented the outcome of every conversation. The system allowed interviewers to set appointments for future follow-up. Through the ICS, the interviewer had the ability to designate an IC, provide contact information, and access the institution questionnaire and other data collection instruments. The ICS gave interviewers the ability to generate an automatic e-mail to ICs containing the password and IPEDS unit ID required for access. The problem report form feature of the ICS allowed institution contactors to immediately forward specific call notes to an e-mail box monitored by project staff. This ensured that refusals, requests for remails, and calls requiring follow-up by project staff were handled promptly. Quality Circle meetings, attended by interviewers, supervisors, team leaders, and project staff, were held on a weekly basis to share ideas for gaining institutional cooperation and suggestions for improving procedures. Project staff solicited feedback from call center personnel on the ICS, scripts, and handling problems reported by respondents (e.g., difficulties accessing the website)."}, {"section_title": "Faculty Data Collection", "text": "The NSOPF:04 utilized a mixed-mode data collection methodology that allowed sample members to participate either by web-based self-administered questionnaire or via an interviewer-administered telephone interview. At the start of faculty data collection, introductory materials were sent to sample members via first class mail as well as electronic mail (if an e-mail address was available). The initial letter included instructions for completing the selfadministered questionnaire on the Internet or by calling a toll-free number to complete a telephone interview. After an initial 4-week period, telephone interviewers began calling sample members. The self-administered web instrument remained available to respondents throughout data collection. An early-response incentive, designed to encourage sample members to complete the self-administered questionnaire prior to outgoing CATI calls, was offered to sample members who completed the questionnaire within 4 weeks of the initial mailing. Incentives were also offered to selected sample members as necessary (i.e., those who refused and other nonrespondents)."}, {"section_title": "Faculty Website", "text": "The website for the NSOPF:04 served a dual purpose. The primary function was to provide access to the web questionnaire for the sampled faculty and instructional staff. The secondary function was to provide information about the study, the selected sample, the sponsor, the contractor, and confidentiality. In addition to the information available on the site, links were provided to other relevant sites (e.g., NCES). The home page of the NSOPF:04 faculty website is depicted in figure 3. The initial login page provided access to the self-administered questionnaire. The login process involved entering a specific study ID and password, which were provided to the respondent in every letter and e-mail message. Respondents could also obtain their study ID and password by sending an e-mail to the project, or by contacting a help desk agent at the NSOPF:04 toll-free number. As with the institution application, the web instrument was protected by SSL encryption, an automatic time out feature, and omission of any persistent cookies. "}, {"section_title": "Locating and Interviewing Procedures", "text": "The NSOPF:04 faculty data collection design involved locating sample members, providing an opportunity for the faculty or instructional staff to complete the self-administered questionnaire, and following up with web nonrespondents after 4 weeks to conduct a computerassisted telephone interview. The data collection period lasted approximately 9 months (January 15 through October 6, 2004). Data collection activities for faculty are shown in figure 4.  1 If a home address was available for the sample member, the lead letter package was mailed to the home. If there was no home address, the package was mailed to the school address. If there was no specific school address available, the package was mailed to the main address on file for the school. Sending packages to the home address resulted in a higher response rate compared to sending packages to the school address (78 percent versus 67 percent; \u03c72 = 565.6, p < .0001). 2 The web interview option was available throughout data collection, even after telephone follow-up began. 3 The sample member's office and home telephone numbers were called by computer-assisted telephone interview (CATI) interviewers. If no specific telephone number was available for the sample member, the school's main telephone number was used. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2004 National Study of Postsecondary Faculty (NSOPF:04)."}, {"section_title": "Mailouts", "text": "Faculty and instructional staff were sent a lead letter, instructions for accessing the web instrument via the Internet or with the assistance of a telephone interviewer, and a study pamphlet. (Examples of these materials are included in appendix F.) The lead letter introduced the study and listed the organizations that endorsed the study. If an e-mail address was available for a sample member, the introduction to the study was also sent via e-mail. Periodically throughout the data collection period, reminder letters and e-mail messages were sent to nonrespondents to encourage their participation and to notify them of the incentive, if applicable. Examples of these follow-up contacts are included in appendix F."}, {"section_title": "Locating", "text": "Identifying a valid mailing address and telephone number for all selected faculty and instructional staff sampled from known institutions was critical to the success of the NSOPF:04. Locating activities were conducted in two stages: advance tracing, which took place before data collection began, and intensive tracing conducted during data collection. Advance tracing. Upon receipt of faculty lists from participating institutions, contact information for the sampled faculty and instructional staff was reviewed and assessed for completeness. Schools for which fewer than 75 percent of the sampled cases had e-mail addresses (n = 430) were selected for tracing before being sent a lead letter. Prior to CATI operations, home contact information was sent to Telematch to obtain the latest telephone numbers. Initial tracing efforts included searches on the school's website for contact information. When this was not an option, more extensive database searches were employed during intensive tracing. In some cases, the searches confirmed or updated the contact information provided by the institution; in other cases, the searches resulted in new contact information. All locating information obtained as a result of these searches was loaded into the NSOPF:04 database. Intensive tracing. Intensive tracing was performed on a case if advance tracing did not yield a telephone number for loading in CATI, or if the case was designated as a dead end in CATI (i.e., there were no more telephone numbers to call for the case). The following steps were performed by the tracing unit to locate sample members. \u2022 Check the preloaded information using an online directory assistance search. This step was intended to identify the easy-to-locate cases (e.g., cases with the correct telephone number but the wrong area code). \u2022 Conduct credit bureau database searches. The tracing staff had access to various proprietary databases (TransUnion, Equifax, and Experian) containing current address and phone listings for the majority of consumers with a credit history. \u2022 Conduct additional intensive tracing. This step included (but was not limited to) searches using Lexis-Nexis and FastData, directory assistance calls, and searches of institution websites for campus directories. Tracing staff checked all new leads procured during their tracing efforts to confirm the addresses and telephone numbers that were obtained. When a telephone number for a sample member was confirmed, telephone interviewing resumed for that case. Cases with new address information were mailed a lead-letter packet. If the tracing staff located a new e-mail address for a sample member, the information was loaded into the database for future e-mail reminders and other mailings to nonrespondents."}, {"section_title": "Staff training", "text": "The mixed-mode design of the NSOPF:04 data collection required the development of three separate training programs for data collectors: help desk training, CATI interviewer training, and tracing. In addition, separate training sessions were conducted for supervisors and monitors. Detailed NSOPF:04 interviewer manuals were distributed at the outset of each training session. These manuals served as both an instruction guide for the training lectures, discussions, and practical exercises and as a reference guide for use after completion of training. Supplemental chapters that covered additional duties were provided for supervisors, monitors, and help desk agents. The manual's table of contents and an agenda for telephone interviewer training are included in appendix G. All training sessions included a study overview, a review of the confidentiality requirements, a demonstration interview, an in-depth review of the instrument, hands-on practice exercises with the instrument, and open-ended coding modules. In addition, the help desk and telephone interviewer training sessions included the following additional topics: \u2022 Help desk agents reviewed the \"frequently asked questions\" in detail, with a focus on responses to technical issues as well as instrument-specific questions, and instructions for documenting each call to the study hotline. \u2022 Telephone interviewers were trained in techniques for gaining cooperation of sample members, and of other contacts, as well as techniques for addressing the concerns of reluctant participants and for avoiding refusals."}, {"section_title": "Self-administered questionnaires", "text": "The first phase of data collection, lasting 4 weeks after the lead letters were mailed, provided an opportunity for respondents to complete the self-administered questionnaire via the Internet before the telephone follow-up calls began. The web interview site remained available 24 hours a day, 7 days a week, thereby giving sample members the option to complete the questionnaire online during the entire 9 months of data collection."}, {"section_title": "Help desk operations", "text": "The NSOPF:04 help desk opened on January 15, 2004, in anticipation of the first respondent calls after the lead-letter mailing. The help desk staff were available to assist sample members who had questions or problems accessing and completing the self-administered questionnaire. A toll-free hotline was set up to accept incoming help desk calls. If technical difficulties prevented a sample member from completing the self-administered questionnaire, a help desk staff member, also trained to conduct telephone interviews, would encourage the caller to complete a telephone interview rather than to attempt the self-administered questionnaire. All incoming calls from sample members were documented using the help desk software. In addition to this primary documentation function, the software provided information needed to verify a sample member's identity, login information (study ID and password) for the web questionnaire, and a means for tracking calls that could not be resolved immediately. The help desk software also provided project staff with reports on the types and frequency of problems experienced by sample members, as well as a way to monitor the resolution status of all help desk inquiries."}, {"section_title": "Telephone interviewing", "text": "Telephone prompts to nonrespondents began on February 12, 2004, at the end of the early-response incentive period. CATI procedures included attempts to locate, gain cooperation from, and interview study sample members who had not completed the questionnaire online. Interviewers encouraged respondents to complete the interview by telephone as soon as they made contact. However, if the sample member expressed a preference for completing the selfadministered questionnaire via the Internet, a callback was scheduled for 1 week later. During these callbacks, interviewers again prompted the faculty members to complete the questionnaire by telephone. Refusal conversion procedures were used to gain cooperation from individuals who refused to complete the questionnaire. When a refusal was first encountered, either because the sample member refused or because a \"gatekeeper\" (secretary or spouse) refused on behalf of the sample member, the case was referred to a refusal conversion specialist. Refusal conversion specialists were selected from among those interviewers most skilled at obtaining cooperation and were given training in refusal conversion techniques tailored to NSOPF:04. The refusal training emphasized ways to gain cooperation, overcome objections, address the concerns of gatekeepers, and encourage participation."}, {"section_title": "Data Collection Systems", "text": ""}, {"section_title": "Instrument Development and Documentation System", "text": "The Instrument Development and Documentation System (IDADS) is a controlled web environment in which project staff developed, reviewed, modified, and communicated changes to specifications, code, and documentation for the NSOPF:04 instrument. All information relating to the NSOPF:04 instrument was stored in a Structured Query Language (SQL) Server database and was made accessible through Windows and web interfaces. There are three modules within IDADS: specification, programming, and documentation. Initial specifications were generated within the IDADS specification module. This module enabled access for searching, reviewing, commenting on, updating, exporting, and importing information associated with instrument development. All records were maintained individually for each item, which provided a historical account of all changes requested by both project staff and NCES. Once specifications were finalized, the programming module within IDADS produced hypertext transfer markup language (html), Active Server Pages (ASP), and JavaScript template program code for each screen based on the contents of the SQL Server database. This output included screen wording, response options, and code to write the responses to a database, as well as code to automatically handle such web instrument functions as backing up and moving forward, and recording timer data. For questions that had changed significantly since the field test, the programming staff edited the automatically generated code to customize screen appearance and program response-based routing. For questions with minor changes, the programming staff simply modified the program code used in the field test. The documentation module contained the finalized version of all instrument items, their screen wording, and variable and value labels. Also included were the more technical descriptions of items such as variable types (alpha or numeric), information regarding to whom the item was administered, and frequency distributions for response categories. The documentation module was used to generate the instrument facsimiles and the Electronic Codebook (ECB) input files."}, {"section_title": "Integrated Management System", "text": "All aspects of the study were under the control of an Integrated Management System (IMS). The IMS was a comprehensive set of desktop tools designed to give project staff and NCES access to a centralized, easily accessible repository for project data and documents. The NSOPF:04 IMS consisted of three components: the management module, the Receipt Control System (RCS), and the Case Management System (CMS). The management module of the IMS contained tools and strategies to assist project staff and the NCES project officer in managing the study. All information pertinent to the study was located there, accessible via the Internet, in a secure desktop environment. Available on the IMS website were the project schedule, monthly progress reports, daily data collection reports and status reports (available through the RCS described below), project plans and specifications, project information and deliverables, instrument specifications, staff contacts, the project bibliography, a document archive, and frequencies for the faculty and institution data. The IMS management module also had a download area from which the client and subcontractors retrieved large files when necessary. The Receipt Control System (RCS) was an integrated set of systems that monitored all activities related to data collection, including tracing and locating. Through the RCS, project staff were able to perform stage-specific activities, track case statuses, identify problems early, and implement solutions effectively. RCS locator data were used for a number of daily tasks related to sample maintenance. Specifically, the mailout program produced mailings to sample members, the query system enabled administrators to review the locator information and status for a particular case, and the mail return system enabled project staff to update the locator database. The RCS also interacted with the Case Management System and tracing unit databases, sending locator data among the three systems as necessary. The Case Management System (CMS) was the technological infrastructure that connected the various components of the CATI system, including the questionnaire, utility screens, databases, call scheduler, report modules, links to outside systems, and other system components. The call scheduler assigned cases to interviewers in a predefined priority order. In addition to delivering appointments to interviewers at the appropriate time, the call scheduler also calculated the priority scores (the order in which cases need to be called based on preprogrammed rules), sorted cases in non-appointment queues, and computed time zone adjustments to ensure that the sampled respondents were not phoned outside the specified calling hours. 21 The call scheduler also allowed callbacks to be set and assigned status codes to the case. Using an algorithm based on the previous call results, the call scheduler determined which telephone number (e.g., home or work) associated with the case should be called next. 21 Call Center hours were 9:00 a.m. to 11:00 p.m. Monday through Friday, 9:00 a.m. to 5:00 p.m. Saturday, 1:30 p.m. to 9:30 p.m. Sunday, Eastern Standard Time. The CMS was programmed to account for time zones such that respondents would not be called after 9:00 p.m. local time. Work numbers were only called 8:00 a.m. to 5:00 p.m. Monday through Friday, local time."}, {"section_title": "Chapter 3 Data Collection Outcomes", "text": "The success of the 2004 National Study of Postsecondary Faculty (NSOPF:04) was dependent upon achieving high levels of cooperation at all stages of the data collection process. The data collection results-namely the institution and faculty response rates, along with the results of the efforts that contributed to those rates-are the focus of this chapter."}, {"section_title": "Institution Data Collection Results", "text": ""}, {"section_title": "Institution Participation", "text": "Of the 1,080 institutions selected to participate in NSOPF:04, 1,070 were eligible institutions. 22 Of the eligible institutions, 97 percent (unweighted) appointed an Institution Coordinator (IC) to assist with study requirements and 85 percent completed the Coordinator Response Form (CRF), indicating their initial intent to participate in both components of the study and adhere to project timelines. Ultimately, 91 (unweighted) percent of the eligible institutions provided a list of faculty and 86 percent completed institution questionnaires. Fifty-seven institutions indicated having policies that required the 2004 National Study of Faculty and Students (NSoFaS:04) survey request be submitted to their Institutional Review Board (IRB) for formal approval. One advantage of the advance notification period is that it allowed the contractor sufficient time to prepare customized IRB approval packages for submission to each of these institutions. This procedure expedited the approval process and alleviated the burden on the IC. Of the 60 institutions that were sent IRB approval packages, all but three approved participation in NSOPF:04."}, {"section_title": "Faculty lists", "text": "Two key changes in data collection procedures had the potential to impact faculty list participation rates for NSOPF:04; namely the advance notification initiative begun in March 2003, and the decision to combine the data collection efforts for NSOPF:04 with the 2004 National Postsecondary Student Aid Study (NPSAS:04) under the NSoFaS:04. Table 9 compares the participation rate achieved in NSOPF:04 with previous cycles. A total of 980 (91 percent, unweighted and weighted) of eligible institutions provided a faculty list, with all institutional strata exceeding a weighted participation rate of 85 percent. The breakdown of institutions providing faculty lists, by institution type, is presented in table 10. "}, {"section_title": "Institution questionnaire", "text": "A total of 920 institutions, representing 84 percent of eligible institutions, completed the institution questionnaire. Table 11 provides a breakdown of institution participation by strata.  Table 12 shows the breakdown of completed institution questionnaires by mode of administration. Those completed in computer assisted telephone interview (CATI) mode include instances where the questionnaire was finalized with interviewer assistance (e.g., questionnaires wholly or partially data-entered by project staff from information supplied on hardcopy by the institution) and questionnaires completed, wholly or partly, by CATI. Web completions are defined as those questionnaires transmitted as complete by the institution, although some institutions may have provided some responses in CATI. Nearly 81 percent of institutions completed the institution questionnaire using the Web, and 19 percent completed it with the help of an interviewer. By comparison, in 1999, the only previous cycle in which a web questionnaire was used, 69 percent of the questionnaires were done on paper, and 31 percent were done on the Web. The percentage of interviews completed at least in part by CATI is fairly consistent with the number of questionnaires completed with interviewer assistance in previous cycles. "}, {"section_title": "Institution Survey Completion Timing", "text": "The timing analysis was conducted by embedding time stamps in the programming code for each form (screen) in the survey. From these time stamps, the number of seconds spent on each screen (on-screen time) and the transit time between screens (i.e., the time required to transmit data to the server, for the server to store the data and assemble the next page, and for the page to be transmitted and loaded on the computer) were calculated. A cumulative on-screen time and a cumulative transit time for the institution survey also were calculated from the time stamps. The sum of the cumulative on-screen and transit times was the total instrument time (i.e., the number of minutes it took to administer the institution questionnaire). Unlike most questionnaires, which require the respondent to complete the survey in sequential order, the institution questionnaire included a status screen that allowed respondents to jump to particular questions they could answer, and skip over ones they could not answer. For most institutions, the questionnaire was completed in multiple internet sessions and, in some cases, by multiple people at the institution. The target time to complete the institution questionnaire was 45 minutes. Based on the time stamps for each form, the actual time to complete the questionnaire ranged from less than 1 minute to 4 hours and 9 minutes, with an average of 35 minutes. 23 Of these 35 minutes, approximately 31 minutes, on average, were spent answering questions (on-screen time) and 5 minutes, on average, were spent in transit. These numbers may be misleading because some institutions may have completed the sample hardcopy version of the questionnaire in advance, so their time to complete the web questionnaire simply reflected the time it took to key in their responses. Table 13 reports the average and maximum times (in seconds) to complete each form in the institution instrument. Ten forms (screens) of the institution survey took more than 1 minute to complete, on average. Each of these forms required the respondent to look up information and/or requested several pieces of information, which accounts for the longer times on these screens. "}, {"section_title": "Faculty Data Collection Results", "text": "Faculty data collection efforts for NSOPF:04 consisted of three essential steps: locating (identifying telephone numbers and addresses for sample members), contacting (carrying out the necessary steps to reach the faculty member), and encouraging survey completion by web-based self-administration or CATI. This section describes the results of the NSOPF:04 data collection effort and evaluates the effectiveness of the data collection procedures used in locating, contacting, and interviewing sample members."}, {"section_title": "Response Rate", "text": "Overall contacting and survey completion results for the faculty contact phase of NSOPF:04 are presented in figure 5. Of the 35,630 cases in the original sample, 1,300 (4 percent) were excluded because they were ineligible for the study or deceased. Of the 34,330 eligible sample members, 29,820 (87 percent) were contacted and 26,110 completed the survey, for an unweighted and weighted response rate of 76 percent achieved in the 9-month period from January 15 to October 6, 2004.     "}, {"section_title": "Locating and Survey Completion", "text": "Most of the faculty lists provided by the institutions contained contact information for sample members, including the sample member's name, office telephone number, school name, school address, and department. For some cases, home addresses also were provided. In addition, a number of approaches were used to locate faculty and instructional staff, including advance tracing, the initial mailing to all sample members, follow-up letters and e-mails to nonrespondents, telephone tracing (interviewers calling telephone numbers provided on the faculty lists as well as any additional numbers obtained during the course of making those calls), and intensive tracing (i.e., using consumer databases, internet searches, and criss-cross directories). Before the start of data collection, schools' faculty lists were assessed for completeness of contact information. As necessary, advance tracing, described in section 2.4.2, was conducted. As shown in table 16, the contact information provided by the school proved effective in contacting faculty and instructional staff; 83 percent of sample members required no intensive tracing, while the remaining 17 percent required intensive tracing. Intensive tracing was required when a case did not have a telephone number associated with it or the CATI calls had exhausted all numbers for the case without reaching the sampled individual. Approximately 52 percent of cases sent to intensive tracing were located, compared to 90 percent of cases not sent to intensive tracing. Further, only 40 percent of cases sent to intensive tracing completed an interview compared with 80 percent of cases not sent to intensive tracing.  Table 17 provides an overview of the primary sources used by tracers during the intensive tracing process. Tracers generally used multiple sources when tracing a case, so no one source can be pinpointed as the one that resulted in the \"locate.\" Among the sources used most frequently for intensive tracing were internet searches, directory assistance, and various consumer database searches. The breakdown of faculty and instructional staff requiring intensive tracing, by faculty status and institution type, is presented in table 18. Thirty-two percent of part-time faculty required intensive tracing, compared to 7 percent for full-time faculty (\u03c7 2 = 3806.9, p < .0001). Seventeen percent of faculty at public institutions required intensive tracing compared to 16 percent at private not-for-profit institutions (\u03c7 2 = 16.5, p < .0001). The results of faculty and instructional staff locating and survey completion, by faculty status and institution type, are shown in table 19. Ninety percent of full-time faculty members were located, compared with 75 percent of part-time faculty (\u03c7 2 = 1414.6, p < .0001). Eighty-one percent of full-time faculty completed the survey, compared with 69 percent of part-time faculty (\u03c7 2 = 903.8, p < .0001). When examined by institution type, locate rates ranged from 80 to 88 percent. Survey completion rates ranged from 71 percent for faculty at private not-for-profit doctorate-granting institutions to 81 percent at private not-for-profit baccalaureate-granting institutions. The results of faculty and instructional staff survey completion by mode of data collection are presented in table 20. A total of 19,780 respondents (76 percent) completed the self-administered questionnaire and 6,330 respondents (24 percent) completed the CATI interview. (It should be noted that 59.2 percent completed the survey during the early phase, without telephone followup). While NSOPF:04 exceeded the goal of having 50 percent of completes by web, a substantial portion of these web surveys were completed only after having been called by a CATI interviewer. Eighty-one percent of full-time faculty completed the self-administered survey, compared to 65 percent of part-time faculty (\u03c7 2 = 776.6, p < .0001). Seventy-seven percent of faculty and instructional staff at private not-for-profit institutions completed the self-administered survey, compared to 75 percent of faculty at public institutions (\u03c7 2 = 13.6, p < .0002). Self-administered web survey completion rates by institution type ranged from 68 percent for public associate's degree-granting schools to 84 percent for private not-for-profit associate's degree-granting schools. The cumulative response rate, overall and by mode, is shown in figure 6. "}, {"section_title": "E-mail Contacting Efforts", "text": "E-mail addresses of faculty and instructional staff were requested in the faculty lists. Where e-mail addresses were not provided by the institution, efforts were made through an advance search of the institution's online directory for e-mail addresses of sample members as well as other database searches. In addition, some sample members provided e-mail addresses when contacted by a telephone interviewer. E-mail addresses were available for 27,980 (82 percent) of the 34,330 eligible sample members. Periodically throughout the data collection period, e-mail messages were sent to nonrespondents to encourage their participation (see appendix F). Sample members who received e-mails were more likely to complete the survey (78 percent) compared to sample members to whom no e-mail reminders were sent (53 percent; \u03c7 2 = 1867.2, p < .0001). Respondents with e-mail addresses were more likely to complete the self-administered web questionnaire (79 percent) than were respondents who were not sent e-mail reminders (55 percent; \u03c7 2 = 976.1, p < 0.0001)."}, {"section_title": "Refusal Conversion Efforts", "text": "Refusal conversion measures were used to gain cooperation from individuals who refused to participate when contacted by telephone interviewers. Refusals came not only from sample members, but also occasionally from other household members or other contacts (such as secretaries). 24 Whenever a refusal was encountered, unless it was deemed hostile, the case was referred to a specialist trained in refusal conversion techniques. Refusal conversion specialists were chosen based on their performance as interviewers, with those who were the most skilled in obtaining cooperation given additional training in converting refusals. This training was tailored to the concerns of faculty members and gatekeepers regarding participation, and focused on gaining cooperation and encouraging participation. Ten percent of contacted cases refused to participate at some point during data collection. However, 18 percent of these cases were successfully converted and eventually completed the survey. Fifty-nine percent of the converted cases completed the web self-administered questionnaire, and 41 percent completed a telephone interview. An abbreviated instrument, consisting of sections A (nature of employment), B (academic/professional background), and G (sociodemographic characteristics) from the faculty instrument, was developed to convert nonrespondents by offering a shorter (10 minute) interview. The abbreviated instrument, used only in the final 3 weeks of data collection, yielded 1,610 interviews."}, {"section_title": "Incentives", "text": "For the NSOPF:04 full-scale data collection, three types of incentives were offered to eligible sample members. In accordance with findings from the NSOPF:04 field test incentive experiment 25 (Heuer et al. 2004), incentives were offered during two phases of data collection: an initial early-response incentive period and a nonresponse incentive period. In addition to those periods, refusal incentives were made available following initial refusals. During each incentive phase, respondents were offered the choice of a $30 check or a $30 gift certificate to Amazon.com. The initial early-response incentive was offered to all sample members for completion of the questionnaire within the first 4 weeks of data collection. The early-response incentive was designed to increase the response rate during the initial phase of data collection and promote a higher rate of web self-administered responses and reduce costs associated with telephone interviewing. Following the initial 4-week period, CATI telephone prompting began. During this second phase of the study, no incentive was offered to respondents for completing the interview. All nonrespondents from the first phase were contacted by telephone and asked to complete the survey, either on the phone or via the Web at their convenience. Any sample member who refused to participate in the study was flagged for the refusal incentive. A refusal conversion letter was sent out to explain the study and request that the sample member reconsider the decision not to participate and to announce the reinstitution of the $30 incentive for participating. 24 Nearly 77 percent of all refusals were made by sample members, while the remaining 23 percent were made by other household members or other contacts. Of the sample members who initially refused, 17 percent eventually completed an interview. 25 The field test experimental design consisted of three randomly assigned early-response incentive groups who were offered $0, $20, or $30 to complete the self-administered questionnaire over the Internet within 3 weeks of the initial mailing and two nonresponse incentive groups of $0 and $30 for those who had not completed the survey by a certain date during data collection. The early-response incentive yielded 31 and 34 percent response rates for the $20 and $30 incentives, respectively, compared with a 16 percent response rate for the control group. The nonresponse incentive yielded a 47 percent response rate for those offered $30 and a 34 percent response rate for the control group. The differences between the treatment and the control groups were statistically significant for both phases of the experiment; however, the apparent difference in amounts ($20 versus $30) for the early-response incentive period, while in the expected direction, was not statistically significant. Nonresponse incentives were introduced after 8 weeks of CATI prompting of all nonrespondents who had not already been offered the refusal incentive. Letters and e-mail prompts were sent periodically to nonrespondents throughout the data collection period. All correspondence mentioned the incentive when it was available to sample members. Table 21 provides a breakdown of the types of incentives offered and the results of each incentive period. These results indicate that the combination of early-response incentive and other later incentives was required to reach the targeted response rate within the data collection schedule. While the early-response incentive was effective in getting 44 percent of the eligible sample members to complete the survey within the initial 4 weeks of the study, and an additional 15 percent of the sample members completed within 8 weeks after the initial incentive period, the cumulative response rate after 12 weeks was only 59 percent. The refusal and nonresponse incentives were undoubtedly helpful in attaining the additional 17 percent needed to reach the 76 percent response rate."}, {"section_title": "Burden and Effort", "text": ""}, {"section_title": "Faculty Survey Completion Timing", "text": "Like the institution timing analysis, the faculty timing analysis was conducted by embedding time stamps in the programming code for each form (screen) in the survey. From these time stamps, the number of seconds spent on each screen (on-screen time) and the transit time between screens (i.e., the time required to transmit data to the server, the time for the server to store the data and assemble the next page, and the time for the page to be transmitted and loaded on the computer) were calculated. A cumulative on-screen time and a cumulative transit time for the faculty survey also were calculated from the time stamps. The sum of the cumulative on-screen and transit times was the total instrument time (i.e., the number of minutes it took to administer the faculty questionnaire). Following the 1999 cycle of NSOPF-which averaged over 50 minutes-the faculty questionnaire was shortened substantially, with a goal of achieving a 30-minute survey. The NSOPF:04 field test averaged 42 minutes. Based on the time stamps for each form in the fullscale instrument, the time to complete the entire survey ranged from 8 minutes to 3 hours and 6 minutes, 26 with an average time of 30 minutes. Of these 30 minutes, approximately 26 minutes, on average, were spent answering questions (on-screen time) and 3 minutes, on average, were spent saving data and loading forms (transit time). Table 22 presents the overall timing data by mode for completed surveys (excluding partial and abbreviated interviews). Average on-screen time was significantly longer for CATI respondents than for web respondents (27 and 26 minutes, respectively; t = -4.46, p < .0001), while the average transit time was significantly shorter for CATI respondents than for web respondents (1 and 4 minutes, respectively; t = 34.94, p < .0001). Presumably, the longer onscreen time for CATI respondents is due to the time it takes to read text out loud, and to the fact that the respondent may ask questions. The shorter transit time for CATI is likely due to the use of a high-speed internet connection by interviewers. Some web respondents may have used slower dial-up connections, which increase transit time. Overall, the interview took less time for CATI respondents than for web respondents (29 minutes and 30 minutes, respectively; t = 7.80, p < .0001). The onscreen, transit, and total times were significantly shorter for surveys that were completed during business hours (Monday through Friday, 9:00 a.m. to 6:00 p.m.) compared to those completed during evening and weekend hours (onscreen: 26 and 27 minutes, respectively; t = 4.79, p < .0001; transit: 3 and 4 minutes, respectively; t = 17.71, p < .0001; total: 29 and 31 minutes, respectively; t = 10.29, p < .0001), as shown in table 23. This may be due to faster internet connections for web respondents at their offices compared to their homes or time pressures during the workday.  Table 24 provides the average and maximum times (in seconds) to complete each form in the faculty instrument. Seven forms (screens) in the faculty survey took more than 1 minute to administer, on average. These tended to be the more complicated forms and those that collected multiple pieces of information on a single screen. These forms are described in greater detail below.   Q31 and Q32. The questions that asked for the number of hours per week spent on work activities, Q31 (by paid and unpaid activities at the target institution and outside that institution), and the percent distribution of work activities, Q32, took 94 and 78 seconds, respectively, to administer. Each of these forms took longer when administered by telephone interviewers than when self-administered via the web instrument. Q31 averaged 91 seconds for web respondents compared with 103 seconds for CATI respondents (t = -13.64, p < .0001). Web respondents averaged 76 seconds on Q32 compared with an average time of 83 seconds for CATI respondents (t = -7.59, p < .0001). The complexity of these questions may have led to the longer times for CATI administration, as respondents often asked interviewers to repeat the question and examples, and asked questions about the appropriate category for certain types of activities. Q37 and Q38. Two consecutive forms, Q37 and Q38, asked for a great deal of information on a single screen. Q37 was a matrix-style question that asked six questions about each of the credit classes (up to five) the respondent taught. This form took 99 seconds, on average, to administer, with CATI respondents taking significantly less time than web respondents (94 and 100 seconds, respectively, t = 4.26, p < .0001). The matrix of items on Q37, visually different from the rest of the forms in the questionnaire, likely took web respondents extra time to make sense of and answer. Q38 asked respondents to identify which of 10 different types of student evaluation tools were used in their classes and whether they were used in all, some, or none of the classes. This form took an average of 68 seconds to administer, with CATI respondents taking significantly longer than web respondents (93 and 60 seconds, respectively, t = -49.69, p < .0001)."}, {"section_title": "Q48.", "text": "This form asked for the number of hours per week the respondent spent on four activities (thesis/dissertation committees, administrative committees, with advisees, and office hours). On average, respondents took 61 seconds to complete this form, with CATI respondents taking significantly longer than web respondents (72 and 57 seconds, respectively, t = -24.37, p < .0001). Q52A. Q52A, which asked for the number of career publications or presentations in seven categories, took an average of 100 seconds to complete. This may have required respondents to locate their curricula vitae and count the number of publications. CATI respondents spent significantly more time on this item than web respondents (106 and 98 seconds, respectively, t = -5.88, p < .0001)."}, {"section_title": "Q66.", "text": "The form asking about respondents' compensation from the target institution and from other sources, Q66, took 111 seconds to complete, on average. This form consisted of six income questions, which were considered to be among the most sensitive items in the questionnaire. Average time to complete this form was shorter for web respondents (109 seconds) than for CATI respondents (118 seconds; t = -7.60, p < .0001)."}, {"section_title": "Help Desk", "text": "To gain a better understanding of the problems encountered by faculty members attempting to complete the web self-administered questionnaire, software was developed to record each help desk incident that occurred during data collection. For each occurrence, help desk staff confirmed contact information for the sample member, recorded the type of problem, described the problem and resolution, noted its status (pending or resolved), and recorded the approximate time it took to assist the faculty member. Help desk staff were trained not only to answer any calls received from the help desk hotline, but also to conduct telephone interviews when needed. Help desk staff members assisted sample members with questions about the web instrument and provided technical assistance to sample members who experienced problems while completing the self-administered web survey. Help desk agents also responded to voice mail messages left by respondents when the call center was closed. Help desk staff assisted 3,860 faculty members (11 percent of the sample). Eighty-one percent of these cases called the help desk only once, 12 percent called twice, 4 percent called three times, and 3 percent called four or more times. Of the 3,860 faculty members who called the help desk, 2,940 (76 percent) eventually completed the survey. Twenty-nine percent of the problems reported by faculty members who called the help desk were for miscellaneous issues. The miscellaneous issues were first coded into specific issues and then these issues were coded into five broader categories as shown in table 25. First time calls included setting an appointment for the CATI interview, providing a new phone number or e-mail address, promising to complete by phone at a later date, or promising to complete the survey on the Web. Nearly 7 percent of help desk contacts were faculty members calling in to refuse. Follow-up calls to the help desk (6 percent) included faculty members checking on the incentive, or verifying that they had completed the survey. Other miscellaneous issues were less than 2 percent of all contacts. Slightly more than 1 percent of help desk calls reported that the faculty member was not at the phone number, e-mail address, or college that was contacted. Other specific issues handled by the help desk included requests to complete the survey by telephone (21 percent), questions about the study (19 percent), browser setting and computer problems (14 percent), requests for study ID and/or password (12 percent), errors in questionnaire programming (3 percent), questions about questionnaire content (2 percent), website being down or unavailable (1 percent), and routing/skip problems (less than 1 percent). "}, {"section_title": "Interviewer Hours", "text": "A total of 17,639 telephone interviewing staff hours (including help desk staffing, telephone follow-up calls, and CATI interview hours) were expended during faculty data collection. These hours do not include supervision, monitoring, administration, and Quality Circle meetings. The average time spent per completed CATI interview was 2.7 hours and per completed interview overall (including web completes) was 0.7 hours. The average time to administer the CATI was 29 minutes, which shows that a majority of interviewer time was spent on other activities. These other activities focused on contacting and locating the sample member, with a small portion of time devoted to bringing up a case, reviewing its history, and closing the case (with the appropriate reschedule, comment, and disposition). A significant proportion of the web completes occurred after the period of telephone follow-up began and were completed only after several CATI follow-up calls had been made to the respondent."}, {"section_title": "Number of Calls", "text": "Telephone interviewers made 226,777 call attempts to faculty members during the NSOPF:04 data collection period (see table 26). The number of calls per case ranged from 0 to 152. On average, six calls 27 were made to each sample member. Those who were not interviewed received the highest average number of calls. An average of four call attempts were required for respondents compared to an average of 13 call attempts for nonrespondents (t = 60.9, p < .0001). Faculty members who completed the web self-administered questionnaire were called significantly fewer times, with an average of three call attempts per completed survey, compared to an average of eight calls to CATI respondents (t = 41.5, p < .0001). Call screening is a growing problem for studies that rely on the telephone as a mode of contact. Devices such as telephone answering machines can be used to screen unwanted calls. Of the 19,394 cases called by telephone interviewers, 15,183 cases (78 percent) reached an answering machine at least once (see table 27). Interviewers made significantly more calls to cases where an answering machine had been reached at least once (mean attempts = 13) than they did to cases where no answering machine was reached (mean attempts = 5; t = -46.81, p < .0001). Likewise, cases where an answering machine had been reached at least once were less likely to have completed the interview (54 percent) than cases where no answering machine was reached (63 percent; \u03c7 2 = 92.4, p < .0001). 4.0 1 Numbers rounded to the nearest 10. NOTE: Excludes 16,240 completed cases that were never called by telephone interviewers because they completed the self-administered questionnaire during or soon after the early-response period of data collection. Some of the cases called by telephone interviewers actually completed the web self-administered questionnaire. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2004 National Study of Postsecondary Faculty (NSOPF:04). Looking only at completed cases, significantly fewer calls were required to obtain a completed interview when no answering machine was reached (mean attempts = 4) compared to cases in which an answering machine was reached at least once (mean attempts = 11, t = -40.69, p < .0001). Those who possessed answering machines were included in the survey definition of \"accessible\"; however, it took considerable persistence and resources (in the form of repeated call attempts) to reach these faculty members. This finding demonstrates that answering machines and other call screening devices are increasing the effort that must be expended to reach these cases, thereby driving up interviewing costs. In addition, cases where an answering machine was reached on more than one-half of the call attempts required significantly more effort to contact and interview. The mean number of attempts for cases that reached an answering machine less than one-half of the time was 10 compared to 13 (t = -15.3, p < .0001) for cases that reached an answering machine more than one-half of the time. Similarly, among completed cases, significantly fewer calls were needed to complete an interview with cases where an answering machine was reached less than one-half of the time (mean attempts = 8) compared with those where an answering machine was reached more than one-half of the time (mean attempts = 11; t = -12.9, p < .0001)."}, {"section_title": "Conclusions", "text": "Of the 1,070 eligible institutions, 980 (91 percent, unweighted and weighted) provided faculty lists and 920 (86 percent) completed the institution questionnaire. A total of 26,110 faculty and instructional staff completed the faculty survey for a 76 percent response rate. Approximately three-quarters (76 percent) of respondents completed the web self-administered questionnaire rather than the CATI (24 percent). Strategies that helped attain this response rate included tracing, e-mail contacting, and refusal conversion efforts, along with targeted incentives."}, {"section_title": "Chapter 4 Evaluation of Data Quality", "text": "Evaluations of data quality serve to identify problems with the data collection processes and instruments in order to remedy them for the next cycle of the study. Project staff evaluated faculty list quality, item nonresponse, item mode effects, breakoffs, coding, quality control monitoring of interviewers, and interviewer feedback. The results of these evaluations are presented in this chapter."}, {"section_title": "List Quality", "text": ""}, {"section_title": "List Types", "text": "Faculty lists may be characterized both by type of media-whether they are electronic or hardcopy-and method of transmission (e.g., fax or mail, e-mail, electronic upload). For the 2004 National Study of Postsecondary Faculty (NSOPF:04), institutions were asked to provide a single, unduplicated (i.e., duplicate entries of names removed) electronic list of faculty in any commonly-used and easily processed format (e.g., ASCII fixed field, comma delimited, spreadsheet format). These preferred electronic file formats are far less labor intensive to process than paper lists and more easily unduplicated by ID number. However, as in previous cycles, paper lists were accepted, as were multiple files (e.g., separate files of full-and part-time faculty) and lists in electronic formats that did not lend themselves to electronic processing (such as word processing formats). For the first time, institutions were given the option to transmit their electronic faculty lists via a secure upload to the National Study of Faculty and Students (NSoFaS:04 ) website and were encouraged to do so. (In previous cycles, direct upload was available only by file-transfer protocols, an option that few institutions utilized). Institutions were also given the option of sending a CD-ROM, diskette, or paper list containing the list data or sending the list via e-mail (as an encrypted file, if necessary). As shown in table 28, the vast majority of lists received were in electronic formats. Of 980 participating institutions, 830 (85 percent) supplied an electronic list by upload, e-mail, CD-ROM, or diskette. Institutions showed a clear preference for uploading their list by direct upload; 590 institutions (60 percent of lists overall and 71 percent of electronic lists) delivered their data in this manner. NSOPF:04 clearly benefited from the increased capability and willingness of institutions to supply lists in electronic formats, compared to previous cycles. As table 29 shows, 65 percent of institutions supplied an electronic list for NSOPF:99, with a majority of them in CD-ROM or diskette formats sent by mail.  2 FTP was utilized only in 1999; upload was utilized only in 2004. 3 In 1999, lists abstracted from web resources were processed as, and included with paper lists. NOTE: Numbers rounded to the nearest 10. Detail may not sum to totals because of rounding. FTP = file transfer protocol. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2004 National Study of Postsecondary Faculty (NSOPF:04). For institutions that indicated they lacked the staff or resources to compile a list of faculty on their own within schedule constraints, it was sometimes possible to abstract a list from employee directories, course schedules, or course catalog listings available on the institution's website or through other web resources. As in past cycles (where course catalogs or directories were used as lists of last resort), all such lists were reviewed to ensure they were sufficiently complete for sampling (i.e., included both full-and part-time faculty, did not systematically exclude any subset of faculty and instructional staff). It should be noted that in past cycles, course catalogs and directories comprised a large percentage of lists supplied on (or processed as) paper. While the web listings utilized for NSOPF:04 required more processing than electronic lists (including reformatting into a spreadsheet or re-keying), they proved, overall, to be far less problematic for processing and sampling than an equivalent paper list. Only 15 percent of institutions submitted paper lists or had lists abstracted from web resources for NSOPF:04; this compares to 35 percent of institutions who submitted paper lists (including lists abstracted from web resources) in NSOPF:99."}, {"section_title": "List Data Quality", "text": "As in prior administrations of this study, secured faculty lists were evaluated for accuracy and completeness of information before they were processed for sampling. To facilitate quality control, faculty list counts were compared against counts obtained from the following supplementary sources: \u2022 the institution questionnaire and/or the file layout form, if a questionnaire was not completed but an overall faculty count was supplied; \u2022 the 2001 Integrated Postsecondary Education Data System (IPEDS) Fall Staff Survey; \u2022 the Contact Information and File Layout (CIFL) form (which included faculty counts, and used when questionnaire data was unavailable); and \u2022 NSOPF:99: frame data from the 1999 survey. Discrepancies in counts of full-and part-time faculty between the faculty list and other sources that were outside the expected range were investigated. All institutions with submitted lists that failed any checks were recontacted to resolve the observed discrepancies. Because of time and definitional differences between NSOPF and IPEDS, it was expected that the faculty counts obtained from the institutions and IPEDS would include discrepancies. Consequently, quality control checks against IPEDS were less stringent than those against the institution questionnaire. However, list count comparisons against IPEDS and NSOPF:99 data were useful in identifying systematic errors, particularly those related to miscoding of the employment status of faculty members. Table 30 shows the types of discrepancies encountered by type of institution.  Table 31 shows the percent differences between the three sources of data for all cycles of NSOPF (1988, 1993, 1999, and 2004). The discrepancies between the faculty lists and institution questionnaire counts have declined over time. Also, table 32 shows mean differences between sources of data across all cycles of NSOPF. More details regarding the quality of faculty lists secured for NSOPF:04 are provided in appendix H.   (0.7) 7.8 * (0.7) * Statistically significant at alpha = .05, based on paired t-test. 1 Standard errors assume simple random sampling. 2 Observations with percent differences greater than 50 in absolute value were excluded. NOTE: LIST refers to the faculty list provided by sampled institutions; IPEDS is the National Center for Education Statistics' Integrated Postsecondary Education Data System; QUEX refers to the institution questionnaire. Numbers rounded to the nearest 10. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2004 National Study of Postsecondary Faculty (NSOPF:04)."}, {"section_title": "Item Nonresponse", "text": "Recent studies (for example, DeRouvray and Couper 2002) using web self-administered questionnaires have shown higher than usual rates of missing data when the \"refuse\" and \"don't know\" options are presented on the screen. To limit the rate of nonresponse in the institution and faculty instruments, the refusal option was unavailable to respondents and the \"don't know\" option was limited to selected screens where the respondent might not know the answer (e.g., expected age at retirement). On the information page at the start of the questionnaire, respondents were instructed to click the \"continue\" button to proceed to the next question if they wished to decline to answer a question. For the institution questionnaire, items with a high rate of missing responses are often those that required lookup by an office on campus other than the Institution Coordinator's (e.g., Human Resources or Academic Affairs) and might reflect a lack of cooperation from those other offices. Two of the 90 items in the questionnaire had more than 15 percent of the data missing. Details of institution item nonresponse, including the nonresponse bias analysis, are presented in appendix I. Thirty-four of the 162 items in the faculty questionnaire had more than 15 percent of the data missing. 28 With the exception of the income items, which were expected to have higher rates of refusal due to their sensitive nature, the primary reason item nonresponse exceeded 15 percent for these items is that each applies to a relatively small subset of respondents (i.e., small denominator) and these items were not included in the abbreviated instrument. The nonresponse bias analysis and details of faculty item nonresponse are presented in appendix I."}, {"section_title": "Faculty Data Quality", "text": ""}, {"section_title": "Item Mode Effects", "text": "The NSOPF:04 faculty instrument was designed to minimize potential mode effects by using a single instrument for both self-administration and CATI. However, whenever multiple modes are used for data collection, the possibility of mode effects is inherent. Because respondents were offered the option of completing the interview by themselves on the Web or with an interviewer, there was the potential for bias due to self-selection or other factors which cannot be accounted for. Therefore, these results should be interpreted as how respondents in different modes of administration answered the survey questions and not as true mode differences. Due to the large sample size, nearly all test statistics used to measure differences between self-administered and CATI respondents were significant. Reporting all of these statistically significant differences is not substantively meaningful; therefore, only differences of five percentage points or greater are reported. 29,30 For this analysis, 47 variables were selected, covering the following topic areas: demographic variables, descriptive items, factual items, and opinion-based questions. Criteria for selection of items included importance to the content of this study. Items for which project staff had concerns that there might be mode effects (e.g., complex matrix items) were also selected. Although not presented in tables, the following discussion on item mode effects is based on special tabulations from the 2004 NSOPF faculty data."}, {"section_title": "Demographics", "text": "Compared to their CATI counterparts, web respondents were more likely to be White (Q74E: 87 percent versus 81 percent, z = 11.65, p < .001). Conversely, CATI respondents were 28 The items included in this analysis are listed in appendix K (Q1 through Q83). The number of items differs from the number of faculty items reported elsewhere in this document. For example, the difference between the number of analysis variables (162) and the number of items in the faculty questionnaire (183) occurs because some items in the faculty questionnaire were for internal use; similarly, there are fewer stochastically imputed variables (144) than analysis variables (162) because some variables had no missing data after logical imputations were performed. 29 For questions where means were used, the unit of measurement, range of answers, and standard deviation was evaluated to determine which statistically significant differences to report. 30 Footnotes are used to report differences in the 3 to 4 percent range since these could be seen as indicative of a substantively important difference. more likely to be Black or African American (Q74C: 14 percent versus 7 percent, z = -16.98, p < .001) than their web counterparts. No mode differences were observed for gender or age. 31 "}, {"section_title": "Descriptors", "text": "Web respondents were more likely than CATI respondents to report research as their primary activity (Q4: 11 percent versus 6 percent, z = 11.62, p < .001), be employed full-time (Q5: 73 percent versus 53 percent, z = 29.72, p < .001), be an assistant professor (Q10: 19 percent versus 12 percent, z = 17.81, p < .001), be tenured (Q12: 34 percent versus 28 percent, z = 8.79, p < .001) or be on the tenure track (Q12: 16 percent versus 10 percent, z = 11.68, p < .001), and not be employed outside the target institution (Q18: 72 percent versus 59 percent, z = 61.15, p < .001). CATI respondents were more likely than web respondents to report teaching as their principal activity (Q4: 77 percent versus 70 percent, z = -10.73, p < .001), be an instructor (Q10: 27 percent versus 17 percent, z = -17.49, p < .001), not be on the tenure track (Q12: 51 percent versus 41 percent, z = 28.58, p < .001), and be employed outside the target institution (Q18: 31 percent versus 22 percent, z = -14.54, p < .001). 32"}, {"section_title": "Factual items", "text": "Twenty-four factual items were chosen, based on their importance to the study objectives. These factual items were expected to show few, if any, mode differences. These questions centered on eight main topic areas: number of classes taught, year began first postsecondary job, employment sector of previous job, hours per week spent on various tasks, percent time spent on various tasks, use of various methods in the classroom, other activities, and publications. Classes taught. There were no significant differences observed in mean number of credit and noncredit classes taught at the target postsecondary institution (Q35A1 and Q35A2). Year began first postsecondary job. There was no significant difference in the mean year web respondents began their first postsecondary job (Q23) compared to their CATI counterparts. Employment sector of previous job. Web respondents were more likely to have no other job prior to their current position (Q28: 10 percent versus 5 percent, z = 12.21, p < .001) than were CATI respondents. 33 Hours per week spent on various tasks. Web respondents reported spending more time on paid tasks at the target institution (Q31A), on average, than their CATI counterparts (37 hours versus 31 hours, t = 22.70, p < .001), while CATI respondents reported spending more time on paid tasks outside the institution (Q31C) than web respondents (12 hours versus 7 hours, t = -20. 37, p < .001). No significant differences were found on hours spent on unpaid tasks at the 31 Two measures showed differences at the 3 and 4 percent level. Web respondents (57 percent) were more likely to be male (Q71) than their CATI counterparts (53 percent, z = 5.58, p < 0.001), and web respondents (7 percent) were more likely to be Asian (Q74B) than their CATI counterparts (4 percent, z = 8.44, p < 0.001). 32 Three measures showed differences at the 3 and 4 percent level. Web respondents were more likely to report administration as their primary activity (Q4: 9 percent versus 6 percent, z = 7.52, p < 0.001) and be an associate professor (Q10: 17 percent versus 13 percent, z = 7.54, p < 0.001). CATI respondents were more likely than web respondents to not report an academic title (or use the \"other\" category) (Q10: 23 percent versus 19 percent, z = -6.92, p < 0.001). 33 CATI respondents were more likely to have been employed in an elementary or secondary school prior to their current position (Q28: 20 percent versus 16 percent, z = -7.37, p < 0.001). institution (Q31B), unpaid tasks outside the institution (Q31D), or hours spent e-mailing students each week (Q41). Percentage of time spent on various tasks. Respondents were asked to provide the percentage of time they spent on undergraduate instructional activities (Q32A), graduate instructional activities (Q32B), research activities (Q32C), and other activities (Q32D). CATI respondents reported spending a greater percentage of their time each week on instructional activities with undergraduates than web respondents (61 percent versus 53 percent, t = -13.71, p < .001). 34 Use of various methods in the classroom. Of the 11 methods in question, only four showed a significant difference by mode. Compared to web respondents, CATI respondents were more likely to report using multiple choice exams (Q38A: 61 percent versus 55 percent, z = -6.64, p < .001), using essay midterm or final exams (Q38B: 61 percent versus 51 percent, z = -10.99, p < .001), and to report using service learning experiences (Q38J: 33 percent versus 26 percent, z = 25.23, p < .001). Web respondents were more likely to report using a website for instructional duties (Q39) compared to CATI respondents (45 percent versus 35 percent, z = 13.47, p < .01). 35 Publications. The average number of articles published in refereed journals in their careers (Q52AA) was no different for web and CATI respondents."}, {"section_title": "Opinion", "text": "Thirteen opinion-based questions were evaluated for mode differences. Eight of these questions asked how satisfied respondents were with various aspects of their job, including: authority to make decisions, technology-based activities, equipment/facilities, institutional support for teaching improvement, workload, salary, benefits, and job overall (Q61 and Q62). As shown in table 33, CATI respondents were significantly more likely to report being either somewhat or very satisfied with five of the eight items-including equipment/facilities, institutional support for teaching improvements, workload, salary, and job overall-compared to web respondents. 36 These differences may be due to the effect of social desirability on responses when an interviewer is involved. 34 Web respondents reported spending a greater percentage of their time each week on research (Q32C) compared to CATI respondents (15 percent versus 12 percent, t = 11.63, p < 0.001). 35 Three items showed differences at the 4 percent level. Compared to web respondents, CATI respondents were more likely to report using multiple drafts of written work (Q38E: 43 percent versus 39 percent, z = -4.48, p < 0.001), oral presentations by students (Q38F: 65 percent versus 61 percent, z = -4.52, p < 0.001), and student evaluations of each other's work (Q38H: 41 percent versus 37 percent, z = -4.52, p < 0.001). 36 Two additional questions showed significant differences at the 3 and 4 percent level. CATI respondents were more likely than web respondents to report being somewhat or very satisfied with institutional support for technology based instructional activities (Q61B: 89 percent versus 85 percent, z = -6.96, p < 0.001) and the benefits available to them (Q62C: 74 percent versus 71 percent, z = -4.15, p < 0.001). The remaining five opinion-based questions asked respondents to indicate whether they agreed or disagreed that teaching was rewarded, part-time faculty were treated fairly, female faculty were treated fairly, and racial minorities were treated fairly (Q82); and whether they would choose an academic career again (Q83). CATI respondents were more likely than web respondents to somewhat or strongly agree that good teaching was rewarded (82 percent versus 76 percent, z = -9.35, p < .001) and part-time faculty were treated fairly (75 percent versus 65 percent, z = -13.62, p < .001). 37 "}, {"section_title": "Breakoffs", "text": "A total of 27,350 sample members started the faculty interview. Of these, 800 were deemed ineligible. Of the 26,550 eligible sample members who started the interview, 26,110 completed either a full, abbreviated, 38 or partial interview. 39 An additional 10 cases either refused to be included as respondents or provided insufficient data to be useful. The remaining 430 broke off before completing the workload section (C) and were not considered to be partial completes. Table 34 lists the forms (screens) that had more than 15 breakoffs. In most cases, the forms with the highest number of breakoffs required detailed recall or requested sensitive information. 37 One additional question showed a significant difference at the 3 percent level. CATI respondents were more likely than web respondents to either somewhat or strongly agree that female faculty members are treated fairly (Q82: 91 percent versus 88 percent, z = -6.04, p < 0.001). 38 The abbreviated interview consisted of sections A (nature of employment), B (academic/professional background) and G (sociodemographic characteristics) of the faculty interview. 39 Interviews that broke off after completing section C (workload) were considered partial completes. Of the 140 respondents who did so, 48 percent broke off in the scholarly activities section (D), 9 percent in the job satisfaction section (E), 29 percent in the compensation section (F), 11 percent in the characteristics section (G), and 4 percent in the opinions section (H). "}, {"section_title": "Classification of Instructional Programs (CIP) Coding", "text": "The assisted coding system was designed for the NSOPF:04 full-scale study to decrease respondent burden by reducing the time and effort needed to code responses. The assisted coding system was used to code field of teaching, highest degree field, and principal field of scholarly activity. The codes for each of these fields were identical (see appendix J for a list of codes). Respondents were asked to provide a verbatim string. The assisted coding system parsed the string, looking for key words or phrases that matched categories in the database. If a match was located, a list of possible fields was provided for the respondent to choose from. In the event a match was not located or the respondent rejected the fields provided by the system, the respondent could manually code the field. This involved choosing a general category from the 32 categories provided in a drop-down box, and then selecting the specific category within the general category. There were a total of 136 specific categories, but within a general category there were never more than 19 specific categories to choose from. The anticipated benefit to performing this coding in the interview for web respondents is obvious; the sample member can see the categories and select the appropriate general and specific categories. For telephone-administered interviews, this real-time coding may also improve data quality by capitalizing on the availability of the respondent to clarify coding choices at the time the coding was performed; interviewers were trained to use probing techniques to assist in the coding process. The assisted coding system coded 75 percent of field of teaching strings, 79 percent of highest degree strings, and 50 percent of field of research strings. The assisted coding matches were accepted more readily by CATI interviewers than by web respondents for field of teaching and highest degree (teaching: 86 percent versus 69 percent, \u03c7 2 = 703.7, p < .0001; highest degree: 90 percent versus 74 percent, \u03c7 2 = 711.8, p < .0001) but the difference was not significant for field of scholarly activity (57 percent versus 48 percent, \u03c7 2 = 2.0, p < 0.16). As part of the data evaluation activities, a random sample of 10 percent of the results for each of the three Classification of Instructional Program (CIP) codings (teaching, research, and highest degree) was selected. An expert coder evaluated the verbatim strings for completeness and for the appropriateness of the assigned codes, determining whether a string was too vague to code or whether a different code should be assigned. Overall, 71 percent of those sampled for recoding were coded correctly, 13 percent were incorrectly coded, and 15 percent of the strings were too vague to determine whether they were correctly coded. Table 35 shows the results of the 10 percent recode, by mode. The expert coder agreed with the coding performed by the web respondent more often than that done by the CATI interviewer (\u03c7 2 = 9.69, p = 0.002). In addition to the 10 percent recode, all strings that were not coded, were partially coded (into a general area but not a specific discipline), or were coded \"other\" were evaluated by the expert coder and upcoded into the appropriate CIP categories, where possible. Of the 52,018 verbatim strings provided, a total of 1,506 strings (3 percent) qualified for this upcoding; 79 percent of these were web respondents and 21 percent were CATI respondents. Of these 1506 strings for which upcoding was attempted, 82 percent were upcoded, 18 percent were too vague to code, and less than 1 percent were correctly coded as \"other.\""}, {"section_title": "IPEDS Coding", "text": "The faculty instrument included a coding system that assisted web respondents and interviewers in collecting postsecondary institution information. This system was designed to improve data quality by allowing respondents to clarify coding choices at the time coding was performed. To assist in the coding process, web respondents were given detailed instructions on screen that enabled them to locate the postsecondary institution. In addition to these on-screen instructions, interviewers were given additional supervised training on how to effectively probe and code respondents' answers. The institution coding system assigned a six-digit IPEDS identifier for the postsecondary institution that awarded the respondent's highest degree. To facilitate coding, the coding system requested the state and city in which the school was located; from that information, a list of possible schools was displayed, allowing the respondent to select the correct school. The system relied on a look-up table of institutions constructed from the IPEDS institution database. Of the approximately 25,760 institutions coded over the course of data collection, 1,130 were initially deemed uncodeable. However, based on the information collected (institution name, location, level, and control), 1,025 institutions were positively identified and recoded during the data file editing stage of the project. Of the remaining 105 uncodeable institutions, 65 provided insufficient data, 20 were identified as closed, 10 were identified as foreign, and 10 were online institutions for which no IPEDS ID was available."}, {"section_title": "Monitoring", "text": "Regular monitoring of telephone data collection serves a number of goals, all aimed at maintaining a high level of data quality. These objectives are to identify problem items; to improve interviewer performance by reinforcing good interviewing behavior and discouraging poor behavior; to detect and prevent deliberate breaches of procedure, such as data falsification; and to assess the quality of the data collected. Two types of monitoring were performed during the NSOPF:04 data collection. The first type was monitoring by project staff, which involved listening to the interview and simultaneously viewing the progress of the interview on screen, using remote monitoring telephone and computer equipment. Project staff evaluated such things as whether the interviewer sounded professional, probed for complete answers, and handled refusal cases appropriately. Interviewers received feedback on their skills, and additional training was provided, if necessary. When monitoring interviews, project staff also evaluated whether the interview was functioning properly and identified questions in the interview that were difficult to administer so that those items could be revised in future studies. The second type of monitoring, quality assurance monitoring, was conducted by specially trained monitoring staff within the call center. Similar to project staff monitoring, the monitoring system provided for simultaneous listening and viewing of the interview. Monitors evaluated the interviewer-respondent interchange on whether the interviewer (1) delivered the question correctly and (2) keyed the appropriate response. Each of these measures was quantified and daily, weekly, and cumulative reports were produced. Monitoring took place throughout data collection, although monitoring efforts were scaled back around the 19 th week due to lighter caseloads corresponding with the end of the academic year for many schools. Of the 3,221 items monitored, a total of 28 question delivery errors and 14 data entry errors were observed. 40 This yielded an average error rate of 0.9 percent for question delivery and 0.4 percent for data entry."}, {"section_title": "Interviewer Feedback", "text": ""}, {"section_title": "Quality Circle meetings", "text": "Quality Circle meetings provided opportunities for interviewers, supervisors, and project staff to discuss data collection issues. These meetings were scheduled regularly throughout the data collection period to ensure that CATI interviews were being conducted in the most effective manner. Interviewer representation was determined by a supervisor so that all staff would have the opportunity to attend these meetings. Project staff updated interviewers and supervisors on the progress of data collection and gathered information to solve problems encountered by interviewers while conducting interviews. The minutes from these meetings were prepared by project staff and were distributed to all interviewers and supervisors. Meeting minutes were available in hardcopy and online. Examples of issues raised in Quality Circle meetings included the following. Progress of data collection. Project staff provided updates regarding the interviews completed to date and goals for the upcoming week. This information benefited both the interviewers and technical staff by recognizing interviewers' efforts and encouraging continued professionalism. CATI Case Management System (CMS) issues. Interviewers had an opportunity to report CMS issues that required project staff review and discussion. Using the information provided by interviewers, project staff resolved these issues throughout data collection. Data collection reminders. Several issues were stressed throughout data collection: reminders to verify address information for cases that needed to be remailed and for addresses for incentive checks, how to handle eligibility questions, and tips for locating sample members who are part-time employees. Interviewers were also reminded to complete problem sheets (see later section in this chapter) for any cases that needed attention."}, {"section_title": "Instrument issues.", "text": "During the Quality Circle meetings, project staff clarified specific items in the instrument for the interviewers. These items were brought to the attention of project staff in problem sheets, project staff monitoring, or during the Quality Circle meetings themselves. Discussions focused on how to properly code responses (e.g., for Q10, adjunct faculty should be coded as \"other,\" for questions expecting a numeric response, answers between zero and one should be rounded up to one). Coding. The majority of online coding during data collection was accurate, based on evaluation of verbatim strings and the codes assigned (see earlier section in this chapter on CIP coding), although in some cases the verbatim string was too vague to code. Interviewers were reminded to ask the sample member for the necessary level of detail while entering the verbatim string. Web issues. A number of web-related issues were raised during Quality Circle meetings. Some sample members reported problems connecting to the website so interviewers were asked to first try to collect the data via CATI or to have someone from the help desk assist the sample member to get connected. Interviewers were reminded to clearly state the study web address (URL) to sample members."}, {"section_title": "Problem sheets", "text": "When interviewers encountered problems during an interview, a description of the issue was documented in the form of an electronic problem sheet. Project and interviewer supervisory staff regularly reviewed these problem sheets and worked on resolving problems, as appropriate. Approximately 1,169 problem sheets were submitted during the data collection period. Problem sheets were used as follows: \u2022 To address technical CMS issues. Interviewers documented details of the front-end issues so that a programmer could resolve them. \u2022 To report system and web delays or access problems. \u2022 To document sample member contact information as a workaround for front-end issues. \u2022 To alert project staff to questions about sample member eligibility, contact information, and refusals. \u2022 To record incorrect data that were entered (but not corrected) for a case. Interviewers noted cases where project staff needed to take specific action. Project and interviewer supervisory staff ensured that issues pertinent to data collection were resolved as soon as possible."}, {"section_title": "Interviewer debriefing", "text": "A debriefing meeting was held at the end of data collection. The purpose of this meeting was to elicit feedback from the interviewers on various aspects of the data collection process, particularly the administration of the faculty questionnaire. In attendance were telephone interviewers, help desk operators and their supervisors, selected project staff, and the study project officer. The debriefing session was highly informative and gave project staff a wealth of information that will inform instrumentation and data collection activities for future studies. Project staff asked interviewers which items in the instrument were problematic. Interviewers responded with general comments as well as item-specific ones, based on their interviewing experience. General comments. Interviewers reported that sample members repeatedly indicated that parts of the questionnaire did not apply to them. Typically these respondents were part-time faculty or those who taught at community colleges, medical, or other specialty schools. Interviewers felt that that the pop-up boxes used to confirm out-of-range values were intrusive, and slowed the pace of the interview unnecessarily. They recommended that pop-up boxes be used sparingly in future web questionnaires. Question 1. Interviewers felt that the first question in the interview, which asked whether the respondent had instructional duties, was too long and \"wordy.\" They recommended that the question be shortened or broken into parts. Question 3. Interviewers reported that adjunct faculty did not know what was meant by faculty status. Question 9. The second sentence in the wording of this item (\"consider promotions in rank as part of the same job\") was confusing for respondents. Interviewers suggested restructuring the question to include that information before the respondent attempts to answer the question. Question 15. Q15 (reason for not being a member of a union) had a high rate of don't know responses. Interviewers said this was because adjunct faculty often did not know whether unions were available. Questions 16,17,and 54. Interviewers were quite pleased with the new assisted coding system for field of teaching, highest degree, and scholarly activity. It proved to be less burdensome for them, although they indicated some difficulties finding the exact categories that the respondent wanted. Question 17. Interviewers reported that IPEDS coding screens (Q17A4) were easy to use. One concern was that some schools were listed in the wrong city. Question 31. Sample members had difficulty distinguishing between paid and unpaid activities, and their ideas of each often differed from the examples provided in the instrument. Some respondents were upset at having to account for their time. Interviewers reported that respondents found this set of questions (Q31, Q32) difficult to answer as it was a lot of information to account for and difficult to break it down precisely. Questions 31,41,47B,48. Interviewers pointed out that sample members had a hard time providing answers to the hours per week questions when it is something they only do a couple of weeks out of the term (e.g., advising students). They thought some other unit of time might make it easier to collect this information. Questions 32, 37, 47. Project staff questioned whether there was any confusion over \"first-professional students.\" Interviewers indicated that some faculty at technical schools did not know what was meant by first-professional students. Question 35. Interviewers reported that sample members often were unclear what was meant by the term \"distance education\" in Q35C and suggested including the words \"Internet courses\" in the question wording. Question 37. Interviewers indicated that the screen takes a lot of time to complete and those who teach unstructured courses found these items difficult to answer. Question 50. Advising of students (Q50) was a difficult concept for some sample members in the field test and the wording was changed in the full-scale instrument to clarify the meaning. Interviewers indicated this was still a problem and suggested changing the definition provided on-screen. Respondents also wanted clarification of whether this was designated advisees only or whether it included other advising. Question 52. Interviewers reported that Q52 (number of scholarly works) was administered fairly smoothly; most respondents had a general idea of the number of publications and presentations although a few consulted their resumes. A small number of respondents had numbers of publications that exceeded the maximum allowed and became upset that their volume of scholarly activity was not properly reflected. Interviewers reported that respondents seemed to get tired around this point in the instrument and felt that combining screens for Q52A and Q52B would improve the flow and reduce burden on the respondents. Question 53. In the field test, respondents sometimes reported confusion over what was meant by scholarly activity. The question text was revised, and this problem was not reported in the full-scale questionnaire. However, some respondents were unsure whether to report only scholarly activities associated with the target institution or all scholarly activities. Question 62. In the field test, Q62C (satisfaction with benefits) was not answered by many respondents (mostly part-timers) because they did not receive benefits. The wording was slightly altered for the full-scale questionnaire; however, interviewers reported that many parttime and adjunct faculty still could not answer this question. In particular, some respondents were unsure whether the question was asking about medical benefits or other benefits. Questions 66 and 70. Sample members complained that Q66 and Q70 (income) items were intrusive. Interviewers suggested that having scripted text for why this question is asked would be helpful. Interviewers felt that income questions were unnecessarily repetitive. Question 74. Respondents insisted that \"Caucasian\" be listed among the response options (in parentheses after \"White\"). Interviewers suggested adding scripted text to explain why race is asked about on this form. Question 82. Q82D (racial minorities treated fairly) had more than 10 percent missing when administered in CATI. Interviewers explained that some part-time and adjunct faculty did not have an opinion on this set of items. They suggested adding a \"no opinion\" option for each item on this form. Interviewers who worked on the field test requested that Q84 (feedback textbox) be put back in the instrument as many sample members wanted to provide feedback."}, {"section_title": "Instrument Feedback", "text": "Two issues with the faculty instrument became apparent in the data editing process. The first issue had to do with Q1, whether the respondent had any instructional duties. Despite question wording intended to get the respondent to think beyond classroom teaching, half of the respondents who said they did not have any instructional duties provided responses indicating they did have instructional duties on other items in the instrument (i.e., taught one or more credit or noncredit classes [Q35A1>0 or Q35A2>0], provided any individual instruction [Q46=1], spent time on thesis or dissertation committees, comprehensive exams or orals committees [Q48>0], indicated that teaching was their principal activity [Q4=1], or spent time on undergraduate or graduate instructional activities [Q32A>0 or Q32B>0]). Items Q32A and Q32B contradicted Q1 most often. Rather than reconciling in the data editing phase, future cycles of NSOPF would benefit from asking follow-up questions immediately after Q1 for those respondents who said they did not have instructional duties. The other issue concerned items asking about first-professional students (Q32B, Q37E, Q47A, and Q47B). This term, first-professional student, was apparently misunderstood by many faculty and instructional staff at 2-year institutions who indicated they taught first-professional students at that institution. While on-screen examples of first-professional programs were available on some of these forms, in the future it is advised that a check against level of target institution be inserted into the instrument logic for questions concerning first-professional students."}, {"section_title": "Comparisons with NSOPF:99", "text": "To assess the consistency of survey estimates between the current and prior administrations of NSOPF, weighted estimates were obtained from the 1999 and 2004 survey data for a series of key analytical variables. The results of these assessments are summarized in table 36. 15.7 12.1 1 First-professional health science is a subset of health sciences (previous row). NOTE: Detail may not sum to totals due to rounding. Differences in estimates between NSOPF:99 and NSOPF:04 may be due to a number of factors, including actual changes over time, differences in how an item was asked between the two years (see table 8), and data editing and imputation procedures (see chapter 5). SOURCE: U.S. Department of Education, National Center for Education Statistics, 2004 National Study of Postsecondary Faculty (NSOPF:04)."}, {"section_title": "Chapter 5 Data File Development and Imputation", "text": "This chapter provides an overview of all procedures used in the development of data files, including descriptions of data editing processes, data swapping, statistical imputations, and derived variable creation."}, {"section_title": "Overview of the NSOPF:04 Data Files", "text": "Data obtained from the 2004 National Study of Postsecondary Faculty (NSOPF:04) faculty and institution questionnaires are contained in two restricted data files (faculty and institution), which are available on a CD-ROM to researchers who have applied for and received authorization from the National Center for Education Statistics (NCES) to access restricted research files. The restricted data files are documented by an Electronic Codebook (ECB), a Windows-based interface that allows users to view descriptive information and statistics about variables and to select variables for extraction into SAS or SPSS data files. The faculty and institution data files can be merged together for joint analysis. The following files were produced: Faculty data file. Provides faculty-level questionnaire data collected from 26,110 respondents. These data have been edited, swapped, and imputed. The file contains survey variables (variables that start with Q), derived variables (variables that start with X), and study weights for the faculty file (WTA00) and for the combined faculty and institution files (WTC00-or contextual weight). It also contains replicate weights for variance estimation for the faculty file (WTA01-WTA64) and for the combined faculty and institution files (WTC01-WTC64), the imputation flags (variables that start with F), and INSTID (the IPEDS ID) that will allow faculty file data to be merged with institution file data. Institution data file. Provides institution-level data collected from 920 institutions. These data have been edited, perturbed, and imputed. The file contains the institution survey variables (variables that start with the letter I), derived variables (variables that start with the letter X), study weight for the institution file (WTB00), replicate weights for variance estimation for the institution file (WTB01-WTB64), imputation flags (variables that start with the letter FI), and INSTID (the IPEDS ID) that will allow data on the institution file to be merged with data on the faculty file. The faculty and institution files can be merged together for joint analysis by performing a match merge using the variable INSTID. Please note that not all institutions that completed the institution questionnaire have responding faculty, and not all faculty have associated institution questionnaire data. For this reason, when analyzing the faculty and institution data together, responses should be weighted using the contextual weight variables on the faculty file. The NSOPF:04 institution and faculty analysis variables are presented in appendix K."}, {"section_title": "Data Coding and Editing", "text": "The NSOPF:04 data were coded and edited using procedures developed and implemented for previous NCES-sponsored studies. These coding and editing procedures were refined during the field test for use in the processing of NSOPF:04 full-scale data. A large part of the data editing and coding was performed in the data collection instruments, including range edits; across-item consistency edits; and coding of fields of teaching, scholarly activities, and highest degree. During and following data collection, the data were reviewed to confirm that the data collected reflected the intended skip-pattern relationships. At the conclusion of data collection, special codes were inserted in the database to reflect the different types of missing data. There are a number of explanations for missing data; for example, the item may not have been applicable to certain respondents or a respondent may not have known the answer to the question. Table 37 lists the set of consistency codes used to assist analysts in understanding the nature of missing data associated with the NSOPF:04 data elements. With the exception of the not applicable codes, missing data were stochastically imputed (see section 5.4). Moreover, for hierarchical analyses and developing survey estimates for faculty members corresponding to sample institutions that provided faculty lists and responded to the institution survey, contextual weights were produced for such subsets of the responding faculty members. These weights, which aggregate to a number less than the weighted total for all responding faculty and instructional staff, are named WTC00 and can be found in weights.dat on the ECB file."}, {"section_title": "Table 37. Description of missing data codes: 2004", "text": "Missing data code Description -1 Don't know; later set to missing and imputed -3 Not applicable (item was intentionally skipped) -5 Not applicable (item was asked but respondent indicated it was not applicable) -7 Item was not administered (abbreviated interview) or reached (partial interview); later imputed -9 Respondent did not provide an answer; later imputed SOURCE: U.S. Department of Education, National Center for Education Statistics, 2004 National Study of Postsecondary Faculty (NSOPF:04). The data cleaning and editing process for NSOPF:04 consisted of the following steps: Step 1. Review of one-way frequencies for every variable to confirm no missing or blank values and to check for reasonableness of values. This involved replacing blank or missing data with -9 for all variables in the instrument database and examining frequencies for reasonableness of data values. Step 2. Review of two-way cross-tabulations between each gate-nest 41 combination of variables to check data consistency. Legitimate skips were identified using the interview programming code as specifications to define all gate-nest relationships and replace -9 (missing values that were blank because of legitimate skips) with -3 (legitimate skip code). Additional checks ensured that the legitimate skip code 41 Gate variables are items that determine subsequent instrument routing. Nest variables are items that are asked or not asked, depending on the response to the gate question. For example, in the faculty questionnaire, Q1 (which asks whether the respondent had instructional duties) determines whether Q2 (which asks whether the respondent's instructional duties were related to credit courses/activities) is asked. Q1 is a gate item and Q2 is a nested item. Q2 is only asked if the response to Q1 was \"yes.\" was not overwriting valid data and that no skip logic was missed. In addition, if a gate variable was missing (-9), then the -9 was carried through the nested items. Step 3. Identify and code items that were not administered due to a partial or abbreviated interview. This code replaced -9 values with -7 (item not administered) based on the section completion and abbreviated interview indicators. Step 4. Recode \"don't know\" responses to missing. This code replaced -1 (don't know) values with -9 (missing) for later stochastic imputation. For selected items for which \"don't know\" seemed like a reasonable response, variables were created both with and without the \"don't know\" category. Step 5. Identify items requiring recoding. During this stage, previously uncodeable values (e.g., text strings) collected in the various coding systems were upcoded, if possible (see sections 4.3.3 CIP coding and 4.3.4 IPEDS coding). Step 6. Identify items requiring range edits, logical imputations, and data corrections. Descriptive statistics for all continuous variables were examined. Values determined to be out-of-range were either coded to the maximum (or minimum) reasonable value or set to missing for later imputation. Logical imputations were implemented to assign values to legitimately skipped items whose values could be implicitly determined from other information provided. Data corrections were performed where there were inconsistencies between responses given by the sample member. Concurrent with the data cleaning process, detailed documentation was developed to describe question text, response options, recoding, range edits, logical imputations, data corrections, and the \"applies to\" text for each delivered variable."}, {"section_title": "Data Perturbation", "text": "A restricted faculty-level data file was created for release to individuals who apply for and meet standards for such data releases. While this file does not include personally identifying information (i.e., name and Social Security number), other data (i.e., institution, Integrated Postsecondary Education Data System [IPEDS] ID, demographic information, and salary data) may be manipulated in such a way to seem to identify data records corresponding to a particular faculty member. To protect further against such situations, some of the variable values were swapped between faculty respondents. This procedure perturbed and added additional uncertainty to the data. Thus, associations made among variable values to identify a faculty respondent may be based on the original or edited, imputed and/or swapped data. For the same reasons, the data from the institution questionnaire were also swapped to avoid data disclosure."}, {"section_title": "Imputation Methodology", "text": "The NSOPF:04 data files include institution-level and faculty-level data obtained from the institution and faculty surveys. All non-verbatim and non-text variables on the NSOPF:04 that had missing variables have been imputed. Specifically, a total of 144 variables were stochastically imputed for the faculty data, and 87 variables were stochastically imputed for the institution data. All remaining missing data were deemed not suitable for imputation, such as the postsecondary institution that awarded the highest degree of a faculty respondent. Most of these variables were imputed using a weighted sequential hot-deck imputation procedure. A number of variables, including gender and race/ethnicity, were imputed using a combination of cold-deck and logical imputation during the data editing process before the data file was considered ready for stochastic imputation. The specific imputation method used for each variable is specified in the imputation flags on the final restricted datasets. Table 38 shows the number of variables that were imputed based on the percent missing (imputed) for faculty and institution survey data. Accordingly, data for 26 of the 144 faculty variables were imputed for less than 1 percent of all faculty respondents, whereas data for 7 of the faculty variables were imputed for more than 15 percent of the faculty respondents. "}, {"section_title": "Imputation Methods", "text": "In broad terms, there are three methods of imputation: logical, cold-deck, and hot-deck imputation. Logical imputation is a process that aims to infer or deduce the missing values from answers to other questions. Cold-deck imputation involves replacing the missing values with data from sources such as data used for sampling frame construction. While resource intensive, these methods often obtain the actual value that is missing. Consequently, attempts were made to fill in the missing values of data using these two methodologies, to the extent possible. In contrast, stochastic imputation methods, such as sequential hot-deck imputation, rely on the observed data to provide replacing values (donors) for records with missing values. Sequential hot-deck imputation involves defining imputation classes, which generally consist of a cross-classification of covariates, and then replacing missing values sequentially from a single pass through the survey data within the imputation classes. When this form of imputation is performed using the sampling weights, the procedure is called weighted sequential hot-deck imputation. This procedure takes into account the unequal probabilities of selection in the original sample to specify the expected number of times a particular respondent's answer will be used as a donor. These expected selection frequencies are specified so that, over repeated applications of the algorithm, the weighted distribution of all values for that variable-imputed and observed-will resemble that of the target universe. Under this methodology, while each respondent record has a chance to be selected for use as a hot-deck donor, the number of times a respondent record can be used for imputation will be controlled To implement the weighted sequential hot-deck procedure, imputation classes and sorting variables that are relevant (strong predictor) for each item being imputed were defined. For this study, imputation classes were developed by using a Chi-squared Automatic Interaction Detection (CHAID) analysis. The CHAID segmentation process divides the data into groups based on the most significant predictor of the item being imputed. Subsequently, this procedure will be repeated using the remaining predictor variables to split each of the emerging groups into smaller subgroups. In this process, a number of subgroups created during a previous iteration might get merged back to form new subgroups. This splitting and merging process continues until no more statistically significant predictors are found, at which point imputation classes are defined from the resulting segments. When dealing with categorical variables, the CHAID process may merge certain categories of such variables that are found not to be significantly different. Similarly, continuous variables are categorized to create the strongest categorical predictors of the item in question. Using RTI's sequential hot-deck method of imputation, once imputation classes are constructed, items within each class are sorted before the process of donor selection begins. If more than one sorting variable is chosen, a serpentine sort will be performed where the direction of the sort (ascending or descending) changes each time the value of a variable changes. The serpentine sort minimizes the change in the respondent's characteristics every time one of the variables changes its value. It should be noted that, for this study, distinction was made between legitimate and nonlegitimate missing items for imputation. All responses that were left missing as a result of refusal were set to missing and then imputed. Additionally, if the interview was terminated early and some questions were not asked of the respondent, then the value of missing was assigned in those cases as well. However, respondents could legitimately skip questions that did not apply to them. In these cases, the missing responses were coded as legitimate skips (-3) and were not imputed."}, {"section_title": "Imputation of Faculty Data", "text": "Item imputation for the faculty questionnaire was performed in several steps. In the first step, the missing values of gender, race, and ethnicity were filled-using cold-deck imputationbased on the sampling frame information or institution record data. These three key demographic variables were imputed prior to any other variables since they were used as key predictors for all other variables on the data file. After all logical and cold-deck imputation procedures were performed, the remaining variables were imputed using the weighted sequential hot-deck method. Initially, variables were separated into two groups: unconditional and conditional variables. The first group (unconditional) consisted of variables that applied to all respondents, while the second group (conditional) consisted of variables that applied to only a subset of the respondents. That is, conditional variables were subject to \"gate\" questions. After this initial grouping, these groups were divided into finer subgroups as detailed next. The unconditional group was divided into two subgroups based on the percent of missing values: less than 1 percent versus greater than 1 percent missing. The conditional variables were divided into three subgroups based on the level of conditionality where this level was essentially determined by the sequence of the questionnaire. For variables in the conditional group, the questionnaire skip patterns were reviewed and variables were grouped according to which variables determine the values of other variables. After these subgroups were constructed, missing values of the variables were imputed in order from lowest percent missing to highest percent missing within each subgroup, first for the unconditional variables and then for the conditional variables in an ascending level of their conditionality. All unconditional variables that had less than one percent missing were imputed using imputation classes defined by a combination of gender, race, and ethnicity. Moreover, institution type, 42 institution size, and faculty type 43 were used as sort variables to place like records in closer proximity to improve the donor selection process. The imputation classes for the remaining unconditional variables (that had more than one percent missing) and all conditional variables were determined by a CHAID analysis based on key demographic variables that were logically imputed and all imputed variables that had less than one percent missing. After all variables were imputed, consistency checks were applied to the entire faculty data file to ensure that the imputed values did not conflict with other questionnaire items, observed or imputed. This process involved reviewing all of the logical imputation and editing rules as well."}, {"section_title": "Imputation of Institution Data", "text": "The imputation process for the missing data from the institution questionnaire involved similar steps to those used for imputation of the faculty data. The missing data for variables were imputed using the weighted sequential hot-deck method. Analogous to the imputation process for the faculty data, the variables were partitioned into conditional and unconditional groups. The unconditional variables were sorted by percent missing and then imputed in the order from the lowest percent missing to the highest. The conditional group was partitioned into three subgroups based on the level of conditionality for each variable, and then imputed in that order. The imputation class for both unconditional and conditional variables consisted of the institution sampling stratum, and the sorting variables included the number of full-time and part-time faculty members."}, {"section_title": "Evaluation of Imputations", "text": "A common measure for determining whether an imputation method produces acceptable results (donors) is based on the similarity of the before and after imputation distributions within imputation classes. For evaluation of the imputation results, distributions were considered to be similar when absolute differences were less than 5 percent, where the absolute difference was calculated by comparing the before and after imputation weighted frequencies. If absolute differences were greater than 5 percent, the unweighted distributions were examined to see if the large differences were due to small imputation cells. When possible, such cases were evaluated and resolved by collapsing neighboring imputation classes. The before and after imputation distributions of several key variables are presented in table 39 for the faculty data and table 40 for the institution data. For more information regarding the bias due to item nonresponse, refer to appendix I.  "}, {"section_title": "Derived Variables", "text": "For NSOPF:04, a total of 45 institution-level and 130 faculty-level derived variables were constructed to simplify access to standard queries useful to analysts, as well as to enhance substantive analysis. Since research questions often require independent or control variables, this set of derived variables was added to the faculty data files. The 45 institution-level derived variables were also added to the institution data files. Multiple sources of data were used to create institution-derived variables, including selected 2000-01 and 1997-98 IPEDS surveys, the Carnegie classification system, and NSOPF:04 sampling information. "}, {"section_title": "Chapter 6 Weighting and Variance Estimation", "text": "Three sets of analysis weights were calculated for this administration of the 2004 National Study of Postsecondary Faculty (NSOPF:04), details of which are provided in this section. First, a set of analysis weights was calculated for institutions responding to the institution survey. Next, analysis weights were constructed for responding faculty, which reflected the selection probabilities of institutions providing faculty lists and selection of faculty members within sample institutions. In addition, a set of contextual weights was calculated to use when linking faculty and institution survey data. These analysis weights were constructed as the product of corresponding sampling weights and adjustment factors for frame multiplicity, nonresponse, and poststratification to known control totals. As detailed in the following sections, each component of the final analysis weights represents either the inverse of a selection probability or a weight adjustment to reduce bias. The institution analysis weights were computed as the product of the following five weight components and adjustment factors: (1) institution sampling weight (WT1); (2) institution multiplicity adjustment factor (WT2); (3) institution nonresponse adjustment factor (WT3); (4) institution poststratification adjustment factor (WT4); and (5) institution ratio adjustment factor (WT5). In order to compute the analysis weights for faculty, first a set of primary sampling unit (PSU) weights were created for institutions providing faculty lists. These interim weights, which are of no analytical utility, were only used as component weights for construction of the final analysis weights for faculty members. Ultimately, the faculty analysis weights were computed as the product of the following nine weight components and adjustment factors: (1) institution sampling weight (WT1); (2) institution multiplicity adjustment factor (WT2); (3) institution nonresponse adjustment (WT3) 44 ; (4) institution poststratification adjustment factor (WT4); (5) faculty sampling weight (WT5); (6) faculty multiplicity adjustment factor (WT6); (7) faculty unknown eligibility adjustment factor (WT7); (8) faculty nonresponse adjustment factor (WT8); and (9) faculty poststratification adjustment factor (WT9). Analogous to the calculation of analysis weights for the faculty, a set of contextual weights was constructed for the subset of faculty for whom their corresponding institutions had responded to the institution survey. Table 41 summarizes the distribution of institutions providing faculty lists and responding to the institution questionnaire by sampling strata. RTI's weighting software GEM (Generalized Exponential Modeling)  has been used for calculation of all weight adjustment factors. Taking advantage of an iterative proportional fitting algorithm and the logit method, GEM provides a comprehensive weighting program that can utilize a large number of predictor variables for creating a more balanced set of weights while automatically curtailing extreme weights that can reduce the efficiency of weighted estimates. For more details on the GEM adjustment procedure, see appendix L. This section provides details of steps taken to construct the resulting weights."}, {"section_title": "Institution Weights", "text": "The institution sampling frame for the NSOPF:04 included a total of 3,380 eligible units, detailed composition of which is provided in section 2.1.1. Reflecting the probability proportional to size scheme of sample selection, the probability of selection for institution i in stratum r was calculated by: The initial sample consisted of 1,220 institutions. However, this sample was reduced to a subsample of institutions, since a smaller sample was deemed adequate to secure all precision requirements of NSOPF:04. Therefore, the sampling weight for institution i in stratum r was calculated as a function of its initial and subsequent selection probabilities. With R r representing the subsampling rate in stratum r, the sampling weight for the i-th institution in that stratum was calculated by: It should be noted that during the sample refreshing step, institutions were added to the sample of institutions, resulting in total sample of 1,080 institutions for NSOPF:04."}, {"section_title": "Adjustment for Institution Multiplicity", "text": "During the institution recruitment and faculty list sampling stages, a number of institutions were identified that had two or more records listed on the Integrated Postsecondary Education Data System (IPEDS). In some cases this was caused by institutions that had recently merged, while in other cases the sample institution had sent a single faculty list covering multiple campuses. For sampling purposes, combined faculty lists that could not be separated were treated as merged institutions and identified under a single IPEDS ID for purposes of tracking survey results. For institutions with more than one chance of selection, a multiplicity adjustment factor was calculated by estimating, as if the selections were independent, the probability that each record could be selected. Consequently, when an institution had n chances of selection, its probability of selection was calculated by: Next, a multiplicity adjustment factor for the i-th sample institution was calculated by: If the given institution did not require such adjustment, its multiplicity adjustment factor was set to unity. This way, the product of WT1 and WT2 equals the reciprocal of the resulting multiple chance of selection for the institutions with positive multiplicity, and equals WT1 for all other institutions."}, {"section_title": "Nonresponse Adjustment", "text": "For calculating the analysis weights for institutions responding to the institution questionnaire, an institution (questionnaire) level nonresponse adjustment factor (WT3) was constructed using the product of the institution sampling weights (adjusted for multiplicity) and the faculty counts from the frame. 45 For this purpose, the institutional respondent definition provided in section 3.1.1 was used to identify the institution subset belonging to the denominator of this adjustment factor. The resulting adjustment factors, which were calculated using GEM within cells defined by the 10 sampling strata and region, aimed to reduce or eliminate nonresponse bias in survey estimates. Construction of the nonresponse adjustment cells was based on variables that were deemed to be predictive of response status and available for both respondents and nonrespondents."}, {"section_title": "Poststratification Adjustment", "text": "A set of poststratification adjustment factors (WT4) was calculated for the 920 institutions responding to the questionnaire using the GEM program. Specifically, nonresponse-adjusted weights for these institutions were ratio-adjusted to the counts of institutions obtained from the sampling frame. Moreover, an additional adjustment factor was calculated to ensure that weighted counts of faculty obtained from the institution survey data would coincide with those obtained from the faculty survey data. As detailed in the next section, the final analysis weights for faculty included ratio adjustments to counts of faculty obtained from the Employees by Assigned Position Survey (EAP) conducted in the Winter 2003-04 IPEDS data collection cycle. In order to achieve the needed concurrence between the weighted estimates obtained from the institution and faculty surveys, the poststratified weights of the 920 institutions were ratio adjusted to the corresponding weighted totals from the faculty data. With this last adjustment factor computed (WT5), the final analysis weight for each responding institution (WTB00) was calculated by:"}, {"section_title": "Faculty Weights", "text": "The final analysis weights for faculty were constructed as the product of the final institution weights for the 980 institutions that provided faculty lists (PSU weights), inverse of selection probabilities for faculty, and a series of adjustment factors at the faculty level. The needed PSU level weights, which are different from those calculated above for the 920 institutions responding to the institution questionnaire, were calculated by calibrating the product of the institution sampling weights (adjusted for multiplicity) and the faculty frame counts to the institution counts within each of the sampling strata. Note that since a minimum weighted response rate of 85 percent was secured overall and within each of the sampling strata for institutions providing faculty lists, a nonresponse adjustment factor was not calculated for these institution. Operationally, these institutions were assigned a nonresponse adjustment factor of unity, i.e., WT3 = 1."}, {"section_title": "Selection Probability for Faculty", "text": "The overall faculty sampling strata were defined as the institution sampling strata crossed with the faculty strata within institutions. The sample faculty members were systematically selected from the faculty lists at institution-specific rates that were inversely proportional to the institution's probability of selection, as dictated by the sample design. That is, the overall stratum sampling rate divided by the institution's probability of selection: where f s represented the overall faculty sampling rate, and \u03c0 ri represented the institution's probability of selection. The sampling weights (WT5) for each of the 35,630 sample faculty members were calculated as the reciprocal of the above institution-specific faculty sampling rates."}, {"section_title": "Adjustment for Faculty Multiplicity", "text": "Faculty members who worked at more than one eligible institution during the 2003-04 academic year had multiple chances of being selected, since they could have been selected from any of the eligible institutions they attended. When this was the case, the resulting multiplicity was adjusted for by dividing the sampling weight of the given faculty by the number of institutions he/she worked at that were eligible for sample selection. Specifically, the faculty multiplicity weight adjustment factor was defined as WT6 = 1/M, where M is the multiplicity or number of institutions attended by sample faculty, based on the interview data."}, {"section_title": "Adjustment for Unknown Eligibility Status", "text": "For nonresponding faculty members whom project staff were unable to contact, the final eligibility status could not be determined. These faculty members were treated as eligible, and their weights were adjusted to compensate for the small portion of faculty members who were actually ineligible. These weight adjustment factors (WT7), which were calculated within cells defined by a cross-classification of institution and faculty types, represented the estimated eligibility rates among faculty members with known eligibility status. For faculty members known to be eligible the weight adjustment factor was set to one."}, {"section_title": "Nonresponse Adjustments", "text": "As reported earlier, faculty-level response rates were less than 85 percent, both overall and within a number of sampling strata. Subsequent to a nonresponse bias analysis, details of which are provided in appendix I, adjustment factors were calculated within cells indexed by a cross-classification of the faculty and institution strata and length of time to respond. Again, the weighting program, GEM, was used to create the needed nonresponse adjustment factor (WT8) for each of the 26,110 responding faculty members."}, {"section_title": "Poststratification/Raking Adjustment", "text": "To ensure population coverage, nonresponse adjusted weights were further adjusted to match published faculty totals. Specifically, these weights were raked along two dimensions to control totals that were constructed using the Winter 2003-04 Employees by Assigned Position Survey (EAP:03). This source was used to obtain the total number of full-and part-time faculty members by institution type. Moreover, the NSOPF:04 sampling frame was used to generate the distribution of faculty members by race/ethnicity and gender, detailed construction of which is provided in section 2.1.4 and appendix A. The resulting two raking dimensions are summarized in tables 42 and 43. The raking adjustment factors (WT9) were calculated using GEM.  Finally, eligibility definitions for NSOPF:04 include non-faculty who provide instruction but do not have teaching as their principal activity. Since there were no published counts for such faculty members that could be used for weighting, nonresponse-adjusted weights were used to develop an estimate for this small subgroup. For this purpose, all emerging 560 non-faculty members retained their nonresponse adjusted weights-totaling to 26,803-as their final weights and were not included during the above raking process. 46 That is, a value of one was assigned to the raking adjustment factor for these respondents when calculating their final analysis weights. In general, however, the final analysis weights (WTA00) were computed as the product of the institution and the faculty component weights by:"}, {"section_title": "Variance Estimation", "text": "The 2004 National Study of Postsecondary Faculty (NSOPF:04) sampling design was a stratified two-stage design. A stratified sample of postsecondary institutions was selected with probabilities proportional to a composite measure of size at the first stage, and a stratified systematic sample of faculty and instructional staff was selected from sample institutions providing lists at the second stage. Because of this complex sampling design, statistical analyses should be conducted using software packages that properly account for the employed survey design through use of survey weights. Most commonly used statistical procedures assume that data are obtained from a simple random sample; that is, that the observations are independent and identically distributed. When the data have been collected using a complex sampling design, the simple random sampling assumption usually leads to underestimating the sampling variance, which would lead to artificially narrow confidence intervals and liberal hypothesis test results; that is to say, rejecting the null hypothesis when it is true more often than indicated by the nominal Type I error level. (Carlson et al. 1993). Statistical strategies that have been developed to address this issue include: first-order Taylor series expansion of the variance equation; balanced repeated replication; and the Jackknife approach (Wolter 1985). Software packages that have been developed for analyzing complex sample survey data include SUDAAN, WesVar, Stata, and SAS. SUDAAN is a commercial product developed by RTI. Further information can be obtained from the website http://www.rti.org/sudaan. WesVar is a product of Westat, Inc., for which additional information can be obtained from the website http://www.westat.com/wesvar. Stata is a product of StataCorp LP; additional information about Stata can be found at the following website: http://www.stata.com. SAS information may be found on the SAS corporate website: http://www.sas.com. Also, the National Center for Education Statistics (NCES) has developed a software tool called the Data Analysis System (DAS) for analysis of complex survey data. Information about DAS is available from the website http://nces.ed.gov/das. The variance estimation strategy chosen for NSOPF:04 has aimed to satisfy the following requirements and design features: \u2022 variance reduction due to stratification at all stages of sampling; \u2022 unequal weighting effects due to nonresponse adjustment and poststratification; \u2022 variance inflation due to clustering; \u2022 estimation of linear and nonlinear statistics such as quantiles; and \u2022 variance reduction due to finite population corrections at the PSU (institution) stage of sampling and the high sampling rates in certain strata. Commonly applied bootstrap variance estimation techniques satisfy the first four requirements. To meet the last requirement, however, the methodology developed by Kaufman was applied (Kaufman 2004). This methodology incorporates the finite population correction factors at both stages of sampling. However, for NSOPF:04, application of this method reflected the finite population correction factor at the first stage only where sampling fractions were often high. At the second stage, where the sampling fractions were generally low, the finite population correction factor was set to 1.00. The Kaufman methodology was used to develop a vector of 64 bootstrap sample weights that are included on the analysis file, along with the full sample analysis weights. Replicate weights were set to zero for units not selected in a particular bootstrap sample while weights for other units were inflated for the bootstrap subsampling. Note that analogous to the full sample weights, these replicate weights were also poststratified to the same set of control totals for calibration. The number of replicate weights was set at 64 based on an empirical investigation of the behavior of variance estimates as the number of replicates increased. This investigation showed that the stability of variance estimates improved with increasing numbers of replicates and became fairly stable for most estimates when between 50 and 55 replicate weights were used. Also, a similar process of generating replicate weights was used for the institution file except that all procedures relating to the second stage of sampling were omitted. The vector of B replicate weights allows for computing additional estimates for the sole purpose of estimating a variance. With the 64 sets of replicate weights, the variance of any statistic,\u03b8\u02c6, can be estimated by separately calculating the statistic of interest from each replicate and then using the variability among the resulting estimates to calculate the variance of the given statistic by: Once the replicate weights are provided, this estimate can be produced by survey software packages such as SUDAAN, STATA, and WesVar. Here, the analyst should specify the full study and replicate weights appropriate for the given analysis. In this case, the analyst should specify the full study and replicate weights, which are appropriate for the given analysis. Below is an example of a generic SUDAAN code for producing point estimates and their associated standard errors using replicate weights that reflect the reduction in variance due to finite population correction (fpc) at the institution stage of sampling. The symbols /* and */ in the code indicate the beginning and end of a comment. Note that the dataset does not need to be sorted. Again, it should be noted that there are three sets of study (analysis) weights and their corresponding replicate weights. These weights are: For each of the three types of analyses-institution, faculty, and contextual (merged)specific design variables, which are generically named analysis stratum and analysis PSU in the above code, need to be identified. These variables are available on corresponding final datasets as described below. \u2022  Table 44 provides estimates of design effects for selected faculty data. These estimates, which consist of the ratio of variance of estimates under the employed design and simple random sampling (DEFF) 48 and the square root of this ratio (DEFT), 49 are typically used as measures for the efficiency of a sample design. The larger the design effect, the larger the variance of the estimate relative to what would have been obtained under simple random sampling where all units have the same chance of selection."}, {"section_title": "Design Effects and Standard Errors", "text": "The standard errors were calculated using SUDAAN with the replicate weights that were calculated for these data, details of which are provided section 6.3. The average design effect for the listed key faculty estimates in table 44 was 1.88. Briefly, this indicates that due to differential sampling and weight adjustments, the resulting sample is 1.88 times less effective as compared to a simple random sample with 100 percent response rate. That is, the original sample size should be divided by 1.88 to obtain the effective sample size under the employed design. More detailed tables are available in appendix M. "}, {"section_title": "A-3", "text": ""}, {"section_title": "A.1 Institution Frame Construction", "text": "The institution sample selection has been based on a probability proportional to size (PPS) selection methodology, where each institution was assigned a composite measure of size (MOS) that reflected the number of eligible faculty and instructional staff in each of the following six hierarchical strata in the given order of inclusion: 1 \u2022 Hispanic; \u2022 non-Hispanic Black; \u2022 Asian and Pacific Islander; \u2022 full-time female; \u2022 full-time male; and \u2022 all other. Faculty counts needed for MOS calculations were initially obtained from the Fall Staff Survey Component of the Winter 2001-2002 Integrated Postsecondary Education Data System (IPEDS) Data Collection (Winter:02 IPEDS). However, this source could not provide all of the information necessary to classify faculty members into one of the above sampling strata. For instance, faculty counts were not reported in a number of institutions, while for others, reported counts were not indexed by race and ethnicity. As a result, the missing information had to be imputed in two steps. As detailed in the next section, the first step consisted of imputing unreported (missing) faculty counts, while in the second step, faculty reported as having unknown race/ethnicity or as nonresident aliens were distributed among the known race categories using a special procedure. Subsequent to these two steps, faculty members in each institution were classified into one of the six sampling strata."}, {"section_title": "A.1.1 Imputation of Missing Faculty Counts", "text": "As summarized in table A-1, starting with the 3,379 eligible institutions in the NSOPF:04 universe, the Winter:02 IPEDS provided faculty counts for 3,148 institutions, including counts of faculty with unknown race/ethnicity and those listed as nonresident aliens. 2 Of these 3,148 institutions, 59 were main campuses (parents) that reported to IPEDS the total faculty at the main campus as well as those for their branch (child) campuses. Among the 231 institutions that had no reported faculty counts, 80 were children of the 59 parent campuses and the remaining 151 were campuses without any reported faculty counts. The IPEDS allows institutions to provide combined faculty counts for themselves and their branch campuses. The unreported faculty counts for the 80 child campuses were reallocated from their parents according to the following steps. Here, all child institutions corresponding to a parent institution were included, even if such institutions were not eligible for NSOPF:04. \u2022 For the 75 child institutions whose parents had a Carnegie code-accredited, degreegranting colleges and universities-the total count of faculty for each faculty group was reallocated from the parent institutions such that the parent retained twice as many faculty members as each of the children for the given group. This is the procedure followed for IPEDS. \u2022 For the remaining five child institutions whose parents did not have a Carnegie code, the total count of faculty for each group was allocated equally between the parent and its child. There were 151 institutions in the NSOPF universe that were not eligible for IPEDS imputation 3 and had no reported faculty counts in the Winter:02 IPEDS. In order to calculate MOS for such institutions, missing counts of faculty members were imputed using a methodology similar to that used to impute all IPEDS data. Specifically, the following steps were taken: \u2022 If data were available from the Fall Staff Survey (IPEDS-S:97 or -S:99), these data were used without any adjustments, with preference given to the more recent data. \u2022 If data were not available from either the IPEDS-S:97 or -S:99, faculty counts were imputed as a function of student counts according to the following steps: \u2212 Using the IPEDS 2000 Fall Enrollment dataset, for each institution the full-time equivalent (FTE) for students was calculated using the following formula: \u2212 Each institution was assigned to an imputation group based on institution type and within each group the institution with the closest FTE was selected as the donor institution. Subsequently, the missing faculty counts for each subgroup were imputed using the following ratio estimator, in which the function \"Integer\" indicates the integer part of the resulting number:"}, {"section_title": "A.1.2 Imputation of Missing Faculty Stratification Information", "text": "The majority of the 3,379 NSOPF:04 institutions included faculty members whose race and ethnicity were reported as nonresident alien or as unknown. These race/ethnicity categories had to be reconciled so that such faculty could be allocated to the six sampling strata. As detailed next, this process was carried out separately for faculty with reported unknown race/ethnicity and for faculty who are reported as nonresident aliens. A large number of institutions included faculty for whom race/ethnicity was reported as unknown. These faculty were assigned to known race categories available from IPEDS (non-Hispanic Black, Hispanic, Asian/Pacific Islander, American Indian/Alaska Native, and White) using the following steps. \u2022 For each institution, the percentage of faculty with known race/ethnicity was obtained for groups indexed by gender and employment status (part-time and full-time). \u2022 When the reported race/ethnicity counts of faculty within a gender/employment group were at least 50 percent of the total faculty count for that group (including those with unknown race/ethnicity and nonresident aliens), the count of faculty with unknown race/ethnicity was distributed to each of the race categories in proportion to the reported counts within the gender/employment group. \u2022 Conversely, when the reported race/ethnicity counts of faculty within a gender/employment group were less than 50 percent of the total faculty count, faculty with unknown race/ethnicity were distributed to each of the race categories in proportion to the average distribution of race for that gender/employment group within classes indexed by level of institution (2-and 4-year) and region. Average group distributions were constructed from those institutions with more than 50 percent of race/ethnicity of their faculty reported in the five categories. Tables A-2 through A-5 summarize the resulting average distributions and other statistics for each of the 16 institution groups within the four gender/employment groups. In addition to faculty with unknown race/ethnicity, many institutions included counts of nonresident alien faculty for whom race/ethnicity was not reported. In such cases, counts of nonresident alien faculty members were distributed among the known race/ethnicity groups within groups indexed by gender and employment status. For this purpose, the needed distributions for nonresident A-6 aliens were obtained from the 1990 through 1999 Survey of Earned Doctorates (SED) data, 4 since the distribution of reported race/ethnicity based on IPEDS is not representative of nonresident alien faculty members.  Accordingly, in the first region there are eight 4-year or above institutions at which race/ethnicity was known for less than 50 percent of full-time male faculty members, while there were 185 institutions at which race/ethnicity was known for more than 50 percent of full-time male faculty. The corresponding numbers for at least 2-but less than 4-year institution are 4 and 45, respectively in region 1.      Using data from the prior 10 years of SED, counts of doctorate recipients with temporary resident status were obtained to construct surrogate distributions of race/ethnicity for nonresident alien faculty members. Specifically, the average race/ethnicity distributions of these individuals were calculated within groups indexed by gender, type of institution (public and private), and region. The appropriate distribution(s) were applied to the number of nonresident aliens in each institution to allocate such counts to one of the known race/ethnicity categories. Tables A-6 and A-7 provide a summary of the resulting average distributions and other statistics for nonresident aliens for each gender.     "}, {"section_title": "A-7", "text": ""}, {"section_title": "A-8", "text": ""}, {"section_title": "A.2 Institution Sample Selection", "text": "For sampling purposes, 10 institution strata were defined for the NSOPF:04 based on Carnegie classification codes and control. Since the institution sample for the faculty study was selected jointly with that for students, the 10 NSOPF sampling strata were collapsed from the related student-based 58 strata (STRAT58) for the 2004 National Study of Faculty and Students (NSoFaS:04). Table A-8 summarizes the distribution of the resulting sample of institutions by stratum for NSOPF:04. Table A-9 provides a crosswalk between the two sets of strata. Institution sample sizes and their corresponding sampling rates were established using a customized cost/variance optimization procedure, which aimed to identify the allocation that would accommodate all analytical objectives of this survey while minimizing data collection costs. "}, {"section_title": "A.3 Faculty Frame Construction", "text": "All sampled institutions were contacted to provide lists of faculty and instructional staff who were eligible for NSOPF:04. For this purpose, each institution was requested to provide a complete list of full-and part-time faculty and instructional staff as of November 1, 2003 (or during the fall term of the 2003-04 academic year)."}, {"section_title": "A.3.1 List Request and Requirements", "text": "Each institution was given several options for providing faculty lists, including uploading an electronic copy of the list to a secure website, sending the list as an e-mail attachment, or mailing the list on diskette using the provided shipment material. It was requested that files containing the faculty lists follow a specific layout. Acceptable file formats included ASCII fixed field, ASCII comma-delimited, or Excel spreadsheet. For those institutions not capable of providing electronic lists of faculty and instructional staff, paper lists were suggested as a last choice of format. In addition to campus and home contact information, institutions were asked to provide basic demographic information such as gender, race/ethnicity, academic field, and employment status of each faculty member. Towards the end of the list collection period, online course catalogs and institution websites were used to abstract lists for those sampled institutions that had failed to provide faculty lists, yet had online sources that could provide adequate information about their faculty and instructional staff. In all, a total of 139 such lists were abstracted to supplement the other lists that had been supplied by sample institutions. Online resources were approved for abstraction based on the completeness and inclusiveness of the information provided."}, {"section_title": "A.4 Faculty Sample Selection", "text": "The sample of faculty was selected using stratified systematic sampling within cells indexed by institutional and faculty strata as summarized in tables A-9 and A-10. Table A-9 presents the complete list of institutional strata used for NSoFaS, and indicates their correspondence to the strata used for NSOPF. Moreover, institution eligibility rates from the prior administrations of NSOPF were available only at a different level of aggregation (sector), a listing of which is provided in table A-11. Table A-12 provides a summary of faculty counts by NSoFaS institutional and NSOPF faculty strata, which were used for sample allocation based on the Winter:02 IPEDS. Stratum counts that are zero correspond to those that are specific to NPSAS:04. Private not-for-profit less-than-4-year other -degree-granting 9 13 Private not-for-profit less-than-4-year other -NPSAS only NPSAS only 14 Private not-for-profit 4-year master's 7 15 Private not-for-profit 4-year bachelor's 8"}, {"section_title": "16", "text": "Private not-for-profit 4-year other 8,9,10 17 Private not-for-profit 4-year doctoral 6 18 Private not-for-profit 4-year doctoral master's 7 19 Private not-for-profit 4-year doctoral other 7,8,10 20 Private not-for-profit 4-year NPSAS only NPSAS only"}, {"section_title": "21", "text": "Private for-profit less-than-2-year NPSAS only 22 Private for-profit 2-year or more NPSAS only 23 CA Public 2-year 4 24 CA Public 4-year 1,2,3,5 25 CA Private not-for-profit 4-year 6,7,8,10 See notes at end of table.     "}, {"section_title": "A-12", "text": ""}, {"section_title": "A.4.1 Determining Initial Faculty Sample Sizes and Sample Allocation", "text": "This section provides an overview of the faculty sample selection procedure, which includes technical details of the cost/variance optimization process for selection of the initial sample sizes as well as specifications for calculation of initial and final (adjusted) sampling rates. A customized cost/variance optimization program was developed to determine the desired allocation of respondents to institution-by-person strata, which aimed to secure at least the same level of precision for key estimates as those achieved during the previous administration of the survey. This optimization process consisted of the following steps: a. establishing precision requirements for key estimates; b. constructing a cost model specific to the structure of the sample; c. developing a relative variance model; and d. determining the optimum sample allocation."}, {"section_title": "A.4.2 Precision Requirements for Key Estimates", "text": "The precision goals were to secure national-level survey estimates with precisions comparable to or better than those of NSOPF:99 for the overall faculty population. For this purpose, the following two publications were reviewed to establish 268 key national-level estimates: \u2022 Background Characteristics, Work Activities, andCompensation of Faculty andInstructional Staff in Postsecondary Institutions: Fall 1998 (NCES 2001-152) (Zimbler 2001); and \u2022 Salary, Promotion, and Tenure Status of Minority and Women Faculty in U.S. Colleges and Universities (NCES 2000-173). (Nettles, Perna, and Bradburn 2000)."}, {"section_title": "A.4.3 Cost/Variance Optimization", "text": "As mentioned earlier, a customized cost/variance optimization program was developed to determine the desired allocation of respondents to institution-by-person strata. The cost model necessary to support the cost/variance optimization process was the following: where C represents the total cost of the NSoFaS, C 0 represents the \"fixed costs\" that do not depend on the number of sample institutions or person, C h represents the variable cost per participating institution in stratum h, C hk represents the variable cost per respondent in person stratum h within institution stratum k, n h represents the number of participating institutions selected from stratum h, and n hk represents the number of responding persons selected from the given stratum. Only the components of variable cost, C h and C hk , had to be estimated to support the cost/variance optimization. For this purpose, they were estimated using the spreadsheet developed for the study budget. The cost per participating institution was then estimated by holding the numbers of responding persons constant while varying the numbers of participating institutions. Likewise, the variable cost per participant was estimated by holding the number of participating institutions constant while varying the number of participating persons."}, {"section_title": "A.4.4 Relative Variance Model", "text": "The following model was used to represent the relative variance of estimates in different domains: where the parameters of this model were defined for each institution stratum h and person stratum k as follows: \u2022 W dhk = proportion of domain d members who belong to stratum (h,k) \u2022 UWE hk = unequal weighting effect within stratum (h,k) \u03c3 ) were computed using the method of moments procedures in SAS Proc Nested, which resulted in some negative estimates. Unequal weighting effects, UWE hk , were computed based on the statistical analysis weights. Since these values were highly variable, it was decided that they were not reliable estimates of the unequal weighting effects. Consequently, all the UWEs were set to a constant value of 1.05. Finally, the coefficient of variation, CV md , of cluster sizes was computed for the members of each analysis domain using the same prior survey data."}, {"section_title": "A.4.5 Optimum Sample Allocation", "text": "The technique developed by Chromy (1987) was used to determine the sample allocation for each institution and person strata. This technique aimed to satisfy all precision constraints while minimizing the cost and relative variance models discussed earlier. The initial results from the optimization process were discussed with the National Center for Education Statistics (NCES) and the Technical Review Panel (TPR) in August 2002 before further refinements were applied to the resulting samples. The results of this initial sample optimization exercise were used as the basis for the sample of 1,080 institutions for NSOPF:04. As in previous cycles of NSOPF, all institutions with a Carnegie classification as public doctoral or private not-for-profit doctoral institutions were selected with certainty. After selecting the sample institutions, further refinements were made to determine which binding constraints could be relaxed in the optimization procedure. As precision constraints were iteratively relaxed during the optimization process, the sample size distributions were constrained to achieve approximately the institution-and person-level marginal distributions that were requested by NCES following the August 2002 TRP meeting. The optimization process was rerun conditional on the sample of institutions that had already been selected to determine the optimum allocation of the faculty sample sizes to these institutions. The results of this conditional optimization were used to set the final faculty sample rates, as discussed below. Note that the corresponding respondent counts are provided in tables 4 and 14 in the main body of this methodology report. "}, {"section_title": "A.5 Sampling Rates", "text": "Initial population-level sampling rates were calculated and adjusted in several steps to obtain the final institution-level sampling rates, a brief outline of which is listed below. \u2212 no cell will have a sampling rate larger than 1; \u2212 minimum sample size for each institution will be 10; and \u2212 maximum sample size for each institution will be controlled. In order to facilitate communication of computational details, the following notations will be used throughout this section. \u2212 N ij+ total number of faculty members in the i-j th institution-faculty stratum; and \u2212 n ij+ required sample of faculty respondents in the i-j th institutionfaculty stratum."}, {"section_title": "A.5.1 Calculation of the Initial Sampling rates", "text": "The initial sampling, which are calculated as the ratio of the required sample sizes and population counts within cells indexed by the institutional and faculty strata. That is, These rates represent the faculty sampling rates that would be used if a census was to be conducted of all institutions, with all institutions being eligible and participating in the study."}, {"section_title": "A.5.2 Calculation of the Conditional Sampling Rates", "text": "Faculty sampling rates were computed for the institutions in the sample, conditional on the institutions that have been selected (as if they will all be participants). The conditional sampling rate was calculated for each sample institution as the product of its initial sampling rate and institution sampling weight. That is, the initial conditional sampling rate for the j th faculty stratum in the i th institutional stratum, given the selection of the l th institution was given by:  "}, {"section_title": "A.5.3 Adjustment of Conditional Sampling Rates for Expected Rates of Faculty Ineligibility and Nonresponse", "text": "Initial sampling rates were adjusted to account for anticipated faculty ineligibility and nonresponse. Since the only reliable information for this purpose was available at the institution sector level based on results from NSOPF:99, this adjustment was carried out at sector level. The needed rates for this adjustment are summarized in Table A-14. With FE k and FR k representing the expected rates of faculty level eligibility and response for the k th sector, respectively, the adjusted sampling rate to account for faculty level attrition for the j th faculty stratum for all institutions in the k th institutional sector is given by: Because of the above adjustment and because some sample institutions were expected to be ineligible, application of the resulting sampling rates to the frame counts for the eligible sample institutions could produce sample sizes that are far from the proposed total sample of 35,671 faculty members. Consequently, the adjusted sampling rates were ratio-adjusted to the desired total in two steps. First, the revised sample size for the j th faculty stratum in the i th institutional stratum was computed as follows, where N ijl represents the l th institution in the (i-j) th stratum: "}, {"section_title": "A.5.4 Adjustment of Sampling Rates for Expected Rates of Institution Ineligibility and Nonresponse", "text": "The above rates were further adjusted to account for anticipated institution level ineligibility and nonresponse, as summarized in table A-15. With IE k and IR k representing the expected rates of institution level eligibility and response for the k th sector, the revised rates were calculated as: These calculations were performed on the data file of all eligible sample institutions among the final sample of 1,080 institutions. The eligibility rate, IE k , was set to 1.00 for all institutions that were known to be eligible. The eligibility was known for all institutions. Again, because of the above adjustment, the resulting total sample of faculty members was larger than the proposed sample of 35,671. The resulting total count for each institutionfaculty stratum was calculated to be used for further adjustments, by:"}, {"section_title": "A.5.5 Final Adjustment of Sampling Rates", "text": "The final sampling rates must satisfy a number of design requirements before they could be used for selection of sample faculty members. Specifically, all sampling rates should be bound by 1 while ensuring that the resulting sample sizes are between 10 and a reasonable maximum for each sample institution. Achieving these objectives, starting with the above initial conditional sampling rates, entailed an iterative process as described next. \u2022 Calculate the revised sample size for the j th faculty stratum of the l th sample institution in the i th institutional stratum by: \u2022 Review the distribution of the total sample size for each institution, n i+l , to detect potential outliers. Subsequently, the value of Max will be set to a new limit, which is determined after implementation of the above step. \u2022 Next, the subsequent conditional sampling rates should be calculated as the ratio of the latest sample sizes and their corresponding population counts. That is, \u2022 Calculate the next revised sample size for the j th faculty stratum of the l th sample institution in the i th institutional stratum by: The above steps were repeated until no more adjustments were needed. That is, until all sampling rates were less than or equal to one, no institution sample size was less than 10 or greater than the established maximum number, and the sample size was close to what was expected for every institution-faculty stratum. At this point, the final sampling rates were calculated by: "}, {"section_title": "A.6 Faculty Sample Selection", "text": "Faculty members were sampled as faculty lists were received for participating institutions. Prior to selecting the faculty sample for a given institution, expected sample sizes for each faculty stratum were calculated using the institution-specific faculty list counts and sampling rates. Now that the actual list counts were available these sampling rates were then modified, as necessary, for the reasons given below. \u2022 Rates were increased across all faculty strata to ensure that at least ten faculty members were selected from each institution, if possible. \u2022 Rates were increased within faculty strata to guarantee that at least one faculty member was selected per stratum, when the calculated rates called for selection of less than one faculty, if possible. \u2022 The sample yield was monitored throughout the months during which faculty lists were received, and the faculty sampling rates were adjusted periodically for institutions for which sample selection had not yet been performed to ensure that the desired faculty sample sizes were achieved. Stratified systematic sampling was used to select faculty members from the faculty lists. Using PROC SURVEYSELECT in SAS, lists were sorted in a serpentine fashion by the academic field, race/ethnicity, gender, and employment status of the faculty members, and individuals were systematically selected within faculty strata. These procedures had to be modified for lists that were received on hard copy. Quite often, these paper lists contained little information about the faculty members' race/ethnicity, gender, and employment status. When this information was not available, and therefore faculty strata could not be identified, a systematic sample of faculty was selected using the overall sampling rate for the institution. If this personal data was provided, however, a systematic sample was selected using the largest stratum-specific sampling rate. This initial sample (subframe) was then keyed to create an electronic file to avoid data entry for the entire list. Subsequently, however, extra faculty members were subsampled out using PROC SURVEYSELECT to achieve the needed allocation of faculty from the given institution. After the sample of faculty had been selected for an institution, the available information of the sample faculty members, including name, academic field, institution, race/ethnicity, and residence, was compared to that of faculty who had already been selected from other institutions. When duplicates were detected, the duplicate was eliminated from the sample of the current institution so that no faculty member would be included in the sample twice. Once the de-duplication process was complete and the final sample file was created, the final step in sample selection was to add the institution's final sample file to the master dataset. The master dataset contained all sampled faculty members and their relevant sampling information.  Form: I10a Name: I10aa Label: Full-time benefit: medical insurance Name: I10ab"}, {"section_title": "A.7 Appendix A References", "text": "Label: Full-time benefit: dental insurance Name: I10ac Label: Full-time benefit: disability insurance Name: I10ad Label: Full-time benefit: life insurance Name: I10ae Label: Full-time benefit: child care Name: I10af Label: Full-time benefit: retiree medical insurance Name: I10ag Label: Full-time benefit: cafeteria-style plan Label: Full-time benefit: dental insurance subsidized Name: I10bc Label: Full-time benefit: disability insurance subsidized Name: I10bd Label: Full-time benefit: life insurance subsidized Name: I10be Label: Full-time benefit: child care subsidized Name: I10bf Label: Full-time benefit: retiree medical insurance subsidized Name: I10bg Label: Full-time benefit: cafeteria-style plan subsidized Form Administered To: Institutions that provide at least one employee benefit to full-time faculty and instructional staff "}, {"section_title": "Form: I15a", "text": "Name: I15aa Label: Part-time benefit: medical insurance Name: I15ab Label: Part-time benefit: dental insurance Name: I15ac Label: Part-time benefit: disability insurance Name: I15ad Label: Part-time benefit: life insurance Name: I15ae Label: Part-time benefit: child care Name: I15af Label: Part-time benefit: retiree medical insurance Name: I15ag Label: Part-time benefit: cafeteria-style plan Label: Part-time benefit: dental insurance subsidized Name: I15bc Label: Part-time benefit: disability insurance subsidized Name: I15bd Label: Part-time benefit: life insurance subsidized Name: I15be Label: Part-time benefit: child care subsidized Name: I15bf Label: Part-time benefit: retiree medical insurance subsidized Name: I15bg Label: Part-time benefit: cafeteria-style plan subsidized"}, {"section_title": "Form Administered To:", "text": "Institutions that provide at least one employee benefit to part-time faculty and instructional staff    (Enter the name of the principal field or discipline in the box below. This name will be used to match against a list of academic fields, so please be specific and do not use abbreviations or acronyms. If you have no principal field, select the \"Not applicable\" box.)  If you do not agree with these codes, select \"None of these codes\" to manually code the field. Autocoding Explanation: Using the verbatim string of the respondent's highest degree field (provided in Q17A3VS), item Q17A3AC matches the string to selected CIP categories (see pages C-28 through C-30 for a list of codes and descriptions). Descriptions that match the verbatim string appear on the screen, and the respondent selects the code that best describes the degree field. Strings that do not match the CIP descriptions are routed to Q17A3CD for manual coding. (The respondent can also modify the verbatim string and redo the match or manually code the teaching field in Q17A3CD.)\nFaculty and instructional staff who provided a verbatim highest degree field, but whose results were not autocoded\nFaculty and instructional staff who hold a degree\nFaculty and instructional staff who provided a verbatim field of scholarly activity\nFaculty and instructional staff paid on something other than a 9-, 10-, 11-, or 12-month contract"}, {"section_title": "C-32", "text": "Form: Q17a3CD Name: Q17a3C2 Label: Highest degree field-general code Name: Q17a3C4 Label: Highest degree field-specific code"}, {"section_title": "StemWording:", "text": "Please help us categorize \"[FILL Q17a3VS]\" using the drop-down list boxes below. [IF Q16CD \u2265 0] (Select one from the list of disciplines you've already told us about:) [ENDIF] (Coding Directions: Please select a general area and then the specific discipline within the general area. Use the arrow at the right side of the first dropdown box to display the general areas. Click to select the desired general area, and then select the desired specific discipline within the area from the second dropdown box.) * General Area: * Specific Discipline: Note: Please refer to the complete list of instructional program codes on pages C-28 through C-30.\nPlease help us code the postsecondary institution that awarded your [FILL Q17A1 or Q17A1B] by providing the state and city in which it was located. (Steps: 1. Please select the state in which the school was located. If the school was located in another country, select \"foreign country.\" 2. Enter the name of the city in which the institution was located. You can also use the \"Browse\" link to identify the city. 3. Select the \"Continue\" button to list the schools located in that state and city.\nWe would like to consider the level of your scholarly activities during the last two years. * Of the [FILL Q52aa] articles or creative works published in refereed journals or juried media in your career, how many were done in the last two years? * Of the [FILL Q52ab] articles or creative works published in nonrefereed journals or nonjuried media in your career, how many were done in the last two years? * Of the [FILL Q52AC] reviews of books, articles, or creative works; chapters in edited volumes published in your career, how many were in the last two years? * Of the [FILL Q52AD] textbooks, other books; monographs; and client reports you published during your career, how many were done in the last two years? * Of the [FILL Q52ae] presentations you made at conferences or workshops in your career, how many were made in the last two years? * Of your [FILL Q52af] career exhibitions or performances, how many were in the last two years? * Of your [FILL Q52ag] career patents, software products, or other works, how many were done in the last two years? \nWhat is your principal field or discipline of scholarly activity? (Enter the name of your principal field/discipline of scholarly activity. This name will be used to match against a list of academic fields, so please be specific and do not use abbreviations or acronyms.) * Name of principal field/discipline of scholarly activity: Form: Q54AC Label: Principal field of scholarly activity-autocode\nPlease select the appropriate code for your field of scholarly activity: If you do not agree with these codes, select \"None of these codes\" to manually code the field. Autocoding Explanation: Using the verbatim string of the respondent's field of scholarly activity (provided in Q54VS), item Q54AC matches the string to selected CIP categories (see pages C-28 through C-30 for a list of codes and descriptions). Descriptions that match the verbatim string appear on the screen, and the respondent selects the code that best describes the field. Strings that do not match the CIP descriptions are routed to Q54CD for manual coding. (The respondent can also modify the verbatim string and redo the match or manually code the scholarly field in Q54CD).\nPlease help us to categorize \"[FILL Q54VS]\" using the drop-down list boxes below. [IF Q17A3AC \u2265 0] (Select one from the list of disciplines you've already told us about:) [ENDIF] Coding Directions: Please select a general area and then the specific discipline within the general area. Use the arrow at the right side of the first dropdown box to display the general areas. Click to select the desired general area, and then select the desired specific discipline within the area from the second dropdown box.) Label: Satisfaction with technology-based activities Name: Q61c Label: Satisfaction with equipment/facilities Name: Q61d Label: Satisfaction with institutional support for teaching improvement Name: Q62a Label: Satisfaction with workload Name: Q62b Label: Satisfaction with salary Name: Q62c Label: Satisfaction with benefits Name: Q62d Label: Satisfaction with job overall Form Administered To: All faculty and instructional staff with instructional responsibilities (Q61a-Q61d); All faculty and instructional staff (Q62a-Q62d)  \nWe are almost finished. The next questions will be about your compensation and about your background. Your responses to these items-as with all items on this instrument-are voluntary and strictly confidential. They will be used only in statistical summaries. a. What is your basic salary during the calendar year from this institution? b. How much compensation did you receive from other income from this institution not included in basic salary (e.g., for summer session, overload courses, administration, research, coaching sports, etc.)?\nThe following ranges may make it easier for you to estimate your total income from all sources for the 2003 calendar year. (Your responses to these items are strictly confidential. They will be used only in statistical summaries.) 1 = $1-24,999 2 = $25,000-49,999 3 = $50,000-74,999 4 = $75,000-99,999 5 = $100,000-149,999 6 = $150,000-199,999 7 = $200,000-300,000 8 = More than $300,000 Form: Q67 Label: Type of contract, length of unit Form Administered To: All faculty and instructional staff\nIs your basic salary at [FILL INSTNAME] this academic year based on a 9-or 10-month contract, an 11-or 12-month contract, or some other arrangement? (Please answer based on the length of your contract and how long you work rather than on the number of months you are paid.) 1 = 9-or 10-month contract 2 = 11-or 12-month contract 3 = Other, for example, by course or credit hour\nWhat was the basis of your pay? Was it by. . . [ENDIF] (By household income, we mean the total income received by all persons, including yourself, residing in the house during the 2003 calendar year, but excluding minors and full-time students. Please include income from employment and from other sources including your spouse or partner, self-employment, interest earnings, alimony or child support, insurance benefits, and pension payments.) * Enter amount:\nThe following ranges may make it easier for you to report your total household income. Was your income between. . . (Your responses to these items are strictly confidential. They will be used only in statistical summaries.) -1 = Don't know 1 = $1-24,999 2 = $25,000-49,999 3 = $50,000-74,999 4 = $75,000-99,999 5 = $100,000-149,999 6 = $150,000-199,999 7 = $200,000-300,000 8 = More than $300,000 (By this we mean do you have a physical, visual, auditory, mental, emotional, or other disabling condition that limits your ability to see, hear, or speak; to learn, remember, or concentrate; to dress, bathe, or get around the house, or to get to school or around campus.) The crosswalk below links the NSOPF:04 questionnaire items with similar items from the three previous NSOPF institution questionnaires: NSOPF:88, NSOPF:93, and NSOPF:99. This crosswalk will facilitate analyses of trends among postsecondary institutions. Linked questions may be identical in content and format or may differ in one or more ways. The question, item, or response wording; the order in which response options were presented; the manner in which the data were collected (e.g., categorical response option versus open-ended response fields, instructions to mark one versus all that apply); and the population to which the question applies may have changed. It is strongly recommended that analysts review documentation to determine whether linked questions are equitable for their purpose.      "}, {"section_title": "Form: Q17a4", "text": "Name: Q17a4ST Label: Highest degree institution-state Name: Q17a4C Label: Highest degree institution-city Name: Q17a4N Label: Highest degree institution-name Name: Q17a4I Label: Highest degree institution-IPEDS"}, {"section_title": "4.", "text": "Select the desired school."}, {"section_title": "C-33", "text": "Problems? Try searching for the school by state without listing a city. If you still can't find the school, select the \"Unable To Find School in List\" button at the bottom of the search results.) (By remedial or developmental classes, we mean courses in reading, writing, math, or other courses for students lacking the skills necessary to perform college-level work at the level required by your institution. Some institutions refer to these courses as compensatory, basic skills, or some other term. By distance education, we mean classes where students and instructors are separated primarily or exclusively by distance or time.) C-40 * a. How many were remedial or developmental classes? * b. How many were taught through distance education, either exclusively or primarily?   "}, {"section_title": "C-47", "text": ""}, {"section_title": "C-48", "text": "Form: Q54CD Name: Q54CD2 Label: Principal research field-general code Name: Q54CD4 Label: Principal research field-specific code Form Administered To: Faculty and instructional staff who provided a verbatim field of scholarly activity, but whose results were not autocoded"}, {"section_title": "C-51", "text": "Next, your compensation from other sources c. How much were you paid for employment at another postsecondary institution? d. How much were you paid for outside consulting or freelance work? e. How much were you compensated for any other employment besides consulting and another postsecondary institution (e.g., speaking fees and honoraria, self-owned business, legal/medical/psychological services, professional performances/exhibitions)? f. How much income did you receive from any other source (e.g., investment income, royalties/commissions, pensions, real estate, loans, alimony, or child support)? Form: Q66b Label: Amount of total individual income (range) Form Administered To: Faculty and instructional staff who did not complete all compensation item amounts"}, {"section_title": "C-52", "text": "Form: Q68 Label: Income paid per course/credit unit or term"}, {"section_title": "= Course", "text": ""}, {"section_title": "C-53", "text": "Form: Q70b Label: Amount of total household income (range) Form Administered To: Faculty and instructional staff who did not provide their household income"}, {"section_title": "Faculty Questionnaire Crosswalk", "text": "The crosswalk below links the NSOPF:04 questionnaire items with similar items from the three previous NSOPF faculty questionnaires: NSOPF:88, NSOPF:93, and NSOPF:99. This crosswalk will facilitate analyses of trends among faculty at postsecondary institutions. Linked questions may be identical in content and format or may differ in one or more ways. The question, item, or response wording; the order in which response options were presented; the manner in which the data were collected (e.g., categorical response option versus open-ended response fields, instructions to mark one versus all that apply); and the population to which the question applies may have changed. It is strongly recommended that analysts review documentation to determine whether linked questions are equitable for their purpose.      NCES) to periodically gather information from students, faculty, and instructional staff on two pivotal areas of national concern: \u2022 How do students and their families finance education after high school? \u2022 Who teaches in our colleges and universities, and how do they conduct their work? In response to the continuing need for these data, information was collected from students in 1987, 1990, 1993, 1996, and 2000 as part of the National Postsecondary Student Aid Study (NPSAS). Data on full-and part-time faculty and instructional staff were collected for the National Study of Postsecondary Faculty (NSOPF) in 1988(NSOPF) in , 1993(NSOPF) in , and 1999. NCES has contracted with RTI International (RTI) to conduct the next data collection cycle for both studies under the 2004 National Study of Faculty and Students (NSoFaS:04) in order to minimize the reporting burden to postsecondary institutions. Additional information about our plans for NSoFaS:04 is provided in the enclosed materials, which include an NSoFaS brochure and copies of the brochures that participating students or faculty will receive. Your institution's participation is crucial to the success of NSoFaS:04. I am writing to request that you appoint an NSoFaS coordinator to oversee the preparation of lists of faculty/instructional staff and students at your institution. The NSoFaS coordinator will also complete a brief questionnaire on the Internet about your institution's policies and procedures related to faculty and instructional staff. We will use the lists prepared by your institution to draw samples of faculty/instructional staff and students for participation in the 2004 NSOPF and NPSAS data collection cycles, respectively. Sampled faculty and students will be asked to complete a questionnaire on the Internet."}, {"section_title": "F-4", "text": "The individual whom you designate as coordinator should be someone (such as the Director of Institutional Research) who is familiar with data and information sources at your institution. If you require assistance with selecting an appropriate coordinator, you may call the NSoFaS Help Desk at 1-866-NSOFAS4 (1-866-676-3274, toll-free). We are aware that you and the staff at your institution are confronted with many competing demands for your time. Therefore, we are providing you-and the coordinator you designate-with this advance notice of the study to allow you adequate time to plan for this data collection effort and, if needed, to contact us for more information prior to the start of data collection in the fall 2003/2004 term. Once designated, an RTI representative will contact your coordinator to discuss the study timeline and procedures required for your institution. Your coordinator will also be provided with a complete summary of our data request for the NPSAS and NSOPF components of NSoFaS. All responses that relate to or describe identifiable characteristics of individuals may be used only for statistical purposes and may not be disclosed, or used, in identifiable form for any other purpose, unless otherwise compelled by law. The enclosed pamphlets detail our data collection procedures and provide a full description of the laws and procedures safeguarding the confidentiality of questionnaire responses, contact information, and other data. Additional information, including reports based on data from previous NSOPF and NPSAS studies, is available on the NSoFaS web site: https://surveys.nces.ed.gov/nsofas2004 If you have any questions about the study or procedures involved, please contact the RTI Project Coordinator, Brian Kuhr, at 1-866-676-3274 or via e-mail at nsofas2004@rti.org. You may also direct questions to NCES by contacting James Griffith at 1-202-502-7387 (e-mail address: James.Griffith@ed.gov) or Linda Zimbler at 1-202-502-7481 (e-mail address: Linda.Zimbler@ed.gov)."}, {"section_title": "At your earliest convenience, please complete the NSoFaS Designate a Coordinator form online at the NSoFaS web site, using the IPEDS UNITID and password printed on the first page of this letter.", "text": "We look forward to your participation in this important study.  NCES) to periodically gather information from students, faculty, and instructional staff on two pivotal areas of national concern: \u2022 How do students and their families finance education beyond high school? \u2022 Who teaches in our colleges and universities, and how do they conduct their work? In response to the continuing need for these data, information was collected from students in 1987, 1990, 1993, 1996, and 2000 as part of the National Postsecondary Student Aid Study (NPSAS). Data on full-and part-time faculty and instructional staff were collected for the National Study of Postsecondary Faculty (NSOPF) in 1988(NSOPF) in , 1993(NSOPF) in , and 1999. NCES has contracted with RTI International (RTI) to conduct the next data collection cycle for both studies under the 2004 National Study of Faculty and Students (NSoFaS:04) in order to minimize the reporting burden to postsecondary institutions. Additional information about our plans for NSoFaS:04 is provided in the enclosed materials, which include an NSoFaS brochure and copies of the brochures that participating students or faculty will receive. The chief administrative officer of your institution has selected you as your institution's coordinator for NSoFaS:04. The enclosed materials detail your role and the role of your institution in this study and contain a timetable of major project activities. You will have four primary responsibilities for NSoFaS:04: \u2022 Complete the Coordinator Response Form online at the NSoFaS web site, within the next few weeks, using the user name and password printed at the top of this letter. We will schedule data collection for your institution based on the information you provide. A facsimile of the Coordinator Response Form is included in the attached folder. \u2022 Oversee the preparation of two data files: (1) a list of faculty and instructional staff and (2) an enrollment list of students at your institution. These data files will be used to draw samples of faculty/instructional staff and students for participation in NSoFaS:04. Sampled faculty and students will be asked to complete a questionnaire on the Internet. \u2022 Complete a separate web-based program requiring institution record information for a sample of F-6 students. NSoFaS:04 will begin in September 2003. At that time, complete instructions for your institution's participation will be sent directly to you. In the meantime, please review the enclosed materials at your earliest convenience. We are aware that you and other staff at your institution are confronted with many competing demands for your time. We hope that giving you this advance notice of the study will provide you with ample time to plan for your school's participation in NSoFaS:04. A project representative will call you in the next few days to ensure that you have received this notification and to answer any questions that you may have. You may also call the NSoFaS Help Desk directly at 1-866-NSOFAS4 (1-866-676-3274). All responses that relate to or describe identifiable characteristics of individuals may be used only for statistical purposes and may not be disclosed, or used, in identifiable form for any other purpose, unless otherwise compelled by law. The enclosed materials detail our data collection procedures and provide a detailed description of the laws and procedures safeguarding the confidentiality of questionnaire responses, contact information, and demographic data. Additional information, including reports based on data from previous NSOPF and NPSAS studies, is available on the NSoFaS web site: https://surveys.nces.ed.gov/nsofas2004 If you have questions about the study or procedures, please contact the RTI Project Coordinator, Brian Kuhr, at 1-866-676-3274 or via e-mail at nsofas2004@rti.org. You may also direct questions to NCES by contacting James Griffith at 1-202-502-7387 (e-mail address: James.Griffith@ed.gov) or Linda Zimbler at 1-202-502-7481 (e-mail address: Linda.Zimbler@ed.gov)."}, {"section_title": "At your earliest convenience, please complete Coordinator Response Form online at the NSoFaS web site, using the IPEDS UNITID and password printed on the first page of this letter.", "text": "We look forward to your participation in this important study. Dear <NAME>: As the person designated to be the Institution Coordinator for the 2004 National Study of Faculty and Students (NSoFaS:04) at your institution, you are receiving detailed instructions (see enclosed binder) to ensure your full participation in both the study's faculty and student components. We look forward to working with you on this important research effort, and are available to answer any questions you may have on how to carry out the coordination activities requested of you. As described in materials provided during the early notification period of the study this past spring/summer, NSoFaS:04 is being conducted for the U.S. Department of Education's National Center for Education Statistics (NCES) by RTI International (RTI). This ongoing study, designed to collect data from nationally representative samples of postsecondary students and faculty and instruction staff, provides vital information on changes over time in two pivotal areas of national concern: \u2022 How students and their families finance education after high school, and \u2022 Who teaches in our colleges and universities and how they conduct their work. In response to the continuing need for the data provided by NSoFaS, Congress has authorized NCES to collect these data periodically. Data on full-and part-time faculty and instructional staff were collected through the faculty component-the National Study of Postsecondary Faculty (NSOPF)-in 1988(NSOPF)-in , 1993(NSOPF)-in , and 1999. Information on students and student financial aid was previously collected in 1987, 1990, 1993, 1996, and 2000 as part of the student component-the National Postsecondary Student Aid Study (NPSAS). Your institution has been sampled for participation in both the faculty and student components of NSoFaS:04. As the Institution Coordinator, you are asked to oversee the completion of the following activities for NSoFaS:04: \u2022 Completion of the Coordinator Response Form (CRF) online at the NSoFaS web site, https://surveys.nces.ed.gov/nsofas2004/, using the IPEDS UNITID and password printed at the top of this letter. If you have already completed this document, a copy of the form may be printed from the web site after log in. A data collection timeline for your institution has been scheduled based on the information you provided. If you have not completed the CRF online, please do so at your earliest convenience. For reference, a facsimile of the CRF is included in the enclosed binder."}, {"section_title": "F-8", "text": "\u2022 Preparation of a complete data file listing all full-and part-time faculty, adjunct faculty, and instructional staff (including available contact and demographic information). The file should be current as of November 1, 2003, or  \u2022 Completion of the Institution Questionnaire online at the NSoFaS web site. The questionnaire may be completed in multiple sessions; however, Question 1 (which asks for counts of full-and part-time faculty and instructional staff at your institution) should be answered at the time you send your list of faculty. A facsimile of the questionnaire is included in your binder. Please complete this questionnaire online by December 5, 2003, or by the date you submit your faculty list noted above if different. \u2022 Preparation of a complete data file listing all students enrolled at your institution at any time between July 1, 2003, and April 30, 2004. Please refer to the enclosed NPSAS materials for a complete set of student eligibility criteria. Your list of students enrolled should be transmitted to RTI as early as possible. This data file will be used to draw a sample of students for participation in NPSAS. Sampled students will be asked to complete a questionnaire on our secured web site over the Internet. It is critical that we allow students ample time to respond before the end of the academic year. [FOR INST THAT COMPLETED A CRF: <Information provided on the CRF indicates that you will send the student list to RTI on <DATE>. [ NO CRF/ UNKNOWN AFTER DATE: The NSoFaS help desk will call to confirm the date at which we can expect your institution's list.] \u2022 Completion of a separate web-based computer-assisted data entry (webCADE) program that requires institution record information for those students who are sampled. This includes specific information on their enrollment status, financial assistance, and demographic characteristics. More details can be found in the enclosed binder. All responses that relate to or describe identifiable characteristics of individuals may be used only for statistical purposes and may not be disclosed or used, in identifiable form, for any other purpose, unless otherwise compelled by law. The enclosed materials detail our data collection procedures and provide a detailed description of the laws and procedures safeguarding the confidentiality of individual questionnaire responses, contact information, and demographic data. Additional sources of information, including reports based on data from previous NSOPF and NPSAS studies, are available on the NSoFaS web site: https://surveys.nces.ed.gov/nsofas2004/. If you have questions about the study purposes or procedures, please contact either of us or Brian Kuhr,Project Coordinator, or via e-mail at nsofas2004@rti.org. You may also direct questions to NCES by contacting either James Griffith at 1-202-502-7387 (e-mail address: James.Griffith@ed.gov) or Linda Zimbler at 1-202-502-7481 (e-mail address: Linda.Zimbler@ed.gov). We look forward to your participation in this important study. Thank you for your cooperation. To ensure representation of the entire range of postsecondary institutions in the nation, we count on cooperation from each of the sampled institutions. We are grateful for the outstanding cooperation that we have received in previous cycles of these studies. We urgently request your institution's participation in NSoFaS:04. We are well aware that, especially under difficult economic conditions, postsecondary institutions have limited staff and resources to devote to participating in research studies, regardless of their importance. That is why we have instructed RTI International, NCES' contractor for NSoFaS:04, to provide your institution with the assistance necessary to accomplish the following: \u2022 Provide a list of faculty and instructional staff employed by your institution as of November 1, 2003; \u2022 Complete a brief Institution Questionnaire concerning your institution's policies and procedures regarding faculty; \u2022 Provide a list of postsecondary students enrolled at your institution between July 1, 2003 and April 30, 2004;and \u2022 Complete a student record abstraction form for a small number of students selected from the enrollment list. To assist your institution in participating in the study, NCES has authorized RTI International to provide compensation for the staff and resources required by your institution to compile lists of faculty and students and associated documentation."}, {"section_title": "F-10", "text": "Moreover, if necessary, RTI will also arrange for one of its specially-trained staff to visit your institution and perform the record abstractions for sampled students. Data collection for NSoFaS:04 is both authorized and protected by federal confidentiality laws, including the Family Education Rights and Privacy Act (FERPA). The small number of faculty and students sampled from the lists provided by your institution will be asked to participate in NSoFaS:04 by completing a questionnaire online or by telephone in a confidential and secure manner. We encourage you to review the additional information available about NSoFaS:04 at the following web site: https://surveys.nces.ed.gov/nsofas2004/ Both the Institution Questionnaire and secure uploads for faculty and student lists may be accessed at this site. The user name (IPEDS UNITID) and password required to access the forms and procedures for your institution are printed at the top of this letter. Over the course of the next 2 weeks, a representative from RTI will be contacting you to discuss your needs and the best way to facilitate your institution's participation in NSoFaS:04. You may also contact Brian Kuhr, the Project Coordinator at 1-866-676-3274 or by e-mail at nsofas2004@rti.org to confirm your participation in the study and to request any necessary assistance in providing the data requested. You may direct questions to NCES by contacting James Griffith at 1-202-502-7387 (e-mail address: James.Griffith@ed.gov) or Linda Zimbler at 1-202-502-7481 (e-mail address: Linda.Zimbler@ed.gov). Once again, thank you for your consideration. Sincerely, C. Dennis Carroll, Ph.D."}, {"section_title": "Associate Commissioner Postsecondary Education Division", "text": "The NSoFaS forms may be completed online at https://surveys.nces.ed.gov/nsofas2004/ To access the online form, enter the user name (which is your IPEDS UNITID) and password printed on the first page of this letter. "}, {"section_title": "COORDINATOR RESPONSE FORM (CRF) FACSIMILE", "text": "If you completed the CRF in spring/summer 2003, a report can be viewed and/or printed from the web site with your responses-specifically, the due dates established for submitting your list of faculty and instructional staff and/or list of students enrolled. Follow the steps below to connect to the study's secure web site. "}, {"section_title": "Connect browser to", "text": ""}, {"section_title": "Coordinator Response Form", "text": "Your response to these questions will allow RTI to customize some of the systems on the NSoFaS web site with characteristics unique to your institution. This will make it easier for you and your staff to move through the various study components. 3. Identify the names of each of the terms/enrollment periods (sometimes referred to as payment periods) that a student may enroll in between July 1, 2003, and June 30, 2004. Please include all terms, even those that may apply to special types of students (e.g., medical or MBA students). NOTE: SOME PORTION OF THE TERM MUST OCCUR BETWEEN JULY 1, 2003, AND JUNE 30, 2004, BUT MAY START PRIOR TO JULY 1 OR END AFTER JUNE 30. After all the terms are added, please press the Continue button."}, {"section_title": "Add Term", "text": "Please add a term. Please enter the name of the term and the associated start and end dates. Please list up to 12 names of the most prevalent institution grants and scholarships awarded and indicate whether \"need,\" \"merit,\" or \"both\" is considered when making these awards. Check here if your institution does not award institution grants or scholarships. Then click on the Continue button below. Please list up to 12 names of the most prevalent institution grants and scholarships awarded and indicate whether \"need,\" \"merit,\" or \"both\" is considered when making these awards. 6. Please provide a list of all students enrolled at your institution. The table to the right depicts the data elements to be included on the list for each student. We'd like to receive the enrollment list as soon as possible. Based on the dates you provided for terms during the 2003-04 academic year, February 24, 2004, is 2 weeks after the beginning of the \"Spring 2004\" term, which is the last term with a start date that is on or before April 30, 2004."}, {"section_title": "Delete? Name of Award Basis of Award Decision", "text": "When will you be able to provide the list of all students enrolled? On or before February 24, 2004 After February 24, 2004. (A project staff member will call to establish a specific date.) Your responses will be secured behind firewalls and will be encrypted during Internet transmission. All identifying information is maintained in a separate file for follow-up purposes only. Your responses may be used only for statistical purposes and may not be disclosed, or used, in identifiable form for any other purpose, except as required by law. We have enclosed a pamphlet that answers common questions about the study, and contains additional information on laws protecting your confidentiality."}, {"section_title": "Student Data Element", "text": "To respond to the questionnaire over the Internet: \u2022 Go to: https://surveys.nces.ed.gov/nsopf/ \u2022 Type the study ID and password (see below) on the Home/Login page, and \u2022 Press \"Enter\" or click \"Login\" to begin the questionnaire. To respond to the questionnaire by telephone with one of our trained interviewers, or ask questions about the study: \u2022 Call 1-866-NSOPF04 (1-866-676-7304). If you complete the questionnaire by <DATE>, you may choose to receive either a $30 check or gift certificate from Amazon.com as a token of our appreciation. To complete the self-directed web questionnaire: 1. Go to: https://surveys.nces.ed.gov/nsopf/ 2. At the login and password prompts, enter the study ID and password printed in the lower right of the attached letter. 3. Press \"Enter\" or click \"Login\" to begin the questionnaire. You will need to use Internet Explorer or Netscape as your browser to complete the self-directed web version. If you need assistance in completing the web questionnaire or would like to complete the questionnaire over the phone, please call our Help Desk at 1-866-NSOPF04 (1-866-676-7304) for assistance. While you may complete the NSOPF web questionnaire throughout the data collection period, we will begin calling sample members to complete the interview over the phone starting <DATE>. For more information about this study visit the web site at: https://surveys.nces.ed.gov/nsopf/ According to the Paperwork Reduction Act of 1995, no persons are required to respond to a collection of information unless such collection displays a valid OMB control number. The valid OMB control number for this information collection is 1850-0608. The time required to complete this information collection is estimated to average 30 minutes per response, including the time to review instructions, search existing data resources, gather the data needed, and complete and review the information collection. You have been randomly selected to participate in the fourth cycle of the National Study of Postsecondary Faculty (NSOPF:04) that is being conducted on behalf of the United States Department of Education. We are requesting that you complete a questionnaire over the Internet using the secure study ID and password listed below. Participation in the study is voluntary; however, to ensure that the study represents the range of postsecondary faculty and instructional staff in the United States, the participation of each person selected in the sample is critical to the study's success. To find out more about the study, click the link below. To respond to the questionnaire over the Internet, log in using your study ID and password: https://surveys.nces.ed.gov/nsopf/ Study ID: Password: You will need to use Internet Explorer or Netscape as your browser to complete the web version. The U.S. Department of Education has contracted with RTI International, an independent non-profit research organization, to conduct the study. To respond to the questionnaire by telephone or ask questions about the study, please call the RTI help desk at: 1-866-NSOPF04 (1-866-676-7304) As a small token of our appreciation, if you complete the questionnaire by <DATE>, you may choose to receive either a $30 check or a $30 gift certificate from Amazon.com. Your responses may be used only for statistical purposes and may not be disclosed, or used, in identifiable form for any other purpose, except as required by law. Your responses will be secured behind firewalls and will be encrypted during Internet transmission. On average, the questionnaire takes about 30 minutes to complete. To learn more about the study and the laws protecting your confidentiality, please click on the link above. Thank you in advance for your participation in this important study. We are writing to urge your completion of the questionnaire for the National Study of Postsecondary Faculty (NSOPF), sponsored by the U.S. Department of Education. As indicated previously, you were randomly selected for participation in this nationally representative sample of faculty and instructional staff. At a time of rapid change in postsecondary education, NSOPF will provide critical updated information on the characteristics, workload and career paths of faculty and instructional staff in the United States. To adequately represent the full range of faculty and instructional staff throughout the nation, all persons having any full-or parttime instructional duties, or having faculty status in the fall of 2003, are eligible for inclusion. The participation of each individual selected is critical to the study's success. To access the questionnaire on the web or to obtain more information about the study, go to https://surveys.nces.ed.gov/nsopf and log in using your Study ID: Password: You will need to use Internet Explorer or Netscape as your browser to complete the web version. If you need help accessing the web or if you prefer to complete the questionnaire by telephone, please call the RTI Help Desk at 1-866-NSOPF04 (1-866-676-7304). If you do not wish to receive an additional reminder e-mail message regarding this early-response incentive, you may call the number listed above and request to be removed from the mailing list. The U.S. Department of Education has contracted with RTI International, an independent non-profit research organization, to conduct the study. Whether by web or telephone, we urge you to complete the questionnaire promptly. If you complete the questionnaire by <DATE>, you may choose to receive either a $30 check or a $30 gift certificate from Amazon.com. On average, the questionnaire takes about 30 minutes to complete. Your responses may be used only for statistical purposes and may not be disclosed, or used, in identifiable form for any other purpose, except as required by law. Your responses will be secured behind firewalls and will be encrypted during Internet transmission. To learn more about the study and the laws protecting your confidentiality, please go to the web address listed above. On behalf of the U.S. Department of Education, we would like to thank you in advance for your participation in this very important study. This message is only intended to be a gentle reminder to you that the early-response period for the National Study of Postsecondary Faculty (NSOPF) is drawing near. We are pleased to report that about 50 percent of faculty and instructional staff invited to participate along with you have already completed the questionnaire online. However, to adequately represent the entire range of faculty and instructional staff in the nation, we need at least 80 percent of the sample to complete the survey. We hope you will find the time to participate in the study soon. As a small token of our appreciation, if you complete the questionnaire by <DATE>, you may choose to receive either a $30 check or a $30 gift certificate from Amazon.com. To access the questionnaire on the web or to obtain more information about the study, go to https://surveys.nces.ed.gov/nsopf and log in using your Study ID: Password: You will need to use Internet Explorer or Netscape as your browser to complete the web version. Please be assured that your responses will be secured behind firewalls and will be encrypted during Internet transmission. If you need help accessing the web or if you prefer to complete the questionnaire by telephone, please call our Help Desk at 1-866-NSOPF04 (1-866-676-7304). If you do not wish to receive additional reminder e-mail messages, you may call the number listed above and request to be removed from the mailing list. Thank you in advance for your participation in this very important study. Your participation is so very critical to its success. As we approach the end of data collection for the 2004 National Study of Postsecondary Faculty (NSOPF), the U.S. Department of Education wants to ensure that all faculty and instructional staff employed in the 2003 Fall Term are well represented in the study. It is important that you are counted so that we have an accurate representation of the diversity and experience of our nation's postsecondary educators. In fact, your participation is so critical to the success of the study that we have created an abbreviated version of the questionnaire. The current questionnaire takes about 10 minutes to complete. We hope that this reduced time requirement will allow those of you with more demanding schedules to participate in this important study and represent others with similar time constraints. We still offer two convenient ways to participate. You can visit our web site to answer these questions online in the privacy of your home or office. If you prefer to respond by phone, you can call 1-866-NSOPF04, a toll-free number. To participate or to learn more about the study, log in with the ID and password provided below. https://surveys.nces.ed.gov/nsopf Study ID: Password: If you complete the questionnaire by September 30, 2004, we will send you either a $30 check or $30 gift certificate from Amazon.com, as a token of our appreciation. Your responses will be kept confidential, will be encrypted during Internet transmission, and will be secured behind firewalls. All identifying information is maintained in a separate file and will never be linked to answers you provide. On behalf of the National Center for Education Statistics, U.S. Department of Education, we would like to thank you for your consideration of this important study. As we approach the end of data collection for the 2004 National Study of Postsecondary Faculty (NSOPF), the U.S. Department of Education wants to ensure that all faculty and instructional staff employed in the 2003 Fall Term are well represented in the study. It is important that you are counted so that we have an accurate representation of the diversity and experience of our nation's postsecondary educators. In fact, your participation is so critical to the success of the study that we extended the deadline and have created an abbreviated version of the questionnaire. The current questionnaire takes about 10 minutes to complete. We hope that this reduced time requirement will allow those of you with more demanding schedules to participate in this important study and represent others with similar time constraints. We still offer two convenient ways to participate. You can visit our web site to answer these questions online in the privacy of your home or office. If you prefer to respond by phone, you can call 1-866-NSOPF04, a toll-free number. To participate or to learn more about the study, log in with the ID and password provided below. https://surveys.nces.ed.gov/nsopf Study ID: Password: If you complete the questionnaire by October 5, 2004, we will send you either a $30 check or $30 gift certificate from Amazon.com, as a token of our appreciation. Your responses will be kept confidential, will be encrypted during Internet transmission, and will be secured behind firewalls. All identifying information is maintained in a separate file and will never be linked to answers you provide. As a reminder, we still offer two convenient ways to participate. You can visit our website to answer these questions online in the privacy of your home or office. If you prefer to respond by phone, you can call 1-866-NSOPF04, a toll-free number. To participate or to learn more about the study, log in with the Study ID and password provided below. https://surveys.nces.ed.gov/nsopf/ Study ID: Password: If you complete the 10 minute questionnaire, we will send you either a $30 check or $30 gift certificate from Amazon.com, as a token of our appreciation. Your responses will be kept confidential, will be encrypted during Internet transmission, and will be secured behind firewalls. All identifying information is maintained in a separate file and will never be linked to answers you provide.  Tables H-1 through H-3 show the comparisons between institution questionnaire counts and tallied faculty list counts. Among the 900 institutions that provided faculty lists and responded to the questionnaire, 740 (83 percent) had list counts that were less than 10 percent discrepant. There were a greater number of discrepancies in part-time faculty counts than fulltime faculty counts. Providing a complete and accurate list of part-time faculty is, for most institutions, the most difficult part of the NSOPF data request.   "}, {"section_title": "I-3", "text": ""}, {"section_title": "I.1 Overview", "text": "The bias in an estimated mean based on respondents, R y , is the difference between this estimate and the target parameter, \u00b5, which is the mean that would result if a complete census of the target population was conducted and all units responded. This bias can be expressed as follows: However, for variables that are available from the frame, \u00b5 can be estimated by \u03bc without sampling error, in which case the bias in R y can then be estimated by: Moreover, an estimate of the population mean based on respondents and nonrespondents can be obtained by: where\u03b7 is the weighted unit nonresponse rate, based on weights prior to nonresponse adjustment. Consequently, the bias in R y can then be estimated by: That is, the estimate of the nonresponse bias is the difference between the mean for respondents and nonrespondents multiplied by the weighted nonresponse rate, using the design weights prior to nonresponse adjustment."}, {"section_title": "I.2 Unit-level Nonresponse Bias Analysis", "text": "A faculty respondent was defined as any sample member who was determined to be eligible for the study and had valid data for the selected set of key analytical variables. As shown in section 3.2.1 (table 13) of the main body of this report, for the 34,330 eligible sample faculty members the unweighted and weighted response rates were both 76 percent. Since the faculty weighted response rate was below 85 percent for virtually all institution types, a nonresponse bias analysis was conducted for faculty members from all institution types. The nonresponse bias was estimated for the variables known for both respondents and nonrespondents within each institution type. The steps for nonresponse bias analysis included estimating the nonresponse bias and testing (adjusting for multiple comparisons) to determine if the bias was significant at the 5 percent level. Second, nonresponse adjustment factors were computed, as detailed in section 6.2.3, to significantly reduce or eliminate nonresponse bias for variables included in the corresponding models. Third, after the weights were computed, any remaining bias was estimated and statistical tests were performed to determine their significance. As shown in table I-1, the faculty weighting adjustments have reduced, and in some cases eliminated, bias for faculty members for all institution types. Significant bias was reduced for the variables known for most respondents and nonrespondents, which are considered key analytical variables and correlated with many of the other variables where bias is measured as a significant difference from zero using the National Center for Education Statistics (NCES) recommended method detailed in section I.1. Analogous analyses were conducted for the institution survey where a responding institution was one that had responded to the institution questionnaire. The corresponding institution level results are summarized in table I-2. Here too, the institution weighting adjustments have reduced percent significant bias overall and for all institution types. Note that such analyses were not carried out when computing the institution component weights for the faculty analysis weights, since the weighted response rate for institutions providing lists of faculty and instructional staff exceeded 85 percent. "}, {"section_title": "I.3 Item-Level Bias Analysis", "text": "For items with less than 85 percent weighted response rate, respondents and nonrespondents are compared on the sampling frame and/or questionnaire variables when data on respondents and nonrespondents were available. For this purpose, item response rates (RRI) were calculated as the ratio of the number of respondents for whom an in-scope response was obtained (I x for item x) to the number of unit level respondents (I) minus the number of respondents with a valid skip item for item x (V x ), or: A faculty member was defined to be an item respondent for an analytic variable if the given faculty member had data for that variable, observed or deduced via logical imputation. Table I-3 provides a summary of response rates for variables with a response rate less than 85 percent-overall or within an institution type. A nonresponse bias analysis was conducted for these items, results of which are summarized in table I-4. For these items, the nonresponse bias was estimated for variables known for both respondents and nonrespondents and tested (adjusting for multiple comparisons) to determine if the bias was significant at the 5 percent level. With the exception of the income items, which were expected to have higher rates of refusal due to their sensitive nature, the primary reason item nonresponse exceeded 15 percent for these items is that each applies to a relatively small subset of respondents (i.e., small denominator) and none of these items were asked on the abbreviated instrument, which was administered to about 1,600 responding faculty members. The Q37 items were presented as a matrix that asked 6 questions about each of the classes taught by the respondents. The rate of nonresponse increased for each subsequent class described, due primarily to the smaller number of respondents to whom the question applied, relative to the static number of respondents who completed the abbreviated instrument and were not asked this question. Income paid per course, credit unit, or term (Q68) was missing for 26 percent of respondents to whom this item applied. This item was asked only of those who indicated their salary was not based on a 9-or 10-month, or 11-or 12-month contract. Of those who provided the basis of their pay (course, credit hour, academic term [Q68]), a follow-up question (Q69) asked for the amount of income paid per course, credit unit, or term. This item was missing for 36 percent of respondents to whom this item applied."}, {"section_title": "I-6", "text": "Appendix I. Nonresponse Bias Analysis Table I   Similarly, an institution was defined to be an item respondent for an analytic variable if the given institution had data for that variable, observed or deduced via logical imputation. Table I-5 provides a summary of response rates for all institution items, overall and by institution type. For variables with a response rate less than 85 percent-overall or within an institution type-a nonresponse bias analysis was conducted, results of which are summarized in table I-6. Analogous to the faculty item nonresponse bias analysis, for these items the nonresponse bias was estimated for variables known for both respondents and nonrespondents and tested (adjusting for multiple comparisons) to determine if the bias was significant at the 5 percent level. Two of the 90 items had an overall response rate of less than 85 percent. Item 7E2 of the institution questionnaire was asked only if the institution reported having offered early or phased retirement to any tenured full-time faculty or instructional staff (item 7E). Apparently, the number of full-time faculty and instructional staff who took early or phased retirement in the previous 5 years was difficult for some institutions to quantify. Asking about a single academic year may yield a lower rate of nonresponse. Item 15BG was asked if the institution reported having a cafeteria-style plan benefit for all or some part-time faculty and instructional staff (item 15AG). Only a small percentage of schools offer this benefit to part-time staff, and a relatively high percentage of schools were not sure whether this benefit was offered. Hence the high nonresponse on this nested question asking whether the benefit was subsidized is due almost entirely to the nonresponse on the gate question, coupled with the small number of schools to which the nested item applied."}, {"section_title": "I-30", "text": ""}, {"section_title": "I.4 Bias Reduction due to Imputation", "text": "Bias resulting from missing data can occur at the unit level due to differential nonresponse or undercoverage, while bias at the item level is often due to unanswered questions or inconsistent responses that are typically set to missing once they fail edit checks. Section I.2 described measures taken to reduce bias due to unit nonresponse, while this section evaluates how well the imputation succeeded in reducing bias for items with a weighted response rate of less than 85 percent (using weights prior to nonresponse adjustment) by estimating bias before and after imputation. For continuous variables, the estimated bias was calculated as the mean before imputation minus the mean after imputation. For categorical variables, the estimated bias was computed for each category as the percentage of faculty members in that category before imputation minus the corresponding percentage after imputation. The estimated bias was then tested (adjusting for multiple comparisons) to determine if the bias was significant at the 5 percent level. A categorical variable was considered significantly biased if the bias for any of its categories was significant. The results for faculty items are shown in table I-7 for continuous variables and table I-8 for categorical variables.  The above analyses were repeated for institution items as well, the analogous results of which are summarize in tables I-9 and I-10. Again, for continuous variables, the estimated bias was calculated as the mean before imputation minus the mean after imputation, while for categorical variables, the estimated bias was computed for each category as the percentage of institutions in that category before imputation minus the corresponding percentage after imputation. The estimated bias was then tested (adjusting for multiple comparisons) to determine if the bias was significant at the 5 percent level.   Since no significant differences were found at the nominal 5 percent level, this suggests that no discernible bias has been introduced by securing survey responses late in the field period. The only differences that were significant were those comparing percentages of faculty responding via CATI or the Web at the beginning to those at the last period of data collection. Figures I-1 and I-2 provide a visual summary of the results for CATI and Web responses; on each graph the plotted lines reflect the cumulative mean response rates (unweighted) for the specific subdomain of faculty members, overall and by institution sector. However, such differences are intuitive, as during the first part of the field period (day 1 to 28) faculty members were offered incentives to complete the survey via the Web and no outbound calls were made to secure responses by CATI.        L-3"}, {"section_title": "J-3", "text": ""}, {"section_title": "K-11", "text": ""}, {"section_title": "Generalized Exponential Model (GEM) -an overview", "text": "In survey practice, design weights are adjusted to correct the bias introduced by differential nonresponse and undercoverage via nonresponse adjustment and post-stratification. Since these adjustments can increase variance of estimates by creating extreme weights, oftentimes, extreme weight adjustments are applied to reduce variance inflation due to weighting. The Generalized Exponential Model (GEM) program was developed at RTI  to provide a unified method for all weight adjustments. GEM is an expansion of the commonly used method of iterative proportional fitting (raking) based on a generalization of  logit method, in that bounds on weights are not required to be uniform. For this purpose, GEM has a built-in extreme weight control feature that allows for different bounds on the adjusted weights for different sample units. This control feature can be used for a separate extreme weight adjustment after poststratification such that sample distribution of weights obtained after the initial poststratification is preserved. The unadjusted initial weights were classified as extreme if they fell outside of the interval of median \u00b1 3 \u00d7 interquartile range (IQR) within specified domains, where domains were defined as functions of design strata with a minimum sample size requirement of 30. The goal of model fitting was to keep as many variables as possible without unduly increasing the unequal weighting effects (UWE) and the extreme weight proportion. A mixture of forward and backward selection schemes were used for fitting the GEM model. The model started with the main effects and then two-way and higher order interaction effects were added in a forward manner. Subsequently, the bounds were successively tightened until the model statistics and characteristics were satisfied. During the modeling, a number of statistics were closely monitored to uncover any unusual impact of weight adjustment on the initial weights. These statistics included UWEs, extreme weight proportions, and distribution of large adjustment factors. GEM summary/ diagnostic statistics provide information on the distribution of the initial and adjusted weights, number of variables in the final model, and the extreme weight proportions."}, {"section_title": "M-3", "text": "Researchers who do not have access to software packages such as SUDAAN for calculation of design-based standard errors can use a relevant estimate of design effect from this table to approximate the standard errors of statistics for the 2004 National Study of Postsecondary Faculty (NSOPF:04). For estimates of a proportion, p , a design-based standard error can be approximated by: Similarly, design-based standard errors for estimates of means can be approximated by the following formula, in which S 2 represents the sample variance under simple random sampling. For instance as reported in table M-1, it is estimated that among the 7,460 sample faculty members at public doctoral institutions, 39.7 percent were with tenure as of fall 2003 (Q12). The design based (proper) standard error of this estimate is 0.67 percent, while under the simple random sampling assumption the corresponding (improper) standard error is 0.57 percent. However, a rough approximation of the designed based standard error of this estimate can be produced by multiplying the value of standard error obtained under the simple random estimate assumption (0.57 percent) by the estimate of root design effect. This technique is not recommended, however; there are many commercially available statistical software packages to do this.     1 Numbers rounded to the nearest 10."}, {"section_title": "M-4", "text": "2 Institution types are defined as follows: 1 = public doctor's; 2 = public master's; 3 = public bachelor's; 4 = public associate's; 5 = public other; 6 = private not-for-profit doctor's; 7 = private not-for-profit master's; 7 = private not-forprofit bachelor's; 8 = private not-for-profit associate's; 9 = private not-for-profit associate's; and 10 = private not-forprofit other. NOTE: Detail may not sum to totals because of rounding. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2004 National Study of Postsecondary Faculty (NSOPF:04).   "}, {"section_title": "M-11", "text": ""}, {"section_title": "M-12", "text": ""}, {"section_title": "M-13", "text": ""}]