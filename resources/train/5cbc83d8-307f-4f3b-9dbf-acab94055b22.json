[{"section_title": "Abstract", "text": "Spatiotemporal processes are ubiquitous in our life and have been a trending topic in the scientific community, e.g. the dynamic brain connectivity study in neuroscience. There is usually complicated dependence among spatial locations and such relationship does not necessarily stay static over time. Spatiotemporal Gaussian process (STGP) is a popular nonparametric method to model this type of data. However, the classic STGP has a covariance kernel with space and time separated, failed to characterize the temporal evolution of spatial dependence (TESD). Even for some recent work on non-separable STGP, location and time are treated with no difference, which is unnecessarily inefficient. This paper generalizes STGP by introducing the time-dependence to the spatial kernel and letting its eigenvalues vary over time. A novel STGP model with the covariance kernel having a Kronecker sum structure is proposed and proved to be superior to the popular kernel with a Kronecker product structure. A simulation study on the spatiotemporal process and a longitudinal analysis of Alzheimer's patients' brain images demonstrate the advantage of the proposed methodology in effectively and efficiently characterizing TESD."}, {"section_title": "Introduction", "text": "Spatiotemporal data are ubiquitous nowadays in our daily life. For example, the climate data manifest a trend of global warming. The traffic data feature network structure in space and periodic intensity in time. They can be viewed as either multiple time series observed across various locations, or geographic data recorded at different time points.\nThere is usually intricate relationship between space and time in this type of data. The dependence among spatial locations does not necessarily stay static over time. For example, in the study of dynamic brain connectivity (association among brain regions) during certain cognitive processes (Cribben et al., 2012; Fiecas and Ombao, 2016; Lan et al., 2017) , multi-site brain signals are correlated to each other and such spatial dependence varies along the time. In the longitudinal analysis of brain images (Hyun et al., 2016) , different brain regions also have changing connection in the progression of some disease, e.g. Alzheimer. In general, the temporal evolution of spatial dependence (TESD) is important to understand the mechanism of some natural phenomena, e.g. disease development, to predict their progress and to extrapolate to unknown territory. In this paper, a fully Bayesian nonparametric model based on spatiotemporal Gaussian process (STGP) is proposed to characterize the spatial correlation and its evolution in time. In particular, the proposed mothod, generalized STGP, is used to analyze Alzheimer's patients' brain images to uncover TESD of their brain regions.\nSTGP is a special type of Gaussian process that can model both spatial and temporal information simultaneously. There is a rich literature on STGP including, but not limited to, Singh et al. (2010) ; Hartikainen et al. (2011) ; Luttinen and Ilin (2012) ; Soh et al. (2012) ; Sarkka and Hartikainen (2012) ; Sarkka et al. (2013) ; Lindstr\u00f6m et al. (2013) ; Liu (2013) ; Marco et al. (2015) ; Niu et al. (2015) ; Datta et al. (2016) ; Hyun et al. (2016) ; Senanayake et al . (2016) ; Todescato et al. (2017) ; Nabarro et al. (2018) ; Kuzin et al. (2018) . However, most of them are based on the separability between spatial and temporal processes, and not all of them can handle the non-stationarity of the processes. As detailed below, under such separability assumption, the spatial correlation of the spatiotemporal process conditioned on anytime is independent of time. Therefore, models based on the separability assumption doom to fail in modeling the temporal evolution of spatial dependence (TESD). To effectively characterize the spatiotemporal changes, several models (Singh et al., 2010; Marco et al., 2015; Datta et al., 2016; Hyun et al., 2016; Kuzin et al., 2018) are built on non-separability conditions. However, they treat spatial and temporal variables without difference in the joint (full-sized space-time) covariance kernel, which may not be efficient in learning TESD (See more details in Section 2.5). More specifically, Marco et al. (2015) propose STGP based on kernel convolutions of a white noise GP with a full-sized kernel, which is similar to our model I with a 'Kronecker product' structure (See more details in Section 2). Datta et al. (2016) also use a full-sized space-time kernel but sparsify it using nearest neighbors. Hyun et al. (2016) consider a functional PCA model with again the full-sized kernel for the random component. Similar to our proposal, their kernel admits a spectral decomposition. However it has eigenfunctions in the extended coordinates of both space and time, which is not computationally efficient. Additionally, almost all the existing work on STGP focuses on modeling and predicting the mean functions, but none of them models and predicts the covariance kernel in order to fully describe TESD. In general, there is a lack of flexible and efficient Bayesian non-parametric models that could effectively characterize TESD. This paper aims to fill the blank in the literature.\nStarting from the classic separable STGP, we generalize it by introducing a timedependent spatial kernel. To learn TESD, we compare two model structures based on the Kronecker product (the most popular one) and the Kronecker sum and discover that the latter is more efficient and effective. The second model does not use a full-sized spacetime kernel thus avoids modeling unnecessary blocks off the main diagonal in learning TESD (See more details in Section 2.5). To introduce the time-dependence to the spatial kernel, we model the eigenvalues in the Mercer's representation of the spatial kernel dynamically using another independent GP. The resulting time-dependent spatial kernel, rigorously defined under given conditions, is essential to reveal TESD of spatiotemporal processes. Due to the construction, it can be used to capture the non-stationarity of the processes. We find that Senanayake et al . (2016) ; Nabarro et al. (2018) have a similar additive structure of the time, space, and space-time covariances, however missing the time-dependence in the description of spatial correlation. Kuzin et al. (2018) is the most relevant work that gives a time-evolving representation of the interdependencies between the signal components and they use a spike-slab prior to induce sparsity. Our second model also generalizes the semi-parametric scheme of dynamic covariance modeling (Wilson and Ghahramani, 2011; Fox and Dunson, 2015; Lan et al., 2017) with a covariance matrix of fixed size for the spatial domain to have a kernel (infinite-dimensional spatial covariance) in order to enable extrapolation to new locations while modeling the temporal evolution (See more details in Section 3.2.2).\nThe contributions of this work include: (1) The separable STGP is generalized efficiently by introducing the time-dependence to the spatial kernel via Mercer's representation;\n(2) Two model structures based on the Kronecker product and the Kronecker sum are compared and the latter is shown to be more effective in learning TESD both theoretically and numerically; (3) TESD is modeled and predicted effectively and efficiently. The impact of this work does not limit to the included application of the longitudinal analysis of brain images. The proposed methodology, generalized STGP, is generic to learn TESD for all spatiotemporal processes, with potential applications to genome-wide association study (GWAS) in evolution, climate change analysis, investment portfolio maintenance etc..\nThe rest of the paper is organized as follows. Section 2 reviews the separable Spatiotemporal Gaussian Process (STGP) and generalize it to be non-separable by introducing the time-dependence to the spatial kernel. Two model structures, the Kronecker product (model I) and the Kronecker sum (model II) are introduced to STGP and compared for learning the temporal evolution of spatial dependence (TESD). Section 3 discusses the posterior inference and the prediction of both mean and covariance functions. Model II with the Kronecker sum structure is shown to have computational advantage over model I based on the Kronecker product. Section 4 contains a thorough simulation study to illustrate the effectiveness of the proposed generalized STGP (mainly model II) in modeling and predicting TESD of spatiotemporal processes. In Section 5, the proposed methodology is applied to analyze a series of positron emission tomography (PET) brain images of Alzheimer's patients obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) (ADN).\nTESD of their brain regions is effectively characterized and predicted during the progression of AD. Finally we conclude in Section 6 with a few comments on the methodology and some discussions of future directions."}, {"section_title": "Generalized Spatiotemporal Gaussian Processes", "text": "In this section, we first define the (separable) STGP using the matrix normal distribution and explain why it fails to characterize TESD. This motivates the generalization of STGP to introduce the time-dependent spatial kernel. Two model structures based on the Kronecker product and the Kronecker sum are compared and the latter is more effective and efficient.\nWe fix some notations first. Let X \u2282 R d be a bounded (spatial) domain and let T \u2282 R + be a bounded (temporal) domain. Denote Z := X \u00d7 T as the joint domain and z := (x, t).\nThe spatiotemporal data {y ij |i = 1, \u00b7 \u00b7 \u00b7 , I; j = 1, \u00b7 \u00b7 \u00b7 , J} are taken on a grid of points {z ij = (x i , t j ) |x i \u2208 X , t j \u2208 T } with the spatial discrete size I and the temporal discrete size J. A (centered) STGP is uniquely determined by its covariance kernel C z : Z \u00d7Z \u2192 R, a bilinear symmetric positive-definite function. C z could be defined without differentiating x and t (full-size covariance) or by exploring structures in space and time, to be detailed below."}, {"section_title": "Spatiotemporal Gaussian Process", "text": "The spatiotemporal data {y ij } are usually modeled using the standard (separable) STGP model:\nwhere the (joint) spatiotemporal kernel C z often has the following separability condition assumed\nWith these notations, we can define the (separable) STGP through the matrix normal distribution as follows."}, {"section_title": "Definition 1 (STGP).", "text": "A stochastic process f (x, t) is called (separable) spatiotemporal Gaussian process with a mean function m(x, t), a spatial kernel C x and a temporal kernel\nRemark 1. If we vectorize the matrix F I\u00d7J , then we have\nTherefore, we often denote a sample of the (centered) STGP as f \u223c GP(0, C x \u2297 C t ).\nNote that conditioned on any fixed time t \u2208 T , the covariance of the process f (x, t) in the space domain is reduced to\nwhich is static in time. Such drawback of the separable kernel makes the corresponding STGP fail to characterize TESD in the field. Neither can it capture the spatial variation of the temporal correlation (SVTC) among the processes because conditioned on any fixed location x \u2208 X , the covariance in the time domain becomes constant in space\nThis paper focuses on the time progression of the spatial kernel (TESD), which motivates the following generalization of STGP to include the time-dependence."}, {"section_title": "Generalized Spatiotemporal Gaussian Process", "text": "In this subsection, we generalize the standard separable STGP to introduce the timedependence to the spatial kernel C x while keeping the desirable structure in the joint kernel C z . We present two generalizations of the spatiotemporal kernel C z based on two different ideas. One is to replace the spatial kernel C x in (2) with a time-dependent analogy C x|t , resulting in a popular Kronecker product structure; the other stems from vectoring the standard (scalar) GP regression model (Lan et al., 2017) which yields a Kronecker sum structure.\nFirst, the observations in the model (1) can be viewed as taken from the following process\nThe mean function is given a STGP prior f (z) \u223c GP(0, C z ) with\nThen the marginal covariance for y becomes\nwhere I x (x, x ) = \u03b4(x = x ), and I t (t, t ) = \u03b4(t = t ) with \u03b4(\u00b7) being the Dirac function.\nThe most informative part C x|t \u2297 C t bares a Kronecker product structure.\nOn the other hand, Lan et al. (2017) vectorize the standard GP regression model\nand introduce the time-dependence in C \u03b5 by replacing it with C(t). In this case, the spatial dependence is coded in the parametric covariance matrix (which itself evolves along time); while the temporal evolution is modeled by various non-parametric GPs. The marginal covariance is\nA fully non-parametric generalization could be\nwhich has the following marginal covariance for y with a Kronecker sum structure:\nThe generalized STGP models can be summarized in the following unified form\nNote the above two models specify different structures for the marginal covariance C y = C y|m + C m . As both models can incorporate the time-dependence for the spatial kernel through C x|t , C y in model I is in general non-sparse but more expressive, while C y in model II is sparse but less expensive compared with model I. However as detailed in Section 2.5, when modeling TESD, it will be wasteful for model I to impose complex and unnecessary structure on the blocks off the main diagonal. See Figure 1 for an illustration of these structures in C y and the (red) blocks that describe TESD."}, {"section_title": "Construction of Time-Dependent Spatial Kernel", "text": "In both models (13), a time-dependent spatial kernel C x|t is needed to effectively characterize TESD. In this subsection, we construct C x|t through dynamically varying eigenvalues of the spatial kernel C x . Figure 1 : Joint kernels C y specified by model I with a Kronecker product structure (left) and model II with a Kronecker sum structure (right). Blocks in red illustrate the temporal evolution of spatial kernel.\nFirst, the centered (spatial) GP GP(0, C x ) is determined by its covariance kernel C x which defines a Hilbert-Schmidt integral operator on L 2 (X ) as follows.\nDenote {\u03bb 2 , \u03c6 (x)} as the eigen-pairs of T Cx such that T Cx \u03c6 (x) = \u03bb 2 \u03c6 (x). Then {\u03c6 (x)} serves as an orthonormal basis for L 2 (X ). By Mercer's theorem, we have the following representation of the spatial kernel C\nwhere the series converges in L 2 norm. To introduce the time-dependence to the spatial kernel, therefore denoted as C x|t , we let eigenvalues {\u03bb 2 } change with time and denote them as {\u03bb 2 (t)}. Without imposing any parametric constraints, we model \u03bb (t) in L 2 (T ) using another Gaussian process for each \u2208 N and treat it as a random draw:\nTo ensure the well-posedness of the generalization, we make the following assumption 1 1 It can be realized almost surely by e.g. setting \u03bb (t) = \u2212s/2 \u03bb0(t) with s > 1 and \u03bb0(t) \u223c GP(0, I).\nUnder this condition we can have the series representation of the time-dependent spatial kernel C x|t as in Mercer's theorem (15):\nWith this representation, for any t \u2208 T , {\u03bb 2 (t)} can be interpreted as eigenvalues of the integral operator T C x|t as in (14) with C x replaced by C x|t . Assumption 1 essentially requires that the trace of C x|t is finite in L 1 (T ). Using this we can define the joint kernel C z := C x|t \u2297 C t in model I in a symmetric way:\nwhere we will still denote C 1 2 x|t C 1 2 x|t as C x|t for simplicity. And we can also define the likelihood kernel C y|m = C x|t \u2297 I t in model II as follows:\nIn this paper, we assume the locations {x i } are fixed. When the locations change with time, e.g. moving players in a game, they become a (vector) function of time, denoted x(t).\nThen it is more natural to define the time-dependent spatial kernel by simply substituting al., 2012) . We will treat this case in another paper."}, {"section_title": "Properties", "text": "Now we can prove the well-posedness of (19) and (20). "}, {"section_title": "Proof. See Appendix A.", "text": "We formalize the definition of the temporal evolution of spatial dependence (TESD).\nDefinition 2 (TESD). The temporal evolution of spatial dependence (TESD) of a spatiotemporal process y(x, t) is the spatial covariance conditioned on a common time\nIt is straightforward to verify the format of TESD for the generalized STGP.\nProposition 2.2. If the process y is according to the generalized STGP model (13) with kernel definitions (18) (19), then we have the following conditional GPs in the space domain y(\u00b7, t)|C x|t \u223c GP(0, C y|t )\nwe consider a Gaussian process \u03bb(t) and denote its associated reproducing kernel Hilbert space (RKHS) as H. Define the contraction rate as follows\nLet p be a centered (assume m \u2261 0 for simplicity) Gaussian model, which is uniquely determined by its covariance\nFor a fixed spatial basis {\u03c6 }, the model density p is parametrized by \u03bb, hence denoted as p \u03bb . Denote P (n) \u03bb := \u2297 n j=1 P \u03bb,j as the product measure on \u2297 n j=1 (X j , B j , \u00b5 j ). Each P \u03bb,j has a density p \u03bb j with respect to the \u03c3finite measure \u00b5 j . Define the average Hellinger distance as d 2 n,\n. Note they are iid in model I and independent but not identically distributed (inid) in model II. Let n = I \u2227J be the minimal of the sizes of discretized spatial domain (I) and temporal domain (J). Then we can have the following posterior contraction about C x|t in model II, which generalizes Theorem 2.2 of Lan et al. (2017) .\nzero-mean tight Gaussian random element in 2,s (L \u221e (T )) and P (n) \u03bb = \u2297 n j=1 P \u03bb,j be the product measure of Y (n) parametrized by \u03bb. If the true value \u03bb 0 \u2208 \u0398 is in the support of \u03bb, and \u03b5 n satisfies the rate equation\nRemark 2. The observations Y (n) in model I are iid however the likelihood model is determined by C y|m = \u03c3 2 \u03b5 mI x \u2297 I t which does not contain \u03bb, thus the posterior of \u03bb in model I cannot contract to the correct distribution. Therefore, model I cannot be used to learn TESD, mainly conveyed in C x|t . See more details in the next subsection."}, {"section_title": "Comparison of Two Kernels", "text": "Before concluding this section, we compare these two spatiotemporal kernels with different structures as follows:\nthat, despite of the time-dependence in the spatial kernel, model I with the Kronecker product structure in the covariance kernel is the most popular model for spatiotemporal data (Hartikainen et al., 2011; Luttinen and Ilin, 2012; Sarkka and Hartikainen, 2012; Sarkka et al., 2013; Lindstr\u00f6m et al., 2013; Liu, 2013; Niu et al., 2015; Senanayake et al., 2016; Todescato et al., 2017) .\nFrom the modeling perspective, it is not completely natural for model I to assume iid noise. Apart from the mean function sufficiently modeled with the Kronecker product kernel a priori, the noise processes could possibly be inter-correlated over time among the components but this is not correctly reflected in model I. It does not affect modeling the mean function with an expressive prior kernel, however model I suffers from modeling the covariance function due to the lack of structure in the likelihood kernel \u03c3 2 \u03b5 I x \u2297 I t . To make it worse, with more and more data, the posterior of the covariance function may not contract to the true value as it becomes dominated by the likelihood. This is verified in the numerical simulation in Section 4.2. On the other hand, model II puts balanced structures on the prior and likelihood kernels so that it becomes a good trade-off between the two. What is more, the posterior concentrates on the likelihood with the kernel structure C x|t \u2297I t that could correctly capture TESD (See more numerical details in Section 4.2).\nConsidering the computation complexity of the two models, model II has significantly lower complexity than model I. As illustrated in Figure 1 , if we are only interested in TESD as illustrated by the blocks in red -each block being a spatial covariance matrix evolving along the time, it is wasteful to model the blocks off the main diagonal (not in red) with complicated structures in model I. On the contrary, model II puts enough details in the main diagonal blocks (in red) for describing TESD and yet the resulting joint kernel is highly sparse. When discretizing the space-time domain, both C I y and C II y become matrices of size IJ \u00d7IJ with C 1 2 x|t C 1 2 x|t and C x|t as the most computation-intensive part respectively. The latter has a matrix spectral decomposition inherited from that of the spatial kernel thus many calculations can be simplified with the Sherman-Morrison-Woodbury formula and the matrix determinant lemma; however it does not apply to the former. See more details in Appendix B.3. In the following we discuss the posterior inference by MCMC and various predictions for the mean and covariance functions."}, {"section_title": "Posteriors and Predictions", "text": "Suppose we are given the spatiotemporal data D := {Z, Y}, and there are K independent trials in the data Y :\nConsider the model (13) with hyper-parameters which are in turn given priors respectively, summarized as follows\nwhere the likelihood kernel C y|m and the prior kernel C m are specified in (13). For simplicity we let {\u03bb (t)} be iid draws from GP(0, Ct) a priori. The data will force their posteriors to decrease as \u2192 +\u221e thus the series in the posterior of C x|t converge (See more numerical evidence in Section 5.3). Denote\nWe adopt the Metropolis-Within-Gibbs scheme and use the slice samplers (Neal, 2003; Murray et al., 2010) for the posterior inference. More details can be found in Appendix B.\nWith the posteriors we can consider the following various prediction problems at new points (x * , t * ), (x, t * ) or (x * , t):"}, {"section_title": "Prediction of Mean", "text": "We only consider the prediction m(x * , t * )|D because the other two predictions m(x * , t)|D, m(x, t * )|D are sub-problems of it. The following proposition gives the predictive distribution of the mean function m(x, t).\nProposition 3.1. Fit the spatiotemporal data D = {Z, Y} with the model (24). Then given a new point z * = (x * , t * ) we have"}, {"section_title": "Prediction of Covariances", "text": "Now we consider the prediction of covariances. There are two types of prediction of particular interest. The first one, C x|t * (x, x )|D, evolves the spatial dependence among existing locations to other (future) times t * . The second one, C x|t (x, x * )|D, extends the temporal evolution of spatial dependence to new neighbors x * . The latter is also exclusive to the proposed fully nonparametric model. The semi-parametric methods of dynamic covariance modeling (Wilson and Ghahramani, 2011; Fox and Dunson, 2015; Lan et al., 2017) with a covariance matrix (instead of a kernel) for the spatial dependence do not have this feature.\nBoth predictions have practical meaning and useful applications. For example, the former could predict how the brain connection evolves during some memory process, or in the progression of brain degradation of Alzheimer's disease. With the latter we could extend our knowledge of climate change from observed regions to unobserved territories."}, {"section_title": "Evolve Spatial Dependence to Future Time", "text": "Note from the definition (18), we know that C x|t is a function of dynamic eigenvalues {\u03bb (t)} with fixed spatial basis {\u03c6 (x)}. Therefore, the prediction of the kernel C x|t in the time direction can be reduced to predicting \u03bb (t * )|D as follows. Denote \u03bb := \u03bb (t).\nwhere p(\u03bb (t * )|\u03bb ) is the standard GP predictive distribution. We can use the standard GP predictive mean and covariance to predict and quantify the associated uncertainty for (\u03bb (t * )|\u03bb (s) ) 2 with \u03bb (s) \u223c p(\u03bb |D), and then take average over all the posterior samples to get an approximation of \u03bb 2 (t * )|D. Therefore, C x|t * (x, x )|D can be obtained/approximated by substituting \u03bb 2 (t) with \u03bb 2 (t * )|D in (18)"}, {"section_title": "Extend Evolution of Spatial Dependence to Neighbors", "text": "Recall that in the definition of C x|t , the fixed basis {\u03c6 (x)} is taken from the eigenfunctions of the spatial kernel C x . To extend C x|t as a function of time to other locations based on existing knowledge informed by data, one could predict the basis at a new position, namely \u03c6 (x * ), using its known values \u03c6 := \u03c6 (X) as in the conditional Gaussian:"}, {"section_title": "Simulation", "text": "In this section, we focus on the study of a simulated example of spatiotemporal processes. We compare two models with different kernel structures in modeling and predicting mean and covariance functions. In particular we illustrate that model II with the Kronecker sum structure is more effective and efficient in characterizing TESD compared with model I with the Kronecker product structure. "}, {"section_title": "Model Setup", "text": "Now we consider the following simulated spatiotemporal process with the spatial dimension d = 1.\nNote that conditioned on time, we have the following true spatial covariance function (TESD)\nTo generate observations, we discretize the domain by dividing X into N x = 200 equal subintervals and T into N t = 100 equal subintervals. Setting x = 0.5, t = 0.3, xt = \u221a x t \u2248 0.39 and \u03c3 2 \u03b5 = 10 \u22122 , we generate 20301 data points {y ij } over the mesh grid. Such random process can be repeated for K trials and one of them is plotted in Figure 2 "}, {"section_title": "Model Fit", "text": "For simplicity we use a subset of these 20301 data points taken on an equally spaced sub-mesh with I = 5 and J = 101. Now we fit the resulting data with model I and model II in (13) respectively. In the discretized model (24) \nNow we fit model II with the graph-Laplacian based spatial kernel (30) to the PET brain images. The following setting for hyper-parameters is used: a = [1, 1, 1], b = [0.1, 1, 0 .1], m = [0, 0, 0] and V = [0.1, 1, 1]; however the results are not sensitive to the setting. The truncation number of Mercer's kernel expansion is set to L = 100. We run MCMC to collect 2.4 \u00d7 10 4 samples, burn in the first 4000, and subsample every other. The resulting 10 4 samples are used to obtain posterior estimates of mean functions M(t) and covariance functions C y (t). Figure 6 shows the fitted brain images at 6 scheduled scan times. The estimated brain images of patients in the control group (CN) have higher pixel values than the other two groups with bigger (blue) hollow regions. This can be seen more clearly from the summary of their estimated pixel values in Figure 7 . Note in Figure 7 , the highest quantiles (horizontal bars) of AD patients decrease with time, which could be an indicator of shrinking brains. Figure 8 compares the dynamic eigenvalues \u03bb (t) for different groups. They all have a decreasing trend as (on y-axis) becomes larger. However interestingly, they do not decrease monotonically in but rather fluctuate in a damped way. When gets close to L = 100, the magnitude of \u03bb (t) becomes small enough to be negligible. To investigate TESD in the PET brain images, we select a point of interest (POI) (marked as red cross) in the occipital lobe and plot the correlation between the brain region of interest (ROI) (chosen based on the pixel values above the 83.5% quantile) and the selected POI in Figure 9 . The spatial correlation evolves with time. We can see that the POI is highly correlated to its nearest region across all the time. It is also interesting to note the high correlation between the POI and some area in the frontal lobe. "}, {"section_title": "Model Prediction", "text": "Now we consider the predictions for both mean and covariance. The prediction of mean functions has been well studied in the literature. For simplicity we only illustrate the mean prediction in the time direction, i.e. m(x, t * )|D. We hold out 15% data for testing as indicated as the black dash-dot ticks in subplots of Figure 4 . Then we train the models (solid lines with light shaded regions) based on the rest 85% data (shorter gray ticks) and use them to predict the mean functions on the testing data set. The thick dash-dot lines are the predicted values of mean with dark shaded regions as the credible bands. In general, with more data (trials) we get better fit and prediction as they are closer to the truth, illustrated by the dash lines. In this example, model II gives similar results for the Lastly, we consider the prediction of covariance functions. Since model I cannot learn the covariance function well, we only test model II for the covariance prediction. Here we consider two types of prediction, namely TESD to future (Section 3.2.1) and TESD to neighbor (Section 3.2.2). The left column of Figure 5 shows the prediction of covariance functions in the time direction, i.e. C x|t * (x, x). Again we hold out 15% testing data (black dash-dot ticks) and train the generalized STGP model II based on the rest 85% data (shorter gray ticks). In general, the predicted values of the covariance in this case follow the trend of the fitted values with more certainty (narrower credible bands) when there are still nearby training points (interpolation) but become more uncertain (wider credible bands) while entering the 'no-data' region (extrapolation). Now we consider the second type of covariance prediction, TESD to a new neighbor (e.g. x * = 0.1), C x|t (x, x * ). Note, we do not have any data at this location x * = 0.1, nor can we fit model II for covariances involving this spatial point. The right column of Figure 5 shows that model II can extend TESD (thick dash-dot lines) to new locations with decent precision, in the reference to the truth, indicated by the dash lines. Again, such prediction becomes more credible (narrower bands) with increasing data information (trials). This prediction is of particular interest since we can 'lend' TESD information to new neighbors based on existing knowledge learnt from the data. It can enable us to discover new mechanism of some underlying process (e.g. global warming) at uninformed locations (e.g. unobserved territories).\nWe will apply the generalized STGP models to Alzheimer's neuroimaging data to study the association of brain regions in the progression of the disease in the next section. Since model I fails to characterize TESD and it involves much heavier computation, we will only focus on model II in the following.\nNow we hold out the data at the last time point for testing. For each group, the generalized STGP model II is built based on the rest of the data. Then we predict the mean and covariance functions of the brain image at the last time point. Figure 10 compares the actual individuals' brain images (upper row) with the predicted brain images (lower row) at the last time point. We can see that the prediction reflects the basic feature of the brain structure in each group. The predicted correlation between the brain ROI and the selected POI (marked as red cross) is plotted in Figure 11 . Note that the POI is less correlated to the middle region (thalamus), especially for the AD group. This is consistent with the fitted results shown in Figure 9 . There are more numerical results regarding the evolution of the variance and the connection of the brain images in Appendix C."}, {"section_title": "Longitudinal Analysis of Alzheimer's Brain Images", "text": "Alzheimer's disease (AD) is a chronic neurodegenerative disease that affects patients' brain functions including memory, language, orientation, etc. in the elder population generally above 65. It is one of the globally prevalent diseases that cost billions or even trillions of dollars every year. According to the World Alzheimer Report (Report, 2018) , there were about 50 million people worldwide living with dementia in 2018, and this figure is expected to skyrocket to 132 million by 2050. Yet the cause of AD is poorly understood. Longitudinal studies have collected high resolution neuroimaging data, genetic data and clinical data in order to better understand the progress of brain degradation. In this section, we analyze the positron emission tomography (PET) brain imaging data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) (ADN). A distinctive feature of these neuroimaging data is that they contain both spatial and temporal information that is not separable from each other. We will use the proposed generalized STGP: (i) to characterize the change of the brain structure and function over time; and (ii) to detect the spatial correlation between brain regions and describe its temporal evolution."}, {"section_title": "Positron Emission Tomography (PET) data", "text": "We obtain PET scans scheduled at the baseline, 6 months, 1 year, 18 months, 2 years and 3 years from the ADNI study. There are 51 subjects in this data set, with 14 Cognitively Normal (CN), 27 Mild Cognitive Impairment (MCI) and 19 Alzheimer's Disease (AD). Among these patients, only the MCI group has data at 18 months and the AD group is followed up till 2 years. These subjects are chosen because they participate the study for the longest time available so that we can investigate the longitudinal change of their brains over a sufficiently long period. Each subject takes fludeoxyglucose (FDG), an analogue of glucose, as the biologically active tracer that could indicate the tissue metabolic activity. PET brain image scans are obtained 30-60 minutes post-injection and then processed by co-regsitering to have the same position, averaging over 6 five-minute frames, standardizing to 160 \u00d7 160 \u00d7 96 voxel image grid, and smoothing to have uniform resolution. A detailed description of PET protocol and acquisition can be found at http://adni.loni.usc.edu. Though our proposed method can be applied to the whole 3d images, we focus on a (48-th) slice in the middle (horizontal section) and model the images of size 160 \u00d7 160. For each subject k at a specific time point t during the study, the response function y k (x, t) represents the pixel value of the image being read. Therefore the discrete data {y ijk } have dimension I \u00d7 J \u00d7 K, with I = 160 2 = 25600, J \u2208 {5, 6, 4} and K \u2208 {14, 27, 19}. To study the spatial dependency in these brain images, we need a kernel with discrete size 25600 \u00d7 25600, which is enormous if it is in a full matrix. In the following we introduce a spatial kernel based on the graph Laplacian. The resulting precision matrix is highly sparse and thus amenable to an efficient learning of TESD. Graph Laplacian has been a popular tool in the analysis of brain images (Shen et al., 2010; Ng et al., 2012; Hu et al., 2015; Huang et al., 2018) ."}, {"section_title": "Spatial Kernel Based On Graph Laplacian", "text": "Graph Laplacian, also known as discrete Laplace operator, is a matrix representation of a graph. It is a popular tool for image processing, clustering and semi-supervised/unsupervised learning on graphs (Chung et al., 1997; Smola and Kondor, 2003) . For a weighted graph G = (Z, W ) with Z being the vertices {x i } n i=1 of the graph and W being the edge weight matrix, the graph Laplacian L is defined as follows where \u03b7 \u03b5 is some distance function, e.g. Euclidean distance, D is called degree matrix, and x i \u223c x j means two vertices x i , x j connected with an edge. When w ij \u2261 1, W is also called adjacency matrix, denoted as A. If we assume x j \u2208 \u2126 are sampled i.i.d from a probability measure \u00b5 supported on the graph domain \u2126 with smooth Lebesgue density \u03c1 bounded above and below by positive constants, then L can be viewed as an approximation of the Laplace operator L in the following PDE:\nBased on the graph Laplacian, we can define the following discrete spatial kernel for the brain images (Dunlop et al., 2018) C x = (s n L + \u03c4 2 I) \u2212s , s n = 1\nwhere d is the spatial dimension, i.e. d = 2 for the chosen slice of brain images. Further assuming conditions (open, connected, and with smooth boundary) on the graph domain \u2126, Dunlop et al. (2018) prove that for s > d/2 and \u03c4 \u2265 0, Gaussian measure N (0, C) with C = (L + \u03c4 2 I) \u2212s is well-defined on the weighted Hilbert space L 2 \u00b5 . In another word, the graph-Laplacian based spatial kernel (30) is well-behaved for large graphs including the brain images we investigate with n = 25600 nodes.\nTo obtain the spatial kernel (30) for the analysis of PET scans, we first construct the graph Laplacian. On the 160 \u00d7 160 mesh grid, each node is connected to its (2w + 1) 2 \u2212 1 neighbors, where we can choose w = 1 for example. Depending on the location, some nodes may have 2w(w + 1) + w neighbors (on the edge) or w(w + 1) + w neighbors (at the corner). The resulting graph Laplacian matrix L has the size 25600 \u00d7 25600 but is highly sparse (with the density of non-zero entries 3.4864 \u00d7 10 \u22124 ). We also assume a hyper-prior for \u03c4 2 \u223c log \u2212N (m x , V x ) and fix s = 2 in this experiment. Then for given \u03c4 2 , we calculate the precision matrix C \u22121\nx based on (30). Hence the full sized covariance matrix C x is not directly calculated in the inference procedure."}, {"section_title": "Conclusion", "text": "In this paper, we generalize the separable spatiotemporal Gaussian process (STGP) to model the temporal evolution of spatial dependence (TESD) of the data. Instead of treating the space variable x and the time variable t as an extended variable z = (x, t) in the traditional non-separable STGP (Marco et al., 2015; Datta et al., 2016; Hyun et al., 2016) , we generalize STGP by varying the eigenvalues of the spatial kernel in the Mercer's representation. Two Bayesian nonparametric models are introduced and compared based on the generalized STGP with the joint kernels structured as the Kronecker product and the Kronecker sum respectively . The Kronecker sum structure is proved to be superior to the Kronecker product in characterizing TESD, from both modeling and computing perspectives. The advantage of the Kronecker sum kernel is demonstrated by a simulation study of spatiotemporal process. The generalized STGP model based on this structure is then applied to analyze PET brain images of Alzheimer's patients recorded for up to 3 years to describe and predict the change of the brain structure in these patients and uncover TESD in their brain regions in the past and for the future. The numerical evidence verifies the effectiveness and efficiency of the proposed model in characterizing TESD.\nThere is room to improve the current method. For example, it requires to have full data on the grid of the space and time and can be relaxed to handle missing data. The model can also be generalized to include covariates to explain the response variable (process) (Hyun et al., 2016) . Therefore, the estimation and prediction, e.g. of the brain images, can be done for each individual. We can further incorporate such information in the covariance and investigate the effect of covariates on TESD. They will be our future work.\nFor the longitudinal analysis of AD patients' brain images, subjects who are followed up for the whole study are limited in number. There are more dropped in the middle or missing scheduled scans from the ADNI (ADN) study thus discarded in the paper. Therefore, there could be large variance in the estimation e.g. of TESD, due to the insufficient data. As ADNI continues collecting more data, we hope they can facilitate more accurate description of TESD of AD brain images to aid the exploration of the mechanism behind this disease. Another important topic is the diagnosis of AD. It would be interesting to investigate TESD of the subjects' brains before and after being diagnosed as AD, which could shed more light on the reason of such disease. Figure 11 : Predicted correlation between the brain region of interest and a selected point of interest (red cross) for CN (left), MCI (middle) and AD (right) respectively. Sarkka, S. and Hartikainen, J. (2012) . Infinite-dimensional kalman filtering approach to spatio-temporal gaussian process regression. In Lawrence, N. D. and Girolami, M., Sarkka, S., Solin, A., and Hartikainen, J. (2013) . Spatiotemporal learning via infinitedimensional bayesian filtering and smoothing: A look at gaussian process regression through kalman filtering. IEEE Signal Processing Magazine, 30(4):51-61.\nSenanayake, R., O'Callaghan, S. T., and Ramos, F. T. (2016) . Predicting spatio-temporal propagation of seasonal influenza using variational gaussian process regression. In AAAI.\nShen, X., Papademetris, X., and Constable, R. (2010) . Graph-theory based parcellation of functional subunits in the brain from resting-state fmri data. NeuroImage, 50 (3):1027-1035.\nSingh, A., Ramos, F., Whyte, H. D., and Kaiser, W. J. (2010) . Modeling and decision making in spatio-temporal processes for environmental surveillance. In 2010 IEEE International Conference on Robotics and Automation, pages 5490-5497.\nSmola, A. J. and Kondor, R. (2003) . Kernels and regularization on graphs. In Sch\u00f6lkopf, B. and Warmuth, M. K., (19) and (20) are well defined positive definite kernel on Z = X \u00d7 T .\nProof of Proposition 2.1. We first prove both series (19) and (20) converge in L 1 (Z \u00d7 Z).\nNote for (19) we have\nThe convergence of series (19) and (20) follows by the dominated convergence theorem. Now we prove the positive-definiteness. \u2200f (z) \u2208 L 2 (Z), denote f (t) := X f (z)\u03c6 (x)dx. Then we have\nwhere the convergence can be shown as above.\nSimilarly we have\nTherefore we complete the proof.\nFor the dynamic spatial kernels\nFor \u03bb(t) \u2208 2,s (L \u221e (T )), we can bound the Hellinger distance, Kullback-Leibler distance and variance distance V (p 0 , p 1 ) = E 0 (log(p 0 /p 1 )) 2 in the following lemma with \u00b7 k,s ,\u221e for s > s. Notation ( ) means \"smaller (greater) than or equal to a universal constant times\".\nLemma A.1. Let p i \u223c N n (0, C i (t)) be Gaussian models for i = 0, 1, with {\u03bb 2 i, (t)} being the eigenvalues of C i . Assume for i = 0, 1\nThen we have\nFirst we calculate the Kullback-Leibler divergence\nConsider m i \u2261 0. By the non-negativity of K-L divergence we have for general C i > 0,\nTherefore we can bound K-L divergence\nwhere we use\n3) Now we calculate the following variance distance\nConsider m i \u2261 0 and we can bound it by the similar argument as (A.3)\nIt is easy to see that the centered variance distance V 0 = Var 0 (log(p 0 /p 1 )) 2 can be bounded\nLastly, the squared Hellinger distance for multivariate Gaussians can be calculated\nConsider m i \u2261 0. Notice that 1 \u2212 x \u2264 \u2212 log x, and by (A.2) we can bound the squared Hellinger distance using the similar argument in (A.3) zero-mean tight Gaussian random element in 2,s (L \u221e (T )) and P (n) \u03bb = \u2297 n j=1 P \u03bb,j be the product measure of Y (n) parametrized by \u03bb. If the true value \u03bb 0 \u2208 \u0398 is in the support of \u03bb, and \u03b5 n satisfies the rate equation \u03d5 \u03bb 0 (\u03b5 n ) \u2264 n\u03b5 2 n with \u03b5 n \u2265 n \u2212 1 2 , then there exists \u0398 n \u2282 \u0398 such that \u03a0 n (\u03bb \u2208 \u0398 n : d n,H (\u03bb, \u03bb n,0 ) > M n \u03b5 n |Y 1 , \u00b7 \u00b7 \u00b7 , Y n )) \u2192 0 in P (n) \u03bb n,0 -probability for every M n \u2192 \u221e.\nProof of Theorem 2.1. We use Theorem 1 of Ghosal and van der Vaart (2007) and it suffices to verify two conditions (the entropy condition 2.4, and the prior mass condition 2.5) as follows:\nsup \u03b5>\u03b5n log N (\u03b5/36, {\u03bb \u2208 \u0398 n : d n,H (\u03bb, \u03bb n,0 ) < \u03b5}, d n,H ) \u2264 n\u03b5 2 n (A.4) \u03a0 n (\u03bb \u2208 \u0398 n : \u03ba\u03b5 n < d n,H (\u03bb, \u03bb n,0 ) < 2\u03ba\u03b5 n ) \u03a0 n (B n (\u03bb n,0 , \u03b5 n )) \u2264 e n\u03b5 2 n \u03ba 2 /4 , for large \u03ba (A.5)\nwhere B n (\u03bb n,0 , \u03b5) := {\u03bb \u2208 \u0398 : 1 n n j=1 K j (\u03bb n,0 , \u03bb) \u2264 \u03b5 2 , 1 n n j=1 V j (\u03bb n,0 , \u03bb) \u2264 \u03b5 2 }, with \u0398 = 2,s (L \u221e (T )), K j (\u03bb n,0 , \u03bb) = K(P \u03bb n,0 ,j , P \u03bb,j ) and V j (\u03bb n,0 , \u03bb) = V (P \u03bb n,0 ,j , P \u03bb,j ). For each 1 \u2264 \u2264 N , define the coordinate rate function \u03d5 \u03bb 0 , (\u03b5 n, ) = inf h\u2208H : h\u2212\u03bb 0, \u2264\u03b5 n, Now let \u03b5 n, = 2 \u2212 \u2212s \u03b5 2 n for = 1, \u00b7 \u00b7 \u00b7 , n. Set \u0398 n = {\u03bb : \u03bb \u2208 B n, }, and N (\u03b5 n , \u0398 n , d n,H ) = max 1\u2264 \u2264n N (3\u03b5 n, , B n, , \u00b7 \u221e ). By Lemma A.1 and (A.7) , we have the following global entropy bound because d 2 n,H (\u03bb, \u03bb ) \u2264 \u03bb \u2212 \u03bb 1,s ,\u221e \u2264 \u03b5 2 n for \u2200\u03bb, \u03bb \u2208 \u0398 n .\nlog N (\u03b5 n , \u0398 n , d n,H ) \u2264 6Cn(2 \u2212 \u2212s \u03b5 2 n ) 2 \u2264 Cn\u03b5 4 n \u2264 n\u03b5 2 which is stronger than the local entropy condition (A.4). Now by Lemma A.1 and (A.9) we have \u03a0 n (B n (\u03bb n,0 , \u03b5 n )) \u2265 \u03a0 n ( \u03bb n,0 \u2212 \u03bb 1,s ,\u221e \u2264 \u03b5 2 n , \u03bb n,0 \u2212 \u03bb 2 1,s ,\u221e \u2264 \u03b5 2 n ) = \u03a0 n ( \u03bb n,0 \u2212 \u03bb 1,s ,\u221e \u2264 \u03b5 2 n ) \u2265 exp n =1 log \u03a0 ( \u03bb \u2212 \u03bb 0, \u221e < 2\u03b5 n, )\n\u2265 e \u2212 n 4 n =1 \u03b5 2 n, = e \u2212n\u03ba 2 \u03b5 4 n /4 , \u03ba 2 = n =1 2 \u22122 \u22122s Then (A.5) is immediately satisfied because the numerator is bounded by 1. Therefore the proof is completed.\nRemark 3. This theorem generalizes Theorem 2.2 of Lan et al. (2017) where the spatial domain has fixed dimension D. Therefore the Hellinger metric, KL divergence and variance are easier to bound (Lemma B.1). Note we do not have the complementary assertion as in Lemma 1 of Ghosal and van der Vaart (2007) thus the resulting contraction is only on \u0398 n , weaker than that in Theorem 2.2 of Lan et al. (2017) .\nonly requires log-posterior density and works well for scalar parameters, Figure 12 : Estimated variance of the brain images for CN (top row), MCI (middle row) and AD (bottom row) respectively."}, {"section_title": "C More Results of Longitudinal Analysis of Brain Images", "text": "In Section 5.3 we summarize the correlation between the brain ROI and POI. In fact, we have more results regarding TESD presented in different forms. Figure 12 shows the estimated variances of the brain images as functions of time. They are all small across different groups with small variation along the time. Comparatively, the thalamus and some part of the temporal lobe are more active than the rest of the brain. TESD of the brain images in the discretize field is a (covariance) matrix valued function of time t. At each time the covariance matrix is of size 25600\u00d725600. To better summarize it, we threshold the (25600 2 ) absolute correlation values at the top 10%, then we obtain the adjacency matrix (25600 \u00d7 25600) based on the nonzero values of the correlation. Finally we define the connection graph as the degree matrix of size 160 \u00d7 160 (row sums of the adjacency). Therefore, the value of each point on the connection graph indicates how many nodes are connected to it. Figure 13 plots the connection graphs of the brain ROI for different groups. For each of these connection graphs in Figure 13 , the truncation at any value yields a network of connected nodes that the most active. As seen from Figure  13 , these networks are most likely to concentrate on certain region in the temporal lobe. We successfully characterize the dynamic changing of such connectivity network of in these brain images. Note that the connectivity becomes weaker (thus the network of connected nodes become smaller) in the later stage for the MCI group (2 and 3 years) and the AD group (2 years), which could serve as an indicator of brain degradation. Figure 13 : Estimated connection of the brain images for CN (top row), MCI (middle row) and AD (bottom row) respectively."}]