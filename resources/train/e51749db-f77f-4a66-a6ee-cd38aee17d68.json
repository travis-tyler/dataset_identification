[{"section_title": "Abstract", "text": "Recent results in coupled or temporal graphical models offer schemes for estimating the relationship structure between features when the data come from related (but distinct) longitudinal sources. A novel application of these ideas is for analyzing group-level differences, i.e., in identifying if trends of estimated objects (e.g., covariance or precision matrices) are different across disparate conditions (e.g., gender or disease). Often, poor effect sizes make detecting the differential signal over the full set of features difficult: for example, dependencies between only a subset of features may manifest differently across groups. In this work, we first give a parametric model for estimating trends in the space of SPD matrices as a function of one or more covariates. We then generalize scan statistics to graph structures, to search over distinct subsets of features (graph partitions) whose temporal dependency structure may show statistically significant group-wise differences. We theoretically analyze the Family Wise Error Rate (FWER) and bounds on Type 1 and Type 2 error. On a cohort of individuals with risk factors for Alzheimer's disease (but otherwise cognitively healthy), we find scientifically interesting group differences where the default analysis, i.e., models estimated on the full graph, do not survive reasonable significance thresholds."}, {"section_title": "Introduction", "text": "Multivariate data analysis exploiting the conditional independence structure between features or covariates using undirected graphical models is now standard within any data analysis toolbox. When the data are multivariate Gaussian, the zeros in the inverse covariance (precision) matrix give conditional independences among the variables (Lauritzen, 1996) . Further, if the precision matrix is sparse, we can derive dependencies between features when the data are high-dimensional and/or the number of measurements are small. The estimation of a graphical model has been extensively studied and a rich literature is available describing its statistical and algorithmic properties (Koller and Friedman, 2009; Jordan, 1998) . For instance, the so-called graphical lasso formulation uses an 1 -norm penalty on the precision matrix and is widely used, and consistency properties in the large p regime (Cai et al., 2011; Friedman et al., 2008; Yuan, 2010) are now well understood. These formulations have also been extended to various transformations of Gaussian distributions (e.g., non-paranormal) using rank statistics (Liu et al., 2009; Xue and Zou, 2012; Liu et al., 2012) .\nCoupled and Temporal Graphical Models. Often, data come from two (or more) disparate sources or multiple timepoints. Within the last few years, a few proposals have described strategies for linking the sparsity patterns of multiple graphical models, e.g., using a fused lasso penalty (Danaher et al., 2014) (Yang et al., 2015) . Observe that if the data sources correspond to longitudinal acquisitions, we should expect the 'structure' to gradually evolve. Several authors have offered generalizations to address this problem: (Zhou et al., 2010) removes the assumption that each graph is independent and structurally 'close'. Instead, (Zhou et al., 2010) can be thought of as a growth model (McArdle and Bell, 2000) defined on these structures: they show how non-identically distributed graphs can be learned over time. Recently, the nonparametric procedure in (Qiu et al., 2015) extends these ideas to handle multiple sources, each with multiple samples.\nThe ideas in the literature so far to \"couple\" multiple graphical model estimation modules are mostly nonparametric. While such a formulation offers benefits, in many estimation problems, parametric models may be more convenient for downstream statistical analysis, particularly for hypothesis testing (Hardle and Mammen, 1993; Geer, 2000; Roehrig, 1988) . Given that the topic of coupled graphical models, by itself, is fairly recent, algorithms for parametric estimation of temporal or coupled Gaussian graphical models have not yet been heavily studied. This will involve parameterizing trends in the highly structured nature of the 'response' variable (SPD matrices). We find that parametric formulations for manifold-valued data have been proposed recently (Kim et al., 2014; Cornea et al., 2016) . Because SPD matrices form a Riemannian manifold, algorithms that estimate a parametric model respecting the underlying Riemannian metric are more suitable in many applications as opposed to assuming a Euclidean metric on positively or negatively curved spaces (Xie et al., 2010; Fletcher and Joshi, 2007; Jayasumana et al., 2013) . We will make a few simple modifications (for efficiency purposes) to such algorithms and make use of the estimated parameters for follow-up analysis.\nFinding Group-wise Differences. Assuming that we have a black-box procedure to estimate a parametric model on the SPD manifold available, in many tasks, such an estimation is merely a segue to other analyses designed to answer scientifically meaningful questions. For example, we are often interested in asking whether the temporally coupled model estimated using the procedure above differs in meaningful ways across groups induced"}, {"section_title": "Characterizing Covariance Trajectories", "text": "Our main statistical testing framework, to be described shortly, needs an efficient means for calculating a \"trajectory\" of the feature-by-feature interaction graphs over time for the given longitudinal data. We now describe a scheme which offers this capability. Let X t \u2208 R nt,p be the design matrix of all n t samples at time t, where t \u2208 {1, . . . , T }, and T is the total number of distinct timepoints. We wish to capture the trends in the relationships between the features as a function of t. To evaluate the groupwise differences in changes of such interactions, we make use of the fact that these interactions are commonly captured by correlation or conditional independence, represented by the covariance matrix (with normalized features) and the precision matrix (the inverse of covariance matrix).\nHere we simply use the covariance matrix for each timepoint t to denote the interaction between features, C t = cov(X t ). Our goal now is to estimate the parameters of the function, t \u2192 C t . We may vectorize the covariance matrix and apply a linear model; its parameters will give the trajectory in \"vectorized covariance space\" as we scan through t. But these predictions are not guaranteed to be valid SPD matrices and even if a projection is performed to obtain a covariance estimate, distortions introduced by the process may be significant (Fletcher, 2013) . It is well known that classical vector space models tend to be suboptimal in the manifold setting (covariance matrices live on the SPD manifold) since they use Euclidean metrics which are defined in the ambient space. For manifold-valued data, Riemannian metrics are shown to be superior in many applications (Fletcher and Joshi, 2007; Banerjee et al., 2015; Jayasumana et al., 2013; Tuzel et al., 2007) , and are increasingly being deployed in machine learning/statistics. We will utilize an appropriate statistical model informed by the manifold-structure of the data and then derive a hypothesis testing procedure to detect groupwise difference in the changes of interactions between features in longitudinal analysis. To do so, we first summarize basic differential geometry notations (Do Carmo, 1992; Lee, 2012) and then describe our models. If desired, any other (efficient) manifold-valued linear model (Fletcher, 2013) can be substituted in; no change in the workflow is needed. A reader familiar with manifold regression algorithms may consider this module as a black-box and skip ahead to Section 3 which uses the parameter estimates from this procedure."}, {"section_title": "Riemannian Geometry", "text": "Let M be a differentiable (smooth) manifold in arbitrary dimensions. A differentiable manifold M is a topological space that is locally similar to Euclidean space and has a globally defined differential structure. A Riemannian manifold is a differentiable manifold M equipped with a smoothly varying inner product. The geodesic curve is the locally shortest path, analogous to straight lines in R p -this geodesic curve will be the object that defines the trajectory of our covariance matrices in SPD space. Unlike the Euclidean space, note that there may exist multiple geodesic curves between two points on a curved manifold. So, the geodesic distance between two points on M is defined as the length of the shortest geodesic curve connecting two points. The geodesic distance helps in measuring the error of our trajectory estimation (analogous to a Frobenius or 2 norm based loss in the Euclidean setting). The geodesic curve from y i to y j is parameterized by a tangent vector in the tangent space anchored at y i with an exponential map Exp(y i , \u00b7) : T y i M \u2192 M. The inverse of the exponential map is the logarithm map, Log( The left and right figures represent two linear models on the SPD(p) manifold. Points x i in the tangent space are our covariate or predictor, and points y i in the manifold space represent SPD(p) matrices. In our regression setting, we wish to minimize the error (brown curves) between the estimation and the sample points. Because each linear model has a different base point, the trajectories cannot be directly compared as in the Euclidean setting.\nmove us back and forth between the manifold and the tangent space. For completeness, Table 1 shows corresponding operations in the Euclidean space and Riemannian manifolds. Separate from the above notation, matrix exponential (and logarithm) are simply exp(\u00b7) (and log(\u00b7)). Finally, parallel transport is a generalized parallel translation on manifolds. Given a differentiable curve \u03b3 : I \u2192 M, where I is an open interval, the parallel transport of v 0 \u2208 T \u03b3(t 0 ) M along curve \u03b3 can be interpreted as the parallel translation of v 0 on the manifold preserving its length and the angle between v(t) and \u03b3. The parallel transport of v from y to y is \u0393 y\u2192y v."}, {"section_title": "Riemannian Manifold Regression", "text": "Several regression models for manifold-valued data have been proposed recently, a majority of which are nonparametric (Jayasumana et al., 2013; Banerjee et al., 2015) . Because of the longitudinal nature of our dataset (and recruitment considerations in neuroimaging studies), sample sizes do not exceed a few hundred participants (typically much smaller). We have found that generally, in this regime, parametric methods are better suited and also offer other benefits for downstream applications. Next, we will give a simple parametric model for this problem. Let x and y be vectors in R p and R p respectively.\nDefinition 1 (Standard GLM.) The Euclidean multivariate multilinear model is\nwhere \u03b2 0 , \u03b2 i and the error are in R p and x = [x 1 . . . x p ] T are the predictor variables.\nHenceforth, we will use the terms covariate and predictor interchangeably to describe those specific features we wish to control for in our model (e.g., time-points in our experiments). For manifold-valued data, we adapt the formulation proposed by (Kim et al., 2014) .\nwhere\nThis formulation generalizes (1), by replacing the intercept \u03b2 0 and each vector \u03b2 j for a covariate with a base point b \u2208 M and a geodesic basis V j \u2208 T b M respectively. The geodesic basis V j at b parameterizes a geodesic curve Exp(b, V j x j ). Intuitively, this model is a 'generalized' linear model with the inverse exponential map Exp \u22121 (or logarithm map Log) as a 'link' function (Kim et al., 2014; Cornea et al., 2016) . When the covariate/predictors are univariate, we will obtain a single geodesic curve, modeled via the so-called Geodesic Regression (Fletcher, 2013) ."}, {"section_title": "Efficient Estimation of Trajectories", "text": "The objective in (2), can be solved by both gradient descent (Fletcher, 2013; Kim et al., 2014) and MCMC methods (Cornea et al., 2016) . Unfortunately, these schemes can be expensive, especially when the dimension of the manifold is large. Further, if the algorithm needs to be run a large number of times, the computational footprint quickly becomes prohibitive. Motivated by these considerations, we use a so-called log-Euclidean approximate algorithm introduced in (Kim et al., 2014) with some adaptations, which requires mild assumptions on the manifold-valued data.\nRecall that in classical ordinary least squares (OLS), the regression curve goes through the mean of covariates and response variables, i.e., y \u2212\u0233 = \u03b2(x \u2212x). Similarly, we assume that geodesic curves go through the mean of response variables on the manifold. Then, the base point, or intercept, \"b\" in (2) can be approximated by the manifold-valued mean of the sample points, the Karcher mean (Karcher, 1977) . The propositions derived from (Kim et al., 2014) lead directly to the following.\nProposition 3 LetC be the unique Karcher mean of a sufficiently close set of covariance matrices that lie on a curve \u2126. ThenC \u2208 \u2126, and for some tangent vector V \u2208 TCM and each C, there exists x \u2208 R such that C = Exp(C, V x).\nThis allows us to bypass the fairly involved variational procedure to estimate the base point b.\nWith this approximation ofb via\u0233, the remaining variables to optimize are the tangent vectors V . We do so by taking advantage of log-Euclidean schemes. Once the base point is established as the Karcher mean, each data point on the manifold is projected into the tangent space at that point: Log(\u0233, y). These \"centered\" points\u1ef9 are now Euclidean, and if the covariates are centered as well (x), a closed form solution exists in the standard form of V =\u1ef9x (xx ) \u22121 .\nIn this setting, it is often assumed that two points y 1 , y 2 have a distance defined as d(y 1 , y 2 ) := Log(y 1 , y 2 ) y 1 \u2248 Log(b, y 1 ) \u2212 Log(b, y 2 ) b . However, on SPD manifolds with an affine invariant metric, each tangent space has a different inner product varying as a function of the base point b, i.e., u, v b := tr(b \u22121/2 ub \u22121 vb \u22121/2 ). This makes comparison of trajectories difficult without moving to tangent bundle formulations. This issue is discussed in some detail in (Muralidharan and Fletcher, 2012; Hong et al., 2015) . However, note that Remark 4 When the base point b is the identity I, then the inner product is exactly the Euclidean metric u,\nThis follows from the fact that u and v are symmetric matrices on SPD(p). We take advantage of this property through parallel transport. Specifically, we can bring all of the data to T I M which will allow for a meaningful comparison of two tangent vectors from different base points. Similar schemes have been used for projection on submanifolds in (Xie et al., 2010 ) and other problems (Sommer et al., 2014) . With a fast algorithm to compute (2) available, we can now accurately model longitudinal trajectories of covariances matrices. Our statistical procedure described next simply assumes the availability of some suitable scheme to solve the manifold-regression as defined in (2) efficiently and does not depend on particular properties of the foregoing algorithm."}, {"section_title": "Test Statistics for SPD(p) Trajectories", "text": "With an algorithm to construct a regression model for covariance matrix responses in hand, we can now describe a key component of our contribution: a test statistic which allows addressing the main question of interest: Is the progression/trajectory of covariance matrices (over time) different across two groups? In the standard two-sample testing problem, a hypothesis test is set up to check if the parameters of each group are significantly different:\nRecall that in a general linear model (GLM), when testing for mean group differences, the test parameters are the regression slopes from a standard GLM fit. In our setting, the parameters of interest are the population covariance trajectories estimated from the manifold regression in (2), see Fig. 1 . While the trajectories and the slopes are related, note that our parameters are estimated on the manifold. Two unique manifold trajectories, when projected as simple multivariate responses in Euclidean space, may not be significantly different under the GLM hypothesis testing framework, as has been observed by (Du et al., 2014) . Returning to our longitudinal trajectory formulation, we have the following na\u00efve Covariance GLM:\nDefinition 5 Let vec(C g,t ) be the vectorized covariance matrix at timepoint t for group g \u2208 {1, 2}. Then the na\u00efve Covariance GLM is defined as\nwith the slope \u03b8 = \u03b2 in the hypothesis test in (3), and vec(\u00b7) is the vectorized form of the input matrix.\nWith this model, our hypothesis testing reduces to a simple difference of slopes, which is well-studied in classical statistics literature.\nDefinition 6 (Seber and Lee, 2003) Let \u03b2 1 , \u03b2 1 be the multivariate slopes calculated from estimating (4). Then an \u03b1-level hypothesis test rejects the null hypothesis\nKnowing that the response space is structured, i.e., our covariance matrices lie on the SPD manifold, we seek a more appropriate test and corresponding test statistic which adequately captures this knowledge.\nObserve that we can directly apply the manifold regression in \u00a72 to solve for a linear model on the manifold. That is, we construct the manifold GLM as Definition 7 Let C g,t be the covariance matrix at timepoint t for group g \u2208 {1, 2}. Then the Longitudinal-Covariance GLM (LCGLM) is defined as\nwith b g and V g being the base point and tangent vector respectively, as described in \u00a72.\nBut instead of solving p(p \u2212 1)/2 independent regressions, now we must concurrently solve for the entire manifold-valued response variable. In this case, we cannot directly compare our trajectories because they lie in different tangent spaces. To accurately compare two tangent vectors, we must parallel transport both vectors to the same tangent space. Once they are both in the same space, we can construct a simple test statistic for the trajectory difference.\nRecall that the inner product at the Identity I coincides with the Euclidean metric. This can now be naturally interpreted as a difference of slopes, and together with a standard Euclidean Normal noise assumption yields the following hypothesis test.\nProposition 8 Assume that \u0393 b\u2192I V is normally distributed N (0, I). Then the statistic defined in (7) follows a \u03c7 2 p distribution with p degrees of freedom, and the threshold test in (6) is an \u03b1-level hypothesis for the covariate trajectory group difference."}, {"section_title": "Incorporating First-Order Differences", "text": "In many real world situations, first-order information in the data is often valuable in identifying group differences. Restricting our analysis to only the second-order interactions, i.e., covariances, may be inefficient (or sub-optimal) when the mean signal difference between groups is large. Our construction easily extends to these cases. Particularly, the product space over both means and covariances is in R p \u00d7 SPD(p)."}, {"section_title": "Remark 9", "text": "The typical GLM on the first order information is defined in the standard Euclidean space. So, computing the regression in the product space R p \u00d7 SPD(p) amounts to simply computing the regression on the first and second order statistics (mean and covariance) separately.\nThe above statement suggests that by applying the manifold regression to the covariances and the standard regression model for the means, we are directly solving the product space regression problem, incorporating both first and second order statistics. However, in these cases, the statistic defined above in (7) does not directly take into account the potential difference in means. However, given our Normal noise assumption we can easily invoke the standard Gaussian multivariate likelihood statistic for group differences.\nDefinition 10 Let\u03bc t ,\u03a3 t be the estimated mean and covariance from the standard linear model and our manifold-covariance GLM respectively. Then the Gaussian likelihood of our data X is\nwhere X t is the subset of our data collected at timepoint t. Additionally, we can define a standard likelihood ratio test statistic as:\nThis statistic is again \u03c7 2 p -distributed (Seber and Lee, 2003) , and an \u03b1-level hypothesis test for group difference analysis can be defined in the same way as above. While our manifold regression modeling is focused on the case of centered data (where the mean signal may not be significantly different between the groups), we use the product space construction, wherever appropriate, in experimental evaluations."}, {"section_title": "Localizing Group Differences for SPD(p) Trajectories", "text": "The above procedure provides a precise mechanism to derive a statistic from the group-wise covariance matrix trajectories. However, when the effect sizes are poor, any scheme operating on the trajectories of the full covariance matrix may still fail to identify group differences (as is the case in our experiments). To improve statistical power, localizing the process of computing the trajectories only to the relevant features is critical. To this end, we consider the following global hypothesis testing problem\nwhere \u03b2 denotes the slope and R is the region of the covariance matrix which only includes the relevant features, see Fig. 2 . It turns out that by adapting Scan statistics (Fan et al., 2012; Arias-Castro et al., 2011) , we will be able to exclude the effect of irrelevant regions of the covariance matrix in the calculated trajectories. By extending this concept to graphs, we obtain an algorithm to identify subsets of features of the covariance matrix which show group differences that are otherwise unidentifiable, in a statistically rigorous way."}, {"section_title": "Scan Statistics", "text": "Scan statistics are a valuable tool for structured multiple testing. In its simplest form, we can consider a setting where we place a window (or box) over a region R in an image and calculate a local statistic L R , e.g., an average or a response to a convolution filter. Then, the window can be raster scanned at various locations in the image (R) and the maximum over the set of local statistics can be called the scan statistic. Intuitively, if the image is assumed to be a Gaussian random field, we can set up a null hypothesis using a critical value and finding a statistically significant signal (i.e., regions) corresponds to comparing the local region-wise statistic with the critical value. Of course, there is flexibility in terms of specifying properties of the regions as described next.\nDefinition 11 Let R be the collection of all possible structured regions, and L R be some statistic over region R, a structured subset of R. The scan statistic is defined as\nRecent results in scan statistics show how size corrections can be used to increase detection power in multi-scale analysis with nice guarantees (Walther et al., 2010; Wang et al., 2016) . To utilize these ideas for our hypothesis test, we must extend scan statistics and these size corrections to a graph setting where the graph is induced by a sparse estimation of the precision matrix, e.g., graphical lasso (or any other algorithm of choice) over the features. To do so, structured regions R and a statistic L R on each region must be defined on the graph. Intuitively, in our case, L R must capture the \"difference\" in group-wise covariance trajectories. As we will describe shortly, it is in the context of this statistic where we utilize the LCGLM (6), which will be invoked at the level of individual regions R, one by one.\nLet G := (V, E) be a graph over the features (represented in the covariance matrix) with vertex set V and edge set E. We define the structured region R \u2286 G as a connected subgraph of G corresponding to the selection of those vertices as our feature subset (block of the covariance matrix, see Fig. 2 ). A natural question is whether such an enumeration is tractable if the number of connected subgraphs R is exponential. It turns out that if we make a mild assumption on the graph, the number of induced regions can be shown to be polynomially bounded. Further, it then naturally provides a size correction, the analog for a multiple testing adjustment.\nRemarks. In our motivating application, the group differences we seek to identify will involve a cohesive set of features that will be connected to each other, by definition (large changes in covariances indicate dependent features). Based on this observation, we assume that the true localized subgraph is a \"ball\" subgraph. The ball subgraph constructed with r = 1. These subgraphs with bounded radius act as the structured regions on which scan statistics can be applied.\nDefinition 12 A ball subgraph consists of nodes with a given radius r from a particular node (see Fig. 2 ). The collection of ball subgraphs is defined as\nwhere the ball subgraph B(v, r) : and d(v, v ) is the minimum length path connecting v and v .\nWith this assumption, it can be verified that we now only need to search a polynomially bounded set of regions."}, {"section_title": "Remark 13", "text": "The number of unique ball subgraphs in any graph G is bounded above by D|V|, where D is the diameter (longest chain) of the graph G.\nOn these regions (i.e., blocks of covariance matrix), we will invoke LCGLM to provide us a statistic L R . This is just the difference in slopes of the calculated manifold regression across groups in (7). We will iteratively obtain this statistic for distinct regions R and find subgraphs that differ in their trajectories across groups using a size correction for hypothesis tests. Let us revisit the standard linear model setting and assume that our slopes \u03b2 R g correspond to the subset of slopes from features in R, and\u03b2 R g is an estimate of that slope. In this case, we have the following statistic (see e.g. (Seber and Lee, 2003) ),\nwhere \u03a3 \u22121 R is the covariance matrix of\u03b2 R 1 \u2212\u03b2 R 2 . With a normal noise assumption, this covariance will be identity and the statistic would simply be the 2 -norm difference as in the classical analysis. To make the statistics comparable across different sizes, we use the standardized version of a \u03c7 2 |E(R)| distribution,\nWe can extend this analysis to our manifold setting.\nDefinition 14 For a given structured region R, the region-based LCGLM is written as\nwhere C R g is the covariance matrix subblock defined by features included in R for group g (t g is our univariate predictor, i.e., time).\nTo compare the group trajectories, we first parallel transport each tangent vector to the identity as described in \u00a72 and then compute the statistic in (7) given as\nI . In the case of the product space construction, we apply the test in (8) to the data subset corresponding to the features in region R, with the same correction as in (12).\nSummary. We now have a region-based statistic for the manifold regression setting that is approximately normally distributed N (0, 1), allowing effective comparison across differently-sized regions."}, {"section_title": "Size Correction", "text": "A final unresolved yet important issue is that we must correct L R based on the number of edges E(R) in R. This has a direct consequence on detection power. Observe that the normalization for size correction should be determined by the null distribution of L R , i.e., when there is no slope difference in the trajectories between groups. In order to derive a correction, we need to characterize the behavior of scan statistics within roughly similar regions, max R\u2208R(A) L R , where R(A) is the collection of region Rs with similar size as E(R),\nClearly, the behavior of max R\u2208R(A) L R depends on the \"complexity\" of R(A). A clear understanding of how similar subgraphs relate to each other leads directly to a correction tied to their relative sizes.\nTo investigate the complexity of R(A), we define the following quantities."}, {"section_title": "Definition 15", "text": "The distance between subgraphs R 1 and R 2 can be given as\nDefinition 16 Let the -covering number of R(A), denoted by N (A, ), be the smallest integer such that there is a subset R approx (A, ) of R such that\nwhere\nWe can verify that all regions in R(A) can be approximated by regions in R approx (A) with reasonably small error. From the definitions, notice that the complexity of R(A) is reflected by N (A, ). If N (A, ) is nicely bounded (as is the case here), scan statistics can be calculated very efficiently (Lemma 18).\nBefore stating this result, we make a mild assumption on our graph. For any ball subgraph, the edges around its center are not too sparse, compared to the edges in the outer region of the ball subgraph, i.e., hard on the inside, soft on the outside. This yields, (Right) Star graph that does not satisfy the property: from the center node the graph is \"too dense on the outside.\"\nAssumption 17 (Avocado) There exist constants S and H such that, for any r/2 \u2264 r \u2264 r and v \u2208 V,\nWe see that this assumption holds for many classes of graphs: a ring graph satisfies this condition when H = 1 and S = 1 and the 2-d lattice satisfies this condition when H = 1/4 and S = 2 (see Fig. 3 ). With this assumption, we have the following result for the -covering number N (A, ).\nLemma 18 Let |E| be the total number of edges in G. If (17) holds and A is given, then, for a constant C H,S which only depends on H and S in (17),\nThe proof of this result follows from our ball-subgraph construction and our Avocado assumption and provided in the Appendix. Intuitively, this result upper bounds the number of graphs that are necessary to search over to completely exhaust the search space of subgraphs. With this result, we can now construct a suitable size correction. Following the work of (Davies and Kovac, 2001 ) and (Wang et al., 2016) , we can increase the power of our test by using the following statistic:\nThe significance of this size correction is that we now have a single critical value for each candidate subgraph, regardless of the subgraph size. Our final test is defined as I[T * > q \u03b1 ], where q \u03b1 is the \u03b1-level quantile of T * under the null hypothesis (that no region is truly significant across groups). By construction, we can control the type 1 error at a specified \u03b1-level.\nUnder the alternative hypothesis of this framework, it is important to note that in many cases, large subgraphs that subsume smaller significant graphs may also have large test statistics, and our hypothesis test only indicates the existence of some significant region. To identify or localize the smaller subsets, we follow the procedure from (Jeng et al., 2010) , by beginning with the subgraph with the largest test statistic and iteratively removing overlapping subsets from the total set of subgraphs. This requires testing each regional/local statistic, (L R \u2212 2 log(|E|/|E(R)|)) against q \u03b1 . Under this procedure, we can control the weak family-wise error rate (wFWER) if we view our problem via the lens of multiple testing. The weak FWER is the probability of false discovery under the null hypothesis. To see that this is inherently controlled, note\nwhere F N is the number of false discoveries under the null hypothesis. With this correction at the group difference level, we completely avoid any multiple comparisons issues that would arise in the case of a test for each subgraph. In addition to controlling the false positive rate, we have the following guarantee on identifying truly significant regions under the normal noise assumption."}, {"section_title": "Theorem 19", "text": "If (17) holds and the number of edges in the candidate subgraph is larger than log 2 |E|, i.e.,\nthen the critical value q \u03b1 satisfies q \u03b1 = O(1).\nMoreover, as |E| \u2192 \u221e, if a subgraph R 0 obeys\nthen as |E| \u2192 \u221e,\nThe full proof of this result follows a generic chaining argument (see, e.g. (Talagrand, 2006) ) along with application of concentration inequalities and union bounds, and can be found in the Appendix. Summary. At a high level, this result directly characterizes the behavior of T * under the null hypothesis H 0 and the alternative hypothesis H 1 , respectively. We see that (22) implies that T * can roughly be seen as a constant under the null hypothesis, and under the alternative hypothesis when (23) is satisfied, the test based on T * is consistent, see (24)."}, {"section_title": "Workflow for conducting hypothesis tests on temporal trends of graphs", "text": "With these guarantees, our full workflow is as follows. First, we use an oracle procedure to generate a graph over our features that roughly captures the conditional independences. Any procedure that provides a conditional independence graph is sufficient. Next, for each ball subgraph over this graph, we compute the Longitudinal-Covariance GLM over these features for both groups, and compute the statistics outlined in \u00a73. We then compute the size-corrected statistic, and compare against the single critical value. For all regions that pass this threshold, we apply the procedure from (Jeng et al., 2010) . This workflow shows how to conduct hypothesis tests on temporal trends of large covariance matrices, with improved power and bounded Type 1 error. Additional implementation details can be found in the Appendix."}, {"section_title": "Localization Evaluation: Trends of Tobacco Usage Across Gender", "text": "We begin our empirical analysis of the model by first applying the subgraph localization procedure by itself (standalone), separate from our manifold regression scheme. In this case, our statistic is derived from only Generalized Linear Models (GLM) constructions, where the\u03b2 R g in equation (12) is the slope estimated from fitting standard first order linear models. Identifying the differentially varying subgraphs across groups in this way is similar to a simpler version of the planted clique identification problem (Arora and Barak, 2009) , where the clique we are trying to identify corresponds to those nodes whose slopes vary significantly across groups.\nData. The Center for Disease Control (CDC) provides extensive statistics regarding tobacco and alcohol usage across the US. This data has been collected systematically for the last few decades and is publically available (includes demographic information and gender). As a simple application of our proposed framework, we may pose the following question: which \"sub-groups\" of states tend to evolve differently in their correlation (pertaining to tobacco/alcohol usage) over time? Our framework extends easily to answer this question. In this setup, the oracle graph is simply the adjacency graph of the continental US naturally which will be used directly in our scanning procedure. For this dataset, we have direct observations of node measures: the percentage of males and females who reported smoking or drinking heavily in each state. Using gender as the group, we fit standard linear models for each candidate subgraph, and compute the difference of gender-wise slopes statistic as described above. In Figure 4 , we see the regions identified using our method, and interpret some of the tobacco usage findings here.\nIn the northeast, we see that women have reduced their tobacco usage at a significantly faster rate than men compared to the rest of the country. We suspect that this may be at least partly tied to the development of women's cigarette brands in the late 1960s and 1970s followed by subsequent aggressive public policy campaigns in the 1990s and 2000s to highlight health risks beyond pulmonary or cardiovascular diseases for women (e.g., infertility, reduced bone-density in post-menopausal women). We also see that state-wide indoor smoking bans were put in place in the Northeast ahead of many other states in the union. In the South, the trends among men and women also seems to differ significantly. (see Fig. 4 ). Apart from health factors, the group-wise differences in the group-wise trends may also be explained by a few reasons identified in a study in 2007 (Stehr, 2007) which found that as the state sales tax on cigarettes changed (increased), women were significantly more price elastic than men. Between 2006 and 2008, the cigarette tax increased dramatically for all of the 4 states identified except for Louisiana, whose tax rate has remained constant. Additionally, while Arkansas did increase their cigarette tax in 2009, they did not increase taxes in locations near borders shared with higher taxing states. These intricate relationships among states lend credibility to the fact that our scan statistics framework is indeed identifying interesting sub-regions, and suggests that the full covariance-trajectory pipeline may be more appropriate if effects beyond the means are relevant within an analysis."}, {"section_title": "Pipeline Evaluation on Simulations and Baby Name Trends Over Time", "text": "We next evaluate the ability of our entire analysis pipeline to identify group differences across temporally evolving covariance trajectories. In many existing analyses, the effect of the mean differences may be stronger than the effect of the interaction matrix. However, in cases where the mean signal is weak, we expect that the covariance effect will be important. To evaluate our model in this regime, we perform a set of simulation studies and also analyze a publicly available longitudinal dataset.\nSimulations. We randomly generate SPD matrices from a 'path' of 4 discrete points along the manifold, and use these data as population covariance matrices to generate 0-mean sample data. Table 6 shows the results of the hypothesis testing procedure with 50 features averaged over 100 runs, where both the true number of features with covariance trajectory differences, p t , and the number of samples per group, n, were varied. As expected, our recovery rate increases nicely as a function of the number of samples n and decreases as the size of region of change p t is increased when n is held constant. We compare our model to baseline methods that may be used in practice for the foregoing group difference hypothesis test. In standard applications, general linear models (GLMs) are often the first line of attack. When the covariates are assumed to be independent, a simple linear model as in (6) may be suitable. However, when the group difference is influenced by specific interactions between covariates, such linear models require additional care. A typical solution is to introduce pairwise interaction terms into the model -a choice between all possible interactions or specific interactions specified by an expert. The first model has problems since the number of samples n p 2 . In the second model, we depend completely on the user's choice of interactions, and must correct for multiple testing when testing different models, at least partly reducing the power of the final test. Figure 5 shows the value of our method over these models. For the interaction GLM case, we randomly select interaction terms to include in the GLM, with size p t (the ground truth number of variables in the interaction). In this way, we approximate the effect of an oracle specifying to the GLM which terms may describe the underlying interaction. We report the fraction of significance tests where a significance threshold of p \u2264 0.05 was found for each model, averaged over 100 runs. We see that our proposed scheme consistently achieves near-perfect results in terms of the percentage of null hypotheses that were correctly rejected (i.e., there was a significant group-difference signal). The power of scan statistics on graphs is particularly evident in the needle in haystack setting where the true differential signal is small (p t \u2264 8) and the sample size is small to medium. When the sample size is large and p t is also large, the standard linear model with additional interaction terms starts to approach the statistical performance of our algorithm.\nLongitudinal trends in Baby Names. In addition to the simulations above, we report results from a simple analysis of how male/female baby names evolve over time over the last century. The United States Social Security Administration provides a publicly available dataset listing the frequency of the top 1000 baby names in each state for the last 106 years. We evaluate our model in this context to examine which \"sub-group\" of states tend to evolve (or change) in their \"name agreement\" (or correlation) over time between boy names and girl names. Here, rather than calculating a sample covariance at each timepoint, we calculate a rank correlation matrix instead. For example, if two neighboring Gulf Coast states, say Georgia and Alabama, substantially agreed on both boys and girls names in the period following the second World War, but gradually this agreement declined over time for girls (but not boys), we expect that our scan statistics on graphs hypothesis test will segment out this differential signal (in slope trends) from the planar graph induced by the states sharing a border. Shown in Figure 6 are the regions identified using our method, applied on only the rank correlations for the top 10 names for both genders per state per year. Each highlighted region indicates a sub-group where their \"trends of correlation (or agreement/disagreement)\" in preferred baby names over the last century varies between boys and girls. For states not identified by our model (in gray), we can conclude that the state-to-state name preference-interactions may have still evolved over time but we have insufficient statistical evidence to conclude that such trends (slopes) are different between boys and girls."}, {"section_title": "Identifying Differentially Covarying Features in Preclinical", "text": "Alzheimer's Disease\nWe now describe experiments and results focused on the key motivation of this work -to facilitate analysis of a longitudinal study of individuals at risk for Alzheimer's disease (AD) where the statistical signal is weak (with small to medium sample sizes). We describe the dataset details followed by the analysis and then interpret our conclusions in the context of scientific results that have been published in the literature in aging and dementia.\nStudy background. We analyzed data from a cohort of individuals who have been longitudinally tracked for at least three visits over multiple years, as part of an ongoing study (since 2001) to understand the disease processes in the brain before an individual exhibits signs of cognitive decline due to Alzheimer's Disease (AD) (Sager et al., 2005) . The study, Wisconsin Registry for Alzheimer's Prevention (WRAP) is among the largest of its kind in existence, focused on \"preclinical\" AD, i.e., when the individuals are still cognitively healthy, offering a window into the early disease processes where treatments, drugs and interventions are likely to be most effective. WRAP and its ancillary studies acquire neuroimaging data (MRI, PET with different tracers, diffusion MRI) and various clinical test scores, genetic and demographic data as well as clinical measures such as Cerebrospinal Fluid (CSF). Our analysis seeks to understand subtle group-wise differences in longitudinal patterns of dependencies between these measures at this early stage of the disease.\nDataset. The dataset consisted of 114 subjects with imaging data from at least two types of imaging modalities: Positron emission tomography and diffusion weighted Magnetic Resonance (MR) images. Positron emission tomography (PET) images were used to calculate, using well-validated pre-processing pipelines, the mean amyloid-plaque load (an important biomarker for AD) in 16 different anatomical regions of interest in the brain. Amyloid plaque is known to be an AD-related pathology and generally precedes onset of cognitive symptoms. Separately, diffusion tensor MR imaging (DTI) data were processed and used to calculate both Fractional Anisotropy (FA) and Mean Diffusivity (MD) in 48 distinct regions (Mori et al., 2008) . DTI images provide information about structural connectivity between gray matter regions in the brain. In addition to these 108 (48 \u00d7 2 + 16) image-derived features, we also included in the analysis the participant's scores on a battery of cognitive tests, known to be correlated with various neuropsychological functions (Lezak, 2004) . Differences were evaluated on various groupings of the subjects which were, for the most part, based on known results in the literature. Specifically, gender, APOE (Apolipoprotein E) genotype and amyloid positivity (based on thresholding the amyloid plaque summaries) have all been evaluated as significant in AD studies (Racine et al., 2014 ) but often such analyses involve a population covering a broader disease spectrum where the signal is much stronger.\nIs analysis of second order statistics necessary? In Figure 7 , we present histograms detailing the distribution of two critical cognitive tests, stratified across various groups of scientific interest. Evaluating these distributions were the key motivation for our exploration into the methods described in the paper. Small differences in means across groups regardless of grouping selection (i.e., stratification variable), and the saturation that occurs at the ceiling of cognitive test scores and other preliminary experiments conducted by us suggest that standard analyses are not sensitive enough to identify subtle higher-order differences. "}, {"section_title": "Results for Group difference analysis for individuals with imaging data", "text": "We now describe, one by one, the components of the largest feature subset discovered for each stratification scheme and highlight the main scientific findings. In most cases, we provide a brief scientific interpretation of the results for the interested reader. Additional details and results are available in the appendix. A) Graph Scan Statistics on slope differences across gender. The most significant (based on region-score) subset identified by the gender grouping was between the FA DTI measurement in the left cingulum gyrus as well as the scores on the Rey Auditory Verbal Learning Test (RAVLT). In recent AD research, gender has been identified as a factor in the progression of various pathology measures (e.g., incidence and prevalence of AD is higher in women (Fratiglioni et al., 1991; Rimol et al., 2010) ), and has contributed to a formal NIH notice (NOT-OD-15-102). However, we note that previous work in the field has not identified gender-related differences when looking only at diffusion measures in the cingulum (Lin et al., 2014) . Our algorithm successfully identified longitudinal changes in interaction between these variables which supports the earlier results, and provides some evidence that as men and women age, their cognitive decline as measured by RAVLT manifests differently in relation to the cingulum gyrus.\nB) Graph Scan Statistics on slope differences across genotype. Next, we stratified the cohort based on the genotype known to be most closely linked with AD, i.e., the APOE (Apolipoprotein E) gene (Corder et al., 1993 ) -we inherit one APOE allele from each parent; having one or two copies of the e4 allele increases a person's risk of getting AD Amyloid Load (PiB Positivity) Table 4 : Group difference across Amyloid Load (PiB Positivity) whereas the rarer e2 allele is associated with a lower risk of AD. Using this stratification, we obtain a low-risk and an at-risk group of individuals. Here, we identified amyloid-load regions within the medial and lateral parietal lobes and find that in the \"low-risk\" group, the covariances between Digit Span and Stroop Color-Word scores (attention and concentration scores) and amyloid load moves from strongly negative towards 0 as a function of age (Table 3 ). In the \"at-risk\" group (APOE4), however, we find that as a function of age, the features become more and more positively correlated. Existing studies have shown that the accumulation of amyloid is significantly different across APOE4 gene expression (Mormino et al., 2014) , and our results provide some evidence that the expression of the genotype may interact with cognitive scores as well, even at this early stage of the disease, when the individuals in our cohort are cognitively healthy. The sets of features showing a differential signal are presented in Table 3 .\nC) Graph Scan Statistics on slope differences across amyloid load positivity. As briefly described above, amyloid load is an important biomarker for AD. For our analysis, amyloid (or PiB) positivity is calculated using the mean amyloid PiB measures across all brain regions using a PiB PET image scan of the participant. When we used this measure for stratification (threshold was set at 1.18, following (Darst et al., 2017 )), our model identified fifteen of the sixteen PiB regions that were input to the model when the density of the oracle graph was set to be high. This result is as expected, but interestingly we find that controlling for the linear combination of the features (through centering), the residual error still has significant signal with the PiB positivity measure, indicating that amyloid burden interactions across brain regions plays a very important role in AD progression (Hardy and Selkoe, 2002; Hardy and Higgins, 1992; Tanzi and Bertram, 2005; Jack Jr et al., 2010) . When the sparsity of the oracle graph was increased, however, four neighboring regions, the left and right corticospinal tract and the left and right cerebral peduncle were identified on both PiB and DTI measures (supported by the literature (Douaud et al., 2011) ), together with Part A of the Trail Making Test (see Table 4 ) which happens to be used in AD diagnosis (Albert et al., 2011) . This suggests that changes in atrophy within these regions, as measured by DTI, co-occur with changes in amyloid burden. Additionally, because these regions are highly correlated with rough and fine motor ability (Naidich et al., 2009), it seems plausible that amyloid positivity will lead to higher 'covariation' in the regions associated with a measure of fine motor speed, i.e., the Trail Making Test."}, {"section_title": "Results for for Group difference analysis for individuals with Cognitive Testing data", "text": "In addition to the dataset presented above, we apply our method to a much larger dataset consisting of approximately 1500 individuals with only cognitive testing data collected in a longitudinal manner. Each individual was administered these tests for between two and three time-points, yielding approximately n = 4000 samples for our model. For each assessment, a conference of experts applied a diagnostic label indicating normal cognition or mild cognitive impairment. Using this binary classification, we can stratify our population for group difference analysis. We find that among many different significant subsets, the covariance trajectory among the scores on both parts of the Trail-Making Test and on all trials of the RAVLT test explain a significant group difference. These have previously been shown to be the most sensitive tests for early cognitive decline (Albert et al., 2001) . Table  5 displays the other tests identified by our algorithm, and additional experiments on this larger cohort can be found in the appendix."}, {"section_title": "Baseline.", "text": "In various experiments on this dataset, when the MMGLM procedure is performed for the entire feature set in totality (not utilizing any of the proposed ideas based on scan statistics), and the null distribution derived using permutation testing, the procedure yields no significance across any scientifically interesting group stratifications. This implies that the ability to search over different blocks of the covariance matrix is critical in identifying meaningful group differences in the trajectories, unavailable using alternate schemes. For instance, simpler strategies work well enough for datasets such as ADNI -which includes diseased subjects as well as controls -where the signal is stronger and even temporal modeling may be unnecessary. While the scientific results need to be interpreted with caution and reproducibility experiments on other similar datasets (both within the US and internationally) are in the planning phase, we believe that the ability to localize differences in these interaction patterns in a statistically rigorous manner is valuable and these findings can be investigated standalone, via more classical schemes (e.g., structural equation modeling)."}, {"section_title": "Conclusions", "text": "The analysis of datasets to identify where clinically disparate groups differ is pervasive in biology, neuroscience, genomics and epidemiological studies. We find that graphical models are an ideal tool to analyze high-dimensional data in these areas but have been sparingly used for the analysis of group-wise differences, especially in a longitudinal setting. Motivated by an application related to longitudinal analysis of imaging and clinical/cognitive data from otherwise healthy individuals who are at risk for Alzheimer's disease (AD), we show how a combination of manifold regression with a generalization of scan statistics to the graph setting yields tools that can be directly deployed. We present an efficient algorithm and develop the theoretical results showing the regimes where its application is appropriate. In various experiments, while the standard schemes are not sufficiently powered to detect the signal, our proposed formulation is able to detect meaningful group difference patterns, many of which have a clear scientific interpretation. We believe that these results are promising for the neuroimaging application described and other regimes where group-wise analysis is desired but the number of features is large.\nAppendix A. Technical Proofs."}, {"section_title": "A.1 Proof of Lemma 18", "text": "To remind the reader, this result was necessary in order to allow us to reduce the number of subgraphs (regions) that need to be evaluated over the graph. By bounding the covering number we have a guarantee that we do not need to consider an exponential number of subgraphs in order to find a localization. Proof To upper bound N (A, ), we first construct the -covering set of R(A) under metric d. To this end, we decompose R(A) into several disjoint sets\nfor j = 0, 1, . . . , 1 . Our strategy is to construct -covering set for each set R j (A). We only construct -covering set for R 0 (A); R j (A) (j \u2265 1) can be treated similarly. To construct the -covering set for R 0 (A), we denote by d v,r the largest positive number such that\nfor every v \u2208 V and r \u2208 N. Let D 1 the collection of d v,r such that B(v, r) \u2208 R 0 (A), i.e.\nand V 1 the collection of nodes such that B(v, r) \u2208 R 0 (A), i.e.\nWe pick up the largest number in\nThen we can pick up the largest number in D 2 , denote by d v 2 ,r 2 and\u1e7c 2 can be defined similarly. We can repeat the above process until D M and V M are empty for some M .We actually obtain a partition of V 1 ,\nwe are ready to prove the set\nis actually an -covering set for R 0 (A). To this end, it is equivalent to show that for arbitrary\nwhen v \u2208\u1e7c i . To show (26), we consider two cases where r > r i \u2212d v i ,r i /2 and r\nCombining above result, (25), and the definition of R 0 (A) yields\nBy definition of R 0 (A), we can get\nTherefore, (26) is proved and R 0 (A, ) is an -covering set for R 0 (A). The rest of the proof is to bound the cardinality of R 0 (A, ), i.e. M . Note that (17) implies there exists some constant D H,S only depending on H and S such that, for any v \u2208 V and r \u2208 N,\nThe last inequality is suggested by (17) and (25). The volume argument yields\nis obtained upon application of the above to each R j (A)."}, {"section_title": "A.2 Proof of Theorem 19", "text": "Before we are ready to prove Theorem 19, we need the following result:\nGaussian variable, i.e. N (0, 1) and a 1 , . . . , a d be a sequence of numbers. If\nwhere\nProof This is a direct extension of lemma 1 in (Laurent and Massart, 2000) to the negative case. We follow arguments similar to theirs. Let \u03c6(x) be the the logarithm of the Laplace transform of Y 2 i \u2212 1. For any \u22121/2 < x < 1/2,\nThis leads to\nWith the same arguments in (Laurent and Massart, 2000) , we could prove that\nThe other direction can be proved if we apply the same argument for \u2212Z.\nWith this in hand we proceed to prove Theorem 19. Proof In the following proof, C always refers to some constant, although its value may change from place to place. First, we prove (22). To this end, we prove concentration inequalities for L R for some R and L R 1 \u2212 L R 2 for some R 1 = R 2 . Since we assume the noise follows normal distribution, we have\nBy tail bound for \u03c7 2 random variables (see e.g. (Laurent and Massart, 2000) ), we can yield\nBy definition, L R 1 \u2212 L R 2 can be written as\nwhere Z i are independent random variable following distribution \u03c7 2 1 \u2212 1. Lemma 20 implies\nWe now proceed to prove (22) by applying a chaining argument (See (Talagrand, 2006) ) and concentration inequalities (30) and (31). Recall R app (A, ) is the smallest -covering set of R(A) and N (A, ) is the covering number of R(A). For any subgraph candidate R, we denote by\nFor any l * > l * , which will be specified later, we write max R\u2208R(A) L R into three parts\nNow, we bound these three terms above separately.\nTerm 1. Let l * = 2 log |E|. By concentration inequality (31) and union bound, we have\nfor x < log |E|. Therefore, we have\nTerm 2. Let l * = log log(|E|/A). Recall that the Avocado assumption (17) suggests that\nApplying concentration inequality (30) along with\nand the union bound, we have\nfor x < log |E|. Here we also apply condition (21). Therefore, we obtain\nfor x < log |E|.\nTerm 3. For any given l, application of concentration inequality (31), covering number condition (32), and the union bound yields,\nfor any x < log |E|. With another standard application of the union bound, we have\nPutting the three terms above together yields\nwhere we apply A log 2 |E| and the inequalities"}, {"section_title": "Now, we apply this bound to", "text": "This immediately suggests that q \u03b1 = O(1). Now, let's turn to the case when a subgraph is significant, that is to prove (24). Assume the significant region is R 0 . Using standard statistics we calculate the mean and variance of\nBy Chebyshev's inequality, we have\nIf (\u03b2\nby taking x as a sequence (e.g., log log(|E(R 0 )|)) which increases slow enough in (34). This leads to (24). If (\u03b2\nAppendix B. Implementation Details.\nThe workflow below describes one run of our model given a sparsity is specified for the oracle graph procedure.\n1. Oracle Graph. As noted in the main paper, we use graphical lasso (glasso) to generate an oracle graph, which allows to define structured regions (subgraphs) for scan statistics on graphs. Each element of the input matrix C in (35) for glasso is generated by calculating the slope for each position of the covariance matrix across the predictors for each group, and then taking the difference between the groups. The following inverse covariance estimation problem, glasso, is then solved using existing MATLAB interfaces to fast C implementations.\nWith sparsity parameter \u03bb, this procedure generates a reasonably sparse oracle graph.\n2. Candidate Subgraphs. With the oracle graph in hand, we then construct the set of all ball subgraphs, as defined in Section 4 of our main paper. By limiting ourselves to only a few (D|V |) subgraphs, we can perform scan statistics more efficiently.\n3. Characterizing the Null Distribution. In the case where we have few samples, we cannot directly apply the \u03c7 2 result. In these cases, the null distribution is then characterized using permutation testing over all candidate subgraphs. For each subgraph the input data is permuted a number of times to generate a good representation of the distribution at that subgraph. All normalized (but not size-corrected) scan statistics are then calculated for all permutations across all subsets and then combined in order to create the null distribution.\n4. Calculating the Test Statistic For a specific subset of the data, the scan statistic is calculated and corrected as described in Section 4 of the main paper, over the original grouping of the data. For each group, the logitudinal-covariance GLM (7) is computed using the procedures in \u00a72.3.\n5. Region Identification. We first identify all subsets whose statistic falls above the \u03b1-level threshold specified. Then the subset-collection procedure outlined in the main paper, developed by (Jeng et al., 2010) , is applied, and the non-overlapping critical regions are output."}, {"section_title": "Numerical Considerations", "text": "In practice, our empirical covariance matrices calculated on the sample data may not be positive definite. The matrix can be rank deficient when we do not have enough linearly independent samples. In addition, we may use a rank correlation matrix in its place, which also may not be PD. To resolve this issue, we project the empirical covariance matrix onto the symmetric-positive definite SPD(n) manifold. We first apply a standard procedure for transforming a symmetric matrix into a symmetric positive semidefinite (SPSD) one. As described in (Wu et al., 2005) , the standard eigenvalue thresholding, or clipping, \u03bb SP SD = max(0, \u03bb) is sensible since it provides the optimal projection of any matrix onto the SPSD manifold. Let \u03a3 = U \u039bU be the eigenvalue decomposition of the matrix \u03a3. The SPSD projection of \u03a3 is then proj SP SD (\u03a3) = U diag(max(\u03bb 1 , 0), . . . , max(\u03bb n , 0))U . And so to project to the SPD(n) manifold we can simply add some epsilon to each element of the diagonal:\nA remark on the term I will be useful here. We find that in experiments, numerical problems can arise if the smallest eigenvalue of the projected matrix is too small. By iteratively adding a small until the smallest eigenvalue is above our threshold, we ensure that the matrix is positive definite for the exponential and logarithmic maps. They are necessary for moving back and forth between the manifold and the tangent space."}, {"section_title": "A note on localization accuracy", "text": "In addition to simply checking whether or not we were able to correctly answer the hypothesis test group difference, it is important that if a significance is found, that it is found in the features that were originally used to generate the data. Using the same simulation setup as previous, we take the union of all subsets returned to be significant and check if each of the truly changing features p t are contained within the superset. In this particular case we find that our localization is only dependent on the graphical lasso procedure we use to generate the oracle graph. As long as the sparsity specified is large enough to include at least p t edges, we find that in every simulation where we find a significant difference, the features that express the difference are a superset of the true features.\nAppendix C. Preclinical AD Extended Details and Results."}, {"section_title": "Data and Variable Descriptions", "text": "In our neuroimaging experiments, a large number of our features describe specific and localized regions of the brain across multiple imaging modalities. Below we list and describe each of regions for each modality, and give a brief background on each of methods used to acquire the data. We also include the list of cognitive scores used in our analysis."}, {"section_title": "PET Imaging", "text": "Positron emission tomography has become an increasingly popular method of imaging the brain, specifically in the areas where cognitive decline can be strongly correlated with the specific matter being imaged. Pittsburgh compound B (PiB) was used as the tracer for these images, and the 16 mirrored (Left and Right) regions labeled below were selected as strongly correlated with the development and progression of Alzheimer's Disease. "}, {"section_title": "DTI Imaging", "text": "Diffusion tensor imaging is used to measure the restricted diffusion of water through and about regions of the brain. The 48 regions here are the aggregated measurements of total rates of diffusion for each voxel in that region. The two measurements, Fractional Anisotropy (FA) and Mean Diffusivity (MD) collectively well describe the diffusion in a specific region. The following is the full list of regions used in our analysis. Regions that spanned across both the left and right sides of the brain are indicated as such, and were treated as separate and independent in our analyses. Figure 9 : 17 major DTI fiber bundles measured using Fractional Anisotropy (FA). The 48 selected for our analysis include a subset of these, which have been identified as critical regions that signal the beginnings of cognitive impairment."}, {"section_title": "Cognitive Evaluations", "text": "The battery of cognitive test scores in our analysis included a breadth of evaluations chosen specifically for their coverage of various measures of cognition. Among all tests given to the cohort, the following 17 were selected by expert clinicians and researchers in the field for their coverage and their potential value in understanding trends across groups.\nWAIS-III. This is the most widely used IQ test. The Digit Span examination is specifically meant to evaluate the working memory of an individual. Participants are required to attempt to recall a series of numbers in order, both forwards and backwards. Letter-Number sequencing reflects a similar idea, but with a mix of both numbers and letters in increasing and alphabetical order, and is meant to be an indicator of more complex mental control (Wechsler, 2014) .\nRey Auditory Visual Learning Test. This test is specifically meant to evaluate all aspects of memory. Each trial evaluates a different type of memory, ranging from short-term and working memory to procedural and episodic memory. (Schmidt et al., 1996) .\nTrail-Making Test. This is a very popular test in providing information about executive function in the brain. The test consists of drawing lines among a randomly generated set of points in a square, where each point is labeled with a number. In Part A, participants must 'connect the dots' in increasing numerical order, and in Part B in increasing numerical and Amyloid Load (PiB Positivity) Table 7 : Group difference in gender alphabetical order. The score on the test is primarily dictated by the time in seconds it takes to complete the task for 25 of these 'dots.' More background information and normative analyses can be found in (Tombaugh, 2004) . Other tests similarly measure various cognitive function. While the Depression Scale Score did not crop up in any of our analyses here, it has been shown that depression is strongly associated with AD-related decline (Wragg and Jeste, 1989) ."}, {"section_title": "Detailed Imaging with Cognitive Tests Results", "text": "In the following tables we provide additional details of the statistical test we performed on the preclinical AD cohort. Each set contains a set of features found to display significant group difference (at the p \u2264 0.05 level) along the covariance trajectory divided by the group variable indicated.\nWhile some of these associations are well-known, few have been indicated as novel by AD researchers and clinicians, and to be of interesting value for further analysis."}, {"section_title": "Detailed results on larger cohort with only cognitive scores", "text": "We also applied our method to a larger cohort consisting of approximately 1500 subjects with varying temporal measurements on the battery of cognitive tests. Each individual had approximately 3 visits worth of data, and so our total number of measurements was approximately n = 4000. In addition to the groupings used above, we were able to use an algorithmic cognitive impairment (ACI) measure to further evaluate the model against a factor which is known to be group-separating. Below are the tabulated feature sets identified by our model for each of the group separations described in the main paper. In this case to increase interpretability of the results we limited our search to groups of 3-6 features. When grouped by genotype, the most indicative subset as shown in Table 11 . These tests are most closely associated with memory, and we see that no tests of executive function or spatial ability (Trail-Making or Clock Drawing) were included.\nIn addition to an algorithmic measure of impairment, a conference of expert clinicians and researchers have given each individual a clinical impairment diagnosis for each time they underwent the cognitive battery. Using this as a group separator, we found a large number of overlapping subsets that displayed significant group difference at the p = 0.05 level. These are shown in Table 12 . Trail-Making Test Parts A and B appeared in all identified subsets."}, {"section_title": "Algorithmic Cognitive Impairment", "text": "Set 1 Boston Naming Test Total Score RAVLT Learning Trial A1 Raw Score RAVLT Learning Trial A6 Raw Score Figure 10: Histograms of the Delayed Recall Scores for all time points for the \u223c 4000 individual measurements across different group separations. We note in particular that the results found from the genotype separation above would have been hard to identify since given the distributions are extremely overlapping (top left) for this particular separation.\nmultiply nested, exponential map and its inverse logarithm map are denoted by Exp(p, x) and Log(p, v) respectively, where p, x \u2208 M and v \u2208 T p M. They are usually denoted exp p (x) and log p (v) in most of differential geometry books. Separate from the above notations, matrix exponential, i.e, exp(X) := 1 k! X k , where 0! = 1 and X 0 = I and matrix logarithm are denoted by as exp(\u00b7) and log(\u00b7). Intrinsic mean. Let d(\u00b7, \u00b7) define the distance between two points. The intrinsic (or Karcher) mean is the minimizer to\u0233\nwhich may be an arithmetic, geometric or harmonic mean depending on d(\u00b7, \u00b7). A Karcher mean is a local minimum to (38) and a global minimum is referred as a Fr\u00e9chet mean. On manifolds, the Karcher mean satisfies This identity implies the first order necessary condition of (38), i.e.,\u0233 is a local minimum with a zero norm gradient (Karcher, 1977) . In general, on manifolds, the existence and uniqueness of th.e Karcher mean is not guaranteed unless we assume, for uniqueness, that the data is in a small neighborhood. vector in T c(t 0 ) M, where t 0 \u2208 I. Then, there exists a unique parallel vector field V along c, such that V (t 0 ) = V 0 . Here, V (t) is called the parallel transport of V (t 0 ) along c."}, {"section_title": "Geometry of SPD manifolds", "text": "Covariance matrices are symmetric positive definite matrices. Let SPD(n) be a manifold for symmetric positive definite matrices of size n \u00d7 n. This forms a quotient space GL(n)/O(n), where GL(n) denotes the general linear group (the group of (n \u00d7 n) nonsingular matrices) and O(n) is the orthogonal group (the group of (n \u00d7 n) orthogonal matrices). The inner product of two tangent vectors u, v \u2208 T p M is given by u, v p = tr(p \u22121/2 up\nThis plays the role of the Fisher-Rao metric in the statistical model of multivariate distributions. Here, T p M is a tangent space at p (which is a vector space) is the space of symmetric matrices of dimension (n + 1)n/2. The geodesic distance is d(p, q) 2 = tr(log 2 (p \u22121/2 qp \u22121/2 )). The exponential map and logarithm map are given as Exp(p, v) = p 1/2 exp(p \u22121/2 vp \u22121/2 )p 1/2 , Log(p, q) = p 1/2 log(p \u22121/2 qp \u22121/2 )p 1/2 .\nLet p, q be in SPD(n) and a tangent vector w \u2208 T p M, the tangent vector in T q M which is the parallel transport of w along the shortest geodesic from p to q is given by "}]