[{"section_title": "Abstract", "text": "Quality control (QC) of medical images is essential to ensure that downstream analyses such as segmentation can be performed successfully. Currently, QC is predominantly performed visually at significant time and operator cost. We aim to automate the process by formulating a probabilistic network that estimates uncertainty through a heteroscedastic noise model, hence providing a proxy measure of task-specific image quality that is learnt directly from the data. By augmenting the training data with different types of simulated k-space artefacts, we propose a novel cascading CNN architecture based on a student-teacher framework to decouple sources of uncertainty related to different k-space augmentations in an entirely self-supervised manner. This enables us to predict separate uncertainty quantities for the different types of data degradation. While the uncertainty measures reflect the presence and severity of image artefacts, the network also provides the segmentation predictions given the quality of the data. We show models trained with simulated artefacts provide informative measures of uncertainty on real-world images and we validate our uncertainty predictions on problematic images identified by human-raters."}, {"section_title": "Introduction", "text": "Quality control (QC) in magnetic resonance imaging (MRI) is the process of establishing whether a scan or dataset meets a required set of standards. QC typically relates to the acceptable level of image quality required for a particular task, which may be affected by acquisition noise, resolution, and/or image artefacts induced for instance by blood, motion, bias field, zipper or radio-frequency (RF) spikes. In MRI, a large variety of potential artefacts need to be identified so that problematic images can either be excluded or accounted for in further image processing and analysis. To date, the gold standard for identification of these issues remains labour-intensive visual inspection of the data (Graham et al., 2018) .\nHowever, with the current trend towards acquiring and exploiting very large imaging datasets, the time and resources required to perform visual QC have become prohibitive. Furthermore, as with other rating tasks, visual QC is subject to inter and intra-rater variability due to differences in radiological training, rater competence, and sample appearance (Sudre et al., 2019) . Some artefacts, such as those caused by motion, can also be diffi-cult to detect with visual QC, as their identification require the careful examination of every slice in a volume. These challenges have led to an increased interest in automated methods. In addition to the challenges inherent to visual QC, it is worth highlighting the task-dependent nature of a quality assessment: what may be deemed of acceptable quality for a radiological assessment may not be sufficient to provide reliable measurements for some of the automated analyses the image would undergo.\nIn this work, we propose to estimate task-specific uncertainty in a deep-learning framework and show this can be used as a measure of image quality for the task of segmentation. Furthermore, we show that we can decouple sources of uncertainty related to different imaging artefacts. Being able to decouple and identify sources of uncertainty can have a direct impact on the management of both clinical and research logistics. For instance, if the observed uncertainty is associated to acquisition artefacts (noise for instance) inspection of the scanner by an engineer may be required while if the uncertainty stems from subject motion, introducing ghosting or blurring, recall of the subject may be the appropriate path of action. In addition, in the context of population studies, producing the uncertainty associated with the desired measurement allows for appropriate statistical treatment of the samples while limiting the number of exclusions for quality reasons. Lastly, real-time indication of levels of uncertainty concerning a downstream task would enable radiographers to best manage session time and ensure repeat scans are taken as needed.\nContributions The main contributions of this work are three-fold:\n1. A general method of estimating MR image quality in a self-supervised manner."}, {"section_title": "2.", "text": "A novel cascading student-teacher CNN architecture and probabilistic loss function to decouple sources of uncertainty related to the task and different image artefacts.\n3. Validation of uncertainty predictions on problematic images identified by expert raters."}, {"section_title": "Related Work", "text": "In recent years, estimating uncertainty in the data/model has become increasingly recognised as an important step to enable the safe transition of automated methods into the clinical environment (Wang et al., 2018) , (Tanno et al., 2019) . In Bayesian Deep Learning, two main types of uncertainty are commonly distinguished: epistemic uncertainty which is uncertainty in the model, and aleatoric uncertainty which depends on noise or randomness in the data. In a similar approach to the one presented in this work, Prado et al. (Prado et al., 2019) have used a dual network to learn both epistemic and aleatoric uncertainty, assuming their independence. However, since the focus of this work is on the assessment of image quality, only the aleatoric uncertainty is considered here."}, {"section_title": "A Heteroscedastic Aleatoric Uncertainty Model", "text": "Aleatoric uncertainty is classically divided into two categories; the homoscedastic component is the task-dependent uncertainty, while the heteroscedastic component depends on the input data, reflecting for instance its quality, and can be predicted as a model output. Following this classification, task-specific image quality is modelled according to a heteroscedastic noise model. Heteroscedastic models assume that observation noise \u03c3 2 can vary with the input x, allowing for regions of the observation space to have higher noise levels than others . In this work, a single task, grey matter segmentation is considered; thus task uncertainty should be similar across experiments. The total predicted uncertainty is further assumed to be the sum of the task uncertainty (uncertainty given clean data) and the heteroscedastic uncertainties introduced as a function of image corruption. For the segmentation task, the problem is presented as a voxel-wise classification. The likelihood to maximise is defined as the softmax function of the scaled output logits i.e.\nis the output of a neural network with weights W and input x (Bragman et al., 2018) . The negative log likelihood is therefore:\nis the c th element of the output vector f W (x). Note, in practice for segmentation we compute the unscaled cross entropy loss of y, given by CE y\nFollowing the likelihood is approximated:\n. Substituting into Eq. 3 results in the weighted cross entropy loss that is used as the base loss function for all segmentation networks, as given by Eq. 4.\nDecoupling Multiple Uncertainties We use the weighted cross entropy loss in Eq. 4 to learn voxel-wise uncertainty, i.e. the network has two outputs: the segmentation y and the variance \u03c3 2 , as shown by the task network in Figure 1 . We adapt this loss function to predict multiple uncertainty quantities related to different aspects of image quality. The aim is to decompose the total predicted uncertainty \u03c3 2 into multiple uncertainty quantities related to the inherent difficulty of the task and to different types of image degradation or augmentation that may affect image quality. Our model assumes the variance sum law for independent events, such that the total predicted variance is the sum of the individual variances associated with each mode of augmentation, i.e. \u03c3 2 = \u03c3 2\nwhere \u03c3 2 t is the task uncertainty and \u03c3 2 i is the uncertainty due to the i th augmentation. This assumption of independence has the merit to simplify the model and ensure training tractability. While interactions with task uncertainty (task harder to learn with noisier data) or between degradation types (blurring and noise for instance) exist, their modelling would require the learning of new covariance terms and would greatly complexify both model and training procedure.\nSubstituting for \u03c3 2 in Eq. 4 results in the combined loss function in Eq. 5.\nFigure 1: Proposed network architecture: First, the task network is trained on clean images x t to predict segmentation y t and task uncertainty \u03c3 2 t . Then, for each augmentation i, a teacher network is trained to predict both the task uncertainty\u03c3 2 t and augmentation uncertainty \u03c3 2 i , where the output from the task network supervises with loss L(\u03c3 2 t , \u03c3 2 t ). Lastly, the combined student network is trained, where the uncertainty outputs from all previous teacher networks supervise its uncertainty predictions. The loss functions for each CNN are shown in corresponding colours.\nThe purpose of L combined is to enable the network to predict task uncertainty \u03c3 2 t and augmentation uncertainty \u03c3 2 i for each mode of augmentation. Since networks trained with this probabilistic loss function learn uncertainty in an unsupervised way, we do not have explicit labels for uncertainty. Therefore the network cannot determine how to decompose the total variance into separate quantities \u03c3 2 t and \u03c3 2 i by itself, i.e. without supervision. To do this, inspired by student-teacher networks (Tarvainen and Valpola, 2017) and knowledge distillation (Hinton et al., 2015) , (Xie et al., 2019) , we use a series of intermediate teacher networks where each network predicts the uncertainty due to a single augmentation, creating \"pseudo labels\" of uncertainty maps. By training these teacher networks sequentially, the output uncertainties from each intermediate network are used as self-supervising labels for the uncertainties predicted by a final combined student network. This is shown schematically in Figure 1 .\nThe training procedure can be summarised in the following three steps: 1) Train a teacher network on clean data only to predict the segmentation y t and task uncertainty \u03c3 2 t . 2) Freeze the task network and train a new network N i for each mode of augmentation we wish to decouple, where each augmented network predicts the segmentation y i , the task uncertainty and the noise uncertainty \u03c3 2 i for augmentation i. The output uncertainty from the first network acts as a pseudo label for the task uncertainty. 3) Freeze all previous networks and train a final student network with all modes of data augmentation to predict the task uncertainty and all possible augmentation uncertainties, where each uncertainty is supervised by the pseudo uncertainty labels from their respective teacher networks. For each network to learn the task uncertainty, an additional loss term L(\u03c3 t 2 , \u03c3 2 t ) is added to the weighted cross entropy loss to minimise the difference in uncertainty. Therefore, each augmentation network N i minimises a loss function L aug i given by Eq. 6,\nwhere, L(\u03c3 2 , \u03c3 2 ) = L 1 (\u03c3 2 , \u03c3 2 ) + L grad (\u03c3 2 , \u03c3 2 ) + \u03bbL SSIM (\u03c3 2 , \u03c3 2 ).\nL 1 is the L1 loss of the uncertainty and L grad is the L1 loss of gradient differences of the uncertainty maps in all three axes. The term L SSIM computes the 3D structural similarity (SSIM). The gradient and structural similarity losses help preserve the structure of the predicted uncertainty maps as the level of degradation increases. However, in the presence of severe image artefacts, the position, shape, appearance/visibility of the segmentation boundary can change causing SSIM to breakdown. Therefore L SSIM is down-weighted by \u03bb = 0.1. A simplified SSIM with a 3 \u00d7 3 \u00d7 3 average filter is used in our implementation."}, {"section_title": "k-Space Augmentation", "text": "During the training of each segmentation CNN, random k-space augmentations affecting image quality are applied on-the-fly. The types of augmentation are designed to emulate realistic MRI artefacts and are detailed below. Each augmentation is applied in the kspace by computing the 3D Fast Fourier Transform (FFT) of input volumes, modifying the k-space, and then computing the inverse 3D FFT, taking the magnitude image scaled between 0 and 1 as input to the network. All augmentations are applied at a rate such that roughly 50% of images seen by each CNN during training contain artefacts. The order in which k-space augmentations are applied is important to best reflect the MR imaging process, with for instance RF spike \u2212 \u2192 noise \u2212 \u2192 lowpass filter/k-space sampling. In addition to k-space augmentation, image rotation, scaling and flipping augmentations are applied, as well as bias field augmentation to account for variation in image intensity across samples. RF spike artefact is characterised by dark stripes over the image, as shown in Fig. 2 a) caused by the convolution of spikes in k-space of very high/low intensity during the FFT (Zhuo and Gullapalli, 2006) . For augmentation we sample uniformly its location in k-space which specifies the angle/frequency of stripes and its magnitude which defines the intensity. k-Space noise augmentation involves injecting Gaussian noise into the k-space, as shown in Fig. 2 b) , to model Rician noise in the image domain. The desired signal-to-noise ratio segmentation y, third: task uncertainty \u03c3 2 t , and fourth: corresponding augmentation uncertainty \u03c3 2 i . Best viewed zoomed-in on digital copy.\n(SNR) of the image is uniformly sampled between [-10dB, 30dB] and the corresponding amount of complex noise with zero mean and equal variance is added to the k-space. Blurring artefact can be observed when acquiring data at lower resolution along one axis prior to resampling. Low-pass filter applied by truncating the k-space along one randomly chosen axis as shown in Fig. 2 c) can simulate this effect. The width of the filter defines the equivalent downsampling ratio, which is uniformly sampled between 2\u00d7 and 12\u00d7. Aliasing/wrap artefact occurs when the imaging field of view (FOV) is smaller than the anatomy being imaged. This is retrospectively simulated by masking out k-space lines as shown in Fig. 2 d) . A proportion of k-space lines are either randomly masked uniformly, or at regularly spaced intervals, along a random axis that defines the wraparound direction."}, {"section_title": "Experiments", "text": "Proposed Network Architecture All CNNs use the updated U-Net architecture from (Isensee et al., 2019) as their base architecture implemented in NiftyNet (Gibson et al., 2017) . Each network is modified with two output heads, one for the segmentation\u0177 and one for the uncertainty \u03c3 2 , where the uncertainty output from each CNN has a different number of channels -one for each decoupled uncertainty prediction. We first train the task network to learn a single task uncertainty \u03c3 2 t . We then train N teacher networks for each augmentation i, where each CNN outputs two uncertainties \u03c3 2 t and \u03c3 2 i . Finally a combined network is trained to learn the task uncertainty and N augmentation uncertainties. Each CNN is trained for 30,000 iterations with a patch size of 96 3 and batch size of 2 across 4 GPUs with Adam optimiser (Kingma and Ba, 2015) and an initial learning rate of 10 \u22124 . Implementation Details As in , for numerical stability, each CNN is trained to predict log variance s := log \u03c3 2 instead of variance \u03c3 2 . In addition, the ex- Figure 4 : Left: Real-world artefact results. From left to right: artefact, segmentation y, task uncertainty \u03c3 2 t , and corresponding augmentation uncertainties \u03c3 2 i . Right: Total augmentation uncertainty over the data. View zoomed-in on digital copy. ponential mapping \u03c3 2 = exp(s) enforces valid positive uncertainty values. We add a small constant to the variance to ensure the proper definition of the weighted cross entropy loss, L N N = CE/(\u03c3 2 + ) + 1 2 log \u03c3 2 + . It's value controls the network's sensitivity to noise and the amount of output uncertainty. For instance, if is small, the network is penalised more for making mistakes, outputting higher uncertainty to compensate. If is large, the amount the network is penalised is limited by CE/ as \u03c3 2 \u2212 \u2192 0. Furthermore, smaller values of lead to training instability. Initially, is set to 0.05 and divided by 2 every time the loss plateaus until < 10 \u22123 . In parallel, at each of these steps, the learning rate is also halved.\nTraining Data for this work was obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) (adni.loni.usc.edu). Launched in 2003, ADNI attempts to assess whether medical imaging and biological markers and clinical assessment can be combined to measure progression of Alzheimer's Disease. For training we use 272 MPRAGE scans that were deemed to be artefact-free, split into 80% train, 10% valid and 10% test. We evaluate our model on simulated and real-world artefacts in the task of grey matter segmentation. Simulated Data A model trained with artefact augmentation was used to perform inference on the hold-out test set. Fig. 3 presents a selection of these results. For each sample, predicted segmentation, task and corresponding augmentation uncertainties are displayed. Areas of high uncertainty are generally in the artefacted regions. This enables us to quickly locate in the volume the image quality issue and judge its effect on the prediction by the level of uncertainty. Note, that in cases of heavy noise, the task uncertainty decreases as the signal is impaired, and therefore the model reverts back to the prior distribution.\nReal-world Data Using the model trained on synthetic artefacts, we performed inference on a dataset of real-world artefacts identified as low quality by expert raters. A selection of these are shown in Fig. 4 . We note that the uncertainty predictions generalise well to real-world artefacts and that the uncertainty is generally higher in the artefacted regions."}, {"section_title": "Entropic Uncertainty", "text": "The per-voxel variance values predicted by the network pertain to the logit space. While these values directly are useful indicators of uncertainty given the data, they relate to the actual uncertainty in the segmentation prediction via the entropy. Therefore, we can estimate a measure of uncertainty in our probabilities by computing the (a) (b) Figure 5 : Predicted confidence intervals at \u00b1\u03c3 on volume measurements from the grey matter segmentation for images with a) increasing noise and b) increasing blur.\nentropy as H = \u2212 c p c log p c , where p c = p(y = c|f W (x), \u03c3) are the scaled output logits from the network. Furthermore, using standard variance-entropy relations (Jee and Ratnaparkhi, 1986) we can obtain approximate error bars on segmentation measurements. In Fig. 5 , we use our uncertainty predictions to estimate the confidence interval for increasing noise and blurring in the image, relative to clean data. As this is a relative measure of uncertainty, i.e. even noise-free images will have some level of predicted uncertainty, we must transform the uncertainty by some amount to obtain calibrated error bars. In this work we simply compute the difference from the known uncertainty of a noise-free image, but these scaling parameters could be learnt from the data as in (Eaton-Rosen et al., 2019)."}, {"section_title": "Discussion", "text": "The aim of this work was to build a deep-learning framework capable of identifying and decoupling sources of uncertainty due to MRI artefacts that may affect a given segmentation task. We have shown that it is possible to obtain approximately decoupled uncertainties that reflect the presence (location and severity) of k-space artefacts. We have also shown that these uncertainties can be used to generate error bars on segmentation measurements. Limitations Our uncertainty predictions are limited by the fact that we are estimating uncertainty given the data p(y|x, \u03c3), but have not modelled the likelihood of the data itself p(x), achievable through methods such as autoencoders. Extrapolation problems also limit our ability to decouple uncertainty, as the introduction of extreme artefacts results in an unstable learning process. Lastly, we assume that the original data (used for training the task network) is artefact-free, possibly resulting in inflated task uncertainty estimates."}, {"section_title": "Conclusion", "text": "We have presented a method for estimating quality-induced task-specific uncertainty using a heteroscedastic noise model. Entirely self-supervised, the proposed model can approximately decouple and localise sources of uncertainty related to different MRI artefacts, thus automatically highlighting problematic areas affecting segmentation predictions. The method is general and may be applied to other automated image analysis processing tasks.\nGuotai Wang, Wenqi Li, Michael Aertsen, Jan Deprest, S\u00e9bastien Ourselin, et al. Aleatoric uncertainty estimation with test-time augmentation for medical image segmentation with convolutional neural networks. In Neurocomputing, 2018.\nQizhe Xie, Eduard Hovy, Minh-Thang Luong, and Quoc V. Le. Self-training with noisy student improves imagenet classification. arXiv preprint arXiv:1911.04252, 2019.\nJiachen Zhuo and Rao P. Gullapalli. Aapm/rsna physics tutorial for residents: Mr artifacts, safety, and quality control. Radiographics, 26 1: 275-97, 2006. "}]