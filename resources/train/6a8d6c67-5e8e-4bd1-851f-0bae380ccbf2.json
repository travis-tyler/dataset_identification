[{"section_title": "Abstract", "text": "Over the past decade a wide spectrum of machine learning models have been developed to model the neurodegenerative diseases, associating biomarkers, especially non-intrusive neuroimaging markers, with key clinical scores measuring the cognitive status of patients. Multi-task learning (MTL) has been commonly utilized by these studies to address high dimensionality and small cohort size challenges. However, most existing MTL approaches are based on linear models and suffer from two major limitations: 1) they cannot explicitly consider upper/lower bounds in these clinical scores; 2) they lack the capability to capture complicated non-linear interactions among the variables. In this paper, we propose Subspace Network, an efficient deep modeling approach for non-linear multi-task censored regression. Each layer of the subspace network performs a multi-task censored regression to improve upon the predictions from the last layer via sketching a low-dimensional subspace to perform knowledge transfer among learning tasks. Under mild assumptions, for each layer the parametric subspace can be recovered using only one pass of training data. Empirical results demonstrate that the proposed subspace network quickly picks up the correct parameter subspaces, and outperforms state-of-the-arts in predicting neurodegenerative clinical scores using information in brain imaging."}, {"section_title": "INTRODUCTION", "text": "Recent years have witnessed increasing interests on applying machine learning (ML) techniques to analyze biomedical data. Such data-driven approaches deliver promising performance improvements in many challenging predictive problems. For example, in the field of neurodegenerative diseases such as Alzheimer's disease and Parkinson's disease, researchers have exploited ML algorithms to predict the cognitive functionality of the patients from the brain imaging scans, e.g., using the magnetic resonance imaging (MRI) as in [1, 36, 40] . A key finding points out that there are typically various types of prediction targets (e.g., cognitive scores), and they can be jointly learned using multi-task learning (MTL), e.g., [6, 9, 36] , where the predictive information is shared and transferred among related models to reinforce their generalization performance.\nTwo challenges persist despite the progress of applying MTL in disease modeling problems. First, it is important to notice that clinical targets, different from typical regression targets, are often naturally bounded. For example, in the output of Mini-Mental State Examination (MMSE) test, a key reference for deciding cognitive impairments, ranges from 0 to 30 (a healthy subject): a smaller score indicates a higher level of cognitive dysfunction (please refer to [31] ). Other cognitive scores, such as Clinical Dementia Rating Scale (CDR) [11] and Alzheimer's Disease Assessment Scale-Cog (ADAS-Cog) [23] , also have specific upper and lower bounds. Most existing approaches, e.g., [22, 36, 40] , relied on linear regression without considering the range constraint, partially due to the fact that mainstream MTL models for regression, e.g., [2, 13, 36, 39] , are developed using the least squares loss and cannot be directly extended to censored regressions. As the second challenge, a majority of MTL research focused on linear models because of computational efficiency and theoretical guarantees. However, linear models cannot capture the complicated non-linear relationship between features and clinical targets. For example, [3] showed the early onset of Alzheimer's disease to be related to single-gene mutations on chromosomes 21, 14, and 1, and the effects of such mutations on the cognitive impairment are hardly linear (please refer to [19, 30] ). Recent advances in multi-task deep neural networks [26, 33, 38] provide a promising direction, but their model complexity and demands of huge number of training samples prohibit their broader usages in clinical cohort studies.\nTo address the aforementioned challenges, we propose a novel and efficient deep modeling approach for non-linear multi-task censored regression, called Subspace Network (SN), highlighting the following multi-fold technical innovations: V (1) !\"#$%&'() !*(+',-./ U (1) 0&1&2(+(1) !\"#$%&'( !\"#$ V (2) U (2) !\"#$ % 3&4(1)5\nx y\nx !\"#$ V (3) U (3) % 3&4(1)6\nx V (k) U (k) % 3&4(1)*\nx Figure 1 : The proposed subspace network via hierarchical subspace sketching and refinement.\n\u2022 It efficiently builds up a deep network in a layer-by-layer feedforward fashion, and in each layer considers a censored regression problem. The layer-wise training allows us to grow a deep model efficiently. \u2022 It explores a low-rank subspace structure that captures task relatedness for better predictions. A critical difference on subspace decoupling between previous studies such as [18] [28] and our method lies on our assumption of a low-rank structure in the parameter space rather than the original feature space. \u2022 By leveraging the recent advances in online subspace sensing [18, 28] , we show that the parametric subspace can be recovered for each layer with feeding only one pass of the training data, which allows more efficient layer-wise training. Synthetic experiments verify the technical claims of the proposed SN, and it outperforms various state-of-the-arts methods in modeling neurodegenerative diseases on real datasets."}, {"section_title": "MULTI-TASK CENSORED REGRESSION VIA PARAMETER SUBSPACE SKETCHING AND REFINEMENT", "text": "In censored regression, we are given a set of N observations\n, \u00b7 \u00b7 \u00b7 ,T }, can be cognitive scores (e.g., MMSE and ADAS-Cog) or other biomarkers of interest such as proteomics 1 . For each outcome, the censored regression assumes a nonlinear relationship between the features and the outcome through a rectified linear unit (ReLU) transformation, i.e.,\nReLU is defined by ReLU(z) = max(z, 0). We can thus collectively represent the censored regression for multiple tasks by:\nwhere W = [W 1 , . . . ,W T ] \u2208 R T \u00d7D is the coefficient matrix. We consider the regression problem for each outcome as a learning task. One commonly used task relationship assumption is that the transformation matrix W \u2208 R T \u00d7D belongs to a linear low-rank subspace U . The subspace allows us to represent W as product of two matrices, W = UV , where columns of U \u2208 R T \u00d7R = [U 1 , . . . ,U T ] span the linear subspace U, and V \u2208 R R\u00d7D is the embedding coefficient. We note that the output y can be entry-wise decoupled, such that for each component y i,t = ReLU(U t V x i + \u03f5). By assuming Gaussian noise \u03f5 \u223c N (0, \u03c3 2 ), we derive the following likelihood function:\nwhere \u03d5 is the probabilistic density function of the standardized Gaussian N (0, 1) and Q is the standard Gaussian tail. The full explanation of the likelihood function can be found in Appendix. \u03c3 controls how accurately the low-rank subspace assumption can fit the data. Note that other noise models can be assumed here as well. The likelihood of (x i , y i ) pair is thus given by:\nThe likelihood function allows us to estimate subspace U and coefficient V from data D. To enforce a low-rank subspace, one common approach is to impose a trace norm on UV , where trace norm of a matrix A is defined by A * = j s j (A) and s j (A) is the jth singular value of A. Since UV * = min U ,V 1 2 ( U 2 F + V 2 F ), e.g., see [18, 29] , the objective function of multi-task censored regression problem is given by:"}, {"section_title": "An online algorithm", "text": "We propose to solve the objective in (2) via the block coordinate descent approach which is reduced to iteratively updating the following two subproblems:\nAlgorithm 1 Single-layer parameter subspace sketching and refinement.\nRequire:\n, rank parameters \u03bb and R, Ensure: parameter subspace U , parameter sketch V Initialize U \u2212 at random for i = 1, . . . , N do // 1. Sketching parameters in the current subspace\nDefine the instantaneous cost of the i-th datum:\nand the online optimization form of (2) can be recast as an empirical cost minimization given below:\nAccording to the analysis in Section 2.2, one pass of the training data can warrant the subspace learning problem. We outline the solver for each subproblem as follows: Problem (P:V) sketches parameters in the current space. We solve (P:V) using gradient descent. The parameter sketching couples all the subspace dimensions in V (not decoupled as in [28] ), and thus we need to solve this collectively. The update of V (V + ) can be obtained by solving the online problem given below:\nV + can be computed by the following gradient update:\nwhere the gradient is given by:\nThe algorithm for solving (P:V) is summarized in Alg. 2. Problem (P:U) refines the subspace U + based on sketching.\nWe solve (P:U) using stochastic gradient descent (SGD). We note that the problem is decoupled for different subspace dimensions t = 1, . . . ,T (i.e., rows of U ). With careful parallel design, this procedure can be done very efficiently. Given a training data point Algorithm 2 Gradient descent algorithm for problem P:V.\nRequire: Training data (x i , y i ), U \u2212 , step size \u03b7, Ensure: sketch V Initialize V \u2212 at random. // 1. Perform gradient step and update the current solution of V. for t = 1, . . . ,T do\nCompute the gradient for y t :\n, the problem related to the t-th subspace basis is:\nWe can revise subspace by the following gradient update:\nwhere the gradient is given by:\nThe detailed derivation of the gradients can be found in Appendix. We summarize the procedure in Algorithm 1 and show in Section 2.2 that under mild assumptions this procedure will be able to capture the underlying subspace structure in the parameter space with just one pass of the data."}, {"section_title": "Theoretical results", "text": "We establish both asymptotic and non-asymptotic convergence properties for Algorithm 1. The proof scheme is inspired by a series of previous works: [14, 16-18, 27, 28] . We briefly present the proof sketch, and more proof details can be found in Appendix. At each iteration i = 1, 2, ..., N , we sample (x i , y i ), and let U i , V i denote the intermediate U and V , to be differentiated from U t , V t which are the t-th columns of U , V . For the proof feasibility, we assume that\nlies in a compact set. Asymptotic Case: To estimate U , the Stochastic Gradient Descent (SGD) iterations can be seen as minimizing the approximate cost 1\nwhere \u0434 \u2032 is a tight quadratic surrogate for \u0434 based on the second-order Taylor approximation around U N \u22121 . Furthermore, \u0434 can be shown to be smooth, by bounding its first-order and second-order gradients w.r.t. each U t (similar to Appendix 1 of [28] for k = 1, . . . , K \u2212 1 do // 1. Subspace sketching based on the current subspace using Algorithm 1:\n// 2. Expand the layer using the refined subspace as our new network:\nFollowing [16, 18] , it can then be established that, as N \u2192 \u221e, the subspace sequence {U i } N i=1 asymptotically converges to a stationarypoint of the batch estimator, under a few mild conditions. We can se-\naccording to the quasi-martingale property in the almost sure sense, owing to the tightness of \u0434 \u2032 ; 2) the first point implies convergence of the associated gradient sequence, due to the regularity of \u0434;\nNon-Asymptotic Case: When N is finite, [17] asserts that the distance between successive subspace estimates will vanish as fast as\ni , for some constant B that is independent of i and N . Following [28] to leverage the unsupervised formulation of regret analysis as in [14, 27] , we can similarly obtain a tight regret bound that will again vanish if N \u2192 \u221e."}, {"section_title": "SUBSPACE NETWORK VIA HIERARCHICAL SKETCHING AND REFINEMENT", "text": "The single layer model in (1) has limited capability to capture the highly nonlinear regression relationships, as the parameters are linearly linked to the subspace except for a ReLU operation. However, the single-layer procedure in Algorithm 1 has provided a building block, based on which we can develop an efficient algorithm to train a deep subspace network (SN) in a greedy fashion. We thus propose a network expansion procedure to overcome such limitation.\nAfter we obtain the parameter subspace U and sketch V for the single-layer case (1), we project the data points byx = ReLU(UV x). A straightforward idea of the expansion is to use (x, y) as the new samples to train another layer. Let f [k \u22121] denote the network structure we obtained before the k-th expansion starts, k = 1, 2, ..., K \u2212 1, the expansion can recursively stack more ReLU layers:\nHowever, we observe that simply stacking layers by repeating (3) many times can cause substantial information loss and degrade the generalization performance, especially since our training is layer-by-layer without \"looking back\" (i.e., top-down joint tuning). Inspired by deep residual networks [10] that exploit \"skip connections\" to pass lower-level data and features to higher levels, we concatenate the original samples with the newly transformed, censored outputs after each time of expansion, i.e., reformulatin\u1e21 x = [ReLU(UV x); x] (similar manners could be found in [41] ). The new formulation after the expansion is given below:\nWe summarize the network expansion process in Alg. 3. The architecture of the resulting SN is illustrated in Fig. 1 . Compared to the single layer model (1), SN gradually refines the parameter subspaces by multiple stacked nonlinear projections. It is expected to achieve superior performance due to the higher learning capacity, and the proposed SN can also be viewed as a gradient boosting method. Meanwhile, the layer-wise low-rank subspace structural prior would further improve generalization compared to naive multi-layer networks."}, {"section_title": "EXPERIMENT", "text": "The subspace network code and scripts for generating the results in this section are available at https://github.com/illidanlab/subspace-net."}, {"section_title": "Simulations on Synthetic Data", "text": "Subspace recovery in a single layer model. We first evaluate the subspace recovered by the proposed Algorithm 1 using synthetic data. We generated X \u2208 R N \u00d7D , U \u2208 R T \u00d7R and V \u2208 R R\u00d7D , all as i.i.d. random Gaussian matrices. The target matrix Y \u2208 R N \u00d7T was then synthesized using (1) . We set N = 5, 000, D = 200, T = 100 R = 10, and random noise as \u03f5 \u223c N (0, 3 2 ). Figure 2a shows the plot of subspace difference between the ground-truth U and the learned subspace U i throughout the iterations, i.e., \u2225U \u2212 U i \u2225 F /\u2225U \u2225 F w.r.t. i. This result verifies that Algorithm 1 is able to correctly find and smoothly converge to the underlying low-rank subspace of the synthetic data. The objective values throughout the online training process of Algorithm 1 are plotted in Figure 2b . We further show the plot of iteration-wise subspace differences, defined as Figure 2c , which complies with the o(1/t) result in our non-asymptotic analysis. Moreover, the distribution of correlation between recovered weights and true weights for all tasks is given in Figure 3 , with most predicted weights having correlations with ground truth of above 0.9.\nSubspace recovery in a multi-layer subspace network. We regenerated synthetic data by repeatedly applying (1) for three times, each time following the same setting as the single-layer model. A three-layer SN was then learned using Algorithm 3. As one simple baseline, a multi-layer perceptron (MLP) is trained, whose three hidden layers have the same dimensions as the three ReLU layers of the SN. Inspired by [24, 32, 34] , we then applied low-rank matrix factorization to each layer of MLP, with the same desired rank R, creating the factorized MLP (f-MLP) baseline that has the identical architecture (including both ReLU hidden layers and linear bottleneck layers) to SN. We further re-trained the f-MLP on the same data from end to end, leading to another baseline, named retrained factorized MLP (rf-MLP)."}, {"section_title": "Research Track Paper", "text": "KDD 2018, August 19-23, 2018, London, United Kingdom Table 1 evaluates the subspace recovery fidelity in three layers, using three different metrics: (1) the maximum mutual coherence of all column pairs from two matrices, defined in [5] as a classical measurement on how correlated the two matrices' column subspaces are; (2) the mean mutual coherence of all column pairs from two matrices; (3) the subspace difference defined the same as in the single-layer case 2 . Note that the two mutual coherencebased metrics are immune to linear transformations of subspace coordinates, to which the 2 -based subspace difference might become fragile. SN achieves clear overall advantages under all three measurements, over f-MLP and rf-MLP. More notably, while the performance margin of SN in subspace difference seems to be small, the much sharper margins, in two (more robust) mutual coherencebased measurements, suggest that the recovered subspaces by SN are significantly better aligned with the groundtruth.\nBenefits of Going Deep. We re-generate synthetic data again in the same way as the first single-layer experiment; yet differently, we now aim to show that a deep SN will boost performance over singlelayer subspace recovery, even the data generation does not follow a 2 The higher in terms of the two mutual coherence-based metrics, the better subspace recovery is achieved.That is different from the subspace difference case where the smaller the better, known multi-layer model. We compare SN (both 1-layer and 3-layer) with two carefully chosen sets of state-of-art approaches: (1) single and multi-task \"shallow\" models; (2) deep models. For the first set, the least squares (LS) is treated as a naive baseline, while ridge (LS + 2 ) and lasso (LS + 1 ) regressions are considered for shrinkage or variables selection purpose; Censor regression, also known as the Tobit model, is a non-linear method to predict bounded targets , e.g., [4] . Multi-task models with regularizations on trace norm (Multi Trace) and 2,1 norm (Multi 2,1 ) have been demonstrated to be successful on simultaneous structured/sparse learning, e.g., [35, 37] . 3 We also verify the benefits of accounting for boundedness of targets (Uncensored vs. Censored) in both single-task and multitask settings, with best performance reported for each scenario (LS + 1 for single-task and Multi Trace for multi-task). For the set of deep model baselines, we construct three DNNs for fair comparison: i) A 3-layer fully connected DNN with the same architecture as SN, with a plain MSE loss; ii) A 3-layer fully connected DNN as i) with ReLU added for output layer before feeding into the MSE loss, which naturally implements non-negativity censored training and evaluation; iii) A factorized and re-trained DNN from ii), following the same procedure of rf-MLP in the multi-layer synthetic experiment. Apparently, ii) and iii) are constructed to verify if DNN also benefits from the censored target and the low-rank assumption, respectively. We performed 10-fold random-sampling validation on the same dataset, i.e., randomly splitting into training and validation data 10 times. For each split, we fitted model on training data and evaluated the performance on validation data. Average normalized mean square error (ANMSE) across all tasks was obtained as the overall performance for each split. For methods without hyper parameters (least square and censor regression), an average of ANMSE for 10 splits was regarded as the final performance; for methods with tunable parameters, e.g., \u03bb in lasso, we performed a grid search on \u03bb values and chose the optimal ANMSE result. We considered different splitting sizes with training samples containing [40%, 50%, 60%, 70%, 80%] of all the samples. Table 2 further compares the performance of all approaches. Standard deviation of 10 trials is given in parenthesis (same for all following tables). We can observe that: (1) all censored models significantly outperform their uncensored counterparts, verifying the necessity of adding censoring targets for regression. Therefore, we will use censored baselines hereinafter, unless otherwise specified;\n(2) the more structured MTL models tend to outperform single task models by capturing task relatedness. That is also evidenced by the performance margin of DNN iii over DNN i; (3) the nonlinear models are undoubtedly more favorable: we even see the single-task Tobit model to outperform MTL models; (4) As a nonlinear, censored MTL model, SN combines the best of them all, accounting for its superior performance over all competitors. In particular, even a 1-layer SN already produces comparable performance to the 3-layer DNN iii (which also a nonlinear, censored MTL model trained with back-propagation, with three times the parameter amount of SN), thanks to SN's theoretically solid online algorithm in sketching subspaces.\nFurthermore, increasing the number of layers in SN from 2 to 20 demonstrated that SN can also benefit from growing depth without "}, {"section_title": "Experiments on Real data", "text": "We evaluated SN in a real clinical setting to build models for the prediction of important clinical scores representing a subject's cognitive status and signaling the progression of Alzheimer's disease (AD), from structural Magnetic Resonance Imaging (sMRI) data. AD is one major neurodegenerative disease that accounts for 60 to 80 percent of dementia. The National Institutes of Health has thus focused on studies investigating brain and fluid biomarkes of the disease, and supported the long running project Alzheimer's Disease Neuroimaging Initiative (ADNI) from 2003. We used the ADNI-1 cohort (http://adni.loni.usc.edu/). In the experiments, we used the 1.5 Tesla structural MRI collected at the baseline, and performed cortical reconstruction and volumetric segmentations with the FreeSurfer following the procotol in [12] . For each MRI image, we extracted 138 features representing the cortical thickness and surface areas of region-of-interests (ROIs) using the Desikan-Killiany cortical atlas [8] . After preprocessing, we obtained a dataset containing 670 samples and 138 features. These imaging features were used to predict a set of 30 clinical scores including ADAS scores [23] at baseline and future (6 months from baseline), baseline Logical Memory from Wechsler Memory Scale IV [25] , Neurobattery scores (i.e. immediate recall total score and Rey Auditory Verbal Learning Test scores), and the Neuropsychiatric Inventory [7] at baseline and future.\nCalibration. In MTL formulations we typically assume that noise variance \u03c3 2 is the same across all tasks, which may not be true in many cases. To deal with heterogeneous \u03c3 2 among tasks, we design a calibration step in our optimization process, where we estimate task-specific\u03c3 2 t using \u2225y \u2212\u0177\u2225 2 2 /N before ReLU, as the input for next layer and repeat on layer-wise. We compare performance of both non-calibrated and calibrated methods. Performance. We adopted the two sets of baselines used in the last synthetic experiment for the real world data. Different from synthetic data where the low-rank structure was predefined, for real data, there is no groundtruth rank available and we have to try different rank assumptions. Table 8 compares the performances between \u03c3 2 non-calibrated versus calibrated models. We observe a clear improvement by assuming different \u03c3 2 across tasks. Table 6 shows the results for all comparison methods, with SN outperforming all else. Table 5 shows the SN performance growth with increasing the number of layers. Table 7 further reveals the performance of DNNs and SN using varying rank estimations in real data. As expected, the U-shape curve suggests that an overly low rank may not be informative enough to recover the original weight space, while a high-rank structure cannot enforce as strong a structural prior. However, the overall robustness of SN to rank assumptions is fairly remarkable: its performance under all ranks is competitive, consistently outperforming DNNs under the same rank assumptions and other baselines. Qualitative Assessment. From the multi-task learning perspective, the subspaces serve as the shared component for transferring predictive knowledge among the censored learning tasks. The subspaces thus capture important predictive information in predicting cognitive changes. We normalized the magnitude of the subspace into the range of [\u22121, 1] and visualized the subspace in brain mappings. The the 5 lowest level subspaces in V 1 are the most important five subspaces, and is illustrated in Figure 4 . We find that each subspace captures very different information. In the first subspace, the volumes of right banks of the superior temporal sulcus, which is found to involve in prodromal AD [15] , rostral middle frontal gyrus, with highest A\u03b2 loads in AD pathology [21] , and the volume of inferior parietal lobule, which was found to have an increased S-glutathionylated proteins in a proteomics study [20] , have significant magnitude. We also find evidence of strong association between AD pathology and brain regions of large magnitude in other subspaces. The subspaces in remaining levels and detailed clinical analysis will be available in a journal extension of this paper."}, {"section_title": "CONCLUSIONS AND FUTURE WORK", "text": "In this paper, we proposed a Subspace Network (SN), an efficient deep modeling approach for non-linear multi-task censored regression, where each layer of the subspace network performs a multi-task censored regression to improve upon the predictions from the last layer via sketching a low-dimensional subspace to perform knowledge transfer among learning tasks. We show that under mild assumptions, for each layer we can recover the parametric subspace using only one pass of training data. We demonstrate empirically that the subspace network can quickly capture correct parameter subspaces, and outperforms state-of-the-arts in predicting neurodegenerative clinical scores from brain imaging. Based on similar formulations, the proposed method can be easily extended to cases where the targets have nonzero bounds, or both lower and upper bounds."}, {"section_title": "APPENDIX Explanation of Likelihood Function", "text": "We start from single task censored regression, also known as the Tobit model. Consider the assumption:\nThe likelihood function of censored regression consists of two parts, uncensored and censored part. For uncensored part, it is regular likelihood function:\nwhere \u03d5 is the probability density function for standard Gaussian distribution. For censored part, the likelihood is the probability of latent target y * being beyond the bound:\nwhere \u03a6 and Q are the cumulative distribution function and tail function for standard Gaussian, with Q(x) = 1 \u2212 \u03a6(x).\nGiven above, it is thus straight forward to derive the likelihood for one data point in multi-task case with decomposed weights:\nThe objective function takes log transformation, after which 1 \u03c3 before \u03d5(\u00b7) becomes a constant and we did not include it. The rest part of the objective function is easy to verify."}, {"section_title": "Derivation of Gradients", "text": "The objective for i \u2212 th sample can be decoupled into the following:\n, the derivatives of likelihood part w.r.t V are:\n. The derivative w.r.t U can be derived in similar ways."}, {"section_title": "Proof of Theory", "text": "We hereby give more details for the proofs of both asymptotic and non-asymptotic convergence properties for Algorithm 1 to recover the latent subspace U . The proofs heavily rely on a series of previous results in [14, 16-18, 27, 28] , and many key results are directly referred to hereinafter for conciseness. We include the proofs for the manuscript to be self-contained.\nAt iteration i = 1, 2, ..., N , we sample (x i , y i ), and let U i , V i denote the intermediate U and V , to be differentiated from U t , V t which are the t-th columns of U , V . For the proof feasibility, we assume that {(x i , y i )} N i=1 are sampled i.i.d., and the subspace sequence\nlies in a compact set."}, {"section_title": "Proof of Asymptotic Properties", "text": "For infinite data streams with N \u2192 \u221e, we recall the instantaneous cost of the i-th datum: and the online optimization form recasted as an empirical cost minimization:\nThe Stochastic Gradient Descent (SGD) iterations can be seen as minimizing the approximate cost:\nwhere \u0434 N is a tight quadratic surrogate for \u0434 N based on the secondorder Taylor approximation around U N \u22121 :\n. \u0434 N is further recognized as a locally tight upper-bound surrogate for \u0434 N , with locally tight gradients. Following the Appendix 1 of [28] , we can show that \u0434 N is smooth, with its first-order and second-order gradients bounded w.r.t. each U N .\nWith the above results, the convergence of subspace iterates can be proven in the same regime developed in [18] , whose main inspirations came from [16] that established convergence of an online dictionary learning algorithm using the martingale sequence theory. In a nutshell, the proof procedure proceeds by first show-\nasymptotically, according to the quasi-martingale property in the almost sure sense, owing to the tightness of \u0434 . It then implies convergence of the associated gradient sequence, due to the regularity of \u0434.\nMeanwhile, we notice that \u0434 i (x i , y i , U , V ) is bi-convex for the block variables U t and V (see Lemma 2 of [28] ). Therefore due to the convexity of \u0434 N w.r.t. V when U = U N \u22121 is fixed, the parameter sketches V can also be updated exactly per iteration.\nAll above combined, we can claim the asymptotic convergence for the iterations of Algorithm 1: as N \u2192 \u221e, the subspace sequence {U i } N i=1 asymptotically converges to a stationary-point of the batch estimator, under a few mild conditions."}, {"section_title": "Proof of Non-Asymptotic Properties", "text": "For finite data streams, we rely on the unsupervised formulation of regret analysis [14, 27] to assess the performance of online iterates. Specifically, at iteration t (t \u2264 N ), we use the previous U t \u22121 to span the partial data at i = 1, 2, ..., t. Prompted by the alternating nature of iterations, we adopt a variant of the unsupervised regret to assess the goodness of online subspace estimates in representing the partially available data. With \u0434 t (x t , y t , U t \u22121 , V ) being the loss incurred by the estimate U t \u22121 for predicting the t-th datum, the cumulative online loss for a stream of size T is given by:\nFurther, we will assess the cost of the last estimate U T using:\nWe define C T as the batch estimator cost. For the sequence {U t } T t =1 , we define the online regret:\nWe investigate the convergence rate of the sequence {R T } to zero as T grows. Due to the nonconvexity of the online subspace iterates, it is challenging to directly analyze how fast the online cumulative lossC t approaches the optimal batch cost C t . As [28] advocates, we instead investigate whether\u0108 t converges toC t . That is established by first referring to the Lemma 2 of [17] : the distance between successive subspace estimates will vanish as fast as O(1/t): U t \u2212 U t \u22121 F \u2264 B t , for some constant B that is independent of t and N . Following the proof of Proposition 2 in [28] , we can similarly show that: if {U t } T t =1 and {V t x t } T t =1 are uniformly bounded, i.e., U t F \u2264 B 1 , and V t x t 2 \u2264 B 2 , \u2200t \u2264 T , then with constants B 1 , B 2 > 0 and by choosing a constant step size \u00b5 t = \u00b5, we have a bounded regret as: . This thus concluded the proof. "}, {"section_title": "Clinical Targets Statistics", "text": "Here we provide statistics for the data in real experiments. The mean and standard deviation for each given target is calculated and distribution for all targets are plotted. We also examine the relationship between targets by computing the correlation for pairwise targets. Results are shown in Figure 5 . We can see that all the targets are non-negative, and most target means are within 50 with standard deviation following the similar trend. Figure 5 (c) reveals clustering pattern among different tasks where some tasks are similar with each other, which again supports the multi-task assumptions. Figure 6 shows the value of weights associated with one random split, where most weights are close to zero."}]