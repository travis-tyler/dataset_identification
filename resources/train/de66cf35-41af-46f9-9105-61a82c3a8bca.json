[{"section_title": "Abstract", "text": "Abstract-This paper proposes a novel approach for improving the accuracy of statistical prediction methods in spatially normalized analysis. This is achieved by incorporating registration uncertainty into an ensemble learning scheme. A probabilistic registration method is used to estimate a distribution of probable mappings between subject and atlas space. This allows the estimation of the distribution of spatially normalized feature data, e.g., grey matter probability maps. From this distribution, samples are drawn for use as training examples. This allows the creation of multiple predictors, which are subsequently combined using an ensemble learning approach. Furthermore, extra testing samples can be generated to measure the uncertainty of prediction. This is applied to separating subjects with Alzheimer's disease from normal controls using a linear support vector machine on a region of interest in magnetic resonance images of the brain. We show that our proposed method leads to an improvement in discrimination using voxel-based morphometry and deformation tensor-based morphometry over bootstrap aggregating, a common ensemble learning framework. The proposed approach also generates more reasonable soft-classification predictions than bootstrap aggregating. We expect that this approach could be applied to other statistical prediction tasks where registration is important."}, {"section_title": "", "text": "normal controls using a linear support vector machine on a region of interest in magnetic resonance images of the brain. We show that our proposed method leads to an improvement in discrimination using voxel-based morphometry and deformation tensor-based morphometry over bootstrap aggregating, a common ensemble learning framework. The proposed approach also generates more reasonable soft-classification predictions than bootstrap aggregating. We expect that this approach could be applied to other statistical prediction tasks where registration is important.\nIndex Terms-Alzheimer's disease, ensemble learning, registration uncertainty."}, {"section_title": "I. INTRODUCTION", "text": "M EDICAL imaging data is often used to make quantitative predictions about the current or future disease state of a subject. In the case of structural magnetic resonance (MR) images of the brain, image data can be analyzed to identify differences in morphology which are distinctive between pathological and healthy states. The identification of these differences allows for a quantitative diagnostic measure, which have the potential to assist in early diagnosis of pathology. Several automated approaches have been demonstrated which allow the diagnosis and prediction of Alzheimer's disease (AD) [1] .\nMachine learning techniques such as statistical classifiers and regressors are often used to facilitate this objective. These approach predict the value of an outcome variable, such as disease score, or group, on the basis of some feature data, e.g., grey matter probability maps.\nWhen analyzing MR images of the brain for distinctive morphometric features across a group of subjects, most standard machine learning approaches require the image data to be transformed into a common frame of reference to facilitate comparison. This requires the use of an image registration tool to estimate the mathematical mapping between each subject image and the atlas space (usually an average, or representative subject), a process which is known as spatial normalization. From the estimated mapping, spatially normalized feature data can be derived, which are used as a basis for making predictions.\nFollowing spatial normalization, statistical prediction may be performed using grey matter, or other tissue class probabilities as feature data. This is referred to as voxel-based morphometry (VBM) [2] . Grey matter (GM) probability maps have previously been used in the discrimination of Alzheimer's disease [3] , while other methods have incorporated white matter (WM) and cerebrospinal fluid (CSF) as well [4] .\nAlternatively, features of interest can be derived from the estimated mapping between subject and atlas space, a process which is known as deformation tensor-based morphometry 0278 -0062/$31.00 \u00a9 2012 IEEE (TBM) [2] . Differences have been found in TBM data between subjects with Alzheimer's disease and age matched healthy controls [5] .\nThe majority of machine learning techniques that are widely used for making predictions from medical imaging data are supervised, meaning that they require a set of training data with known outcome variables. This training set is used to derive a predictive model for estimating the mapping between feature data and outcome.\nFor any supervised learning approach any predictions on test data are highly dependent on the set of training data. As the feature data is required to be spatially normalized prior to analysis, each item of training data is dependent on the inferred image registration between the subject and atlas space. As has been previously shown in many studies, and is highlighted well in the comparison study in [6] , inter-subject brain registration is far from an exact process. Therefore, we expect there to be some residual registration error in the feature data. Consequently, it is unlikely that any estimated statistical relationship derived from a given set of training data will be exactly correct and any residual misregistration of data may contribute to errors in prediction.\nThe majority of registration methods estimate only the maximum a-posteriori (MAP), which is the most likely mapping subject to some regularization constraints. However, recent registration methods have emerged that provide estimates of the registration uncertainty [7] , [8] . This facilitates the consideration of a distribution of probable registration mappings, as opposed to just the MAP.\nThere are two published 3-D medical image registration methods that we are aware of which infer a distribution of probable mappings: Risholm et al. [7] use Markov chain Monte Carlo (MCMC) to numerically estimate the full posterior distribution of transformation parameters, whilst marginalizing over the regularization parameters. This method allows the estimation of a nonparametric posterior distribution of mappings. However, the computational expense of this approach makes it impractical in the context of this work. An alternative approach, previously proposed by the authors [8] , uses variational Bayes (VB) to infer an approximate posterior distribution of the set of transformation, regularization, and noise parameters. This approach assumes that the true distribution of transformation parameters follows a multi-variate normal distribution making it computationally much more efficient.\nAllassonni\u00e8re et al. [9] describe a Bayesian deformable template registration framework which yields an approximate posterior distribution of transformation parameters. This approximative posterior may be a mixture of multi-variate normals, and is inferred using expectation-maximization. Their work has the limitation that the strength of the prior, which greatly affects the posterior distribution, has to be hand-defined. This algorithm was demonstrated on 2-D digit recognition.\nAn alternative view of registration uncertainty has been proposed by Van Leemput [10] . They describe an approach for creating a deformable labelled anatomical atlas through the use of a Bayesian statistical model. Their approach does not attempt to estimate the PDF of the true deformation field, but rather modelling the distribution of an image labelling. Such a method may provide complementary uncertainty information.\nSome previous work has been performed on visualizing transformation uncertainty in nonrigid registration [11] , [12] . More related work utilizing the concept of uncertain registration includes: estimating local anisotropic smoothing kernels to compensate for uncertainty in registration when estimating spatially normalized statistics [13] . Very recently, Iglesias et al. [14] introduced an approach to hippocampal subfield segmentation, where registration uncertainty is integrated out of a combined registration/Gaussian mixture model approach to segmentation using MCMC. Risholm et al. [15] proposed an approach to calculating the uncertainty of a delivered dose in radiotherapy under uncertain registration. Here, we follow in a similar fashion by estimating the variability in statistical predictors that is due to the uncertainty in registration. The novel contribution of this work is to leverage this variability within a statistical learning framework to provide a more robust prediction.\nWe propose to incorporate the estimated registration uncertainty within an ensemble learning approach [16] . Ensemble learning methods have been demonstrated to be an effective mechanism to measure the uncertainty of the space of statistical predictors, providing a more robust prediction by combining estimates. To provide variability in predictive models, they need to be trained using different subsets of the training data. Each subset needs to be selected appropriately to encapsulate an appropriate level of variability in predictive models. In many settings, bootstrap aggregating or bagging, has proved itself to be an effective tool [17] . Bagging creates variability between predictors by sampling with replacement from the set of training subjects. However, the random selection of subjects in each training subset can lead to large differences in predictors due to inter-subject differences. Conversely, in this work we seek to leverage our knowledge of the derivation of the data to create sets of training data which encapsulate the intra-subject variability due to registration uncertainty.\nIn the case of spatially normalized feature data, the distribution of the data can be estimated from the set of probable mappings inferred from the registration algorithm. Samples can be drawn from this estimated distribution of feature data for each training subject, and used as a parametric variant of bootstrapping [18] . These samples of feature data can be used in place of the MAP observations to build up a set of training data sets, which all contain the same subjects, but with examples based on different probable registrations. Such an ensemble of statistical predictors accounts for the inherent uncertainty in the registration process, and therefore leads to a more robust prediction. A limited evaluation of this approach has been previously presented [19] .\nIn this paper, we first describe how the distribution of feature data can be derived using a probabilistic registration tool. Subsequently, we explain how this distribution can be used in an ensemble learning scheme. To demonstrate the performance of our approach, we apply it to discriminating between subjects with Alzheimer's disease and age matched healthy controls using a standard black box classifier, detailed in Section III. We find that the use of this scheme leads to an improvement in classification accuracy."}, {"section_title": "II. METHODS", "text": ""}, {"section_title": "A. Probabilistic Registration Method", "text": "A probabilistic registration method that can estimate posterior transformation distributions is required to estimate the distribution of the feature data, which accounts for the uncertainty in registration. Standard registration procedures use a MAP approach to infer the mapping between images. These approaches do not provide any estimates on the confidence of the inferred mapping, and consequently do not lend themselves to this work. Therefore, we use our previously published registration algorithm in this work [8] , for which we now provide a summary. Image registration can be described using a generative model (1) where is the target image, is the transformed source image , where parametrizes the transformation. models the image mismatch where , is the matrix identity and is the global image noise precision (inverse variance). We use a free-form deformation (FFD) model [20] using cubic B-splines to provide a smooth mapping.\nis the set of B-spline knot displacements. An FFD model was chosen due to the compact parameter representation, but any transformation model could be used.\nPriors are included on all the unknown model parameters. Most importantly, a prior on is required to provide regularization of the mapping, , where encodes the bending energy regularization model and is an inferred spatial precision parameter, controlling the strength of the spatial prior. Bending energy is defined as (2) where refers to the transformation, where the direction is indexed by . , , refer to locations within the co-ordinate system box, bounded by the origin and , , . In this model, is purely defined by , which are the B-spline coefficients of the FFD model. is calculated by differentiating the effects of on , such that gives the bending energy of the transformation. The regularization model used in this work uses free boundary conditions.\nThe priors on and are modelled using uninformative gamma distributions.\nUsing variational Bayes [21] the posterior probability distribution of the model parameters are approximated using a parametric probability distribution function . The mean-field approximation is used to assume independence in the posterior distribution of parameter groups,\n. A set of iterative update equations can be derived which fit the parameters of approximate distributions such that they best resemble the data.\nOf particular interest in this work is the approximate posterior distribution of transformation parameters, .\nThe update equations for the two hyper-parameters of this distribution are given as\nwhere is the previous mean estimate of the transformation parameters , and is the matrix of first-order partial derivatives of the transformation parameters with respect to . and are the expectation of the posterior regularization , and image noise distributions . is a virtual decimation factor [22] , which models the correlation in the residual image . is an estimate of the posterior distribution of the final inferred mapping parameters. The shape and scale of this distribution is dependent on the structure of the image information, weighted by the noise precision, which indicates the level of mismatch in the fitted model. It also depends on the form of the spatial prior, e.g., the bending energy weighted by the spatial precision which is related to the similarity of the transformation to the spatial prior."}, {"section_title": "B. Statistical Prediction", "text": "Statistical predictors such as statistical classifiers and regressors take as input some feature data , and output an estimated outcome variable . Therefore, for a given class of statistical predictor, , we can write . We are considering the class of supervised learners, which require a labelled training set of data items, where indexes the subjects in the training set, from which they learn about the relationship between and . Therefore, the trained predictive model provides an estimate for a test subject based on , . The relationship between a new test image , and its predicted outcome variable , is highly dependent on . Each training item of spatially normalized feature data in is dependent on the inferred image registration. Therefore, training a predictor using is susceptible to misregistration. Accordingly, misregistration may contribute to errors in prediction."}, {"section_title": "C. Ensemble Learning to Incorporate Uncertain Registration", "text": "The novel aspect of this work is to incorporate the estimated registration uncertainty into statistical prediction using an ensemble learning approach. Ensemble learning methods [16] use a set of predictors to provide a more robust estimate of a prediction. To provide variability in predictive models, they need to be trained using different data sets, , each of length , where indexes the different training sets.\nThe class of ensemble learning methods that we are concerned with use a linear combination of multiple statistical predictors to provide a more robust estimate (5) where is index of the test subject, is the number of predictive models, and is a vector containing the relative weights Fig. 1 . Graphical examples of how sampled data can be used in classification. The left plot illustrates the scheme Train+, where multiple classification boundaries are estimated using random samples of each subject. In this illustration two random samples are drawn from each subject, and thus three classification boundaries can be drawn, two from sets of samples and one using the distribution expectation. The right plot shows the scheme Test+, where the variability in classification label can be calculated using random samples of each test subject with a fixed classification boundary. In this case, three random samples were drawn for each subject.\nattributed to each trained prediction model. Only binary statistical predictors are used in this work, although any form of predictor can be used. In the ensemble learning schemes used in this paper, , but alternative weighting schemes could be derived using, for example, Bayesian model averaging [23] .\nA standard approach to generating multiple predictors is bootstrap aggregating, or bagging [17] . In bagging, each is selected by random uniform sampling with replacement from the set of training subjects. For a large number of bootstraps, each would be expected to contain 63.2% of the unique training subjects [24] . This approach has been found to be effective at sampling the space of prediction models based on the inter-subject variability. A limitation with bagging is that it only considers the variation between subjects to create different predictive models, whereas in some situations the intrinsic uncertainty of the measurements from which the model is derived may lead to a comparably large source of variability.\nIn this work, we leverage our knowledge of the derivation of the feature data when creating such that it considers the distribution of feature data as estimated from the set of probable registration mappings, . This is achieved by selecting to contain a random sample drawn from each subject's feature data distribution . This is a parametric variant of bootstrapping [18] , where instead of using observations, new data is drawn from the distribution of observations. We refer to this scheme as Train+. A graphical illustration of how multiple classification models can be generated in such a fashion is given in the left plot of Fig. 1 . Using a sufficiently large ensemble of statistical predictors should account for the inherent uncertainty in the registration process, and we expect that this exploitation of the instability of predictive models under different training data should lead to an improvement in prediction for all but the most stable of predictors and data.\nOur knowledge of can also be used to provide additional information on the prediction variability for each predictive model. This is achieved by averaging the predicted outcome for a set of random samples drawn from the test subject distribution , rather than simply testing using only the most likely observation. We refer to this scheme as Test+, which is graphically illustrated in the right plot of Fig. 1 . Train+ and Test+ can be used separately, or can be combined together by using multiple test samples with each predictor in the ensemble. All of these variants can also be incorporated into a bagging framework which would help encapsulate both the inter, and intra-subject variability."}, {"section_title": "D. Feature Data", "text": "A voxel-or deformation tensor-based morphometry [2] approach can be taken to provide a framework for the classification of subjects into their respective groups.\nVBM requires the registration of segmentation probability maps, for example grey matter probability maps, of each subject to an atlas space. In a conventional approach, e.g., [3] , where a MAP registration tool is used, the image is transformed using only the expectation of the transformation distribution, . The transformed grey matter probability map is modulated (multiplied) by the determinant of the warp field Jacobian to compensate for the expansion/contraction of voxels [25] .\nIn TBM, instead of examining the spatially normalized image information, the assumption is made that the discriminative differences between subjects are contained in the deformation field which maps each subject to the atlas space [5] . Feature data is usually derived from the voxelwise 3 3 Jacobian matrix of the transformation of the mapping . Often a scalar measure of the Jacobian matrix is used for comparison, most commonly , where is a voxel index. provides a measure of the expansion/contraction of a particular voxel as a result of the mapping. The log transform is commonly applied to make the data normally distributed [26] ."}, {"section_title": "E. Estimating a Distribution of Feature Data", "text": "The proposed method requires samples of the feature data under different probable spatial normalizations, rather than simply using the MAP estimate of the mapping. Therefore, the distribution of feature data according to inferred registration parameters needs to be calculated. A distribution of feature data for each subject , can be estimated for either VBM or TBM data. This is achieved by drawing samples from the inferred approximate posterior distribution of transformation parameters , and calculating the resulting feature data for that sampled mapping. By sampling a large number of mappings, we can build up an estimate of the distribution ."}, {"section_title": "III. EXPERIMENTS", "text": "A. Materials 1) ADNI: Data used in the preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.ucla.edu). The ADNI was launched in 2003 by the National Institute on Aging (NIA), the National Institute of Biomedical Imaging and Bioengineering (NIBIB), the Food and Drug Administration (FDA), private pharmaceutical companies and nonprofit organizations, as a $60 million, five-year public-private partnership. The primary goal of ADNI has been to test whether serial magnetic resonance imaging (MRI), positron emission tomography (PET), other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment (MCI) and early Alzheimer's disease (AD). Determination of sensitive and specific markers of very early AD progression is intended to aid researchers and clinicians to develop new treatments and monitor their effectiveness, as well as lessen the time and cost of clinical trials.\nADNI is the result of efforts of many coinvestigators from a broad range of academic institutions and private corporations, and subjects have been recruited from over 50 sites across the U.S. and Canada. The initial goal of ADNI was to recruit 800 adults, ages 55-90, to participate in the research, approximately 200 cognitively normal older individuals to be followed for three years, 400 people with MCI to be followed for three years and 200 people with early AD to be followed for two years. For up-to-date information, see http://www.adni-info.org.\n2) Subject Grouping: In this work a total of 311 subject images were taken from the ADNI database [27] . 149 of these subjects were patients with Alzheimer's Disease (AD) and 162 were age matched normal controls (NC). These images were broken up into training and testing sets, the properties of which are given in Table I ."}, {"section_title": "B. Pipeline 1) Processing:", "text": "Each image was skull stripped using BET [28] with the option \"-B\" which deals with bias fields and help remove extra neck voxels. Each image was then registered using an affine registration algorithm [29] to the MNI152 atlas, and resampled to 1 mm isotropic resolution. For VBM analysis, grey matter probability maps were extracted using FAST [30] .\nAs drawing samples from multivariate normal distributions becomes computationally expensive for large number of transformation parameters, a region of interest (ROI) analysis was required. Atrophy in medial temporal lobe structures, particularly the hippocampus has been shown to be a sensitive marker of Alzheimer's disease [31] . Therefore ROIs of voxels surrounding the left and right hippocampi were extracted from the structural MR and grey matter images for registration.\n2) Atlas Creation: For spatial normalization, most methods require the definition of an atlas space. The use of an exemplar subject to define an atlas space can induce a statistical bias. Therefore, in an effort to reduce the bias, a sharp atlas can be estimated from the subjects in the training set in an iterative manner as in [32] . The initial atlas estimate is made by averaging the ROI feature data in the training set. Subsequent iterations nonrigidly register each of the images using our Bayesian nonrigid registration algorithm, with a 5 mm knot spacing, to the current atlas estimate. The new atlas estimate is created by averaging the intensities of the registered images, and deforming this image by the average warp from the atlas to the subjects. This is calculated by averaging the inverse of the transformations from all the subjects to the atlas. The B-spline FFD transformations are constrained to be diffeomorphic using the method of [33] , which allows a smooth and well defined inverse to be estimated. This procedure is repeated until the estimated atlas image changes by less than 1% between iterations. This is used to create a structural MR and grey matter atlas for each ROI.\n3) Feature Generation: To create the feature data, each subject image was nonrigidly registered to the relevant atlas image using either a 5 mm FFD knot spacing for TBM to give high-resolution features, and a 10 mm spacing for VBM to align the images, but still retain subject differences. Once the registration algorithm has converged, warp samples are drawn from to characterize . To avoid artefacts related to the edges of the ROI, the voxels within 4 mm of the edge are removed, leaving a feature region of 32 72 28 voxels. 4 mm was chosen because the sampled deformations around the edge of the image are very unlikely to exceed this.\nTo allow the tractable storage of samples from , the feature data needs to be sub-sampled by a factor of 4. This gives a total of 1008 voxels. To make the classification step computationally efficient, 3600 samples of the data feature are stored per subject to provide a sufficiently accurate description of the distribution, rather than fitting the data to a parametric distribution and later sampling from it. In the classification stages, samples are randomly chosen for each subject in both the Train+ and Test+ methodologies. In practice, all of these samples may be required for a run using Test+. An example illustration of VBM and TBM features for a subject in the test set with Alzheimer's disease is given in Fig. 2 ."}, {"section_title": "C. Classification", "text": "In the experiments we use a linear support vector machine (SVM) [34] as implemented in [35] , using all 1800 feature voxels to classify between subject groups. Subject age is regressed out for each voxel based on the empirical expectation Fig. 2 . Examples of the data features acquired when registering a region of interest around the left hippocampus from a subject with AD taken from the test set, to the left atlas image for both TBM (left) and grey matter VBM (right). For both TBM and VBM a single slice of the volume is illustrated. The images marked Atlas and Subject are the high-resolution cropped Atlas and Subject images on which the registration is performed. The mean feature images show the mean of the estimated distribution for the same slice from the sub-sampled feature volume. The covariance matrices illustrate the estimated voxelwise covariance of the feature data for the displayed slice. The covariance matrix is ordered as where and are positions in the feature image. The bottom left and the top right of the covariance matrix corresponds to the bottom left and top right of the feature image accordingly. The TBM data shows that there is expansion of the ventricle, but also that the in this region has a high degree of variance and covariance. The VBM feature data is a grey matter probability map which shows high variability across the image, except in the ventricles where there is no grey matter. of for the healthy controls in the training set using ordinary least squares linear regression [36] , and each voxel is given 0 mean and unit standard deviation.\nTo select the most appropriate SVM classifier parameter for use in the Original scheme, we use a leave-one-out cross-validation procedure on the training set to test. This tests a range of the soft margin penalty parameter values, , to find the value with the best generalization accuracy. The optimal parameter and its corresponding correct rate, defined as the ratio of correctly identified examples from the number of testing examples, are given in Table II . For the SVM classifiers used within the ensemble learning schemes, which effectively removes the soft margin. This is beneficial as the training data is usually linearly separable, and removing the soft-margin introduces greater variability between classifiers as there is a greater dependence on the training data.\nIn our classification experiments we compare seven different training schemes.\n\u2022 Original, train using the empirical expectation of the data in the whole training set, and test using the empirical expectation of the testing data.\n\u2022 Train+, train using a random sample of each subject in the training set.\n\u2022 Bagging, the standard bootstrap aggregating approach where the training set is sampled with replacement.\n\u2022 BaggingTrain+, where a random subject sample is used within a bagging scheme. It can be seen that BaggingTrain+Test+ and Train+Test+ outperform, or do as well as standard Bagging for all the features types, except R TBM, which is the lowest performing feature for all methods. BaggingTrain+Test+, when combining ensembles using na\u00efve Bayes, gives the best trade-off of classification sensitivity, and specificity of any of the approaches considered.\n\u2022 Test+, train using the empirical expectation of the whole training set and test using 20 random samples for each subject in the test set.\n\u2022 BaggingTest+, the combination of bagging and test+.\n\u2022 Train+Test+, the combination of train + and test+. In our experiments 300 classification models were generated to make an ensemble as this was sufficient for convergence for all methods. A summary of the results of these experiments is given in Fig. 3 , and the classification correct rate for all of the methods is given in Table III. TABLE III  CLASSIFICATION CORRECT RATE USING THE DIFFERENT PREDICTOR TRAINING AND TESTING VARIANTS USING TBM AND VBM FEATURE DATA. L  AND R INDICATE THE LEFT AND RIGHT HIPPOCAMPUS DATA, RESPECTIVELY. NA\u00cfVE BAYES REFERS TO THE COMBINATION OF SOFT PROBABILITIES  FROM DIFFERENT FEATURE DATA TYPES. TRAIN+TEST+ AND IT'S VARIANTS TEND TO DO AS WELL, OR BETTER THAN A STANDARD BAGGING  APPROACH FOR BOTH VBM AND TBM. BAGGINGTRAIN+TEST+ PROVIDES THE BEST OVERALL CLASSIFICATION As we can see from Table III , the left hippocampus provides stronger features than the right for discriminating between Alzheimer's disease and age matched normal controls for all methods and both feature types. VBM provides good separation for both left and right hippocampi, whereas only TBM on the left hippocampus provided a reasonable level of discrimination. Fig. 3 provides a summary of some of the results illustrating the sensitivity and specificity of each ensemble. Sensitivity is defined as the proportion of correctly identified disease cases, out of the total number of disease cases. Specificity is the proportion of correctly identified control subjects, out of the total number of controls. This summary shows that ensemble learning approaches generally provide more accurate classification than the Original approach. All of the Train+Test+ approaches outperform, or do as well as Bagging for all features except right TBM, which produced the lowest classification results for all methods. This implies that the additional data variability provided by registration uncertainty assists in creating a more accurate ensemble.\nFurthermore, we find the largest improvement over the original approach in the left hippocampus TBM feature data, particularly using the BaggingTrain+ schemes. The strength of this improvement is likely to be due to the data feature being derived from the warp field. Small changes in warp field which might have little effect on the image likelihood, may lead to more substantial changes in . This is likely to also be a contributing factor in the improvement in the VBM results, as the warped GM probability map is modified by the . The Test+ schemes do not have much impact on classification correct rates.\nIn terms of computational time, the Original scheme is fastest, as only one classifier is constructed. Including the overhead of loading, and preprocessing the data, the training takes approximately 5 s. The use of 300 bootstrapping samples in Bagging takes 5 min to complete. Train+ and BaggingTrain+ take about 5 min of CPU time. However, because of the slow speed of disk access which is required to load the samples, Train+ and BaggingTrain+ take 10-15 min. The use of Test+ schemes adds an additional 30 min to run-time. This extra time is almost entirely taken up by disk access. Once the ensembles have been created, classification of a new sample is very fast, s, and 10 s using Test+. All of the experiments were conducted on a dual core 2.8 GHz laptop with a serial ATA (7200RPM) hard-drive."}, {"section_title": "D. Combining Soft Classification Probabilities", "text": "Each ensemble gives a soft classification result for each subject as it is an average of multiple predictions, given by (6) Fig. 4 . Histogram of the probability estimates given by a classification ensemble, to the correct class label. These histograms are plotted for the right hippocampus TBM feature data. Although Bagging produces a more accurate classification for this data than the other schemes, it is also over confident in some estimates which are incorrect. This can be seen in the larger number of subjects which are assigned a very low probability of belonging to the true class which they are from. This relatively poor estimate of uncertainty given by the Bagging ensemble classifiers is a likely cause of the worse performance of Bagging in the na\u00efve Bayes classification results shown in Table III. where is restricted to being a binary classifier and the superscript denotes the feature data type. An illustration of the soft classification resulting from the ensemble in given in Fig. 4 . This figure shows that Train+Test+ and BaggingTrain+Test+ give greater probability to the correct classification for subjects which they get wrong, than bagging does. To assess how reasonable the soft predictions given by each classifier ensemble are, they can be combined in a postclassifier analysis. Here, we combine the soft classification probabilities using a naive Bayesian classifier (7) where indicates the alternative classification label in a binary classification problem. Results for the na\u00efve Bayes combination are provided in Table III and Fig. 3 . The combination of Bagging with Train+Test+ leads to the most reasonable soft-predictions, as shown by the highest correct rate and best sensitivity/specificity trade-off. All of the Train+ schemes also outperform Bagging in the na\u00efve Bayes classification. This is likely to be caused by Bagging showing greater prevalence for overconfidence in incorrect predictions, as illustrated in Fig. 4 . It can be seen that the use of Test+, on its own, or in combination with BaggingTrain+ is beneficial, implying the additional testing samples helps to estimate the uncertainty of prediction."}, {"section_title": "IV. DISCUSSION AND CONCLUSION", "text": "In this work we have demonstrated a scheme in which registration uncertainty can be incorporated into an ensemble learning scheme to provide more accurate prediction, with more reasonable estimates of classifier uncertainty, than standard approaches such as bootstrap aggregation. This was achieved by sampling probable registration transformations inferred from a probabilistic registration algorithm, and then estimating the distribution of a data feature given the uncertainty in the registration. Samples of the feature data distribution are used in place of the most likely observations in the training and testing phase for statistical predictors. The proposed approach generates prediction variability from the intra-subject uncertainty as opposed to the inter-subject variation, as is achieved by boostrapping. We describe a method of combining predictors trained using sampled data into an ensemble. In our experiments, we provide results on the problem of classification of subjects with Alzheimer's disease, from age matched healthy controls using a linear SVM.\nOur results demonstrate that the proposed scheme tends to lead to an improvement in classification correct rate over a standard scheme and bootstrap aggregating when examining grey matter voxel-based morphometry, and tensor-based morphometry. This implies that the registration uncertainty contains more useful information for the discriminative problem than that obtained from bootstrapping. We also found that improved classification results can be achieved by the naive Bayesian combination of the ensembles created from the separate data features. Here, we showed that the variability induced by bootstrap aggregating provides less reasonable estimates of prediction uncertainty than the proposed approach, and this is reflected in our results.\nIn this work the strength of a prior with a fixed covariance structure, based on bending energy, is inferred from the data. A fixed prior covariance structure expects a similarly smooth transformation across the image. The choice of the prior covariance structure will have an effect on the posterior transformation distribution, particularly in regions where little image information is available. The structure of this prior distribution could be estimated from the data. Such an approach could provide a more biologically plausible prior. Some general approaches have been proposed which allow the estimation of the parameter covariance structure from the data using variational methods [37] , [38] . These methods are likely to be computationally expensive, sometimes necessitating the approximation of independence between regions [39] .\nIf the prior distribution, , with any associated parameters is known, then it may be possible to draw samples of the true posterior distribution of transformation parameters, , using MCMC in a computationally efficient manner. Such approaches have been previously described for registration [14] , [40] , but in these cases was predetermined for a group of subjects, which may have an effect on the inferred distribution. In future work we will experiment with MCMC inference of , fixing and based on the expectation of and , respectively. The choice of atlas is also an important prior of this model, and therefore if it is biased towards a particular anatomy, it may affect the estimated distribution of probable registrations, . Future work will experiment with a groupwise registration approach, which should help to reduce any atlas induced bias.\nAn alternative approach to utilize the full estimated distribution of registration mappings would be to use the transformation parameters themselves as data features. As the inferred distribution is multivariate normal, there is a finite length description of the distribution. This description could be used directly, rather than through sampling the distribution. A disadvantage of using such an approach is that some of the interpretability associated with VBM/TBM is lost when using the transformation parameters directly as features.\nAn entirely different approach to mitigate misregistration would be to use multiple atlases. This has been successfully applied in integrating out registration from propagated segmentations [41] . Koikkalainen et al. [42] suggest approaches to generating more robust TBM features by registering each subject to several atlases, which are all registered to the reference space. This work is complementary, and further investigation could compare the benefits of each approach.\nPosterior probabilities can be directly estimated from a variety of classifiers, including SVMs, but in a more principled manner from logistic regression or relevance vector machines [43] . Such probabilistic classification estimates, or regression outputs, could be incorporated within the proposed framework. Future work could investigate including and comparing against uncertainty estimates from these classifiers.\nIn this work we have presented results discriminating subjects with AD from age matched healthy controls. A more interesting clinical problem lies in the diagnosis of subjects suffering from mild cognitive impairment [44] , and the prognosis of those who may convert to AD. Imaging data of such subjects is available in ADNI, and this analysis will be investigated in future work.\nWe have shown that incorporating registration uncertainty leads to an improved classification performance over standard approaches for this particular experimental pipeline. However, this experimental process itself may not be optimal for the classification problem, and further work could be carried out to improve the overall classification accuracy to match current state of the art pipelines. Firstly, we could consider looking at multiple functional areas, and combining their soft-classification probabilities. We could consider a more flexible ensemble learning scheme, such as Bayesian model averaging. Feature selection could be used in place of, or following voxel sub-sampling for creating the feature data distribution, to select the most discriminative voxels. In particular, the choice of classifier, and its associated parameterization could be addressed as well as the data processing scheme. Nevertheless, the proposed approach has demonstrated that information that is pertinent to the classification problem can be exploited from the uncertainty of image registration."}]