[{"section_title": "List of Tables", "text": ""}, {"section_title": "Appendix Exhibits", "text": ""}, {"section_title": "Introduction", "text": "The Program for International Student Assessment (PISA) is an international assessment of 15-year-old students. PISA measures how well students apply their knowledge and skills learned in and out of school in reading, mathematics, and science literacy to problems within a real-life context. PISA also gathers information from students and school principals about school context and from students about their educational experiences and attitudes. Analyses of PISA data have provided information about the relative performance of students across nations, as well as variation across nations in relationships between student background, attitudes, and experiences and their performance in mathematics, reading, and science literacy at age 15. PISA, which began in 2000, is conducted in 3-year cycles. Each PISA administration assesses one of the three subject areas in depth, although all three are assessed in each cycle. Assessing all three areas allows participating countries to have an ongoing source of achievement data in every subject area while rotating one area as the main focus over the years. In the fourth cycle of PISA, reading was the subject area assessed in depth, as it was in 2000 (see figure 1). PISA 2009 included an in-depth assessment of reading, as well as shorter assessments of mathematics and science. The in-depth assessment of reading included a sufficient number of items to develop performance estimates for reading literacy overall as well as for subscales involving specific aspects or processes of reading: accessing or retrieving information, integrating and interpreting, and reflecting and evaluating. Subscale scores were not available for mathematics literacy or science literacy for 2009 (subscale scores are available for mathematics in the 2003 PISA data and for science in the 2006 data). In addition, PISA 2009 collected information on students' backgrounds, attitudes towards reading, and learning strategies. School principals also were asked to provide information on school demographic characteristics and the learning environment in the school. The PISA 2009 main study in the United States consisted of three major elements: (1) a 2-hour student assessment of reading literacy, mathematics literacy, and science literacy; (2) a student questionnaire that required approximately 30 minutes to complete; and (3) a school questionnaire to be completed by the principal or designee that also required approximately 30 minutes to complete."}, {"section_title": "What PISA Measures", "text": "PISA assesses the application of knowledge in reading, mathematics, and science literacy to problems within a real-life context (OECD 1999). PISA uses the term \"literacy\" in each subject area to denote its broad focus on the application of knowledge and skills. For example, when assessing reading, PISA assesses how well 15-year-old students can understand, use, and reflect on written text for a variety of purposes and settings. In science PISA assesses how well students can apply scientific knowledge and skills to a range of different situations they may encounter in their lives. In mathematics, PISA assesses how well students analyze, reason, and interpret mathematical problems in a variety of situations. Scores on the PISA scales represent skill levels along a continuum of literacy skills. PISA provides ranges of proficiency levels associated with scores which describe what a student can typically do at each level (OECD 2006). PISA's target age of 15 allows jurisdictions to compare outcomes of learning as students near the end of compulsory schooling. PISA's goal is to answer the question, \"what knowledge and skills do students have at age 15?\" taking into account schooling and other factors that may influence their performance. In this way, PISA's achievement scores represent a \"yield\" of learning at age 15, rather than a direct measure of attained curriculum knowledge at a particular grade level. Fifteen-year-old students participating in PISA from the United States and other jurisdictions are drawn from a range of grade levels. In PISA 2009, 69 percent of the U.S. students were enrolled in grade 10, and another 20 percent were enrolled in grade 11. To provide valid estimates of student achievement and characteristics, PISA selects a sample of students that represents the full population of 15-year-old students in each participating jurisdiction. This population is defined internationally as 15-year-olds attending both publicly and privately controlled schools in grade 7 and higher. Students in all programs of study (e.g., academic, vocational) are included in PISA. A minimum of 4,500 students from a minimum of 150 schools was required in each jurisdiction. In PISA 2009, jurisdictions were permitted to exclude up to 5 percent of the population of 15-year-old students by excluding students who were unable to participate in the assessment (e.g., because of functional or intellectual disabilities, or limited language ability in the test language) or who were attending schools that were inaccessible to the test administrators. All but 5 countries (Denmark, Luxembourg, Canada, Norway and the United States) achieved this standard, and in 36 countries and jurisdictions, the overall exclusion rate was less than 2 percent. When language exclusions were accounted for (i.e., removed from the overall exclusion rate), the United States no longer had an exclusion rate greater than 5 percent. (For more information on allowed exclusions see chapter 2; for additional details on coverage of student populations, see OECD (2010a)). In addition to PISA, the United States has for many years conducted assessments of student achievement at a variety of grade levels and in a variety of subject areas through the National Assessment of Educational Progress (NAEP), Trends in International Mathematics and Science Study (TIMSS), and Progress in International Reading Literacy Study (PIRLS). These studies differ from PISA in terms of their purpose and design (see appendix A). NAEP reports information on achievement of U.S. students using benchmarks of performance (i.e., basic, proficient, advanced achievement levels) established through the collaborative input of a wide range of experts and participants from government, education, business, and public sectors in the United States. Furthermore, the information is used to monitor trends in achievement specific to U.S. students and at the state level. To provide a critical external perspective on the mathematics, science, and reading achievement of U.S. students, in addition to PISA, the United States participates in TIMSS and PIRLS. TIMSS provides the United States with information on the mathematics and science achievement of 4th-and 8th-grade students compared to students in other countries. PIRLS allows the United States to make international comparisons of the reading achievement of students in the fourth grade. TIMSS and PIRLS seek to measure students' mastery of specific knowledge, skills, and concepts and are designed to broadly reflect curricula in the United States and other participating countries, in contrast to PISA which does not focus explicitly on curricular outcomes but rather on the application of knowledge to problems in a real-life context."}, {"section_title": "PISA 2009 Administration", "text": "PISA is sponsored internationally by the Organization for Economic Cooperation and Development (OECD). PISA 2009 was coordinated and administered internationally by the PISA international consortium, led by the Australian Council for Educational Research (ACER), through a contract with the OECD. Technical standards and a series of manuals provided standardized procedures for all countries to follow. Sixty-five countries and other education systems located in non-national entities participated. More than 400,000 students participated worldwide. The National Center for Education Statistics (NCES) was responsible for the implementation of PISA in the United States in accordance with the international standards and procedures. PISA 2009 data collection and associated tasks were carried out through a contract with Windwalker Corporation and its two subcontractors, Westat and Pearson. Windwalker Corporation was responsible for project coordination, preparation of recruitment materials, adaptation of the international instruments, preparation of the U.S. data files, and reporting. Westat was responsible for school and student sampling, recruitment of schools and students, and data collection. Pearson was responsible for the printing of materials, data entry, and coding and scoring. The key personnel involved in data collection included a school coordinator (a school staff member designated by the principal), and a test administrator and assistant administrator (both Westat employees). In 2009, 5,233 U.S. students and 165 U.S. schools participated. Data collection occurred from September 23, through November 19, 2009, and the final report and data were released on December 7, 2010."}, {"section_title": "Organization of This Document", "text": "This technical report and user's guide is designed to provide researchers with an overview of the design and implementation of the 2009 Program for International Student Assessment (PISA), as well as with information on how to access the PISA 2009 data. This information is meant to supplement that presented in Organization for Economic Cooperation and Development (OECD) publications by describing those aspects of PISA 2009 that are unique to the United States. Chapter 2 provides information about sampling requirements and sampling in the United States. Chapter 3 describes participation rates at the school and student level. Chapter 4 describes the details of how schools and students were recruited, while Chapter 5 provides information on instrument development. Chapter 6 describes field operations used for collecting data, while Chapter 7 provides detail concerning various aspects of data management. Chapter 8 describes international activities related to data processing, scaling, and weighting. Chapter 9 describes the data available from both international and U.S. sources, while Chapter 10 discusses some special issues involved in analyzing the PISA 2009 data."}, {"section_title": "Chapter 2. Sampling", "text": "The PISA 2009 U.S. sample for the main study was selected using a two-stage design-a sample of schools and a sample of students within sampled schools. The two-stage sample design was implemented to attain an approximately self-weighting sample of students where each 15-year-old student in the United States had an equal probability of being selected for the study."}, {"section_title": "International Requirements", "text": "To provide valid estimates of student achievement and characteristics, the sample of PISA students had to be selected in a way that represented the full population of 15-year-old students in each jurisdiction. The international desired population in each jurisdiction consisted of 15-year-olds attending both publicly and privately controlled schools in grade 7 and higher. A minimum of 4,500 students from a minimum of 150 schools was required in each jurisdiction. The international guidelines specified that within schools, a sample of 35 students was to be selected in an equal probability sample unless fewer than 35 students age 15 were available (in which case all students were selected). International standards required that students in the sample be 15 years and 3 months to 16 years and 2 months at the beginning of the testing period. In the United States, sampled students were born between July 1, 1993 and June 30, 1994. The school response rate target was 85 percent for all jurisdictions. A minimum of 65 percent of schools from the original sample of schools were required to participate for a jurisdiction's data to be included in the international database. Jurisdictions were allowed to use replacement schools (selected during the sampling process) to increase the response rate once the 65 percent benchmark had been reached. PISA 2009 also required a minimum participation rate of 80 percent of sampled students from schools within each jurisdiction. A student was considered to be a participant if he or she participated in the first testing session or a follow-up or makeup testing session. Data from jurisdictions not meeting this requirement could be excluded from international reports. PISA's intent was to be as inclusive as possible. Guidelines allowed for schools to be excluded for approved reasons (for example, remote regions, very small schools, or special education schools). Schools used the following international guidelines on student exclusions: Students with functional disabilities. These were students with a moderate to severe permanent physical disability such that they could not perform in the PISA testing environment. Students with intellectual disabilities. These were students with a mental or emotional disability and who had been tested as cognitively delayed or who were considered in the professional opinion of qualified staff to be cognitively delayed such that they could not perform in the PISA testing environment. Students with insufficient language experience. These were students who met the three criteria of not being native speakers in the assessment language, having limited proficiency in the assessment language, and receiving less than one year of instruction in the assessment language. Overall estimated exclusions including both school and student exclusions were to be under 5 percent of the PISA target population."}, {"section_title": "School Sampling in the United States", "text": "The 2009 PISA school sample was drawn for the United States in July 2008. The sample design for this school sample was developed to retain most of the properties of the PISA 2006 U.S. school sample, and to follow international requirements as given in the School Sampling Preparation Manual: PISA 2009 Main Study (March 2008). The school universe includes all educational institutions that serve PISA eligible students at age 15. The U.S. school sampling frame was developed from two national databases in the National Center for Education Statistics-public schools in the Common Core of Data (CCD, http://nces.ed.gov/ccd/) and private schools in the Private School Survey (PSS, http://nces.ed.gov/surveys/pss/). These sources provide full coverage of all PISA-eligible students in the education system in the U.S. The PISA 2009 school frame was constructed using the 2005-2006CCD and the 2005-2006 PSS, the most current data at the time of the PISA frame construction. Eligible schools in the PISA 2009 school frame included 67,309 schools. These included schools with grade 7 or higher operating in the 50 states and the District of Columbia, ungraded schools, Department of Defense (DoD) domestic schools, Bureau of Indian Education (BIE) schools, special education schools, and vocational education schools. Schools in Puerto Rico and U.S. territories, DoD schools overseas, adult education institutions with no PISA eligible students and non-education institutions (e.g., home bound schools, correspondence schools) were ineligible for the study. A small fraction of PISA eligible schools were excluded in the United States because administration of the PISA assessment within these schools would not be feasible. The excluded schools were: special education schools for students with physical disabilities, and schools in hospitals, training centers, and detention centers. A total of 1,251 schools were excluded from sampling. The student loss as a result of these exclusions was estimated at 15,199 students or 0.36 percent of the enrollment of 15-year-old students. Schools were stratified by Census region (Northeast, Midwest, South, and West) and school type (public and private). Within each stratum, schools were sorted by grade range, locality, first three digits of the zip code, high/low minority percentage, and student enrollment. A systematic sample was selected independently in each stratum. The selection probability for each school was proportional to a measure of size based on the target cluster size (TCS) of 42 students in each sampled school. The student population for the 2009 PISA is defined by age to include all students between 15 years and 3 (completed) months and 16 years and 2 (completed) months at the start of the testing period. The enrollment of PISA-eligible students (ENR) in each school was estimated using school enrollment and the percentage of age-eligible students per grade. The U.S. school sample included 236 schools-201 large schools with at least 42 estimated eligible students, 10 moderately small schools with between 21 and 45 estimated eligible students, and 25 very small schools with less than 21 estimated eligible students. Table 1 shows some other characteristics of schools selected to be in the sample."}, {"section_title": "Table 1. Characteristics of sampled schools", "text": "In addition, for each school selected in the sample, the schools directly above and below the sampled school in the sampling frame were designated as substitute schools. The first school following the sample school was the first substitute and the first school preceding it was the second substitute. If an original school refused to participate, the first substitute was then contacted. If that school also refused to participate, the second substitute was then contacted. There were several constraints on the assignment of substitutes. A sampled school was not allowed to be a substitute for another, and a given school could not be assigned to be a substitute for more than one sampled school. Furthermore, substitutes were required to be in the same explicit stratum as the sampled school. If the sampled school was the first or last school in the stratum, then the second school following or preceding the sampled school was identified as the substitute. If the first substitute school did not have the same implicit stratification values as the sampled school, the first and second substitute schools were switched. Under these rules, it was possible to identify two substitutes for each sampled school. The PISA 2009 school sample was also selected to minimize potential overlaps with the school sample for the High School Longitudinal Survey (HSLS), an U.S. education study with data collection beginning in fall 2009. If any PISA substitute school overlapped with an originally sampled or first substitute HSLS school, the substitute was not to be contacted for PISA. Under this rule, 36 such schools were eliminated from the list of PISA substitute schools."}, {"section_title": "Student Sampling", "text": "In order to achieve the required student yield of 35 assessed students per school (taking into account student exclusions and absences), the United States set a target cluster size (TCS) of 42 students per school. The TCS for the main study was similar to the TCS used on PISA 2006 and was approved by the international consortium. School coordinators were asked to provide lists of all 15-year-old students (defined as students with birthdates between July 1, 1993 and June 30, 1994) in their schools. To reduce burden, a simple form was provided to schools both in hard copy and as an attachment in the follow-up email to the first mailing to school coordinators (exhibit 1). A total of 56,221 students were listed from the 165 participating schools. The average list size was 341 students. Once the list of students was received from a school, it was formatted for importing into KeyQuest, the sampling and data management software provided by ACER. After importing the list from a school, the appropriate validation checks were run, the students were sampled, and the Student Tracking and Session Attendance Forms were output from KeyQuest (exhibit 2 and exhibit 3). Westat provided the lists of sampled students to schools two to four weeks before the scheduled testing date, depending on when the school provided the list of age-eligible students. A total of 6,677 students (an average of 40.5 per school) were randomly sampled.   Codes to enter into column 8: 1 -present; 0 -absent; 2 -partially present (absent for more than 10 min) Codes to enter into column 9: 1 -present; 0 -absent."}, {"section_title": "Do NOT use code 2 in column 9.", "text": "Chapter 3."}, {"section_title": "Response Rates", "text": "As described in chapter 2, PISA 2009 international requirements stipulated that the school response rate target was 85 percent for all jurisdictions. A minimum of 65 percent of schools from the original sample of schools were required to participate for a jurisdiction's data to be included in the international database. Jurisdictions were allowed to use replacement schools (selected during the sampling process) to increase the response rate once the 65 percent benchmark had been reached. PISA 2009 also required a minimum participation rate of 80 percent of sampled students from schools within each jurisdiction. A student was considered to be a participant if he or she participated in the first testing session or a follow-up or makeup testing session. Data from jurisdictions not meeting this requirement could be excluded from international reports. The PISA 2009 standards also required that nonresponse bias analyses need to be conducted if school response rates were less than 85 percent. NCES standards for assessment surveys stipulated that a nonresponse bias analysis is required at any stage of data collection with a weighted unit response rate less than 85 percent. The non-response bias analyses are provided in Appendix H. Table 2 provides the response status of original and substitute schools. Of the 165 participating schools, 145 schools were original schools and 20 schools were substitutes. At the close of recruitment, 21 substitute schools were in a pending status or had no contact. Contact with these schools had been stopped as the focus returned to approaching refusing original schools."}, {"section_title": "School Participation", "text": "The unweighted and weighted school response rates before and after replacements are shown in Table 3.  Table 4 reports the participation status of students to be assessed including categories of nonparticipating students as defined by PISA. In total, 612 students (9.2 percent of students sampled) were coded as non-participating due to special education needs or having been withdrawn from school. Students excluded due to special education needs were considered non-participating as were students who were home-schooled or who had withdrawn. However, refusals were reported under students to be assessed because the calculation of the response rate includes in the denominator students who were absent and refused.  Table 5 reports the participation status of students to be assessed. Of the 6,677 students sampled, there were 6,065 students to be assessed. A total of 5,233 students were assessed, with 603 students absent and 229 students not assessed due to parent refusal. The overall unweighted student response rate was 86 percent. The weighted student response rate was 87 percent. Four schools had participation rates below 50 percent. Eight other schools required make-up sessions to reach an acceptable response rate of 50 percent or better."}, {"section_title": "Student Participation", "text": ""}, {"section_title": "Chapter 4. School and Student Recruitment", "text": "After experiencing difficulties achieving high levels of school participation in a number of international studies, NCES convened a task force prior to the beginning of the previous PISA administration (PISA 2006) to make recommendations to improve participation rates in international studies. The plan for recruiting schools for PISA 2009 was based partially on the recommendations of the task force. The PISA 2009 school recruitment strategy included: (1) starting recruitment early, beginning at the start of the 2008 academic year, one year in advance of the data collection; (2) approaching schools directly, and sending information to relevant school districts and states; (3) assigning recruiters to specific schools, to strengthen the personal relationship between the recruiter and school staff; (4) providing cash incentives at both the school and student levels; and (5) holding a June 2009 Summer Conference in Washington, D.C. for school staff participating in PISA."}, {"section_title": "Recruitment Materials", "text": "The materials used for recruitment included a PISA study brochure, the Summary of Activities for Schools, a PISA Resource Kit on CD, letters to states, districts, schools, and parents, a fact sheet for parents, and a student invitation form. Examples of materials used at the state, district, and school level are provided in appendix B. Examples of materials used with parents and students are provided in appendix C."}, {"section_title": "Recruitment of Schools", "text": "Five recruiters with experience in gaining school cooperation were hired to recruit schools for the PISA Main Study. Each recruiter was responsible for approximately 47 schools. Recruiters were trained prior to beginning to recruit schools. During the last week of August 2008, packages were mailed to the state commissioners and testing directors, district superintendents, and district test directors. The packages contained: \u2022 A letter from the NCES Commissioner; \u2022 A PISA study brochure; \u2022 The Summary of Activities for Schools; \u2022 The Study Timeline; and \u2022 A guide to locating information about PISA (including sample assessment items and results). Twenty-six districts (12 percent of districts with original schools) required explicit approval before schools could be contacted. Formal research requests were prepared and sent to these districts. In addition, NCES called each state assessment director to inform them about PISA, and NAEP State Coordinators in each state with sampled schools were sent a folder of materials with a cover letter alerting them that PISA was beginning to contact schools. School packages were mailed to principals in mid-September with phone contact from recruiters beginning a few days after the mailing. The materials included a description of school and student incentives. Participating schools received $200 and school coordinators received $100. Participating students received $20 and a certificate showing two and one-half hours of volunteer time. If the assessment was conducted out of school hours participating students received $35. Recruiters contacted schools by telephone to request their participation in PISA 2009. Recruiters also had frequent (at least weekly) individual and group contacts with the recruitment coordinator to discuss recruitment issues."}, {"section_title": "Challenges in School Recruitment", "text": "The PISA 2009 school recruitment was initially planned to last 4 to 5 months beginning in August 2008 and ending by the end of 2008. By mid-October 2008 it was clear that gaining cooperation from schools was not progressing at a satisfactory rate. Of schools that did not agree to participate, many were unwilling to discuss the study with us or they refused to participate because of the amount of other, mandated testing and the loss of instructional time, or because of staff shortages. Reluctance from schools required the recruitment period to be extended well beyond what was planned. School contact began in mid-September 2008 and officially ended in the first week of November 2009 -one week before the end of the assessment window on November 13. Later in the spring of 2009, as recruitment staff were attempting to schedule the participating schools, 12 schools that had initially agreed to participate early in the recruitment process (September and early October, 2008) withdrew their participation. The reasons provided tended to involve the reasons stated above as well as the local economic pressures facing schools."}, {"section_title": "Solutions and Approaches Used with Refusing Schools", "text": "Two initial approaches were implemented to increase participation: \u2022 PISA recruiters began making personal visits to schools. Staff visits continued through the end of the academic year. A total of 91 visits were made to 81 refusing or pending schools. \u2022 A letter was distributed to state chiefs through the Council of Chief State School Officers asking for state assistance. This resulted in several states contacting the project to either offer direct help or discuss the schools and approaches to use. Table 6 reports the results of visits and state assistance. Some schools received both a visit and state influence so these are not mutually exclusive results in all cases. Nor did joint contact always result in cooperation by the school. "}, {"section_title": "Student Recruitment", "text": "Once the student sample was selected within a school, PISA staff worked with the school coordinator to obtain parental consent and school coordinators distributed student invitations to participate (provided in appendix C). Study recruiters and test administrators also worked with school coordinators to answer any student or parent questions, including sharing the PISA Fact Sheet for Parents (provided in Appendix C). There were three levels of parent consent: (1) explicit consent (parent consent agreement was required); (2) implicit consent (parents could opt out of study by returning a form); and (3) notification (parents were informed of the study). The level of consent used was determined by school or district requirements. Chapter 5."}, {"section_title": "Instrument Development and Distribution", "text": ""}, {"section_title": "Test Instrument Design", "text": "The 2009 assessment instruments were developed by international experts and PISA Consortium test developers, and items were reviewed by representatives of each jurisdiction for possible bias and relevance to PISA's goals. The assessment included items submitted by participating jurisdictions as well as items that were developed by the Consortium's test developers. The final assessment consisted of 102 reading items, 36 mathematics items, and 52 science items allocated to 13 test booklets. Each booklet was made up of 4 test clusters. Altogether there were 7 reading clusters, 3 mathematics clusters, and 3 science clusters. The clusters were allocated in a rotated design to the 13 booklets. The average number of items per cluster was 15 items for reading, 12 items for mathematics, and 17 items for science. Each cluster was designed to average 30 minutes of test material. Each student took one booklet, with about 2 hours worth of testing material. Approximately half of the items were multiple-choice, 20 percent were closed or short response types (for which students wrote an answer that was simply either correct or incorrect), and about 30 percent were open constructed responses (for which students wrote answers that were graded by trained scorers using an international scoring guide). In PISA 2009, every test booklet included reading items. Some booklets also included mathematics items, science items, or both."}, {"section_title": "Assessment Materials Development", "text": "The materials for PISA 2009 in the United States included: (1) 13 different test booklets; (2) a School Questionnaire; (3) a Student Questionnaire; (4) a Test Administrator Manual; (5) a School Coordinator Manual; and (6) three separate coding guides for test items assessing reading literacy, mathematics literacy; and science literacy. The international English versions of these materials were developed by two consortia of organizations, one for the test booklets, coding guides, and manuals and the other for the student and school questionnaires. NCES was responsible for adapting the questionnaires, test booklets, and coding guides, and administration manuals for use in the United States. The goal of such adaptation was to ensure that the material used spellings and vocabulary that were most commonly used in the United States (but did not change meaning) and which reflected the actual U.S. administration plans. This involved: (1) changing spellings and vocabulary into common U.S. usage (e.g., changing \"analyse\" to \"analyze\" and \"biscuits\" to \"cookies\"); (2) adding a limited number of U.S. national items to the school and student questionnaires (e.g., adding items on racial/ethnic group to the student questionnaire); and (3) adapting the administration manuals to reflect the U.S. plans for data collection. These adaptations were checked and reviewed by the international consortium through an iterative process that occurred over the period from March 25, 2009 through May 20, 2009. After the adaptations had been approved by the international consortium, the final versions of the 13 test booklets, the Student Questionnaire, and the School Questionnaire were produced and submitted to the two consortia for a \"final optical scan.\""}, {"section_title": "Printing of Instruments", "text": "The PISA 2009 data collection instruments consisted of 13 test booklets, the Student Questionnaire, and the School Questionnaire. The Student Questionnaire was specially adapted to make it a scannable form. Copies of the School Questionnaire and Student Questionnaire are provided in appendix D. A detailed process for the review of test booklets and questionnaires was conducted by study staff and NCES to ensure a high quality of printed products. This process began with reviews of electronic copies of instruments and then reviews of hard copy versions. Numerous electronic and hard copy drafts were produced and reviewed by study staff and NCES. The final versions of all instruments were approved by NCES on July 20, 2009."}, {"section_title": "Packaging and Distribution of Materials to Field Staff", "text": "Final printed books and questionnaires were sent to a packaging facility, where security bar code labels were applied to the test books, and the booklets were bundled. To identify each test booklet, each test booklet was assigned a bar code ID number. The bar code ID numbers were printed on labels and then applied to the back cover of each test booklet. The bar codes were tracked for security purposes to ensure that all booklets sent out were returned. Each document type was also assigned an inventory number. The test booklets were then spiraled into bundles. Bundles of 13 booklets were created and spiraled in booklet 1 through 13 bar code sequence order. In addition, extra bundles of each of the 13 booklets were created for use by test administrators. The booklet type to be used was randomly assigned by the KeyQuest system and printed on the Session Attendance Form. Each bundle had a header sheet that indicated the bundle number and a range of the booklet IDs within the bundle. A customized packing list was created for each test administrator. Each test administrator was also assigned specific bundle numbers. This enabled study staff to identify where the test booklets should be at any time during the assessment. The distribution effort included packaging and mailing of test booklets, student questionnaires, and assessment related materials to test administrators. The test administrators were sent a supply of bulk materials in addition to a session box for each PISA assessment scheduled."}, {"section_title": "Chapter 6. Field Operations", "text": "Data collection consisted of three major elements: \u2022 A School Questionnaire requiring approximately half an hour that was sent to schools prior to data collection and collected during the testing visit; \u2022 A student assessment administered in a two-hour testing session, with a short break in the middle; and \u2022 A Student Questionnaire taking approximately half an hour for students to complete."}, {"section_title": "Pre-Assessment Contacts with School Staff", "text": "Each participating school was required to designate a staff member to serve as school coordinator. School coordinators received a School Coordinator Handbook to use in preparing for the assessment. A significant portion of this document provided instruction on assembling a list of students and identifying students with special needs. The international version of the Handbook instructed schools to include a special needs code on the list of PISA eligible students. The United States adapted this by sampling the students first, and then asking that students with special needs be identified from the sampled students listed on the Student Tracking Form, combining this step with determining nonparticipation. This reduced the burden on the school by significantly reducing the number of students that needed to be evaluated. In many cases, school coordinators were required to consult other student records or meet with special education staff to identify these students' specific needs and whether or not they could participate. Prior to the assessment, each school coordinator was contacted at least five times. 1. Beginning in mid-August, 2008, school coordinators were sent an initial mailing containing the School Coordinator Handbook, a hard copy student listing form, and a cover letter outlining the process for assembling and submitting a list of students to the study. 2. This mailing was followed by an e-mail outlining the process in more detail and describing options for the submission of the list of students. An electronic student listing form was attached in this email. 3. After the student list was received, study staff processed the list following PISA guidelines and using the required international sampling software, KeyQuest. This process resulted in the production of a Student Tracking Form indicating which students in the school had been chosen to participate. A second mailing was then sent to the school coordinator containing the Student Tracking Form, the School Questionnaire, the school coordinator payment, student invitations, and a cover letter. The cover letter explained the next steps of identifying students with any special education needs (SEN), indicating any students who would not be able to participate (either due to an SEN, parent refusal, or the student transferring out of the school), solidifying arrangements for the assessment, and discussing parent consent and the importance of student participation. 4. A follow-up e-mail was sent the day after the mailing had been received. This e-mail reviewed the SEN and participation information and consent materials. The consent letters requested by the school were attached to this e-mail with the direction that schools could use these items as a template and change as necessary to meet the needs or requirements of the school. 5. The final contact prior to the assessment was the pre-assessment call conducted by the test administrator approximately one week before the assessment. Using a Pre-Assessment Call Checklist, the test administrator reviewed with the school coordinator the logistics for assessment day, the Student Tracking Form, and anticipated student response. As a general rule, test administrators were instructed to make a \"courtesy call\" to the school coordinator one to two days before the assessment. The courtesy call was implemented to determine if student participation was a problem and if the test administrator could assist in any way and to cover any lastminute questions or concerns with the school coordinator. In many cases, additional contacts were made in fielding questions from school coordinators via the tollfree phone line or the project e-mail. These contacts generally dealt with questions or clarifications about student sampling. Almost all of the student lists required some level of verification or further contact with the school. "}, {"section_title": "Data Collection Training", "text": ""}, {"section_title": "Data Collection Approach", "text": "The study employed 30 test administrators, one assigned to each work area. Test administrators were assigned to one of two field managers who coordinated and monitored their work. During the testing period, test administrators reported to their field manager almost on a daily basis. To assist test administrators, 30 assistant administrators were hired to create two-person assessment teams. These assistants assisted in labeling forms, setting up the testing areas, and monitoring students during the testing sessions. The assistant was only responsible for test administration if two session areas were required. Test administrators were responsible for: a) familiarizing themselves with the Test Administrator Manual; b) successfully completing training prior to the start of assessments; c) hiring and training assistant administrators; d) conducting pre-assessment calls with school coordinator within 2 weeks of the assessment; e) conducting follow-up contacts with school coordinator 1-2 days before the assessments to ascertain if any problems with student attendance; f) ensuring that each student received the correct testing materials; g) administering the test in accordance with the internationally-specified instructions, including following the administration script; h) ensuring the correct timing of the testing sessions; i) completing the Session Administration Form; j) recording student participation on the Session Attendance Form; k) ensuring that the test booklets, student questionnaires, Student Tracking Form (STF), Session Attendance Form and Session Report Form, and completed school questionnaire, were returned to Pearson typically within 24 hours; l) reporting any issues or problems with the assessment to the field manager immediately after the assessment; and m) updating the Field Management System (FMS) with final student counts and changes from the STF, notes on the assessment, tracking numbers of the session boxes and disposition code for the school. The assessment administration consisted of three segments. The students were assessed in two segments, each 1 hour in length. These were to be administered on the same day, with a short break of approximately 5 minutes in between. After the second hour the students received another break and were administered the student questionnaire. This questionnaire took approximately 30 minutes. The timing of the assessment sessions was as follows: a) approximately 10 minutes for preparation, including preparation of students, reading instructions, distribution of test booklets, etc.; b) 1 hour for answering Part 1 of the test booklet; c) a short break of approximately 5 minutes; d) 1 hour for answering Part 2 of the test booklet; e) a short break of no more than 10 minutes; f) approximately 30 minutes for the student questionnaire; and g) 10 minutes for collecting materials, distributing incentives and ending the session. Students were told to bring something to read in case they finished early. "}, {"section_title": "Data Collection Activities", "text": "The PISA 2009 data collection was administered between September 23 and November 19, 2009. Table  7 shows the number of assessments that were completed in each month. Ten of the assessments were conducted outside of regular school hours (after school or on Saturday). Make-up sessions were scheduled only if the schools had a very low student response rate or if the logistics of holding a make-up were favorable and the response rate was below 85 percent. Seventeen make-up sessions were conducted. Chapter 7. Data Management"}, {"section_title": "Receipt Control", "text": "PISA materials were returned by the test administrators to data management staff from September 22nd through November 23th. When a shipment arrived, the materials were forwarded to a receiving/opening area. The school ID, receipt date, and the return shipper's tracking number were recorded in a receipt system. Data management staff notified test administrators if a school's materials were not received within three days after its test date. Using specifications provided by the managers, staff were trained to check each school's materials using the Student Attendance Form (SAF) returned with the booklets. They verified: \u2022 All student booklets and student questionnaires for students listed on the SAF were returned to Pearson; \u2022 Information coded on the booklet covers matched the SAF, i.e., school number, student ID, and participation status; \u2022 A Student Tracking Form (STF) and Session Report Form (SRF) had been returned with each school's materials; and \u2022 The school questionnaire was returned. If one of these three forms (STF, SAF, SRF), student booklets or student questionnaires were missing, data management managers were notified. Data management managers then contacted the data collection managers and test administrators for resolution. Once a school's material was verified as complete, booklets and questionnaires were sorted into separate work units. Unused or unassigned booklets were separated and the security bar codes were recorded into a file. After processing was completed, this file was combined with the bar codes of the completed booklets. Staff compared the bar codes to those shipped out to identify if any books were not returned. All booklets and questionnaires were returned by data collectors."}, {"section_title": "Batching", "text": "After check-in was completed, the completed booklets were forwarded to the batching area. Each batch was assigned a unique batch number. This number, created on a Work Flow Management (WFM) system for all documents, facilitated the internal tracking of the batches and allowed departmental resource planning. The batch number was written on a control sheet and was present on all batch/stack headers. Each batch type had its own unique control sheet that listed all of the workstations (departments) the group of work needed to pass through before reaching a clean post-edit. A clean post-edit meant that everything had been scanned, key entered, and edited correctly based on the specifications written by software development staff and the International Codebooks. ACER guidelines required that a specialized batching system be used for test booklets. This system generated \"blue batches\" (those booklets to undergo multiple coding) and \"yellow batches\" (those to undergo regular coding). The WFM indicated when the batch had moved to the next process -data entry and/or scanning. Data management staff was able to view what process the batch was in at any given time using this system. The student booklet and School Questionnaire batches/carts were forwarded to the Data Input area for manual key entry. The Student Questionnaire batches/carts were sent to the Scanning area."}, {"section_title": "Data Entry", "text": "Depending on the PISA document, one of two methods was used to transcribe PISA student data to an electronic form. \u2022 The test booklets and school questionnaire data were key entered into a data file using the Falcon data entry system, which is a mainframe data system. \u2022 The data on the student questionnaires was entered using 9909I optical-scanning equipment. Data entry staff entered 6,792 test booklets (including unused booklets) and 163 School Questionnaires into the Falcon system (two School Questionnaires arrived late and were entered manually during file creation). There were 6,792 Student Questionnaires scanned using the 9909I scanning equipment. For the test booklets and School Questionnaire, Software Development staff created key-entry specifications based on the national booklets and the international codebooks. Data Input senior staff created key entry screens on the Falcon System using the specifications and the actual books as guides. Ten qualified, permanent trained staff keyed the student booklets and School Questionnaires. A standard practice to ensure the data was captured correctly required the student booklets and School Questionnaires to be keyed twice (keyed and then verified) by two different Data Input staff members. If the information from the keyer did not match the verifier, the field came up in error. The discrepancy was resolved by a senior team member."}, {"section_title": "Scanning", "text": "Scan development staff created a scan program for the Student Questionnaire using specifications provided to them by the Software Development staff. The specifications were based on the International Codebook and the layout of the actual questionnaires. The scanned data values captured were coded in the same manner as the student booklets and school questionnaires. To properly scan the booklets, the spine (folded or left side of the booklet) needed to be removed. A slitter machine was used for this process. A scan operator entered the WFM batch number and application name from the control sheet into the machine to \"call-up\" the correct scan program. This insured that the program being run accepted the correct, and only the correct, book/form types for that batch of work. Diagnostic sheets were run through the scanner first to ensure the scanner was picking up the gridded circles correctly. Quality control sheets were also randomly inserted into each stack of books by the scan operator. This was another quality check to help ensure the documents were processed accurately within stringent control limits. The images of constructed-response response items in the student questionnaire were saved as Tagged Image Files (TIF). The area of the page that needed to be clipped was defined prior to scanning through the document definition process. The fields from unreadable pages were coded \"X\" as a flag for resolution staff to correct. These images were sent to the Image editing area for manual key entry. Any image document or sheet unreadable by the image scanning system was taken to a flatbed scanner to be scanned into the system. Once scanning of all documents in the batch was complete, the scan operator closed out the batch and the batch of work was then forwarded to the next workstation listed on the control sheet (Image Editing)."}, {"section_title": "Data Editing", "text": "To ensure all data entry and editing functions had been set up properly and per the International Codebooks, the first batch of the student assessment booklets, Student Questionnaires, and School Questionnaires went through a quality control check. Software Quality Specialists checked to make sure the data was collected and edited according to the specifications given to the programmers. They verified that the output file matched the responses recorded in the booklets and questionnaires."}, {"section_title": "Assessment Booklets and School Questionnaires", "text": "The data from the assessment booklets and questionnaires was run through a Pre-Edit Program and edited according to specifications supplied by the Software Development staff based on the PISA International Codebook. If there were problems, the paper or image editing staff corrected them to ensure the data were clean. The Editing staff reviewed a computer-generated edit log and the area of the source document that was noted as being suspect or as containing possible errors in the assessment booklets and school questionnaires. The Editing staff checked this piece of information against the PISA test booklet or school questionnaire. The corrected edit log was forwarded to the Key Entry staff for processing (re-keying). When all corrections were entered and verified for a batch, an extract program pulled the corrected records into a mainframe dataset. At this point, the mainframe edit program was initiated. If there were further errors, a new post-edit listing was printed and the cycle was repeated. These edited batch files were uploaded to the mainframe."}, {"section_title": "Student Questionnaires", "text": "After a student questionnaire batch had been scanned, it was routed to the Image Editing area. Staff reviewed errors identified in each batch through an on-line editing system. Image clips requiring edits were routed to on-line editing stations. For rapid resolution, the edit criteria for each item in question appeared on the editing screen along with the suspect item. Corrections were made immediately. The system employs an edit/verify system which ultimately means that two different people view the same suspect data and operate on the item separately. The verifier made sure the two responses were the same before the system accepts that item as being correct. If the editor cannot determine the appropriate response, he or she escalated the suspect situation to a supervisor. After the data from the assessment booklets and questionnaires had been entered, edited, and no more errors existed, the batch was considered clean. Project staff received a \"processed documents\" report from those batches on a daily basis or on-demand. This information taken from a Process Control System (PCS) which listed the number of documents processed for each school received."}, {"section_title": "Occupational Coding of Student Questionnaires", "text": "Students were asked to report their parents' or guardians' occupations on the Student Questionnaire. These responses were coded during the processing of the student questionnaires. Student responses in the student questionnaire for occupational coding were captured during the scanning process. Once a batch was designated as clean, a report listed the student ID and the occupational responses. Pearson staff coded each response using the International Standard Classification of Occupations (ISCO) manual (International Labour Organization (ILO) version 1988) occupational codes located in the Data Management Manual. Two reports were printed for each batch. Two staff members coded them separately. These were entered separately into a mainframe file. A comparison was run and if any disagreements in codes were identified, a third person decided on the correct code. Development staff corrected the file and continued to run the comparison until all discrepancies were resolved. Study staff coded 13,584 responses due to double-coding of each response."}, {"section_title": "Coding of Test Booklet Constructed-Response Items", "text": "There were 53 reading items, 15 mathematics items, and 19 science items that required students to construct a response. The constructed-response assessment items for PISA 2009 were coded during November and December 2009. The study used two (2) Scoring Directors, four (4) supervisors, and 24 coders to code the responses from U.S. students. Both Scoring Directors attended coding training in Brussels, Belgium."}, {"section_title": "Coding Materials", "text": "Separate coding guides were prepared for reading, mathematics, and science constructed-response items. International coding guides were provided by ACER, and these guides were adapted to reflect U.S. adaptations of the test booklets. The Scoring Directors received photocopied examples of U.S. student responses in early October, and used these responses to supplement the international coding guides. Unlike the neatly typed responses on a single page in the international coding guides, these practice materials looked like actual student responses in booklets (for example, handwritten, written under the prompt, other prompts on the page). These additional practice materials were reviewed and approved by NCES before scorer training began."}, {"section_title": "Scorer Training", "text": "Supervisor training occurred on November 12 and 13, 2009 in Mesa, Arizona. The Scoring Directors trained the four scoring supervisors on the items that each trained and supervised during the coding window. Training and scoring began on November 16, 2009. A brief introduction to the PISA project was given to all scorers as a large group. All scorers were experienced scoring personnel. An overview of the PISA assessment included general coding guidelines, inter-rater reliability and rate expectations, scorer bias and grade-level considerations regarding the PISA items. These experienced scorers already understood confidentiality and quality issues and the importance of coding accuracy. All scoring staff signed the PISA 2009 confidentiality agreement prior to coding. The scorers were then separated into three teams and the training began with the first cluster for each team. Reading had one team of 16 scorers. Mathematics and science each had one team of four scorers. PISA training and coding occurred on a cluster basis. Thus, the team trained on all items within the cluster and then coded all responses for those items before the trainer introduced the next cluster. Since all of the PISA scorers were experienced, the study had requested a deviation from ACER in September 2009, in order to code all of the responses within the cluster for one student before moving onto the next student's responses. This request was approved. Scorers read the item and any accompanying materials, such as a reading passage and ancillary materials (e.g., cartoons, graphs, maps, brochures). The supervisor then read the item aloud, and then read the description for each code level category from the coding guide. Supervisors discussed the coding guide and any student examples within the coding guide. Scorers were encouraged to ask questions, but the supervisor discouraged over-generalizations until scorers had seen more examples. After reviewing the coding guide and the examples, scorers independently coded the practice examples. Practice examples generally consisted of: (a) the range of codes on the scoring guide; (b) different levels of responses within a code level (i.e., high \"2\" and low \"2\"); (c) a mix of solid and borderline examples; and (d) examples of common student responses. Practice examples were ordered randomly. Some of the practice examples were from the international coding guides while others were from U.S. students. Upon completion of the practice examples, the supervisors reviewed the practice sets with their teams. The supervisors asked scorers to explain why examples were scored as they were, and compared the codes given by individual scorers with the pre-determined codes. During training, supervisors monitored each scorer on the individual items within each PISA cluster, noting any who had difficulty. Supervisors kept notes on all coding decisions and changes made to practice sets during training of the items. Coding guides and training materials with annotations were complete and correct and reflected actual training for each item. Information from flip charts and/or wall charts developed by supervisors and used to document coding decisions were included in the documentation. The supervisors made certain that all scorers were taking notes on such decisions throughout the training sessions."}, {"section_title": "Paper Flow", "text": "Coding of test booklets occurred in an internationally-defined sequence. The yellow batches (those not requiring multiple scoring) were coded first. Then the second, third, and fourth clusters in the blue batch booklets were coded. Finally, the first cluster in the blue batch booklets (those requiring coding by four scorers) were coded. The coding of multiple-coded clusters followed the prescribed procedure by ACER, in which the same set of coders scored every booklet in a batch. The single-coded clusters were coded by any scorer on the team. Score sheets were created for each cluster in a booklet. The first cluster in each blue batch booklet had four score sheets provided for first, second, third and fourth coding. The remaining clusters had only one score sheet per cluster. Scoring Directors scheduled the scoring of clusters to avoid crossover between subjects wherever possible (as all three subjects occurred in the same booklets, all three teams needed access to a booklet at some point in the process). Warehouse staff routed batches to appropriate scorers as directed by Scoring Directors."}, {"section_title": "Scoring Supervision", "text": "Careful, ongoing monitoring of scorers was crucial to all scoring. While this monitoring involved Scoring Directors, the \"front line\" was the supervisor for each scoring team. The supervisors ensured that backreading took place at the specified rate and that it was careful and thoughtful. Backreading allowed Scoring Directors and supervisors to read student responses that had previously been scored by each team member. The Supervisors reviewed the code assigned and evaluated whether or not it was accurate. The supervisors were required to backread (check the scoring of) all scorers, with a target of backreading five percent of all scorer work done on a daily basis. However, this level of backreading could be adjusted depending upon a variety of factors (e.g., rate of scoring, item type). Backreading began almost immediately after scoring started and continued throughout the scoring process. This allowed the supervisors to identify coding problems quickly. When beginning backreading a new item, the supervisors considered what he or she observed during the training and practice scoring sessions. The supervisors paid particular attention to the accuracy of scorers who had difficulty applying the coding guide consistently during training and would backread those scorers first. Additional training on an individual or group basis was sometimes needed based on information gleaned from backreading. Some scorers needed to adjust their interpretation of the coding guide or specific types of student responses. Supervisors shared examples of student responses as part of this follow-up training. Training and coding of the mathematics items was completed on December 4, 2009, science items on December 7, 2009, and reading items on December 9, 2009. Table 8 shows the number of hours required to complete the coding process. "}, {"section_title": "Score Recording", "text": "A paper-based scoring system, ePS, was used to record these scores of constructed-response items. Software Development staff created individual score sheets for each cluster within each of the 13 booklets. They had only one cluster's items printed per score sheet, with the bar code identifier of the booklet preprinted. Once a score sheet had been coded by a scorer, the score sheet was scanned using the ePS system. Study staff scanned 6,794 score sheets using the ePS scanning system. During scanning, the scoring system notified the technician of any missing codes, blank responses, or out-of-range codes on the score sheets. This system also produced two reports: sheets-left-to-scan and inter-rater reliability. The \"Sheets left to Scan\" report insured all score sheets had been coded and scanned. After coding was complete, the responses captured were uploaded to a mainframe file. Software Development staff then merged these with the multiple-choice responses. The combined file was then imported into KeyQuest."}, {"section_title": "Inter-Rater Reliability (IRR)", "text": "The study used the one hundred responses in the blue batches of each booklet that were coded by four different scorers to calculate the IRR percentage. About one third of all booklets were selected to be placed in the blue batches. Only the items in the first cluster within these booklets were read by four scorers. All of the yellow batch booklets (as well as the clusters in the second, third, and fourth positions in the blue batches) were coded before the IRR are calculated. The inter-rater reliability for specific items ranged from 92.2 to 100.0 percent, with 63 percent of the items having reliabilities of 90.0 percent or above. The reliabilities of specific items are shown in Appendix E."}, {"section_title": "File Creation and Consistency Checks", "text": "All data files were loaded into the KeyQuest software, the international data management software, as directed in the Data Management Manual. Before the upload process, Software Quality Specialists verified the files were in the correct format based on the KeyQuest International Codebooks. Staff members ran several validation reports to check for mismatches and duplicate records."}, {"section_title": "Data Confidentiality Safeguards", "text": "The Education Sciences Reform Act of 2002 explicitly requires that NCES protect the confidentiality of all those responding to NCES-sponsored surveys so that no individual respondent can be identified. More specifically, NCES Standard 4-2, Maintaining Confidentiality (NCES 2002), provides guidelines for limiting the risk of data disclosure for data released by NCES. Data disclosure occurs when an individual respondent has been identified through the use of the survey item responses and other external data sources. The following discussion describes the procedures used to reduce the risk of data disclosure for PISA 2009, in accordance with the guidelines specified in NCES Standard 4-2. All students and schools participating in the PISA do so with the assurance that their identities will not be disclosed. Confidentiality procedures in place included the following: (1) all employees with access to the data signed affidavits of data confidentiality; (2) questionnaires were sealed by students after completion; and, (3) names of students and schools were removed by field staff from the assessment booklets, the questionnaires and all other related materials, and replaced with unique identification numbers. In addition to data collected directly from schools and students, additional information was used during the PISA sampling, data collection, and weighting processes and these variables too were considered as part of the review to determine disclosure risk levels. In addition, after the School Questionnaire and Student Questionnaire files were successfully entered into Keyquest, study staff conducted disclosure analyses to determine if individual schools or students could be identified using data from other publicly available data sources. While no public data collections identify students by name, three publicly available data collections do identify schools by name. These are: the Common Core of Data (CCD), a detailed public school listing; the Private School Survey (PSS), a detailed private school listing; and, the QED data collections produced by Quality Education Data, Inc., a privately owned education research firm. The QED data contain a school-based file that provides demographic information for both public and private schools along with the names of the schools. Thus, there is some possibility that schools at least, and perhaps students as well, could be identified if comparisons of these data sets with the PISA data set allowed the identification of schools. It might be possible to identify PISA schools by taking variables from the PISA school data and searching the publicly available data files for schools with a matching profile. However, since the variables in the PISA data files were obtained from responses to the school questionnaire, for the most part, exact profile matches are unlikely. Even then, one would not know for certain whether any of the matched schools were the actual PISA schools, or whether the match had simply arisen by chance. Nevertheless, school matching analyses were undertaken using probabilistic matching algorithms approved by the IES Disclosure Review Board (DRB) for use in disclosure analyses. These algorithms identify schools with some potential for identification. In order to provide further protection, elements of the data from schools identified as \"disclosure risks\" in this way were perturbed using the procedures approved by the DRB. After perturbation, the data were subjected to another round of analyses to ensure that the potential for identification no longer existed. An additional measure was taken to reduce further the risk of disclosure of an individual respondent. This measure is referred to as \"data swapping\", a DRB requirement that reduces risk by modifying microdata. In data swapping, a probability sample of records is paired with other records on the file using selected characteristics, and then some identifying variables are swapped between pairs of records. The sampling rate for PISA swapping was designed to protect the confidentiality of the data without affecting the usability of the dataset. All questionnaire data (school and student) were involved in the swapping. This method is an effective way of keeping as much valuable data as possible while protecting respondent identity. Swapping preserves the univariate frequencies, means, and variances, although it may affect multivariate relationships a little. Pre-and post-swapping percentage distributions (unweighted and weighted) and correlations were reviewed to ensure data quality was maintained."}, {"section_title": "Data Editing and File Delivery", "text": "After the data files were successfully entered into Keyquest and disclosure review activities had been completed, study staff implemented the data editing procedures required by ACER. On the basis of those procedures, staff identified a few minor data inconsistencies (e.g., missing forms for students marked as present on the Student Attendance Form). Based on review of the data, these inconsistencies were resolved and the dataset was forwarded to ACER on January 27, 2010. After receipt, ACER implemented some additional data checking procedures and asked for clarification on a few selected items. Study staff provided responses, and ACER made some final editing changes to the dataset. For the International Coding Review (a reliability check of coding across countries), the international consortium sent a list of 80 student IDs each from Booklets 8 and 12 which were multiple-coded in the United States and which would be recoded by the international contractor. The study provided an electronic copy of the first cluster in each of these booklets to ACER via their secure website on February 10, 2010."}, {"section_title": "Chapter 8. Processing, Scaling and Weighting", "text": ""}, {"section_title": "International Data File Cleaning and Editing", "text": "ACER cleaned each of the national data files to ensure that data cleaning was standardized among all participating countries. ACER's role at this point was to check that the international data structure was followed, check the identification system within and between files, correct single case problems manually, and apply standard cleaning procedures to questionnaire files. Results of the data cleaning process were documented and shared with the national project managers and included specific questions when required. The national project manager then provided ACER with revisions to coding or solutions to anomalies. ACER then compiled background univariate statistics and preliminary classical and Rasch item analysis."}, {"section_title": "Missing Data", "text": "PISA does not impute missing information for questionnaire variables. The international database and the U.S. database contain four kinds of missing data codes that are used across all countries. \"Missing\" data occur when a respondent is expected to answer an item but no response is given. Responses that are \"invalid\" occur in multiple choice items for which an invalid response is given. An item is coded \"not applicable\" when it is not possible for the respondent to answer the question (e.g., an assessment item not included in the student's booklet or an item to be skipped based on a previous item). Finally, test booklet items that are \"not reached\" are consecutive missing values starting from the end of each test session. All four kinds of missing data are coded differently in the PISA 2009 database."}, {"section_title": "Weights for U.S. Data", "text": "The use of sampling weights is necessary for the computation of statistically sound, nationally representative estimates. Survey weights adjust for the probabilities of selection for individual schools and students, for school or student nonresponse, or for errors in estimating the size of the school or the number of 15-year-olds in the school at the time of sampling. Survey weighting for all jurisdictions participating in PISA 2009 was carried out by Westat, as part of the PISA consortium. The internationally defined weighting specifications for PISA 2009 included base weights and adjustments for nonresponse. The school base weight was defined as the reciprocal of the school's probability of selection. (For substitute schools, the school base weight was set equal to the original school it replaced.) The student base weight was given as the reciprocal of the probability of selection for each selected student from within a school. These base weights were then adjusted for school and student nonresponse. The school nonresponse adjustment was done individually for each jurisdiction using implicit and explicit strata defined as part of the sample design. In the case of the United States, two variables were used for stratification: school control and Census region. The student nonresponse adjustment was done based on students' explicit stratum; within cells, grade and gender were also used to define nonresponse adjustment. Trimming factors at the school and student levels were used to reduce the size of large weights, since large weights can substantially increase sampling variance. All PISA analyses were conducted using these adjusted sampling weights."}, {"section_title": "Scaling of Student Test Data", "text": "Thirteen versions of the PISA test booklet were used, each containing a different subset of items. The fact that each student completed only a subset of items means that classical test scores, such as the percent correct, are not accurate measures of student performance. Instead, scaling techniques were used to establish a common scale for all students. For PISA 2009, item response theory (IRT) was used by the international contractor to estimate scores for reading, mathematics, and science literacy, as well as for three reading literacy subscales: accessing and retrieving information, integrating and interpreting, and reflecting and evaluating. 1 IRT identifies patterns of response and uses statistical models to predict the probability of answering an item correctly as a function of the students' proficiency in answering other questions. With this method, the performance of a sample of students in a subject area or sub-area can be summarized on a simple scale or series of scales, even when students are administered different items. "}, {"section_title": "Codebooks", "text": "Codebook for student questionnaire data file Codebook for school questionnaire data file Codebook for parent questionnaire data file Codebook for cognitive item response data file Codebook for scored cognitive item response data file SAS control files SAS syntax to read in student questionnaire data file SAS syntax to read in school questionnaire data file SAS syntax to read in parent questionnaire data file SAS syntax to read in cognitive item response data file SAS syntax to read in scored cognitive item response data file SPSS control files SPSS syntax to read in student questionnaire data file SPSS syntax to read in school questionnaire data file SPSS syntax to read in parent questionnaire data file SPSS syntax to read in cognitive item response data file SPSS syntax to read in scored cognitive item response data file"}, {"section_title": "Data sets in TXT format (compressed)", "text": "Note that some of these files are very large. Student questionnaire data file School questionnaire data file Parent questionnaire data file Cognitive item response data file Scored cognitive item response data file"}, {"section_title": "Compendia", "text": "The compendia provide the distribution of students according to the variables collected through the student, information communication technology, parent and school questionnaires. The performance means per category are also provided. Compendium for the student questionnaire Compendium for the school questionnaire Compendium for the parent questionnaire Compendium for the ICT and EC questionnaire Compendium for the RFS questionnaire Compendium for the cognitive item responses"}, {"section_title": "U.S. National Data Files", "text": "Data collected in the United States for PISA 2009 can be downloaded from the international site or from the NCES website. The files on the international website contain data for all countries, including the United States. The NCES files, which include only data for the United States, are as follows:"}, {"section_title": "Student data", "text": "\u2022 The data are contained in US_ST09.TXT. This file contains questionnaire items and derived variables and index scores based on the student questionnaire; plausible values for overall reading, reading subscales, science scale, and the mathematics scale from the assessment; and student sampling weights and replicate weights. There are 5,233 cases in this file. Since the data are hierarchical (students are clustered with schools), each student record contains identification variables that enable the user to merge the school data with the student data, using the variable SCHOOLID. "}, {"section_title": "School data", "text": "\u2022 The data are contained in US_SC09.TXT. This file contains items from the school questionnaire, derived variables and index scores based on the school questionnaire, and the school sampling weight. There are 165 cases in this file. "}, {"section_title": "Scored cognitive (assessment) item data", "text": "\u2022 The data are contained in US_ASSESM09.TXT. This file contains scores of student responses to each item in the assessment. There are 5,233 cases in this file. It should be noted that data are in two response formats: (1) valid scores = 0, 1; 0 = no credit, 1 = full credit; and (2) valid scores = 0, 1, 2; 0 = no credit, 1 = partial credit, 2 = full credit. Also note that unlike the international files, there is no cognitive item response file (unscored responses). The majority of the items have not been released, so there is little descriptive information about them. The variable label for S131Q04D in the file is S131Q04D in the file is \"SCIE -P2006 (broken link) Good Vibrations (Q04).\" This means that because of changes in responses to the item, responses on this item should not be compared to responses on the same item in previous PISA administrations. "}, {"section_title": "National and International Variables", "text": "The U.S. national data contain both the \"international variables\" (questionnaire and assessment variables used by all countries) and a few \"national variables\" (questionnaire variables used only in the United States). Note that the same assessment items were used by all countries. There are also some variables that appear in the international files that are missing for U.S. cases. These include three questionnaires not used in the United States: the parent questionnaire, the Information Communication Technology (ICT) questionnaire, and the Education Career (EC) questionnaire. Variables used only in the United States and those not used in the United States are shown in tables 9 and 10.  "}, {"section_title": "International variables recoded from U.S. questionnaire variables", "text": "Three international questionnaire items on the Student Questionnaire needed to be rewritten to be applicable for U.S. questionnaires, and thus required international recoding. \u2022 The international question ST05Q01 (Did you attend <ISCED level 0>?) was adapted into two U.S. questions ST05N01 and ST05N02 (Did you attend pre-school? and Did you attend kindergarten?). The U.S. values were recoded in ST05Q01 to reflect international values. \u2022 The international questions ST10Q01 and ST14Q01 (What is the highest level of education completed by your mother/father?) included five options, one of which (ISCED level 3B, 3C) was not relevant in the U.S. Thus the U.S versions of these variables (ST14N01 and ST18N01) have four valid responses rather than five and have different value labels than the international versions."}, {"section_title": "Variable Names", "text": "The variable names created by the SPSS and SAS syntax files are those used on international data sets. It should be noted that on the Student Questionnaire, those variable names do not correspond with the item numbers on the questionnaire because a number of U.S.-only items were inserted near the front of the questionnaire. For convenience, the item numbers on the U.S. Student Questionnaire and School Questionnaire are listed in parentheses at the end of the variable name (e.g., \"Mother's occupation (Q13)'')."}, {"section_title": "Derived Variables", "text": "The international contractors for PISA have developed a number of derived variables for use in their analyses, and these variables have been included on the student and school files. They appear after the questionnaire variables and have variable names that do not contain numerals. The explanation for many of these variables is included in Appendix E, which is abstracted from two volumes of the international report (OECD 2010c and OECD 2010d). A more complete explanation of these variables will be provided in the international PISA 2009 Technical Report (forthcoming). Chapter 10. Using the PISA 2009 Data Files"}, {"section_title": "Special Considerations-Plausible Values and Replicate Weights", "text": "Three aspects of the design of PISA need careful attention in any analysis. The first stems from the sample design. Schools and students had unequal but known probabilities of selection. As a consequence, to generalize to the population sampled, analyses will need to apply the sampling weights provided in the file. The second aspect to be considered also stems from the sampling design and bears on the calculation of standard errors. Since the sample design is complex (a two-stage, stratified cluster design), most software packages, operating on the assumption of a simple random sample, will produce biased estimates of standard errors. Special procedures that use the replicate weights contained in the data file are called for, and are described in detail in the PISA Data Analysis Manual, SPSS, Second Edition (OECD 2009a). These procedures are implemented in several stand-alone software packages (WesVar, AM, and SUDAAN, for example), but can also be implemented in SPSS using the macro posted on the NCES website, or in SAS using the information provided in the PISA Data Analysis Manual, SAS, Second Edition (OECD 2009b). Standard errors produced in published reports were estimated using Fay's method of Balanced Repeated Replicates (BRR) with 80 replicates and the Fay coefficient set to 0.5. That method should be specified when using SUDAAN or other stand-alone software packages to analyze the PISA data. The third aspect arises from the design of PISA and the use of plausible values in analysis. In PISA, as in many national or international assessments, students are not administered every assessment item. Each item then has missing student responses, though these are missing by design. As a consequence, it is not possible to estimate scores for individual students. Instead, the results of individual students are aggregated to produce scores for groups of students (e.g., all U.S. students, U.S. female students, etc.). For analysis purposes, PISA datasets include sets of five \"plausible values\" for each student for each overall subject area score and each subscale score. The plausible values are intended to represent the estimated distribution of scores of students similar to the given student in terms of responses to the assessment items and background questionnaire items. 2 What this means for analyses is that, in effect, any analyses involving the achievement scores must be done five times, once for each plausible value, and then the results must be averaged. A special provision also needs to be made in the estimation of the standard errors and is best done using the SPSS or SAS macro developed for this purpose. Again, these issues are discussed in the PISA Data Analysis Manual, SPSS, Second Edition (OECD 2009)."}, {"section_title": "Nonresponse Bias", "text": "Detailed analyses were conducted to determine if nonresponse at either the school or questionnaire item level resulted in apparent biases in the results. The results indicated that school nonresponse to the study resulted in limited apparent bias of results. On the other hand, analysis of nonresponse to specific questionnaire items indicated that for three items on the School Questionnaire (Q9ab, Q9bb, and Q9cb) and ten items on the Student Questionnaire (Q12d, Q12e, Q12f, Q15a, Q15c, Q15d, Q19a, Q19b, Q19c, and Q19d) there was significant nonresponse (more than 15 percent) and statistically significant differences between respondents and nonrespondents. (The full nonresponse bias analysis report is included in Appendix H."}, {"section_title": "Merging School and Student Data", "text": "The PISA sample was designed to yield a representative sample of 15-year-old students enrolled in schools; the school sample was designed to optimize the selection of these students. In these circumstances, it is usually recommended that the school data should be disaggregated across students and school attributes be treated as \"student characteristics\" for the purposes of the analyses. This disaggregation can be accomplished by merging the school-level data to the student file using SCHOOLID and the resulting file analyzed at the student level using the replicate weights (W_FSTR1-W_FSTR80). In anticipation of questions about the similarities and differences between the PISA and NAEP reading assessments, and what each can tell us about U.S. students' reading skills, NCES prepared this paper, which discusses aspects of each assessment. In particular, the paper discusses the purposes, target populations and reporting levels, and content assessed by each. For NAEP, the paper focuses on the reading assessments for students in grades 8 and 12, as these are the two target populations closest to PISA's target population of 15-year-olds. To examine and compare the content measured by PISA and NAEP, NCES convened a panel of reading experts 1 to compare how PISA and NAEP define reading, aspects of the texts used as the basis of the assessments, and the reading processes required in each assessment. This information is intended to help the press and others understand the similarities and differences between the assessments and to help identify what PISA and NAEP each contribute to the overall knowledge base on U.S. student reading performance."}, {"section_title": "Comparison of the PISA 2009 and NAEP 2009 Reading Assessments", "text": ""}, {"section_title": "A.2 Purposes of PISA and NAEP", "text": "The goals of the two assessments have subtle but important distinctions with regard to U.S. curricula. International assessments, such as PISA, support comparisons of student performance among countries but provide less information for within-U.S. comparisons. NAEP supports comparisons of student performance among states, public and private schools, student demographic groups, and a set of urban A-3 public school districts. 2 Both PISA and NAEP are conducted regularly to allow the monitoring of student outcomes over time. PISA is conducted every three years and NAEP is conducted, for some subjects (including reading), every two years; for some subjects, every four years; and for other subjects, less frequently. 3 PISA provides internationally comparative information in the United States on the reading, mathematics, and science literacy of students at an age that, for most countries, is near the end of compulsory schooling. The objective of PISA is to measure the \"yield\" of education systems, or what skills and competencies students have acquired and can apply in these subjects to real-world contexts as they near the transition from compulsory schooling. PISA's literacy concept, which applies to the reading, mathematics, and science assessments, emphasizes the mastery of processes, understanding of concepts, and application of knowledge and functioning in various situations. By focusing on literacy, PISA assesses what students have learned in and outside of the school environments. NAEP reports information on achievement in reading and other subjects at the 4th-, 8th-, and 12th-grade levels across the country. NAEP assessments are based on assessment frameworks and achievement levels (i.e., Basic, Proficient, and Advanced) established by the National Assessment Governing Board. NAEP assessment frameworks and achievement levels are based on the collaborative input of a wide range of experts and participants from the government, education, business, and public sectors in the United States. The frameworks are intended to be a blueprint for the assessment (specifying what should be assessed), and the achievement levels act as performance standards for each subject area and grade, showing what students should know and be able to do (NCES 2009, p. 4-5). The focus of NAEP on subject matter expectations in the United States distinguishes it from PISA, the content of which is determined in collaboration with other countries. The focus in PISA on the yield of education systems and the application of competencies in real-world contexts distinguishes it from NAEP, which measures school-based performance and abilities to read and understand written texts and to interpret and use what students have read in ways that are appropriate to the type of text and situation."}, {"section_title": "A.3 Target Populations Assessed by PISA and NAEP", "text": "The students assessed represent different target populations. PISA and NAEP are both sample-based assessments, meaning that each assessment administered is to a sample of students (rather than to all students) and the results are generalized to the larger population. However, each assessment defines the population to which it is generalizing, and thus from which the sample is drawn, differently. One distinction between main NAEP and PISA is that NAEP uses gradebased samples, whereas PISA uses an age-based sample. These choices relate to the purpose of each 2 In \"main NAEP,\" students in grades 4, 8, and 12 are assessed; in \"long-term trend NAEP,\" students ages 9, 13, and 17 are assessed. This paper focuses on main NAEP (on which the 2009 NAEP results are based); thus, all statements about NAEP in this paper refer to main NAEP. 3 PISA is on a 3-year cycle, with one domain being featured as the major domain every 9 years. program-NAEP, to report on student achievement based on what students learn by a specific grade in school; and PISA, to describe the yield of education systems toward the end of compulsory schooling. The PISA target population is all 15-year-old students. In 2009, this included all students who were 15 years and 3 months to 16 years and 2 months at the beginning of the testing period (fall 2009) and who were enrolled in school, regardless of grade level or full-or part-time status. The majority of respondents in the U.S. PISA 2009 sample were in 10th grade (68.5 percent), but some were in the 11th (20.3 percent), 9th (10.9 percent), or another grade (0.3 percent). The NAEP target populations are all students in the 4th, 8th, and 12th grades, and NAEP reflects the performance of U.S. students enrolled in these grades. Thus, the PISA results are for students who are mostly in grades between those being tested for NAEP (the 8th and 12th grades), and closer in grade proximity to those taking the NAEP 8th-grade assessment because of the timing of the respective assessments (with PISA given earlier in the school year than NAEP)."}, {"section_title": "A.4 Reporting Levels and Sample Sizes for PISA and NAEP", "text": "PISA and NAEP are designed to provide results at different levels of aggregation and, as a result, have different levels of precision. PISA and NAEP are both designed to provide information about U.S. students' performance aggregated to the national level and for subgroups of the population (e.g., subgroups defined by gender and race/ethnicity). NAEP, however, is also designed to provide reading results for individual states and some large urban districts. 4  The reporting requirements for NAEP and PISA have implications for sample sizes and, in turn, for which subgroups' results can be reported and for the precision of the estimates. The NAEP national sample comprises the state and district samples and thus is extremely large. For example, the sample size for the grade 8 NAEP 2009 reading assessment was more than 160,000 students. In contrast, in 2009, PISA assessed 5,233 students. Because of its large sample size, NAEP is able to reliably measure achievement for more subgroups than PISA can. For example, while both NAEP and PISA can report achievement for students based on their racial and ethnic classifications, in 2009 the PISA sample did not include a sufficiently large sample size to report achievement for students who are identified as American Indian/Alaska Native or Native Hawaiian/Other Pacific Islander. Moreover, because of NAEP's large sample size, it can detect smaller differences between subgroups or over time than can PISA. For example, while the standard error associated with the NAEP 2009 reading national mean score for grade 8 was 0.3 (NCES 2009), the standard error for the PISA 2009 reading mean score was 3.7 for the United States (Fleischman et al. 2010). Related to sampling is the degree of inclusion of students with special or language needs. Both NAEP and PISA strive to be inclusive and ultimately achieve similar inclusion rates, although their specific policies differ. PISA is designed to be as inclusive as possible and requires that no more than 5 percent of the target population be excluded from testing. Exclusions are allowed at both the school level (e.g., a geographically remote school) and within schools at the student level, including students with a functional disability, intellectual disability, or insufficient language experience (defined as non-native, limited proficient speakers with less than 1 year of instruction in the testing language). Currently, there are no special accommodations provided for students taking PISA. NAEP's policy endeavors to assess all students selected as part of its sampling process and allows a range of accommodations, as necessary, for students with disabilities (SD) or English language learners (ELL). Accommodations include modifications in presentation format, response formats, test-taking setting, timing, or other aspects, as well as direct and indirect linguistic support. The weighted exclusion rate was 5 percent of students in PISA in 2009. The exclusion rate in the NAEP 8th-grade assessment in 2009 was 4 percent of students."}, {"section_title": "A.5 What Is Measured by the PISA and NAEP Reading Assessments?", "text": "PISA and NAEP measure some similar aspects of reading, but there are differences in how reading is defined in the frameworks, in the types of passages presented to students, and in the types of cognitive skills required of students. To examine the similarities and differences in the content assessed by PISA and NAEP, NCES commissioned a panel of experts to examine and compare the PISA and NAEP reading frameworks, passages, and items. The panel looked at how each assessment defined reading; how the domain was organized in the frameworks; the nature, length, and difficulty of the reading passages; the format of the items used; and the cognitive processes in which students were asked to engage."}, {"section_title": "A.5.1 Definitions of Reading", "text": "There is overlap between the NAEP 2009 and PISA 2009 definitions of reading (see exhibit 1). The definitions for both assessments identify reading as a constructive process that involves interaction between the reader and the text, and both focus on understanding and using written text. There are subtle differences, however. PISA's definition emphasizes the use of reading for personally defined goals and growth and for participation in society, while the NAEP definition reflects the notion that readers draw on the ideas and information they have acquired from text to meet a particular purpose or situational need. The NAEP reading framework calls for the 12th grade assessment to address the preparedness of 12th-graders for postsecondary education and training, although NAEP does not currently report results on a preparedness scale."}, {"section_title": "Exhibit A.1 Definitions of reading Reading Literacy in PISA 2009", "text": "Reading literacy is understanding, using, reflecting on, and engaging with written texts in order to achieve one's goals, to develop one's knowledge and potential, and to participate in society (OECD 2009, p. 23)."}, {"section_title": "Reading in NAEP 2009", "text": "Reading is an active and complex process that involves understanding written text; developing and interpreting meaning; and using meaning as appropriate to type of text, purpose, and situation (National Assessment Governing Board 2008, p. 2). "}, {"section_title": "A.5.2 Framework Organization and Assessment Features", "text": "There are similarities in how PISA and NAEP organize the reading domain, although both PISA and NAEP have unique features and requirements not included in the other assessment. As shown in exhibit 2, the 2009 NAEP reading framework is based on a two-dimensional matrix with text types as one dimension and cognitive processes (\"cognitive targets\") as the other. These can be thought of as \"what students read\" and \"what students do with what they read.\" PISA's framework also organizes the assessment around texts and cognitive processes (\"aspects\"). PISA's text dimension, however, includes four text taxonomies-type, format, medium, and environment. Type is largely analogous to NAEP's \"text type.\" Format distinguishes between \"continuous\" texts-that is, text that is formed by sentences organized into paragraphs-and noncontinuous texts-that is, texts that are composed of less than sentences (e.g., lists, tables, graphs, diagrams, advertisements, schedules, catalogues, indexes and forms) (OECD 2009). Medium and environment are used because the PISA 2009 reading assessment included an assessment of student reading literacy with electronic texts; medium distinguishes between print and electronic text 55 , and environment distinguishes between authored and message-based text. 6 NAEP does not currently include electronic texts in its reading framework or assessment. PISA also uses a unique third dimension, the reading situation, which distinguishes the range of contexts or purpose for which reading takes place. NAEP, on the other hand, has its own unique feature: an assessment of \"meaning vocabulary,\" which refers to students' ability to apply meaning to words vital for comprehending the overall passage. The framework specifies that each NAEP passage will have approximately two items that focus on meaning vocabulary."}, {"section_title": "A.6 Reading Passages", "text": "The reading passages selected for inclusion in PISA and NAEP represent the individual framework and design of each assessment. Both assessments strive to cover a wide range of text types, difficulty, and topics. Both assessments distinguish a range of text types, which are somewhat, but not perfectly, aligned. For example, NAEP's literary category includes fiction, poetry, and literary nonfiction and is more expansive than PISA's corresponding category, narration. Also, there is no clear counterpart to PISA's \"description\" category, which includes documents that typically provide an answer to \"what?\" questions, such as a depiction of a place or a schedule. NAEP texts in the informational category include exposition, argument and persuasion, and procedural texts and documents (e.g., news articles, research reports, historical documents, persuasive essays, and position papers). Across grade levels, NAEP incorporates increasingly complex text structures and features, genre/type of text, and author's craft. Although NAEP includes some noncontinuous material, it is only used as augmentation, embedded in continuous material, at the 8th-grade level, and there are only a few stand-alone examples in the 12thgrade assessment. PISA, on the other hand, makes heavier use of noncontinuous material, including, in particular, texts that fall into PISA's exposition, argumentation, and transaction categories. Even the continuous texts in those categories are often drawn from activities that a 15-year old student might engage in during a daily routine and may come from a wide range of sources that are not strictly academically grounded (e.g., from a mainstream newspaper versus from a student's educational magazine). PISA is designed to cover a wide breadth of what students read and the purpose for reading, which is not always in school but outside of school as well. PISA includes both continuous and noncontinuous text, as well as the range of types described in table 1. The most common text type in PISA is exposition, which includes almost one-third of the passages. The least common text type in PISA is narration, which is represented in about 10 percent of the passages. The framework for NAEP also addresses the different kinds of reading materials students will encounter both in and outside of the classroom and describes NAEP as an \"assessment of varied reading skills.\" The broad categories of literary and informational texts are identified in two ways: first, by the different purposes for which literary and informational texts are read, and, second, by structural differences between literary and informational texts that mark the text and help readers understand what they are reading. Passages in the 8th-grade assessment are evenly distributed between these two main categories. At the 12th-grade level, about one-quarter of the passages are literary and about three-quarters are informational. This is intended to mirror the distribution of the kinds of texts students encounter as they progress through the education system. The PISA reading assessment includes 29 passages, and the NAEP 8th-and 12th-grade assessments include 16 and 17 passages, respectively. In PISA and NAEP, each student receives only a subset of the passages in each assessment. In NAEP, both the 8th-and 12th-grade assessments include some passages that are used at another grade as well-e.g., there is a subset of 8th-grade passages that is also used in the 4th-grade assessment and a subset that is used at 12th grade. Also, NAEP pairs some passages. Students are presented two related passages and are asked inter-textual questions, as well as questions specific to each passage. PISA does not have paired passages in the same sense, although some PISA passages include multiple parts and may even include different text types or formats, and inter-textual questions may be included. In PISA, students respond to 2 to 5 items per passage, whereas in NAEP students respond to between 9 and 11 items per passage or pair of passages. Related to the issue of text types, there is another important but subtle distinction between NAEP and PISA. The NAEP framework explicitly emphasizes the authenticity of text and notes a commitment to selecting high-quality, authentic stimulus materials that students are likely to encounter both in school and out of school. To this end, NAEP sets minimum passage lengths for inclusion and makes very few edits to the original texts. Although PISA is intended to measure authentic tasks, the PISA framework does not explicitly emphasize the use of existing, intact text. It is constrained in some ways by its international nature, as passages must be applicable across a wide range of cultures and languages. Therefore, while passages are selected to represent a range of texts and applicability in real-world settings, more manipulation and editing of passages is used than is in NAEP. Also, the 2009 NAEP reading framework explicitly required that the selection of passages be informed by readability analyses, such as the ones described in the next section; although readability analyses had played a role in passage selection prior to 2009, the 2009 framework explicitly called for their use."}, {"section_title": "Length", "text": "PISA passages are notably shorter than NAEP 8th-and 12th-grade passages, averaging 354 words to NAEP's 924 and 1,174 words per passage or pair of passages, 7 respectively (see table 1). Passages or pairs of passages in NAEP range from 219 words to 1,429 words in the 8th grade and 771 to 1,429 words in the 12th grade, compared with a range of 53 to 758 words in PISA. Thus, even the longest passage in PISA is shorter than the average passage length for each of the two NAEP grades. The NAEP framework specifies passage length by grade level to represent what students encounter in their in-school and out-of-school reading, to ensure usage of strategic reading skills, to ensure that approximately 10 distinct items can be generated from the passage, and to ensure that the structural patterns of the passages are supportive of the range of text types and that the items cover the range of cognitive processes (National Assessment Governing Board 2008, p. 28). PISA does not have a similar requirement for passage length as part of its framework. While PISA's noncontinuous texts tend to be shorter than its continuous texts (see appendix table A), the presence of noncontinuous texts alone does not account for these differences. Rather, it is likely that these differences are driven by the differing framework requirements, constraints, and purposes of the assessments described earlier. "}, {"section_title": "Passage Difficulty", "text": "Several readability measures (i.e., the Dale-Chall Formula, Flesch Reading Ease, Flesch-Kincaid Grade Level, and FORCAST Formula) were used to compare passage difficulty between PISA and NAEP (at the 8th and 12th grades) (see table 2). These analyses excluded passages of less than 250 words (which included 3 poetry passages in NAEP and 8 passages in PISA) because of the lower reliability of applying the formulas to passages of such length. 8 In general, PISA passages were somewhat closer in difficulty to 12th-grade NAEP than to 8th-grade NAEP. The PISA passages tended to cover a broader range of readability or grade levels than did the NAEP passages. Using the formulas best suited to continuous text (Dale-Chall and Flesch-Kincaid)-although applied to both continuous and noncontinuous passages-passages from the NAEP 8th-grade assessment corresponded, on average, to a 7th-grade level according to Dale-Chall and 8th-grade level according to Flesch-Kinkaid, with a range that extended from 5th grade to 12th grade. The average grade level for NAEP 12th-grade passages was the 7th grade according to Dale-Chall and the 9th grade according to Flesch-Kincaid. The range of grade levels extended from 5th grade to 13th grade. The average grade level for PISA passages was the 8th grade for Dale-Chall and the 9th grade for Flesch-Kincaid. The range of grades extended from 3rd grade to 15th grade. Using the formula best suited to noncontinuous texts (FORCAST)-although again applied to both passage types-the NAEP 8th-grade and 12th-grade and PISA passages all averaged at the 10th-grade level. When the passages for each assessment are separated by format (continuous and noncontinuous), other differences emerge (see appendix table B). Within assessments, PISA's continuous passages tend to be more difficult than its noncontinuous passages. On the other hand, NAEP's relatively few strictly noncontinuous passages at the 12th-grade level are more difficult than its continuous texts at that grade level. In fact, NAEP 12th-grade noncontinuous passages are, on average, more difficult than PISA noncontinuous passages, which are more numerous, and the NAEP 12th-grade noncontinuous passages are among the most challenging in either assessment.  The expert panel was asked to determine how well NAEP and PISA passages would fit in the other's framework or, in other words, the likelihood that the passages of one assessment could appear in the other. The expert panel reviewed a sample of about 70 percent of the passages from each assessment selected to represent the full range of the frameworks in terms of text type and length. 10 They found that PISA passages tended to fit better to the NAEP framework than NAEP passages did to the PISA framework, although a substantial number of passages from both assessments were deemed not interchangeable (see table 3). About half of the NAEP 8th-grade and two-thirds of the NAEP 12th-grade passages that were reviewed did not fit within the PISA framework. Just over two-fifths of the PISA passages that were reviewed did not fit within the NAEP framework at either the 8th or 12th grade. The most typical reason for lack of fit of NAEP passages was the prominence of \"author's craft\" in NAEP. (\"Author's craft\" refers to the specific techniques used by an author to relay an intended message.) NAEP texts such as poetry or rhetorical narratives would be difficult to translate into the various languages required by PISA, as would maintaining the tone and quality of a text as the author intended. The experts concluded that some PISA passages would not appear in the NAEP assessment because there was too much disconnected text (or presentation of multiple stimuli not strictly related), the texts were not authentic enough, or the passages were simply too short. "}, {"section_title": "A.6.1 Item Format", "text": "Both NAEP and PISA include multiple-choice items from which students choose one correct answer. All NAEP multiple-choice items include four response options, whereas PISA multiplechoice items include four or five response options (see table 4). In addition to traditional multiplechoice items, PISA also includes what it calls \"complex multiple choice\" items, which require students to answer a series of multiple-choice or true/false questions based on the same information. Both NAEP and PISA also include constructed-response items, for which students must supply the response. NAEP has \"short answer\" and \"extended response\" items. As described in the 2009 NAEP reading framework: Short constructed-response items can be answered by one or two phrases or by one or two sentences; they should take students approximately 2 to 3 minutes to complete. Extended constructed-response items should elicit longer, more elaborated answers of a paragraph or two. They should take students approximately 5 minutes to complete. Scoring rubrics for short and extended constructed-response items will focus on the content included in answers, not on spelling or grammatical considerations. However, students must answer constructed-response questions by using information from the text to receive credit (National Assessment Governing Board 2008, p. 40). PISA classifies its constructed-response items as open constructed response, short constructed response, and closed constructed response. Open constructed-response items may require a description or an explanation to support a response and may be scored for partial credit, though the acceptable length of response is much less than in NAEP. Short constructed-response items typically require students to supply a word or phrase or may require students to provide a specific response from the text. Closed constructed-response items are described as those that \"require the student to generate a response, but that require minimal judgment on the part of a coder\" (OECD 2009, p. 46). "}, {"section_title": "A.6.2 Cognitive Processes", "text": "While the texts used form the content of the assessment, the cognitive processes (\"aspects\" in PISA and \"cognitive targets\" in NAEP) define the skills and abilities that students must draw on in response to the texts. Each item is written to primarily address one process. Although PISA and NAEP have three similarly named and defined cognitive process categories (shown in exhibit 3), there are differences that influence the kinds of items presented to students in each assessment.  "}, {"section_title": "A.6.3 Experts' Comparison of PISA and NAEP Reading Items", "text": "The experts were also asked to review the items associated with the reviewed passages. They assessed the extent to which PISA and NAEP items fit into the other framework's cognitive categories and whether or not the items fit the other framework in terms of the nature and format of the item. Additionally, items were reviewed for factors that contributed to their ease or difficulty. Panelists also looked at the PISA items to consider whether, in terms of level of challenge, they fit more closely to the NAEP 8th-or 12th-grade assessment. "}, {"section_title": "S. Specific Variables", "text": "A-15"}, {"section_title": "A.6.4 Fit to Other Framework: Cognitive Processes", "text": "In general, the NAEP and PISA items reviewed tended to fit within the other framework's cognitive categories-that is, the items required a similar range and type of reading and thinking skills. Only about 4 to 6 percent of items in each assessment (including 8th-and 12th-grade NAEP and PISA) were rated as completely outside the other assessment's framework cognitive categories. However, there were a number of items, especially in PISA, that were considered more borderline for cognitive fit or that were thought to fit within the other assessment's framework, but in a different cognitive category . For example, there were some PISA items classified as \"reflect and evaluate\" that the panel thought would be considered \"integrate and interpret\" in the NAEP framework, as well as some PISA \"integrate and interpret\" items that might be classified as \"locate and recall\" in NAEP. Adjusting for these borderline items, the experts thought that, overall, about 90 percent of both NAEP 8th-and 12th-grade items fit PISA's cognitive categories tightly and well and that about 80 percent of PISA items fit the NAEP cognitive categories tightly and well. The expert panel remarked that it is more difficult for a student to read and answer questions from a passage that is significantly longer in length than it is from a shorter passage. The panel considered this difference when deciding the \"fit\" of PISA passages into the NAEP reading assessment cognitive categories. PISA items that were different from NAEP items cognitively included those that asked students to provide a personal stance or required a written response that was not dependent on text-based evidence and those that drew on multiple cognitive skills-scenarios that would not occur in NAEP. NAEP items that seemed different, or somewhat different, from PISA items cognitively were mainly NAEP vocabulary items, which required students to identify word meaning within the passage context and that have no corollaries in PISA. Within individual cognitive categories, the most challenging items to fit to the other framework's cognitive categories appeared to be the PISA \"reflect and evaluate\" items. About 14 percent of these items did not match the cognitive categories of the NAEP framework at all, and about 20 percent matched a different cognitive category in NAEP. PISA's emphasis on inclusion of the student's own experiences in \"reflect and evaluate\" items sometimes fit better NAEP's \"integrate and interpret\" category than its \"critique and evaluate\" category."}, {"section_title": "Fit to Framework: Item Format", "text": "The experts found more differences between the assessments in item format than in cognitive skills measured. Although PISA and NAEP items tended to measure similar cognitive skills, they often were presented or formatted in ways that were dissimilar between the assessments. Over one-third of NAEP 8th-grade items and nearly two-fifths of NAEP 12th-grade items were judged incompatible with the PISA framework in terms of their nature or format; over half of the PISA items were judged incompatible with the NAEP framework. In general, the NAEP items that did not fit PISA were either vocabulary items or items that required a response that used information from the text to support it, for which PISA does not have corollary formats. PISA items did not fit NAEP for more varied reasons. Some PISA items did not fit NAEP because of the relatively short length of response acceptable for a correct answer, others because of the use of scaffolding or introduction to test items. Many items did not fit because of format differences. PISA used the closed constructedresponse formats for low-level cognitive items in cases in which NAEP would only use multiple choice, and PISA used formats such as \"complex multiple-choice\" or included visuals in the item or responses, which also would not occur in the NAEP reading assessment. "}, {"section_title": "A.6.5 Factors Contributing to Cognitive Challenge", "text": "The experts reviewed the items to identify the factors that contributed to their cognitive challenge, or what drives item difficulty. Each item could be assigned multiple factors from a list of nine , the first 8 of which are based on factors described in the PISA framework (OECD 2009) and the 9th which was added by the experts: 1. number of pieces of information needed to locate/consider; 2. amount of inference required; 3. amount and prominence of competing information; 4. length and complexity of text; 5. type of interpretation required; 6. familiarity with structure and genre; 7. nature of knowledge needed to bring to item (narrow v. broad); 8. depth of understanding required; and 9. type of information. For both 8th-and 12th-grade NAEP, the most prevalent factors contributing to item challenge were the type of interpretation required by the test-taker, the number of pieces of information to be located or considered, and the depth of understanding required to answer the item correctly. Each of these factors was present in at least 35 percent of the items. In most cases, these were viewed as factors that contributed to increased item challenge-that is, the interpretations or understanding required were relatively complex or deep and the amount of information to be sorted through was relatively great. For PISA, the factors contributing to item challenge were typically the type of information that the student is required to handle, the number of pieces of information to be located or considered, and the amount of inference required. As in NAEP, these factors were present in at least 35 percent of items. Identifying the type of information that the student is required to handle was most often viewed as the factor that would increase the challenge level, because of PISA's use of visual or graph-based information, which is not routinely found in NAEP and is considered fairly challenging. The other two factors were most often viewed as contributing to a relatively lower challenge level, as the amount of text to be sorted through was not as great as in NAEP and the amount of inference required was not as great as it might have been in the context of longer passages."}, {"section_title": "Appropriate Level of Challenge for NAEP (PISA Items)", "text": "In a final analysis, the experts examined whether or not the PISA items would be appropriate in terms of level of challenge for the NAEP 8th-or 12th-grade assessment. The experts considered what the PISA items required of students and how well that aligned with the items in the NAEP 8th-and 12th-grade assessments. The experts found that about 55 percent of PISA items would be suitable for the NAEP 8th-grade assessment and about 15 percent would be suitable for the NAEP 12th-grade assessment. However, about 30 percent of the items were thought to be inappropriate for NAEP in terms of level of challenge, and some of the items deemed suitable for the 8th grade were considered more borderline, or on the lower end of what would be acceptable, for this grade. "}, {"section_title": "A.7 Summary", "text": "NAEP measures in detail the reading knowledge of U.S. students as a whole, but can also provide trend information for individual states and some districts, different geographic regions, and demographic population groups. PISA provides a method for comparing the performance of U.S. students in reading with that of students in other nations. The two assessments differ in some key design elements. Differences include the following: \u2022 The content assessed by PISA and NAEP differ in subtle, but important ways. NAEP is tailored specifically to practices and standards used in the United States; in PISA, the content is determined internationally, in collaboration with other countries and reflecting consensus views of key content. Also, PISA's specific focus on the \"yield\" of education systems and the application of competencies in real-world contexts distinguishes it from NAEP, which focuses more closely on measuring school-based performance. \u2022 Different target populations of students are assessed. Main NAEP uses grade-based samples targeting 4th-, 8th-, and 12th-grade students. PISA uses an age-based sample, which targets 15-year-olds, who are most likely between the ages of the NAEP target populations of 8th-and 12th-graders. \u2022 Measurement precision is greater in NAEP than in PISA. NAEP and PISA are both designed to provide valid and reliable measures of U.S. students' performance in the aggregate as well as for major subpopulations, and each study draws a sample sufficient for this purpose. NAEP, however, is also designed to provide estimates for individual states, which requires an increased sample size, and thus measures performance at a higher level of precision than PISA. This difference may have an impact on the assessments' sensitivities in detecting changes in student performance. \u2022 There is some overlap in how reading is defined in the two assessment programs and some similarities in how the frameworks are organized, with both NAEP and PISA specifying a cognitive dimension and a range of text types. However, there are subtle differences in how the cognitive categories are defined and more notable differences in the text types targeted for inclusion, as well as features (e.g., an assessment of vocabulary embedded within NAEP) that are unique to each assessment. \u2022 The passages selected for NAEP and PISA would likely fit in each other's frameworks to only a limited degree. For example, NAEP passages, on average, are longer than PISA passages. Another, related difference is PISA's more frequent use of graphic and other visual displays of text rather than continuous text passages. In terms of readability and grade level, PISA passages were generally more comparable to 12th-grade NAEP than to 8thgrade NAEP. \u2022 NAEP and PISA items generally tend to measure similar cognitive skills; however, they often are presented or formatted in ways that would not be interchangeable between the assessments. Key differences include PISA's less extensive use of multiple-choice and more extensive use of short-constructed response formats than NAEP, while NAEP requires much longer, text-based responses for its extended constructed response formats. \u2022 Finally, there are differences in the source of challenge for NAEP and PISA items; these differences appear to be driven by the inclusion of longer passages in NAEP and the inclusion of more visual and other noncontinuous text formats in PISA. PISA items were found more frequently to be appropriate for the NAEP 8th-grade assessment than the 12thgrade assessment. A-20  I am writing to inform you about the upcoming Program for International Student Assessment (PISA) 2009, in which the United States will participate along with more than 60 other countries. PISA provides international comparisons of student performance in reading, mathematics, and science literacy among 15-year-old students throughout the world. We are notifying you now because one or more schools in your state have been selected to take part in PISA 2009 in the fall of 2009. We ask your agency to support the participation of schools in your state in this study. Within the next few days, a representative of Westat will contact sampled school districts and schools to discuss conducting data collection. In the meantime, if you have questions about the study, please feel free to call Dr. Paul Hopstock of Windwalker Corporation at (703) 970-3522 or send an email to PISA2009@westat.com. Also, more information about PISA is available at the NCES website at: http://nces.ed.gov/surveys/pisa/. I am writing to inform you about the upcoming Program for International Student Assessment (PISA) 2009, in which the United States will participate along with more than 60 other countries. PISA provides international comparisons of student performance in reading, mathematics, and science literacy among 15-year-old students throughout the world. We are notifying you now because one or more schools in your district have been selected to take part in PISA 2009 in the fall of 2009. We ask your agency to support the participation of schools in your district in PISA 2009. Since PISA is designed to test a representative sample of U.S. students, the accuracy of PISA depends on the full participation of the sampled schools and students. In appreciation for their time and efforts, the schools that participate will each receive an honorarium. In addition, school-level coordinators and participating students will also receive honoraria. We ask you to permit the participation of schools in your district in PISA 2009. Since PISA is designed to test a representative sample of U.S. students, the accuracy of PISA depends on the full participation of the sampled schools and students. In appreciation for their time and efforts, the schools that participate will each receive an honorarium. In addition, school-level coordinators and participating students will also receive honoraria. I am writing to inform you about the upcoming Program for International Student Assessment (PISA) 2009, in which the United States will participate along with more than 60 other countries. PISA provides international comparisons of student performance in reading, mathematics, and science literacy among 15-year-old students throughout the world. We are notifying you now because your school has been selected to take part in PISA 2009 in the fall of 2009. I encourage your school's participation in PISA 2009. In light of concerns about the nation's international economic competitiveness and the skills of our workers, knowing how U.S. students compare with peers around the world is very important. This is why leading national education organizations, such as the National Association of Secondary School Principals, have endorsed the study. Your school can make a valuable contribution to learning more about where our education system has been successful and where we face challenges in educating our youth. Since PISA is designed to test a representative sample of U.S. students, the accuracy of PISA depends on the full participation of you and your students. In appreciation for your time and efforts, if your school participates, the school will receive a $200 honorarium, the school-level coordinator will receive $100, and participating students will receive $20 each."}, {"section_title": "C.2 Explicit Parent Materials", "text": ""}, {"section_title": "Explicit Consent Letter", "text": "Dear Parent or Guardian, This letter is to ask you to allow your child to take part in an important international study of student learning. The study is known as PISA (the full name is Program for International Student Assessment). PISA looks at student learning in reading, mathematics, and science around the world. It has been conducted every three years since 2000, and documents world-wide trends in the knowledge and skills of 15-year olds in these areas. Along with 66 other nations, the United States will take part in PISA in 2009, just as we have in previous years. Your child's school has accepted an invitation to take part in PISA 2009, and your child is one of approximately 40 students who have been invited to take part. It is important that each student selected take part in the assessment. Each time a selected student does not participate, the accuracy of the U.S. information suffers. I urge you to support this effort by allowing and encouraging your child to take part. The enclosed summary sheet provides some background information on PISA, explains what is involved for each student, and gives a contact phone number and e-mail address where you can get answers to any questions you might have. For participating in PISA, the school is receiving a $200 honorarium. Each of the participating students also will receive $20 as a token of appreciation. It goes without saying that all of the information collected is completely confidential. In fact, the study is required to do so by law. So, students and schools are never identified in any reports."}, {"section_title": "Before your child can take part in the PISA assessment, the study must have your written consent. You can let us know by completing the attached form and returning it to the school. The form should be returned to [School Coordinator] by [Date].", "text": "Thank you for taking the time to think about this study. Your child has been asked to take part in an international study of student learning called the Program for International Student Assessment (PISA). This assessment will be conducted by a team of researchers from Westat Corporation, who are operating under contract with Windwalker Corporation on behalf of the U. S. Department of Education. In the Fall of 2009, the assessment will be administered in approximately 160 schools and with approximately 5,000 students in the United States. The study needs your permission for your child to take part."}, {"section_title": "Confidentiality", "text": "Every precaution will be taken to protect your child's privacy. Your child's name will never be associated with any results that are reported from the assessment. Only group-level findings will be presented in any reports or other documents that are created as part of the assessment. No individually identifiable results will be presented."}, {"section_title": "Your Child's Rights 1.", "text": "Your child's participation in the assessment is completely voluntary. However, we encourage each student selected take part. Each time a selected student does not participate, the accuracy of the U.S. information suffers. Refusal to participate will not affect your child's opportunities in school. You or your child may decide not to participate."}, {"section_title": "2.", "text": "The assessment will last for about 3 hours."}, {"section_title": "3.", "text": "Your child will be given a $20 check at the completion of the assessment in appreciation for his or her contribution."}, {"section_title": "4.", "text": "If you have any questions about PISA or your child's rights as a study participant, you can contact PISA at 1-888-270-6227 or PISA2009@westat.com."}, {"section_title": "5.", "text": "You will be given a copy of this form to keep. "}, {"section_title": "C.3 Implicit Parent Materials", "text": ""}, {"section_title": "Implicit Consent Letter", "text": "Dear Parent or Guardian, This letter is to inform you about an important international study of student learning being conducted in your child's school. The study is known as PISA (the full name is Program for International Student Assessment). PISA looks at student learning in reading, mathematics, and science around the world. It has been conducted every three years since 2000, and documents world-wide trends in the knowledge and skills of 15-year olds in these areas. Along with 66 other nations, the United States will take part in PISA in 2009, just as we have in previous years. Your child's school has accepted an invitation to take part in PISA 2009, and your child is one of approximately 40 students who have been invited to take part. It is important that each student selected take part in the assessment. Each time a selected student does not participate, the accuracy of the U.S. information suffers. I urge you to support this effort by allowing and encouraging your child to take part. The enclosed summary sheet provides some background information on PISA, explains what is involved for each student, and gives a contact phone number and e-mail address where you can get answers to any questions you might have. For participating in PISA, the school is receiving a $200 honorarium. Each of the participating students also will receive $20 as a token of appreciation. It goes without saying that all of the information collected is completely confidential. In fact, the study is required to do so by law. So, students and schools are never identified in any reports."}, {"section_title": "If you are willing to allow your teenager to participate, you do not need to return the attached form. If for any reason you object to your teenager's participation, please fill out the enclosed form and return it to [School Coordinator] at the school by [Date].", "text": "Thank you for taking the time to think about this study. "}, {"section_title": "C.4 Parent Notification", "text": ""}, {"section_title": "Notification Letter", "text": "Dear Parent or Guardian, This letter is to inform you about an important international study of student learning being conducted in your child's school. The study is known as PISA (the full name is Program for International Student Assessment). PISA looks at student learning in reading, mathematics, and science around the world. It has been conducted every three years since 2000, and documents world-wide trends in the knowledge and skills of 15-year-olds in these areas. Along with 66 other nations, the United States will take part in PISA in 2009, just as we have in previous years. Your child's school has accepted an invitation to take part in PISA 2009, and your child is one of approximately 40 students who have been invited to take part. It is important that each student selected take part in the assessment. Each time a selected student does not participate, the accuracy of the U.S. information suffers. I urge you to support this effort by encouraging your child to take part. The enclosed summary sheet provides some background information on PISA, explains what is involved for each student, and gives a contact phone number and e-mail address where you can get answers to any questions you might have. For participating in PISA, the school is receiving a $200 honorarium. Each of the participating students also will receive $20 as a token of appreciation. It goes without saying that all of the information collected is completely confidential. In fact, the study is required to do so by law. So, students and schools are never identified in any reports. Thank you for taking the time to think about this study. "}, {"section_title": "C.5 Fact Sheet for Parents", "text": ""}, {"section_title": "Facts for Parents About PISA 2009", "text": "In the fall of this year, your child's school will be one of about 160 nationwide taking part in PISA 2009. The schools were selected randomly to represent the nation's schools, and within each school, about 40 students were selected randomly to take part. Your child was among the students selected to take part in the study."}, {"section_title": "What is PISA?", "text": "PISA (the Program for International Student Assessment) is an international assessment that measures student learning in reading, mathematics, and science. The assessment occurs every three years (2000, 2003, 2006 and 2009), and provides information about how students in the U.S. compare in achievement with students in other countries. Sixty-seven (67) countries will be participating in PISA 2009. The National Center for Education Statistics within the U.S. Department of Education sponsors U.S. participation in PISA."}, {"section_title": "What is involved?", "text": "PISA staff will visit the school and administer a two-hour assessment to the selected students. There is one break during the assessment. Students will also be asked to complete a background questionnaire that takes about 30 minutes to complete."}, {"section_title": "What are the benefits?", "text": "The nation as a whole benefits from PISA by having a greater understanding of how the knowledge and skills of U.S. students compare with those of students from other countries. Schools that participate in PISA will receive $200, and each student who participates will receive a $20 check. Where can I find out more about PISA? More information about PISA is available at the PISA website at http://www.nces.ed.gov/surveys/pisa. If you have specific questions you can call PISA staff at 1-888-270-6227 or e-mail us at PISA2009@westat.com."}, {"section_title": "D-3", "text": "In this booklet you will find questions about: In some of the questions you will be asked about reading. What we specifically mean by reading is the skill to understand, use and think about written texts. This skill is needed to reach one's goals, to develop one's knowledge and potential, and to take part in society. Please read each question carefully and answer as accurately as you can. In the test you usually circled your answers. For this questionnaire, you will normally answer by darkening a circle. For a few questions you will need to write a short answer. If you make a mistake when darkening a circle, erase your mistake and darken the correct circle. If you make a mistake when writing an answer, simply cross it out and write the correct answer next to it. In this questionnaire, there are no right or wrong answers. Your answers should be the ones that are right for you. You may ask for help if you do not understand something or are not sure how to answer a question. Your answers will be combined with others to make totals and averages in which no individual can be identified. All your answers will be kept confidential.    c) Fiction (e.g., novels, narratives, stories)  Using an online dictionary or encyclopedia (e.g., Wikipedia\u00ae) e) Searching online information to learn about a particular topic Taking part in online group discussions or forums ) Searching for practical information on line (e.g., schedules, events, tips, recipes)  ) When I study, I read the text over and over again. h) When I study, I figure out how the information might be useful outside school. i) When I study, I try to figure out which concepts I still haven't really understood. a) School has done little to prepare me for adult life when I leave school. b) School has been a waste of time. c) School has helped give me confidence to make decisions. d) School has taught me things which could be useful in a job. Q38 How much do you disagree or agree with each of the following statements about teachers at your school? (Please darken only one circle in each row.)  e) Most of my teachers treat me fairly.  b) There is noise and disorder. c) The teacher has to wait a long time for the students to quiet down. e) Students don't start working for a long time after the class begins."}, {"section_title": "Q41", "text": "In your English classes, how often does the following occur? (Please darken only one circle in each row) a) The teacher asks students to explain the meaning of a text. b) The teacher asks questions that challenge students to get a better understanding of a text. c) The teacher gives students enough time to think about their answers. d) The teacher recommends a book or author to read. e) The teacher encourages students to express their opinions about a text. f) The teacher helps students relate the stories they read to their lives. g) The teacher shows students how the information in texts builds on what they already know."}, {"section_title": "Q42", "text": "In your English classes, how often does the following occur? (Please darken only one circle in each row) a) The teacher explains beforehand what is expected of the students. b) The teacher checks that students are concentrating while working on the reading assignment. c) The teacher discusses students' work, after they have finished the reading assignment. The teacher tells students in advance how their work is going to be judged. e) The teacher asks whether every student has understood how to complete the reading assignment. f) The teacher grades students' work. g) The teacher gives students the chance to ask questions about the reading assignment. h) The teacher poses questions that motivate students to participate actively. i) The teacher tells students how well they did on the reading assignment immediately after. "}, {"section_title": "SECTION 8: YOUR STRATEGIES IN READING AND UNDERSTANDING TEXTS", "text": "There are several approaches to studying and understanding texts. Some of them are more useful than others, depending on the kind of reading task. The next two questions present two reading tasks, followed by a list of these approaches or \"strategies.\" We want to know your opinion about the usefulness of these strategies for the different reading tasks. Both questions begin with a short description of a particular reading task. Then several possible reading strategies are listed. Think about the usefulness of each of the strategies in relation to the given reading task only. Some strategies may be useful for one reading task but not for another. "}, {"section_title": "Q45", "text": "Reading task: You have to understand and remember the information in a text. How do you rate the usefulness of the following strategies for understanding and memorizing the text?"}, {"section_title": "Possible strategy Score", "text": "Not useful at Very useful all (1) ( a) I concentrate on the parts of the text that are easy to understand. b) I quickly read through the text twice. c) After reading the text, I discuss its content with other people. d) I underline important parts of the text. e) I summarize the text in my own words. f) I read the text aloud to another person. 2009 Data Files and Database with U.S. Specific Variables D-33\nNot useful at Very useful all (1) ( a) I write a summary. Then I check that each paragraph is covered in the summary, because the content of each paragraph should be included. b) I try to copy out accurately as many sentences as possible. c) Before writing the summary, I read the text as many times as possible. d) I carefully check whether the most important facts in the text are represented in the summary. e) I read through the text, underlining the most important sentences. Then I write them in my own words as a summary."}, {"section_title": "Q46", "text": "Reading task: You have just read a long and rather difficult two-page text about fluctuations in the water level of a lake in Africa. You have to write a summary. How do you rate the usefulness of the following strategies for writing a summary of this two-page text?"}, {"section_title": "Thank you very much for your cooperation in completing this questionnaire! D-35", "text": "This questionnaire asks for information including: The structure and organization of the school; The student body and teachers; The school's resources; The school's instruction, curriculum and assessment; The school climate; The school policies and practices; The characteristics of the principal or designee. This information helps illustrate the similarities and differences between groups of schools in order to better establish the context for students' test results. The questionnaire should be completed by the principal or designee. It should take about 30 minutes to complete. For some questions, specific expertise may be needed. You may consult experts to help you answer these questions. If you do not know an answer precisely, your best estimate will be adequate for the purposes of the study. Some questions ask about 10th grade or 10th graders. If you do not have a 10th grade in your school, then answer these questions for the grade in your school that contains the most 15-year-olds. Your answers will be kept confidential. They will be combined with answers from other principals to calculate totals and averages in which no one school can be identified.  "}, {"section_title": "SECTION A: THE STRUCTURE AND ORGANIZATION OF THE SCHOOL", "text": ""}, {"section_title": "Q4", "text": "Which of the following definitions best describes the community in which your school is located? (Please check only one box) A village, hamlet or rural area (fewer than 3,000 people) A small town (3,000 to about 15,000 people) A town (15,000 to about 100,000 people) A city (100,000 to about 1,000,000 people) A large city (with over 1,000,000 people) \uf06e 5"}, {"section_title": "Q5a", "text": "We are interested in the options parents have when choosing a school for their children."}, {"section_title": "Which of the following statements best describes the schooling available to students in your location?", "text": "(Please check only one box) There are two or more other schools in this area that compete for our students."}, {"section_title": "\uf06e 1", "text": "There is one other school in this area that competes for our students.\nPressure on the school to achieve higher academic standards among students comes from a minority of parents. Pressure from parents on the school to achieve higher academic standards among students is largely absent.  e) Formulating the school budget f) Deciding on budget allocations within the Approving students for admission to the school Deciding which courses are offered"}, {"section_title": "\uf06e 2", "text": "There are no other schools in this area that compete for our students. a) The approximate percentage of students repeating a grade at the middle/junior high school level (grades 7-9) in this school last year was: b) The approximate percentage of students repeating a grade at the high school level (grades 10-12) in this school last year was: More than 0%, but less than 10% "}, {"section_title": "SECTION C: THE SCHOOL'S RESOURCES", "text": "The goal of the following set of four questions is to gather information about the student-computer ratio for students in the 10th grade at your school.  "}, {"section_title": "Q18", "text": "Which statement below best characterizes parental expectations towards your school? (Please check only one box) There is constant pressure from many parents, who expect our school to set very high academic standards and to have our students achieve them."}, {"section_title": "Q25", "text": "Regarding your school, which of the following bodies exert a direct influence on decision making about staffing, budgeting, instructional content and assessment practices? (Please check as many boxes as apply)  e) Teacher groups (e.g., staff association, curriculum committees, trade f) Student groups (e.g., student association,  n) I take over lessons from teachers who are unexpectedly absent."}, {"section_title": "Q27", "text": "Approximately what percentage of students at this school last year were eligible for free-or reduced-price lunches through the National School Lunch Program? (Please write a number on the line. Write 0 (zero) if there are none.) This section explains the indices derived from the student, school and parent context questionnaires used in PISA 2009. Parent questionnaire indices are only available for the 14 countries that chose to administer the optional parent questionnaire. Several PISA measures reflect indices that summarize responses from students, their parents or school representatives (typically principals) to a series of related questions. The questions were selected from a larger pool of questions on the basis of theoretical considerations and previous research. Structural equation modeling was used to confirm the theoretically expected behavior of the indices and to validate their comparability across countries. For this purpose, a model was estimated separately for each country and collectively for all OECD countries. For a detailed description of other PISA indices and details on the methods, see PISA 2009 Technical Report (OECD, forthcoming). There are two types of indices: simple indices and scale indices. Simple indices are the variables that are constructed through the arithmetic transformation or recoding of one or more items, in exactly the same way across assessments. Here, item responses are used to calculate meaningful variables, such as the recoding of the four-digit ISCO-88 codes into \"Highest parents' socio-economic index (HISEI)\" or, teacher-student ratio based on information from the school questionnaire. Scale indices are the variables constructed through the scaling of multiple items. Unless otherwise indicated, the index was scaled using a weighted maximum likelihood estimate (WLE) (Warm, 1985), using a one-parameter item response model (a partial credit model was used in the case of items with more than two categories). The scaling was done in three stages: \u2022 The item parameters were estimated from equal-sized subsamples of students from each OECD country. \u2022 The estimates were computed for all students and all schools by anchoring the item parameters obtained in the preceding step. \u2022 The indices were then standardized so that the mean of the index value for the OECD student population was zero and the standard deviation was one (countries being given equal weight in the standardization process). Sequential codes were assigned to the different response categories of the questions in the sequence in which the latter appeared in the student, school or parent questionnaires. Where indicated in this section, these codes were inverted for the purpose of constructing indices or scales. It is important to note that negative values for an index do not necessarily imply that students responded negatively to the underlying questions. A negative value merely indicates that the respondents answered less positively than all respondents did on average across OECD countries. Likewise, a positive value on an index indicates that the respondents answered more favorably, or more positively, than respondents did, on average, in OECD countries. Terms enclosed in brackets < > in the following descriptions were replaced in the national versions of the student, school and parent questionnaires by the appropriate national equivalent. For example, the term <qualification at ISCED level 5A> was translated in the United States into \"Bachelor's degree, post-graduate certificate program, Master's degree program or first professional degree program\". Similarly the term <classes in the language of assessment> in Luxembourg was translated into \"German classes\" or \"French classes\" depending on whether students received the German or French version of the assessment instruments. In addition to simple and scaled indices described in this annex, there are a number of variables from the questionnaires that correspond to single items not used to construct indices. These non-recoded variables have prefix of \"ST\" for the questionnaire items in the student questionnaire, \"SC\" for the items in the school questionnaire, and \"PA\" for the items in the parent questionnaire. All the context questionnaires as well as the PISA international database, including all variables, are available through www.pisa.oecd.org."}, {"section_title": "Student-level simple indices", "text": ""}, {"section_title": "Age", "text": "The variable AGE is calculated as the difference between the middle month and the year in which students were assessed and their month and year of birth, expressed in years and months."}, {"section_title": "Study program", "text": "In PISA 2009, study programs available to 15-year-old students in each country were collected both through the student tracking form and the student questionnaire (ST02). All study programs were classified using ISCED (OECD, 1999). In the PISA international database, all national programs are indicated in a variable (PROGN) where the first three digits are the ISO code for a country, the fourth digit the sub-national category and the last two digits the nationally specific program code. The following internationally comparable indices were derived from the data on study programs: \u2022 Program level (ISCEDL) indicates whether students are (1) primary education level (ISCED 1); (2) lower secondary education level; or (3) upper secondary education level. \u2022 Program designation (ISCEDD) indicates the designation of the study program: (1) = \"A\" (general programs designed to give access to the next program level); (2) = \"B\" (programs designed to give access to vocational studies at the next program level); (3) = \"C\" (programs designed to give direct access to the labor market); or (4) = \"M\" (modular programs that combine any or all of these characteristics). Occupational data for both a student's father and a student's mother were obtained by asking constructed-response questions in the student questionnaire (ST9a, ST9b, ST12, ST13a, ST13b and ST16). The responses were coded to four-digit ISCO codes (ILO, 1990) and then mapped to Ganzeboom, et al.'s (1992) SEI index. Higher scores of SEI indicate higher levels of occupational status. The following three indices are obtained: \u2022 Mother's occupational status (BMMJ). \u2022 Father's occupational status (BFMJ). \u2022 The highest occupational level of parents (HISEI) corresponds to the higher SEI score of either parent or to the only available parent's SEI score."}, {"section_title": "Educational level of parents", "text": "The educational level of parents is classified using ISCED (OECD, 1999) based on students' responses in the student questionnaire (ST10, ST11, ST14 and ST15). Please note that the question format for school education in PISA 2009 differs from the one used in PISA 2000, 2003 and 2006 but the method used to compute parental education is the same. \u2022 Highest educational level of parents (HISCED) corresponds to the higher ISCED level of either parent. Highest educational level of parents was also converted into the number of years of schooling (PARED). For the conversion of level of education into years of schooling, see Table A1.1.\nThe educational level of parents is classified using ISCED (OECD, 1999) based on parents' responses (PA09 and PA10). Three indices were constructed: educational level for mother (PQMISCED); educational level for father (PQFISCED); and the highest educational level of parents (PQHISCED), which corresponds to the higher ISCED level of either parent. These indices have the following categories: (0) None, (1) ISCED 3A (upper secondary), (2) ISCED 4 (non-tertiary post-secondary), (3) ISCED 5B (vocational tertiary), and (4) ISCED 5A, 6 (theoretically oriented tertiary and post-graduate). Appendix G. Explanation of Indices (abstracted from OECD 2010c and OECD 2010d) 2009 Data Files and Database with U.S. Specific Variables G-11"}, {"section_title": "Immigration and language background", "text": "Information on the country of birth of students and their parents (ST17) is collected in a similar manner as in PISA 2000, PISA 2003 and PISA 2006 by using nationally specific ISO coded variables. The ISO codes of the country of birth for students and their parents are available in the PISA international database (COBN_S, COBN_M, and COBN_F). The index on immigrant background (IMMIG) has the following categories: (1) native students (those students born in the country of assessment, or those with at least one parent born in that country; students who were born abroad with at least one parent born in the country of assessment are also classified as 'native' students), (2) second-generation students (those born in the country of assessment but whose parents were born in another country) and (3) first-generation students (those born outside the country of assessment and whose parents were also born in another country). Students with missing responses for either the student or for both parents, or for all three questions have been given missing values for this variable. Students indicate the language they usually speak at home. The data are captured in nationally-specific language codes, which were recorded into variable ST19Q01 with the following two values: (1) language at home is the same as the language of assessment and (2) language at home is a different language than the language of assessment.  "}, {"section_title": "Family structure", "text": "The index of family structure (FAMSTRUC) is based on students' responses regarding people living at home with them (ST08). This index has the following three values: (1) single-parent family (students living with only one of the following: mother, father, male guardian, female guardian), (2) two-parent family (students living with a father or step/foster father and a mother or step/foster mother) and (3) other (except the non-responses, which are coded as missing or not applicable)."}, {"section_title": "Relative grade", "text": "Data on the student's grade are obtained both from the student questionnaire (ST01) and from the student tracking form. As with all variables that are on both the tracking form and the questionnaire, inconsistencies between the two sources are reviewed and resolved during data-cleaning. In order to capture betweencountry variation, the relative grade index (GRADE) indicates whether students are at the modal grade in a country (value of 0), or whether they are below or above the modal grade level (+ x grades, -x grades). The relationship between the grade and student performance was estimated through a multilevel model accounting for the following background variables: i) the PISA index of economic, social and cultural status; ii) the PISA index of economic, social and cultural status squared; iii) the school mean of the PISA index of economic, social and cultural status; iv) an indicator as to whether students were foreign born first-generation students; v) the percentage of firstgeneration students in the school; and vi) students' gender. Table A1.2 presents the results of the multilevel model. Column 1 in Table A1.2 estimates the score point difference that is associated with one grade level (or school year). This difference can be estimated for the 32 OECD countries in which a sizeable number of 15-year-olds in the PISA samples were enrolled in at least two different grades. Since 15-year-olds cannot be assumed to be distributed at random across the grade levels, adjustments had to be made for the above-mentioned contextual factors that may relate to the assignment of students to the different grade levels. These adjustments are documented in columns 2 to 7 of the table. While it is possible to estimate the typical performance difference among students in two adjacent grades net of the effects of selection and contextual factors, this difference cannot automatically be equated with the progress that students have made over the last school year but should be interpreted as a lower boundary of the progress achieved. This is not only because different students were assessed but also because the content of the PISA assessment was not expressly designed to match what students had learned in the preceding school year but more broadly to assess the cumulative outcome of learning in school up to age 15. For example, if the curriculum of the grades in which 15-year-olds are enrolled mainly includes material other than that assessed by PISA (which, in turn, may have been included in earlier school years) then the observed performance difference will underestimate student progress."}, {"section_title": "Learning time", "text": "Learning time in test language (LMINS) was computed by multiplying students' responses on the number of minutes on average in the test language class by number of test language class periods per week (ST28 and ST29). Comparable indices are computed for mathematics (MMINS) and science (SMINS)."}, {"section_title": "Student-level scale indices", "text": ""}, {"section_title": "Family wealth", "text": "The index of family wealth (WEALTH ) is based on the students' responses on whether they had the following at home: a room of their own, a link to the Internet, a dishwasher (treated as a country-specific item), a DVD player, and three other country-specific items (some items in ST20); and their responses on the number of cellular phones, televisions, computers, cars and the rooms with a bath or shower (ST21)."}, {"section_title": "Home educational resources", "text": "The index of home educational resources (HEDRES) is based on the items measuring the existence of educational resources at home including a desk and a quiet place to study, a computer that students can use for schoolwork, educational software, books to help with students' school work, technical reference books and a dictionary (some items in ST20)."}, {"section_title": "Cultural possessions", "text": "The index of cultural possessions (CULT POSS) is based on the students' responses to whether they had the following at home: classic literature, books of poetry and works of art (some items in ST20). Appendix G. Explanation of Indices (abstracted from OECD 2010c and OECD 2010d) 2009 Data Files and Database with U.S. Specific Variables G-6 Econom ic, socia l a nd cultur a l sta tus The PISA index of economic, social and cultural status (ESCS) was derived from the following three indices: highest occupational status of parents (HISEI), highest educational level of parents in years of education according to ISCED (PARED), and home possessions (HOMEPOS). The index of home possessions (HOMEPOS) comprises all items on the indices of WEALTH , CULT POSS and HEDRES, as well as books in the home recoded into a four-level categorical variable (0-10 books, 11-25 or 26-100 books, 101-200 or 201-500 books, more than 500 books). The PISA index of economic, social and cultural status (ESCS) was derived from a principal component analysis of standardized variables (each variable has an OECD mean of zero and a standard deviation of one), taking the factor scores for the first principal component as measures of the index of economic, social and cultural status. Principal component analysis was also performed for each participating country to determine to what extent the components of the index operate in similar ways across countries. The analysis revealed that patterns of factor loading were very similar across countries, with all three components contributing to a similar extent to the index. For the occupational component, the average factor loading was 0.80, ranging from 0.66 to 0.87 across countries. For the educational component, the average factor loading was 0.79, ranging from 0.69 to 0.87 across countries. For the home possession component, the average factor loading was 0.73, ranging from 0.60 to 0.84 across countries. The reliability of the index ranged from 0.41 to 0.81. These results support the crossnational validity of the PISA index of economic, social and cultural status. The imputation of components for students missing data on one component was done on the basis of a regression on the other two variables, with an additional random error component. The final values on the PISA index of economic, social and cultural status (ESCS) have an OECD mean of 0 and a standard deviation of 1."}, {"section_title": "Enjoyment of reading activities", "text": "The index of enjoyment of reading (JOYREAD) activities was derived from students' level of agreement with the following statements (ST24): i) I read only if I have to; ii) reading is one of my favorite hobbies; iii) I like talking about books with other people; iv) I find it hard to finish books; v) I feel happy if I receive a book as a present; vi) for me, reading is a waste of time; vii) I enjoy going to a bookstore or a library; viii) I read only to get information that I need; ix) I cannot sit still and read for more than a few minutes; x) I like to express my opinions about books I have read; and xi) I like to exchange books with my friends. As all items that are negatively phrased (items i, iv, vi, viii and ix) are inverted for scaling, the higher values on this index indicate higher levels of enjoyment of reading."}, {"section_title": "Diversity of reading materials", "text": "The index of diversity of reading materials (DIVREAD) was derived from the frequency with which students read the following materials because they want to (ST25): magazines, comic books, fiction, non-fiction books and newspapers. The higher values on this index indicate higher diversity in reading."}, {"section_title": "Online reading activities", "text": "The index of online reading activities (ONLNREAD) was derived from the frequency with which students involved in the following reading activities (ST26): reading emails, <chat on line>, reading online news, using an online dictionary or encyclopedia, searching online information to learn about a particular topic, taking part in online group discussions or forums and searching for practical information online. The higher values on this index indicate more frequent online reading activities."}, {"section_title": "Approaches to learning", "text": "How students approach learning is based on student responses in ST27 and measured through the following three indices: memorization (MEMOR), elaboration (ELAB) and control strategies (CSTRAT). The index of memorization (MEMOR) was derived from the frequency with which students did the following when they were studying: i) try to memorize everything that is covered in the text; ii) try to memorize as many details as possible; iii) read the text so many times that they can recite it; and iv) read the text over and over again. The index of elaboration (ELAB) was derived from the frequency with which students did the following when they were studying: i) try to relate new information to prior knowledge acquired in other subjects; ii) figure out how the information might be useful outside school; iii) try to understand the material better by relating it to my own experiences; and iv) figure out how the text information fits in with what happens in real life. The index of control strategies (CSTRAT) was derived from students' reports on how often they did the following statements: i) when I study, I start by figuring out what exactly I need to learn; ii) when I study, I check if I understand what I have read; iii) when I study, I try to figure out which concepts I still haven't really understood; iv) when I study, I make sure that I remember the most important points in the text; and v) when I study and I don't understand something, I look for additional information to clarify this. Higher values on the index indicate higher importance attached to the given strategy."}, {"section_title": "Attitudes towards school", "text": "The index of attitude towards school (ATSCHL ) was derived from students' level of agreement with the following statements in ST33: i) school has done little to prepare me for adult life when I leave school; ii) school has been a waste of time; iii) school has helped give me confidence to make decisions; iv) school has taught me things which could be useful in a job. As all items that are negatively phrased i) and ii) are inverted for scaling, higher values on this index indicate perception of a more positive school climate. "}, {"section_title": "Disciplinary climate", "text": "The index of disciplinary climate (DISCLIMA) was derived from students' reports on how often the followings happened in their lessons of the language of instruction (ST36): i) students don't listen to what the teacher says; ii) there is noise and disorder; iii) the teacher has to wait a long time for the students to <quiet down>; iv) students cannot work well; and v) students don't start working for a long time after the lesson begins. As all items are inverted for scaling, higher values on this index indicate a better disciplinary climate."}, {"section_title": "Teachers' stimulation of students' reading engagement", "text": "The index of teachers' stimulation of students' reading engagement (STIMREAD) was derived from students' reports on how often the following occurred in their lessons of the language of instruction (ST37): i) the teacher asks students to explain the meaning of a text; ii) the teacher asks questions that challenge students to get a better understanding of a text; iii) the teacher gives students enough time to think about their answers; iv) the teacher recommends a book or author to read; v) the teacher encourages students to express their opinion about a text; vi) the teacher helps students relate the stories they read to their lives; and vii) the teacher shows students how the information in texts builds on what they already know. Higher values on this index indicate higher teachers' stimulation of students' reading engagement."}, {"section_title": "Use of structuring and scaffolding strategies", "text": "The index of use of structuring and scaffolding strategies (STRSTRAT) was derived from students reports on how often the following occurred in their lessons of the language of instruction (ST38): i) the teacher explains beforehand what is expected of the students; ii) the teacher checks that students are concentrating while working on the <reading assignment>; iii) the teacher discusses students' work, after they have finished the <reading assignment>; iv) the teacher tells students in advance how their work is going to be judged; v) the teacher asks whether every student has understood how to complete the <reading assignment>; vi) the teacher marks students' work; vii) the teacher gives students the chance to ask questions about the <reading assignment>; viii) the teacher poses questions that motivate students to participate actively; and ix) the teacher tells students how well they did on the <reading assignment> immediately after. Higher values on this index indicate a greater use of structured teaching."}, {"section_title": "Use of libraries", "text": "The index of use of libraries (LIBUSE) was derived from students' reports on the frequency for visiting a library for the following activities (ST39): i) borrow books to read for pleasure; ii) borrow books for school work; iii) work on homework, course assignments or research papers; iv) read magazines or newspapers; v) read books for fun; vi) learn about things that are not course-related, such as sports, hobbies, people or music; and vii) use the Internet. Higher values on this index indicate a great use of libraries."}, {"section_title": "Metacognition strategies: understanding and remembering", "text": "The index of understanding and remembering (UNDREM) was derived from students' reports on the usefulness of the following strategies for understanding and memorizing the text (ST41): A) I concentrate on the parts of the text that are easy to understand; B) I quickly read through the text twice; C) After reading the text, I discuss its content with other people; D) I underline important parts of the text; E) I summarize the text in my own words; and F) I read the text aloud to another person. This index was scored using a rater-scoring system. Through a variety of trial activities, both with reading experts and national centers, a preferred ordering of the strategies according to their effectiveness to achieve the intended goal was agreed. The experts' agreed order of the six items consisting this index is CDE > ABF. Scaling was conducted with two steps. First, a score was assigned to each student, which is a number that ranged from 0 to 1 and can be interpreted as the proportion of the total number of expert pair-wise relations that are consistent with the student ordering. For example, if the expert rule is (ABFD>CEG, 4\u00b43=12 pair wise rules are created (i.e. A>C, A>E, A>G, B>C, B>E, B>G, F>C, F>E, F>G, D>C, D>E, D>G). If the responses of a student on this task follow 8 of the 12 rules, the student gets a score of 8/12 = 0.67. Second, these scores were standardized for the index to have a mean of 0 and a standard deviation of 1 across OECD countries. Higher values on this index indicate greater students' perception of usefulness of this strategy."}, {"section_title": "Metacognition strategies: summarizing", "text": "The index of summarizing (METASUM) was derived from students' reports on the usefulness of the following strategies for writing a summary of a long and rather difficult two-page text about fluctuations in the water levels of a lake in Africa (ST42): A) I write a summary. Then I check that each paragraph is covered in the summary, because the content of each paragraph should be included; B) I try to copy out accurately as many sentences as possible; C) before writing the summary, I read the text as many times as possible; D) I carefully check whether the most important facts in the text are represented in the summary; and E) I read through the text, underlining the most important sentences, then I write them in my own words as a summary. This index was scored using a rater-scoring system. The experts' agreed order of the five items consisting this index is DE>AC>B. Higher values on this index indicate greater students' perception of usefulness of this strategy."}, {"section_title": "Reading for school", "text": "Students' engagement in reading for school is based on student responses to 17 items included in the last page of the test booklets and measured through the following four indices: index of interpretation of literary texts (RFSINTRP), index of use of texts containing non-continuous materials (RFSNCONT), index of reading activities for traditional literature courses (RFSTRLIT), index of use of functional texts (RFSFUMAT). For each item students were asked to report whether they read different texts for school (either in the classroom or as homework) \"many times\", \"two or three times\", \"once\", or \"not at all\". All items are inverted for scaling, so that higher values on this index indicate higher levels of enjoyment of reading. The index of interpretation of literary texts (RFSINTRP) was derived from the frequency with which students reported that in the past month they did the following: i) read fiction; ii) explain the cause of events in a text; iii) explain the way characters behave in a text; iv) explain the purpose of a text. G-9 The index of use of texts containing non-continuous materials (RFSNCONT) was derived from the frequency with which students reported that in the past month they did the following: i) use texts that include diagrams or maps; ii) use texts that include tables or graphs; iii) find information from a graph, diagram or table; and iv) describe the way the information in a table or graph is organized. The index of reading activities for traditional literature courses (RFSTRLIT) was derived from the frequency with which students reported that in the past month they did the following: i) read information texts about writers or books; ii) read poetry; iii) memorize a text by heart; iv) learn about the place of a text in the history of literature; v) learn about the life of the writer. The index of use of functional texts (RFSFUMAT) was derived from the frequency with which students reported that in the past month they did the following: i) read newspaper reports and magazine articles; ii) read instructions or manuals telling how to make or do something (e.g. how a machine works); and iii) read advertising material (e.g. advertisements in magazines, posters)."}, {"section_title": "School-level simple indices", "text": ""}, {"section_title": "School and class size", "text": "The index of school size (SCHSIZE) was derived by summing up the number of girls and boys at a school (SC06)."}, {"section_title": "Student-teacher ratio", "text": "Student-teacher ratio (STRATIO) was obtained by dividing the school size by the total number of teachers. The number of part-time teachers (SC09Q12) was weighted by 0.5 and the number of full-time teachers (SC09Q11) was weighted by 1.0 in the computation of this index."}, {"section_title": "Proportion of girls enrolled at school", "text": "The index of the proportion of girls in the school (PCGIRLS) was derived from the enrolment data (SC06)."}, {"section_title": "School type", "text": "Schools are classified into as either public or private, according to whether a private entity or a public agency has the ultimate power to make decisions concerning its affairs (SC02). This information is combined with SC03 which provides information on the percentage of total funding which comes from government sources to create the index of school type (SCHTYPE). This index has three categories: (1) public schools controlled and managed by a public education authority or agency, (2) government-dependent private schools controlled by a non-government organization or with a governing board not selected by a government agency that receive more than 50% of their core funding from government agencies, (3) government-independent private schools controlled by a non-government organization or with a governing board not selected by a government agency that receive less than 50% of their core funding from government agencies."}, {"section_title": "Availability of computers", "text": "The index of computer availability (IRATCOMP) was derived from dividing the number of computers available for educational purposes available to students in the modal grade for 15-year-olds (SC10Q02) by the number of students in the modal grade for 15-year-olds (SC10Q01). The index of computers connected to the Internet (COMPWEB) was derived from dividing the number of computers for educational purposes available to students in the modal grade for 15-year-olds that are connected to the web (SC10Q03) by the number of computers for educational purposes available to students in the modal grade for 15-year-olds (SC10Q02)."}, {"section_title": "Quantity of teaching staff at school", "text": "The proportion of fully certified teachers (PROPCERT) was computed by dividing the number of fully certified teachers (SC09Q21 plus 0.5*SC09Q22) by the total number of teachers (SC09Q11 plus 0.5*SC09Q12). The proportion of teachers who have an ISCED 5A qualification (PROPQUAL) was calculated by dividing the number of these kind of teachers (SC09Q31 plus 0.5*SC09Q32) by the total number of teachers (SC09Q11 plus 0.5*SC09Q12)."}, {"section_title": "Academic selectivity", "text": "The index of academic selectivity (SELSCH) was derived from school principals' responses on how frequently consideration was given to the following factors when students were admitted to the school, based on a scale from the response categories \"never\", \"sometimes\" and \"always\" (SC19Q02 and SC19Q03): student's record of academic performance (including placement tests); and recommendation of feeder schools. This index has the following three categories: (1) schools where these two factors are \"never\" considered for student admittance, (2) schools considering at least one of these two factors \"sometimes\" but neither factor \"always\", and (3) schools where at least one of these two factors is \"always\" considered for student admittance."}, {"section_title": "Ability grouping", "text": "The index of ability grouping between classes (ABGROUP) was derived from the two items of school principals' reports on whether school organises instruction differently for student with different abilities \"for all subjects\", \"for some subjects\", or \"not for any subject\" (SC12Q01 for grouping into different classes and SC12Q02 for grouping within classes). This index has the following three categories: (1) schools that do not group students by ability in any subjects, either between or within classes; (2) schools that group students by ability for some, but not all, subjects, and that do so either between or within classes; and (3) schools that group students by ability in all subjects either between or within classes."}, {"section_title": "School-level scale indices", "text": ""}, {"section_title": "School responsibility for resource allocation", "text": "School principals were asked to report whether \"principals\", \"teachers\", \"school governing board\", \"regional or local education authority\" or \"national education authority\" has a considerable responsibility for the following tasks (SC24): i) selecting teachers for hire; ii) dismissing teachers; iii) establishing teachers' starting salaries; iv) determining teachers' salaries increases; v) formulating the school budget; and vi) deciding on budget allocations within the school. The index of G-10 school responsibility for resource allocation (RESPRES) was derived from these six items. The ratio of the number of responsibility that \"principals\" and/or \"teachers\" have for these six items to the number of responsibility that \"regional or local education authority\" and/or \"national education authority\" have for these six items was computed. Positive values on this index indicate relatively more responsibility for schools than local, regional or national education authority. This index has an OECD mean of 0 and a standard deviation of 1."}, {"section_title": "School responsibility for curriculum and assessment", "text": "School principals were asked to report whether \"principals\", \"teachers\", \"school governing board\", \"regional or local education authority\", or \"national education authority\" has a considerable responsibility for the following tasks (SC24): i) establishing student assessment policies; ii) choosing which textbooks are used; iii) determining course content; and iv) deciding which courses are offered. The index of the school responsibility for curriculum and assessment (RESPCURR) was derived from these four items. The ratio of the number of responsibility that \"principals\" and/or \"teachers\" have for these four items to the number of responsibility that \"regional or local education authority\" and/or \"national education authority\" have for these four items was computed. Positive values on this index indicate relatively more responsibility for schools than local, regional or national education authority. This index has an OECD mean of 0 and a standard deviation of 1."}, {"section_title": "Teacher participation", "text": "The index of teacher participation (TCHPARTI) was scaled based on all 12 items in SC24 using school principals' responses that \"teachers\" have considerable responsibility. Higher values on this index indicate greater teachers' participation."}, {"section_title": "School principal's leadership", "text": "The index of school principal's leadership (LDRSHP) was derived from school principals' responses about the frequency with which they were involved in the following school affairs in the previous school year (SC26): i) make sure that the professional development activities of teachers are in accordance with the teaching goals of the school; ii) ensure that teachers work according to the school's educational goals; iii) observe instruction in classrooms; iv) give teachers suggestions as to how they can improve their teaching; v) use student performance results to develop the school's educational goals; vi) monitor students' work; vii) take the initiative to discuss matters, when a teacher has problems in his/her classroom; viii) inform teachers about possibilities for updating their knowledge and skills; ix) check to see whether classroom activities are in keeping with our educational goals; x) take exam results into account in decisions regarding curriculum development; xi) ensure that there is clarity concerning the responsibility for coordinating the curriculum; xii) solve the problem together, when a teacher brings up a classroom problem; xiii) pay attention to disruptive behavior in classrooms; and xiv) take over lessons from teachers who are unexpectedly absent. Higher values on this index indicate greater involvement of school principals in school affairs."}, {"section_title": "Teacher shortage", "text": "The index of teacher shortage (TCSHORT) was derived from four items measuring school principals' perceptions of potential factors hindering instruction at their school (SC11). These factors are a lack of: i) qualified science teachers; ii) a lack of qualified mathematics teachers; iii) qualified <test language> teachers; and iv) qualified teachers of other subjects. Higher values on this index indicate school principals' reports of higher teacher shortage at a school."}, {"section_title": "School's educational resources", "text": "The index on the school's educational resources (SCMATEDU) was derived from seven items measuring school principals' perceptions of potential factors hindering instruction at their school (SC11). These factors are: i) shortage or inadequacy of science laboratory equipment; ii) shortage or inadequacy of instructional materials; iii) shortage or inadequacy of computers for instruction; iv) lack or inadequacy of Internet connectivity; v) shortage or inadequacy of computer software for instruction; vi) shortage or inadequacy of library materials; and vii) shortage or inadequacy of audio-visual resources. As all items were inverted for scaling, higher values on this index indicate better quality of educational resources."}, {"section_title": "Extra-curricular activities offered by school", "text": "The index of extra-curricular activities (EXCURACT) was derived from school principals' reports on whether their schools offered the following activities to students in the national modal grade for 15-year-olds in the academic year of the PISA assessment (SC13): i) band, orchestra or choir; ii) school play or school musical; iii) school yearbook, newspaper or magazine; iv) volunteering or service activities; v) book club; vi) debating club or debating activities; vii) school club or school competition for foreign language mathematics or science; viii) <academic club>; ix) art club or art activities; x) sporting team or sporting activities; xi) lectures and/or seminars; xii) collaboration with local libraries; xiii) collaboration with local newspapers; and xiv) <country specific item>. Higher values on the index indicate higher levels of extra-curricular school activities."}, {"section_title": "Teacher behavior", "text": "The index on teacher-related factors affecting school climate (TEACBEHA) was derived from school principals' reports on the extent to which the learning of students hindered by the following factors in their schools (SC17): i) teachers' low expectations of students; ii) poor student-teacher relations; iii) teachers not meeting individual students' needs; iv) teacher absenteeism; v) staff resisting change; vi) teachers being too strict with students; and vii) students not being encouraged to achieve their full potential. As all items were inverted for scaling, higher values on this index indicate a positive teacher behavior."}, {"section_title": "Student behavior", "text": "The index of student-related factors affecting school climates (STUDBEHA) was derived from school principals' reports on the extent to which the learning of students hindered by the following factors in their schools (SC17): i) student absenteeism; ii) disruption of classes by students; iii) students skipping classes; iv) student lacking respect for teachers; v) student use of alcohol or illegal drugs; and vi) students intimidating or bullying other students. As all items were inverted for scaling higher values on this index indicate a positive student behavior."}, {"section_title": "Parent questionnaire simple indices", "text": ""}, {"section_title": "Parent questionnaire scale indices", "text": ""}, {"section_title": "Parents' perception of school quality", "text": "The index of parents' perception of school quality (PQSCHOOL) was derived from parents' level of agreement with the following statements (PA14): i) most of my child's school teachers seem competent and dedicated; ii) standards of achievement are high in my child's schools; iii) I am happy with the content taught and the instructional methods used in my child's school; iv) I am satisfied with the disciplinary atmosphere in my child's school; v) my child's progress is carefully monitored by the school; vi) my child's school provides regular and useful information on my child's progress; and vii) my child's school does a good job in educating students. As all items were inverted for scaling, higher values on this index indicate parents' positive evaluations of the school's quality."}, {"section_title": "Parents' involvement in school", "text": "The index of parents' involvement in school (PARINVOL) was derived from parents' responses to whether they have participated in various school-related activities during the previous academic year (PA15). Parents were asked to report \"yes\" or \"no\" for the following statements: i) discuss my child's behavior or progress with a teacher on my own initiative; ii) discuss my child's behavior or progress on the initiative of one of my child's teachers; iii) volunteer in physical activities; iv) volunteer in extracurricular activities; v) volunteer in school library or media centre; vi) assist a teacher in school; vii) appear as a guest speaker; and viii) participate in local school. Higher values on this index indicate greater parents' involvement in school."}, {"section_title": "Students reading resources at home", "text": "The index of students' reading resources at home (READRES) was derived from parents' reports on whether the followings are available for their children in their home (PA07): i) email; ii) online chat; iii) Internet connection; iv) daily newspaper; v) subscription to journal or magazine; and vi) books of his/her own (not school books). Higher values on this index indicate greater availability of reading resources at home."}, {"section_title": "Parents' current support of their child's reading literacy", "text": "The index of parents' current support of their child's reading literary (CURSUPP) was derived from parents' reports on the frequency with which they or someone else in their home did the following with their child (PA08): i) discuss political or social issues; ii) discuss books, films or television programs; iii) discuss how well the child is doing at school; iv) go to a bookstore or library with the child; v) talk with the child about what he/she is reading; and vi) help the child with his/her homework. Higher values on this index indicate greater parental support of child's reading literacy."}, {"section_title": "Parents' support of their child's reading literacy at the beginning of primary school", "text": "The index of parents' support of their child's reading literacy at the beginning of primary school (PRESUPP) was derived from parents' reports on the frequency with which they or someone else in their home undertook the following activities with their child when the child attended the first year of primary school (PA03): i) read books; ii) tell stories; iii) sing songs; iv) play with alphabet toys; v) talk about what parent had read; vi) play word games; vii) wrote letters or words; and viii) read aloud signs and labels. Higher values on this index indicate greater levels of parents' support."}, {"section_title": "Motivational attributes of parents' own reading engagement", "text": "The index of motivational attributes of parents' own reading engagement (MOT READ) was derived from parents' level of agreement with the following statements (PA06): i) reading is one of my favorite hobbies; ii) I feel happy if I receive a book as a present; iii) for me reading is a waste of time; and iv) I enjoy going to a bookstore or library. As the item iii was inverted for scaling, higher values on this index indicate greater parents' motivation to engage in reading activities."}, {"section_title": "Nonresponse Bias Analysis Report", "text": ""}, {"section_title": "H.1 Introduction", "text": "An important component of survey or assessment data quality is the representativeness of the study sample. This representativeness of the population is achieved by selecting a sample of respondents who are similar to the population in terms of key demographic variables. However, in practice not all sampled respondents participate in surveys. If enough respondents fail to participate or if respondents differ in their response rates by key demographic characteristics, the final sample may not represent the target population. The extent to which the distributions of the sampled respondents differ from the corresponding distributions of the population is termed unit nonresponse bias. One way to characterize and quantify the presence of unit nonresponse bias is to compare responding sample elements with nonresponding sample elements with respect to underlying sociodemographic characteristics for which data are available on the frame. In the case of Program for International Student Assessment (PISA) 2009, unit nonresponse can occur at both the school and student levels. Also, not all sample respondents respond to all applicable items in the questionnaire. If enough respondents fail to respond to a question or if respondents differ in their item response rates by key demographic characteristics, the item respondents may not represent the student respondents. The extent to which the distributions of the item respondents differ from the corresponding distributions of the sample respondents is termed item nonresponse bias. One way to characterize and quantify the presence of item nonresponse bias is to compare responding item cases with nonresponding item cases with respect to underlying sociodemographic characteristics for which data are available on the frame or on other items on the questionnaire. The National Center for Education Statistics (NCES) standards for assessment surveys stipulate that a nonresponse bias analysis is required at any stage of data collection with a weighted unit response rate less than 85 percent. The international PISA 2009 standards also require that nonresponse bias analyses need to be conducted if school response rates are less than 85 percent. For PISA 2009, the weighted school response rate for originally sampled schools was 68 percent 11 and the weighted student response rate was 87 percent. Thus, unit nonresponse bias analysis was required at the school level, but not at the student level. Some questionnaire items also had response rates below 85 percent, and thus this report includes analyses of those items. The objective of this nonresponse bias analysis is to shed light on any biases that might be present in the data because of nonresponse. That is, responding and nonresponding schools are compared using information from the sampling frame to determine whether responding schools are representative of the original sample or whether there are significant differences between the responding and nonresponding schools. Also, respondents and nonrespondents to questionnaire items are compared using information from the sampling frame and other questionnaire items to determine whether students responding to items actually represent a virtually random subsample of the sample respondents or whether there are significant differences between the students responding and not responding to items. "}]