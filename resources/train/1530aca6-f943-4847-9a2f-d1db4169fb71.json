[{"section_title": "Abstract", "text": "Disease progression modeling (DPM) using longitudinal data is a challenging task in machine learning for healthcare that can provide clinicians with better tools for diagnosis and monitoring of disease. Existing DPM algorithms neglect temporal dependencies among measurements and make parametric assumptions about biomarker trajectories. In addition, they do not model multiple biomarkers jointly and need to align subjects' trajectories. In this paper, recurrent neural networks (RNNs) are utilized to address these issues. However, in many cases, longitudinal cohorts contain incomplete data, which hinders the application of standard RNNs and requires a pre-processing step such as imputation of the missing values. We, therefore, propose a generalized training rule for the most widely used RNN architecture, long short-term memory (LSTM) networks, that can handle missing values in both target and predictor variables. This algorithm is applied for modeling the progression of Alzheimer's disease (AD) using magnetic resonance imaging (MRI) biomarkers. The results show that the proposed LSTM algorithm achieves a lower mean absolute error for prediction of measurements across all considered MRI biomarkers compared to using standard LSTM networks with data imputation or using a regression-based DPM method. Moreover, applying linear discriminant analysis to the biomarkers' values predicted by the proposed algorithm results in a larger area under the receiver operating characteristic curve (AUC) for clinical diagnosis of AD compared to the same alternatives, and the AUC is comparable to state-of-the-art AUC's from a recent cross-sectional medical image classification challenge. This paper shows that built-in handling of missing values in LSTM network training paves the way for application of RNNs in disease progression modeling."}, {"section_title": "Introduction", "text": "Alzheimer's disease (AD) is a chronic neurodegenerative disorder that begins with short-term memory loss and develops over time, causing issues in conversation, orientation, and control of bodily functions [1] . Early diagnosis of the disease is challenging and the diagnosis is usually made once cognitive impairment has already compromised daily living. Hence, developing robust, data-driven methods for disease progression modeling (DPM) utilizing longitudinal data is necessary to yield a complete perspective of the disease for better diagnosis, monitoring, and prognosis [2] .\nExisting DPM techniques attempt to describe biomarker measurements as a function of disease progression through continuous curve fitting. In the AD progression literature, a variety of regressionbased methods have been applied to fit logistic or polynomial functions to the longitudinal dynamic of each biomarker [3] [4] [5] [6] [7] [8] . However, parametric assumptions on the biomarker trajectories limit the applicability of such methods; in addition, none of the existing approaches considers the temporal dependencies among measurements. Furthermore, the available methods mostly rely on independent biomarker modeling and require alignment of subjects' trajectories -either as a pre-processing step or as part of the algorithm.\nRecurrent neural networks (RNNs) are sequence learning based methods that can offer continuous, non-parametric, joint modeling of longitudinal data while taking temporal dependencies amongst measurements into account [9] . However, since longitudinal cohort data often contain missing values due to, for instance, dropped out patients, unsuccessful measurements, and/or varied trial design, standard RNNs require pre-processing steps for data imputation which may result in suboptimal analyses and predictions [10] . Therefore, the lack of methods to inherently handle incomplete data in RNNs is evident [11] .\nLong short-term memory (LSTM) networks are widely used types of RNNs developed to effectively capture long-term temporal dependencies by dealing with the exploding and vanishing gradient problem during backpropagation through time [12] [13] [14] . They employ a memory cell with nonlinear reset units -so called constant error carousels (CECs), and learn to store history for either long or short time periods. Since their introduction, a variety of LSTM networks have been developed for different time-series applications [15] . The vanilla LSTM, among others, is the most commonly used architecture that utilizes three reset gates with full gate recurrence and applies backpropagation algorithm through time using full gradients. Nevertheless, its complete topology can include biases and cell-to-gates (peephole) connections.\nThe most common approach to handling missing data with LSTM networks is data interpolation pre-processing step, usually using mean or forward imputation. This two-step procedure decouples missing data handling and network training, resulting in a sub-optimal performance, and it is heavily influenced by the choice of data imputation scheme. Other approaches, update the architecture to utilize possible correlations between missing values' patterns and the target to improve prediction results [10, 11] . Our goal is different; we want to make the training of LSTM networks robust to missing values to more faithfully capture the true underlying signal, and to make the learned model generalizable across cohorts -not relying on specific cohort or demographic circumstances correlated with the target.\nIn this paper, we propose a generalized method for training LSTM networks that can handle missing values in both target and predictor variables. This is achieved via applying the batch gradient descent algorithm together with normalizing the loss function and its gradients with respect to the number of missing points in target and input, to ensure a proportional contribution of each weight per epoch. The proposed LSTM algorithm is applied for modeling the progression of AD in the Alzheimer's Disease Neuroimaging Initiative (ADNI) cohort [16] based on magnetic resonance imaging (MRI) biomarkers, and the estimated biomarker values are used to predict the clinical status of subjects.\nOur main contribution is three-fold. Firstly, we propose a generalized formulation of backpropagation through time for LSTM networks to handle incomplete data and show that such built-in handling of missing values provides better modeling and prediction performances compared to using data imputation with standard LSTM networks. Secondly, we model temporal dependencies among measurements within the ADNI data using the proposed LSTM network via sequence-to-sequence learning. To the best of our knowledge, this is the first time such multi-dimensional sequence learning methods are applied for neurodegenerative DPM. Lastly, we introduce an end-to-end approach for modeling the longitudinal dynamics of imaging biomarkers -without need for trajectory alignmentand for clinical status prediction. This is a practical way to implement a robust DPM for both research and clinical applications."}, {"section_title": "Proposed LSTM algorithm", "text": "The main goal of this study is to minimize the influence of missing values on the learned LSTM network parameters. This is achieved by using the batch gradient descend scheme together with the backpropagation through time algorithm modified to take into account missing data in the input and target vectors. More specifically, the algorithm accumulates the input weight gradients proportionally weighted according to the number of available time points per input biomarker node using the subjectspecific normalization factor of \u03b2 weighted according to the number of available time points per output biomarker node using the subject-specific normalization factor of \u03b2 j m , and normalized with respect to the total number of available input values for all visits of all biomarkers -propagated through the forward pass -using the subject-specific normalization factor of \u03b2 j x . Such modification of the loss function also ensures that all gradients of the network weights are indirectly normalized. Finally, the use of batch gradient descend ensures that there is at least one visit available per biomarker so that each input node can proportionally contribute in the weight updates. Figure 1 shows a typical schematic of a vanilla LSTM architecture. As can be seen, the topology includes a memory cell, an input modulation gate, a hidden activation function, and three nonlinear reset gates, namely input gate, forget gate, and output gate, each of which accepting current and recurrent inputs. The memory cell learns to maintain its state over time while the multiplicative gates learn to open and close access to the constant error/information flow, to prevent exploding or vanishing gradients. The input gate protects the memory contents from perturbation by irrelevant inputs, while the output gate protects other units from perturbation by currently irrelevant memory contents. The forget gate deals with continual or very long input sequences, and finally, peephole connections allow the gates to access the CEC of the same cell state."}, {"section_title": "The basic LSTM architecture", "text": ""}, {"section_title": "Feedforward in LSTM networks", "text": "is the j-th observation of an N -dimensional input vector at current time t. If M is the number of output units, feedforward calculations of the LSTM network under study can be summarized as\nare j-th observation of forget gate, input gate, modulation gate, cell state, output gate, and hidden output at time t before and after activation, respectively. Moreover,\nM \u00d7M are sets of connecting weights from input and recurrent, respectively, to the gates and cell,\nM \u00d71 is the set of peephole connections from the cell to the gates, {b f ,\nrepresents corresponding biases of neurons, and denotes element-wise multiplication. Finally, \u03c3 g , \u03c3 c , and \u03c3 h are nonlinear activation functions assigned for the gates, input modulation, and hidden output, respectively. Logistic sigmoid functions are applied for the gates with range [0, 1] while hyperbolic tangent functions are applied for modulation of both cell input and hidden output with range [\u22121, 1]."}, {"section_title": "Robust backpropagation through time", "text": "Let L \u2208 R M \u00d71 be the loss function defined based on the actual target s and network output y. Here, we consider one layer of LSTM units for sequence learning which means that the network output is the hidden output. The main idea is to calculate the partial derivatives of the normalized loss function (\u03b4) with respect to the weights using the chain rule. Hence, the backpropagation calculations through time using full gradients can be obtained as where\nis the normalization factor handling missing input values and |x j (n)| is the number of available input time points in the n-th node."}, {"section_title": "Momentum batch gradient descent", "text": "As an efficient iterative algorithm, momentum batch gradient descent is applied to find the local minimum of the loss function calculated over a batch while speeding up the convergence. The update rule can be written as\nwhere \u03d1 is the weight update initialized to zero, \u03c9 is the to-be-updated weight array, \u03b4\u03c9 is the gradient of the loss function with respect to \u03c9, and \u03b1, \u03b3, and \u00b5 are the learning rate, weight decay or regularization factor, and momentum weight, respectively."}, {"section_title": "Experiments", "text": ""}, {"section_title": "Data preparation", "text": "We utilize the dataset from The Alzheimer's Disease Prediction Of Longitudinal Evolution 1 [17] (TADPOLE) challenge for DPM using the LSTM network. The dataset is composed of data from the three ADNI phases ADNI 1, ADNI GO, and ADNI 2. This includes roughly 1,500 biomarkers acquired from 1,737 subjects (957 males and 780 females) during 12,741 visits at 22 distinct time points between 2003 and 2017. Table 1 summarizes statistics of the demographics in the TADPOLE dataset. Note that the subjects include missing measurements during their visits and not all of them are clinically labeled.\nIn this work, we have merged existing groups labeled as cognitively normal (CN), significant memory concern (SMC), and normal (NL) under CN, mild cognitive impairment (MCI), early MCI (EMCI), and late MCI (LMCI) under MCI, and Alzheimer's disease (AD) and Dementia under AD. Moreover, groups with labels converting from one status to another, e.g. \"MCI-to-AD\", are assumed to belong the next status (\"AD\" in this example).\nMRI biomarkers are used for AD progression modeling. This includes T1-weighted brain MRI volumes of ventricles, hippocampus, whole brain, fusiform, middle temporal gyrus, and entorhinal cortex. We normalize the MRI measurements with respect to the corresponding intracranial volume (ICV). Out of 22 visits, we select 11 visits -including baseline -with a fix interval of one year to span the majority of measurements and subjects. Next, we filter data outliers based on the specified range of each biomarker and normalize the measurements to be in the range [\u22121, 1]. Finally, subjects with less than three distinct visits for any biomarker are removed to obtain 742 subjects. This is to ensure that at least two visits are available per biomarker for performing sequence learning through the feedforward step and an additional visit for backpropagation.\nFor evaluation purpose, we partition the entire dataset to three non-overlapping subsets for training, validation, and testing. To achieve this, we randomly select 10% of the within-class subjects for validation and the same for testing. More specifically, based on the baseline labels of subjects, we randomly pick within-class samples ensuring to have enough subjects with few and large number of visits in each subset. This process results in 592, 76, and 74 subjects for training, validations, and testing, respectively."}, {"section_title": "Evaluation metrics", "text": "Mean absolute error (MAE) and multi-class area under the receiver operating characteristic (ROC) curve (AUC) are used to assess the modeling and classification performances, respectively. MAE measures accuracy of continuous prediction per biomarker by computing the difference between actual and estimated values as follows\nwhere s t j and y t j are the ground-truth and estimated values of the specific biomarker for the j-th subject at the t-th visit, respectively, and I is the number of existing points in the target array s. Multi-class AUC [18] , on the other hand, is a measure to examine the diagnostic performance in a multi-class test set using ROC analysis. It can be calculated using the posterior probabilities as follows\nwhere n c is the number of distinct classes, n i denotes the number of available points belonging to the i-th class, and SR i is the sum of the ranks of posteriors p(c i |s i ) after sorting all concatenated posteriors {p(c i |s i ), p(c i |s k )} in an increasing order, where s i and s k are vectors of scores belonging to the true classes c i and c k , respectively."}, {"section_title": "Experimental setup", "text": "All the evaluated methods in this study are developed in-house in MATLAB R2017b and run on a 2.80 GHz CPU with 16 GB RAM. We initialize the LSTM network weights by generating uniformly distributed random values in the range [\u22120.05, 0.05] and set the weights' updates and weights' gradients to zero. We set the batch size to the number of available training subjects. Furthermore, for simplicity, we use the first ten visits to estimate the second to eleventh visits per subject and use the estimated values for evaluation. Finally, we train the network using feedforward and the proposed method of backpropagation through time where the network replace the input missing values and corresponding error of the output missing values with zero.\nWe utilize the validation set to tune the network optimization parameters each time by adjusting one of the parameters while keeping the rest at fixed values to achieve the lowest average MAE. Peephole connections are used in the network as they intend to improve the performance. Based on these strategies, the optimal parameters are obtained as \u03b1 = 0.1, \u00b5 = 0.9, and \u03b3 = 0.0001 with 1,000 epochs. The corresponding MAE's for the validation set are also calculated as 2.9590 \u00d7 10 \u22123 , 2.4603 \u00d7 10 \u22124 , 1.4943 \u00d7 10 \u22122 , 2.4161 \u00d7 10 \u22124 , 7.5522 \u00d7 10 \u22124 , 9.6592 \u00d7 10 \u22124 , respectively for ventricles, hippocampus, whole brain, entorhinal cortex, fusiform, and middle temporal gyrus. Moreover, it takes about 340 seconds and 0.025 seconds for training and validation, respectively. It is worthwhile mentioning that all the estimated biomarker's measurements are transformed back to their actual ranges while calculating MAE's."}, {"section_title": "Results", "text": "After successfully training our LSTM network, we examine it using the obtained test subset. Next, we train the network using mean imputation (LSTM-Mean) [11] and forward imputation (LSTMForward) [10] . Moreover, we use the parametric, regression-based method of [3] to model the AD progression. Table 2 compares the test modeling performance (MAE) of the MRI biomarkers using aforementioned approaches. As it can be deduced from Table 2 , our proposed method outperforms all other modeling techniques in all categories. It should be noticed that when we apply data imputation, the backpropagation formulas simply generalize to the standard LSTM network.\nTo assess the ability of the estimated biomarkers' measurements in predicting the clinical labels, we apply a linear discriminant analysis (LDA) classifier to the multi-dimensional training data estimations to compute the posterior probability scores in the test data. The obtained scores are then used to calculate the AUC's. The diagnostic prediction results for the test set are shown in Table 3 for the utilized methods. As can be seen, the proposed method outperforms all other schemes in predicting clinical status of subjects per visits. This, in turn, reveals the effect of modeling on classification performance. One could of course use different classifiers to improve the results. But our focus in this paper is on DPM or sequence-to-sequence learning. On the other hand, it is possible to train the LSTM network for a classification (sequence-to-label) problem. However, since this approach requires labeled data, it would only be able to use a subset of the utilized data in training.\nFurthermore, the diagnostic classification results of the predicted MRI biomarkers' measurements using the proposed approach are comparable to state-of-the-art cross-sectional MRI-based classification results in the recent challenge on Computer-Aided Diagnosis of Dementia (CADDementia) [19] . To be more specific, LDA classification on predicted features using the proposed method achieves a multi-class AUC of 0.76 which is within the top-five multi-class AUCs in the challenge that ranged from 0.79 to 0.75."}, {"section_title": "Summary and discussion", "text": "In this paper, a training algorithm was proposed for LSTM networks aiming to improve robustness against missing data, and the robustly trained LSTM network was applied for AD progression modeling using longitudinal measurements of imaging biomarkers. To the best of our knowledge this is the first time RNNs have been studied and applied for DPM within the ADNI cohort. The proposed training method demonstrated better performance than using imputation prior to a standard LSTM network and outperformed an established parametric, regression-based DPM method, in terms of both biomarker prediction and subsequent diagnostic classification.\nMoreover, the classification results using the predicted MRI measurements of the proposed method are comparable to those of the CADDementia challenge. It should, however, be noted that there are important differences between this study and the CADDementia challenge. Firstly, this work has the advantage of training and testing features from the same cohort whereas CADDementia algorithms were applied to classify data from independent cohorts. Secondly, the top performing CADDementia algorithms incorporated different types of MRI features besides volumetry. Thirdly, in contrast to CADDementia where features were completely available, this work predicts features based on longitudinal data before classification.\nThis study highlights the potential of RNNs for modeling the progression of AD using longitudinal measurements, provided that proper care is taken to handle missing values and time intervals. In general, standard LSTM networks are designed to handle sequences with a fixed temporal or spatial sampling rate within longitudinal data. We used the same approach in the AD progression modeling application by disregarding, for example, visiting months 3, 6 and 18, and confining the experiments to yearly follow-up in the ADNI data. However, one could utilize modified LSTM architectures such as time-aware LSTM [20] to address irregular time steps in longitudinal patient records."}]