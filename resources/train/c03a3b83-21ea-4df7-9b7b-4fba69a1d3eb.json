[{"section_title": "Abstract", "text": "and the Alzheimer's Disease Neuroimaging Initiative 10.1073/pnas.XXXXXXXXXX\nThis appendix present more details regarding the technical results in the main paper, the data and the experimental processing pipeline. We also present a detailed version of the theorems on consistency results in the main paper. The proofs for those theorems (Thm. 2 and Thm. 3 in the main paper) can be found in the reference (1). We also discuss some technical aspects describing how to conduct hypothesis tests in the small sample size case, how to find a minimal d-separating set to control biases efficiently, and how to solve the optimization problem. We give the full proofs of Thm. 1 and Thm. 4 in the main paper here in the appendix. Finally we show the performance of the hypothesis tests and estimation with some simulations."}, {"section_title": "", "text": "The W-ADRC dataset is available to the research community. All requests are handled centrally by the W-ADRC. A researcher can access the data at http://www.adrc.wisc.edu/apply-resources. This includes the user agreement to ensure that investigators are in compliance with their local institutional IRB policy.\nIn our datasets W-ADRC and ADNI, there are 7 proteins available which we used in our analyses, including A\u03b21\u221238, A\u03b21\u221240, A\u03b21\u221242, t \u2212 tau, p \u2212 tau181, neurofilament light (NFL) and neurogranin (NG) (2) . We describe the assays used to get the measurements of those proteins. The reader can find more details regarding the assays in the literature (3) . In W-ADRC, A\u03b21\u221238, A\u03b21\u221240, A\u03b21\u221242 were quantified by electrochemiluminescence (ECL) using an A\u03b2 triplex assay/Meso Scale Discovery (MSD). For the ADNI data acquired at University of Pennsylvania (UPenn), the procedure used was 2D-UPLC-tandem mass spectrometry method. In W-ADRC, A\u03b21\u221242, t \u2212 tau, p \u2212 tau181 were quantified with INNOTEST, whereas in the ADNI data, UPenn uses AlzBio3 xMAP to measure A\u03b21\u221242, t \u2212 tau, p \u2212 tau181. Both W-ADRC and ADNI data use a sandwich ELISA method to measure NFL and electrochemiluminescence technology/MSD to measure neurogranin.\nTo obtain the ADNI datasets, we downloaded \"UPENN CSF Biomarker Master\" file and select each person's baseline data at batch UPENNBIOMK to get A\u03b21\u221242, t \u2212 tau, p \u2212 tau181. We downloaded the \"UPENN 2D-UPLC tandem mass spectrometry measurement\" file to get MSD A\u03b21\u221238, A\u03b21\u221240, A\u03b21\u221242 values. We downloaded the \"Blennow Lab CSF Ng\" and \"Blennow Lab CSF NFL\" files to get neurogranin and NFL. We downloaded the \"Key ADNI tables merged into one table\" file to get the hippocampus volume, diagnosis status, age and sex. Then, we merge all tables by patient ID as the primary key and we only include samples with all measurements available, which gives us 311 samples. Similarly, we get a dataset from W-ADRC only including samples with all measurements available, which gives us 154 samples. Since sample sizes of ADNI and W-ADRC datasets are very different for participants with age larger than 85 and smaller than 55, we decided to only consider subjects with ages between 55 and 85, which leads to 284 samples for ADNI and 125 samples for W-ADRC.\n(2) The kernel K of RKHS used for MMD is non-negative, characteristic and bounded by a constant k.\nfor any x in the support of PS and \u2200\u03bb1, \u03bb2 \u2208 \u2126 \u03bb (3b) K(g \u03b8 1 (x), \u00b7) \u2212 K(g \u03b8 2 (x), \u00b7) H \u2264 Lg \u03b81 \u2212 \u03b82 rg with constants Lg, rg, for any x in the support of PT and \u2200\u03b81, \u03b82 \u2208 \u2126 \u03b8 The Assumption S.1 (2) is used for most consistency analyses based on MMD (4). The Assumption S.1 (3a) and (3b) are satisfied for a big class of transformations with differentiable radial basis kernel. We have following sufficient conditions to satisfy Assumptions S.1 (3a) and (3b). Lemma S.2. If the following two conditions are satisfied, then Assumptions S. 1 (3a) and (3b) hold.\n(1) The kernel function K( x \u2212 y 2) is a radial basis kernel where \u2202K(.) is bounded in a neighborhood of 0.\n(2) The transformation h \u03bb (x) is Holder-continuous as a function of \u03bb with ratio r h for any x in the support of PS. Similarly, the transformation g \u03b8 (x) is Holder-continuous as a function of \u03b8 with ratio rg for any x in the support of PT . Based on Assumptions S.1, we are guaranteed to satisfy the consistency results (Thm. 2 and Thm. 3) in the main paper. For hypothesis testing results, we must take care of constant terms because they are used to construct the threshold in the large sample size case. Therefore, we present the detailed version here with the relevant constant terms. Recall that we define MMD(PS, PT ) as the population MMD distance between distributions PS and PT , and we define MMD(XS, XT ) as the empirical MMD of samples XS and XT . The full theorem for consistency of hypothesis testing is Theorem S.3. [Thm. 3 in the main paper] Given Assumptions S.1, when H0 is true, with probability at least 1 \u2212 \u03b1,\nWhen HA is true, with probability at least 1 \u2212 \u03b1,\nwhere C * = min \u03bb,\u03b8 MMD(h \u03bb (PS), g \u03b8 (PT )) is a positive constant when HA holds (4) .\nThe proof of Thm. S.3 is presented in the reference (1)."}, {"section_title": "S.3. Hypothesis Test in the Small to Moderate Sample Size Case", "text": "The threshold given in Thm. S.3 can be used for performing hypothesis tests when the sample size is large enough. But when the sample sizes are small to moderate, a data-driven method may perform better. The same observation is discussed in related papers (4) for the MMD-based two sample test.\nFor the data-driven method, we require one transformation class to always be the identity. In other words, we consider g \u03b8 (XT ) to always be XT itself and we transform XS using h\u03bb(XS) to match XT . Before presenting the method, let us recall our definition of the null and alternative hypotheses.\nH0 : There exists a \u03bb such that h \u03bb (XS) matches XT HA : There is no \u03bb which matches h \u03bb (XS) and XT To test these hypotheses, we construct a bootstrap-type procedure here to calculate p-values: Randomly generate nS and nT samples:X b S ,X b T from empirical distribution based on XT with replacement."}, {"section_title": "4:", "text": "Calculate the empirical distance between two distribution, mmd b , by\nCompute p-value given the test statistics MMD(h\u03bb(XS), XT ) based on the empirical distributions of (mmd 1 , ..., mmd B ).\nRemark. We can speed up the loop in Alg. S.1 by computing distances between nS + nT samples and permuting them for each iteration. We now show that our procedure is a valid hypothesis test given the following results: Lemma S.4. Given nS samplesXS from PT , we have that (from Thm. 7 in (4)) with probability at least 1 \u2212 \u03b1,\nFig. S1. In this graphical diagram, (a) represents the graphical causal model for our AD study example. CSF X and other variations E P , E B are d-separated by diagnosis information D and age age, whereas (b) represents the moral graph of the subgraph on X, E P , E B and their ancestors, that is, mDAG An (X\u222aE P \u222aE B ) .\n[S.1] tells us that the bootstrap empirical distribution we construct is bounded by some constants converging to 0 with probability at least 1 \u2212 \u03b1. Therefore, we can reject the null hypothesis when HA holds because MMD(h\u03bb(XS), XT ) converges to a positive constant asymptotically with high probability from Thm. 3 in the main paper (or Thm. S.3 in the supplement). Also, we can control the type I error from our bootstrap procedure in the following way. For a critical value (related to a significance level \u03b1) calculated from the bootstrap empirical distribution, when H0 is true, MMD(h \u03bb 0 (XS), XT ) is smaller than the critical value with probability at least 1 \u2212 \u03b1 because of [S.2]. Further MMD(h\u03bb(XS), XT ) must be smaller than MMD(h \u03bb 0 (XS), XT ) since the former quantity is the minimum in the search region \u2126 \u03bb . Therefore, the type 1 error for our hypothesis test is well-controlled."}, {"section_title": "S.4. Finding a Minimal D-separating Set by BFS", "text": "We provide additional details regarding finding a minimal d-separating set when the graphical causal model is not too complicated (5) . Before stating the algorithm, we introduce some notations first. For any node set A, we define An(A) to be the ancestral set containing set A, that is, An(A) = A \u222a (\u222au\u2208A{all ancestors of u}). We call the directed subgraph composed only of nodes from An(A) as DAG An(A) . A so-called moral graph is formed from a directed acyclic graph by adding edges between all pairs of nodes that have a common child, and then making all edges in the graph undirected. We denote the moral graph of DAG An(A) as mDAG An(A) .\nIn order to find a minimal d-separating set Z for the measurements of interest X and bias nodes EP and EB, as mentioned in the main paper, we only need to consider this question on the undirected graph (DAG An(X\u222aE P \u222aE B ) ) instead of the full directed acyclic graph (as shown in (5)). We give an example for finding the minimal d-separating set of the graph in the main paper, which is shown in Fig. S1 . Instead of using Fig. S1 , the minimal d-separating set can be found from Fig. S1 (b), which represents mDAG An(X\u222aE P \u222aE B ) .\nThe authors (5) show that searching a d-separating set for X and the bias nodes on the original DAG is equivalent to finding a node set that can block any path between X and bias nodes in the moral graph of the DAG. Therefore, we can run a Breadth First Search (BFS) algorithm on the undirected graph mDAG An(X\u222aE P \u222aE B ) to check whether Z is the minimal d-separating set and adjust it if it is not. For our example in Fig. S1 , by using BFS we can see that Z = {D, age} will block any path from X to the bias nodes EP and EB in Fig. S1 (b)."}, {"section_title": "S.5. Optimization", "text": "We give a special case of the optimization problem in the main paper in [1] when we set K to be a Gaussian kernel, g \u03b2 (.) as the identity and h W (x) = W Tr(x) as a linear function for parameters W . Here, Tr(x) is a known transformation on x, for example, Tr(x) = x 2 relates to the second order polynomial transformation. Recall that the general form of the optimization is\nFor our example case, the simplified objective can be written a\u015d\nHao Henry Zhou, Vikas Singh, Sterling C. Johnson, Grace Wahba, and the Alzheimer's Disease Neuroimaging Initiative 10.1073/pnas.XXXXXXXXXX This is a continuous optimization and its gradient with respect to F is:\nSince the objective is continuous, the optimization can be performed by gradient descent or stochastic gradient descent. A useful prior based on some domain knowledge can also help the optimization scheme since the objective is non-convex.\nWe now provide a result to setup a confidence region to choose a good initial point for the optimization. \nwhere VAR is the variance and tr(\u00b7) is the trace. For any , \u03b1 > 0 and a sufficiently large sample size, a neighborhood of the true W0 is contained in \u2126W with probability at least 1 \u2212 \u03b1."}, {"section_title": "S.6. Proof for Identifiability Conditions (Thm. 1 in the main paper)", "text": "In this subsection, we provide a proof for the identifiability conditions. Theorem S.6. [Thm. 1 in the main paper] The distribution shift correction is identifiable if there exists a known set of variables Z such that the following three conditions are all concurrently satisfied: 1) Z d-separates X and EB (sample selection bias) and also d-separates X and EP (population characteristic difference);\n2) The conditional probability P(X|Z), after appropriate transformations on X, is the same across multiple participating sites (S and T ); 3) The distribution of Z has a non-trivial overlap across multiple sites (S and T ), which means that there exists an interval [a, b] such that P(a \u2264 Z \u2264 b) \u2265 0.5 for all sites.\nProof. We denote measurements of interest as XS in dataset 1 and XT in dataset 2. We assume EB = 1, EP = 1 for dataset 1 and EB = 2, EP = 2 for dataset 2 to represent the biases between the two datasets. Without loss of generality, we assume that a transformation h \u03bb 0 (.) can resolve the distributional shift, that is, marginal distributions of h \u03bb 0 (XS) and XT are the same. However, when biases exist, we have, P(h \u03bb 0 (XS)|EB = 1, EP = 1) = P (XT |EB = 2, EP = 2).\n[S.6] Therefore, we may not be able to find the correct transformation by matching the distributions (1). Then, the correction problem becomes non-identifiable if we do not have any additional information. The situation changes when we have a set of variables Z which satisfy the three identification conditions in Thm. S.6 (Thm. 1 in the main body). The following explanation describes why.\nThe second equation holds because of condition 1) in Thm. S.6. Similarly,we have,\nThen, since condition 2) holds, we have P(h \u03bb 0 (XS)|Z) = P(XT |Z), that is, they are identical functions of Z. Therefore, the only difference between the two datasets are P(Z|EB = 1, EP = 1) and P(Z|EB = 2, EP = 2). We should keep in mind that this difference is similar to [S.6]. However, there is no longer an unknown transformation h \u03bb 0 (.) in the relation. To address this issue, we can conduct a subsampling procedure SSP on Z to approximately align P(Z|EB = 1, EP = 1) with P(Z|EB = 2, EP = 2). The condition 3) in Thm. S.6 shows that this is possible. After the subsampling procedure, we will approximately have, P(h \u03bb 0 (XS)|EB = 1, EP = 1, SSP) = P(XT |EB = 2, EP = 2, SSP).\n[S.10] Therefore, the correction problem becomes identifiable since we can now learn h \u03bb 0 by matching the distributions."}, {"section_title": "S.7. Proof of extended IJ estimators", "text": "We provide a proof of our extension of the infinitesimal Jackknife estimator (6) to multiple groups. The proofs proceed in two steps."}, {"section_title": "Proof. Step 1.", "text": "Before describing the proof, we define some notations. We have two datasets XS, XT and d groups for each dataset. Similar to the discussion in the main paper, we define our samples from one dataset as\nand samples from the other dataset as\n).\nBased on X = (XS, XT ), we can generate bootstrap samples for every group and we call all such generated samples as V = (VS, VT ). In this proof (similar to (6)), we assume that the bootstrap sample sizes are the same as the original samples, that is,\n).\nBased on V = (VS, VT ), we define the count variables N (V ) = (N (VS), N (VT )) and probability variables P(V ) = (P(VS), P(VT )), which are,\nIn other words, N (Vu) (i,j) records how many times X (i,j) u appears in the bootstrap samples for one iteration. The probability variable P(Vu) (i,j) is the normalized N (Vu) (i,j) such that n i u j=1 P(Vu) (i,j) = 1. The bootstrap process is repeated B times. Later, when we have superscript b for V , N (V ) and P(V ), it implies that those variables are related to the b th iteration of the bootstrap process. Now, we define another term to be the baseline for the probability variables, which is, P(X) = (P(XS), P(XT )), where P(Xu) (i,j) = 1 n i u , for any u \u2208 {S, T }. i \u2208 {1, ..., d}. j \u2208 {1, ..., n i u }.\nWe can see that P(X) represents the uniform distribution for each group of X, which only depends on the original samples X.\nOn the other hand, P(V b ) depends on X and the bootstrap samples V b for the b th iteration.\nThe probability variables P(V b ) and the baseline probability variables P(X) are connected by multinomial distributions as\nwhere P(V b u ) (i) is related to the i th group from dataset V b u and P(Xu) (i) is related to the i th group from dataset Xu. In M ulti n i u (n i u , P(Xu) (i) ), n i u is the total number of variables in the i th group whereas P(Xu) (i) is the uniform distribution for this group. This means that n i u \u00d7 P(V b u ) (i) can be viewed as n i u samples generated from the multinomial distribution on n i u discrete values with equal probability to be drawn.\nLet us consider an estimation function f . For the b th iteration in the bootstrap process, we obtain an estimatorf b from samples V b . We assume that, given X,f b only depends on P(V b ), which means that we can representf b by f (P(V b )). Therefore, we can approximatef b via the tangent hyperplane that goes through f (P(X)) at P(X), which gives us that\nwhere ej is a vector with all zeros except one on the j th position. Because our bootstrap samples are drawn independently from every group, we have that\nHao Henry Zhou, Vikas Singh, Sterling C. Johnson, Grace Wahba, and the Alzheimer's Disease Neuroimaging Initiative 10.1073/pnas.XXXXXXXXXX Further, using the multinomial distribution relation between P(V b ) and P(X) in [S.11], we get\nwhere (\u00b7) t represents the transpose operation on a vector or matrix, and 1 is the all one vector. Further, since n i u j=1 (ej \u2212 P(Xu) (i) ) = 0, we have n i u j=1 U (i,j) u = 0 from its definition in [S.13]. Therefore, we get an approximation for the mean squared error (MSE) off b for estimation f (P(X)), that is\n)] = f (P(X)), then this MSE is the bootstrap estimation for the variance of f (P(X)) (6).\nStep 2. Now, we consider the estimation regarding the variance of\u03bb :\nIn this proof, we assume that B is large enough that it exactly covers all finite possibilities for the bootstrap samples. Therefore, we can consider\u03bb as the value of a function f (\u00b7) at P(X). In other words, we consider f (P(X)) in Step 1 to be\u03bb . Let us imagine that we generate bootstrap samples to estimate the variance of\u03bb as in Step 1, which is a second layer of bootstrap since\u03bb is already based on bootstrap samples. Again, we can use the tangent hyperplane scheme in Step 1 to approximate the bootstrap estimation of the variance. For this estimator, the relation E[f (P(V b ))] = f (P(X)) =\u03bb holds because we assume that B covers all the possibilities. Therefore, the only remaining issue is to calculate U (i,j) u . We define a probability measure on X which is\n. The probability P(V b |\u00b5) is defined to be the probability of V b under the probability measure \u00b5. We can further simplify w b (P(Vu) (i,j) ) by\nLet \u2192 0, we have that\nTherefore, using [S.13], we know that\nBy plugging in that form in equation [S.16], we get the approximation for the bootstrap estimation for the variance of\u03bb, which is \nis an approximation of COV(\u03bb, N (Vu) (i,j) ). For the subsampling case, the expectation of\nAs a result, we adjust the covariance term to COV(\u03bb, N (Vu) (i,j) ) = 1\nas in (7) . This leads to the Thm. 4 in the main paper."}, {"section_title": "S.8. Simulations", "text": "Our the first set of experiments were designed to check the efficacy of the hypothesis test whereas the second experiment evaluated the estimation consistency.\nSimulation for the Hypothesis Test. We generated samples from the standard normal distribution N (0, 1) to synthesize the first dataset XS and use the normal distribution N (10, 2), with mean 10 and standard deviation 2, to synthesize another dataset XT . Notice that, under the correct transformation class h (a,b) (XS) = aXS + b, we can correct the distribution shift and we should accept H0. We consider two types of variations that can potentially affect the correction and check whether the hypothesis test indeed rejects H0 with high power. 1) In Fig. S2(a) , we always choose the transformation class h (a,b) (XS) = aXS + b and one dataset XT comprised of samples from N (10, 2). But we vary the generating distribution for the other dataset choosing between N (0, 1), Laplace(0, 1) and Exponential (1) . We also vary the sample sizes. The hypothesis test with significance level 0.05 is performed on 100 repetitions and the acceptance rate curves are plotted. From Fig. S2(a) we see that our hypothesis test does accept H0 at a high rate when it is true (red curve) and tends to reject H0 with an increase in power as the sample size increases whenever the generating distributions are not Normal (blue and black curves). 2) In Fig. S2(b) , we always choose one dataset XT composed of samples from N (10, 2) and the generating distribution for the other dataset XS is set to be N (0, 1). Then, we vary the transformation class h (a,b,c) (XS) choosing between aX 2 S + bXS + c and a log(|x|) + b. Here, note that aX 2 S + bXS + c includes the true transformation whereas a log(|x|) + b corresponds to an incorrect transformation class. Again, we vary the sample sizes and repeat the hypothesis test 100 times and plot the acceptance curves in Fig. S2(b) . Similar to the first setting, we observe that our hypothesis test accepts H0 at a high rate when it is true (red curve) and tends to reject H0 with high power as the sample size increases in the setting where the transformation class is wrong (blue curve).\nSimulation for Estimation Consistency. In this simulation, we assume that the distributional shift is the only variation across the datasets, XS and XT . We check the estimation consistency of the transformation h\u0174 (\u00b7) which minimizes the MMD distance between h\u0174 (XS) and XT . In Fig. S2(c) , we perform the experiments for two models. For model 1, the generating distributions for two datasets are, In other words, we generate samples for XS and X raw T , and we transform X raw T to get XT . The transformation class we consider is XT = a11 a12 a13 a21 a22 a23 XS 1\nWe define the quadratic mean of the estimation error for \"Model 1, first row\" in Fig. S2 (c) as (a11 \u2212 1) 2 + (a12 \u2212 2) 2 + (a13 \u2212 10) 2 3\nSimilar, the quadratic mean of the estimation error is defined for the second row (a21, a22, a23). The plot shows us that the estimation error is small and decreases as the sample size increases. We also check this behavior under different types of generating distributions, including We call this model 2 and observe a similar trend as model 1 as shown in Fig. S2(c) ."}]