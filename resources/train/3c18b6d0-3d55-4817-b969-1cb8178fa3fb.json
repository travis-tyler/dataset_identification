[{"section_title": "Abstract", "text": "ABSTRACT Neuroimaging techniques have been used for automatic diagnosis and classification of Alzheimer's disease and mild cognitive impairment. How to select discriminant features from these data is the key that will affect the subsequent automatic diagnosis and classification performance. However, in the previous manifold regularized sparse regression models, the local neighborhood structure was constructed directly in the traditional Euclidean distance without fully utilizing the label information of the subjects, which leads to the selection of less discriminative features. In this paper, we propose a novel manifold regularized sparse regression model for learning discriminative features. Specifically, we first adopt 2,1 -norm regularization to jointly select a relevant feature subset among the samples. Then, to select more discriminative features, a novel manifold regularization term is constructed via the relative distance adjusted by the label information, which can simultaneously maintain the compactness of the intra-class samples and the separability of inter-class samples. The proposed feature learning method is further carried out for both the binary classification and the multi-class classification. The experimental results on Alzheimer's Disease Neuroimaging Initiative database demonstrate the effectiveness of the proposed method, which can be utilized for the diagnosis of Alzheimer's disease and mild cognitive impairment.\nINDEX TERMS Alzheimer's disease, feature learning, sparse regression, manifold regularization."}, {"section_title": "I. INTRODUCTION", "text": "Alzheimer's disease (AD) is a degenerative, irreversible, incurable dementia and will eventually cause the patients to lose basic living ability. It is estimated that 1 in every 85 persons will be affected by AD by year 2050 [1] . AD patients have been not only a huge economic burden to the society but also a great trouble to the patients as well as their families. Mild cognitive impairment (MCI) is known as a prodromal stage of AD. Previous studies have shown that the normal controls (NC) subjects progress to AD patients at a rate of approximately 1% to 2% per year [2] , whereas\nThe associate editor coordinating the review of this manuscript and approving it for publication was Liangtian Wan. MCI subjects convert to AD patients at an annualized rate around 10% to 15% [3] , [4] . The stage of MCI is a golden period to effectively curb the conversion of MCI subjects into AD patients [5] , [6] , so the intervention and treatment for MCI subjects can possibly alleviate their pain. Therefore, it is essential to correctly identify AD patients and mine discriminant features for early diagnosis and treatment of AD patients and MCI subjects.\nRecently, biomedical signal processing techniques have been widely applied for AD/MCI studies, such as structural Magnetic Resonance Imaging (MRI), positron emission tomography (PET) and functional magnetic resonance imaging (fMRI) [7] - [9] ; meanwhile, machine learning methods have presented promising performances in various applications [10] - [14] , including biomedical applications. Specifically, structural MRI data provide information about the tissue type of the brain, and could serve as a powerful tool for the analysis of AD patients and MCI subjects due to its clear contrast and high spatial resolution [15] , [16] . Many valuable structural features extracted from the structural MRI data have been identified for AD patients and MCI subjects, such as tissue probability maps, cortical thickness and hippocampal volumes [17] , [18] . Additionally, network techniques is applied in many fields and a series of research has been studied [19] - [25] . The brain is a complex network system where the interrelationships exist between different brain region, rather than in one single brain region. Therefore, based on the network techniques, using the structural features(e.g. cortical thickness(CT) and local gyrification index (LGI)), the personal morphological brain network can be further constructed and the characteristics of personal network is used for AD/MCI classification. In [15] and [26] , personal brain network was constructed using cortical thickness for attaining good classification performance. In [27] , personal network was constructed using multiple structural features to improve the accuracy of classifying AD patients and MCI subjects.\nIn the past decades, many promising performances have been already achieved in AD applications. However, for biomedical signal processing data, even after conducting feature extraction, there is still redundant and irrelevant features, which may lead to poor performance of subsequent classification. It is necessary to conduct feature selection which can remove less discriminant features to obtain an effective feature subset. Feature selection methods can mainly be divided into three categories [28] : the filter models [29] , the wrapper models [30] and the Embedded models [31] . The filter(e.g. Minimum Redundancy Maximum Relevance (mRMR) [32] ) and the wrapper (e.g. recursive feature elimination algorithm (RFE) [33] ) models have been widely applied in AD studies. In [13] and [17] , a two-step feature selection method was adopted, including the mRMR filter method and the RFE wrapper method to find an optimal feature subset and gain higher classification performance with the SVM classifier. Yao et al. [34] used two filter methods and a RFE wrapper method to select features for AD detection using FDG-PET data. The embedded methods (e.g., the method LARS [35] , LASSO [31] , Elastic Net [36] ) could obtain superior performance over the two selection models mentioned above, and have been successfully used for various application researches including AD studies [37] - [42] . For instance, Zhang et al. [38] used Sparse Multi-Task Learning model for feature selection. Zhu et al. [44] proposed a Sparse Multi-Task Learning with Subspace Regularization to select features, where subspace regularization was constructed in an unsupervised manner. Jie et al. [45] developed a manifold regularized multitask feature learning for multimodality disease classification method. Zu et al. [46] presented a Label-aligned manifold regularization for multitask feature selection method. Ye et al. [47] introduced a new discriminative regularization term based on intra-class and inter-class Laplacian matrices. Generally, in most recent studies, the local neighborhood relation of the manifold regularization is constructed directly among samples via the traditional Euclidean distance, without considering the label information of all subjects which may lead to selecting redundant features subset. Importantly, the label information could improve the quality of selected neighborhood and may further improve the subsequent classification performance. Therefore, in this paper, based on the label information, we propose a novel manifold regularized sparse feature selection method to select more discriminative features for AD/MCI classification. Specifically, we first introduce a novel label information-based manifold regularization term into sparse regress model, which could better preserve the local structural information and obtain the optimal feature subset. We then adopt the support vector machine (SVM) [48] with radial basis function (RBF) kernel to evaluate the performance of the proposed method. Experimental results show that our method is more effective than several others methods. The main contributions of ourwork can be summarized as follows:\n1) We propose a novel manifold regularized sparse feature learning method for MCI/AD Classification based on structural magnetic resonance signal processing data.\n2) The selected brain regions by our method can be utilized for the diagnosis of Alzheimer's disease and mild cognitive impairment.\nThe rest of our paper is organized as follows: Materials and image preprocessing are introduced in Section II. Section III gives the details of the proposed method. The experimental results are in Section IV and discussion is in Section V. Sections VI and VII finally gives the limitations and concludes the paper, respectively. "}, {"section_title": "II. MATERIALS AND IMAGE PREPROCESSING", "text": ""}, {"section_title": "B. IMAGE PREPROCESSING AND FEATURE EXTRACTION", "text": "Biomedical Signal image pre-processing and feature extraction are performed by the following procedures. First, T1-weighted images were preprocessed using FreeSurfer software (http://surfer.nmr.mgh.harvard.edu). This process involves the following steps: motion correction, non-brain tissue removal, coordinate transformation, grey matter segmentation, and reconstruction of gray/white matter boundaries. In particular, the reconstruction and segmentation errors were visually checked in FreeView and manually corrected. Next, surface inflation and registration were performed [49] - [51] . The cortical thickness was calculated in each vertex based on the distances between the white matter and pial surface. Finally, we obtained the mean cortical thickness of each brain region according to the Automated Anatomical Labeling (AAL) atlas and removed the subcortical tissues [52] , resulting in 78 cortical regions (78 ROIs)."}, {"section_title": "III. THE PROPOSED METHOD", "text": "In this section, we will propose our method, which is called manifold regularized sparse feature learning with label information. We first briefly introduce the feature selection with sparse regression model, and then present the details of our method, as well as the corresponding iterative optimization algorithm. Finally, we utilize SVM classifier for the three binary classification tasks and one multi-class classification task. An overview of the proposed classification framework is illustrated in Figure1. "}, {"section_title": "A. NOTATIONS", "text": "We begin with a brief description of some notations used in this paper. For a matrix X = [x i,j ] \u2208 R n\u00d7d , its transpose, inverse, and trace operator are denoted by X T , X \u22121 and tr(X ), respectively. Its i-th row and i-th column are denoted by x i and x T i , respectively. The 2,1 -norm [53] of a matrix W is the sum of 2-norm of the rows of W :"}, {"section_title": "B. FEATURE LEARNING WITH SPARSE REGRESSION MODEL", "text": "Let X \u2208 R n\u00d7d denote the feature matrix, where n is the number of training samples, d is the dimension of features. Y \u2208 R n\u00d7c denotes the class label matrix with binary encoding, where c is the number of classes. Consequently, the objective function of sparse regression model with a group lasso [31] , [44] is defined as follows:\nwhere W \u2208 R d\u00d7c is the regression coefficient matrix, and \u03bb is a weighting parameter which not only balances the relative importance between the loss term and the regularization term, but also controls the sparsity of elements in W matrix. The 2,1 -norm encourages the sparsity of rows in W , and then common features will be jointly selected among samples corresponding to multi-class label matrix. When \u03bb = 0, all features are selected, as \u03bb increases, the number of selected features decreases. In other words, a larger \u03bb value means that there are more zero rows in W matrix and fewer features are selected. However, this model only selects common feature subset without considering the local geometric space structure among samples, and the selected features might be less discriminative."}, {"section_title": "C. MANIFOLD REGULARIZED SPARSE FEATURE LEARNING WITH LABEL INFORMATION", "text": "In this section, a joint framework of the sparse regression model is proposed for effectively selecting discriminative features. First, we introduce a graph regularization term (Locality Preserving Projections (LPP)) as follows:\nwhere L = D\u2212S is a positive semi-definite symmetric matrix and S = [s i,j ] \u2208 R n\u00d7n is the similarity matrix with elements s i,j which represent the similar relationship between each pair of samples x i and\n(2) is defined as:\nwhere N k (x i ) denotes the k-neighbors of subject x i . How to choose the optimal k-neighbors among the samples of different classes is the key issue in constructing the neighbor relationship. Unlike the traditional methods in [44] , [46] , and [47] , s i,j is directly constructed based on the Euclidean distance, where the label information of all the subjects is not fully utilized. To solve this problem, we first VOLUME 7, 2019 introduced an adjustable parameter with a label information to control the absolute Euclidean distance between samples, and then the relative distance can be defined as follows:\nif x i and x j belong to the same class,\nFor each subject x i , its k-neighbors N k (x i ) are calculated via the relative distance rd ij which is controlled by the adjustable parameter \u03b4. Notably, if two samples x i and x j belong to the same class, the relative distance rd ij between them is reduced; contrarily, if two samples x i and x j belong to different classes, the relative distance rd ij between them is extended. Therefore, intra-class samples are closer, while inter-class samples are far away. As a result, the optimal neighbors with the most similar features are easily selected via the relative distance. By constructing the k-neighbors graph, the regularization term in Eq. (2) could subtly preserve the relationship among the most similar neighbors (k \u2208 [5, 10] ). Interestingly, when parameter \u03b4 is equal to 1, the novel subspace regularization term in Eq. (2) will degenerate into the ordinary subspace regularization constructed in an unsupervised manner. This indicates that the neighbor relationship could be conducted by our proposed method in both unsupervised and supervised manner. Substituting Eq. (2) into Eq. (1), we can obtain our proposed model as follows:\nwhere \u03bb 1 and \u03bb 2 are two balancing parameters. Our proposed method could preserve the relationship among the optimal neighbors and select more discriminative features to achieve superior performance in subsequent classification tasks. More importantly, feature selection is still guaranteed to perform in the original space which is easy to interpret or investigate the selected features."}, {"section_title": "D. OPTIMIZATION ALGORITHM", "text": "We use an iterative algorithm to solve the optimization problem in Eq. (5). Specifically, we first separate our objective function into the smooth part and non-smooth part. The smooth part is shown as follows:\nWhile non-smooth part is shown as follows:\nAccording to [54] , we can define \u03c6(x) = \u221a x 2 + \u03b5, \u03b5 is a smoothing term, which is usually set to a small value, so that the next iterative procedures can be guaranteed to converge. [55] , [56] . Therefore, we can replace W 2,1 as:\nwhere R = Diag(r), r is an auxiliary vector of the 2,1 -norm. r i can be computed as follows:\nFrom the above formula, our objective functions can be approximately expressed as follows:\nTaking derivative of Eq. (10) with respect to W and set it to zero, we can get:\nWe can then get the solution for W as:\nThe main optimization procedure of our proposed method is summarized in Algorithm1. kernel [48] for classification. We build three binary classifiers, i.e., AD vs. NC, MCI vs. NC and cMCI vs. sMCI respectively, and one multi-class classifier i.e., AD vs. MCI vs. NC. Here, we chose to use one-versus-one approach for multi-class classification. To evaluate the performance of our method, we apply a 10-fold cross-validation method, where all samples are randomly divided into 10 parts, and each part is left out in turn as test set, while the rest are used for training sets that undergo the nested feature selection, the optimal parameter values (i.e., \u03bb 1 , \u03bb 2 , \u03b4 and k) as mentioned above. We repeat this process 10 times independently to avoid the possible bias caused by randomly partitioning the dataset and the average results are reported. For binary classification tasks, we adopt four measures i.e. accuracy (ACC), sensitivity (SEN), specificity (SPE) and Area Under Curve (AUC) to quantify the performance of different methods, whereas only accuracy is calculated to evaluate the multiclass classification performance. In addition, for binary classification, we applied a grid search algorithm and 3-fold cross-validation to select the optimal parameter C of SVM (range from 2 \u22128 to 2 8 ) on the training data. For multi-class classification, to short computing time, the parameter C is set to the default value on the training data."}, {"section_title": "IV. EXPERIMENTAL RESULTS", "text": ""}, {"section_title": "A. EXPERIMENTAL SETTINGS", "text": "We test the performance of our proposed method using cortical thickness of MRI data obtained from ADNI database. For each ROI, we compute the mean cortical thickness of the brain region, and thus each subject contains 78 features.\nWe compare our method with several state-of-the-art feature selection methods including mRMR and RFE. mRMR is a filter feature selection method that maximizes the correlation between each feature and class label variables, while minimizes the redundancy between each feature pair simultaneously [32] . RFE is a wrapper feature selection method, which removes the minimum discriminant features from the feature set to find an optimal feature subset [33] . To reveal the validity of feature selection, we also conduct the classification task using all features without feature selection (denoted as 'Raw'). Since our method combines the novel manifold regularization method and a sparse regression model with an 2,1 -norm method in a unified framework. When \u03bb 1 = 0, our proposed model will be sparse regression model with a group lasso (denoted as SR) without the manifold regularization term. When \u03bb 2 = 0, all features without feature selection are used for classification(denoted as 'Raw'). It's necessary that we should compare our method with SR method to justify the rationale of our method where adds the novel manifold regularization into SR method. In addition, to further verify the validity of relative distance term adjusted by the label information, we also make comparison with the relative distance in Eq. (5) with 0 < \u03b4 \u2264 1 (our method) and the traditional Euclidean distance in Eq. (5) with \u03b4 = 1 (denoted as ED). For fairness comparisons, all methods are conducted on the same training and test samples."}, {"section_title": "B. CLASSIFICATION PERFORMANCE", "text": "The comparison performance of three binary classification tasks are presented in Table2. We can see that our method achieves better performance than the other methods. Specifically, for AD vs. NC classification (\u03bb 1 = 1, \u03bb 2 = 8, k = 9, \u03b4 = 1), our method improves the classification accuracy by 1.32% (mRMR), and 1.13% (RFE), respectively. For MCI vs. NC classification (\u03bb 1 = 0.01, \u03bb 2 = 2, k = 8, \u03b4 = 0.9), our method improves the classification accuracy by 0.63% (mRMR)and 0.45% (RFE), respectively. For cMCI vs. sMCI classification (\u03bb 1 = 0.01, \u03bb 2 = 5, k = 7, \u03b4 = 0.1), our proposed method achieves a classification accuracy of 63.65%, which is 2.11% and 1.51% higher than that of mRMR and RFE method, respectively. We also can see that the classification accuracy of our method is slightly higher than SR, and ED (only except for AD vs. NC classification, our method has the same classification performance as the ED method.). For further validation, we perform the significance test by using the standard paired t-test on the classification accuracy between SR, ED, our method and other methods. The p-values are shown in Table3. The smaller the p-value is, the more significant the difference is. If p-value is less than 0.05, it indicates a significant difference between two sets of data. We can see that our method achieves smaller p-values than SR and ED methods. For AD vs. NC classification, our proposed method is significantly better than all the other method. For cMCI vs. sMCI classification, our method is significantly better than mRMR. Only, for MCI vs. NC classification, there are no significant differences in classification performance between our method and the other methods, so do SR and ED. It is worth mentioning that, for MCI classification, we can see that there are no significant differences in performance between SR, ED and the other methods(except Raw), however, there are significant differences in performance between our method and the other methods(except RFE).\nWe also demonstrate the Receiver Operating Characteristic (ROC) curves for different methods in In addition, the classification accuracy for 3-class i.e., AD vs. NC vs. MCI ((\u03bb 1 = 0.1, \u03bb 2 = 3, k = 9, \u03b4 = 0.4)) is presented in Table 4 . Our proposed method achieves a classification accuracy of 59.16 %, while the other methods are 57.43%,57.88%,57.91%,58.73 and 58.97% respectively. The significance test are shown in Table5. The p-values further confirmed the effectiveness of our method over mRMR and RFE. We also can see that our method achieves smaller p-values than SR and ED. There are significant differences in performance between our method, ED and the other methods, however, there are no significant differences in performance between SR and RFE. We investigated the top 10 selected brain regions by our method for MCI classification and 3-calss classification. Specifically, the most frequently selected brain regions in each cross-validation were defined as the top 10 ROIs. The results are shown in Table6 and Table7. We also visualized the top 10 ROIs in Figure3 and in Figure4. For MCI classification(Table6 and Figure3), the most discriminative regions are Angular gyrus, Cingulate, Parahippocampal gyrus, etc., which have been reported in AD/MCI studies [44] , [58] - [62] and also shown to be highly related to AD/MCI diagnosis [44] , [59] , [63] , [64] . Also, we can see from Table7 and Figure4, for 3-class, the top five-ranked discriminative regions are known to be highly related to AD/MCI which have been reported in the previous studies [46] , [44] , [58] , [59] , [62] ."}, {"section_title": "V. DISCUSSION A. PERFORMANCE COMPARISON", "text": "The classification results listed in Table2 and Table4 demonstrate that our method generally outperforms the other feature section methods. Specifically, SR also obtains higher accuracies for all classification tasks, compared with mRMR and RFE. This indicates that the embedding feature section method is more effective than the filter and wrapper method in AD/MCI data. Notably, by introducing the manifold regularization into the sparse regression model, our proposed method achieves better classification performance than SR. Furthermore, from Table 3 and Table 5 , We can see that our method both achieves smaller p-values than SR and ED methods.And there are significant differences in performance between our method and the other methods in most case. This indicates that combining the manifold regularization and SR in a unified framework can help enhance the classification performance, and relative distance term in Eq. (5) adjusted by the label information of our method is also effective for improving classification performance. This verifies the conclusion that the manifold regularization with label information could positively detect the most discriminative features and remove irrelevant features for improving the classification performance. Besides, all feature selection methods outperform the Raw, which implies the necessity of feature selection for AD/MCI classification."}, {"section_title": "B. EFFECT OF PARAMETERS", "text": "In our method, there are three main parameters including an adjustable parameter \u03b4 and two regularization parameters (i.e., \u03bb 1 and \u03bb 2 ). To evaluate their effects on the final prediction performances, we first study the parameter sensitivity of \u03bb 1 and \u03bb 2 by fixing \u03b4. Specifically, \u03bb 1 is tested from 10 \u22123 to 10 3 and \u03bb 1 is tested from 1 to 10. As shown in Figure5, we can see that \u03bb 1 and \u03bb 2 only have minor effects on the prediction accuracy of our method.\nWe also study different values of \u03b4 by fixing \u03bb 1 and \u03bb 2 , \u03b4 is tested from 0 to 1. As shown in Figure 6 , the classification performance varies with the parameter \u03b4, which means that parameter \u03b4 is valid in our method and the selection of parameter \u03b4 is very important for final classification performance. Specifically, when \u03b4 = 1, for AD vs. NC, our method achieves the best classification accuracy of 90.40% which indicates that it is relatively easy to distinguish AD from NC, and thus there is not need to adjust the samples distance. For MCI vs. NC, when \u03b4 = 0.9, our method achieves the best classification accuracy of 70.89% and 62.23%, respectively, which denotes that it is not very easy to distinguish MCI and NC, so there is need to slightly adjust the samples distance. For AD vs. NC vs. MCI, the best classification accuracy of 59.16% is achieved when \u03b4 = 0.4. Since it is relatively difficult to distinguish MCI, NC and AD, there is need to relatively largely adjust the samples distance. However, for cMCI vs. sMCI, the best classification accuracy of 63.69% is achieved when \u03b4 = 0.1, which denotes that it is difficult to distinguish cMCI from sMCI, and it's necessary that the distance of two samples within the same class is reduced on a larger scale and vice versa. This verified the conclusion that, for different classification tasks, the optimal neighbors are effectively selected via calculating the relative distance adjusted by label information, which contribute to the selection of more discriminative features for further improving the subsequent classification."}, {"section_title": "VI. LIMITATIONS", "text": "There are still some limitations in our study that should be considered in future studies. First, the optimal parameters are data dependent. How to obtain the optimal parameter automatically is still an open problem. In the future, the method of parameter optimization should be exploited to set the optimal parameter. Second, we only use cortical thickness for AD/MCI classification. However, there exist other structural features (e.g., volume and area) may also contain commentary information that can help to improve the classification performance. Finally, the personal network should be further constructed, and the characteristics of network may describe structural changes in a more accurate way."}, {"section_title": "VII. CONCLUSION", "text": "In this paper, we propose a novel manifold regularized sparse feature learning method for MCI/AD Classification based on structural magnetic resonance imaging. We first used a 2,1 -norm regularization to jointly select common features among the samples of different classes. We then constructed a new manifold regularization with class label information to preserve the relationship among the optimal neighbors. Experimental results demonstrate that our proposed method can achieve preferable classification performance compared with several state-of-the-art methods for AD/MCI classification. "}]