[{"section_title": "Abstract", "text": "At this moment, databanks worldwide contain brain images of previously unimaginable numbers. Combined with developments in data science, these massive data provide the potential to better understand the genetic underpinnings of brain diseases. However, different datasets, which are stored at different institutions, cannot always be shared directly due to privacy and legal concerns, thus limiting the full exploitation of big data in the study of brain disorders. Here we propose a federated learning framework for securely accessing and meta-analyzing any biomedical data without sharing individual information. We illustrate our framework by investigating brain structural relationships across diseases and clinical cohorts. The framework is first tested on synthetic data and then applied to multi-centric, multi-database studies including ADNI, PPMI, MIRIAD and UK Biobank, showing the potential of the approach for further applications in distributed analysis of multi-centric cohorts."}, {"section_title": "INTRODUCTION", "text": "Nowadays, a large amount of magnetic resonance images (MRI) scans are stored across a vast number of clinical centers and institutions. Researchers are currently analyzing these large datasets to understand the underpinnings of brain diseases. However, due to privacy concerns and legal complexities, data hosted in different centers cannot always be directly shared. In practice, data sharing is also hampered by the need to transfer large volumes of biomedical data with the associated bureaucratic burden. This situation led researchers to look for an analysis solution within meta-analysis or federated learning paradigms. In the federated setting, a model is fitted without sharing individual information across centres, but only model parameters. Meta-analysis instead performs statistical testing by combining results from several independent assays [1] , for example by sharing p-values, effect sizes, and/or standard errors across centers.\nOne example of such a research approach is the Enhancing NeuroImaging Genetics through Meta-Analysis (ENIGMA) consortium (enigma.usc.edu). With a large number of institutions worldwide [2] , ENIGMA has become one of the largest networks bringing together multiple groups analyzing neuroimaging data from over 10,000 subjects. However, most of ENIGMA's secure meta-analytic studies in neuroimaging are performed using mass-univariate models.\nThe main drawback of mass-univariate analysis is that they can only model a single dependent variable at a time. This is a limiting assumption in most of the biomedical scenarios (e.g., neighboring voxels or genetic variations are highly correlated). To overcome this problem, multivariate analysis methods have been proposed to better account for covariance in high-dimensional data.\nIn a federated analysis context, a few works proposed generalization of standard neuroimaging multivariate analysis methods, such as Independent Component Analysis [3] , sparse regression, and parametric statistical testing [4, 5] . Since these methods are mostly based on stochastic gradient descent, a large-number of communications across centers may be required to reach convergence. Therefore, there is a risk of computational and practical bottlenecks when applied to multi-centric high-dimensional data.\nLorenzi et al. [6, 7] proposed a multivariate dimensionality reduction approach based on eigen-value decomposition. This approach does not require iteration over centers, and was demonstrated on the analysis of the joint variability in imaging-genetics data. However, this framework is still of limited practical utility in real applications, as data harmonization (e.g., standardization and covariate adjustment) should be also consistently performed in a federated way.\nHerein we contribute to the state-of-the-art in federated analysis of neuroimaging data by proposing an end-to-end framework for data standardization, confounding factors correction, and multivariate analysis of variability of highdimensional features. To avoid the potential bottlenecks of gradient-based optimization, the framework is based on schemes analysis through Alternating Direction Method of Multipliers (ADMM) reducing the amount of iterations.\nWe illustrate the framework leveraging on the ENIMGA Shape tool, to provide a first application of federated analysis compatible with the standard ENIGMA pipelines. It should be noted that, even though this work is here illustrated for the analysis of subcortical brain changes in neurological diseases, it can be extended to general multimodal multivariate analysis, such as to imaging-genetics studies.\nThe framework is benchmarked on synthetic data (section 3.1). It is then applied to the analysis of subcortical thickness and shape features across diseases from multicentric, multi-database data including: Alzheimer's disease (AD), progressive and non-progressive mild cognitive impairment (MCIc, MCInc), Parkinson's disease (PD) and healthy individuals (HC) (section 3.2)."}, {"section_title": "METHODS", "text": "Biomedical data is assumed to be partitioned across different centers restricting the access to individual information. However, centers can individually share model parameters and run pipelines for feature extraction.\nWe denote the global data (e.g., image arrays) and covariates (e.g., age, sex information) as respectively X and Y, obtained by concatenating respectively data and covariates of each center. Although these data matrices cannot be computed in practice, this notation will be used to illustrate the proposed methodology. In the global setting, variability analysis can be performed by analyzing the global data covariance matrix S.\nFor each center c \u2208 {1, . . . , C} with N c subjects each, we denote by X c = (x i ) Nc i=1 and Y c = (y i ) Nc i=1 the local data and covariates. The feature-wise mean and standard deviation vectors of each center are denoted asx c and \u03c3 c .\nThe proposed framework is illustrated in Figure 1 and discussed in section 2.1. It is based on three main steps: 1) data standardization, 2) correction from confounding factors and 3) variability analysis.\nData standardization is a data pre-processing step, aiming to enhance the stability of the analysis and easing the comparison across features. In practice, each feature is mapped to the same space by centering data feature-wise to zero-mean and by scaling to unit standard deviation. However, this is ideally performed with respect to the statistics from the whole study (global statistics). This issue is addressed by proposing a distributed standardization method in section 2.1.1. Confounding factors have a biasing effect on the data. To correct for this bias, it is usually assumed a linear effect of the confounders X = YW, that must be estimated and removed. However, for a distributed scenario, computing W is not straightforward, since the global data matrix cannot be computed. We propose in section 2.1.2 to use Alternating Direction Method of Multipliers (ADMM) to estimate a matrix W shared among centers, closely approximating W. In particular, we show that W can be estimated in a federated way, without sharing local data X c nor covariates Y c .\nFinally, through federated principal component analysis (fPCA), we obtain a low dimensional representation of the full data without ever sharing any center's individual information X c , Y c (section 2.1.3)."}, {"section_title": "Federated Analysis Framework", "text": ""}, {"section_title": "Standardization", "text": "The mean and standard deviation vectors can be initialized t\u014d x 0 = 0 and\u03c3 0 = 0. They can be iteratively updated with the information of each center by following standard forms [8] , by simply transmitting the quantitiesx c and \u03c3 c from center to center. For each center the scaled data is denoted as X c and keeps the dimensions of X c ."}, {"section_title": "Correction from confounding factors", "text": "Under the assumption of a linear relationship between data and confounders, the parameters matrix W can be estimated via ordinary least squares, through the minimization of the\nIn a distributed setting, this approach can be performed locally in each center, ultimately leading to C independent solutions. However, this would introduce a bias in the correction, as covariates are accounted for differently across centers.\nTo solve this issue, we propose to constrain the local solutions to a global one shared across centers. In this way, the subsequent correction can be consistently performed with respect to the estimated global parameters. Thus, we can formulate the problem of constrained regression via ADMM [9] ."}, {"section_title": "For a given error function", "text": "associated with each center c and constrained to a estimated global matrix of weights W we can pose:\nAs this is a constrained minimization problem, the extended Lagrangian can be calculated as a combination of the parameters from each center (eqn. 1).\nWhere \u03c1 is a penalty factor (or dual update step length) regulating the minimization step length for W and W. \u03b1 is a dual variable to decouple the optimization of W and W.\nOptimization is performed as follows: i) Each center independently calculates the local parameters W c and \u03b1 c (eqn. 2 and 3); ii) the parameters W c and \u03b1 c are shared to estimate the global parameters W (eqn. 4). We note that this last step is performed without sharing either local data or covariates. The parameters W are subsequently re-transmitted to the centers and the whole procedure is iterated until convergence:\nAfter convergence, W is shared across centers, and used to consistently account for covariates by subtracting their effect from the structural data to obtain the corrected observation matrix:\nPrincipal components analysis (PCA) is a standard approach for dimensionality reduction assuming that the largest amount of information is contained in the directions U (components) of greater variability. Data can be thus represented by projecting on the low-dimensional space spanned by the main components: E = EU.\nFrom the eigen-value decomposition of the global covariance matrix S = U\u03a3 2 U , the first m-eigen-modes U = (u j ) m j=1 provide a low-dimensional representation of the overall variation in E. In our federated setting, we note that S is the algebraic sum of the local covariance matrices S = EE = x , \u03c3 0 0 access to individual data [6] . However, sharing the localcovariance-matrices can still be prohibitive as the dimension is (N features \u00d7 N features ). For this reason, it was proposed to further reduce the dimensionality of the problem by sharing only the principal eigen-components associated with the local covariance matrices:\nFrom the practical point of view, computing the eigen-components can be efficiently performed by solving the eigen-problem associated with the matrix (X c X c ) 2 which is usually of much smaller dimension (N c \u00d7 N c ) [10] .\nIn what follows, the number of components shared across centers is automatically defined by fixing a threshold of 80% on the associated explained variability contained in \u03a3 c ."}, {"section_title": "EXPERIMENTS", "text": ""}, {"section_title": "Synthetic Data", "text": "We randomly generated Y and W matrices. Data matrix was subsequently computed as X = YW, and corrupted with Gaussian noise N (0, \u03c3), with \u03c3 set to 20% of Y . Then, X and Y were split in C centers of equal sample size. Our federated framework was then applied for each scenario across 200 folds, and convergence analyzed as shown in Figure 2 ."}, {"section_title": "Real Data: Neuroimaging", "text": "Data. T1-weighted MRI scans at baseline were analyzed from several research databases (table 1) . In total, we included data for 455 controls (HC), 181 with non-progressive MCI (MCInc), 208 progressive (MCIc), 234 Alzheimer's disease (AD), 232 with Parkinson's disease (PD).\nFeature extraction. ENIGMA Shape Analysis was applied to the MRI data of each center [11, 12] . In our analysis we extracted: a) radial distance (an approximate measure of thickness) and, b) the log of the Jacobian determinant (surface area dilation/contraction) for each vertex of the following subcortical regions: hippocampi, amygdalae, thalami, pallidum, caudate nuclei, putamen and accumbens nuclei. The overall data dimension is of 54,240 features. Table 1 : Data used in this study. Each study here represents an independent center. The centers are jointly analyzed through the federated analysis proposed in Section 2.1. (table 1) .\nResults. The projection in the latent space spanned by the federated principal components is shown in Figure 3 . To ease visualization, the projection for MCI converters and those who remained stable is shown in the bottom panel. Figure 4 shows the weight maps associated to the first principal component. We note that principal components 1 to 3 identify a variability from healthy to AD consistent across centers. Moreover, healthy ADNI participants are in between the AD subjects and the rest of the population. This result may denote some residual effect of Age on the resulting imaging features, even after correction. Interestingly, the issue of 'leaking' spurious variability of confounders after correction has been already reported in a number of multi-centric studies, and is matter of ongoing research [13, 14] . Finally we note that PD subjects are generally similar to the healthy individuals with respect to the modelled subcortical information."}, {"section_title": "CONCLUSIONS", "text": "In this work we proposed, tested, and validated a fully consistent framework for federated analysis of distributed biomedical data. Further developments of this study will extend the proposed analysis to large-scale imaging genetics data, such as in the context of the ENIGMA meta-study. "}, {"section_title": "ACKNOWLEDGMENTS", "text": ""}]