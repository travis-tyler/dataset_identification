[{"section_title": "DATA CONSIDERATIONS", "text": "For many surveys, errors can be introduced from multiple sources including respondents, interviewers, survey questions, and post-data collection processes. The goal in survey research is to limit these errors as much as possible through the use of well-designed instruments, interviewer training programs, and technology, as well as by following best practices for editing and coding data. When reviewing the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011) data, it may be helpful to keep in mind the following: \uf06e The computer-assisted parent interview (CAI) was developed using Blaise, a commercial, off-the-shelf (COTS) software package that is recognized globally as the industry leader in computer-assisted interviewing software. The CAI guides the interview, reducing the potential for error. However, interviewers bring a human component to the survey and even highly experienced and well-trained interviewers can make errors. Additionally, errors in the design or programming of the parent interview are other potential sources of nonsampling error."}, {"section_title": "\uf06e", "text": "The parent CAI was designed to attempt to anticipate most respondent answers and provided prompts for interviewers to probe respondents to ensure accurate responses when a response seemed contradictory to information provided earlier in the interview. Despite these best efforts to design a comprehensible instrument, respondents could still have provided inconsistent answers or may have misunderstood a question. As a result, anomalous findings that cannot always be explained may be present in the data.\nSeveral questions in the data collection instruments include an \"other (specify)\" field that allows for the recording of respondent answers that do not fit into one of the offered response categories. These \"other (specify)\" text responses were reviewed after data collection, and sometimes it was determined that the text response could be coded into one of the existing categories. In some instances, this upcoding of responses resulted in a case being eligible for certain questions that they were not actually asked during the interview, because the response option to which the text data were upcoded was not selected during the interview. This issue mainly affects data related to household composition and identification of household members' relationship to the study child. For example, in a few cases, fathers or male guardians were not correctly identified during the interview, but review of text responses showed there was a father/male guardian in the household. These cases would have been eligible for all the questions about fathers/male guardians had that male been correctly identified during the interview, but none of those questions were asked. In instances such as this, data for the case are coded -9 (not ascertained).\nBecause respondents are free to refuse to respond to specific questions or to end or break off the interview at any time, there may be missing data within the parent interviews. Missing data due to a refusal to respond to a given question or due to a xxv breakoff are given a value of -9. There were 221 break-offs in the fall kindergarten parent interview and 664 break-offs in the spring kindergarten parent interview.\nThe ECLS-K:2011 school administrator and teacher questionnaires were designed as paper questionnaires that would be processed using computer scanning to read and enter the responses into a database. Although scanning errors using this technology are possible, they are rare due to systematic quality checks performed before and during the scanning of completed questionnaires. Many unusual, inconsistent, or seemingly implausible responses from administrators and teachers were reviewed during the data cleaning process and were confirmed as having been scanned accurately. When preparing the kindergarten data for release, some data anomalies and errata were identified. The anomalies consist mostly of allowable responses that seem odd or inconsistent when viewed in conjunction with other responses. In most surveys some real or apparent inconsistencies are observed. As noted, these may result from real but unusual circumstances in the child's household, classroom or school, respondent misinterpretations of the questions asked, or other factors. The errata are mostly the result of errors in the design or programming of the parent interview that resulted in problems such as skipping questions that should have been asked or in the unavailability of an appropriate response option. Both data anomalies and errata are described in appendix A of this manual. The information on data anomalies, errata, and other considerations will be more easily understood, and is most useful, after the survey items or variables to be used in analyses have been identified.\nTeachers provide information about the children they teach, the children's learning environment, and themselves. More specifically, they are asked about their own backgrounds, teaching practices, and experience. They are also asked to provide information on the classroom experiences for the sampled children they teach and to evaluate each sampled child on a number of critical cognitive and noncognitive dimensions. General classroom teachers completed self-administered questionnaires in fall and spring. \uf06e Special education teachers and service providers of sampled children who have an Individualized Education Program (IEP) are asked to provide information on the nature and types of services provided to the children, as well as on their own background and experience. Information is collected from special education teachers via self-administered questionnaires during spring data collection. \uf06e School administrators are asked to provide information on the physical, organizational, and fiscal characteristics of their schools, and on the schools' learning environment and programs. School administrators also provide information on their own background and experience. Information is collected from school administrators via self-administered questionnaires during spring data collection.\nThe kindergarten before-and after-school care (BASC) component collected important information about children's environments and early learning experiences in nonparental care with regular before-or after-school care providers. Adults other than the child's parents/guardians who cared for the study child for at least 5 hours per week were asked to provide information such as the location where care was provided, children's activities while in care, characteristics of other children in care, and their own background and experience. More detailed information about each of these study components can be found in chapter 2.\nLanguage screening. The language screener used in the ECLS-K consisted of three subtests of the preLAS 2000: \"Simon Says\" which measured listening comprehension, \"Art Show\" which was a picture vocabulary assessment, and \"Let's Tell Stories\" which evaluated the child's natural speech. These three subtests were referred to as the Oral Language Development Scale (OLDS). The screener was administered only to children who were identified by the school as language minority children. In the ECLS-K:2011, only two of the preLAS subtests, \"Simon Says\" and \"Art Show,\" were used to screen for English language proficiency. Also, as noted in section 2.1.1, this screener was administered to all children, as opposed to only children with a home language other than English, though it served mainly as a warm-up for children whose home language was English because the items were relatively easy for a native English speaker. The preLAS subtest data are included on the data file for all children to whom the subtests were administered, regardless of home language.\nEnglish reading assessment for all children. In the ECLS-K, only children who achieved at least a minimum score on the English proficiency screener were administered the remaining cognitive assessments in English. In order to capture the beginning English reading skills of English language learners (ELL), all children in the ECLS-K:2011 were administered a set of items from the main English reading assessment, regardless of their performance on the preLAS subtests. These items measured English basic reading skills (EBRS). \uf06e Spanish reading assessment. The ECLS-K did not include a Spanish reading assessment. Spanish-speaking children in the ECLS-K:2011 who did not achieve at least a minimum score on the preLAS subtests were administered a reading assessment in Spanish that measured Spanish early reading skills (SERS) after they were administered the EBRS.\nGeneral knowledge/science. The ECLS-K kindergarten assessment battery included an assessment domain called general knowledge, which consisted of items measuring science and social science knowledge. In the ECLS-K:2011, the general knowledge component was not administered. Instead, an assessment focusing just on science knowledge and skills was included in the spring kindergarten collection.  Fine and gross motor skills. In the fall kindergarten data collection of the ECLS-K, the direct assessment battery included tasks measuring fine and gross motor skills. Motor skills were not measured in the ECLS-K:2011. Parent interview. The differences in content between the parent interviews in the ECLS-K and the ECLS-K:2011 were generally at the item level rather than at the construct level. The broad topics addressed in the ECLS-K:2011 kindergarten parent interviews, as shown in exhibit 2-3, also were addressed in the kindergarten parent interviews for the ECLS-K. An exception is that the questions included in the ECLS-K interview to support a Head Start program confirmation component in the base year were not included in the ECLS-K:2011. Also, unlike the ECLS-K, the ECLS-K:2011 interview included items to identify and select a current before-or after-school child care provider from whom data about the child's care arrangement would be collected (described further below). In the ECLS-K the mother was the preferred respondent if she lived with the child. In the ECLS-K:2011, interviewers started with the parent or guardian contact identified by the school and confirmed that the parent contact was a parent or guardian who lived with the child and knew the most about his or her education, care, and health. If the parent contact did not meet these criteria, a different parent or guardian who did meet these criteria was identified.\nThe IRT-based theta scores are overall measures of ability. They are appropriate for both cross-sectional and longitudinal analyses. They are useful in examining differences in overall achievement among subgroups of children in a given data collection round or across rounds, as well as in analysis looking at correlations between achievement and child, family, and school characteristics. The fall kindergarten and spring kindergarten theta scores are on the same metric. Therefore, an analyst looking at growth across the kindergarten year could subtract the fall kindergarten score from the spring kindergarten score to compute a gain score. The theta scores may be more desirable than the scale scores for use in a multivariate analysis because generally their distribution tends to be more normal than the distribution of the scale scores. 4 However, for a broader audience of readers unfamiliar with IRT modeling techniques, the metric of the theta scores (from -6 to 6) may be less readily interpretable. Researchers should consider their analysis and the audience for their research when selecting between the theta and the scale score.\nThe IRT-based scale scores also are overall measures of achievement. They are appropriate for both cross-sectional and longitudinal analyses. They are useful in examining differences in overall achievement among subgroups of children in a given data collection round or in different rounds, as well as in analysis looking at correlations between achievement and child, family, and school characteristics. The fall kindergarten and spring kindergarten scale scores are on the same metric. Therefore, an analyst looking at growth across the kindergarten year could subtract the fall kindergarten score from the spring kindergarten score to compute a gain score. Results expressed in terms of scale score points, scale score gains, or an average scale score may be more easily interpretable by a wider audience than results based on the theta scores. \uf06e preLAS subtest raw number-right scores provide information on children's basic English proficiency. These scores may be of interest to users conducting research on children with limited English proficiency. However, because of the limited number of items included in these subtests, these scores do not represent a comprehensive measure of proficiency or of reading skills and knowledge. The primary purpose of fielding these subtests in the ECLS-K:2011 was so they could be used as an English language proficiency screener. The majority of children in the ECLS-K:2011 scored highly or near perfect on these subtests, which was expected given that the subtests came from a standardized assessment for preschoolers and the majority of ECLS-K:2011 children spoke English, even if it was not their primary home language. The preLAS scores are of limited value for children who were not English language learners. The IRT-based reading theta or scale scores, which are available for all children, should be used by analysts interested in performance on the reading assessment, regardless of a child's home language. \uf06e EBRS raw number-right scores provide information on children's performance on the first 20 items administered to all children as part of the reading assessment routing test. These EBRS scores would be useful for someone with a specific analytic interest in the knowledge and skills covered in this particular item set, which are among the most basic knowledge and skills measured in the reading assessment. As with the preLAS subtest items, children who were not English language learners tended to do well on this section of the assessment, and so these scores may be of limited value for them. Also, since these are raw scores, the difficulty of the items children answered correctly is not reflected in the score. A child who answered only the first 10 items correctly would have the same score as a child who answered 5 easier and 5 more difficult items correctly. The IRT-based reading theta or scale scores, which are available for all children, should be used by analysts interested in overall performance on the reading assessment, regardless of a child's home language. \uf06e EBRS/SERS common item raw number-right scores provide information on Spanishspeaking children's performance on 10 items that were administered in both English and Spanish. Researchers may find these scores useful in an analysis focusing on Spanish-speaking English language learners because the scores allow for a comparison of the number of correct responses in English with the number of correct responses in the child's primary home language. It is important to note that these items are direct translations from the existing English items to Spanish. They have not been scaled together, and the item difficulties may not be exactly comparable from one language to the other. Although this is the case, the items have very limited language load, and expert reviewers selected items that translated easily and that could be expected to be roughly equivalent in difficulty in either language.\nThe second digit of X1CLASS indicates whether the teacher provided data on a fullday class (A1FULDAY), a half-day A.M. class (A1HALFAM), a half-day P.M. class (A1HALFPM), or both full-day and half-day classes (A1BOTHCL) in the teacherlevel questionnaire (TQA). There are five values for the second digit of X1CLASS, which points data users to the appropriate class-specific variables from the teacherlevel questionnaire that should be used for each child, or indicates if no TQA data are available: 0 = missing teacher data, 1 = all-day teacher data, 2 = A.M. teacher data, 3 = P.M. teacher data, and 9 = teacher data reported in multiple columns. There are five values for the second digit of X2CLASS, which points data users to the appropriate class-specific variables from the teacher-level questionnaire that should be used for each child, or indicates if no TQA data are available: 0 = missing teacher data, 1 = all-day teacher data, 2 = A.M. teacher data, 3 = P.M. teacher data, and 9 = teacher data in multiple columns. Users interested in knowing the type of classroom in which a child was enrolled should use the first digit of the X*CLASS variable to determine this. Users interested in incorporating teacher and classroom characteristics from the teacher-level questionnaire into their analyses should use the second digit to identify which group of class-specific variables (A.M., P.M., or AD (all-day)) should be used for each child. In instances in which the teacher reported information inconsistently, the first and second digits may not agree with one another. However, the second digit was assigned after a careful review of the data, and those are the variables that should be used for each child. For example, if the child was in a full-day kindergarten class according to the TQC and the second digit points to the half-day A.M. variables, the user should use the half-day A.M data, because it was determined that the teacher reported information for that child's full-day class in the half-day A.M. column of the questionnaire. The meaning of each category in the X1CLASS and X2CLASS variables, as well as the frequencies for children classified in each category, are provided below in exhibit 7-4.  As can be seen from the information above, for the majority of children, the classroom data provided in TQA were reported for a classroom type that matched the type of classroom the child was reported to be enrolled in in TQC, but this was not the case for all children. As examples, a value of \"11\" on X1CLASS means that the child was reported to be in a full-day class and the teacher provided data for a full-day class, whereas a value of \"32\" on X1CLASS means that the child was reported to be in a halfday afternoon class, but the teacher provided data for a half-day morning class and did not also provide data for an afternoon class. A value of \"19\" on X1CLASS means that the child was reported to be in a full-day class, and the teacher provided data on multiple types of classes (for example, a teacher may have provided data on a half-day morning class and a half-day afternoon class, or as another example, a teacher may have provided data on a full-day class as well as data on a half-day morning class). For cases with a \"9\" as the second digit of X1CLASS or X2CLASS, the data user should examine the teacher-provided data to determine which class-specific data they want to link to the child. Although the teacher did not provide data consistently for one type of class in these cases, there may be some class-specific data that match the child's class type and there may be data associated with another class type that the data user would want to use for the child.\nIf there was only one mother (of any type) and only one father (of any type) in the household, the mother was identified as parent 1 (X1IDP1/X2IDP1) and the father was identified as parent 2 (X1IDP2/X2IDP2).\nIf there was only one mother (of any type) in the household, the mother was identified as parent 1. If there was a mother and she had a male spouse/partner in the household, the spouse/partner was identified as parent 2. If there was no spouse/partner in the household, parent 2 is coded -1 (not applicable).\nIf there was only one father (of any type) in the household and no mother, the father was identified as parent 1. If there was a father and he had a female spouse/partner in the household, the spouse/partner was identified as parent 1 and the father was identified as parent 2. 28 If there was no spouse/partner in the household, parent 2 is coded -1 (not applicable).\nIf there were two mothers in the household, an order of preference was used to identify one mother to be parent 1, with the order specified as biological, adoptive, step-, foster mother or female guardian, then other female parent or guardian. 29 The other mother was identified as parent 2. 30 If there were two mothers of the same type 27 In the ECLS-K, the parent identifiers were P1MOMID, P1DADID, P2MOMID, and P2DADID. These have been combined into parent 1 and parent 2 variables in the ECLS-K:2011. 28 In two households (10016769 and 10017525), the spring 2011 respondent refused to provide information (name, age, sex, etc.) about the other parent. In one of these cases (10016769), the father respondent appears as parent 1 (X2IDP1), due to the absence of information about the other parent figure, who is identified as parent 2 (X2IDP2). In the other case (10017525), the biological mother respondent appears as parent 1 (X2IDP1) and her spouse/partner appears as parent 2 (X2IDP2). In this case, the biological mother would have appeared as parent 1 regardless of the sex and relationship of the other parent to the child. 29 There were new categories in the ECLS-K:2011 parent interview for \"Other female parent or guardian\" in FSQ.140 and \"Other male parent or guardian\" in FSQ.150 that were not included in the ECLS-K. 30 For case 10018131, the child is reported as having two father figures and two mother figures in the household. The detailed relationship questions indicate that the household contains the biological mother and stepfather, as well as two older persons identified as adoptive parents. Based on established priorities for the designation of parents, the biological mother and adoptive father were chosen as parent figures for whom the composite variables were created. Analysts will wish to use their own judgment in how to treat the adult household members in their own analyses.\nIf there was no one in the household identified as a mother or father, then a female parent figure was identified as person 1. 31 If the female parent figure had a male spouse or partner, the spouse/partner was identified as person 2. For example, if a child lived with his grandmother (the respondent) and grandfather, and neither his mother nor father also lived in the household, then the grandmother was identified as parent 1 and the grandfather was identified as parent 2. If only the grandfather lived in the household, the grandfather would be parent 1. Demographic information such as age, race, and education was collected for these \"parent figures.\" 32 Once parents/parent figures were identified, X1HPAR1, X1HPAR2, X2HPAR1, and X2HPAR2 were created to identify the specific relationship of parent 1 and parent 2 to the study child. 33 It should be noted, however, that for households in which the child lived with parent figures other than his or her mother and/or father, the parent figures identified in X1IDP1 and X1IDP2 (and X2IDP1 and X2IDP2 in the spring) were not defined as parents (meaning biological, step-, adoptive, or foster) for the construction of X1HPAR1, X1HPAR2, X1HPAR1, and X2HPAR2. For example, if there are a grandmother and grandfather and there are no parents listed in the household, X1HPAR1 and X1HPAR2 would be coded as category 15 (no resident parent). 31 There is one case (10014051) that has a -9 (not ascertained) for X2IDP1, X2IDP2, and other household composites (X2RESREL, X2HPAR1, X2HPAR2, X2HPARNT, X2NUMSIB, X2LESS18, X2OVER18, X2HTOTAL) because the respondent ended the interview before data about additional household members and their relationship to the study children were obtained. 32 Some households have parent configurations that appear unusual. Among these are case 10000679, where the grandmother respondent reported that her spouse/partner was the child's stepfather, 10013393 where the grandmother respondent reported that her spouse/partner was the child's adoptive father, and 10018019 where the child's aunt reported that her spouse/partner was the child's father. 33 These variables are a combination of P1HMOM and P1HDAD, and P2HMOM and P2HDAD from the ECLS-K.\nThis category includes those engaged primarily in the application of scientific principles to research and development. Natural scientists are those in the physical sciences (e.g., chemistry, physics) and the life sciences (e.g., biology, agriculture, medicine). In addition, this category includes those in computer science, mathematics (including statistics), and operations research.\nThis category includes health care professionals who diagnose and treat patients. In addition to physicians, dentists, and veterinarians, this category includes optometrists, podiatrists, and other diagnosing and treating professionals, such as chiropractors, hypnotherapists, and acupuncturists.\nThis category includes occupations providing personal and protective services to individuals, and current maintenance and cleaning for building and residences. Some examples include food service, health service (e.g., aides or assistants), cleaning services other than household, and personal services.\nMechanics and repairers are persons who do adjustment, maintenance, part replacement, and repair of tools, equipment, and machines. Installation may be included if it is usually done in conjunction with other duties of the repairers.\nThis category includes occupations that involve helping other workers and performing routine nonmachine tasks. A wide variety of helpers, handlers, etc., are included in this category. Examples include construction laborers, freight, stock, and material movers, garage and service station-related occupations, parking lot attendants, and vehicle washers and equipment cleaners.\nDepending on your PC's configuration, you may encounter warning messages during installation. To respond, always keep the newer version of a file being copied and ignore any access violations that occur during file copying.\nIf you are installing multiple ECBs (not different versions of the same ECB) on your PC, you may receive a message warning that Setup is about to replace pre-existing files. To respond, always opt to continue the installation although the default is to cancel the setup. When you get a follow-up message to confirm whether the installation should be continued, press Yes to continue, although the default is No.\nFor case 10005685, the interviewer initially completed the interview through CCQ, backed up and changed responses to some previously completed items (consequently changing the skip patterns that should have been followed in the interview), and then broke off the interview after CCQ260 (P1CTRNOW). As a result, some items PRIOR to the final breakoff point that should have been asked were skipped. In particular, the interviewer changed the response to PLQ020 (P1ANYLNG) (are any languages other than English spoken in your home?) from NO to YES, thus rerouting the case to some items dependent upon a \"yes\" response to PLQ020. Items HEQ020 (P1NOENG), HEQ035 (P1RDOTHL), and HEQ045 (P1BKOTHL) are three of those items. These variables have been set to -9 for this case.\nCase 10016933 has multiple problems with data collected in section CCQ. The interviewer completed the interview through section CCQ and then backed up to question CCQ030 (P1NUMREL). At that point the case broke off, deleting the information that had been collected for the remaining questions in this section.\nThere are cases in the fall parent interview where the relationship between the respondent and the respondent's spouse seems unusual. For example, the respondent identified himself or herself as an aunt, uncle, other relative, or other nonrelative, but indicated that he or she was married or partnered with the child's parent/guardian. Users should review the data for these cases to ensure that the responses make sense for their analyses. For 4 of these cases (CHILDID= 10000159, 10012776, 10013246, and 10017311), it is likely that a family member was serving as an informal translator 1 Throughout this document, the interview item number is referenced followed by its corresponding variable name. For example, PLQ060 (P1PRMLNG) refers to the interview question \"What is the primary language spoken in your home?\" and its corresponding variable name is (P1PRMLNG). Question wording for all interview item numbers and variable names can be located by searching the Electronic Codebook (ECB). The data collection instruments are also available in Appendix A on the CD-ROM and online at http://nces.ed.gov/ecls.\nOne case (CHILDID=10002529) appears to have a data entry error for NRQ040 (P1BDCON). In the answers about who was in the household, the household had an adoptive mother, no adoptive father, and no biological parents. The biological parents were nonresident parents and were reported to be living. In section NRQ, which asks about contact with a nonresident parent, it appears the interviewer incorrectly coded the variable P1BDCON (NRQ040) for an \"adoptive father\" when a \"biological father\" should have been selected.\nSections PLQ, PEQ, and EMQ can have -9 values for the individual parent items if the parent was not appropriately identified in FSQ during the interview. For example, for case 10002341, person 3 was initially identified as an \"other\" nonrelative. Subsequent coding of the other (specify) text indicated that person 3 was actually the child's stepfather. Because he was not properly identified in FSQ, the CAI program did not ask questions about him in sections PEQ or EMQ. As a result, valid EMQ data are available for the mother and for the last few household-level items in section EMQ, but data from section EMQ are all missing for the stepfather. The CAI programming for Box 5 in section MHQ of the fall parent interview did not follow the programmer specifications. Cases with respondent biological parents who were never married and never lived together were inadvertently skipped over MHQ175 (P1BIMMAR) and MHQ180 (B1BIMLIV); this skip is not specified in the Box 5 programmer specifications. According to Box 5, only cases for which the biological mother was married/living as married at the time of child's birth should have skipped MHQ175/180 (P1BIMMAR/B1BIMLIV).\nThe CAI programming for Box 3 of HRQ did not follow the specifications. For cases for which the biological mother was not living (HRQ030 (P1NRMOLV=2)) AND the mother's birth year was not provided (HRQ040B (P1BIOBY=-7 or -8)), questions about the mother's age at and year of death were mistakenly not asked. Cases affected by this error were mistakenly skipped to question HRQ090 (HRP1MOMHSP). The same problem occurred for biological fathers. If the biological father was not living (HRQ030 (P1BIDLIV=2)) AND the father's birth year was not provided (HRQ040B (P1BIODBY=-7 or -8)), questions about the father's age at and year of death were mistakenly not asked. Cases affected by this error were skipped to question HRQ090 (P1DADHSP).\nThe CAI programming for Box 3 in section NRQ and Box 1 in section CFQ did not match the programming specifications. Box 3 in section NRQ indicates that if the response for HRQ030 (P1NRMOLV, P1BIDLIV) was 2, 3, 8, or 9 (parent not living, don't know biological parent, refused, or don't know, respectively), the case should skip to Box 4, therefore NRQ040 was not asked and was assigned a value of -1. Cases for which NRQ040 is -1 were not routed to CFQ010 (P1LIKMOM) and CFQ020 (P1GRNDMA, P1BIOMTHR, P1STPMOM, P1ADPTMA, P1FOSMOM, P1RESPM, P1TCHMOM, P1CLGMOM, P1AUNT, P1SIBMOM, P1FRDMOM, P1SITMOM, P1OTHMOM, P1NONRMA) as they should have been according to the programmer specifications. Because a large number of cases (about 150) were incorrectly skipped out of these items and the error was not random, users should not use these data.\nThere are over 300 cases where CFQ030 (P1LIKDAD) and CFQ040 (P1GRNDPA, P1BIOFTHR, P1STPDAD, P1FOSDAD, P1RESPD, P1TCHDAD, P1UNCLE, P1SIBDAD, P1FRDDAD, P1SITDAD, P1OTHDAD, P1NONRDA) were skipped because respondents indicated that the biological father was not alive, they did not know who the nonresident father was, or the respondent answered \"refused\" or \"don't know\" to HRQ030 (P1BIDLIV). Because a large number of cases were incorrectly skipped out of these items and the error was not random, users should not use these data.\nThere are two different problems related to the skip instructions in Box 3 of section EMQ. The first is a design error and the second is a Blaise programming error. Because of the design error, cases with two responses to EMQ070 that included an answer of 1, 2, 3, 4, or 5 in combination with a 6 were routed to Box 4 and skipped EMQ100 (P1TAK_1, P1TAK_2). The programming error was such that the program A-5 did not follow the design whenever there was an answer of 91 in combination with any other answer in EMQ070. According to the specifications in Box 3, a response of 91 for EMQ070 should have routed a case to EMQ100 (P1TAK_1, P1TAK_2). For example, if responses of 6 and a 91 were provided for EMQ070, the response of 91 should have led to EMQ100 being asked, but it was not. The same is true for answers such as 1, 6, and 91. The data have been edited in the data file so that all cases with a response of 1 through 5 or 91 for EMQ070 have either valid data or -9 for EMQ100, and all other cases have EMQ100 set to -1 (indicating they were correctly skipped out of this question). Additionally, three cases (10000652, 10011256, and 10008007) have P1TAK_1 set to -9 because they had an answer of \"don't' know\" for the parent figure's sex. One case (10000552) also had P1TAK_2 set to -9 because of a \"don't' know\" answer for the second parent figure's sex. There is also a case (10000652) set to -9 on P1TAK_2 because the stepfather was initially coded as an \"other nonrelative\"; subsequent coding of the other (specify) text for this case identified the individual as a stepfather. Because he was not appropriately identified as a parent figure during the interview, questions were not asked about this parent in section EMQ. \uf06e Section CMQ was supposed to contain skip instructions so that CMQ695 (P1WHRITV/P2WHRITV), which asks where was the interview conducted, was only asked for interviews conducted in person. That skip was mistakenly omitted and CMQ695 (P1WHRITV/P2WHRITV) was asked of everyone. Because most interviews were conducted on the telephone, the interviewer did not really know where the respondent was at the time of the interview. Information in CMQ695 is only meaningful for interviews that were conducted in person. Users may wish to set CMQ690 (P1WHRITV/P2WHRITV) to -1, (not applicable) for cases completed by telephone or for which the mode was not ascertained, CMQ680 (P1MODE=1 or -9.) Parent Interview: Spring 2011 Anomalies and Errata \uf06e There are cases in the parent interview where the relationship between the respondent and the respondent's spouse seems unusual. For example, the respondent identified himself or herself as an aunt, uncle, other relative, or other nonrelative, but indicated that he or she was married or partnered with the child's parents/guardian. Users should review the data for these cases to ensure that the responses make sense for their analyses. For 3 of these cases (CHILDID=10010293, 10006123, and 10017404), it is likely that a family member was serving as an informal translator and did not always report the family relationships consistently during the interview. For example, for CHILDID=10010293 the respondent was a teenage sister and her spouse was the child's father. There are 13 cases where the respondent appears to have been both the aunt or grandmother or cousin and female guardian. Also the respondent may report his or her spouse's relationship to the child as father/male guardian when the spouse may also be considered the grandfather or uncle or partner of the guardian. These situations apply to the following cases: brother. There are 3 cases (CHILDID=10009281, 10005906, and 10000788) where the interviewer may not have picked the correct person from the fall household roster as the respondent.\nThere are some household members that have data for race and ethnicity even though the rules in the specifications indicate that they would not be asked these questions. One reason for this is that there were different household members in fall 2010 than in spring 2011. For example, in fall 2010, if a nonrelative female was the respondent and the grandfather was the nonrelative's spouse, and there was not a parent in the household, race and ethnicity would be asked about the nonrelative female and her spouse, the grandfather. This is because if there are no parents in the household, the race and ethnicity questions are asked about the respondent and his/her spouse/partner. If there are parents in the household, the race and ethnicity questions are only asked about the parents. In spring 2011, if these household members were still in the home, but the biological mother moved in, race and ethnicity would be asked about the biological mother, and the race of the grandfather and his spouse would also be carried forward despite the fact that their race data does not fit the skip pattern for race data collection in spring 2011. The race and ethnicity of these household members were retained so that if the household composition changed in the future and the grandfather and his nonrelative spouse were candidates for race data collection again, race and ethnicity would not need to be collected a second time for the same people. In other cases, there are household members with values for race and ethnicity because an interviewer initially set the family structure section (FSQ) relationship codes such that we collected data for the spouse/partner of the respondent, but then the interviewer backed up and changed the relationship values such that the spouse/partner was no longer eligible for the race and ethnicity questions. The race data were eliminated from section FSQ in these instances, but the data were not removed from the permanent roster. In each of these instances, the spouse is actually one of the two parent \"figures\" in the household; thus the race and ethnicity data for these individuals were retained.\nFor 1 case (CHILDID=10011120) the respondent is identified in the data as the grandfather, but during the process of data cleaning it was determined that the respondent was actually the child's biological mother.\nFor 1 case (CHILDID=10017805,) the grandmother began the interview, but the father took over the interview at item PIQ090 and answered the remaining questions. \uf06e There are 83 cases that do not have household roster information. These 83 cases all have a value of -9 for X2RESID (round 2 respondent roster number). These 83 cases include 81 cases that did not respond to the fall kindergarten parent interview and completed the SPQ section of the spring kindergarten parent interview (which included questions only asked of fall nonrespondents) but did not complete the FSQ section (family structure questions) in the spring. Two of the 83 cases (10006399 and 10016064) have incomplete household roster data in section FSQ due to a bug in the Blaise application that was subsequently fixed. In both cases the household roster questions were erroneously skipped due to the Blaise issue, but marital status was asked in FSQ200 (P2CURMAR) and answered. Unfortunately, without the household roster information, it is not possible to indicate which household member is the respondent's spouse, so FSQ120 (P2SPOUSE) for these cases is set to -9. There is also one additional case (10014051) that has missing household roster data because the case broke off in the household roster before it was completed and before questions about the household members' relationship to the study child were asked.\nCases 10005155, 10006399, 10009174, 10014653, 10016064 are otherwise complete interviews that had problems with the household roster information resulting in data being missing in sections of the interview that are dependent upon the roster data to route respondents through the sections. Two of these cases completed the interview with no household roster (CHILDID=10016064 and 10006399). The others had roster data entered after the interview. \uf06e There are 5 cases that have P2HIG_1 and P2HIG_2 set to -1. As noted above, two of these cases (CHILDID=10006399, 10016064) do not have household roster data because of a technical problem in the interview program that was corrected after these interviews were completed. The composite variables associated with household composition and child and parent characteristics are coded as -9 (not ascertained) for these cases. For another two cases (CHILDID=10009174, 10014653), the household roster data are included in the data file, but the cases were incorrectly skipped out of the FSQ question asking about parent's country of origin (FSQ212 (P2PARCT1, P2PARCT2)) and highest level of education (FSQ221 (P2HIG_1, P2HIG_2)); thus these variables are set to -9. These two cases originally did not have data in the household roster (thus causing the skip error), but the data were entered later from updates from the field. The remaining case (10005155) is a partially complete case that broke off after the question about parent's country of origin was asked.\nThe structure of the questionnaire was such that when asking for information about the parents or guardians, a set of questions within a section was first asked about parent 1, and then the same set of questions was asked about parent 2. In the fall in opposite-sex parent households, questions were first asked about the \"key female\" (mother/female guardian) in the household, and then asked for the \"key male\" (father/male guardian) in the household. In the spring, the order was changed, and information was collected for the father/male guardian first, then for the mother/female guardian. In both the fall and spring, data for the mother/female guardian are stored in the parent 1 variables and data for the father/male guardian are stored in the parent 2 variables. In the spring, there are some cases that have country of origin and education information for parent 2 (the male parent figure) but not for parent 1 (the female parent figure), because for various reasons this information was collected for fathers but not for mothers. For example, if the data were collected for fathers, then the interview broke off before the questions could be asked for mothers.\nAges for household members other than the child were not incremented from fall to spring. However, because of editing and interviewer comments, there are six cases for which a household member's age changed from the fall to the spring interview (CHILDID=10001976, 10009790, 10002814, 10001513, 10015099, 10008501). .) The interview was designed to prompt the interviewer to verify if such inconsistent information was correct before continuing with the interview. This was done to prevent interviewer entry errors. However, the consistency check allowed for the age the parent moved to the United States to be older than the parent's current age in case the age recorded for the household member in a previous interview or in the current interview was incorrect. Three of these 11 cases (10005643, 10007293, 10003870) included interviewer notes indicating correct ages that were used to update the data. No additional information is available for the other cases.\nOne case (CHILDID=10002307) does not contain data in section SSQ although there are data for a later section (DWQ). The interviewer entered data through section DWQ and then returned to section SSQ, deleting some of the data. The interview ended before the interview was complete. Data in section SSQ have been coded -9.\nFor case CHILDID=10012336, the biological father was mistakenly identified as an adoptive father in the fall. Thus, in section NRQ the interview asked about a nonresident biological father even though section NRQ was not applicable for this respondent in either the fall 2010 or spring 2011 interviews, given the presence of a biological father in the household.\nIn section NRQ, there are about 330 cases that should have been asked the child support questions: NRQ261 (P2CSCRT, P2CSWRT, P2CSINF, P2CSPEN, P2CSNOAG, P2CSOTH); NRQ265 (P2RECPAY); NRQ266 (P2PAYREG); and NRQ264 (P2CSBIOF, P2CSBIOM, P2CSADPF, P2CSADPM), if relevant, but were not because the programming instructions in Box 5 mistakenly resulted in these cases, for which NRQ040 (P2BMCON, P2BDCON) was equal to 4 \"no contact since birth,\" being skipped out of those questions. In Box 5, the specifications exclude nonresident parents for whom the response to NRQ040 (P2BMCON, P2ADMCON, P2BDCON,  P2ADDCON) is 5 (parent deceased), 6 (no contact since adoption), 7 (no adoptive mother/father), or 8 (parent unknown/only a donor), but not 4 (no contact since birth). Subsequently, those same 330 or so cases were not considered in the specifications in Box 6 used to determine whether NRQ264 would be asked.\nA programming error in the spring CAI instrument caused the relationship data collected in the household roster in the fall to be eliminated when an interviewer opened a case and then closed the case without entering any new household roster data. This error was fixed as soon as it was identified during data collection, but roughly 240 cases were affected before the error was fixed. The fall relationship data were restored for these cases because the fall data accurately reflected the spring household composition (since no new roster data needed to be entered for these cases in the spring). However, this caused problems with the skip patterns for questions in sections CFQ, DWQ, NRQ, and PPQ that are based on the presence or absence of parents in the household. This is reflected in the larger number of cases coded -9 for some of the variables pertaining to these sections. Other cases are coded -9 on these variables because they broke off the interview or there were changes made to the household composition as a result of data editing.\nIn section CFQ, programming errors caused 219 cases to skip question CFQ100 (P2RELSHP) and 274 cases to skip questions CFQ300 (P2DADHOM) and CFQ310 (P2DADHM2).\nIn section DWQ, box 1 determined if the respondent was the mother/female guardian or father/male guardian; if so, items DWQ010 (P2WARMCL) through DWQ060 (P2FLANGR) were asked of that parent/guardian. DWQ010 through DWQ060 were to be asked of a nonparental respondent only if there were no parents or guardians currently living in the household. For 221 cases, it was mistakenly recorded that there was at least one parent/guardian in the household; therefore, DWQ010 through DWQ060 were not asked of the nonparental respondent when they should have been.\nSimilar to the issue noted above for section DWQ, in section PPQ, for 240 cases it was mistakenly recorded that there was at least one parent/guardian in the household, and thus items PPQ100 (P2BOTHER) through PPQ260 (P2RPRVHP) were not asked.\nThere are 64 cases that are coded -9 for PPQ270 (P2BFNDHP) and PPQ280 (P2BFRVHP) that are not breakoffs. Of these 64 cases, 48 were edited after the interview was completed to show a biological parent in the household who was not the respondent. If it had been known there was a biological parent in the household during the interview, PPQ270 (P2BFNDHP) and PPQ280 (P2BFRVHP) would have been asked. The other 16 cases were not asked these questions because of the problem described above that caused the fall relationship data to be deleted.\nIn English, question HEQ310 included baseball as an example in the question text. Baseball was omitted as an example in the Spanish text.\nIn English, question FDQ130c asked parents how true it was that the family \"couldn't afford to eat balanced meals.\" The Spanish translation asked parents how true it was that the family \"couldn't afford to eat more balanced meals\" (\"una alimentaci\u00f3n m\u00e1s balanceada\").\nAbout 755 children attended schools in which the administrators reported that zero percent of students come from the local neighborhood (school administrator questionnaire, question A8).\nSchool administrators were asked to report the date by which children were required to turn 5 years old in order to start kindergarten (question A11, S2NOCUTO, S2MMFIVE, S2DDFIVE, S2YYFIVE). More than 5,000 of the 18,174 children in the base-year data file attended schools in which the administrators reported a year of 2009 or earlier. This suggests that these school administrators misinterpreted the question as referring to birth date rather than the date the child turned age 5.\nAbout 255 children attended schools in which the administrators reported in A10 no half-day kindergarten classes (S2HLFKIN) but reported wrap-around care for half-day kindergartners (S2HLFDAY, question C1b).\nAt the beginning of section F of the school administrator questionnaire (concerning Title I and Title III programs), administrators of private schools were instructed to check a skip box and go to section G. In some cases, administrators of private schools did not do so, and in other cases, administrators of public schools did so. Data for children in schools for which the administrator checked the box and skipped to section G are coded as -1 (not applicable) on the section F items; data for children in schools for which the administrator did not check the box and did not answer the questions are coded -9 (not ascertained).\nThere are 153 children who attended schools in which administrators reported time spent each week on several activities at item H5 (S2INSTRU, S2INRMGT, S2DISCAT, S2MONITR, S2TEACH, S2TALKPT, S2STUDNT, S2PPRWRK) that sum to more than 168, the total number of hours in a week. Teacher Questionnaire: Fall 2010 \uf06e There was some inconsistency in teachers' reporting of information about classroom characteristics across columns for morning, afternoon, or full-day kindergarten classes. Some teachers used columns inconsistent with the class type(s) they reported. When possible to do so unambiguously, the data were realigned to match the class type reported by the teacher.\nThere are discrepancies between the reported individual student counts by age, sex, and race and the sums of students the classroom. For these three questions (age, sex, and race of students in the classroom), the sums were calculated based on the individual counts and comparison against the other count questions. Before-and After-School Care Teachers/Care Providers, Child-Level Questionnaire (WCQ) \uf06e Question 9 (Z2LNGTCH) was formatted as a \"mark one\" question though the language of the question was such that it read as a \"mark all that apply\" question. Providers who provided more than one response have Z2LNGTCH set to -9 (not ascertained).\nThere are two cases (0312014 and 0470020) for which the youngest calculated ages and age unit (Z2CHDAGE and Z2AGUNIT) are unexpectedly high (e.g., 607 months old). The providers' questionnaires were reviewed, and the data are as reported by the providers. A-12 \uf06e There are two cases (0324004 and 0324007) linked to the same provider where the number of hours of adult-directed whole class activities (Z2WHLCLH) reporter by the provider is 25. The providers' questionnaires were reviewed, and the data are as reported.\nThe computer-assisted interview (CAI) application for the ECLS-K:2011 parent interviews was programmed in a way that did not capture information on both parent figures in a small number of households. As described in chapter 7, missing values were imputed for parent education (X12PAR1ED_I and X12PAR2ED_I) and parent occupational prestige (X1PAR1SCR_I and X1PAR2SCR_I). In a small number of cases, however, the second parent was identified after imputation took place and, as a result, these cases have missing data for X12PAR2ED (10017525, 10016769, 10017336, 10001179, and 10012475).\nIn two households (10016769 and 10017525), the spring 2011 respondent refused to provide information (name, age, sex, etc.) about the other parent. In one of these cases, the father respondent appears as parent 1 (X2IDP1) due to the absence of information about the other parent figure, who is identified as parent 2 (X2IDP2). In the other case (10017525), the biological mother respondent appears as parent 1 (X2IDP1) and her spouse/partner appears as parent 2 (X2IDP2). In this case, the biological mother would have appeared as parent 1 regardless of the sex and relationship of the other parent to the child.\nFor case 10018131, the child is reported as having two father figures and two mother figures in the household. The detailed relationship questions indicate that the household contains the biological mother and stepfather, as well as two older persons identified as adoptive parents. Based on established priorities for the designation of parents, the biological mother and adoptive father were chosen as parent figures for whom the composite variables were created. Analysts will wish to use their own judgment in how to treat the adult household members in their own analyses.\nAs noted above, there are 83 cases without information on household members. Most composite variables based on the parent interview are coded as not ascertained (-9) for these cases. These 83 cases can be easily identified by specifying X2RESID (the spring 2011 respondent ID) = -9. An additional case (10014051) is coded -9 for X2IDP1, X2IDP2, and other household composites because the respondent ended the A-13 interview before data about additional household members or their relationship to the study child could be obtained.\nThere are two cases (10007781 and 10012950) where the coding of the composite race variable X2PAR2RAC may appear to be anomalous. In these cases, race and ethnicity were asked twice for the same parent, because that parent was listed twice during the enumeration of the household members, and race was reported differently in each instance. The final composite variable value was based on an assessment of the responses.\nThe ECLS-K:2011 was not designed to specifically identify whether a child sampled for the study was a twin or sibling of another sampled child. Following the base-year data collection, attempts were made to determine whether any of the children had a twin in the study by matching information on their school, date of birth, last name, and race. See chapter 7 for more information on the identification of twins and the variables X12TWIN and TWIN_ID. There are some twin pairs that have different ages listed in the household matrix for the twin in one of the two interviews. All twin pairs have the same birthdate, but there were differences in the reporting of ages. For example, the twin is listed in the household matrix as the same age as the focal child in cases 10000128, 10005321, 10014435, 10012521, and 10011688; however, in the twin pairs for these cases the twins are listed as different ages than the focal child or not listed (cases 10003630, 10004908, 10009486, 10012017, 10013873). This difference in ages could occur because the focal child's date of birth determines the age in the household roster, but the ages of siblings are reported by the respondent. In the cases noted above, the respondents were the same for both interviews. Errors in respondent report or interviewer entry may have caused the differences in ages. In the spring of 2011, there also may be mismatches between the ages of twins in a household because the focal children's ages were updated based on their date of birth and the interview date, but the ages of siblings in the household matrix were not incremented in age. Thus, ages that matched between the focal child and the twin in the fall of 2010 may not appear to match in the spring of 2011, even though the birthdates of the two children are the same.\nInformation on hours in nonparental care arrangements was collected in the fall kindergarten parent interview. If the child's primary arrangement of a given type (for example, relative care) occurred less than once per week, the hours of care were not asked for that arrangement. However, the total weekly hours for other arrangements of that type (if the child had any) were collected. If the child had only that one type of care (relative care but not nonrelative or center-based care), the composite X1HRSNOW is calculated to be zero, even though the child did receive nonparental A-14 care. Information for irregular care was not asked because it leads to significant response burden on the part of the respondent to recall information about child care that is used infrequently. It is also difficult to generalize from child care that is not used regularly because it is not clear how much exposure the child has to each care arrangement.\nThe variable X1BASC indicates the type of arrangement that was selected for the Before-and After-School Care (BASC) component. If a respondent did not provide enough information about a particular type of care to determine a child's primary arrangement, but the respondent did provide enough information about another type of care that qualified for the BASC component, the child care arrangement that had complete data was selected for the BASC component. If a child had no child care arrangements that were eligible, if the data needed to determine whether the case could be selected for the BASC (e.g., hours in care) were coded -7 (refused) or -8 (don't know), if the parent did not give permission to contact the child's child care provider in parent interview question CHQ380, or if the child care provider was less than 18 years old, X1BASC is coded as -1 (not applicable). If the respondent to the parent interview broke off the interview in the child care section, and the parent interview question CHQ380 is coded -9 (not ascertained), X1BASC is also coded as -9 (not ascertained).\nIn the spring collection, 29 study children were identified as homeschoolers. These children have values of -1, not applicable, for school variables such as X2LOCALE, X2PUBPRI (public/private school), and X2KSCTYP (school type). While they have a value for S2_ID (spring school ID number) for data processing purposes, all homeschooled children have the same value for S2_ID.\nThere are 512 cases of children whose recorded height is shorter in the spring than it was in the fall. In about half of these cases (n=252), the difference in height is an inch or less, which could be attributed to differences in the shoes they were wearing, their hair style on the day of measurement, or slouching. However, in 260 cases, the difference in height was greater than an inch, and in some cases more than 3, 4, or 5 inches shorter. Analysts should use their own judgment in how to classify these cases in their analysis.\nThere are some anomalous responses related to the calculation of X1PAR1EMP and X1PAR2EMP (parent employment status). In some cases, a respondent reported that a parent was looking for work, but the strategies he or she used in looking for work included only reading want ads or some other strategy that did not qualify as \"actively\" looking for work according to definitions based on those used by the U. S. Department of Labor. These are parents for whom P1LOK_1 = 1 but P1DO1_1 through P1DO5_1 are all equal to 2 (for parent 1), or P1LOK_2 = 1 but P1DO1_2 through P1DO5_2 are all equal to 2 (for parent 2). These parents are classified as not in the labor force rather than looking for work.\nOccupation data were collected for parents who stated that they were looking for work (specifically, they were asked for information about their most recent occupation). However, as noted above, some of these parents were not using strategies that qualified as \"actively\" looking for work. Occupation data were retained for these A-15 parents and appear in composite variables X1PAR1OCC_I (for parent 1) and X1PAR2OCC_I (for parent 2).\nFor case 10012475, the parent occupation code and occupational prestige code were assigned to the wrong parent. Specifically, X1PAR2OCC_I and X1PAR2SCR_I correctly belong to parent 1 (X1PAR1OCC_I, X1PAR1SCR_I) rather than parent 2. In a different case, 10017336, variables X1PAR2OCC_I and X1PAR2SCR_I should be -9 and are not.\nCases 10007811 and 10008017 originally had household members listed in the parent interview who were not actually in the household. These persons were edited out of the variables indicating relationship to the study child, but their presence in the household is still reflected in X1HTOTAL and X1LESS18. For case 10007811, X1HTOTAL should be 9 rather than 10, and X1LESS18 should be 7 rather than 8. For case 10008017, X1HTOTAL should be 3 rather than 4, and X1LESS18 should be 1 rather than 2. \uf06e Three cases (10002683, 10012672, 10013143) with missing data for the location of child care were set to -9 (not ascertained) on X12PRIMPK, but should have been set to 6 (non-relative care location varies/not asked). \uf06e Case 10012665 has a correct value for X1TQCDAT (X1TQCDAT=0) because all the teacher questionnaire data were -9 (not ascertained)), but it has a valid weight associated with analysis of teacher data from the child-level teacher questionnaire because this issue was discovered late in data processing after weighting had already been completed.\nDue to inconsistencies in reporting by teachers, it is not always clear which variables should be used for the specific class in which the child is enrolled. Some teachers did not always report data in the column associated with the type of class he or she indicated teaching (for example, in TQA the teacher reported teaching a full-day kindergarten class but reported data in the A.M. kindergarten column), some teachers did not report teaching the same type of kindergarten class in which he or she indicated the child was enrolled (for example, in TQA the teacher reported teaching only a half-day P.M. kindergarten class but reported in TQC that the child was in an A.M. kindergarten class), and some teachers reported teaching another class in addition to the type of class in which the child was enrolled (for example, in TQA the teacher reported teaching both half-day A.M. and P.M. kindergarten classes and reported in TQC that the child was in an A.M. kindergarten class). There are composite variables, X1CLASS and X2CLASS, that were created as indicators of the agreement in class type information between the TQA and TQC and to tell users which set of variables (A.M., P.M., or full-day) describe the particular kindergarten classroom in which the child was enrolled.\nVariables that provide a particularly identifying characteristic, such as a specific disability, or information that could be matched against external data sources to obtain a specific identifying characteristic, such as exact date of marriage or divorce, are also suppressed. The values for these variables are set to -2. There is a comment field in the variable frequency distribution view screen of the ECB that displays a comment for each masked variable indicating whether the variable from the restricted-use file has been recoded or suppressed in the K-1 PUF. Exhibits 1 to 12 below present the lists of masked variables for the base year. The exhibits display the variable name, variable label, and a comment indicating whether the variable was recoded or suppressed. When applicable, the reason for suppression is also provided. Exhibits 13 to 21 present the lists of masked variables for first grade. Section 7.1 of the user's manuals explain the variable naming conventions. All variables from the special education teacher questionnaire part A (i.e., all variables with the prefix D2 or D4) and from the special education teacher questionnaire part B (i.e., all variables with the prefix E2 or E4) are suppressed on the K-1 PUF. In addition, all variables from the teacher-level questionnaire for children in kindergarten in the spring 2012 round of data collection are suppressed, with the exception of the variable indicating the year the questionnaire was completed. For brevity, these variables are not included in the exhibits."}, {"section_title": "INTRODUCTION", "text": "This manual provides guidance and documentation for users of the kindergarten (or base year) data of the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011). It begins with an overview of the ECLS-K:2011 in this chapter. Subsequent chapters provide details on the study data collection instruments and methods; the direct and indirect child assessment data; the sample design; weighting procedures; response rates; data file content, including composite variables; and the structure of the data file. Data for the ECLS-K:2011 are released in both a restricted-use and a public-use version. This manual, which has been developed for public dissemination and use with the public version of the data, is almost identical to the manual released with the kindergarten restricted-use file. 1 Edits have been made to round or remove unweighted sample sizes that cannot be generated with the public-use file (PUF). Estimates such as means that are presented in the tables throughout the manual were calculated with the restricted-use file. Some estimates may not be able to be reproduced exactly with variables in the PUF because the variables have been masked to make them suitable for public release. Appendix C provides information about the ways in which data were masked on the PUF and includes tables that list all variables that have been masked or suppressed. Also, throughout this manual references are made to materials that are on the restricted-use CD-ROM. Public-release versions of these materials are available under \"Data Products\" on the ECLS-K:2011 website, nces.ed.gov/ecls/kindergarten2011.asp. The ECLS-K:2011 is following a nationally representative sample of children from kindergarten through their elementary school years. It is a multisource, multimethod study that focuses on children's early school experiences. It includes interviews with parents, self-administered questionnaires completed by teachers and school administrators, and one-on-one assessments of children. During the kindergarten year, it also included self-administered questionnaires for nonparental before-and afterschool care providers. The ECLS-K:2011 is sponsored by the National Center for Education Statistics 1-1"}, {"section_title": "Background", "text": "The ECLS-K:2011 is the third and latest study in the Early Childhood Longitudinal Study (ECLS) program, which comprises three longitudinal studies of young children: the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K); the Early Childhood Longitudinal Study, Birth Cohort (ECLS-B); and the ECLS-K:2011. The ECLS program is unprecedented in its scope and coverage of child development, early learning, and school progress, drawing together information from multiple sources, including school administrators, parents, teachers, early care and education providers, and children, to provide data for researchers and policymakers to use to improve children's early educational experiences and address important policy questions. The ECLS-K:2011 provides current information about today's elementary school children and data relevant to emerging policy-related domains not measured fully in the previous studies. Also, coming more than a decade after the inception of the ECLS-K, the ECLS-K:2011 allows for cross-cohort comparisons of two nationally representative kindergarten classes experiencing different policy, educational, and demographic environments. More information about all three of these studies can be found on the ECLS website (http://nces.ed.gov/ecls)."}, {"section_title": "Conceptual Model", "text": "The design of the ECLS-K:2011 is guided by a framework of children's development and learning that emphasizes the interrelationships between the child and family; the child and school; the family and school; and the family, school, and community. For this reason, the study collects information about children's experiences in many contexts and on a wide array of topics, including the characteristics 1-2 of the child and the child's family, community, nonparental care and education arrangements, and school and classroom environments. The study pays particular attention to the role that parents and families play in helping children adjust to formal school and in supporting children's education in various ways through the elementary grades. Although the focus of the ECLS-K:2011 is the child, multiple respondents are included in the study in order to obtain accurate and reliable data on the children's experiences in different environments."}, {"section_title": "Periods of Data Collection", "text": "The ECLS-K:2011 is following children from kindergarten (the 2010-11 school year) through the 2015-16 school year, when most of the children are expected to be in fifth grade (Exhibit 1-1). The sample includes both children in kindergarten for the first time and kindergarten repeaters. Although the study refers to later rounds of data collection by the grade the majority of children are expected to be in (that is, the modal grade for children who were in kindergarten in the 2010-11 school year), children will be included in subsequent data collections regardless of their grade level. 2 During the 2010-11 school year, when both a fall and a spring data collection were conducted, approximately 18,000 kindergartners from about 970 schools and their parents, teachers, school administrators, and before-and after-school care providers participated in the study. Fall and spring data collections were also conducted for the first-and second-grade rounds of data collection. Although the fall kindergarten collection included the full ECLS-K:2011 sample, the fall collections for first and second grade were conducted with approximately one-third of the sample of children who participated in the base-year data collection. For third through fifth grade, spring data collections with the entire sample of children who participated in the base-year data collection are planned. 3 2 Children may not be in the modal grade due to retention in a grade or promotion to a higher grade ahead of schedule. 3 Beginning with fall first grade, children who move away from their original base-year schools are sampled for follow-up. Approximately 50 percent of movers will be subsampled out and will stay out of the sample unless they move back into the original sample school. The subsample rate will stay at 50 percent for first and second grade but may be increased in third grade if it is necessary to increase the sample size due to unexpected low response rates. 1 Grade indicates the modal grade for children who were in kindergarten in the 2010-11 school year. After the kindergarten rounds of data collection, children will be included in data collection regardless of their grade level. 2 All but two rounds of data collection include the entire sample of children. The fall first-grade data collection includes approximately one-third of the total ECLS-K:2011 sample of children. The fall second-grade data collection includes the same subsample selected for fall first grade. SOURCE: U.S. Department of Education, Institute of Education Sciences, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011)."}, {"section_title": "Study Components", "text": "The emphasis placed on measuring children's experiences within multiple contexts and development in multiple domains has critical implications for the design of the ECLS-K:2011. Data are collected on a wide array of topics at a broad level rather than on a select set of topics in more depth. The design of the study includes the collection of information from the children, their parents or guardians, their teachers, their schools, and their before-and after-school care providers as described here: \uf06e Children are administered various assessments containing age-and grade-appropriate items measuring important cognitive skills and knowledge in each round of data collection. The untimed assessments are administered directly to the sampled children, one-on-one, by a trained assessor. The kindergarten child assessment measured reading (fall and spring), mathematics (fall and spring), and science (spring) knowledge and skills, as well as executive function (fall and spring). Also in the kindergarten year, Spanish-speaking English language learner (ELL) children who did not achieve a minimum score on assessment items measuring their basic English language skills had their Spanish early reading skills assessed. In addition to the cognitive components, all children had their height and weight measured in the fall and spring. \uf06e Parents or guardians are an important source of information about the study child, the child's family, and the child's home environment. Information is collected from parents in each data collection round using computer-assisted interviews (CAIs). The parent interview asks about family structure, family literacy practices, parental involvement in school, nonparental care arrangements, household composition, family income, parent education level, and other demographic indicators. Parents are also asked to report on their children's health, socioemotional well-being, and disability status."}, {"section_title": "ECLS-K:2011 Data File", "text": "The ECLS-K:2011 kindergarten data file includes the base-year data encompassing both the fall kindergarten and spring kindergarten rounds of data collection. In preparing data files for release, NCES takes steps to minimize the likelihood that individual schools, teachers, parents, or students participating in the study can be identified. Every effort is made to protect the identity of individual respondents. The process of preparing the files for release includes a formal disclosure risk analysis. Small percentages of values are swapped across cases with similar characteristics to make it very difficult 1-5 to identify a respondent with certainty. The modifications used to reduce the likelihood that any respondent could be identified in the data do not affect the overall data quality. Analysts should be aware that the ECLS-K:2011 data file is provided as a child-level data file containing one record for each child participating in the kindergarten-year data collection. The record for each child contains information from each of the study respondents described above: the child, as well as his or her parent, teacher, school administrator, and (if applicable) before-and after-school care provider. However, the ECLS-K:2011 data do generalize to the population of schools educating kindergartners or kindergarten-age children; an analyst interested in a school-level analysis can create a school-level file using the restricted-use kindergarten data file. School-level analysis is possible because schools were a sampling point; later rounds of data collection will not be representative of schools with higher grades. Appendix B to this manual has more information about how to create a school-level file. The ECLS-K:2011 kindergarten data are provided in an Electronic Codebook (ECB) that permits analysts to view the variable frequencies, tag selected variables, and prepare data extract files for analysis with SAS, SPSS, or Stata."}, {"section_title": "Contents of Manual", "text": "The remainder of this manual contains more detailed information about the topics discussed briefly in this chapter, including the data collection instruments and methods (chapter 2) and the direct and indirect child assessments (chapter 3). It also describes the ECLS-K:2011 sample design and weighting procedures (chapter 4), response rates and bias analysis (chapter 5), and data preparation procedures (chapter 6). In addition, this manual explains the structure of the kindergarten data file and the composite variables that have been developed for the file (chapter 7) and describes how to install and use the ECB (chapter 8). The ECB contains unweighted frequencies for variables included in the file. "}, {"section_title": "DATA COLLECTION INSTRUMENTS AND METHODS", "text": "This chapter describes the data collection instruments used in the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011) kindergarten year collection, including the child assessment, parent interview, school administrator questionnaire, teacher questionnaires, and before-and after-school care provider questionnaires. Major differences in the study instruments and data collection procedures for the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K) and the ECLS-K:2011 are discussed. This chapter also provides an overview of the data collection methods employed in the ECLS-K:2011 kindergarten year including staff training, district and school recruitment, child assessment, parent interviewing, and the distribution and collection of school administrator, teacher, and before-and after-school care provider self-administered questionnaires."}, {"section_title": "Data Collection Instruments", "text": "As noted in chapter 1, the design of the ECLS-K:2011 and its survey instruments is guided by a conceptual framework of children's development and learning that emphasizes the interaction among the various environments in which children live and the resources within those environments to which children have access. A comprehensive picture of children's environments and experiences is created by combining information from children themselves, their parents, their school administrators, their teachers, and their before-and after-school care providers. Exhibit 2-1 presents a listing of the ECLS-K:2011 data collection instruments and the rounds of data collection in which they were used. The instruments for the kindergarten year are included on the ECLS-K:2011 CD-ROM and are available online at http://nces.ed.gov/ecls, with the exception of copyrighted materials or items adapted from copyrighted materials that cannot be publicly distributed without copyright holder and NCES permission. Study instruments and items for which copyright permissions are needed are discussed further in section 2.1.7. More information on the assessments can be found in chapter 3. The data from the ECLS-K:2011 instruments can be used to answer a wide variety of research questions about how home, school, and neighborhood factors relate to children's cognitive, social, emotional, and physical development. The following sections describe the major topics covered in each instrument."}, {"section_title": "Direct Child Assessment", "text": "The fall and spring kindergarten rounds of the ECLS-K:2011 data collection included a direct child assessment with cognitive 1 and physical measurement components. The assessment was 1 The selection and testing of items for the cognitive domains is described in detail in The components of the ECLS-K:2011 assessment administered to children who spoke a language other than English at home depended on the children's performance on a language screener used in the fall and spring data collections. The screener consisted of two tasks from the Preschool Language Assessment Scale (preLAS 2000). 3 The \"Simon Says\" task required children to follow simple, direct instructions given by the assessor in English. The \"Art Show\" task was a picture vocabulary assessment that tested children's expressive vocabulary. All children, regardless of home language, were administered the language screener as the first component of the direct cognitive assessment. For children whose home language was English, the screener primarily served as a warm-up or practice for the rest of the assessment since the items were of low difficulty. While the screener also served as a warm-up for children whose home language was one other than English, it also determined whether the children understood English well enough to receive the full direct child assessment in English. All children also received the first 18 items of the reading assessment in English, regardless of their home language or performance on the preLAS tasks. These items, plus two items from the preLAS \"Art Show\" task (a total of 20 items), make up the section of the reading assessment referred to as the English basic reading skills (EBRS) section because they measure such skills. Once the EBRS items were administered, the cognitive assessments in English ended for children whose home language 2 Before the assessments were conducted, data collection staff obtained information about the children's home language from school records, the school staff member assigned to coordinate study activities (referred to as the school coordinator), or the child's teacher. Because parents often were not interviewed before children were assessed in school, parent report of home language could not be used to determine assessment routing. 3 Duncan, S.E. and De Avila, E. A., preLAS 2000 Cue Picture Book English Form C, CTB/McGraw-Hill Companies, Inc., 1998."}, {"section_title": "2-3", "text": "was not English and who did not achieve at least a minimum score on the language screener. 4 Spanishspeaking children who did not achieve at least the minimum score on the screener were then administered a short reading assessment in Spanish that measured Spanish early reading skills (SERS), as well as the mathematics and executive function assessments that had been translated into Spanish. Children whose home language was one other than English or Spanish and who did not achieve at least the minimum score on the screener were not administered any of the remaining cognitive assessments beyond the EBRS. All children had their height and weight measured. asked the child to identify information specifically stated in text (e.g., definitions, facts, supporting details), make complex inferences within and across texts, and consider the text objectively and judge its appropriateness and quality. Yes No 1 Home language was obtained from school records, the school staff member assigned to coordinate study activities (referred to as the school coordinator), or the child's teacher. Because parents often were not interviewed before children were assessed in school, parent report of home language could not be used to determine assessment routing. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2010 and spring 2011."}, {"section_title": "2-5", "text": "As noted above, the first 18 items in the reading assessment and two preLas items constitute a measure of English basic reading skills (EBRS) that all children received regardless of their performance on the language screener. The EBRS functioned as the first set of 20 items in the 40-item routing test. Children who got at least 10 of the 20 EBRS items correct were administered the second set of 20 items in the routing section. Scores on the routing section determined which second-stage test (low, middle, or high) the child received. Spanish speakers who routed out of the English cognitive assessment after the EBRS were administered an assessment that measured Spanish early reading skills (SERS). The SERS consisted of 31 items included in the English reading assessment (in the low or middle secondstage test) that had been translated into Spanish. Mathematics. The mathematics assessment was designed to measure skills in conceptual knowledge, procedural knowledge, and problem solving. The assessment consisted of questions on number sense, properties, and operations; measurement; geometry and spatial sense; data analysis, statistics, and probability; and patterns, algebra, and functions. A set of 18 routing items was administered to all children, and the children's score on these items determined which second-stage test (low, middle, or high difficulty) the child received. Most text that the children could see on the easel pages, for example, question text for word problems or graph labels, was read to them to reduce the likelihood that their reading ability would affect their mathematics assessment performance. 5 Paper and pencil were offered to the children to use for the mathematics assessment, and children were periodically reminded of their availability as part of the assessment protocol. Each of the second-stage mathematics assessment tests also contained several items for which wooden blocks were available for children to use in solving the problems. However, they were not required to the use blocks. Spanish-speaking children who did not pass the language screener completed the full mathematics assessment administered in Spanish.  (Zelazo 2006). In this task, children were asked to sort a series of 22 picture cards into one of two trays according to different rules. Each card had a picture of either a red rabbit or a blue boat; one tray had a picture of a red boat and the other had a picture of a blue rabbit. Children were asked to sort the cards first by color and then by shape. If the child correctly sorted four of the six cards by shape, then he or she moved on to a third sorting rule: if the card had a black border, the child was to sort by color; if the card did not have a black border, the child was to sort by shape. 5 Numbers were read to the child only when the question text referenced the number."}, {"section_title": "2-6", "text": "After the card sort, children were administered the Numbers Reversed task. In this task, they were asked to repeat increasingly long strings of orally presented numbers in reverse order. When children responded incorrectly to a certain number of items in a row, the task ended so that they would not be asked to continue at a level that was too difficult. Spanish-speaking children who did not pass the language screener completed the full executive function assessment administered in Spanish. Science (spring kindergarten). The science domain was added to the cognitive assessment in the spring kindergarten data collection. This assessment domain included questions about physical sciences, life sciences, environmental sciences, and scientific inquiry. The science assessment included 20 items that all children who were administered the science assessment received; a two-stage assessment was not used for this domain. The questions, response options, and any text the child could see on the easel pages (for example, graph labels) were read to the children to reduce the likelihood that their reading ability would affect their science assessment score. Height and weight measurement. In addition to the cognitive domains described above, children's height and weight were measured at each data collection point. Assessors recorded the children's height (in inches to the nearest quarter inch) and weight (in pounds to one decimal place). A Shorr board (a tall wooden stand with a ruled edge, used for measuring height) and a digital scale were used to obtain the measurements, which were recorded on a height and weight recording form and then entered into a laptop computer by field staff. 6 Each measurement was taken and recorded twice to ensure reliable measurement."}, {"section_title": "Parent Interview", "text": "The children in the ECLS-K:2011 come from a broad range of family backgrounds and communities. The parent interviews conducted in fall and spring of kindergarten addressed many important topics. Across the two waves of kindergarten data collection, parents provided information about parent involvement in the child's school; school practices; out-of-school activities; children's nonparental care arrangements the year before kindergarten and during the kindergarten year; the home environment, including family practices such as rules and routines, food security, and discipline; parent and child health and well-being, including the child's level of physical activity and child disabilities; child behavior; household composition and family structure; child and parent characteristics, including the 6 The Shorr board is manufactured by Weigh and Measure, LLC, and is model ICA. The digital scale was Seca Bella model 840."}, {"section_title": "2-7", "text": "primary language spoken in the home, parent education, and parent employment; and the involvement of nonresident parents. Exhibit 2-3 shows the content areas included in the parent interview in the fall kindergarten and spring kindergarten rounds. While many of the same topics were addressed in both fall and spring, there were differences in the specific questions asked within each topic. For example, questions in the parent involvement section in the fall parent interview asked what parents thought children should know or be able to do to be ready for kindergarten, whereas questions in that section in the spring parent interview asked about parent involvement with the school. Some questions were asked at both data collection points, but some were asked in the spring only if the information had not been obtained during a fall parent interview. This might occur because the parent respondent failed to answer a question within an otherwise complete interview or because a fall interview was not conducted. Some sections or topics were included in only one round of kindergarten data collection to limit respondent burden. Other information (e.g., household composition) was confirmed during the spring interview and updated if necessary. The fall and spring parent interviews are provided in appendix A of the CD-ROM. The average length of the parent interview was approximately 45 minutes in both fall and spring kindergarten. The respondent to the parent interview, which was conducted by telephone for most cases, was usually a parent or guardian in the household who identified himself or herself as the person who knew the most about the child's care, education, and health. During the spring kindergarten data collection round, interviewers attempted to complete the parent interview with the same respondent who answered the parent interview in the fall kindergarten round, though another parent or guardian in the household who knew about the child's care, education, and health was selected if the fall respondent was not available. The parent interview was fully translated into Spanish before data collection began and could be administered by bilingual interviewers if parent respondents preferred to speak in Spanish. Because it was cost prohibitive, the parent interview was not translated into other languages. However, interviews could be completed with parents who spoke other languages by using an interpreter who translated from the English during the interview."}, {"section_title": "2-8", "text": "Exhibit 2-3. Parent interview topics, by round of data collection in the ECLS-K:2011 kindergarten year: School year 2010-11 Parent interview content Fall kindergarten Spring kindergarten Child care arrangements, currently and in the year before kindergarten 1 X X Child demographic characteristics X X Child disabilities and services X Child health and well-being X X Child social skills, problem behaviors, and approaches to learning X X Country of origin of parent and child X Family rules and disciplinary practices X Family structure X X Food sufficiency and food consumption X Historical household roster X Home environment, activities, resources, and cognitive stimulation X X Home language 1 X X Involvement of nonresident parent X X Neighborhood safety X Parent characteristics X X Parent-child relationship X X Parent education 1 X X Parent employment X Parent income and assets X Parent involvement with the child's education X X Parent marital history 1 X X Parent respondent's psychological well-being and health X Parental beliefs and expectations related to education X Parental discipline, warmth, and emotional supportiveness X Welfare and other public transfers X X 1 Asked in spring kindergarten if missing from the fall data. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2010 and spring 2011."}, {"section_title": "General Classroom Teacher Questionnaires", "text": "In fall and spring of the kindergarten year, the general classroom teachers of children in the study completed self-administered hard-copy teacher-level questionnaires about themselves and their classrooms, as well as child-level questionnaires about each child in their classroom who was participating in the ECLS-K:2011. 7 The purpose of the teacher-level questionnaire was to collect information about children's classroom experiences that may relate to children's academic and social development. The questionnaire included questions about classroom and student characteristics, class schedules, class materials, instructional practices, and curricula. It also included questions on the teacher's background, teaching experience, and attitudes about teaching and the school climate. The 7 A child was considered to be participating if he or she completed a child assessment or had a parent who completed the parent interview. 2-9 purpose of the child-level questionnaire was to collect information specifically about each study child's experiences and performance in the classroom. It included questions about the teacher's assessment of the child's academic and cognitive abilities, behaviors, social skills, and relationship with the teacher, as well as information about parents' involvement at school and program placements and services that each child may have received. During the spring data collection round, a supplementary questionnaire was distributed to teachers who were new to the ECLS-K:2011 in the spring collection or who had not responded in the fall data collection. 8 The supplementary questionnaire included the background questions that had been asked in the fall teacher-level questionnaire; these items were not asked again in the spring for teachers who responded in the fall. Teacher-level questionnaire content"}, {"section_title": "Fall kindergarten", "text": "Spring kindergarten Classroom and student characteristics X X Instructional activities and curricular focus X Instruction for English language learners X X Content coverage for language arts, mathematics, and science X Resources/materials X Teacher evaluation and grading practices X Parent involvement X Collegial relations and opportunities for professional development X X Teacher's views on teaching, school climate, and environment X X Teacher's experience, education, and background X Teacher supplement for teachers new to the study in the spring: Teacher's background Exhibit 2-5 shows the topics addressed in the fall and spring kindergarten child-level questionnaires by data collection period. During both the fall and spring, teachers reported information about the type of class in which the child was enrolled and rated each study child's skills and behavior. In the spring, teachers also reported on any services the child might have received, specialized programs in which the child might have participated, their specific relationship with each study child, and each study child's parents' involvement. Exhibit 2-5. General classroom teacher child-level teacher questionnaire topics, by round of data collection in the ECLS-K:2011 kindergarten year: School year 2010-11 "}, {"section_title": "Special Education Teacher Questionnaires", "text": "The special education teacher questionnaires were completed in spring kindergarten for each participating child with an Individualized Education Program (IEP) or Individualized Family Service Plan (IFSP). The respondent to the questionnaire could have been a staff member identified as the child's special education teacher, a related service provider if the child was not taught by a special education teacher, or the child's general classroom teacher if that teacher provided all of the child's education and services required by an IEP. As with the general classroom teacher questionnaires, two self-administered hard-copy instruments were used: one to collect information on the special education teacher's background and experience (a teacher-level questionnaire) and one to collect information on the study child's disabilities, placement, and services received (a child-level questionnaire). The teacher-level questionnaire addressed the following: "}, {"section_title": "2-11", "text": "The child-level special education teacher questionnaire addressed the following: Expectations regarding general education goals."}, {"section_title": "School Administrator Questionnaire", "text": "The school administrator questionnaire was a hard-copy paper questionnaire completed by the school principal/administrator or his or her designee during the spring data collection round. It addressed the following topics: with questions specifically about the study child. There were two versions of the care provider questionnaire, one for providers in center-based arrangements and one for providers in home-based arrangements. Some questions asked in each questionnaire were identical to one another but others were tailored to the care setting. For center-based care arrangements, the center director also was asked a brief set of questions about the center setting and staffing in a center director questionnaire. Thus, there were four BASC instruments: a home-based care provider questionnaire, a center-based care provider questionnaire, a center-based center director questionnaire, and a child-level questionnaire that was used in both home-and center-based care settings. The data from these instruments can be used in conjunction with the data obtained in the other ECLS-K:2011 instruments and the direct assessments to answer a wide variety of research questions about how home, school, before-and after-school care settings, and neighborhood factors relate to children's cognitive, social, emotional, and physical development. The BASC questionnaires addressed the following specific content areas: "}, {"section_title": "Copyrighted Materials", "text": "A number of the measures used in the ECLS-K:2011 assessment and questionnaires are taken directly or adapted from copyrighted instruments. Exhibit 2-6 lists these copyrighted instruments and identifies the copyright holder for each. "}, {"section_title": "Data Collection Methods", "text": "The following sections discuss the data collection methods used in the kindergarten year of the ECLS-K:2011. Information is provided on school recruitment (section 2. The ECLS-K:2011 kindergarten data collections were conducted in the fall (August 2010 through January 2011) and spring (March through July 2011) of the 2010-11 school year. Fall data collection activities included interviews with parents and visits to the schools to select children for the study, to collect forms indicating parent consent for the children to participate, to conduct the direct child assessments, and to collect completed questionnaires from general classroom teachers. Spring data collection included interviews with parents and visits to the schools to conduct the direct child assessments and to collect completed questionnaires from general classroom teachers, special education teachers, and school administrators. The spring data collection also included the mailing and receipt of questionnaires from before-and after-school care providers. The modes of data collection were computer-assisted personal interviewing (CAPI) for the child assessments; telephone and in-person computer-assisted interviewing (CAI) for the parent interview; and hard copy self-administered questionnaires for gathering information from teachers, school administrators, and before-and after-school care providers."}, {"section_title": "Study Endorsement and School Recruitment", "text": "Prior to recruitment for the study, key educational organizations were contacted and asked for an endorsement, as it was believed that having these organizations support the study would help with efforts to recruit schools and families into the study. The ECLS-K:2011 received the endorsement of many national associations and organizations representing parents, school administrators, teachers, and private religious and nonreligious schools. Once the endorsements were received, letters describing the study were prepared on letterhead that noted all the endorsing organizations, and these letters were sent to educational staff at various organizational levels to inform them about the planned study data collections. For data collection efforts to begin in schools sampled for the study, it was necessary to secure a commitment to participate in the study from the school's administrator. However, before school administrators were contacted about the study, staff at higher organizational levels were contacted to determine whether they would have any objections to the study being conducted in schools within their purview and also to answer any questions 2-15 they may have had about the study. The levels of contact varied for public, Catholic, and non-Catholic private schools. Public schools had three levels of contact-state, school district, and school; Catholic schools had two levels-diocese and school; and non-Catholic private schools had one-the school. The process of notifying states, districts, dioceses, and non-Catholic private schools began in fall 2009. Contact with public and Catholic schools began in February 2010."}, {"section_title": "State-Level Contacts for Public Schools", "text": "Letters were sent to the Chief State School Officer, testing director, and early childhood program director (if one was identified) of each state that contained the ECLS-K:2011 sampled schools to explain the objectives of the study and the data collection procedures, in particular those for protecting individual and institutional confidentiality. Once contact was completed at the state level, contact was made with public school district superintendents."}, {"section_title": "District and Catholic Dioceses Contacts", "text": "For public schools, a package containing a letter describing the study, a study brochure, a timeline of data collection activities, a summary sheet prepared for parents, and a list of the sampled schools within the district was sent to the district superintendent. A similar package of materials was sent to the Catholic dioceses and archdioceses in the sample to obtain permission to contact Catholic schools about the study. Beginning in late September 2009, calls were placed to the selected district superintendents and Catholic dioceses to explain the study, answer questions, and obtain permission to contact sampled schools within the district or diocese to secure the schools' participation. Once approval was obtained at the district or diocesan level, contact was made with each school administrator."}, {"section_title": "School Contacts", "text": "A letter and other study materials were mailed to school administrators in all the sampled schools in February 2010. Once the study materials were mailed, data collection staff began contacting school administrators by telephone to answer any questions they might have about the study and to secure their schools' participation. These telephone contacts began in February 2010 and continued through the end of the school year in June 2010; school recruitment continued in fall 2010 with the start of the new school year. Once the school administrator agreed to participate, he or she was asked to set an 2-16 appointment for two visits by the ECLS-K:2011 field staff. The purpose of first visit, the preassessment visit, was to select the sample of children within the school (see section 2.2.3 for more detail on this visit), and the second visit was to conduct the child assessments (see section 2.2.4.1 for more detail on this visit). The school administrator was also asked to identify an individual, referred to as the school coordinator, to act as the school liaison with the ECLS-K:2011 staff."}, {"section_title": "Efforts to Achieve High Participation Rates at the School Level", "text": "Recruitment for the study began a year before the start of data collection to ensure that study staff had ample time to contact staff at the various educational levels noted above, answer questions about the study, and attempt to secure participation at the school level, including any effort that was needed to convince reluctant school administrators to participate. A small group of staff with experience successfully recruiting schools and school districts to participate in other national studies, including the ECLS-K, worked to convert sampled schools that had initially refused to participate. In May 2010, it was determined that the number of schools that had agreed to participate was probably too low to achieve the desired number of participating schools before data collection began at the end of August 2010. For this reason, an additional sample of schools, referred to as substitute schools, was selected for recruitment into the study. Several waves of substitute schools were added between May and August 2010 (more information on school substitution can be found in chapter 4 section 4.1.2.8). To further increase the number of participating schools, an additional wave of substitute schools was selected for recruitment beginning in January 2011, before the spring kindergarten data collection began. The same levels of contact and procedures used in recruiting originally sampled schools were used with substitute schools."}, {"section_title": "Field Staff Training", "text": "In-person training sessions were conducted to prepare field staff for the kindergarten data collection tasks; each training session had a home study component that included review of a field manual detailing study procedures and staff responsibilities. Three training sessions for the fall kindergarten round were conducted: one for staff recruiting schools into the study, one for data collection team leaders, and one for team leaders and assessors. Team leaders managed the data collection activities within their assigned regions, supervised assessors, and conducted child assessments and parent interviews. Assessors conducted the child assessments and the parent interviews. Two training sessions for the spring kindergarten round were conducted: one for continuing staff (i.e., staff who worked on the study in the 2-17 fall kindergarten round) and one for new staff. There were no new recruiters hired for the spring kindergarten collection; the few new team leaders were trained via home study."}, {"section_title": "School Recruitment Training", "text": "School recruitment staff-primarily field managers (staff who supervised multiple teams in a data collection region), team leaders, and supervisors with experience recruiting schools or working on other educational studies such as the ECLS-K, the National Assessment of Educational Progress (NAEP), and the Third International Mathematics and Science Study (TIMSS)-were trained in two sessions. Public school district and private school recruiters were trained in October 2009. In February 2010, training was held for recruiters of public and Catholic schools. The topics covered in the training included an introduction to the study, practice exercises in recruiting schools, refusal avoidance techniques, and exercises on scheduling schools efficiently within the group of cases assigned to a team. Exercises on scheduling schools covered topics such as scheduling preassessment visits and assessment visits so as to minimize downtime for the team between schools, avoid overlapping school visits, allow sufficient time between the preassessment and assessment date, as well as to minimize travel for work areas where a trouble shooter is needed."}, {"section_title": "Fall 2010 Field Staff Training", "text": "In August 2010, two training sessions were held to prepare for the fall 2010 data collection: one for team leaders and one for team leaders and assessors. Team leaders attended the second training so they could meet the assessor staff they would be managing and to be trained on all aspects of the study. In addition, staff identified as troubleshooters were trained in both training sessions so that they could step in for team leaders or assessors, as necessary, as well as conduct child assessments and parent interviews. Team leaders and assessors were trained in person over a period of 10 days in August 2010 (3 days of training for team leaders, 6 days of training for both team leaders and assessors, and 1 additional day of training for bilingual assessors). Prior to the in-person training, each staff member completed 8 hours of home study training on the study design, field procedures, and techniques in CAI. Team leaders completed an additional 8 hours of home study on managing the field work. Table 2-1 shows the numbers of field staff trained before each wave of data collection. "}, {"section_title": "2-18", "text": ""}, {"section_title": "In-Person Team Leader Training", "text": "Team leader training preceded the assessor training and lasted 3 days. Topics covered included an introduction to the study, an overview of recruitment activities, contacting school coordinators, collecting information to select the sample in each school, performing within-school sampling, preparing parent consent packets for the schools to distribute, and entering sampled child and teacher data into the Field Management System (FMS). The FMS collects information at multiple levels (i.e., school level, teacher level, and child level) to manage field work and report progress."}, {"section_title": "In-Person Assessor Training", "text": "The assessor training sessions lasted 6 days and focused on administration of the parent interview (2 days) and the child assessments (4 days). In-person training sessions included an overview of study activities, interactive lectures 9 on conducting the parent interview and the direct child assessment, practice parent interviews conducted in pairs using role-play scripts, practice direct child assessments 9 Interactive lectures are those involving interview or assessment scripts in which the trainees adopt the role of the interviewer or assessor and practices administering the instrument while the trainer both fills the role of respondent and gives training points throughout the administration of the instrument."}, {"section_title": "2-19", "text": "using role-play scripts, techniques for avoiding refusals from parents, and strategies for building rapport with children. Trainees practiced entering information into the CAI system on laptop computers during the training presentations. Training for the parent interview focused on instructions for selecting the correct respondent to provide consent for the child to participate in the study and to complete the parent interview, as well as proper administration techniques, such as reading the questions verbatim, when to read response categories to respondents, and remaining neutral when asking sensitive questions or speaking with difficult respondents. Training for the child assessment focused on following standardized procedures for administration of all the assessment items, including reading the questions verbatim; avoiding giving the child feedback on his or her responses, either verbally or through nonverbal cues; and responding appropriately to children's behaviors. The sessions provided trainees with hands-on experience with all the direct child assessment materials and procedures and the CAI program prior to data collection. Spanish-speaking bilingual trainees spent a 7th day at in-person training to learn about and practice the Spanish-language versions of the parent interview and the child assessment."}, {"section_title": "Certification of the Assessors", "text": "In order to ensure that the data collection staff who completed training administered the direct child assessments in a standardized manner, staff had to show competency through certification exercises. Certification consisted of written exercises on each section of the reading and math child assessments, written exercises on the executive function tasks, and an observation of each trainee administering the assessment to a child specifically recruited for certification purposes. Written certification exercises. Each section of the assessments for reading and math and the two executive function tasks was reviewed in detail during an interactive lecture. Each interactive lecture was followed by practice in dyads using role-play scripts. After the practice, written exercises were distributed. The written exercises were used to ensure that each trainee understood the administration and coding rules for select questions with particularly complex administration and scoring rubrics. The exercises were collected and scored the same day. Trainees who did not achieve a passing score were asked to attend a help session that evening to review the items they answered incorrectly. These trainees then repeated the same exercises that they had previously failed. Child assessment certification. In the final stage of the certification process, the trainees were observed conducting a direct child assessment with children brought on site to the training session."}, {"section_title": "2-20", "text": "Training staff who were already certified on the assessment observed trainees as they administered parts of the assessment to kindergarten-age children. The observers used a certification form to make general notes and track administration of selected items. They also rated the trainees on skills such as rapport with the child, avoidance of coaching, following proper administration procedures, and pacing. While the trainees administered the assessment, the observer simultaneously coded the child's answers to preselected questions. After the assessment was completed, a screen was brought up in the CAI program that displayed the assessor's coding of child responses to the selected questions. The answers recorded by the assessor were compared with those recorded by the observer. Discrepancies between the child's actual response, as recorded by the observer, and the assessor's recorded answers affected the assessor's overall score on the certification form. Trainees who scored at least 85 percent of possible points on the certification form were certified to administer the child assessments. Trainees who scored between 70 and 84 percent were required to complete remedial training after the in-person training under the supervision of the team leader until the team leader judged that the assessor was ready to conduct an observed assessment during the data collection. Trainees scoring less than 70 percent were released from the study."}, {"section_title": "Spring 2011 Training", "text": "Most staff trained for the fall data collection also collected data during the spring data collection. However, there was some data collection staff attrition, so 39 new field staff were hired for the spring data collection period. The new field staff members completed 8 hours of home study activities and then were trained in person in March 2011. They were trained on the child assessments and the parent interviews following the same agenda (with the addition of training on the science assessment, which was new to the spring data collection) and certification procedures used in the fall 2010 field staff training. Four of the new staff were team leaders, and, in addition to the in-person assessor training, they were trained via home study on team leader responsibilities and procedures. The team leader training generally followed the training agenda used in the fall 2010 team leader training, though there were two primary differences. Because most of the child sample selection had been completed in the fall, the new team leaders were not trained in the child sampling procedures. Also, information about how to handle cases of children who moved from their sample school between the fall and spring data collections was added. Returning field staff members (112 team leaders and troubleshooters and 140 assessors) were trained via home study in March 2011. The home study training on the child assessment included watching videos, reading sections of their field manual, written exercises, solo practice of the assessment, 2-21 moderated discussions with their field managers, and practice with a kindergarten-age child who was not part of the national study. The child assessment home study took approximately 7 hours to complete. The home study training on the parent interview included watching videos, reading sections of their field manual, written exercises, solo practice of the parent interview, and dyad role-plays with another field interviewer over the telephone. The parent interview home study took approximately 7 hours to complete. Returning team leaders and troubleshooters received an additional 8-hour refresher training on their supervisory responsibilities, including reviewing the team leader manual; reading new information about how to handle cases of children who moved from their sample school between the fall and spring data collections; reviewing assignment materials; reviewing the FMS; completing exercises on calling schools; and moderated discussion with their field managers about the new team leader responsibilities in the spring via conference call. Topics covered in this moderated discussion included preassessment activities (calling the school to confirm information collected in the fall and confirming receipt of questionnaires mailed from the home office) and the transfer school and child procedures (identifying transfer children, collecting new information, using the screens in the field management system, and fielding new transfer cases)."}, {"section_title": "Fall Preassessment Visit", "text": "Beginning in August 2010, an advance package was mailed via FedEx to each participating ECLS-K:2011 school. During recruitment in early 2010, the schools were asked to identify a school staff person to act as the school coordinator who would serve as a liaison with the study. The advance package was directed to the named school coordinator. The package contained instructions and forms for collecting kindergarten enrollment information in preparation for child sample selection during a preassessment visit by the team leader. The package also included informational brochures for staff and parents at the school. At the beginning of the fall data collection, team leaders contacted the schools in their assignments to introduce themselves, confirm the preassessment visit appointment date set by the recruiter at the time the school agreed to participate, and answer any questions about the study. During the preassessment visit, the team leader listed all children enrolled in kindergarten in the school and, using a sampling program on the study laptop, randomly selected a sample of 23 children. All kindergartners in a school were selected if the school's kindergarten enrollment total was 27 or fewer students."}, {"section_title": "2-22", "text": "Once children were selected, the team leader prepared parent consent packets, including a parent letter, consent form, and brochure, for the school to distribute to the parents of the sampled children. The team leader also determined the classroom assignments of the sampled children and prepared teacher packets to distribute to the sampled children's teachers at the preassessment visit. These packets contained a letter, brochure, and timeline of study activities, as well as the teacher-and childlevel questionnaires. Each teacher received a child-level questionnaire for each study child in his or her classroom, as well as one teacher-level questionnaire. Finally, the team leader confirmed the scheduled school assessment visit, the date when the team would visit the school to conduct the assessments. The team leader worked with the school coordinator to identify the best locations for conducting the child assessments. Between the preassessment visit when children were selected for the study and the assessment visit when the assessments were conducted, the team leader stayed in touch with the school coordinator to monitor the collection of the parent consent forms."}, {"section_title": "Data Collection", "text": "During both the fall 2010 and spring 2011 data collection periods, the field staff were organized into data collection teams consisting of one team leader and two or more assessors. All team members participated in the assessment visit to the school, during which the direct child assessments were conducted with the sampled children for whom parent consent to participate had been obtained. Completed teacher questionnaires also were collected during the assessment visit. All team members also conducted parent interviews, which were generally conducted by telephone, outside the school. During the fall data collection, child assessments were conducted from August through mid-December 2010, and parent interviews were conducted from August 2010 through mid-January 2011. During the spring data collection, child assessments were conducted from March through June 2011, and parent interviews were conducted from March through early July 2011. Child care providers of sampled children who were in nonparental care 5 hours or more per week were identified from the fall parent interview; data were collected from these providers during the spring data collection."}, {"section_title": "2-23", "text": ""}, {"section_title": "2.2.4.1", "text": ""}, {"section_title": "Conducting the Direct Child Assessments", "text": "In both fall and spring, the direct child assessments were usually conducted in an unoccupied school classroom, an unoccupied meeting room, or the school library. Before conducting the assessments each day, team leaders and assessors set up the room(s) for the assessments, which included arranging tables and chairs in a way that allowed children to concentrate on the questions being asked without being distracted by other study children being assessed at the same time and setting up a station where children's height and weight could be measured. Each child was signed out of his or her classroom prior to the assessment and signed back into the classroom upon conclusion of the assessment. The child assessment was designed to take, on average, 1 hour to complete. In each school, at the conclusion of the fall 2010 assessment, team leaders scheduled an appointment for the spring assessment visit. In the spring, team leaders made preassessment calls to the schools in their assignments prior to the scheduled assessment date to confirm the date and assessment logistics and prompt for completion of school and teacher questionnaires."}, {"section_title": "Collecting the Teacher and School Administrator Questionnaires", "text": "During the fall data collection, the self-administered hard-copy teacher-level and child-level questionnaires were collected from the child's general classroom teacher. The spring data collection questionnaires included those from the child's main classroom teacher and, if applicable, from a special education teacher who was assigned to work with the child (if he or she had an IEP). The special education teacher completed both teacher-level and child-level questionnaires. In addition, a selfadministered school administrator questionnaire was completed at each school, usually by the school administrator. During the preassessment call before spring data collection began, the team leaders reminded the school coordinators to gather completed questionnaires from teachers and school administrators so that the team leader could collect them during the assessment visit. If the questionnaires were not completed by the end of the assessment visit, team leaders attempted to make arrangements to return to the school to pick up the questionnaires once completed."}, {"section_title": "Conducting the Parent Interviews", "text": "In both rounds of data collection, the parent interview was administered to most parents (92 percent in the fall and 88 percent in the spring) by telephone, using CAI. The parent interview was 2-24 conducted in person if the respondent preferred it (for example, if the respondent was concerned about using minutes on a cellular telephone plan for the interview) or if the respondent did not have a telephone. Assessors attempted to complete the parent interview with the parents of the children they themselves had assessed at school. Assessors also offered to conduct the interview at a time that was most convenient for parents and in more than one phone call if parents could not complete the entire interview all at once. Interviews were conducted in English and Spanish in accordance with the parents' language preference; 90 percent of the parent interviews were conducted in English in the fall, and 88 percent were conducted in English in the spring. The Spanish interviews (9 percent in fall and 11 percent in spring) were administered by bilingual interviewers. A few interviews were completed with parents who spoke other languages by using an interpreter who translated from the English during the interview (less than 1 percent in fall, 1 percent in spring). Both the fall and spring parent interviews averaged 45 minutes. When interviewers first called the children's homes, they initially asked to speak to the contact person identified by the school. If that person no longer resided with the child or was not available during the field period, the interviewer asked for another respondent. Specific questions were then asked to determine that the respondent was a knowledgeable parent or guardian who lived with the child. If there was no parent or guardian in the household who knew about the child, a household member who was 18 or older and knew about the child was asked to complete the interview. Most respondents to the fall parent interview were mothers (87 percent) or fathers (10 percent). In about 3 percent of cases, another relative or a nonrelative responded to the parent interview. During the spring data collection round, interviewers attempted to complete the parent interview with the same respondent who answered the parent interview in the fall round, if one had been completed. However, if the fall parent interview respondent was no longer a member of the household, was not available during the field period, or could not be contacted after multiple attempts, the interviewer asked for another adult household member who could complete the parent interview, using the same criteria: a parent or guardian in the household who knew about the child's care, education, and health. The fall and spring parent interview respondents were the same for about 96 percent of cases with completed interviews in both rounds."}, {"section_title": "Before-and After-School Care (BASC) Data Collection", "text": "Before-and after-school care (BASC) providers of sampled children who were in nonparental care 5 or more hours per week were identified during the fall parent interview. To be eligible"}, {"section_title": "2-25", "text": "for the BASC data collection, the care provider had to be 18 years old or older. If the child had more than one care provider, then the provider with whom the child spent the most time during the week was selected for the BASC data collection. If the child spent an equal amount of time with two or more care providers during the week, then one care provider was randomly selected. Parental consent to contact the child's care provider was requested on the same consent form used to obtain permission for the child's participation in the study that was included in the parent information packets distributed by the school. Information was collected from the care providers using self-administered hard-copy questionnaires mailed directly to the providers. Providers were classified as either center-based or home-based, depending on whether they provided care in a center-based setting (e.g., at a school, recreation center, or child care facility) or in a private home. Each provider received a provider-level questionnaire with questions tailored to the type of setting in which he or she provided care. All providers received one child-level questionnaire per study child in their care. In addition, administrators of centers attended by study children were asked to complete a self-administered questionnaire. The packages were mailed to before-and after-school care providers starting in April 2011 and included a letter, signed parent consent forms to assure the child care provider that the children were part of the study, the relevant questionnaires and a return FedEx mailer. Questionnaires were received from before-and after-school providers from April until the end of July 2011. All providers not responding by mail were prompted by staff in Westat's Telephone Research Center (TRC) to return the completed questionnaires. TRC staff called providers who had not yet returned completed questionnaires and attempted to complete the questionnaires over the telephone."}, {"section_title": "Tracing Activities", "text": "In the fall 2010 round of data collection, schools were able to provide contact information for most of the children's parents or guardians. However, sometimes contact information was missing or no longer correct by the time field staff tried to contact parents, and field staff had to conduct online searches for parent telephone numbers and addresses or visit the neighborhood in which the address provided by the school was located to find parents. In spring 2011, school coordinators were mailed packages with a list of the sampled children at their school. As part of the preassessment call to each school, team leaders reviewed the list with the school coordinator to determine if the sampled children still attended that school. If a sampled child had left the school after the fall visit, the school coordinator was asked for any updated contact information the school may have had, including the name of the child's new school and new contact information for 2-26 the parent. All children who moved out of their original sample school but still lived within one of the study's primary sampling units (PSUs) were followed for data collection; for children who moved outside the PSUs, an attempt was made to conduct the parent interview but no other study activities were completed. Approximately 350 children who were participants in the fall were not included in the spring collection because they moved out of a PSU included the study. If a sampled child transferred to a school in a study PSU but the school was not already participating in the ECLS-K:2011, the new school needed to be recruited for the study. The first step for this recruitment was to contact the new school's district or diocese (if applicable) and inform it of the transfer of an ECLS-K:2011 student into one of its schools. If the district or diocese was not already participating, the necessary recruitment steps were completed. If the district or diocese was already participating, this contact was informational. After district/diocese contact, a team leader contacted the school to ask the administrator to participate so that an assessor could visit the school to conduct the child assessment and obtain completed teacher questionnaires. About 820 children who were participants in the fall had transferred to a different school by the spring. Approximately 10 percent of the transfer school children moved to a school that refused to participate. In these cases, the parent was contacted to make arrangements to conduct the child assessment in the home or some other location outside the school."}, {"section_title": "Data Collection Quality Control", "text": "Continuous quality assurance procedures were employed throughout all stages of data collection (e.g., in the staff training program, through assessment certification, and as part of the ongoing staff observations and evaluation activities). During assessor training, field staff practiced conducting the parent interview in pairs and administered the direct child assessments with kindergarten-age children brought to the training site for this purpose. The supervisors and assessors were certified on the child assessments. When the fieldwork began, team leaders observed each assessor conducting at least two child assessments; home office staff validated parent interviews by reviewing audio recordings; and field managers made telephone calls to the schools to collect information on the school activities for validation purposes."}, {"section_title": "Child Assessment Observations", "text": "Team leaders and home office staff conducted in-field observations of child assessments conducted by assessors during data collection. In each round, each assessor was observed administering 2-27 two assessments. The first observation was scheduled to be conducted within the first 2 weeks of data collection, and the second observation was scheduled to be conducted about midway through the data collection period. The same assessment certification form used to certify assessors at training was used to evaluate the assessor's performance conducting the child assessments during data collection. The assessor was rated in three areas: 1. Rapport building and working with the child-praising the child for attending to the tasks and working on the tasks (i.e., praising the child's effort rather than the performance) and the assessor's response to various child behaviors."}, {"section_title": "2.", "text": "Cognitive assessment activities-reading questions verbatim, the use of acceptable probes, the use of appropriate hand motions for items requiring gestures, and the absence of coaching the child to assist with his or her performance.\nSelect the Control Panel tab.\nClick the Go button.\nIn the Search Text Box (shown in exhibit 8-21), type in \"edu\" and then click the Search button.\nSelect the Add option to display a list of previously saved taglists, shown in exhibit 8-27.\nClick the Save or Save As button above the Working Taglist column. You can also select the Save or Save As options from the Taglist pull-down menu.\nClick the Save As button above the Working Taglist column. You can also click the Taglist pull-down menu and select the Save As option.\nClick the Taglist pull-down menu (exhibit 8-31) and select the Export option.\nThe Working Taglist will be replaced by a new taglist.\nGo to the Codebook View screen as detailed above and select the Printer icon.\nClick the Codebook pull-down menu and select the View option. The codebook for the current taglist opens in a new window, similar to the one shown in exhibit 8-38.\nWith the Variable Quick View being the active window on top, press <Alt> + <Print Screen> to save the image of the Variable Quick View window."}, {"section_title": "3.", "text": "Specific assessment activities-correctly coding answers to open-ended questions in the assessments and following administration procedures. During the observation, the team leaders recorded their observations on the form and then reviewed the form with the assessor being observed. Feedback was provided to the assessors on the strengths and weaknesses of their performance and, when necessary, remedial training on weak performance was provided by a team leader under direction by the field manager. After completing the remedial training, additional observations were scheduled to ensure that the weaknesses were addressed. The completed observation forms were sent to a field manager who reviewed the forms and passed them on to the field directors for storage at the home office. All field staff who completed at least two observations earned passing scores, including those who completed the remedial training.\nIn the Control Panel window, click the Programs and Features icon.\nThe Variable List will then scroll down automatically to show the selected variable.\nThe new Variable List will include only the variables that have the text \"edu\" in the variable name or the variable description. The catalog-specific topical variable groupings can be found in appendix E on the CD-ROM. Simply find the topic of interest in the Topic column first and then enter in the Search Text Box the matching keywords in the Variable Identifier to narrow the search.\nHighlight the saved taglist whose variables you wish to add to your Working Taglist.\nThe Save Taglist As dialog box appears as shown in exhibit 8-29.\nThe Save Taglist As dialog box appears, shown in exhibit 8-30, with the current taglist name in the Taglist Name field.\nThe Export Working Taglist To dialog box appears (exhibit 8-32).\nClick the Taglist pull-down menu and select the Delete option.\nThe Print dialogue box, shown in exhibit 8-40, will appear. Enter the print range and number of copies. Note that the codebook prints on your PC's default printer.\nWhen you are done reviewing the variable information, close the window by clicking the \"X\" in the upper right corner of the screen. This will return you to the main screen."}, {"section_title": "Quality Control of Parent Interviews", "text": "Approximately 10 percent of the respondents who completed parent interviews were selected for validation to assure quality data collection. Validations were selected to include the first parent interview completed by an assessor and then every 10th completed interview thereafter. Over the course of the field period, a running count of an assessor's completed parent interviews was maintained, and each 10th completed parent interview was selected for validation, thus ensuring that about 10 percent of each assessor's cases were selected for validation. To validate the parent interviews, a Telephone Research Center (TRC) staff member listened to an audio file that included sections of a parent interview recorded by the laptop while the interview was being conducted. The TRC staff reviewed the answers that had been recorded and compared them to the 2-28 responses entered by the assessor to determine if there were any differences between the recorded responses and the CAI-entered responses. If a difference was identified, field management staff were immediately alerted to discuss any issues with assessors and correct problems. In addition, interviewers were rated on their skill in conducting the parent interview. Feedback was provided to the field staff to both individuals and all staff through the weekly field communication."}, {"section_title": "Validation of School Visits", "text": "To ensure that assessment visits in the schools were proceeding smoothly, field managers conducted school validations by telephone with school administrators in at least two of each team leader's assigned schools in the fall. Another two schools per team leader were validated in the spring data collection. This was approximately 20 percent of each team's assignment per round of data collection for approximately 400 total school validations. The first school visited by each team was called to ascertain how well the preassessment and assessment activities went, and then the fifth school that the team visited was called for the second validation. Field managers used a standardized script to call the school administrators. The script covered the following topics: an overall rating of how the assessments went; feedback about the study from the children and teachers; suggestions for improving procedures and making it easier for a school to participate; and general comments and suggestions. In the fall, the feedback from the validation calls with each team's first and fifth schools was positive in all but two schools. The team leaders in those two schools were released from the study. In the spring, the feedback from all of the schools was positive."}, {"section_title": "Differences Between the ECLS-K and ECLS-K:2011", "text": "To a great extent, the ECLS-K:2011 draws from the domains and measures developed for the ECLS-K to allow for comparisons of the characteristics and experiences of the two cohorts of children who were in kindergarten more than a decade apart. Much of the studies' content and data collection procedures are the same or similar because the instrumentation and procedures used in the ECLS-K served as the starting point for development of the ECLS-K:2011. However, the ECLS-K:2011 is not a replication study. During the design phase for the ECLS-K:2011, content and assessment experts, education policymakers, and other specialists were consulted for their input on ways in which the ECLS-K instrumentation and procedures could or should be changed to ensure that the information 2-29 collected reflects current knowledge and issues in education. This section summarizes major areas of difference between the two studies. Direct child assessment. There are six general areas of difference between the kindergarten assessment in the ECLS-K and the ECLS-K:2011."}, {"section_title": "School administrator questionnaires. Changes in education policy and current issues of", "text": "interest necessitated some changes to the topics included in the school administrator questionnaire. For example, the implementation of policies related to No Child Left Behind led to the collection of information about Adequate Yearly Progress (AYP) in the ECLS-K:2011. Title III services for English language learners were a new area of interest. Other items related to school attendance, school choice, school breakfast and lunch programs, and school-family connections were either new for the ECLS-K:2011 compared to the ECLS-K or were revised from the way they were asked in the ECLS-K. Teacher questionnaires. Items also were added to the teacher-level questionnaire to address topics of current policy interest including materials and services for ELL students, methods of student assessment, the use of homework in kindergarten, and whether teachers were considered \"highly qualified\" according to the criteria for their state. Items related to the teachers' credentials and licensing that were not included in the ECLS-K kindergarten teacher questionnaires were asked in the ECLS- Scale (Pianta and Steinberg 1992). Some items were also added to the student information section of the instrument, including enrollment-related questions and items about programs for ELL students. Special education teacher questionnaires. The ECLS-K:2011 incorporated additional measures of teacher educational background, certification/licensure, and professional experience. Additional items about services and direct/indirect service delivery also were added to the child-level instrument. Before-and after-school care questionnaires. Information about children's current beforeor after-school care arrangements was not collected directly from care providers in the ECLS-K. The ECLS-K:2011 included questionnaires for these care providers in the spring kindergarten data collection in order to capture more information about the study children's experiences with caregivers other than their parents. These questionnaires were modeled after the nonparental care provider interviews conducted in the Early Childhood Longitudinal Study, Birth Cohort (ECLS-B)."}, {"section_title": "Student record abstract.", "text": "In the ECLS-K, some information about children was collected from student records using a standard abstracting form. Field staff used this data collection form to record information from the student record and sent this form to the home office with other data collection materials. This form was not used in the ECLS-K:2011. Some of the information that had been collected in the abstract in the ECLS-K (e.g., absences from school, whether the child's home language was English, whether the child had an IEP) was collected in the ECLS-K:2011 child-level teacher questionnaires. Facilities checklist. Field staff in the ECLS-K completed a series of questions about their observations in and around the school building in an instrument referred to as a facilities checklist. The ECLS-K:2011 did not include this instrument. The school administrator questionnaire in the ECLS-K:2011 covered much of the information that had been collected through the ECLS-K facilities checklist."}, {"section_title": "Supplemental studies. The Head Start verification study and the Salary and Benefits", "text": "Questionnaire component included in the base year of the ECLS-K were not a part of the ECLS-K:2011. Twins. In the ECLS-K, both twins in a twin pair were included in the sample if at least one of the twins had been sampled for the study. In the ECLS-K:2011, both twins were included only if both twins were separately sampled. If one twin was sampled and one was not, only the sampled twin was included in the study. In the ECLS-K, the parent interview was structured and programmed so that the full interview was conducted for one twin and then only child-specific questions were asked about the second 2-32 child. In the ECLS-K:2011, if both twins in a family happened to be sampled, the entire interview was to be administered for each child, but field staff were instructed to manually skip through sections collecting information that would be the same for each child (such as household and parent information) and only ask questions in the child-specific sections about the twin in the second interview. However, without such skips being programmed in the CAI instrument, this procedure was difficult to follow in the field. Users who wish to conduct analyses with the 98 households with sampled twins should review those data carefully."}, {"section_title": "2-33", "text": "This page intentionally left blank."}, {"section_title": "ECLS-K:2011 DIRECT AND INDIRECT ASSESSMENT DATA", "text": "This chapter provides information about the direct and indirect assessment data from the kindergarten year of the ECLS-K:2011. The chapter begins with a description of the direct cognitive assessments, providing information about the scores available on the data file. The chapter then presents information on the executive function assessments. Finally, the chapter closes with information on teacher and parent assessments of children's cognitive and socioemotional knowledge and skills."}, {"section_title": "Direct Cognitive Assessment: Reading, Mathematics, Science", "text": "The kindergarten direct cognitive assessments measured children's knowledge and skills in reading, mathematics, and science. This section presents information about the assessment scores available on the data file. More detailed information about the development of the scores, including a more complete discussion of Item Response Theory (IRT) procedures, can be found in the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11, Kindergarten Psychometric Report (Najarian et al. forthcoming). A description of the administration of the direct assessments is provided in chapter 2, section 2.1.1. It must be emphasized that the assessment scores described below are not directly comparable with those developed for the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K). Although the IRT procedures used in the analysis of data were similar in the ECLS-K and in the ECLS-K:2011, each study incorporated different items and the resulting scales are different. A subsequent release of the ECLS-K:2011 data will include IRT scores that are comparable with the ECLS-K cohort."}, {"section_title": "IRT-Based Scores Developed for the ECLS-K:2011", "text": "Broad-based scores using the full set of assessment items in reading, mathematics, science, and Spanish Early Reading Skills (SERS) were calculated using IRT procedures. IRT is a method for modeling assessment data that makes it possible to calculate an overall score for each domain measured for each child that can be compared to scores of other children regardless of which specific items a child is administered. This method was used to calculate scores for the ECLS-K:2011 because, as discussed in chapter 2, the study employed a two-stage assessment in reading and mathematics in which children were administered a set of items appropriate for their demonstrated ability level, rather than all the items in the assessment. Although this procedure resulted in children being administered different sets of items, there was a subset of items that all children received (the items in the routing tests, plus a set of items common across the different second-stage forms). These common items were used to calculate scores for all children on the same scale. Similarly, for the single-stage science and SERS assessments, IRT was used to calculate scores for all children on the same scale. In the single-stage forms, the assortment of items a child received was not dependent upon routing to a second stage, but instead on omissions by the child or the discontinuation of the assessment form. In those cases, IRT is used to estimate the child's probability of a correct response when no response is indicated. IRT uses the pattern of right, wrong, and omitted responses to the items actually administered in an assessment and the difficulty, discriminating ability, 1 and \"guess-ability\" of each item to estimate each child's ability on the same continuous scale. IRT has several advantages over raw number-right scoring. By using the overall pattern of right and wrong responses and the characteristics of each item to estimate ability, IRT can adjust for the possibility of a low-ability child guessing several difficult items correctly. If answers on several easy items are wrong, the probability of a correct answer on a difficult item would be quite low. Omitted items are also less likely to cause distortion of scores, as long as enough items have been answered to establish a consistent pattern of right and wrong answers. Unlike raw number-right scoring, which treats omitted items as if they had been answered incorrectly, IRT procedures use the pattern of responses to estimate the probability of a child providing a correct response for each assessment question. Finally, IRT scoring makes possible longitudinal measurement of gain in achievement, even when the assessments that are administered to a child are not identical at each point (for example, when a child was administered a different level of the second-stage form of a given domain in the spring data collection than in the fall data collection)."}, {"section_title": "Theta and the Standard Error of Measurement (SEM) of Theta", "text": "On the ECLS-K:2011 base-year data file, a theta score is provided for each child who participated in the direct cognitive assessment for each cognitive domain assessed. The theta score 2 is an estimate of a child's ability in a particular domain (e.g., reading, mathematics, science, or SERS) based on his or her performance on the items he or she was actually administered. Theta scores for reading, mathematics, and SERSs are provided in the data file for the fall and spring kindergarten data collection rounds. A science theta score is provided for spring kindergarten only because the science assessment was not administered in the fall. The theta scores are reported on a metric ranging from -6 to 6, with lower scores indicating lower ability and higher scores indicating higher ability. Theta scores tend to be normally distributed because they represent a child's latent ability and are not dependent on the difficulty of the items included within a specific test. The standard error of theta provides a measure of uncertainty of the theta score estimate for each child. Adding and subtracting twice the standard error from the theta score estimates provides an approximate 95 percent confidence interval or range of values that is likely to include the true theta score. Unlike in classical item theory where the precision of the scores is consistent across all examinees, IRT allows the standard error to vary. Larger standard errors of measurement can be the result of estimations of thetas in the extremes of the distribution (very low or very high ability) or for the estimates of abilities of children who responded to a limited number of items (i.e., children who responded to all items administered generally have lower standard errors of measurement than those children responding to fewer items.) Tables 3-1 and 3-2 list the names of the variables pertaining to the IRT theta scores and   standard errors of measurement available in the data file, along with the variable descriptions, ranges, weighted means, and standard deviations. 3  "}, {"section_title": "Scale Scores", "text": "The IRT-based overall scale score for each content domain is an estimate of the number of items a child would have answered correctly in each data collection round if he or she had been administered all of the questions for that domain (that is, all of the 83 unique questions in the router and the three second-stage reading forms administered in kindergarten, all of the 75 unique questions in the router and the three second-stage mathematics forms, all of the 20 items administered in the single-stage science form, and all 31 items administered in the single-stage SERS form)."}, {"section_title": "3-4", "text": "To calculate the IRT-based overall scale score for each domain, a child's theta is used to predict a probability for each assessment item that the child would have gotten that item correct. Then, the probabilities for all the items fielded as part of the domain are summed to create the overall scale score. Because the computed scale scores are sums of probabilities, the scores are not integers. (for example, if a child's IRT scale score in reading is higher than in mathematics, it would not be appropriate to interpret that to mean the child is doing better in reading than in mathematics).  Additionally, number-right scores are provided for the 10 items common to the EBRS and SERS. Only Spanish-speaking children who did not obtain a high enough score on the preLAS subtests to take all the assessments in English were administered the SERS items, so these number-right scores are only available for those children. A child who was administered the SERS has responses to these 10 items administered in English as part of the EBRS and to these 10 items administered in Spanish as part of the SERS. Each child administered both the EBRS and SERS will thus have two scores for the 10 common items: (1) number correct for the 10 EBRS items and (2) number correct for the 10 SERS items.  "}, {"section_title": "Choosing the Appropriate Score for Analysis", "text": "When choosing scores to use in analysis, researchers should consider the nature of their research questions, the type of statistical analysis to be conducted, the population of interest, and the audience. The sections below discuss the general suitability of the different types of scores for different analyses."}, {"section_title": "Analytic Considerations for Measuring Gains in the ECLS-K:2011", "text": "An important issue to be considered when analyzing achievement scores and gains is "}, {"section_title": "Reliability of the ECLS-K:2011 Scores", "text": "Reliability statistics assess consistency of measurement, or the extent to which test items in a set are related to each other and to the score scale as a whole. For tests of equal length, reliability estimates can be expected to be higher for sets of items that are closely related to the underlying construct than for tests with more diversity of content. Conversely, for tests with similar levels of diversity in content, reliabilities tend to be higher for longer tests compared to shorter tests. In general, the domain with the most diverse content in the ECLS-K:2011 assessment, science, had lower reliability coefficients than reading and mathematics. 5 Reliabilities were highest for the scores derived from the largest number of test items, namely the IRT ability estimates, which are based on all items taken by each child. Reliabilities were lowest for the scores based on the fewest items, namely the raw number-right scores. Reliability statistics appropriate for each type of score were computed for each subject area for fall and spring kindergarten. For the IRT-based scores, the reliability of the overall ability estimate, theta, is based on the variance of repeated estimates of theta for each individual child compared with total sample variance.    "}, {"section_title": "Dimensional Change Card Sort", "text": "The Dimensional Change Card Sort is used to collect information on children's cognitive flexibility. In this task, children are asked to sort a series of 22 picture cards according to different rules. Each card has a picture of either a red rabbit or a blue boat. The children are asked to sort each card into one of two trays depending on the sorting rule they have been told. One tray has a picture of a red boat and the other has a picture of a blue rabbit. For the first set of items, the Color Game (each set is referred to as a game), the rule is to sort the cards by color (i.e., red or blue). For example, a blue boat card would be sorted into the blue rabbit tray. In the second game, the Shape Game, the rule is to sort the cards by shape (i.e., rabbit or boat). For example, a red rabbit card would be sorted into the blue rabbit tray. If the child correctly sorts four of the six cards in the Shape Game, then he or she moves on to the third game: 3-12 the Border Game. In the Border Game the sorting rule (by color or by shape) depends on whether or not the card has a black border around the edges. If the card has a border, the child is to sort by color; if there is no border on the card, the child is to sort by shape. Item-level data for the Dimensional Change Card Sort are provided on the base-year data file. There are six variables with results for the color game, six variables with results for the shape game, and six variables with results for the Border Game. There were four practice items administered to children, but the results from these practice items are not included on the data file. The item-level data for the color and shape games are scored \"Correct\" (i.e., card sorted into the correct tray according to the sorting rule) or \"Incorrect\" (i.e., card sorted into the incorrect tray). There is a third score provided for the Border Game, \"Not administered\"; this code indicates that the child was not administered the item because he or she did not answer enough items correctly to advance to this item in the assessment. The \"Not administered\" code is different than a system missing code in that only those children who were Using scoring rules provided by the developers, two scale scores were developed from the Dimensional Change Card Sort data collected in the kindergarten rounds of data collection: the postswitch score and the Border Game score. The post-switch score is the number of cards the child correctly sorted by shape (i.e., after switching from sorting by color to sorting by shape), not including the practice trials. The Border Game score is the number of cards the child correctly sorted when the sorting rule was determined by the presence (or absence) of a border around the card. 6 The post-switch score has a relatively high mean (  "}, {"section_title": "Numbers Reversed", "text": "This measure assesses the child's working memory. It is a backward digit span task that requires the child to repeat an orally presented sequence of numbers in the reverse order in which the numbers are presented. For example, if presented with the sequence \"3\u20265,\" the child would be expected to say \"5\u20263.\" Children are given 5 two-number sequences. If the child gets three consecutive twonumber sequences incorrect, then the Numbers Reversed task ends. If the child does not get three consecutive two-number sequences incorrect, the child is then given 5 three-number sequences. The sequence becomes increasingly longer, up to a maximum of eight numbers, until the child gets three consecutive number sequences incorrect (or completes all number sequences)."}, {"section_title": "3-14", "text": "Item-level data for the Numbers Reversed subtask are provided in the base-year data file. The maximum number of items any child was administered in either the fall or spring kindergarten collections was 30 items (5 two-digit number items; 5 three-digit number items; 4 four-digit number items; 4 five-digit number items; 4 six-digit number items; 4 seven-digit number items; 4 eight-digit number items). Each item is scored \"Correct\" (i.e., the child correctly repeated the number sequence in reversed order), \"Incorrect\" (i.e., the child did not correctly repeat the number sequence in reversed order), or \"Not administered\" (i.e., the child was not administered the item because he or she did not answer enough items correctly to advance to this item). The \"Not administered\" code is different In addition to the item-level data, three scores developed using guidelines from the publisher scoring materials are included in the data file for Numbers Reversed. Before analyzing the Numbers Reversed data, it is important that researchers understand the characteristics of these scores and how these characteristics may affect the analysis and interpretation of the Numbers Reversed data in the context of the ECLS-K:2011. It is strongly recommended that even researchers familiar with the Numbers Reversed task review the information presented here to assist in their analysis and interpretation of the findings. The three scores developed using publisher guidelines are a W score, a standard score, and percentile rank. Depending on the research question and analysis being conducted, one of the scores may be more preferable than another. For example, the W score may be best for a longitudinal analysis, whereas the percentile rank and standardized score may be better suited for an analysis focusing on one point in time. The descriptions below provide more information about which score may be better suited for a given analysis. 7 The W score, a type of standardized score, is a special transformation of the Rasch ability scale and provides a common scale of equal-intervals that represents both a child's ability and the task difficulty. The W scale is particularly useful for the measurement of growth and can be considered a growth scale. Typically, the W scale has a mean of 500 and standard deviation of 100. Furthermore, the publisher of the Woodcock-Johnson III (WJ III) has set the mean to the average of performance for a child of 10 years, 0 months. This means that it would be expected that most children younger than 10 years, 0 months would obtain W scores lower than the mean, and most older children would be expected the W score in the ECLS-K:2011 may be attributed to the derivation of the score being a comparison to the average 10-year-old or to differences between the ECLS-K:2011 population and the WJ III norming sample. 8 The standard score and the percentile rank also show a lower mean in the ECLS-K:2011, which may also be attributable to differences between the ECLS-K:2011 population and the WJ III norming sample.  data from the spring kindergarten child-level teacher questionnaire begin with \"T2.\" 3-20"}, {"section_title": "Teacher-Reported Social Skills", "text": "In both the fall and spring kindergarten collections, teachers reported how often their ECLS-K:2011 students exhibited certain social skills and behaviors using a four-option frequency scale ranging from \"Never\" to \"Very Often.\" Teachers also had the option of indicating that they had not had an opportunity to observe the described behavior for the child being asked about.    3-22"}, {"section_title": "3.5", "text": ""}, {"section_title": "Parent-Reported Social Skills", "text": "In both the fall and spring kindergarten parent interviews, parents were asked to report how often their child exhibited certain social skills and behaviors using the same frequency scale described above for the teacher-reported social skills items. These parent items also are based on items from the   Table 3-13 presents the internal consistency reliability estimates of the self-control, social interaction, and sad/lonely scales derived from information reported by the parent. Reliability statistics are not reported for the impulsive/overactive scale; it is computed from only two parent-reported items, which is not enough to calculate an alpha reliability. "}, {"section_title": "3-23", "text": ""}, {"section_title": "Teacher-Reported Approaches to Learning Items and Scale", "text": "The child-level teacher questionnaire included seven items, referred to as \"Approaches to Learning\" items, that asked the teachers to report how often their ECLS-K:2011 students exhibited a selected set of learning behaviors (keeps belongings organized; shows eagerness to learn new things; works independently; easily adapts to changes in routine; persists in completing tasks; pays attention well; and follows classroom rules). 10 These items were presented in the same item set as the social skills items adapted from or based on the Social Skills Rating System (described above in section 3.4), and teachers used the same frequency scale to report how often each child demonstrated the behaviors described. The Approaches to Learning scale score is the mean rating on the seven items included in the scale (the category \"No opportunity to observe\" is treated as missing data and not included in the derivation of these scales). A score was computed when the respondent provided a rating on at least 4 of the 7 items that composed the scale. Higher scale scores indicate that the child exhibited positive learning behaviors more often. The variable names, descriptions, value ranges, weighted means, and standard deviations for the fall kindergarten and spring kindergarten teacher Approaches to Learning scale scores 10 The Approaches to Learning teacher items were developed specifically for the ECLS-K; they are not taken from an existing source. These are the same items that were fielded as part of what was called the Teacher Social Rating Scale in the ECLS-K. The first six items (i.e., keeps belongings organized; shows eagerness to learn new things; works independently; easily adapts to changes in routine; persists in completing tasks; pays attention well) were included in the Teacher Social Rating Scale of the kindergarten round in ECLS-K. The seventh item (i.e., follows classroom rules) was added in the first-grade round of ECLS-K."}, {"section_title": "3-24", "text": "are shown in table 3-14. The Approaches to Learning scale has a reliability estimate of .91 for each round of data collection. Additionally, the item-level data for the teacher-reported Approaches to Learning items are included on the data file along with the other child-level teacher questionnaire data. Variable names for the item-level data from the fall kindergarten child-level teacher questionnaire begin with \"T1,\" and variable names for the spring kindergarten child-level teacher questionnaire begin with \"T2.\" "}, {"section_title": "Parent-Reported Approaches to Learning Items and Scale", "text": "The parent interview included six items, referred to as \"Approaches to Learning\" items, that The Approaches to Learning parent items were developed specifically for the ECLS-K; they are not taken from an existing source. These are the same items that were fielded as part of what was called the Parent Social Rating Scale in the ECLS-K."}, {"section_title": "3-25", "text": "data. Variable names for the item-level data from the fall parent interview begin with \"P1,\" and variable names for the spring parent interview begin with \"P2.\" "}, {"section_title": "Children's Behavior Questionnaire", "text": "The were asked to indicate how \"true\" or \"untrue\" those statements were about that child on a 7-point scale ranging from extremely untrue to extremely true, with a middle option of \"neither true nor untrue.\" If a statement or situation did not apply to that child, the teacher could indicate \"not applicable.\" The data file includes two scale scores: (1) attentional focus and 2 "}, {"section_title": "Student-Teacher Relationship Scale", "text": "The Student-Teacher Relationship Scale (Pianta and Steinberg 2001) is a 15-item, teacherreported measure of closeness and conflict between the teacher and child. As part of the spring kindergarten child-level teacher questionnaire, the teacher was presented with 15 descriptive statements about his or her relationship with the ECLS-K:2011 child and asked to indicate the degree to which each statement applied to their relationship using a 5-point scale ranging from \"definitely does not apply\" to \"definitely applies.\" Two scales were developed based on guidelines from the publisher: closeness and conflict. The closeness scale score is the average rating on the seven items included in the scale, while the conflict scale score is the average rating on the eight items included in that scale. A score was computed when the respondent provided a rating on at least 5 of the 7 or 8 items that composed the scales.  3-28"}, {"section_title": "SAMPLE DESIGN AND SAMPLING WEIGHTS", "text": "The Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011) will provide national data on children's characteristics as the children progress from kindergarten through the 2015-16 school year, when most of them will be in fifth grade. In the 2010-11 school year, the ECLS-K:2011 collected data from a nationally representative sample of 18,174 children enrolled in 968 schools. 1 This chapter describes the process used to select the sample for the study and provides information necessary to properly analyze the data that were collected."}, {"section_title": "Sample Design", "text": "The optimal sample design for collecting data to produce national child-level estimates is to sample children with probabilities that are approximately the same for each child. In most studies, this is achieved using a multi-stage sampling design that involves sampling primary sampling units (PSUs) and schools with probabilities proportional to the number of children and selecting a fixed number of children per school. Such a sampling procedure was used for the ECLS-K:2011. Additionally, a clustered design was used to minimize data collection costs, which are highly related to the dispersion of the children in the sample. Restricting data collection to a limited number of geographic areas and to as few schools as possible helps to minimize costs while still achieving an acceptable level of precision in the estimates produced with the data. The sample for the ECLS-K:2011 was selected using a three-stage process. In the first stage of sampling, the country was divided into primary sampling units (PSUs), or geographic areas that are counties or groups of contiguous counties, and 90 PSUs were sampled for inclusion in the study. In the second stage, samples of public and private schools with kindergarten programs or that educated children of kindergarten age (i.e., 5-year-old children) in ungraded settings were selected within the sampled PSUs. Both PSUs and schools were selected with probability proportional to measures of size (defined as the population size) that took into account a desired oversampling of Asians, Native Hawaiians, and other Pacific Islanders (APIs). 2 In the third stage of sampling, children enrolled in kindergarten and 5-year-old children in ungraded schools or classrooms were selected within each sampled school."}, {"section_title": "4.1.1", "text": "Sampling PSUs"}, {"section_title": "Sampling Frame for PSUs", "text": "The first-stage sampling frame for the ECLS-K:2011 was a list of the 3,141 counties in the United States. To facilitate sampling with probability proportional to size (i.e., population), the frame included 2007 Census Bureau population estimates of the total population in each county, as well as estimates of the number of 5-year-old children, both overall and by race/ethnicity for each county. Since the only annual population data that are available from the Census Bureau at the county level are reported in 5-year age groups rather than by single year of age, the number of 5-year-old children was estimated by dividing the total population in the 5-9 age group by 5. Also, since 2007 population estimates by race/ethnicity are not available by age, the general population estimates by race/ethnicity were used to derive the approximate percent minority 5-year-old population by applying the percent minority in the county to 5-year-olds. The 2007 Census Bureau population estimates were compared to estimates from the 2007 American Community Survey (ACS) for the 788 counties for which data from both sources were available. This comparison showed that estimates from the two sources were similar for most subgroups except for the American Indian/Alaska Native and the \"other race\" groups. The largest differences in the estimates produced from the two sources are for the American Indian and Alaska Native group, and these differences were attributed to small ACS sample sizes with large sampling errors. The difference in the estimates for the \"other race\" group was due to the difference in how the Census Bureau and ACS define the \"other race\" category. Census estimates for this category include the multi-race population and the other race groups, while ACS estimates include only the \"other race\" groups. The county-level frame was used to form a list of PSUs from which a subset of PSUs would be sampled for the ECLS-K:2011. This was done either by treating larger counties in the frame as discrete PSUs or combining smaller contiguous counties into one PSU. The primary objective when forming PSUs was to maximize the within-PSU heterogeneity on the percent of 5-year-old Blacks in the PSU and the percent of 5-year-old Hispanics in the PSU, subject to the following constraints: \uf06e that the minimum number of 5-year-olds in the PSU was 380; \uf06e that the maximum distance between the farthest points within a PSU was 100 miles; 4-2 \uf06e that the PSU consisted of either all Metropolitan Statistical Area (MSA) or all non-MSA counties 3 ; and, \uf06e that the PSU was formed within a state boundary. In the case of two large MSA PSUs, each PSU was divided into two smaller PSUs. One of the smaller PSUs contained a city, and the other smaller PSU contained the remaining areas within the PSU. The 2007 population estimate for the smaller PSU containing the city was obtained from the 2007 Census Bureau population estimates place-level file; the estimate for the smaller PSU without the city then was obtained by subtraction.  "}, {"section_title": "PSU Measure of Size", "text": "In order to sample PSUs with probability proportional to size, each PSU had to be assigned a measure of size (MOS). The measure of size used for selecting the PSUs was the number of 5-year-old children in the PSU (rather than the total PSU population size) adjusted for the desired oversampling of APIs. The number of 5-year-olds was determined using the method described above. For PSUs consisting of more than one county, the numbers of 5-year-olds in all the counties in the PSU were summed to obtain the number of 5-year-olds in the PSU. The weighted measure of size was calculated as follows: 3 Metropolitan Statistical Area (MSA) is a geographic entity designated as one or more counties in a metropolitan area, except in New England (CT, MA ME, NH, RI VT), where MSA is defined in terms of county subdivisions. Non-MSA designates one or more counties not in a metropolitan area. MSA and non-MSA are as defined by the Office of Management and Budget."}, {"section_title": "4-3", "text": "where r API is the oversampling rate for APIs and n API and n other are the counts of 5-year-old APIs and all other 5-year-olds, respectively. The value for r API was 2.5, meaning that API children were sampled at a rate 2.5 times higher than non-API children in the third sampling stage (i.e., the stage during which children were sampled). The oversampling rate for APIs was used in computing the measure of size for the PSUs so that PSUs with a high concentration of APIs had a higher chance of being selected."}, {"section_title": "PSU Stratification and Selection", "text": "Ten PSUs with a large measure of size were included in the ECLS-K:2011 sample with certainty. They are referred to as self-representing (SR) PSUs. The remaining PSUs, which are referred to as non-self-representing (NSR) PSUs, were sampled using a stratified sampling procedure. They were grouped into 40 strata defined by MSA status, census geographic region, size class (defined using the measure of size), per capita income, and the race/ethnicity of 5-year-old children residing in the PSU (specifically the percent of 5-year-old APIs, the percent of 5-year-old Blacks, and the percent of 5-yearold Hispanics). Two PSUs were selected in each NSR stratum using Durbin's Method 1 (Durbin, 1967). This method selects two first-stage units per stratum without replacement, with probability proportional to size and with known joint probability of inclusion of the pair. The Durbin method was used because it allows selection without replacement with known first and second order probabilities."}, {"section_title": "Sampling Schools", "text": "The second stage of sampling involved selecting samples of public and private schools that have kindergarten programs or that educate children of kindergarten age in an ungraded setting from within the sampled PSUs. The target for the number of schools participating in the base year of the study was 180 private and 720 public schools, for a total of 900 schools. In order to achieve this target number, approximately 280 private schools and 1,030 public schools were initially sampled because it was estimated that about 35 percent of the sampled private schools and 30 percent of the sampled public schools would refuse to participate in the study. 4-4"}, {"section_title": "School Frames and School Eligibility", "text": "In order to sample schools for the ECLS-K:2011, a frame of public schools and a frame of private schools were built using the school frames constructed for the 2010 National Assessment of Education Progress (NAEP). The sources for the 2010 NAEP school frames were the most recent "}, {"section_title": "School Measure of Size", "text": "Schools were selected with probability proportional to size. The measure of size for schools was kindergarten enrollment adjusted to take into account the desired oversampling of APIs. The oversampling rate for APIs was 2.5. Thus, the measure of size for school j in PSU i is where n API,ij is the estimated count of kindergarten children from the API group and n other,ij is the number of all other kindergarten children in school j in PSU i, with enrollment information taken from the NAEP frames. Schools for which data on kindergarten enrollment were missing from the NAEP frame were assigned a size value of 12 (which is half of the target sample of 23 children per school, rounded to the nearest integer). Examples of schools for which kindergarten enrollment data were imputed are those that reported pre-kindergarten and beyond as the school grade span but had missing data on kindergarten enrollment and those that only offered ungraded programs. Of the 11,174 public schools in the ECLS-K:2011 frame, 118 (or 1.1 percent) have imputed data for kindergarten enrollment. Of the 6,411 private schools, 266 (or 4.1 percent) have imputed data for kindergarten enrollment. 4-5"}, {"section_title": "4.1.2.3", "text": ""}, {"section_title": "Clustering of Small Schools", "text": "Public schools with fewer than 23 children and private schools with fewer than 12 children were clustered together for sampling. This would give smaller schools a better chance of being selected than if they were not clustered. Clusters were formed to have as few schools as possible, to have as close to 23 children (in public schools) or 12 children (in private schools) in each cluster as possible, and to be as different as possible in terms of locale, school size, and religious affiliation (if applicable). This was accomplished by creating sorted lists and then combining the lists together in an interleaving fashion 4 before grouping schools together for clustering. For public schools, locale and school size were used as sort variables. For private schools, religious affiliation and school size were used as sort variables. Public schools having more than 23 kindergarten children and private schools having more than 12 kindergarten children were not clustered, except in some rare instances in which they were clustered with a small school. For example, when a small school is not near any other small school on the sorted list, it would have been grouped with a large school. During sampling, a cluster of schools was treated as a single sampling unit, as was each non-clustered school. For ease of presentation, the text in the sections below refers to the sampling units as clusters, which include true clusters composed of at least two schools as well as individual schools that were not combined with other schools for sampling (i.e., \"clusters\" of one)."}, {"section_title": "Implicit Stratification of Schools/Clusters of Schools", "text": "Within each PSU, the public-school clusters were sorted by the measure of size (i.e., kindergarten enrollment adjusted for the desired oversampling of APIs) and separated into three size classes (high, medium, and low). Size classes were defined separately for each PSU based on the distribution of the school measure of size in the PSU; each class size contained one third of the schools/clusters of schools within the PSU. The clusters were also categorized into one of three groups according to their locale: city/suburb, town/rural, or mixed. The mixed group included clusters that were formed to have at least one school classified as city/suburb and at least one school classified as town/rural. Within each of the nine cells created by crossing the three locale groups and the three size classes, clusters were sorted by the number of APIs in the cluster in a serpentine manner. 5 Similarly, each private-school cluster was categorized based on the schools' religious affiliation as religious, non-religious, or mixed. The mixed category included clusters with at least one religious school and at least one non-religious school. School size was not used for stratification of private schools because the number of private schools was small and because there was less variation in school size among private schools than there was among public schools. The list of private-school clusters was then sorted by the three categories based on religious affiliation. Within each religious affiliation category, the clusters were sorted in a serpentine manner by the measure of size. However, the sorting differed slightly for clusters in SR PSUs and schools in non-SR PSUs. All private-school clusters in all SR PSUs were treated as if they were from the same PSU; that is, the aggregated list of clusters from the 10 SR PSUs was sorted by religious affiliation (religious/mixed/non-religious). This sorting allowed for better control of the sample distribution of religious, non-religious, and mixed schools/clusters. Clusters within non-SR PSUs were sorted by religious affiliation in a serpentine manner within their respective PSUs."}, {"section_title": "School Selection", "text": "Clusters were sampled at rates that would result in an approximately self-weighting sample of children within the public and private school strata. The target number of sampled schools per PSU was calculated separately for public schools and private schools, and for self-representing and non-selfrepresenting PSUs. The number of schools selected was the target number of schools adjusted upward by the estimated school response and eligibility rates. Selection of the clusters of schools was done systematically and with probability proportional to the measure of size. Selection of public schools was done independently within PSU. For the SR PSUs, selection of private schools was done on the entire list of clusters in the SR PSUs with one random start. For the non-SR PSUs, selection of private schools was done within PSU but carried over from one PSU to the next rather than selecting a new random start for each PSU in order to have a better representation of the proportion of schools that were religious, non-religious, or mixed."}, {"section_title": "ECLS-K:2011 School Sample", "text": "A total of 1,221 clusters of schools were selected for the ECLS-K:2011, of which 1,003 are clusters of public schools and 218 are clusters of private schools. This resulted in 1,036 public schools and 283 private schools being part of the sample, for a total of 1,319 schools. This many schools were selected to yield 720 participating public schools and 180 participating private schools in the kindergarten rounds of data collection (assuming that approximately 70 percent of the originally sampled public 4-7 schools and 65 percent of the private schools would be eligible and would agree to participate). 6 The characteristics of the school sample are presented in table 4-2."}, {"section_title": "Updating the Frame to Improve School Coverage", "text": "The original sample of schools was selected in April 2009 in order to allow at least a year for school recruitment before the start of fall 2010 data collection activities. In the spring of 2010, after the original sample of schools was selected, procedures were used to update the school frame and select a supplemental sample of newly opened schools and existing schools that added kindergarten programs that were not included in the original frames. This procedure was necessary because the source data used to develop the ECLS-K:2011 school frame were collected a few years prior to the school year in which the   "}, {"section_title": "School Substitution", "text": "Early in the process of recruiting schools that had been sampled for the study, it was determined that the rate at which public schools were agreeing to participate was lower than expected, and it would be difficult to meet the target number of participating schools by the end of the recruitment period. The decision was made to select public schools not selected into the original ECLS-K:2011 sample that would replace those sampled public schools that had already refused to participate. School substitution was used in PSUs in which more than half of the sampled public schools in the PSU had been identified as either initial or final refusals. An initial refusal is a school that refused to participate when it was contacted the first time. A final refusal happened when there was an attempt to convert but the school remained uncooperative. The first step in selecting substitute schools was to determine whether there was a sufficient number of replacement schools in the PSU. Within each PSU, schools in districts that refused to have any of their schools participate were excluded from the list of potential replacement schools. If the PSU had a sufficient number of available replacement schools after schools in nonparticipating districts were excluded, within-PSU substitution was conducted. If there was an insufficient number of available replacement schools in the original PSU to yield at least a response rate of 50 percent 7 in the PSU when taking all schools in the PSU into consideration, a PSU of similar background characteristics was identified, and replacement schools were selected from those PSUs. Background characteristics used to identify PSU and school replacements are discussed below. When substitution is used in sampling, NCES standards require that the replacement or substitute units be identified at the time of sampling. For the ECLS-K:2011, this was accomplished most directly by performing within-PSU substitution. Replacement schools were selected based on the sort order used in the original sampling procedure, with the school most adjacent to the original school being chosen as the replacement school. If more than one school was equally adjacent, the school whose measure of size was closest to the original school was selected. This is equivalent to pre-selecting replacement schools from a sorted frame with substitution efforts concentrated only on low responding PSUs. When within-PSU substitution was not possible because there were not enough potential replacement schools in the PSU to yield a response rate of at least 50 percent in that PSU, a substitute PSU was selected from the same sampling strata as the original PSU. Within each sampling stratum, the 7 Fifty percent was the minimum response rate estimated to yield the required number of cooperating schools."}, {"section_title": "4-10", "text": "PSU that most closely matched the original PSU on important sampling characteristics (the PSU measure of size, per capita income, percent Black, percent Hispanic, and percent API) was selected. This procedure is again equivalent to a pre-selection scheme because it mimics exactly the procedure to select the original PSUs. The schools in the substitute PSUs were then combined with the sampled schools from the original PSUs. Schools were sorted first by three size categories (small, medium, and large) based on kindergarten enrollment adjusted for the desired oversampling of APIs. In most cases, the categories matched those used to categorize schools in the original PSU being replaced in terms of the enrollment numbers used to define each category. However, due to differences in school-level characteristics across original and substitute PSUs, there were a few instances where the categories were modified slightly in order to provide enough potential replacement schools within each size category. After assigning schools to size categories, schools were sorted in a serpentine manner based on selected school characteristics and substitution was completed within each size category. For each nonparticipating school, the most adjacent school with the most similar measure of size was selected to replace it. In the PSUs that had too few schools to have enough replacement schools, replacement schools represent 2 percent of all participating schools. In the PSUs where large school districts refused to cooperate, replacement schools represent 5 percent of all participating schools."}, {"section_title": "Sampling Children", "text": "The goal of the sample design was to obtain an approximately self-weighting sample of children, with the exception of APIs who needed to be oversampled to meet sample size goals. Within each sampled school, field staff obtained a complete list of kindergartners who were enrolled in the school or children of kindergarten age in ungraded settings. Two independent sampling strata were formed within each school, one containing API children and the second containing all other children. API children were sampled from the API stratum with a sampling rate that was 2.5 times the rate of sampling used for non-API children. Within each stratum, children were selected using equal probability systematic sampling. In general, the target number of children sampled at any one school was 23. Sampling was done systematically and with equal probability from the list. If a school was small, then fewer children were sampled from this school compared to other schools with a larger number of children (though for any given student, the probability of selection was higher in a smaller school than it was in a larger school). As a general rule, if a sampled school had 23 or more kindergarten children or kindergarten-age children, 23 children were selected; if a sampled school had fewer than 23 children, all the children were selected. However, for practical reasons,"}, {"section_title": "4-11", "text": "if there were fewer than 28 children in a school, all were selected. Twins were not identified prior to sampling, but both members of a twin pair could enter the sample through this method of probability sampling if they were sampled independently. Table 4-3 shows the distribution of the eligible children sampled for the ECLS-K:2011. Table 4-4 shows the distribution of the children who participated in the base year.    Once the children were sampled from the school lists of enrolled kindergartners, parent contact information for each child was obtained from the school. The information was used to locate a parent or guardian, to conduct the parent interview, and gain parental consent for the child to be assessed."}, {"section_title": "4-12", "text": "Teachers who taught the sampled children and before-and after-care (BASC) providers of children with BASC were also included in the study and were asked to complete questionnaires. All teacher and BASC provider data are linked to their children. There are no teachers or BASC providers included in the sample who did not provide instruction to or care for a sampled child. The procedure for 4-13 identifying and including teachers for the ECLS-K:2011 is different from the procedure used in the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K) where, during the fall kindergarten data collection, a census of kindergarten teachers was taken at each school, resulting in teachers being included who did not teach children sampled for the study."}, {"section_title": "Calculation and Use of Sample Weights", "text": "The Replicate weights are provided in the data file for use with the paired jackknife replication procedure and PSU and stratum identifiers are provided for use with the Taylor series method."}, {"section_title": "Types of Sample Weights", "text": "Full sample weights designed for use with data from a complex sample survey serve two primary purposes. When used in analyses, the full sample weight weights the sample size up to the 4-14 population total of interest. In the ECLS-K:2011, weighting produces national-level estimates. Also, the full sample weight adjusts for differential nonresponse patterns that can lead to bias in the estimates. If people with certain characteristics are systematically less likely than others to respond to a survey, the collected data may not accurately reflect the characteristics and experiences of the nonrespondents, which can lead to bias. To adjust for this, respondents are assigned weights that, when applied, result in them representing their own characteristics and experiences as well as those of nonrespondents with similar attributes. A sample weight could be produced for use with data from every component of the study (e.g., data from the fall child assessment, from the fall parent interview, from the spring child assessment, from the spring parent interview, etc.) and for every combination of components for the study (e.g., data from the fall child assessment with data from the fall parent interview or data from the spring child assessment with data from the school administrator questionnaire). However, creating all possible weights for a study with as many components as the ECLS-K:2011 has would be impractical, especially as the study progresses and the number of possible weights increases. Additionally, for budgetary reasons, the number of base-year weights created was constrained to 12 (see exhibit 4-1). In order to determine which weights would be most useful for researchers analyzing data from the base year, completion rates for each fall kindergarten and spring kindergarten component (e.g., response to the child assessment, the parent interview, various parts of the teacher questionnaire) were reviewed, and consideration was given to how analysts are likely to use the data (i.e., which weights will have greatest analytic utility). The best approach to choosing a sample weight for a given analysis is to select one that maximizes the number of sources of data included in the analyses for which nonresponse adjustments are made, which in turn minimizes bias in estimates, while maintaining as large an unweighted sample size as possible. Exhibit 4-1 identifies the survey component(s), or sources of data, for which nonrespondent adjustments are made for each weight. Exhibit 4-2, which presents the same information in matrix format, was developed to further assist researchers in deciding which weight to use for analyses. In exhibit 4-2, the components for which nonresponse adjustments are made for each weight are noted with a \"Yes.\" Researchers should choose a weight that has a \"Yes\" in the column(s) for the source(s) of data they are using in their analyses. The best weight would have a \"Yes\" for each and every source used. For example, if a researcher is conducting an analysis that includes only child-level data reported by the teachers in the fall and child-level data reported by the teachers in the spring, the weight W12T0 should be used since it adjusts for nonresponse on the child-level teacher questionnaires in both waves (i.e., exhibit 4-2 shows a \"Yes\" in both columns). However, for many analyses, there will be no weight that adjusts for nonresponse to all the sources of data that are included. When no weight corresponds exactly to the combination of components included in the desired analysis, researchers might prefer to use a weight that includes nonresponse adjustments for more components than they are using in their analysis (i.e., a weight with \"Yes\" in columns corresponding to analysis components that are not included in their analyses) if that weight also includes nonresponse adjustments for the components they are using. Although such a weight may result in a smaller analytic sample than would be available when using a weight that corresponds exactly to the components from which the analyst is using data, it will adjust for the potential differential nonresponse associated with the components. If researchers instead choose a weight with nonresponse adjustments for fewer components than they are using in their analysis, missing data should be examined for potential bias. In the ECLS-K:2011, response rate is highest for the child assessment. Most children with data from the parent, teacher, before-and after-school care (BASC) provider, or school administrator have child assessment data. Consequently, decisions about which weight to choose will depend primarily on which components other than the child assessment are being used in analyses. For example, if a researcher is conducting an analysis that includes fall child assessment data, spring child assessment data, fall parent interview data, and spring parent interview data, there is no weight that adjusts for nonresponse on all four components. There is a weight that adjusts for nonresponse to the fall parent interview and nonresponse to the spring parent interview-W12P0. W12P0 would be the appropriate weight to use, even though nonresponse to the child assessment is not adjusted for in this weight, because the majority of cases that have parent data at both rounds also have child assessment data at both rounds. Data from BASC providers were collected for children who were in after-school programs, either in a center or a home environment. There are two weights developed for use with the provider data -W1PZ0 and W12PZ0. Since the collection of the BASC provider data required parent consent obtained during the fall parent interview, these weights adjust for nonresponse associated with both the care provider questionnaires and the parent interview. "}, {"section_title": "Computation of Sample Weights", "text": "There were three stages in the development of the sample weights that correspond to the three sampling stages described above."}, {"section_title": "Computation of the PSU-Level Weight", "text": "The first stage of the weighting process assigned weights to the sampled PSUs equal to the inverse of the PSU probability of selection. The 10 self-representing PSUs in the ECLS-K:2011 were included with certainty, and so their weight is unity. For the 80 non-self-representing PSUs, the overall selection probability of PSU i , where Mi is the measure of size of unit i, and Mg is the measure of size of stratum g."}, {"section_title": "Computation of the School-Level Weight", "text": "The second stage of the weighting process assigned weights to the schools sampled within PSUs. The full sample school weight is W2SCH0, which adjusts for both the probability of selection and nonresponse. This weight and the corresponding replicate weights (described further below) are to be used with school administrator questionnaire data to produce school-level estimates. When combining school administrator questionnaire data with other child-level data such as assessment data or teacher questionnaire data to conduct analyses of children, child-level weights should be used. However, for analyses using hierarchical linear models (HLM), the school weight is more appropriate to account for nested data."}, {"section_title": "Computation of the School-Level Base Weight", "text": "The first step in developing a school-level weight was to calculate a base weight for each school that was the PSU weight multiplied by the inverse of the probability of selecting the school from the PSU. For schools that were in a cluster with at least one other school, the school selection probability was the same for all schools in the cluster and equal to the cluster selection probability. If a sampling stratum included schools with very large measure of size, the large schools in this stratum were selected with certainty. For schools/clusters selected with certainty, the probability is unity. For schools/clusters 4-19 selected with probability proportional to size, the probability of selection for the school and the cluster was calculated as where nij is the target number of noncertainty clusters in stratum j in PSU i, and mijk is the measure of size of cluster k in stratum j in PSU i. In the above equation, the subscript to indicate a school within a cluster is dropped for simplicity. In all discussions that follow, reference is made only to schools and not clusters, keeping in mind that if a school is part of a cluster, then its selection probability (hence, weight) is equal to that of the cluster. As noted above, procedures for augmenting the school sampling frames were adopted to improve the coverage of schools after the original sample was selected. The selection probability for a new public school in a district already in the sample was conditioned on the within-stratum probability of selecting that district. Similarly, the selection probability for a new school identified in a Catholic diocese was conditioned on the within-stratum probability of selecting the diocese. The private schools that were not part of a Catholic diocese were selected directly from lists (i.e., their selection was not dependent on another entity's inclusion in the sample) and, therefore, their selection probability is not conditional. To increase the sample size, in some cases originally sampled public schools that did not participate were replaced by substitute public schools. In these instances, the base weight of the substitute school is equal to the base weight of the original school, adjusted for a difference in school size, if there was such a difference."}, {"section_title": "Computation of the School-Level Nonresponse-Adjusted Weight", "text": "The base weights of responding schools were adjusted to compensate for nonresponse among the set of eligible schools. In this process, sampled schools were classified as respondents, eligible nonrespondents, or ineligible (for example, when it was determined that a sampled school did not educate kindergartners or children of kindergarten age in an ungraded setting). Substitute schools were treated as respondents even though the substitute schools are not counted as respondents in computing response rates (discussed further in chapter 5). Analyses of response propensity were performed to identify school characteristics that are good predictors of response. These characteristics were used to form nonresponse classes, or groups of schools with a particular set of characteristics. Nonresponse classes were formed separately for each school type (public/Catholic/non-Catholic private). Although these groups are referred to as nonresponse 4-20 classes, each group included both responding and nonresponding schools with similar characteristics. For example, one nonresponse class included schools that are public schools in suburb areas of the Northeast region. The school-level nonresponse adjustment was computed as the sum of the weights for all the eligible (responding and nonresponding) schools sampled in a nonresponse class divided by the sum of the weights of the eligible responding schools in that nonresponse class."}, {"section_title": "Computation of the Child-Level Weights", "text": "Weights were assigned to children sampled within schools. The child-level weights are used to produce child-level estimates."}, {"section_title": "Computation of the Child-Level Base Weight", "text": "The first step in developing a child-level weight was to calculate a base weight for each child. The child base weight is the school nonresponse-adjusted weight multiplied by the within-school child weight. The within-school child weight was calculated separately for API and non-API children. For API children, the within-school child weight is the total number of API kindergarten children in the school divided by number of API kindergarten children sampled in the school. For non-API children, the within-school child weight is the total number of non-API kindergarten children in the school divided by number of non-API kindergarten children sampled in the school."}, {"section_title": "Computation of the Child-Level Nonresponse-Adjusted Weight", "text": "The child base weight was adjusted for nonresponse to produce each of the 11 child-level weights described in exhibits 4-1 and 4-2. For each weight, a response status was defined based on the presence of data for particular components. For example, for the weight W1C0, a response status for the fall kindergarten collection was determined according to three criteria: (1) the child has scoreable reading and/or mathematics data; (2) the child does not have scoreable assessment data but has height and/or weight data; (3) or the child was excluded from assessment due to a disability or limitation that was not accommodated (for example, the child was blind and the assessments were not provided in Braille). For the weight W1P0, a respondent is identified as a child with fall kindergarten parent interview data."}, {"section_title": "4-21", "text": "The response status was used to adjust the base weight for nonresponse to arrive at the final full sample weight. Nonresponse classes were formed separately for each school type (public/Catholic/non-Catholic private). Within school type, analysis of child response propensity was conducted using child characteristics such as date of birth and race/ethnicity to form nonresponse classes. Continuing with the example W1C0, a child was classified as respondent, nonrespondent, or ineligible. Children would have been considered ineligible if they moved out of the country, died, or changed grades or withdrew from a kindergarten program between the time of sampling and the time of the assessment. The child-level nonresponse adjustment was computed as the sum of the weights for all the eligible (responding and nonresponding) children in a nonresponse class divided by the sum of the weights of the eligible responding children in that nonresponse class."}, {"section_title": "Replicate Weights", "text": "Replicate weights, constructed using a jackknife replication method, should be used to estimate the standard errors of survey estimates using the replication method, which is described further in section 4.2.4. For the ECLS-K:2011, the number of replicates for the non-self-representing PSUs can only be 40 since this is the number of non-self-representing sampling strata. If 40 replicates are also constructed for the 10 self-representing PSUs (that are also self-representing strata, of one PSU each), the 80 replicates will provide about 77 degrees of freedom for calculating confidence intervals for the survey estimates. The sample of PSUs was divided into 80 replicates or variance strata, the maximum number of replicates that provide sufficient degrees of freedom to support most analyses. The 40 non-self-representing strata form 40 variance strata of two PSUs each; each PSU forms a variance unit within a variance stratum. All schools within a non-self-representing PSU were assigned to the same variance unit and variance stratum. Sampled schools in the 10 self-representing PSUs were grouped into 40 variance strata. To do so, first schools were sorted by sampling stratum, type of school (from the original sample or newly selected via the new school procedures as part of updating the school frame), source of the newly selected schools, and their original order of selection (within stratum). From this sorted list, schools were grouped into pairs within each sampling stratum; the last group in the stratum was a triplet if the number of schools in the stratum was odd. This operation resulted in a number of ordered preliminary variance strata of two or three units each. The first 40 ordered preliminary strata (i.e., the first 40 groups of two or three schools) were then numbered sequentially from 1 to 40; the next 40 ordered preliminary strata (i.e., the next 40 groups of two or three schools) were also numbered sequentially from 1 to 40, and so on until the 4-22 list was exhausted. The preliminary variance strata with the same number were grouped together to form the final 40 variance strata. The final 80 variance strata (40 constructed from non-self-representing PSUs and 40 constructed from self-representing PSUs) can have two units or three units each. In a variance stratum with two units, the weight of the first unit was doubled to form the replicate, while the weight of the second unit was multiplied by zero. In the case of a triplet, two variance strata were created: in the first variance stratum, two of the three units were weighted by 1.5 to form the replicate and the last unit was zero-weighted; in the second variance stratum, a different group of two units was weighted by 1.5, and the third unit was zero-weighted. For each full-sample weight, a set of replicate weights was computed. All adjustments to the full sample weight were repeated for the replicate weights."}, {"section_title": "Characteristics of Sample Weights", "text": "The statistical characteristics of the sample weights are presented in table 4-5. For each weight, the number of cases with nonzero weights is presented along with the mean weight, the standard deviation, the coefficient of variation (i.e., the standard deviation as a percentage of the mean weight), the minimum weight, the maximum weight, the skewness, the kurtosis, and the sum of weights.  The Taylor series method relies on a simplified procedure for estimating the variance for a linear statistic even with a complex sample design and is valid when analyzing data from large samples in which the first stage units are sampled with replacement. The stratum and first-stage unit identifiers needed to use the Taylor series method were assigned as follows: all independent sampling strata were numbered sequentially from 1 to h; within each sampling stratum, first-stage sampling units were numbered from 1 to nh. Care was taken to ensure that there were at least two responding units in each stratum. For instances in which a stratum did not have at least two responding units, the stratum was combined with an adjacent stratum. Stratum and first-stage unit identifiers are provided in the data file. Each full sample weight has corresponding stratum and PSU identifiers for use with the Taylor series method. The stratum and PSU identifiers begin with the same characters as the full sample weight and end with either STR or PSU. For example, the stratum and PSU identifiers corresponding to weight W1C0 are W1C0STR and W1C0PSU, respectively."}, {"section_title": "4-25", "text": ""}, {"section_title": "4.2.4.3", "text": ""}, {"section_title": "Specifications for Computing Standard Errors", "text": "For the jackknife replication method, the full sample weight, the replicate weights, and the method of replication must be specified. All analyses of the ECLS-K:2011 data using the replication method should be done using JK2. As an example, to compute child-level estimates (e.g., mean reading scores) and their standard errors, users need to specify W1C0 as the full sample weight, W1C1 to W1C80 as the replicate weights, and JK2 as the method of replication. For the Taylor series method, the full sample weight, the sample design, the nesting stratum, and PSU variables must be specified. As an example, to compute child-level estimates (e.g., mean reading scores) and their standard errors, users must specify the full sample weight (W1C0), the stratum variable (W1C0STR), and the PSU variable (W1C0PSU). The \"with replacement\" sample design option, WR, must also be specified if using SUDAAN. The basic measure of the relative efficiency of the sample is the design effect (DEFF), defined as the ratio, for a given statistic, of the variance estimate under the actual sample design to the variance estimate that would be obtained with an SRS of the same sample size:"}, {"section_title": "Use of Design Effects", "text": "The root design effect is the square root of the design effect: where SE is the standard error of the estimate."}, {"section_title": "4-26", "text": "As discussed above, jackknife replication and Taylor Series can be used to compute more precise standard errors for data from complex surveys. If statistical analyses are conducted using software packages that assume the data were collected using simple random sampling (i.e., adjustments are not made using jackknife replication or the Taylor series method), the standard errors will be calculated under this assumption and will be incorrect. They can be adjusted using the average root design effect (DEFT), though this method is less precise than JK or Taylor series. 8 The standard error of an estimate under the actual sample design can be approximated as the product of the DEFT and the standard error assuming simple random sampling. In the ECLS-K:2011, a large number of data items were collected from children, parents, teachers, school administrators, and before-and after-school care providers. Each item has its own design effect that can be estimated from the survey data. Standard errors and design effects are presented in the tables below for selected items from the study to allow analysts to see the range of standard errors and design effects for the study variables. They were computed using the paired jackknife replication method in Wesvar. However, as discussed in section 4.2.4, not all statistical analysis software packages have procedures to compute variance estimate or standard error using the replication method, or if they do, they may not be available to the analysts. In such situations the correct variance estimate or standard error can be approximated using the design effect or the root design effect. As the first step in the approximation of a standard error, normalize the overall sample weights for packages that use the weighted population size (N) in the calculation of standard errors (SPSS but not SAS). The normalized weight will sum to the sample size (n) and is calculated as where n is the sample size and N is the sum of weights. See exhibit 4-2 for the type of weights to use and table 4-5 for the sample size n and the sum of weights N. As the second step in the approximation, the standard errors produced by the statistical software, the test statistics, or the sample weight used in analysis can be adjusted to reflect the actual complex design of the study. To adjust the standard error of an estimate, multiply the standard error produced by the statistical software by the square root of the DEFF or the DEFT as follows: A standard statistical analysis package can be used to obtain VARSRS and SESRS. The DEFF and DEFT used to make adjustments can be calculated for specific estimates, can be the median DEFF and DEFT across a number of variables, or can be the median DEFF and DEFT for a specific subgroup in the population. Adjusted standard errors can then be used in hypothesis testing, for example when calculating t-and F-statistics. A second option is to adjust the t-and F-statistics produced by statistical software packages using unadjusted (i.e., SRS) standard errors. To do this, first conduct the desired analysis weighted by the normalized weight and then divide a t-statistic by the DEFT or divide an Fstatistic by the DEFF. A third alternative is to create a new analytic weight variable in your data file by dividing the normalized analytic weight by the DEFF and using the adjusted weight in your analyses.        Inclusion of substitute schools increases the sample size, but how well the sample represents the population should be assessed by the participation of the originally selected sampled schools. The original school sample, before substitution, included 1,352 schools. An additional 93 schools were added to the sample through substitution, meaning that an attempt was made to recruit a total of 1,446 schools during the base year of the study. Of the 1,352 schools in the original sample, 1,264 were eligible for the fall data collection. Table 5-1 presents information on the number of eligible schools providing consent for participation in the fall for both the original (i.e., before substitution) sample and the final sample that includes substitute schools. In the spring collection, four schools that were eligible in the fall became ineligible because the sampled students in these small schools left for other schools. The number of eligible schools in the spring is 1,260, Table 5-2 presents information on the number of eligible schools providing consent for participation in the spring for both the original (i.e., before substitution) sample and the final sample that includes substitute schools.  "}, {"section_title": "4-29", "text": ""}, {"section_title": "4-31", "text": ""}, {"section_title": "4-32", "text": ""}, {"section_title": "4-34", "text": ""}, {"section_title": "5-3", "text": ""}, {"section_title": "5-5", "text": "The before-substitution weighted response rate for fall and spring kindergarten was consistent across time periods: in the fall it was 63.0 percent, while in the spring, it was 62.7 percent. When looking at subgroup response rates, for school type, locale, and school enrollment, the weighted response rates are fairly consistent across subgroups. For census region and percent minority, there was more variability in the response rates of the subgroups. The pattern is true for both the fall and spring data collection.  provider questionnaires) computed at the student level. Response rates for all students and response rates by selected school and student background characteristics are both provided. Information on school characteristics comes from the sampling frame, or from the school administrator questionnaire for schools that completed this questionnaire. Information on student characteristics was collected from parents in the parent interview and from school staff during the process of sampling students for the study. In order to compute response rates by different characteristics, the selected characteristics must be known for both respondents and nonrespondents. In the fall data collection, information on the school characteristics presented in the tables was known for all schools since they were all original sampled schools and the information was available from the sampling frame or the school administrator questionnaire for characteristics such as kindergarten enrollment or percent minority, if they were not missing in the questionnaire. For a very small percentage of cases that have missing data for student's race/ethnicity and/or month and year of birth from both parent interviews and student sampling (0.2 percent or less), data for these characteristics were imputed using the modal value of the affected variable for the students in that school. Also students' sex was imputed based on the student's first name for 0.2 percent of cases. As a result of this imputation for the purpose of computing response rates, the number of respondents by subgroup shown in the tables throughout this chapter may not correspond exactly to data in the data file. 1 In the fall of kindergarten, 18,526 students were sampled within the 860 participating schools. Students who were determined to be ineligible to participate and students who were excluded from the assessment due to lack of accommodations are not included in the calculation of response rates for the child assessment. 2 Therefore 18,099 is the denominator for the unweighted fall child assessment response rate and 18,170 is the denominator for the unweighted fall parent and teacher response rates. The parent and teacher rates are computed at the student level, meaning they indicate the percentages of students for whom a parent interview was completed or for whom a teacher questionnaire was received, respectively. Table 5-4 presents weighted and unweighted student-level response rates for the child assessment and parent interview in the fall kindergarten data collection, by selected school characteristics. For the child assessments, a child is considered a respondent if the child had scoreable reading and/or mathematics data, or height and/or weight measurements. For the fall child assessment, the weighted student-level response rate was 87.0 percent. The highest response rates were in the South census region 1 These imputed values were only used for nonresponse analyses and computing response rates. Variables with imputed data for students' race/ethnicity, date of birth, and sex are not available on the data file. 2 There were about 360 students in the fall sample who were identified as not eligible. These are students who had moved out of the country, or changed grades or withdrew from a kindergarten program between the time of student sampling and the time of student assessment.  5-9 1 Student had scoreable reading and/or mathematics data, or student had height and/or weight measurement. 2 Parent completed family structure portion of parent interview. 3 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total. Indian/Alaska Native students sampled for the study resided. For this reason, response rates for these groups of students are lower than response rates for students of other racial/ethnic backgrounds. This is true for the rates for all the components discussed in this chapter. For the fall child assessment, the weighted student-level response rate was 87.0 percent. The highest response rates were for Black students (90.8 percent) and students in the \"other\" race/ethnicity category (95. Among the subgroups that had a larger sample size, White students had the lowest response rate. This rate was 86.0 percent, which was only 1 percent lower than the rate for the full sample. For the fall parent interview, the weighted response rate was 74.2 percent. Here the highest response rates were for White students (77.0 percent), Black students (73.9 percent), and students in the \"other\" race/ethnicity category  1 Student had scoreable reading and/or mathematics data, or student had height and/or weight measurement. 2 Parent completed family structure portion of parent interview. 3 This category includes children who are more than one race. 4 Sample sizes have been rounded to the nearest 10. Therefore, detail may not sum to total. NOTE: The weighted response rates were calculated using the student base weight (the product of the school base weight and the within-school student weight).  while the lowest rate was found in the Northeast region (81.4 percent). For the student-level teacher questionnaire, the final weighted response rate was 82.0 percent. Again, the highest response rate was for students in Catholic schools (89.7 percent), while the lowest rate was in the West region (77.0 percent).  Table 5-7 presents the weighted and unweighted response rates for the general classroom teacher questionnaires in the fall kindergarten data collection, by selected student characteristics. The first set of rates is for teacher response to the teacher-level questionnaire. Overall, the teacher questionnaire response rates for students with different characteristics were fairly consistent. No response rate was more than 3 percent higher than the full sample rate except for students in the \"other\" race/ethnicity group. A few rates were lower than the average rate. However, each of the subgroups with a response rate lower than the average had a small number of respondents: Native Hawaiian/Other Pacific Islanders (78.7 percent), American Indians or Alaska Natives (78.3 percent), and students born in 2006 (79.9 percent)."}, {"section_title": "5-11", "text": ""}, {"section_title": "5-12", "text": "For the student-level teacher questionnaire, the lowest rates were again found for groups with small sample sizes: Native Hawaiian/Other Pacific Islanders (73.3 percent), American Indians or Alaska Natives (71.9 percent), and students born in 2003 (78.0 percent).      "}, {"section_title": "5-13", "text": ""}, {"section_title": "5-15", "text": ""}, {"section_title": "5-17", "text": "Because recruitment of schools continued through the fall and into the spring, additional students were eligible for the study in the spring. Therefore, in the spring kindergarten collection, 20,601 students were part of the full sample. Students who were determined to be ineligible to participate and students who were excluded from the assessment due to lack of accommodations are not included in the calculation of response rates for the child assessment. 3 The denominator for the unweighted child assessment response rate is 20,158. The denominator for the unweighted parent response rate is 20,234. In spring kindergarten, there were students who were homeschooled 4 and, therefore, they were not eligible for the teacher questionnaires. The denominator for the teacher response rate is 20,197. As with the fall response rates, the parent and teacher rates are computed at the student level, meaning they indicate the percentages of students for whom a parent interview was completed or for whom a teacher questionnaire was received, respectively. Table 5-10 presents weighted and unweighted response rates for the child assessment and the parent interview in the spring kindergarten data collection, by selected school characteristics. As noted above in the discussion of the fall response rates, in order to compute response rates by different characteristics, the selected characteristics must be known for both respondents and nonrespondents. Unlike in the fall data collection, information on the school characteristics presented in the tables was not known for all schools in the spring data collection. Between the fall and spring, some children transferred to new schools that were not part of the school sample selected for the study, did not respond to the study, and did not have information available in the frame. In the tables presenting response rates for the spring data collection, some characteristics have an \"unknown\" category that includes students in these transfer schools. In spring kindergarten, there also were students who were homeschooled and, therefore, the school characteristics do not apply to them. Lastly, there was also a small number of students who moved to schools outside of the sampled PSUs and were not assessed in those schools, or students who moved and could not be located at all; their school characteristics also are unknown. The category \"unknown\" in the following tables includes the homeschooled students, students in transfer schools, students who moved out of the sampled PSUs and were not assessed, and students who could not be located. Although presented in the tables, students included in this \"unknown\" category are excluded from the discussion of response rates among subgroups because of their small sample sizes and low response rates. As was the case in the fall, for a very small percentage of cases that have missing data for student's race/ethnicity and/or month and year of birth from both parent interviews and student sampling (0.2 percent or less), data for these characteristics were imputed using the modal value of the affected variable for the students in that school. Also students' sex was imputed based on the student's first name for 0.2 percent of cases. 5  1 Student had scoreable reading and/or mathematics data, or student had height and/or weight measurement. 2 Parent completed family structure portion of parent interview. 3 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total. and 74.2 percent (students in schools with 15 percent or less minority enrollment). Table 5-11 presents weighted and unweighted response rates for the child assessment and the parent interview in the spring kindergarten data collection, by selected student characteristics. As with fall of kindergarten, operational problems prevented the study from conducting data collection activities in some areas of the country where Asian, Native Hawaiian/Other Pacific Islander, and American Indian/Alaska Native students sampled for the study resided. For this reason, response rates for these groups of students are lower than response rates for students of other racial/ethnic backgrounds. 1 Student had scoreable reading and/or mathematics data, or student had height and/or weight measurement. 2 Parent completed family structure portion of parent interview. 3 This category includes children who are more than one race. 4 Sample sizes have been rounded to the nearest 10. Therefore, detail may not sum to total. NOTE: The weighted response rates were calculated using the student base weight (the product of the school base weight and the within-school student weight). The weighted response rate for the spring child assessment was 85.2 percent. When looking at the subgroups, students in the \"other\" race/ethnicity group had the highest response rate at 91.6 percent."}, {"section_title": "5-21", "text": "The lowest rates were observed for the subgroups with the smallest sample sizes:    1 This category includes children who are more than one race. 2 Sample sizes have been rounded to the nearest 10. Therefore, detail may not sum to total. NOTE: A respondent is defined as a child for whom a teacher questionnaire was returned and the questionnaire had at least one response. The weighted response rates were calculated using the student base weight (the product of the school base weight and the within-school student weight  response rate for the spring child assessment was 53.4 percent. The highest response rates were found in the Midwest (65.3 percent) and in schools in which the percentage of enrolled students of racial/ethnic minorities was 86 percent or more (61.8 percent). The lowest rates were found in the Northeast (47.7 percent) and the West (49.9 percent). For the parent interview, the overall weighted response rate for the spring data collection was 42.1 percent (the product of the spring school response rate (62.7 percent) and the spring parent response rate (67.1 percent)). The highest response rates were seen in Catholic schools and in the Midwest at 48.6 and 52.2 percent, respectively. Conversely, the lowest rate was in the Northeast at 36.6 percent.    4 percent), while the lowest rates were in the Northeast (43.4 percent) and the West (45.2 percent).  West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, New Mexico, Nevada, Oregon, Utah, Washington, and Wyoming. NOTE: A respondent is defined as a child for whom a teacher questionnaire was returned and the questionnaire had at least one response. The weighted overall response rates were calculated using the school base weight for the school response rate component and the student base weight for the student response rate component. The student base weight is the product of the school base weight and the within-school student weight. longitudinal tables is also the student base weight before any adjustments were made to it.   1 Student had scoreable reading and/or mathematics data, or student had height and/or weight measurement, in both fall and spring-kindergarten. 2 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total.   1 Student had scoreable reading and/or mathematics data, or student had height and/or weight measurement in both fall and spring. 2 This category includes children who are more than one race. 3 Sample sizes have been rounded to the nearest 10. Therefore, detail may not sum to total. NOTE: The weighted response rates were calculated using the student base weight (the product of the school base weight and the within-school student weight). SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2010 and spring 2011.  1 Parent completed family structure portion of parent interview in both fall and spring. 2 Parent completed family structure portion of parent interview in fall or spring. 3 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total. 4    1 Parent completed family structure portion of parent interview in both fall and spring. 2 Parent completed family structure portion of parent interview in fall or spring. 3 This category includes children who are more than one race. 4 Sample sizes have been rounded to the nearest 10. Therefore, detail may not sum to total. NOTE: The weighted response rates were calculated using the student base weight (the product of the school base weight and the within-school student weight).    . The overall weighted response for students with both a complete fall and complete spring parent interview was 34.4 percent. This rate was highest for students in Catholic schools (41.9 percent), the Midwest (46.3 percent), and in schools with minority student enrollment of 15 percent or less (41.5 percent). The rates were lowest in the Northeast (27.5 percent) and West (30.0 percent). The response rate for students with a complete parent interview in either fall or spring was 50.0 percent. The rate was highest for students in the Midwest (60.7 percent), and the lowest rates were in the Northeast and West, at 42.0 and 45.5 percent, respectively. 1 Parent completed family structure portion of parent interview in both fall and spring. 2 Parent completed family structure portion of parent interview in fall or spring. 3 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total. The unknown categories are not included in this    6 A child care arrangement was eligible for the BASC component if the child was reported to be in the arrangement on a regular basis for a minimum of 5 hours per week, if the provider was at least 18 years old, if the care was provided before or after school, and if permission to contact the provider had been given by the parent. The arrangement in which the child spent the most number of hours that met these criteria was selected for the BASC component. If a child had more than one eligible arrangement and was cared for in each eligible arrangement for an equal number of hours each week, the child care arrangement was selected for the BASC component using a random number.    West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, New Mexico, Nevada, Oregon, Utah, Washington, and Wyoming. NOTE: A respondent is defined as an eligible student for whom at least one of the BASC questionnaires for which the student was eligible was completed. The weighted response rates were calculated using the student base weight (the product of the school base weight and the withinschool student weight  as well as for students born in 2003 (43.1 percent). As seen for other components, the lowest response rates generally were seen for subgroups with very small sample sizes. 1 This category includes children who are more than one race. 2 Sample sizes have been rounded to the nearest 10. Therefore, detail may not sum to total. NOTE: A respondent is defined as an eligible student for whom at least one of the BASC questionnaires for which the student was eligible was completed. The weighted response rates were calculated using the student base weight (the product of the school base weight and the withinschool student weight). SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2010 and spring 2011. , and it was lowest in the West (32.6 percent) and in schools with minority student enrollment of 86 percent or more (31.6 percent). West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, New Mexico, Nevada, Oregon, Utah, Washington, and Wyoming. NOTE: A respondent is defined as an eligible student for whom at least one of the BASC questionnaires for which the student was eligible was completed. The weighted overall response rates were calculated using the school base weight for the school response rate component and the student base weight for the student response rate component. The student base weight is the product of the school base weight and the withinschool student weight. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2010 and spring 2011."}, {"section_title": "5-22", "text": ""}, {"section_title": "5-25", "text": ""}, {"section_title": "5-26", "text": ""}, {"section_title": "5-27", "text": ""}, {"section_title": "5-29", "text": ""}, {"section_title": "5-32", "text": ""}, {"section_title": "5-35", "text": ""}, {"section_title": "5-36", "text": ""}, {"section_title": "5-37", "text": ""}, {"section_title": "5-40", "text": ""}, {"section_title": "5-41", "text": ""}, {"section_title": "5-42", "text": ""}, {"section_title": "5-43", "text": "The school-level response rates for the school administrator questionnaire were presented in table 5-3. Tables 5-26 and 5-27 present the weighted and unweighted response rates for the student-level school administrator questionnaire. They are rates for students in schools that were eligible for the school administrator questionnaire. The denominator for these tables is 20,197, the same denominator as for the teacher questionnaires. The weighted response rate for the student-level administrator questionnaire was 88.5 percent for all students. In general, the response rates by selected school characteristics are near or above 90 percent. However, the inclusion of the \"unknown\" category, which includes children in schools without complete school administrator surveys (which is why these characteristics are unknown), depresses the response rate for all children somewhat, to 88.5 percent. The response rate is very low (near zero) for this category. By subgroups, the highest rate was for students in Catholic schools (96.9 percent), students in the Midwest (94.9 percent), students in the rural locales (95 percent), and students in schools with 15 percent or less minority enrollment (95.8 percent). By student characteristics, the rate was highest for White (91.5 percent) and students of other race (91 percent) and lowest for Native Hawaiian/Other Pacific Islanders (80.1 percent), American Indians or Alaska Natives (83.5 percent), and for students born in 2006 (78.7 percent). The rate for those born in 2006 is based on about 30 students.  West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, New Mexico, Nevada, Oregon, Utah, Washington, and Wyoming. NOTE: A respondent is defined as an eligible student for whom the school was eligible for the school administrator questionnaire, the questionnaire was returned and there was at least one response. The weighted response rates were calculated using the student base weight (the product of the school base weight and the within-school student weight). SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2010 and spring 2011. 1 This category includes children who are more than one race. 2 Sample sizes have been rounded to the nearest 10. Therefore, detail may not sum to total. NOTE: A respondent is defined as an eligible student for whom the school was eligible for the school administrator questionnaire, the questionnaire was returned and there was at least one response. The weighted response rates were calculated using the student base weight (the product of the school base weight and the within-school student weight). SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2010 and spring 2011. , and it was lowest in the Northeast (49.7 percent). In the first method, estimates from the ECLS-K:2011 were compared with estimates from the 2010-11 Common Core of Data (CCD) and the 2009-10 Private School Survey (PSS) for a limited number of items. These two sources of data were selected for the analysis because the school years to which they pertain are closest to the school year in which the kindergarten data were collected. The ECLS-K:2011 estimates were computed using the fully adjusted weights, which are the weights used to prepare estimates for general analysis purposes. Large differences in the estimates between the ECLS-K:2011 and the school frame may be indicators of nonresponse bias. The difference in the two sets of estimates is very small, suggesting there is not significant nonresponse bias present in the data. For example, the ECLS-K:2011 estimate for male students is 51. In the third method, estimates from the spring ECLS-K:2011 data collection weighted by the nonresponse-adjusted weights were compared with estimates weighted by the base weight. The base nonresponse bias is present in the data."}, {"section_title": "5-44", "text": ""}, {"section_title": "5-45", "text": ""}, {"section_title": "5-46", "text": ""}, {"section_title": "5-49", "text": "This page intentionally left blank."}, {"section_title": "DATA PREPARATION", "text": "As described in chapter 2, two types of data collection instruments were used for the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011) in the base year: computer-assisted interviews and assessments (CAI) and self-administered paper forms (hard copy). Once data were collected, they were reviewed and prepared for release to analysts. The approaches used to prepare the data differed with the mode of data collection. The direct child assessments and parent interviews were conducted using CAI. Editing specifications were built into the computer programs used by assessors or interviewers to collect these data. The teacher, school administrator, and before-and afterschool care provider questionnaires were self-administered. When these hard-copy questionnaires were returned to the data collector's home office, staff recorded the receipt of these forms into a projectspecific form tracking system. Data from the hard-copy forms were captured by scanning the completed forms. Before scanning, coders reviewed the questionnaires to ensure that responses were legible and had been written in appropriate response fields for transfer into an electronic format. Coding of open-ended 1 \"other, specify\" text responses into existing or new categories was conducted after the data were scanned and reviewed for range and logical consistency. The following sections describe the data preparation activities for both modes of data collection in more detail."}, {"section_title": "Coding and Editing Specifications for Computer-Assisted Interviews", "text": "When designing a CAI, edit specifications can be decided upon and programmed into the instrument in advance of administration. Checks to ensure that responses fell within predetermined acceptable ranges and were logically consistent with one another were programmed into the CAI parent interview instrument. Such checks were not programmed into the child assessment because the assessment did not contain items requiring range checks and there was no need for a child's answers to different questions to be consistent with one another. The following sections describe the coding and editing that were conducted on the CAI parent interview."}, {"section_title": "6.1.1", "text": ""}, {"section_title": "Range Specifications", "text": "Within the CAI parent interview instrument, the acceptable range of response values for each closed-ended item was defined by the set of response categories provided for the item. Interviewers could only enter predefined responses (e.g., 1 for \"yes\" and 2 for \"no\"), \"don't know,\" or \"refused.\" Respondent answers to open-ended items for which the response was expected to be a number were subjected to both \"soft\" and \"hard\" range edits during the interviewing process. A soft range is one that "}, {"section_title": "Consistency Checks (Logical Edits)", "text": "Consistency checks, or logical edits, examine the relationship between and among responses to ensure that they do not conflict with one another or that the response to one item does not make the response to another item unlikely. For example, in the household roster, a person could not be recorded as both a mother and male. When a logical error such as this occurred during an interview, a message appeared requesting verification of the last response and a resolution of the discrepancy. In some instances, if the verified response still resulted in a logical error, the interviewer recorded the problem either in a comment field or contacted the project help desk where an electronic problem report was created. It was not possible to program consistency checks for all related questions in the interview, so there may be some inconsistencies in the data because respondents reported the information that way in the interview or because interviewers made an error when editing the data. Consistency checks were not 6-2 used in the child assessments because answers to the assessment items did not have to be consistent with one another."}, {"section_title": "Coding Text Responses", "text": "Additional coding was required for some of the items asked in the CAI parent interview once the data had been collected. These items included \"other, specify\" text responses and responses to questions asking about parent or guardian occupation. Interviewers entered participants' responses to these items verbatim. Data preparation staff reviewed these verbatim responses and were trained to code them into categories using coding rules developed by the data collection contractor and the National Center for Education Statistics (NCES). Review of \"other, specify\" items. Some closed-ended items in the parent interview included an \"other, specify\" option that interviewers could select if a respondent provided a response that was not one of the specific response options offered. During data collection, when a respondent provided an \"other\" response in the parent interview, the interviewer entered the text into an \"other, specify\" overlay box that appeared on the screen. Examples of such items are household members' relationship to the study child, primary language spoken in the home, and travel time/distance to school. The data preparation staff reviewed these text \"other, specify\" responses and, if appropriate, coded them into one of the existing response categories. For example, for the question about how the child usually gets to school, some interviewers entered responses indicating that someone walked the child to school into the \"other, specify\" overlay box, rather than coding such responses in the available category of \"walks\"; such responses were upcoded into the category for walking to school. There were a small number of items in the parent interview for which additional categories were added to accommodate \"other, specify\" text responses that occurred with sufficient frequency. For example, a new category was added to the question about how the child gets to school to indicate that someone other than the parent drives or takes the child to school. Some text responses did not fit into any preexisting category and were not common enough to be coded into new categories; such responses are left coded as \"other\" in the data. New categories added as a result of the review of \"other, specify\" responses are noted as such in the study instruments. There were no \"other, specify\" items in the child assessments. "}, {"section_title": "Household Roster Review", "text": "Three general types of checks were run on the household roster information collected during the parent interview to identify missing or inaccurate information and, therefore, to determine whether the data on household composition collected in the household roster required editing. \uf06e First, the relationship of an individual living in the household to the study child was compared to the individual's listed age and sex. Inconsistencies such as a male mother or a biological mother over age 60 were examined further and corrected when the interview contained sufficient information to support a change fixing the inconsistency. \uf06e Second, while it is possible to have more than one mother or more than one father in a household, households with more than one mother or more than one father were reviewed to ensure they were not cases of data entry error. Corrections were made whenever clear errors were identified and a clear resolution existed. \uf06e Third, the relationship of an individual in the household to both the study child and the respondent was examined, as there were cases in which the relationship of an individual to the study child conflicted with his or her status as the spouse/partner of the respondent. For example, in a household containing a child's grandparents but not his or her parents, the grandmother may be designated the \"mother\" figure, and the grandfather thus becomes the \"father\" figure for the purposes of some questions in the interview by virtue of his marriage to the grandmother. These cases were examined but left unchanged. Both the original-and correct (grandfather)-relationship data and the new \"parent-figure\" designation (father) that had been constructed were retained. In other situations discrepancies in the parent figure relationships to the child indicated an error and the data were edited. For example, in a household containing two mothers, a review of the audio recording from the interview indicated that the relationship of the second mother was documented incorrectly by the interviewer-the second mother was not a mother to the focal child; therefore, the relationship of the second mother was edited (corrected). Two flags on the data file identify cases that were reviewed or edited for any of the reasons described above. The flags are X1EDIT (fall kindergarten) and X2EDIT (spring kindergarten); the flag is set to 1 if the case was identified for review for any of these household roster checks. There were about 700 cases requiring household roster review in fall kindergarten and in spring kindergarten."}, {"section_title": "Partially Complete Parent Interviews", "text": "Parents did not have to complete an entire interview for the data collected from them to be included on the data file. However, parent interviews did have to be completed through a specified section of the interview for those data to be included. For the fall kindergarten round, the respondent had to answer questions at least through the section on family structure (FSQ). There were approximately 220 partially completed fall parent interviews for which the respondent answered at least some questions through the FSQ section but did not finish the entire interview. 2 All data derived from questions asked after the interview termination point for these partially completed interviews are set to -9 for \"not ascertained.\" In the spring kindergarten round, the criterion for completion, and thus inclusion of data on the data file, differed depending on whether cases had completed a fall parent interview. Cases for which an interview was completed in the fall were considered complete in the spring if they answered questions through section FSQ. Cases that did not have a complete parent interview in the fall had to answer questions in the spring interview through the section of supplemental questions (section SPQ) to be considered complete. Section SPQ was designed specifically for fall nonrespondents in order to collect information on important topics, such as child care, child health, and home language, that were not otherwise asked as part of the spring interview. Section SPQ appeared prior to the FSQ section in the spring interview for fall nonrespondents, but given the importance of the information collected in that section, data were retained for anyone who finished SPQ, even if FSQ was left incomplete. There were 658 cases for which parents did not continue through the end of the spring interview but for which data were included on the data file; 81 completed SPQ but not FSQ, and 577 completed FSQ or more of the interview (including 381 fall respondents and 196 fall nonrespondents). As is true for the fall interview data, all data derived from questions asked after the interview termination point for these partially completed interviews are set to -9 for \"not ascertained.\""}, {"section_title": "6.2", "text": "Receipt, Coding, and Editing of Hard-Copy Questionnaires"}, {"section_title": "Receipt Control", "text": "In order to monitor receipt of the more than 50,000 documents that were received during the two kindergarten data collections, a project-specific receipt and document control system was used. This Forms Tracking System (FTS) was initially loaded with information that would enable the study to keep track of all materials received, such as identification numbers for schools, teachers, before-and afterschool care providers, and students; information linking students to particular schools, teachers, and before-and after-school care providers; and the specific questionnaires that were expected to be completed from each school, teacher, or before-and after-school care provider. Field staff team leaders collected completed questionnaires from school staff and shipped them to the data collector's home office. Before-and after-school care providers returned forms directly to the home office. All materials containing information collected from respondents were shipped to the home office using FedEx so that the questionnaires could be tracked in transit. Before materials collected in a school were shipped to the home office, team leaders completed a transmittal form to indicate which questionnaires were included in the shipment. Each questionnaire had an identification number that was unique to the respondent who was supposed to complete it. The team leader listed the identification numbers of the questionnaires being shipped on the transmittal form. Once the questionnaires arrived at the home office, the identification number on each returned questionnaire was matched against the list of identification numbers on the transmittal forms and in the FTS to verify that the appropriate or expected number of forms was returned and received. Discrepancies were documented for follow-up with the team leader to correct the transmittal information or update the field management system (e.g., if a teacher left the school and would no longer be a potential respondent). After the number of received documents was confirmed and verified, the questionnaires were logged into the FTS. Once questionnaires were logged in, if they had any data (some questionnaires had no data because although respondents returned them, they refused to complete them), they underwent visual review. Questionnaires with inappropriate response marks (e.g., a number instead of an X mark or a filled bubble instead of an X mark) or marginal notes requiring adjudication were set aside for supervisor review. Questionnaires that were ready for processing were assigned to batches and scanned, and the scanned data were captured in a database. Following this process, the data were edited as described in section 6.2.4. The following sections describe the scanning, coding, and data editing processes for hardcopy questionnaires."}, {"section_title": "Scanning of Hard-Copy Questionnaires", "text": "The hard-copy school administrator, teacher, and before-and after-school care provider questionnaires were scanned using Teleform, a questionnaire design and scanning software system that provides automated data capture. Once the questionnaires were scanned, they passed into the software's verification and data capture module. Any data items that the software could not read with a level of confidence defined in the questionnaire's template 3 were reviewed for verification by a staff member. The verifier reviewed the questionnaire image against the data as recorded by the software and made corrections to the data as necessary prior to committing the record to an instrument-specific database. If the verifier could not determine the intended response, he or she set the data item to an appropriate \"not ascertained\" value. Staff members also conducted quality checks of randomly selected data records against the hard-copy questionnaires on a flow basis, at a rate of 10 percent of all scanned forms as a minimum to ensure that all data were correctly captured by the Teleform software. To further ensure accuracy of the data captured from hard-copy questionnaires, on a flow basis, a report of missing items was run and staff verified that the items shown as missing were actually missing on the questionnaires. Open-ended items were closely reviewed to ensure that text was captured correctly and coded appropriately. Following completion of the Teleform data conversion process, the data were stored in a database for later conversion to Blaise 4 data files for editing. Scanned images of complete questionnaires were stored in Alchemy, an image database and retrieval system. 3 The level of confidence is a feature of the software that reflects the likelihood that a scanned image is what the software perceives it to be (for example, a specific number or letter). 4 Blaise\u00ae is the computer-assisted interviewing (CAI) system and survey processing tool used for the different components of the study. 6-8"}, {"section_title": "6.2.3", "text": ""}, {"section_title": "Coding for Hard-Copy Questionnaires", "text": "The hard-copy questionnaires also required review and upcoding of \"other, specify\" text responses. The coding staff was trained on the coding procedures and had coding manuals to reference during the coding process. The \"other, specify\" text responses were reviewed by the data editing staff and, if appropriate, upcoded into one of the existing response categories. For example, for the Special Education Teacher questionnaire item about credentials, \"hearing impaired\" was upcoded to the category for disability-specific credentials. Project staff with content expertise were asked to review items flagged by the data editing staff for further consideration when responses included technical terminology not familiar to coders or when coders were unsure whether a particular response (e.g., a type of student service) fit within a particular category. New categories were added for a small number of variables where the numbers of responses were sufficient to justify one or more new categories. For example, during the review of \"other, specify\" responses for the question related to languages used by adults and students in the classroom on the teacher questionnaire, categories for Asian Indian languages and sign language were added. Some \"other, specify\" text responses did not fit into any preexisting category and were not common enough to be coded into new categories; such responses are left coded as \"other\" in the data."}, {"section_title": "Data Editing", "text": "The data editing process consisted of verifying soft and hard ranges, checking skip edits, running consistency edits, and reviewing the results. such as an answer indicating 8 days per week, were not retained, even if they were verified as the answers provided by the respondent. In cases where the value was not retained, data for the variable were set to -9 (\"not ascertained\"). Range edits were rerun after cases were updated, and the results from the new edit check were reviewed. This iterative editing process was repeated until no further range violations were found. Skip edits. Unlike with CAI instruments, the skip patterns in self-administered hard-copy instruments cannot be automatically enforced during the administration of the questionnaire. A check of skip patterns was thus performed in the editing stage of data preparation, with edits being made when necessary. These skip-pattern checks examined the relationship between filter questions and dependent questions. For example, if a school administrator indicated that the school did not provide breakfast (the filter question), there were survey instructions directing the respondent to skip questions about the times breakfast is served and the numbers of children served (dependent questions). The survey instructions were not always followed. The skip edits performed during data preparation enforced the skip patterns in the questionnaire design. Therefore, if the respondent answered a dependent question that should have been skipped given a response to an earlier filter question, the value was set to -1, \"inapplicable\" for the variable pertaining to that dependent question. If a dependent question that should have been answered had no information captured during scanning, the hard-copy form was retrieved and the responses were reviewed. Data for dependent questions that respondents did not answer were set to -9 for \"not ascertained.\" After updates were made, the edits were run again. This iterative editing process was repeated until no further skip errors were found. Logical edits were run for each instrument after scanning and range and skip edits were complete. When an inconsistency was found, the case was identified for an editor to review the hard copy. The original value was either corrected if the editor determined that an error was made when scanning the data or confirmed and kept if the data were correctly captured as reported on the hard-copy instrument. Thus, inconsistent data may be retained in the data file if the inconsistency exists in the information reported by the respondent. If, during the investigation of a consistency check, the data showed clear evidence that the respondent answered a question incorrectly and the correct answer could be ascertained using information reported by the respondent elsewhere on the same hard-copy instrument, the corrected answer was applied to the data. Once an edit was made, data for the case was again run through the consistency edit checks. This was an iterative process; it was repeated until no further inconsistencies were found. Upon the conclusion of this iterative process, the only remaining inconsistencies, if any, were those identified as respondent-reported and therefore retained inconsistencies."}, {"section_title": "Consistency checks (logical edits", "text": ""}, {"section_title": "6-11", "text": "This page intentionally left blank. Class of 1998-99 (ECLS-K). This results in consistency between the data files from the two studies and facilitates comparisons between the two cohorts. However, some composites were created differently in ECLS-K:2011 than had been done in the ECLS-K. Documentation about composites for both studies 1 The Field Management System includes information collected about the data collection effort, the study schools, school staff, and children from available administrative records or existing data sources (such as the Common Core of Data) or from conversations between data collection staff and school staff. "}, {"section_title": "DATA FILE CONTENT AND COMPOSITE VARIABLES", "text": ""}, {"section_title": "Variable Naming Conventions", "text": "Variables are named according to the data source (e.g., parent interview, teacher questionnaire) and the data collection round to which they pertain. With the exception of the identification variables described in section 7. "}, {"section_title": "Identification Variables", "text": "The base-year restricted-use data file contains a child identification (ID) variable child assessment, D2T_ID provides the identification number for their special education teacher or related service provider. For some students, the general classroom teacher was also the student's special education teacher. However, D2T_ID will not match T2_ID for these students. CC_ID is the identification number for the before-and/or after-school care provider of children in such care. The ID variables S1_ID and S2_ID indicate the school the child attended at the time of the fall 2010 and spring 2011 data collections, respectively. As with the general classroom teacher ID variables, S1_ID and S2_ID will be the same for children who attended the same school for the entire school year. If a child does not have an IEP on record with the school that was identified as part of the process for determining accommodations for the child assessment, there will be no special education teacher associated with that child, and D2T_ID will be missing. Also, in most cases, if a child does have an IEP identified as part of the process for determining accommodations for the child assessment and, therefore, a special education teacher associated with him or her, D2T_ID will be filled whether or not the special education teacher responded to the spring 2011 special education teacher questionnaires. 3 There could be missing special education data for the child's teacher-level or child-level questionnaires (for example, if the special education teacher replied to only one of the two questionnaires or did not fully complete the questionnaires). If a special education teacher did not complete a teacher-level questionnaire, completed a child-level questionnaire for one child, and did not complete another childlevel questionnaire for a child to whom the teacher was also linked, both children would have the same D2T_ID. However, only the child for whom the teacher completed the child-level questionnaire would have data for those variables. It is left to users to determine how they would like to set \"not applicable\" versus \"not ascertained\" codes when data for D2T_ID are missing. Users interested in information about whether special education teacher questionnaires were requested, regardless of whether special education questionnaires were received in spring 2011, can use the composite variable X2SPECS, which is based on information from the FMS system rather than the special education questionnaires. information was provided in the parent interview to locate the provider), there will be a child care provider ID (CC_ID). If the child did not have a selected arrangement, or had a child care arrangement that could not be identified or the parent refused permission, CC_ID will be blank. If a case has a child care provider CC_ID, there may still be missing data if the BASC questionnaires were not completed."}, {"section_title": "Missing Values", "text": "Variables on the ECLS-K:2011 data file use a standard scheme for identifying missing data. X2MTHET, X2STHET, X12SESL), -1 is a valid value and should not be identified as missing data. The -7 (refused) code indicates that the respondent specifically told the interviewer that he or she would not answer the question. This, along with the -8 (don't know) code and the -9 (not ascertained) code, indicate item nonresponse. The -7 (refused) code is not used in the school, teacher, and care provider data. The -8 (don't know) code indicates that the respondent specifically told the interviewer that he or she did not know the answer to the question. The -8 (don't know) code is not used in the school, teacher, and care provider data. For questions where \"don't know\" is one of the options explicitly provided, a \"-8\" will not be coded for those who choose this option; instead the \"don't know\" response will be coded as indicated in the value label information for the variable associated with that question. The -9 (not ascertained) code indicates that the respondent left a question blank that he or she should have answered (or for which it is uncertain whether the item should have been answered or legitimately skipped because the respondent also left a preceding item blank). If a gate question 4 (e.g., in the school administrator questionnaire, question D2 asks, \"Are any children given a readiness or placement test before or shortly after entering kindergarten?\"), was left blank, follow-up questions to the gate question (e.g., question D3 which asks, \"How are these assessments used?\") were also coded -9 (not ascertained). For the school, teacher, and care provider self-administered questionnaires, this is the code used for item nonresponse. For data that are not collected using the self-administered questionnaires (e.g., direct assessment scores), a -9 means that a value was not ascertained or could not be calculated due to 4 A gate question is the first question in a series with skips to one or more follow-up questions. 7-7 nonresponse. The -9 (not ascertained) code is also used in the parent interview data for rare cases that ended before the interview was finished (-9 is used for all variables in the rest of the interview except for the pointer variables 5 unless it was not known who the parents were or if the fall interview ended before the section about primary language) 6 and for questions that were edited 7  were not in the CAPI questionnaire (e.g., sign language), are also coded as -8 (don't know). The \"system missing\" code appears as a blank when viewing codebook frequencies and in the ASCII data file. System missing codes (blanks) in the base-year data file indicate that data for an entire instrument or assessment are missing due to unit nonresponse. For example, when a child's parent does not participate in the parent interview, all of the data associated with questions from the parent interview are coded system missing (blank) for that child. These blanks may be translated to another value when the data are extracted into specific processing packages. For instance, SAS will translate these blanks into periods (\".\") for numeric variables. Codes used to identify missing values (-1, -7, -8, -9, or system missing) are not all identified as missing values by default in the data file. Users will need to define these as missing values in the software they are using to analyze the data. Depending on the research question being addressed, in some instances users may want to assign a valid value to cases with missing values. For example, a teacher who reported that he or she did not have any English language learners in his or her classroom in the fall 5 Pointer variables indicate the household number of a person in the household who was the subject of questions about one or more parent figures. 6 Pointer variables are only set to -9 (not ascertained) for two conditions. First, if it is not known who the parents were in the household (for example, there are 83 cases in spring 2011 that did not have fall 2010 parent interviews and lack rosters of household members), the pointer variables are set to -9 (not ascertained). Second, if the respondent ended the fall interview prior to section PLQ (primary language(s) spoken), it is not known whether the PLQ pointers are applicable to a case, so they are set to -9 (not ascertained) along with the remaining skipped response items. All other partially complete cases have their pointers set to the parent roster numbers. 7 Edits to household composition data that result in the addition or deletion of a parent or parent figure in the child's household may result in -9 (not ascertained) codes for variables in multiple sections of the parent interview that have questions that are asked depending on the presence of specific parents or parent figures. For this editing, -9 (not ascertained) codes are used for questions that are asked about parent/parent figures and those that are based on skips from those questions. These sections are: fall 2010 FSQ (family structure), PLQ (primary language), MHQ (marital history), HRQ (historical roster), NRQ (nonresident parent), CFQ (critical family processes), PEQ (education), EMQ (employment); spring 2011 FSQ (family structure), CFQ (critical family processes), NRQ (nonresident parent), DWQ (discipline and warmth), and PPQ (parent's psychological well-being and health). Missing values for composite variables were coded using the same general coding rules as those used for other variables. If a particular composite is inapplicable for a certain case, for example as the variable X2IDP2 (the household roster number of the second parent) would be for a child living in a household with no second parent, the variable is given a value of \"-1\" (not applicable) for that case. In instances where a variable is applicable but complete information required to construct the composite is not available, the composite is given a value of -9 (not ascertained). The -7 (refused) and -8 (don't know) codes are not used for the composites except in the calculations of the height and weight composites. other than English or Spanish and, therefore, did not receive the full cognitive battery or executive function assessments in fall 2010 and spring 2011, respectively. This composite variable is coded 0 for children who were English-speakers who were routed through the English assessment battery. These include those not identified by schools as coming from a home where a language other than English was spoken and for children who were identified by schools as speaking a non-English language at home, but who passed the English language screener."}, {"section_title": "Parent Data Flags (X1PARDAT, X2PARDAT, X2SPQDAT, X1EDIT, X2EDIT)", "text": "There are two flags that describe the presence of parent interview data. which is coded as 1 (true) if a case has information for section SPQ for fall nonrespondents. In addition, there are flags (X1EDIT, X2EDIT) that are coded as 1 (true) if a parent interview household matrix was edited (e.g., if an age of a household member was reported incorrectly and had to be updated, or a person who was added to the household in error needed to be deleted from the household). These flags are noted to make users aware that data cleaning was conducted for a case. Users wishing to exclude edited data can use these flags."}, {"section_title": "Teacher Flags (X1TQTDAT, X2TQTDAT, X1TQCDAT, X2TQCDAT, X2TQSDAT, X2SETQA, X2SETQC)", "text": "In kindergarten, children were expected to have a single general classroom teacher for all subjects. Thus, each child was linked to only one classroom teacher at each round. However, a teacher was linked to all of the study children he or she taught. Thus, children in the same classroom in a given data collection round all have the same classroom teacher ID."}, {"section_title": "7-10", "text": "Two types of data were collected from teachers using two different questionnaires. "}, {"section_title": "School Administrator Data Flag (X2INSAQ)", "text": "There is a flag for the school administrator questionnaire that is 1 (true) if there are data from the school administrator questionnaire and 0 (false) if there are no data."}, {"section_title": "Before-and After-School Care Flag (X1BASC)", "text": "The variable X1BASC indicates the type of arrangement that was selected for the before- "}, {"section_title": "Composite Variables", "text": "To facilitate analysis of the survey data, composite variables were derived and included on the data file. This section identifies the source variables and provides other details for the composite/derived variables. Most composite variables were created using two or more variables that are also available on the data file, each of which is named in the text that explains the composite variable. Other composites, for example, X_CHSEX, were created using data from the Field Management System (FMS) and the sampling frame, which are not available on the data file."}, {"section_title": "Child Composite Variables", "text": "There are many child-level composite variables on the child catalog. Some of these variables are described in further detail here. The child-level composites for the direct and indirect child assessment are described in chapter 3. 7-12 7.5.1.1"}, {"section_title": "Child's Household Roster Number (X1CHLDID, X2CHLDID)", "text": "There are two variables that provide the parent interview household roster number that was assigned to the child. The child is always the second person in the household roster (as can be seen in the roster person number 9 variable P1PER_2 (person type for the second person in the household), which always has a value of 2 (focal child)); thus X1CHLDID (household roster number of the focal child) is equal to 2. X2CHLDID (household roster number of the focal child) is also equal to 2 (again, as can be seen in the roster person number variable P2PER_2 (person type for the second person in the household), which always has a value of 2 (focal child))."}, {"section_title": "Household Has Sampled Twins (X12TWIN)", "text": "The variable X12TWIN was created to identify twins who were both sampled for the study. Twins in the sample were identified by matching children who had the same date of birth, same last name, same race and ethnicity (if available), 10 and were in the same school in fall 2010 or spring 2011. If there was a parent interview for a child, the parent's answer to a question about whether the child was a twin, triplet, or other child born as part of a multiple birth (parent interview items CHQ035 in fall 2010 or SPQ106 in spring 2011 for fall nonrespondents) was also used to verify twin status. It should be noted that for the purposes of this variable, both children in a twin pair had to be sampled to have a value of 1 (twin in household) for X12TWIN. 11"}, {"section_title": "Child's Age at Assessment (X1KAGE, X2KAGE)", "text": "The child's age at assessment was calculated first by determining the number of days between the date the child completed the ECLS-K:2011 direct child assessment and the child's date of 9 Person number refers to the number each household member has on the roster list. Household members are listed in the order they are reported by the respondent, thought the respondent is always person 1 and the focal child is always person 2. 10 Chapter 2 includes information about how parent interviews were conducted for households with twins. There is one twin pair (10013189, 10013415) that does not match on X12RACETH because of nonresponse in the parent interview of one of the twins. The two children have the same mother, and she is the respondent to both parent interviews. The two children are the same age (one child is listed as younger in his brother's interview, but in the child's interview his age is corrected to match the age of his brother), they are full siblings, and both are reported by their mother to be twins. In case 10013189, the child's mother reported the child's race and ethnicity, and X12RACETH is coded according to that report. In case 10013415, the answer for the child's race in the parent interview was recorded as \"don't know.\" The FMS was used to determine race and ethnicity for this child, and X12RACETH is coded according to that information. birth (X_DOBDD (day of birth), 12 X_DOBMM (month of birth), X_DOBYY (year of birth)). The total number of days was then divided by 30 to calculate the child's age at assessment in months. The child assessment date was examined to ensure it was within the field period. If the assessment date fell outside the field period, the modal assessment date for the child's school was used to set the composite and was retained for the data file. One case from the spring kindergarten round remains with a July assessment date; the sampled child was the only sampled child in the school."}, {"section_title": "Child's Sex (X_CHSEX)", "text": "Information about child's sex was collected from schools at the time of sampling and stored in the FMS, collected from parents in the fall parent interview, and confirmed by parents in the spring parent interview. The composite variable indicating the child's sex was derived using data from these three sources (P1CHSEX, P2CHSEX, or the FMS) with an order of preference for which source should be used. The child's first name was also used to examine discrepancies. Specifically, these sources of data were prioritized by using information from the spring 2011 parent interview if it was available, then from the fall 2010 parent interview if it was available, and then the FMS. This hierarchy was used to resolve almost all discrepancies in reports across data sources. If there was a parent interview in both fall 2010 and spring 2011 and both interviews reported that the child was male or both interviews reported that the child was female, the parent-reported value was considered a confirmation of the FMS data or a correction to the FMS data (if the parent and FMS data differed). However, if the FMS value for the child's sex and the parent interview value differed and there was only one parent interview, or there were two parent interviews, but the reported sex of the child was not consistent across interviews, children's first names were examined to assess whether the FMS value appeared to be more accurate than the parent report. The FMS value was used for three cases because the child had a first name that was clearly not associated with the child's sex."}, {"section_title": "Child's Date of Birth (X_DOBYY and X_DOBMM)", "text": "The child's date of birth composite variable was derived from the parent interview (P1CHDOBM, P1CHDOBY, P2CHDOBM, and P2CHDOBY) and the FMS date of birth variable. Specifically, the three sources of data were prioritized by using information from the spring 2011 parent 12 X_DOBDD indicates the child's exact day of birth. It is not included in the data file for issues related to confidentiality."}, {"section_title": "7-14", "text": "interview if it was available, then from the fall 2010 parent interview if it was available, and then the FMS."}, {"section_title": "7.5.1.6", "text": "Race/Ethnicity (X12AMINAN, X12ASIAN, X12HAWPI, X12BLACK, X12WHITE Parents were asked to indicate to which of five race categories (White, Black or African American, Asian, Native Hawaiian or other Pacific Islander, American Indian or Alaska Native) their child belonged, and they were allowed to indicate more than one. From these responses, a series of five dichotomous race variables were created that indicate separately whether the child belonged to each of the five specified race groups. In addition, one additional dichotomous variable was created to identify those who had indicated that their child belonged to more than one race category. 13 Data were collected about the child's ethnicity as well. are coded -9 (not ascertained). The difference between X12RACETHP and X12RACETH is that FMS 13 Unlike the ECLS-K, in the ECLS-K:2011 there was not a field to enter \"other\" race in the race question."}, {"section_title": "7-15", "text": "data were used to identify race/ethnicity for the variable X12RACETH when parent interview data were missing, while only parent report data were used for the variable X12RACETHP. Thus, there are more missing data for X12RACETHP than for X12RACETH. The For the height composites, if the two height measurements obtained within a round (i.e., C1HGT1 and C1HGT2 for fall 2010 and C2HGT1 and C2HGT2 for spring 2011) were less than 2 inches apart, the average of the two height values was computed and used as the composite value. If the two measurements were 2 inches or more apart, for X1HEIGHT (the child's height in fall 2010), the measurement that was closest to 45 inches for boys or 44 inches for girls was used as the composite value. These are the 50th percentile heights for children who were 5 and a half years old (68.5 months: the average age at assessment in fall 2010 using the composite X1KAGE). If the two spring measurements were 2 inches or more apart, the measurement closest to 46 inches was used for both boys and girls for X2HEIGHT. This is the 50th percentile height for children who were 6 years old (74.5 months: the average age at assessment in spring 2011 using the composite X2KAGE). The height averages come from the 2000 Centers for Disease Control and Prevention (CDC) Growth Charts (www.cdc.gov/ growthcharts/html_charts/statage.htm). 14 The two height measurements were 2 or more inches apart in 68 cases for X1HEIGHT and 103 cases for X2HEIGHT. If one value for height was missing, the other value was used for the composite. If both the first and second measurements of height were coded as -8 (don't know), then the height composite was coded as -8 (don't know). If both the first and second measurements of height were coded as -7 (refused), then the height composite was coded as -7 (refused). If both the first and second measurements of height were coded as -9 (not ascertained) because height data were missing from a break off in the child assessment or had different missing values (e.g., one was don't know and the other was not ascertained), then the height composite was coded as -9 (not ascertained). In 512 cases, the child's height in the spring kindergarten round (X2HEIGHT) was shorter than in the fall kindergarten round (X1HEIGHT). A difference of 1 inch or less could be a function of things like slouching versus standing upright or differences in shoes, hairstyle, thickness of socks, or a combination of these factors. However, 260 children were recorded as being more than 1 inch shorter in the spring than in the fall, and 127 were recorded as being more than 2 inches shorter. These discrepancies may result from measurement error or recording error. Analysts should use their own judgment in how to classify these cases in their analysis."}, {"section_title": "Child's Weight (X1WEIGHT, X2WEIGHT)", "text": "To obtain accurate measurements, each child's weight was measured twice in each data collection round. For the weight composites, if the two weight measurements obtained within a round (i.e., C1WGT1 and C1WGT2 for fall 2010 and C2WGT1 and C2WGT2 for spring 2011) were less than 5 pounds apart, the average of the two weight values was computed and used as the composite value. If the two measurements were 5 or more pounds apart, for X1WEIGHT, the measurement that was closest to 44 pounds for boys or 43 pounds for girls was used as the composite value. These are the median weights for children who were 5 and a half years old (68.5 months: the average age at assessment in fall 2010 using the composite X1KAGE). For X2WEIGHT, the measurement that was closest to 47 pounds for boys or 46 pounds for girls, the median weight for children who were 6 years old (74.5 months: the average age at assessment in spring 2011 using the composite X2KAGE) was used as the composite value. The weight averages come from the 2000 Centers for Disease Control and Prevention (CDC) Growth Charts 7-17 (www.cdc.gov/growthcharts/html_charts/wtage.htm). 15 The two weight measurements were 5 or more pounds apart in 44 cases for X1WEIGHT and 65 cases for X2WEIGHT. If one value for weight was missing, the other value was used for the composite. If both the first and second measurements of weight were coded as -8 (don't know), the weight composite was coded as -8 (don't know). If both the first and second measurement of weight in the child assessment were coded as -7 (refused), then the weight composite was coded as -7 (refused). If both the first and second measurement of weight in the child assessment were coded as -9 because weight data were missing from a breakoff in the child assessment or had different missing values (e.g., one was \"don't know\" and the other was \"not ascertained\"), then the weight composite was coded as -9 (not ascertained)."}, {"section_title": "Child's Body Mass Index (X1BMI, X2BMI)", "text": "Composite body mass index (BMI) was calculated by multiplying the composite weight in pounds by 703.0696261393 and dividing by the square of the child's composite height in inches (Keys et al. 1972;Mei et al. 2002). Unrounded values of height and weight were used in the calculation of BMI. If either the height or weight composite was coded as -9 (not ascertained), -7 (refused), or -8 (don't know), the BMI composite was coded as not ascertained (-9)."}, {"section_title": "Child's Disability Status (X2DISABL)", "text": "A composite variable was created to indicate whether a child had a disability diagnosed by a professional. Questions in the spring 2011 parent interview asked about the child's ability to be independent and take care of himself or herself, ability to pay attention and learn, overall activity level, overall behavior and ability to relate to adults and children, emotional or psychological difficulties, ability to communicate, difficulty in hearing and understanding speech, and eyesight. If parents indicated that their child had any issues or difficulties in response to these questions, follow-up questions asked whether 15 For calculating the median weight, the composites X1KAGE and X1KAGE were used to determine children's average ages at assessment. The average age at assessment in fall 2010 was 68.42 years old using the composite X1KAGE. The closest value on the CDC Growth Chart was 68.5. The average age at assessment in spring 2011 was 74.30 months old using the composite X2KAGE. The closest value on the CDC Growth Chart was 74.5."}, {"section_title": "7-18", "text": "the child had been evaluated by a professional for that particular issue and whether a diagnosis of a problem was obtained by a professional (CHQ120, CHQ125, CHQ215, CHQ245, CHQ246, CHQ300, CHQ301). Questions were also asked about current and past receipt of therapy services or participation in a program for children with disabilities (CHQ.340, CHQ.341). 16 The composite variable X2DISABL was coded 1 (yes) if the parent answered \"yes\" to at least one of the questions about diagnosis (indicating a diagnosis of a problem was obtained) or therapy services (indicating the child received services) (CHQ120, CHQ215, CHQ245, CHQ300, CHQ340, CHQ341) and the questions about the specific diagnoses (CHQ125, CHQ246, CHQ301) were not coded -7 (refused,) -8 (don't know), or -9 (not ascertained), or in the case of the vision diagnosis (CHQ301), was not coded as only nearsightedness (myopia); farsightedness (hyperopia); color blindness or deficiency; or astigmatism; or in the case of a hearing diagnosis (CHQ246), was not coded as only external ear canal ear wax. A child could be coded as having a disability according to the criteria above, even if data for some of the questions about diagnoses or therapy services (CHQ120, CHQ215, CHQ245, CHQ300, CHQ340, CHQ341) were missing. A child is coded as not having a disability if there are data for at least one of the questions about diagnoses or therapy services (CHQ120, CHQ215, CHQ245, CHQ300, CHQ340, CHQ341), and the response was either 2 (no) or the item was -1 (inapplicable) (because the child did not have issues that indicated a question should be asked), even if data for some of these questions were missing. In addition to having all \"no\" answers or \"inapplicable\" codes for the diagnoses or therapy services questions, if the child had a diagnosis, but the specific diagnosis was not reported (was refused, don't know, or not ascertained), X2DISABL was also coded 2 (no) because there was no reported disability. The composite was coded as missing only if all of the data for the questions about diagnoses or therapy services (CHQ120, CHQ215, CHQ245, CHQ300, CHQ340, CHQ341) were -7 (refused), -8 (don't know), or -9 (not ascertained), or if the items that skipped to these items were -7 (refused), -8 (don't know), or -9 (not ascertained)."}, {"section_title": "Primary Language in the Child's Home (X12LANGST)", "text": "A composite was created to indicate whether English was a primary language spoken in the home or whether a non-English language was the primary language spoken. In fall 2010, parents were asked if any language other than English was regularly spoken in their home (P1ANYLNG). If a language other than English was not spoken in the home, or if a language other than English was spoken in the home but the primary language of the household (P1PRMLNG) or of the only key parent figure or both key parent figures (P1PRMLN1, P1PRMLN2) was English, the composite is coded as 2 (English language). In cases where there was only one key parent figure in the household, and there were also other adults in the household or persons of unknown age, respondents were asked to report the primary language in the household (P1PRMLNG) and this was used in creating X12LANGST. Otherwise, if there was only one key parent figure in the household and he or she spoke English, and it was not applicable to ask primary language because there were no other adults in the household or persons of unknown age (who could have been adults), the composite is coded as 2 (English language). If both English and another language were spoken in the home, and the respondent reported that two or more languages were spoken equally or they could not choose a primary language, the composite is coded 3 (cannot choose primary language or two languages equally). Otherwise, if a language other than English was spoken (P1ANYLNG), either solely (P1ENGTOO) or primarily in the home (P1PRMLNG), the composite is coded as 1 (non-English language). If it was not applicable to ask primary language because both key parent figures spoke the same non-English language or there was only one key parent figure, he or she spoke a non-English language, and there were no other adults in the household or persons of unknown age (that would have resulted in asking primary language of the household), the composite is coded as 1 (non-English language). If primary language was -9 (not ascertained) and both key parent figures spoke the same non-English language, the composite is coded as 1 (non-English language). If there was not a parent interview in fall 2010, the composite X12LANGST is based on questions about household language from spring 2011. If a language other than English was not spoken in the home (P2ANYLNG), or if a language other than English was spoken in the home but the primary language of the household (P2PRIMLN) was English, the composite is coded as 2 (English language). If the respondent reported that two or more languages were spoken equally or they could not choose a primary language, the composite is coded 3 (cannot choose primary language or two languages equally). Otherwise, if a language other than English was spoken in the home (P2ANYLNG), the composite is coded as 1 (non-English language). Otherwise, the composite is coded -9 (not ascertained). 17"}, {"section_title": "7.5.1.12", "text": ""}, {"section_title": "First-Time Kindergartner (X1FIRKDG)", "text": "Another composite was created to indicate whether a child was in his or her first year of kindergarten in the 2010-11 school year. In fall 2010, parents were asked if it was their child's first, second, or third (or more) year of kindergarten (P1YEARK). If it was the first year of kindergarten, the composite X1FIRKDG is coded 1 (yes). If it was the child's second, third, or greater year of kindergarten, the composite is coded 2 (no). If the fall parent information was missing, data from the child-level teacher questionnaire (T1FIRKDG) was used. Teachers were asked whether the 2010-11 school year was the child's first or second year of kindergarten. If, according to the teacher, it was the child's first year of kindergarten, the composite is coded 1 (yes). If it was the child's second year of kindergarten, the composite is coded 2 (no). If this information was missing from both the parent and the teacher, the composite is coded -9 (not ascertained)."}, {"section_title": "Child's Age at Kindergarten Entry (X1AGEENT)", "text": "A composite for the child's age at kindergarten entry was created using the date of birth composite variables and parent reports in fall 2010 whether it was the child's first, second, or third (or more) year of kindergarten. Using the date of birth composite variables (X_DOBMM, XDOBDD (not on file), XDOBYY), the child's age in months is calculated as of September 1, 2010 if the parent reported that it was the child's first year of kindergarten; as of September 1, 2009 if a parent reported that it was the child's second year of kindergarten; and as of September 1, 2008 if a parent reported that it was the child's third or more year of kindergarten, If data were missing for the parent report of the year of kindergarten, the teacher's report of whether it was the child's first or second year of kindergarten was used. If data are missing for the date of birth composites, or both the parent and teacher reports of kindergarten entry, X1AGEENT is coded as -9 (not ascertained). The first digit of X1CLASS indicates the specific type of kindergarten class in which the child was enrolled (half-day A.M., half-day P.M., or full-day). It was derived primarily from responses on the teacher-reported child-level questionnaire (TQC) (variable T1CLASS). If data on class type from the TQC were missing, then data from the fall Field Management System (FMS) (variable F1CLASS) were used to classify children's class type. There are three values for the first digit of X1CLASS: 1 = fullday class, 2 = half-day A.M. class, and 3 = half-day P.M. class."}, {"section_title": "7.5.1.15", "text": ""}, {"section_title": "Child's Receipt of Special Education Services (X2SPECS)", "text": "The composite variable X2SPECS indicates whether or not special education questionnaires were requested from teachers in the spring of 2011, based on the presence or absence of a link to a special education teacher or related service provider in the FMS. The value is 1 if special education questionnaires were requested and 2 if special education questionnaires were not requested. This link was established automatically when Individualized Education Program (IEP) or Individualized Family Service Plan (IFSP) information was entered in the FMS by study team leaders based on information from school staff. If a child had an IEP/IFSP, the team leader was required to indicate a link to both a classroom teacher and a special education teacher. The links were verified by team leaders by looking at FMS reports that indicated required teacher links for each child. 18 It should be noted that the links were established to determine IEPs or IFSPs in order to determine if an accommodation might be required for the child assessment. Although some children with an IEP/IFSP that did not require an accommodation are included, the way that IEP/IFSPs were identified for the assessment did not capture all children with an IEP/IFSP if no accommodation was required."}, {"section_title": "Child Assessment Status (X1EXDIS, X2EXDIS)", "text": "Two composite variables use FMS data to indicate whether the child was excluded from the assessment due to a disability. Study team leaders obtained information from school staff in fall 2010 and spring 2011 about whether a child had an IEP/IFSP on file and if any information in a child's IEP/IFSP indicated that he or she would need Braille, large print, sign language, or another accommodation that would exclude the child from the assessment. If so, the child was not assessed, and X1EXDIS (or X2EXDIS in spring 2011) was coded 1 (child was excluded from the assessment due to a disability). Otherwise, X1EXDIS (or X2EXDIS in spring 2011) was coded 0 (child was not excluded from the assessment). 18 There was a small number of cases that had an IEP/IFSP according to the FMS, but questionnaires were not requested from teachers. There were no special education teacher IDs (D2T_ID) for these cases because the children were unlocatable, moved out of the sampled PSU, or were no longer enrolled in school. 7-26 (refused,) -8 (don't know,) or -9 (not ascertained), then X1HRSNOW is coded as -9 (not ascertained)."}, {"section_title": "7-27", "text": "The composite variable X1PRIMNW indicates the type and location of the child's primary, regular, nonparental care arrangement; that is, the arrangement that was regularly scheduled at least once Otherwise, for children with more than one type of child care arrangement, the value for X1PRIMNW is identified based on which type of care had the most number of hours and where the care (for relative and nonrelative care) was located. If there are more hours for a center-based program than for relative or nonrelative care, X1PRIMNW is coded as 7 (center-based program) regardless of whether there are missing data for the place of relative care or nonrelative care. If two or more child care arrangements were used for the same number of hours, X1PRIMNW is set to 8 (two or more types of care with equal hours), regardless of, having missing place of relative care or nonrelative care. As noted above, if the total number of hours across all types of care (X1HRSNOW) is -9 (not ascertained), then X1PRIMNW is -9 (not ascertained) in most cases; however, there are two exceptions to this. If the number of hours for one child care arrangement are more than those for another arrangement, and the hours for the secondary arrangement (P1RHROTH, P1NHROTH, or P1CHROTH) that are part of X1HRSNOW are missing for the type of care that has the most hours, X1PRIMNW is coded according to the child care with the most 7-28 hours despite missing data because adding additional secondary hours would not change the outcome of which arrangement had the most hours. 19 , 20 The variable X1BASC indicates the type of arrangement: 1 (relative), 2 (nonrelative), or 3 (center-based) that was selected for the before-and after-school care arrangements that were eligible, or if the data needed to know if the case could be selected for BASC (e.g., hours in care) are -7 (refused) or -8 (don't know), if the parent did not give permission to contact the 19 Case 10011689 has missing place of care in P1RPLACE but is coded on the composite X1PRIMNW based on a comment entered by the interviewer during the interview. Case 10014843 is missing data on the number of centers the child is attending now (P1CTRNUM), which results in X1HRSNOW=-9 because hours for secondary care arrangements in a center were not collected, but the center-based program that the child was in the most number of hours was 1 hour per week. Therefore, other programs the child was also in would have had to be for less than one hour. Because the number of hours in care with a relative was much higher than that, relative care was coded as the primary care arrangement. 20 X1PRIMNW is coded differently than in the ECLS-K. In the ECLS-K, the hours of care were required to have a nonmissing value on the composite, even if there was only one type of child care. Also, in the ECLS-K, if the location of care with a relative or nonrelative was missing, the composite was missing. The values are also different in X1PRIMNW than the ECLS-K. Categories 3 and 6 are new. These were collapsed into a category 7 (location varies) in the ECLS-K, which has been deleted for the ECLS-K:2011. 21 In some cases, the parent reported one type of care in the parent interview but when questionnaires were sent to providers, the providers indicated another type of care. There are four cases (10008710, 10011525, 10013837, and 10015526) for which the type of child care reported in the parent interview was center-based care, so X1BASC is coded as 3 (center-based), but the child care provider indicated that the children were in home-based care. Thus, these children have data for a home-based care provider. There are also 22 cases for which the type of child care reported in the parent interview was with a relative or nonrelative; however, the child care providers indicated they provided center-based care. Thus, these children have data for center-based care providers. There was also one case (10009295) that had relative care selected for BASC, but a comment from the interviewer indicated that the respondent did not want to give the relative's contact information so she instead provided information for the center that the child attends. The center was attended for enough hours to qualify for the BASC component (17 hours a week), but the hours in relative care were greater (20 hours a week). In addition, there are 324 cases (254 cases with X1BASC = 1 (relative) and 70 cases with X1BASC = 2 (nonrelative)) that do not have provider data and are ineligible because one of the children's parents was identified as the provider (68 cases), the provider was unlocatable (173 cases), the respondent did not provide enough information for a match to a provider (48 cases), the provider indicated he or she did not provide before/after school care and did not take care of the child (1 case), or the case was not matched to a provider because the parent interview was a breakoff and mistakenly excluded from the BASC component (20 cases, including one (10005104) with no child care provider contact information). In addition, because of the timing of obtaining consent for the child to participate in the study, home-based provider questionnaires were not fielded in 14 cases that had initial refusals and thus the child's participation in the study was not certain. There are also 92 cases where X1BASC = 3 (center-based), but there are no provider data because the child had a provider who was no longer in business (1 case), was ineligible (e.g., a parent of the child) (21 cases), could not be located (23 cases), a respondent who did not provide enough information for a match to a provider (18 cases), or the case was not matched to a provider because the parent interview was a breakoff and mistakenly excluded from the BASC component (21 cases). In addition, because of the timing of obtaining consent for the child to participate in the study, center provider questionnaires were not fielded for 8 cases that had initial refusals and thus the child's participation in the study was not certain."}, {"section_title": "7-29", "text": "child's child care provider in parent interview question CCQ380, or if the child care provider was less than 18 years old, X1BASC is coded as -1 (not applicable). If the respondent to the parent interview broke off in the child care section, and the parent interview question CCQ380 is coded -9 (not ascertained), X1BASC is also coded as -9 (not ascertained)."}, {"section_title": "Nonparental Care Arrangements During the Year Prior to Kindergarten (X12CAREPK, X12PRIMPK)", "text": "Information about any nonparental care arrangements that the child had during the year prior to kindergarten was collected in the fall 2010 parent interview. If this information was not collected in the fall because there was no parent interview completed in the fall, a reduced set of questions about nonparental care in the year prior to kindergarten was asked in the spring 2011 parent interview. The two composite variables indicating the nonparental care children received during the year prior to kindergarten were created using information from the parent interview in the round in which it was collected. X12CAREPK has a value of \"yes\" or \"no\" and indicates whether the child received any nonparental care during the year before entering kindergarten. 22 If an answer in the parent interview about whether the child received nonparental care during the year before entering kindergarten or an answer in the parent interview about whether the child ever received nonparental care (a question that led to the question about nonparental care during the year before entering kindergarten) was -7 (refused), -8 (don't know), or -9 (not ascertained), X12CAREPK is coded as -9 (not ascertained). Next, if the total number of hours that a child was in any type of care was 0, X12PRIMPK is set to 0 (no nonparental care). After that, if the hours of any type of care (P1RHRSPK, P1NHRSPK, P1CHRSPK) are missing -7 (refused), -8 (don't know), or -9 (not ascertained), then X12PRIMPK is -9 (not ascertained). If an answer in the parent interview about whether the child received nonparental care during the year before entering kindergarten or an answer in the parent interview about whether the child ever received nonparental care (a question that led to the question about nonparental care during the year before entering kindergarten) was -7 (refused), -8 (don't know), or -9 (not ascertained), then X12PRIMPK was also -9 (not ascertained). Otherwise, for children with more than one type of child care arrangement, the value for X1PRIMPK is identified based on which type of care had the most number of hours and where the care (for relative and nonrelative care) was located. If there are more hours for relative care or nonrelative care, but data for the variables about the location of care are missing, X12PRIMPK is -9 (not ascertained). If two or more child care arrangements were used for the same number of hours, X12PRIMPK is set to 8 (two or more types of care with equal hours). 23"}, {"section_title": "Family and Household Composite Variables", "text": "Many composite variables are created to provide information about the sampled children's family and household characteristics. It should be noted that composite variables about household composition take into account only those people who were household members at the time of the parent 23 There are several differences in the way this composite was calculated in the ECLS-K:2011 compared to the ECLS-K. First, if a parent in the fall 2010 ECLS-K:2011 indicated that the child received only one type of regular care the year before kindergarten, but skipped (or answered \"don't know\") for the number of hours in the care arrangement, the regular child care arrangement was coded as the primary arrangement even though there was not a value for hours of care. In the ECLS-K, if the number of hours in the care arrangement was missing, the composite was missing. Second, items from spring 2011 were used in the ECLS-K:2011 composite, but not in the ECLS-K composite. If one type of child care arrangement before kindergarten was reported in spring 2011, that arrangement was coded as the primary arrangement. Third, the Head Start variable that was used to create this composite in the ECLS-K was not in the ECSL-K:2011. Finally, the values for this composite are different than those used in the ECLS-K. Categories 3 and 6 are new. These were collapsed into a category 8 (location varies) in the ECLS-K that has not been used in the ECLS-K:2011."}, {"section_title": "7-31", "text": "interview. If information on household composition was collected in the fall 2010 parent interview, the parent respondent was asked to indicate whether the people living in the household in the fall were still in the household in the spring 2011 parent interview. Household members were included in the derivation of the spring 2011 composite variables if they were still living in the household in the spring, as indicated in the variables P2CUR_1-P2CUR_25."}, {"section_title": "Household Counts (X1HTOTAL, X2HTOTAL, X1NUMSIB, X2NUMSIB, X1LESS18, X2LESS18, X1OVER18, X2OVER18)", "text": "Two composites, X1HTOTAL and X2HTOTAL, provide a count of the total number of household members. In fall 2010, this was a count of the total number of persons identified by the respondent as household members. In spring 2011, the count of the total number of persons in the household was based on household members still in the household (as identified in verification questions that the household member from the fall 2010 interview was still there) and any new persons who were added since the fall 2010 interview. In households that did not participate in fall 2010, X2HTOTAL was a count of the total number of persons identified by the respondent as household members. There are also composite variables on the file that indicate the total numbers of adults and children in the household. Information about household members' ages was collected in the household matrix, or roster, section of the parent interview. These age composites that involve counts of household members are X1LESS18 and X2LESS18 (total number of people in the household under age 18, including the study child, siblings, and other children) and X1OVER18 and X2OVER18 (total number of people in the household age 18 or older). Those household members with missing age who were the child's parent or grandparent are counted as adults in X1OVER18 and X2OVER18. Cases with at least one household member with missing age who is not identified as a parent or grandparent are coded as -9 (not ascertained) on X1OVER18, X2OVER18, X1LESS18, and X2LESS18. 24 All household members who were 18 years old or older in fall 2010 and spring 2011 are counted for X1OVER18 and X2OVER18, respectively. X1LESS18 is created by subtracting X1OVER18 from X1HTOTAL. 25 24 As noted above, for the composites X1OVER18, X2OVER18, X1LESS18, and X2LESS18, household members with missing ages were coded according to whether the household member is a parent or grandparent. However, for skips used in the administration of the parent interview, household members with missing ages were assumed to be adults (e.g., box 6 in fall parent interview section PLQ). 25 Cases 10007811 and 10008017 originally had household members listed in the parent interview who were not actually in the household. These persons were edited out of the variables indicating relationship to the study child, but their presence in the household is still reflected in X1HTOTAL and X1LESS18 were not changed. For case 10007811, X1HTOTAL should be 9 not 10, and X1LESS18 should be 7 not 8. For case 10008017, X1HTOTAL should be 3 not 4, and X1LESS18 should be 1 not 2."}, {"section_title": "7-32", "text": "The composites X1NUMSIB and X2NUMSIB indicate the total number of siblings (biological, step-, adoptive, or foster) with whom the child lived in the household (FSQ130). Siblings were identified by questions in the parent interview asking the relationship of each household member to the study child. X1NUMSIB and X2NUMSIB do not count children of the parent's partner (FSQ180 = 5) as siblings."}, {"section_title": "Food Security Status", "text": "The food security status of the children's household was determined by responses to the 18 food security questions (P2WORRFD through P2NOMONY) asked in the spring 2011 parent interview. 26 The questions measured the households' experiences related to food insecurity and reduced food intake in item severity parameters in the ECLS-K data were near enough to the standards benchmarked by the Current Population Survey Food Security Supplement that it was appropriate to use the standard benchmark household scores, which are based on the latter data source."}, {"section_title": "Food Security Status: Raw Scores (X2FSRAW2, X2FSADRA2, and X2FSCHRA)", "text": "The household food security raw score, X2FSRAW2, is a count of affirmative responses to the 18 food security items. This is an ordinal-level measure of food insecurity. It can be used in analyses as an ordinal measure of food insecurity or to identify more severe or less severe categories of food insecurity than those identified in the categorical food security variables described in section 7.5.3.2.3. The raw score is only ordinal, not interval, so should not be used where a linear measure is required, such as for calculation of a mean. Responses to items skipped because of screening are assumed to be negative for the purpose of creating the score. For cases with missing data but at least some valid responses, missing responses were considered to be negatives. Cases with no valid responses to any of the 18 food security items are coded as missing -9 (not ascertained). X2FSRAW2 ranges from 0 to 18. X2FSADRA2 is the adult food security raw score, which is a simple count of the number of household-and adultreferenced food security items affirmed by the parent. It ranges from 0 to 10. X2FSCHRA is the children's food security raw score, which is a simple count of the number of child-referenced food security items affirmed by the parent. It ranges from 0 to 8.  was a multi-step process. First, it was determined from household roster variables whether there was a mother (biological, adoptive, step-, or foster) and/or a father (biological, adoptive, step-, or foster) in the household. Using this information, the following method was used to create X1IDP1 and X1IDP2 for the fall. The same method was used to create X2IDP1 and X2IDP2 for the spring."}, {"section_title": "7-37", "text": "\uf06e (e.g., two adoptive mothers) or there were two mothers and the type for both was -7 (refused) or -8 (don't know), the mother with the lowest person number in the household roster was identified as parent 1 and the other mother was identified as parent 2. \uf06e If there were two fathers in the household, an order of preference was used to identify one father to be parent 1, with the order specified as biological, adoptive, step-, foster father or male guardian, then other male parent or guardian. The other father was identified as parent 2. If there were two fathers of the same type (e.g., two adoptive fathers) or there were two fathers and the type for both was -7 (refused) or -8 (don't know), the father with the lowest person number in the household roster as identified as parent 1 and the other father was identified as parent 2."}, {"section_title": "7-38", "text": "X1HPARNT and X2HPARNT indicate the type(s) of parents living in the household with the study child. The values for the X1HPARNT and X2HPARNT composites are as follows: Each of these sections was completed during the parent interview for up to two parents or parent figures. 35 To indicate which household member or members were the subject of each section, \"pointer\" variables that hold the original number of the household member on the household roster were used. To illustrate how the pointer variables work, suppose there is a household with both a mother and a father who were listed third and fourth individuals in the household roster. Household member #3, the mother, will be parent 1 and X1IDP1 will equal 3. The pointer variables correspond to the parent identifiers; thus the pointer for the PEQ education section, P1PEQHH1, will also equal \"3.\" The answers to the education questions for the mother will be contained in variables for this section of the interview that end with the suffix \"_1\" (e.g., P1HIG_1, P1ENR_1, P1FPT_1, etc.). The suffix \"_1\" indicates that the data are for the first parent. Similarly, household member #4, the father, will be parent 2 and X1IDP2 will equal 4. The pointer variable for the PEQ education section, P1PEQHH2, will also equal \"4.\" The answers to the education questions for the father will be contained in variables for this section of the interview that end with the suffix \"_2\" (e.g., P1HIG_2, P1ENR_2, P1FPT_2, etc.). The suffix \"_2\" indicates that the data are for the second parent. Table 7-1 identifies the pointer variables included on the data file. X1IDP1 is always equal to the pointer variables P1EMPP1, P1PEQHH1, and P1PLQHH1 (where applicable) and X1IDP2 is always equal to P1EMPP2, P1PEQHH2, and P1PLQHH2 (where applicable). In addition, X2IDP1 is always equal to P2EDUP1 (for R1 nonrespondents) and X2IDP2 is always equal to P2EDUP2 (for R1 nonrespondents). There is no difference between the pointer variables and the composite variables that identify the parents, other than when a pointer is not applicable (the PLQ pointers for household only speaking English, for example). 7-41 "}, {"section_title": "Parent Demographic Variables (X1PAR1AGE, X2PAR1AGE, X1PAR2AGE, X2PAR2AGE, X1PAR1RAC, X2PAR1RAC, X1PAR2RAC, X2PAR2RAC)", "text": "X1PAR1AGE and X2PAR1AGE are the composite variables for the age of the first parent from the household roster, and X1PAR2AGE and X2PAR2AGE are the composite variables for the age of the second parent from the household roster. 36 The ages of all household members were reported by respondents in either the fall or spring parent interview, depending on when each person was first recorded as a household member. For information about how the first and second parents were selected for these and other parent variables, see section 7.5.3.3 above. The composites for race/ethnicity for the parent/guardians were derived in the same way as those for the child, except that there is not a variable that supplements parent-reported race/ethnicity with mother and her boyfriend, the race/ethnicity of the mother was obtained but that of the boyfriend was not unless he was the respondent. 40"}, {"section_title": "Parent Education Variables (X12PAR1ED_I, X12PAR2ED_I)", "text": "There Values of \"doctorate degree\" and \"professional degrees after a bachelor's degree\" (e.g., P1HIG_1=22 or23) were coded as 9 (doctorate or professional degree). The variables reflect the education level of both parent (birth, adoptive, step-, and foster) and nonparent guardians identified in X1IDP1, X1IDP2, X2IDP1, and X2IDP2. For example, if the child did not live in a household with his or her parents and lived with a nonparent guardian, the education of the guardian and his or her spouse or partner was used in the creation of the composites if the guardian was 40 There are 7 cases where a household member has valid race/ethnicity data but does not meet the design criteria for collecting this information (CHILDID=10004013, 10004059, 10010110, 10017053, 10004449, 10015181, 10017209). This inconsistency resulted from post-data collection editing. Since each of the cases involves the spouse of the respondent (who is a parent figure), the race data were retained because they were used in the parent composites and statistical processing. Additionally, there are 9 cases for which a father/male guardian was identified after data collection (CHILDID=10015236, 10015552, 10011399, 10016048, 10001140, 10002341, 10004530, 10006902, 10009138). Because no father/male guardian was identified during the interview, these cases followed the skip specifications for households with no father/male guardian and were not asked questions about the father, including fathers' race/ethnicity. The race and ethnicity items for these cases are set to -9. "}, {"section_title": "Parent Occupation Variables (X1PAR1EMP, X1PAR2EMP, X1PAR1OCC_I, X1PAR2OCC_I, X1PAR1SCR_I, X1PAR2SCR_I)", "text": "Several composites can be used to describe parents' employment, their occupations, and the prestige of their occupations. X1PAR1EMP and X1PAR2EMP describe the work status of each of the parents. 42 To code X1PAR1EMP for parent 1, the parent identification variable for parent 1, X1IDP1, is matched to the pointer variable for employment data, P1EMPP1, to obtain the employment data that 42 These variables are a combination of P1HDEMP and P1HMEMP in the ECLS-K. Otherwise, if parent 1 was looking for work, but the variables for the five activities to actively look for work (described above) were all coded -7 (refused), -8 (don't know), or -9 (not ascertained), or variables about working for pay, being on vacation, hours worked, looking for work, or what the parent did to look for work were all coded -7 (refused), -8 (don't know), or -9 (not ascertained), then X1PAR1EMP is coded as -9 (not ascertained). X1PAR2EMP is created in the same was as X1PAR1EMP, but is for parent 2. The composite variables about parent occupation, X1PAR1OCC_I and X1PAR2OCC_I, are Policy and Planning, 1980) is used to identify the appropriate code. Both of these manuals use an 43 Because some persons were not looking for work according to the five categories described above, even though it was reported that a parent was looking for work (P1LOK_1=1), the parent is coded as not in the labor force (X1PAR1EMP=4) rather than as looking for work (X1PAR1EMP=3). If a parent was reported as looking for work (P1LOK_1=1), the questions about the parent's last occupation were asked. There are 127 cases with occupation data that are categorized as X1PAR1EMP = 4 (not in the labor force), and 85 cases that have X1PAR2EMP = 4 (not in the labor force) because they indicated that all they were doing to look for work was looking at/reading want ads or some \"other\" activity that did not qualify them to be classified as looking for work. 44 These variables are a combination of P1MOMOCC and P1DADOCC in the ECLS-K. "}, {"section_title": "Engineers, Surveyors, and Architects", "text": "\uf06e This category includes occupations concerned with applying principles of architecture and engineering in the design and construction of buildings, equipment and processing systems, highways and roads, and land utilization."}, {"section_title": "Natural Scientists and Mathematicians", "text": ""}, {"section_title": "Social Scientists, Social Workers, Religious Workers, and Lawyers", "text": "\uf06e This category includes occupations concerned with the social needs of people and with basic and applied research in the social sciences."}, {"section_title": "Teachers: College, University, and Other Postsecondary Institution; Counselors, Librarians, and Archivists", "text": "\uf06e This category includes those who teach at higher education institutions and at other postsecondary (after high school) institutions, such as vocational institutes. In addition, vocational and educational counselors, librarians, and archivists are included here."}, {"section_title": "7-47", "text": "Exhibit 7-5. Industry and occupation codes used in the ECLS-K:2011-Continued"}, {"section_title": "Teachers, except Postsecondary Institution", "text": "\uf06e This category includes prekindergarten and kindergarten teachers, elementary and secondary teachers, special education teachers, instructional coordinators, and adult education teachers (outside postsecondary)."}, {"section_title": "Physicians, Dentists, and Veterinarians", "text": ""}, {"section_title": "Registered Nurses, Pharmacists, Dieticians, Therapists, and Physician's Assistants", "text": "\uf06e This category includes occupations concerned with the maintenance of health, the prevention of illness and the care of the ill through the provision and supervision of nursing care; compounding drugs, planning food service or nutritional programs; providing assistance to physicians; and the provision of therapy and treatment as directed by physicians."}, {"section_title": "Writers, Artists, Entertainers, and Athletes", "text": "\uf06e This category includes occupations concerned with creating and executing artistic works in a personally interpreted manner by painting, sculpturing, drawing, engraving, etching, and other methods; creating designs for products and interior decorations; designing and illustrating books, magazines, and other publications; writing; still, motion picture, and television photography/filming; producing, directing, staging, acting, dancing, singing in entertainment; and participating in sports and athletics as a competitor or player and administering and directing athletic programs."}, {"section_title": "Health Technologists and Technicians", "text": "\uf06e This category includes occupations concerned with providing technical assistance in the provision of health care. For example, clinical laboratory technologists and technicians, dental hygienists, radiologic technicians, licensed practical nurses (LPNs), and other health technologists are included here."}, {"section_title": "Technologists and Technicians, except Health", "text": "\uf06e This category includes those providing technical assistance in engineering and scientific research, development, testing, and related activities, as well as operating and programming technical equipment and systems."}, {"section_title": "7-48", "text": "Exhibit 7-5. Industry and occupation codes used in the ECLS-K:2011-Continued"}, {"section_title": "Marketing and Sales Occupations", "text": "\uf06e This category includes occupations involving selling goods or services, purchasing commodities and property for resale, and conducting wholesale or retail business."}, {"section_title": "Administrative Support Occupations, including Clerks", "text": "\uf06e This category includes occupations involving preparing, transcribing, transferring, systematizing, and preserving written communications and records; collecting accounts; gathering and distributing information; operating office machines and data processing equipment; operating switchboards; distributing mail and messages; and other support and clerical duties such as bank teller, data entry keyer, etc."}, {"section_title": "Service Occupations", "text": ""}, {"section_title": "Agricultural, Forestry, and Fishing Occupations \uf06e", "text": "This category is concerned with the production, propagation (breeding/growing), gathering, and catching of animals, animal products, and plant products (timber, crop, and ornamental); the provision of services associated with agricultural production; and game farms, fisheries, and wildlife conservation. \"Other agricultural and related occupations\" include occupations concerned with the production and propagation of animals, animal products, plants, and products (crops and ornamental)."}, {"section_title": "Mechanics and Repairers", "text": ""}, {"section_title": "Construction and Extractive Occupations", "text": "\uf06e This category includes occupations that normally are performed at a specific site, which will change over time, in contrast to production workers, where the work is usually at a fixed location. Construction workers include those in overall construction, brick masons, stonemasons, carpenters, electricians, drywall installers, paperhangers and painters, etc. Extractive occupations include oil well drillers, mining machine operators, and so on."}, {"section_title": "7-49", "text": "Exhibit 7-5. Industry and occupation codes used in the ECLS-K:2011-Continued"}, {"section_title": "Precision Production Occupations", "text": "\uf06e Precision production includes occupations concerned with performing production tasks that require a high degree of precision or attainment of rigid specification and operating plants or large systems. Included in this category are tool and die makers, pattern and model makers, machinists, jewelers, engravers, and so on. Also included are some food-related workers including butchers and bakers. Plant and system operators include water and sewage, gas, power, chemical, petroleum, and other plant or system operators."}, {"section_title": "Production Working Occupations", "text": "\uf06e This category includes occupations concerned with setting up, operating, and tending of machines and hand production work, usually in a factory or other fixed place of business."}, {"section_title": "Transportation and Material Moving Occupations", "text": "\uf06e This category includes occupations concerned with operating and controlling equipment used to facilitate the movement of people or materials and the supervising of those workers."}, {"section_title": "Handlers, Equipment Cleaners, Helpers, and Laborers", "text": ""}, {"section_title": "Unemployed, Retired, Disabled, or Unclassified Workers", "text": "\uf06e This category includes persons who are unemployed, have retired from the work force, or are disabled. It also includes unclassified occupations that do not fit into the categories above (e.g., occupations that are strictly military, such as \"tank crew member\" and \"infantryman\"). The occupation variables X1PAR1OCC_I and X1PAR2OCC_I were recoded to reflect the average of the 1989 General Social Survey (GSS) prestige scores. The variables X1PAR1SCR_I and X1PAR2SCR_I describe the prestige scores associated with each coded occupation. Although the GSS prestige scores are from 1989, they are still being used by the current GSS survey and matched to 1980 census codes. 45 Because these prestige scores were also used for the ECLS-K 1998-99 cohort, they will allow for comparisons to the ECLS-K. Table 7-2 provides details on how occupations were assigned prestige score values (X1PAR1SCR_I, X1PAR2SCR_I). As described in section 7.5.3.8, occupations were imputed if they were missing from the parent interview. If the parent's occupation was either -1 (No Occupation) or 22 (Unemployed or Retired) on X1PAR1OCC_I or X1PAR2OCC_I, the assignment of a prestige score depended upon X1PAR1EMP or X1PAR2EMP (employment status). If, for example, X1PAR1EMP was missing, it was imputed; if the parent was imputed as working, an occupation was also imputed and the appropriate prestige score was assigned. Missing occupations were not imputed for persons who were looking for work. During data preparation, it was decided to assign values of -9 (not ascertained) for the occupation and prestige variables for these parents, since persons looking for work are in the labor force. Additionally, cases in which a parent was identified during data editing and after imputation was completed, as well as cases without information on household composition, are set to missing (-9) on the occupation variables, rather than being imputed. The imputation flag variable IFX1PAR1SCR reflects imputation of the occupation (X1PAR1OCC_I) and resultant coding of prestige (X1PAR1SCR_I) for parent 1. The flag IFX1PAR2SCR reflects imputation of the occupation (X1PAR2OCC_I) and resultant coding of prestige (X1PAR2SCR_I) for parent 2. "}, {"section_title": "7-51", "text": ""}, {"section_title": "Household Income and Poverty (X2INCCAT_I, X2POVTY)", "text": "Household income data were collected in spring 2011. All parents were asked to report income by broad range ($25,000 or less or more than $25,000) and by detailed range (table 7-3). When parent respondents reported a household income indicating the household was close to or lower than 200 percent of the U.S. Census Bureau poverty threshold for a household of its size, the respondents were asked to report household income to the nearest $1,000 (referred to as exact income). 48 Table 7-4 shows the reported income and household size for households near 200 percent of the poverty threshold. The variable for exact income (P2TINCTH), the detailed income range variable (X2INCCAT_I), and the household total (X2HTOTAL) were used to create a household-level poverty variable (X2POVTY). 49 Parent report of exact household income was used to calculate the poverty composite unless it was missing or not asked, in which case the detailed income category was used. When the detailed income category was used, the case was assigned the midpoint of the detailed income range. .At the time that the spring 2011 parent interview was finalized, the most updated poverty thresholds available were the weighted 2009 poverty thresholds. Preliminary 2010 poverty thresholds were available at the time the poverty composite variable X2POVTY was computed. Although the two thresholds were somewhat different, all households that were asked exact income using the 2009 thresholds would have been asked for exact income using the 2010 thresholds, with one exception. The threshold for 200 percent of poverty in a household with eight persons was $74,504 in 2009 and $75,726 in 2010. The skip in the parent interview directed exact income to be collected if a household of eight had a reported income of $75,000 or less. Thus, exact income would not have been collected for someone in a household of eight who reported an income of $75,001 to $100,000. This potentially affected 13 cases that were coded as category 3 (at or above 200 percent of the poverty threshold) when they may have been in category 2 (at or above the poverty threshold, below 200 percent of the poverty threshold). The income category for those cases was $75,001 to $100,000, a wide enough range that it is possible that incomes for these 13 cases were actually above the threshold of $75,726. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2011. Total household income reported in the parent interview was compared to preliminary census poverty thresholds for 2010, which vary by household size. "}, {"section_title": "7-54", "text": "weighted poverty thresholds from the U.S. Census Bureau that were used to determine household poverty status in the base-year data collection. Households with a total income that fell below the appropriate threshold were classified as category 1 in the composite. Households with a total income that was at or above the poverty threshold but below 200 percent of the poverty threshold were classified as category 2 in the composite. Households with a total income that was at or above 200 percent of the poverty threshold were categorized as category 3 in the composite. 50 For example, if a household contained two members and the household income was lower than $14,220, the household was considered to be below the poverty threshold and would have a value of 1 for the composite. If a household with two members had an income of $14,220 or more, but less than $28,440 (200 percent of the poverty threshold for a household of two), the composite would have a value of 2. If a household with two members had an income of $28,440 or more, the composite would have a value of 3. If the detailed income range was used instead of exact income and the midpoint of the range was below the poverty threshold for the household size, then the household was classified as \"below the poverty threshold.\" If the midpoint of the detailed income range fell at or above the poverty threshold for the household size, then the case was classified as \"at or above the poverty threshold, but below 200 percent of the poverty threshold\" or \"at or above 200 percent of the poverty threshold.\"  50 In the ECLS-K:2011, there are three categories in the poverty composite rather than two categories for \"below poverty threshold\" and \"at or above poverty threshold\" as there were in the ECLS-K. 7-55 The information on these characteristics was collected as follows: \uf06e Parent/guardian's education. The education data were collected in fall 2010. In spring 2011, education information was collected only for fall 2010 nonrespondents. \uf06e Parent/guardian's occupation. Parent/guardian occupation data were collected in fall 2010. These data were not collected in the spring for fall 2010 nonrespondents. \uf06e Household income. Household income data were collected in spring 2011. All parents were asked to report income by broad range ($25,000 or less or more than $25,000) and by detailed range. When parent respondents reported a household income indicating the household was close to or lower than 200 percent of the U.S. Census Bureau poverty threshold for a household of its size, the respondents were asked to report household income to the nearest $1,000 (referred to as exact income). Because not all households were asked to report their exact income, the midpoint of the detailed income range was used to compute the SES composite. Not all parents completed the parent interview; among those who did, not all responded to every question. Therefore, there were missing values for some of the components of the SES composite variable. The numbers of cases with missing data for each of the SES components are presented in table 7-6. Because not all parent respondents provided complete education, occupation, and household income information, it was necessary to impute missing values for the SES components before computing the SES composite. 51 Imputation was done separately for each component using the hot deck method. In this method, similar respondents and nonrespondents are grouped or assigned to \"imputation cells,\" and a respondent's value is randomly \"donated\" to a nonrespondent within the cells. in the fall and there were two parents in the spring, the second parent's values were imputed. There are 51 An exception to the imputation procedure was made for the 83 cases that do not have a household roster. These 83 cases include 81 cases that did not respond to the fall kindergarten parent interview and completed the SPQ section of the spring parent interview (which included questions only asked of fall nonrespondents) but did not complete the FSQ section (family structure questions) in the spring. In addition, there are 2 cases that do not have a roster because of a technical problem in the interview program that was corrected early in data collection, but after these interviews were completed. Due to the absence of the household roster and information about the child's household and parents, X12SESL was set to missing (-9)."}, {"section_title": "7-56", "text": ""}, {"section_title": "7-57", "text": "218 cases that are children who, in the fall, did not have a second parent figure in the household but did have a second parent in the spring. Because these cases were fall respondents to the parent interview, education questions were not asked again and there were missing data about the second parent in the in the spring. For these cases, education values for the second parent were imputed. For missing values that were imputed, a value reported by a respondent for a particular component (education, occupation, household income category) was assigned or \"donated\" to a \"similar\" person who failed to respond to that question. A \"similar\" person is one who has the same characteristic as the donor; these characteristics are demographic characteristics chosen to form imputation cells. Auxiliary information known for both donors and nonrespondents were used to form imputation cells. The imputed value for a case with a missing value was taken from a randomly selected donor among the respondents within the cell. For each SES component, imputation cells were created using demographic characteristics that are the best predictors of the component. Characteristics such as census region, school type (public/Catholic/non-Catholic religious/other private), school locale (city/suburb/town/rural), household type (female single parent/male single parent/two parents present), parents' race/ethnicity, and parents' age range were used to form the cells. Chi-square automatic interaction detector (CHAID) analyses were used to determine these predictors. The order of imputation was parent 1's education, parent 2's education, parent 1's labor force status, parent 1's occupation, parent 2's labor force status, parent 2's occupation, detailed income range when the broad income range was known, and detailed income range when the broad income range was not known. Imputation cells for each component to be imputed were created using the other components. For example, education and occupation were used to impute income. In the case of households with two mothers or two fathers, the order of the parent data in the interview was used. In households with a mother and a father, parent 1 is the mother and parent 2 is the father. The hot deck imputation was implemented as follows: \uf06e For households having two parents present, parent 1's and parent 2's variables were imputed separately. \uf06e Imputed as well as reported values were used to create imputation cells. \uf06e Imputed values were not donated."}, {"section_title": "7-58 \uf06e", "text": "A record was not used as a donor more than once for any particular variable or more than twice overall. After imputation, the occupation variables were also recoded to reflect the average of the 1989 GSS prestige scores, described in section 7.5. x w is the weighted mean of x hi ; and sd (x w ) is the standard deviation of x w . Note that where h is household income, x hi is the natural log of the midpoint of the detailed income range. The SES variable for the i-th household will then be computed as where m is the number of components. Note that for households with only one parent present and for parents who are unemployed and not looking for work, retired, or not currently in the labor force, not all the components are defined. In these cases, the SES is the average of the z-scores of the available components."}, {"section_title": "Respondent ID and Relationship to Focal Child (X1RESID, X2RESID, X1RESREL, X2RESREL)", "text": "The Because the interview in spring 2011 asked for the previous round respondent, the respondent in fall 2010 (X1RESID) and spring 2011 (X2RESID) will, in many cases, be the same person. 53 There are 83 cases in the kindergarten year data set that lack rosters of household members. All of these cases were nonrespondents to the fall kindergarten parent interview. Of these 83 cases, 81 of them did not respond to the fall kindergarten parent interview and completed the SPQ section of the spring parent interview (which included questions only asked of fall nonrespondents) but did not complete the FSQ section (family structure questions) in the spring. In addition, there are 2 cases that do not have a roster because of a technical problem in the interview program that was corrected early in data collection, but after these interviews were completed. These 83 cases can be easily identified by specifying X2RESID (spring 2011 respondent) = -9. There is also one additional case (10014051) that has X2RESREL=-9 because the case broke off before the relationship question was asked. 7-60"}, {"section_title": "7.5.3.10", "text": "Whether the Child's Biological Mother and Biological Father Were Married at the"}, {"section_title": "Time of the Child's Birth (X12MOMAR)", "text": "Questions about marital status and the date of marriage were used with the composites for the child's date of birth to create a composite variable X12MOMAR for whether the child's biological mother and father were married at the time of the child's birth. 54 There are four questions in the fall 2010 parent interview about whether the child's biological parents are or were married and, if so, the month and year that the marriage took place. The questions about the month and year of marriage depend on which marriage questions were asked. If the biological parents were married at the time of the parent interview (P1LEGMAR, P1BIOMRY, P1KNOWLE, or P1BIOPAR=1) and the year that the biological parents married (P1MRRYYR if P1LEGMAR=1, P1BIOMYR if P1BIOMRY=1, or P1MDWHY if P1KNOWLE=1 or P1BIOPAR=1) was before the year the child was born (X_DOBYY), or if the year the biological parents married (P1MRRYYR, P1BIOMYR, P1MDWHY) was the same as the year the child was born (X_DOBYY), and the month the biological parents married (P1MRRYMO if P1LEGMAR=1, P1BIOMRM if P1BIOMRY=1, P1MDWHY if P1KNOWLE=1) was the same as or before the month the child was born (X_DOBMM), X12MOMAR is coded as 1 (yes). X12MOMAR is also coded as 1 (yes) if it was reported in the spring 2011 parent interview that the child's biological mother and biological father were married to each other when the child was born (P2BIOMRY=1). X12MOMAR is coded as -9 (not ascertained) if one of the following conditions is true: the question about whether the biological parents were married (P1LEGMAR) was answered as 1 (yes) or 2 (no), but data were -9 (not ascertained) for questions that followed (P1MRRYYR, P1BIOMRY, P1BIOPAR); data are -9 (not ascertained) for whether the biological parents are married (P1LEGMAR=-9) and data are -9 (not ascertained) for questions that followed (P1MRRYYR, P1BIOMRY, P1BIOPAR); the biological parents were married (P1LEGMAR=1), but data are missing for the month of marriage (P12MRRYMO=-7 (refused), -8 (don't know), or -9 (not ascertained); the biological parents were not married (P1LEGMAR=2), and data are missing about whether the biological parents were ever married (P1BIOPAR=-7 (refused), -8 (don't know), or -9 (not ascertained)); or the parents were married (P1BIOPAR=1), but the data for the year and month are missing (P1MDWHY and P1MDWHM=-7 (refused), -8 (don't know), or -9 (not ascertained)). Otherwise if the year of marriage (P1MRRYYR, P1BIOMYR, or P1MDWHY) was after the child's year of birth (X_DOBYY), or the year of marriage was the same as the year of the child's birth (P1MRRYYR=X_DOBYY) and the month of marriage (P1MRRYMO, P1BIOMRM, P1MDWHM) was greater than the child's month of birth (X_DOBMM), or the biological parents were reported to not be married (P1LEGMAR=2 or P1BIOMRY=2 or P2BIOMRY=2 or P1BIOPAR=2 or P2BIOMRY=2), then X12MOMAR is coded as 2 (no). Otherwise, X12MOMAR is coded as -9 (not ascertained). It should be noted that, for biological parents who were not married at the time of the fall 2010 parent interview, but had been married in the past, X12MOMAR did not take into account the date that marriages ended. It is possible that the biological parents were married prior to the child's birth but were no longer married at the time of birth. Analysts may wish to use the variables P1ENDMO and P1ENDYR to modify this composite variable."}, {"section_title": "Teacher Composite Variables", "text": "In addition to the teacher data flags discussed in section 7.4.3 above, there are several composite variables on the file that use data from teachers. There is a composite variable (X12CHGTCH) about whether the child changed teachers from fall to spring. That is discussed below in section 7.7 about children who changed schools or teachers. There are also composite variables about the child's closeness and conflict with the teacher (X2CLSNSS, X2CNFLCT). These are described in chapter 3, along with other variables from teacher reports of children's social skills (e.g., X1TCHEXT). Other variables that use teacher data are about the child and are discussed with the child composites (e.g., X1CLASS)."}, {"section_title": "School and Class Composite Variables", "text": "Variables describing school and class characteristics were constructed from the teacher and school data and the sample frame. Details on how these variables were created follow. 7-62 7.5.5.1"}, {"section_title": "School Type (X1KSCTYP, X2KSCTYP)", "text": "In fall 2010, FMS data about school type (from the sample frame) were used to create the school type composite (X1KSCTYP). In spring 2011, the school administrator questionnaire contained a question on school type that was used in the creation of the spring school type composite (X2KSCTYP). In spring 2011, X2KSCTYP was created as follows: If question A6 in the school administrator questionnaire (\"Which of the following characterizes your school?\") was answered as \"a regular public school (not including magnet school or school of choice)\" (S2REGSKL); \"a public magnet school\" (S2MAGSKL); \"a charter school\" (S2CHRSKL); or \"a public school of choice (open enrollment)\" (S2CHCESK), the school was coded as \"public.\" If the question was answered as \"a Catholic school\" of any type (S2CATHOL, S2DIOCSK, S2PARSKL, or S2PRVORS), the school was coded as \"Catholic.\" If the question was answered as \"other private school, religious affiliation\" (S2OTHREL), the school was coded as \"other religious.\" Otherwise, if the question was answered as \"private school, no religious affiliation\" (S2OTNAIS, S2OTHRNO), then the school was coded as \"other private.\" If data from the school administrator questionnaire were missing, FMS data about school type were used. Homeschooled children (those schooled at home instead of at school) were coded as -1 (not applicable). These children were enrolled at the time of sampling, but became homeschooled during the 2010-11 school year. Children who moved and were not followed for round 2 and children who were not located in spring 2011 have missing values (-9) for X2KSCTYP. In addition, these children have a value of 9900000 on the variable F2CCDLEA. There were seven cases that had discrepancies between the FMS data from the frame and the school administrator questionnaire. In five of the seven cases, public information on the Internet about the schools indicated that the school types corresponded to the FMS rather than to data from the school administrator questionnaire. These five cases were recoded according to the school type in the FMS. Public information about the other two school types indicated that the school type corresponded to the information provided on the school administrator questionnaire. These school types were left as coded according to the school administrator questionnaire and not changed. If a school type for a child is not the same in fall 2010 and spring 2011, the child may have changed schools. The composite variable X12CHGSCH (described in section 7.7.1) can be used to determine if the child changed schools."}, {"section_title": "Percent Non-White Students in the School (X2KRCETH)", "text": "The composite variable X2KRCETH indicates the percentage of the student population that was non-White in the spring of 2011. 55 The composite is derived from a question in the school administrator questionnaire (question A7) that asked the number or percentage of students in the school who were the following race/ethnicities: Hispanic/Latino of any race; American Indian or Alaska Native, or two or more races, not Hispanic or Latino. The composite was calculated by summing the percentages for all categories except White, not Hispanic or Latino. School administrators were allowed to report their answers to the student racial composition questions as either numbers or percents. All answers provided as numbers were converted to percentages before computing the composite variable. The sum of these calculated percentages across all categories was allowed to sum to within +/-5 percent of 100 percent to allow for minor reporting errors of numbers that did not add to the reported total or percentages that did not add to 100 percent. In a few cases, this procedure resulted in a total sum of percentages that was slightly over 100 percent. Totals greater than 100 percent are top-coded to 100 percent. A flag for each individual race/ethnicity variable is included on the data file and indicates whether the school administrator reported the information as a number or a percent. 56 Because the composite is calculated as a percent, these flags will not be needed by users unless they are interested in examining how answers were reported. If the flag (S2ASIAFL S2HISPFL, S2BLACFL, S2WHITFL, S2AIANFL, S2HAWPFL, and S2MULTFL) for each of the race/ethnicity variables (S2ASIAPT, S2HISPPT, S2BLACPT, S2WHITPT, S2AIANPT, S2HAWPPT, and S2MULTPT) is equal to 1, that indicates the information was reported by the school administrator as a percentage. 55 This variable was S2KMINOR in the ECLS-K. In the ECLS-K:2011, there is a new variable factored into the composite that indicates the percentage of students classified as \"two or more races, not Hispanic or Latino\" (S2MULTPT). 56 There were also other questions in the school administrator questionnaire that allowed for answers to be recorded as either a number or percent. ; and S2MULTF2 (question about teachers of two or more races, not Hispanic or Latino, reported as number or percent). In all cases, the variables related to these flags provide information as numbers or percentages, with the flags indicating how the answers were originally reported by school administrators."}, {"section_title": "7-65", "text": "In some cases, the composite could not be derived from the data because of missing data or errors. If the composite could not be derived from the data, the percentage of non-White students in the school was obtained from the 2009-10 CCD (for public schools) or the 2009-10 PSS (for private schools). If these data could not be obtained from the school administrator, the CCD, or the PSS (and thus were missing), the composite is coded -9 (not ascertained). If the study child was homeschooled in spring 2011, the composite is coded -1 (not applicable)."}, {"section_title": "Highest and Lowest Grade at the School (X2LOWGRD, X2HIGGRD)", "text": "There are composite variables derived from information collected from the school administrator during the spring data collection that indicate the lowest grade taught at the school (X2LOWGRD) and the highest grade taught at the school (X2HIGGRD). Data from the frame were used if information collected from the school administrator was missing. Both variables are created by first coding answers of \"ungraded\" in question A5 of the school administrator questionnaire (\"Mark all grade levels included in your school\") or ungraded in the data from the frame as category 15 (ungraded) and then coding the lowest grade in the school and the highest grade in the school, respectively. The grade level for children in transitional kindergarten, kindergarten, or pre-first grade is coded as category 2 (kindergarten)."}, {"section_title": "Students Approved for Free or Reduced-Price School Lunch (X2FLCH2_I, X2RLCH2_I)", "text": "Composites indicating the percent of students in the school who were approved for free school lunch and the percent of students in a school who were approved for reduced-price school lunch were derived from information collected from the school administrator during the spring data collection. 57 There were five recoding rules used for data with errors: (1) If answers were reported as numbers and the total number of students was missing, the total from another question about total enrollment (Q3a S2ANUMCH) was used if the difference between the summed total and the reported Q3a total was within a 5 percent confidence interval (95-105 percent). (2) If the method of reporting was mixed (some in numbers, other in percents), the race percentages were coded as -9. (3) If percentages were recorded, with none of the above errors, and the summed total across categories was within +/-5 percent of 100 percent, any blanks were recoded to 0. (4) If the summed total was not 95 -105 percent of the sum reported or not 95 -105 percent of total enrollment from another question (Q3a S2ANUMCH), the individually reported percentages and numbers were made -9. (5) If numbers were reported, with none of the above errors, and the summed total across categories were within +/-5 percent of the reported total, any blanks were recoded to 0."}, {"section_title": "7-66", "text": "These composites were computed at the school level for public and private schools 58 participating in the study that had at least one participating child or parent respondent in the spring 2011 data collection. School administrators were asked to report on the total enrollment in the school (S2ANUMCH), the number of children in the school who were approved for free school meals (S2NUMFRM), and the number of children who were approved for reduced-price school meals (S2NUMRDM). The percent of children approved for free school lunch is computed based on the ratio of S2NUMFRM to S2ANUMCH. Likewise, the percent of children approved for reduced-price school lunch is based on the ratio of S2NUMRDM to S2ANUMCH. 59 Some school administrators did not complete the school administrator questionnaire, and among those who did, not all responded to all three questions needed to compute these composites related to approval for free or reduced-price meals. If school administrator data for public schools were missing, data were taken from the frame. The frame data used for these composites came from the 2009-10 CCD (Common Core of Data). 60 If data were also missing from the frame, the composite variables were missing. No frame data were available for private schools. Hot-deck imputation was conducted for cases with missing values of these composites for public schools. Hand imputation was used for a small number of private schools. 61 Table 7-7 shows the level of missing data for the school lunch composite variables among the schools that had at least one child or parent respondent in the spring 2011 data collection. In hot-deck imputation, if a school has a nonmissing value of the school lunch compositefree-lunch or reduced-price lunch-that value was assigned or \"donated\" to a similar school with missing value of the composite. Schools are similar if they belong in the same imputation cell. Imputation cells were created using district poverty category (created from the district poverty variable X_DISTPOV described in section 7.5.6.3), and whether the school received Title I funding. Within each imputation cell, the schools were sorted by longitude and latitude."}, {"section_title": "School Year Start and End Dates (X2SCHBDD, X2SCHBMM, X2SCHBYY, X2SCHEDD, X2SCHEMM, X2SCHEYY)", "text": "The composite variables indicating school year start and end dates were derived from the school administrator questionnaire question A2 (S2SYRSMM, S2SYRSDD, S2SYRSYY, S2SYREMM, S2SYREDD, S2SYREYY). If those data were not collected from the school administrator, data were taken from information contained in the FMS.  43-Rural, Remote: Census-defined rural territory that is more than 25 miles from an urbanized area and is also more than 10 miles from an urban cluster."}, {"section_title": "FMS Composite Variables", "text": "Several composite variables were created from data stored in the FMS, which were obtained from frame data as well as by field staff during visits to the school and discussions with school staff."}, {"section_title": "Year-Round Schools (X12YRRND)", "text": "The values for the year-round school composite variable are 1 (year-round school) and 2 (not year-round school). If the child was homeschooled in spring 2011, the composite is coded as -1 (not applicable). This composite is based on the FMS indicator of being a year round school, which is obtained from school coordinator. It is not based on the report from the school administrator. In 14 schools, the FMS indicated that the school was not a year-round school, but the school administrator reported that it 7-70 was (S2YROUND = yes). In 10 schools, the FMS indicated that the school was a year-round school, but the school administrator reported that it was not (S2YROUND = no)."}, {"section_title": "School District Poverty (X_DISTPOV)", "text": "Using "}, {"section_title": "Methodological Variables", "text": "To facilitate methodological research, 11 variables pertaining to aspects of the data collection work are included on the data file. These include identifiers for parent interview work area (F1PWKARE, F2PWKARE), parent interviewer (F1PINTVR, F2PINTVR), child assessor (F1CASSOR, F2CASSOR), and child assessment work area (F1CWKARE, F2CWKARE) and were extracted from the FMS. A \"work area\" is the group of schools that each team leader was assigned. Team leaders managed a group of 2 to 4 other individuals who worked as child assessors and parent interviewers for the sampled cases in the work area."}, {"section_title": "Children Who Changed Schools or Teachers", "text": "There are variables in the file that can be used to determine if a child moved to a different school between rounds of data collection or changed teachers, either because of a change in schools or a change of teachers within a school. or who participated in spring 2011 but not fall 2010, were coded -9, \"not ascertained,\" on X12CHGSCH. Children who were homeschooled in spring 2011 were coded -1, \"not applicable,\" for X12CHGSCH."}, {"section_title": "Children Who Changed Teachers Between Rounds (X12CHGTCH)", "text": "Teacher identification numbers (T1_ID, T2_ID) and the composite X12CHGSCH (school change between fall 2010 and spring 2011) were used to determine whether children changed teachers between fall 2010 and spring 2011. If the fall and spring teacher identification numbers were not missing and were equal to each other, then X12CHGTCH was coded 0 (no change). If a teacher identification number was missing in either fall 2010 or spring 2011, and the school change composite indicated the"}, {"section_title": "4.", "text": "Type \"D:\\Setup.exe\" into the Open field of the Run screen, shown in exhibit 8-1. If your CD-ROM drive is assigned a different drive letter, substitute that letter for the \"D.\" Exhibit 8-1. Windows Run screen\nSelect ECB and click the Uninstall button.\nThe selected variable is highlighted.\nClick the OK button.\nEnter the new name for the taglist in the Taglist Name field.\nEnter the new name of the taglist in the Taglist Name field.\nEnter the file name for your taglist.\nThe Import Taglist From dialog box appears (exhibit 8-34).\nThe Delete Taglist selection screen, shown in exhibit 8-35, appears with the taglists listed that may be deleted.\nSelect OK.\nClick the Export codebook button:\nPrint the image to the printer using the print function of the application that you are using. When you are done viewing the entire codebook, close the window by clicking the \"X\" in the upper right corner of the screen. You will return to the main screen."}, {"section_title": "5.", "text": "Click the OK button to start the installation. You will now see several installation screens, some of which will prompt you for a response.\nFollow any prompts. You will be prompted by the InstallShield Wizard to confirm the software removal and finish the process.\nFollow any prompts. You will be prompted by the InstallShield Wizard to confirm the software removal and finish the process.\nThe field ID of the current variable selected is shown on the right of the Go button (exhibit 8-18).\nClick the Search button to initiate the search.\nClick the Search button to initiate the search.\nThe new variables are added to your Working Taglist. Removing or untagging required variables from the Working Taglist is not permitted by the ECB. A message will be displayed indicating that the required variable cannot be untagged.\nClick the Save button.\nClick the Save button.\nClick the Save button. If a name that already exists is entered, you will be prompted to replace the old taglist file with the new exported taglist file. Click Yes only if you wish to replace the old taglist with the new taglist or enter a unique name. The Working Taglist will be saved under the file name you enter. \nEnter the file name for the taglist you want to import.\nHighlight the taglist that is to be deleted and click the OK button.\nOnce you have finished viewing the codebook, close the screen by clicking on the Windows \"X\" control located in the top right corner of the window. You may also close the window using the other standard Windows defaults: by clicking on the windows icon in the upper left corner and selecting Close, or by pressing <Alt> and<F4>. How to Print the Codebook\nClick the Codebook pull-down menu and select the Print option.\nThe Export codebook selection screen, shown in exhibit 8-41, appears. Exhibit 8-41. Export codebook selection screen"}, {"section_title": "6.", "text": "The program is designed so that the software removal process will keep the taglists when the ECB program is removed in order that all the saved taglists will be retained when the ECB is reinstalled. As a result, removing the software will not remove the directory where the ECB was located. How to Uninstall the ECB for Users with Windows 7 \u00ae Operating Systems\nThe program is designed so that the software removal process will keep the taglists when the ECB program is removed in order that all the saved taglists will be retained when the ECB is reinstalled. As a result, removing the software will not remove the directory where the ECB was located.\nClick the Reset button to return to the top of the original Variable List (Field ID 1) or enter another field ID to scroll to another variable. For field IDs that identify different groups of variables, please refer to appendix E on the CD-ROM for the catalog-specific topical variable groupings. The Go button will not be available in a narrowed or expanded list. Enter a key character string, word, or phrase in the Enter Narrow Text field. Character strings can include a single alphanumeric character or a sequence of several characters. The search is not case sensitive. The results returned will be all entries that contain that exact sequence of letters, numbers, spaces, and words.\nThe variables meeting the specified criteria will be displayed in the Variables List column. If no variable names or descriptions in the catalog contain the specified search text, then the message shown in exhibit 8-20 will appear.\nThe variables meeting the specified criteria will be added to the variables already displayed in the Variables List column.\nThe newly assigned taglist name now appears in the Working Taglist header bar. If a name that already exists is entered, you will be prompted to replace the old taglist with the new taglist. Click Yes only if you wish to replace the old taglist with the new taglist. Complete any changes you wish to make to the existing taglist.\nThe newly assigned taglist name now appears in the Working Taglist header bar. If a name that already exists is entered, you will be prompted to replace the old taglist with the new taglist. Click Yes only if you wish to replace the old taglist with the new taglist or enter a unique name.\nClick the Open button. The Working Taglist will be replaced by the new imported taglist.\nA confirmation screen, shown in exhibit 8-36, verifies your intention to delete the taglist.\nThe Printing Status screen, shown in exhibit 8-40, appears, and the codebook prints on your PC's default printer. How to Export the Codebook\nSelect the desired options from the Format pull-down menu and the Destination pulldown menu."}, {"section_title": "1.", "text": "Click the Windows Start Button.\nType the field ID in the input box on the left of the Go button.\nAdd to the Working Taglist all the variables that you would like to export.\nClose the taglist currently displayed in the Working Taglist column by selecting the New option from the Taglist pull-down menu.\nComplete any changes you wish to make to the displayed taglist. Note that including ALL variables in the taglist is not recommended, as this will create a very large codebook document that is unwieldy to print.\nComplete any changes you wish to make to the displayed taglist."}, {"section_title": "Title Bar", "text": "The Title Bar, shown below in exhibit 8-14, is the horizontal bar located at the top of the main screen. The Menu Bar may also be activated and its options selected using the shortcut keys described in section 8.2.7."}, {"section_title": "Using Shortcut Keys to Navigate", "text": "The shortcut keys provide a means for selecting menu options and screen buttons without the use of a mouse. These shortcut keys are identified by an underscore under the shortcut letter within the option or button label. The menus that appear on the windows are activated by simultaneously selecting the <ALT> key and the underscored letter. An example of this is the activation of the Taglist Menu by selecting the key combination of <ALT> and <T>. Once the menu is activated and all options are displayed, the options can be selected by then pressing the underscored letter for the desired option or by pressing the arrow keys to move between the options. Not all screens have shortcut keys. They may, however, be used without mouse capability by pressing the <TAB> key. The <TAB> key moves the cursor or highlight through the options and buttons within the windows. When the desired option or button is highlighted, it can be selected by pressing the <ENTER> key. How to Use the Go Button"}, {"section_title": "Click the Variable Name, Variable Description, or Both Variable Name and", "text": "Description radio button to specify where to search.\nDescription radio button to specify where to search."}, {"section_title": "8-18", "text": "Exhibit 8-20. No Matches Found message"}, {"section_title": "7.", "text": "Repeat the Narrow Search procedure if necessary. (A variable list that has been narrowed already can be narrowed further.) Please note that the field ID at the upper right corner of the Variable List reflects the order of the variables in the catalog rather than that in the narrowed Variable List.\nRepeat the Expand Search procedure if necessary. (A variable list that has been expanded already can be expanded further.) If no variable names or descriptions in the catalog contain the specified search text, then the \"No matches found\" message shown in exhibit 8-23 will appear. \nClick the Yes button to permanently delete the saved taglist. \nClick the OK button and complete any subsequent screens required for exporting the file. Please note that exporting the codebook for a catalog in its entirety will take a long time due to the large size. In addition, users encountering difficulty with codebooks exported in Word format (due to variations in versions of Word or PC registry settings) should export the codebook using the Rich Text Format (RTF). The document can then be opened using Word or another text-based software package (Notepad, WordPad, TextPad, etc.). The codebook and its variables can be selected to display their information from either the "}, {"section_title": "Example of Narrowing a Search", "text": "The following example shows you how to narrow the Variable List. In this example, you want to include all the variables from the catalog that contain the text \"edu\" in the variable name or description. Do the following: 1. In the Variable List, click the Narrow button."}, {"section_title": "8-19", "text": "Exhibit 8-21. Example of narrowing a search"}, {"section_title": "Expanding Your Variable Search", "text": "The Expand Search function can be used to expand a previously narrowed list of variables The results returned will be all entries that contain that exact sequence of letters, numbers, spaces, and words."}, {"section_title": "Saving Taglists", "text": "The ECB has the ability to save the newly created or modified taglist displayed in the "}, {"section_title": "8-28", "text": "Exhibit 8-30. Save Taglist As dialog box (#2)"}, {"section_title": "Exporting Taglists", "text": "Taglists can be saved as external files (*.tlt) for distribution. However, the exported files should be accessed only through the ECBs. Manually modifying the files outside of the ECB software is not recommended."}, {"section_title": "How to Export a Taglist", "text": ""}, {"section_title": "8-31", "text": "Exhibit 8-34. Import Taglist dialog box"}, {"section_title": "Deleting Taglists", "text": "The ECB provides the capability to permanently delete previously saved taglists. How to Delete a Taglist"}, {"section_title": "Viewing Codebook and Variable Information", "text": "The codebook for a taglist displayed in the Working Taglist column can be created, viewed, and printed from the ECB main screen. The codebook displays several pieces of information about each variable that are described in exhibit 8-37. NOTE: The counter \"1 of 1+\" on the tool bar on top of the screen indicates the current page number and the last page number of the report. Users must navigate to the last page of the report to load the entire report. Once the user has viewed the last page of the report, the \"+\" sign will disappear and the correct last page number will show."}, {"section_title": "How to Print Information for a Single Codebook Variable", "text": "The ECB currently does not support printing the information for a single variable directly to the printer. If you must print the information for a single variable, follow these steps: 1. Double-click the variable to activate the Variable Quick View (see the previous \"How to\" section for details)."}, {"section_title": "8-38", "text": "3. In any application that supports bitmap images (e.g., Microsoft Paint, Microsoft Word, etc.), paste the saved image."}, {"section_title": "How to", "text": ""}, {"section_title": "Extracting Data From the ECB", "text": "Once the variables have been selected (tagged) for extraction and reside in the Working Taglist, the next step is to generate the code through which the statistical analysis software can retrieve and display the results. The ECB provides options for generating the code for analyzing data with the SAS, SPSS for Windows, or Stata statistical analysis programs. To run these programs, you will need the appropriate statistical software and the ECB CD-ROM from which the program can extract data. When extracting data to be used with either the SAS, SPSS for Windows, or Stata programs, a dialog box will be presented that allows the user to define the extract population through the Limiting Fields. See exhibit 8-46. The Limiting Fields include various subgroups of respondents that are typically of interest to analysts. These subgroups can be selected or deselected to narrow the data field that is extracted. Also, please note that the ECB extract function allows the user to specify the drive letter of the CD-ROM drive. If you attempt to run the resulting SAS, SPSS, and Stata programs on a workstation with a different CD-ROM drive letter, you must alter the program code accordingly or regenerate the program code using the ECB. The SAS, SPSS, or Stata source code generated by the ECB to read in the data may contain code statements that are \"commented\" out (e.g., with * in SAS). These code statements either run descriptive statistics (e.g., frequencies, means, etc.) or associate formats with variables. They are commented out because not all analysts will want them included in the source code. SAS users of versions prior to SAS Version 8 should note that although the ECB will allow dataset names larger than eight characters, the SAS system will reject these names at run-time. significantly affected by data issues with even a small number of cases. Therefore, analysts doing such analyses should consider the impact these data issues may have on their results. This appendix is organized as follows: There are 9 cases for which a father/male guardian was identified after data collection (CHILDID=10015236, 10015552, 10011399, 10016048, 10001140, 10002341, 10004530, 10006902, 10009138). Because no father/male guardian was identified during the interview, these cases followed the skip specifications for households with no father/male guardian and were not asked questions about the father, including fathers' race/ethnicity. The race and ethnicity items for these cases are set to -9. In the spring 2012 data, the fathers that were erroneously left out of the household roster in fall 2010 were added to the roster. In one case, by spring 2012, the father/male guardian left and was no longer in the household. In two other cases, there was no spring 2012 parent interview. A-2 \uf06e There are 7 cases where a household member has valid race/ethnicity data but does not appear to meet the design criteria for collecting this information (CHILDID=10004013, 10004059, 10010110, 10017053, 10004449, 10015181, 10017209). This inconsistency resulted from post-data collection editing. Since each of the cases involves the spouse of the respondent (who is a parent figure), the race data were retained because they were used in the parent composites and statistical processing. \uf06e There is 1 case (10018131) where PLQ060 (P1PRMLNG) 1 is missing (-9) and the second set of PLQ items are set to -9. For this case (10018131), the focal child's adoptive parents are in roster positions 1 and 3 (P1 and P3). These parents, who are the respondent and respondent's spouse, are likely to also be the child's grandparents based on age and last name. The household also contains the child's biological mother (P4) and the child's stepfather (P5). Because the grandparents were initially identified as parents, the items in section PLQ were asked of the respondent (\"adoptive father,\" who may be the grandfather), and the child's biological mother. When the data were corrected in editing, the biological mother's PLQ data were stored in the first set of PLQ items, and the second set of PLQ items were set to -9 (as the information was NOT collected about the stepfather)."}, {"section_title": "A-3", "text": "and did not always report the family relationships consistently during the interview. For example, in case 10012776, the respondent was a teenage biological sister who indicated that she was married to the biological father. It is more likely that the sister was acting as a translator for the biological mother (who was also in the household), and her response indicates that the biological parents were married to each other. However, for case 10002814, for example, it looks like a child was responding accurately for his non-English speaking mother. For 3 other cases (CHILDID=10009077, 10018019, 10012320), the respondent was an aunt or other female relative and had a spouse who was the father/male guardian. In the first two cases, it is likely that the respondent was also the female guardian. In the third case, a grandmother respondent was in the household with a mother and father, with the father reported as her spouse/partner. \uf06e There are 10 cases that appear as if they should not have been asked about contacting the care provider for the Before-and After-School Care (BASC) component because data in the data file indicate that the children either were not in care for 5 or more hours or the information needed to determine whether the case was eligible for the BASC component was missing (10011376,10015028,10007620,10009465,10004647,10009721,10008690,10002848,10013574,10008905). For each of these cases, the parent initially indicated that the child had eligible care but then refused to give the contact information for the caregivers. The interviewers then backed up for these cases, changing the answers, but the consent to contact variable CCQ380 (P1CONTCC) had already been set equal to 2."}, {"section_title": "Parent Interview: Spring 2011 Errors in the CAI Programming", "text": "\uf06e There is an error in Step 4 of Box 2 in section NRQ; an \"or 2\" phrase should have been included in step 4. This error resulted in households with both an adoptive father (identified in FSQ150 (P2DAD_*=2)) and an adoptive mother (identified in FSQ140 (P2MOM_*=2)) mistakenly being asked about a nonresident adoptive father."}, {"section_title": "Parent Interview: Spring 2011 Problems with the Spanish Translation", "text": "There were problems with the Spanish translation for three questions in the parent interview that could have resulted in parents who were administered the interview in Spanish interpreting the items differently than parents who were administered the interview in English. Variables P1LANGUA and P2LANGUA can be used to identify those cases for which the parent interview was conducted in Spanish in the fall and spring data collections, respectively. \uf06e Questions FSQ140 and FSQ150 ask parents to identify the specific relationship of the mother and father figures in the household to the study child. Category 4 is meant to be used for foster mothers and other legal guardians, while category 5 is meant to be used for other guardians or parental figures who are not legally in charge of the affairs of the child. However, the Spanish text for category 5 referred to \"another maternal figure or legal guardian\" (\"otra figura materna o guardiana legal\"), which could have A-10 led parents who were interviewed in Spanish to report legal guardians in category 5. Researchers may wish to combine categories 4 and 5 for their analyses."}, {"section_title": "Hard-Copy Questionnaires", "text": "School Administrator Questionnaire: Spring 2011 \uf06e Administrators were asked to report the average daily attendance at school as either a percentage (S2ADA, question A4) or as the average number attending. In some cases, administrators reported both a number and a percentage, and the calculation of the percentage using the number attending and total school enrollment was different from the reported percentage by more than could be attributed to rounding; the reported percentages were retained."}, {"section_title": "A-11", "text": "\uf06e Eighty children attended public schools in which the administrators reported that third-graders attended the school and reported requirements for adequate yearly progress (AYP) in reading, math, or science of 0 to 7 percent (S2RDPTRQ, S2MTPTRQ, S2SCPTRQ, question F12)."}, {"section_title": "Composite Variable Anomalies, Errata, and Considerations", "text": "Chapter 7 of this manual provides detailed information about the composite variables that were created and included on the data file. In this section, several data considerations related to the composite variables are described. Analysts are encouraged to carefully review the descriptions of the composite measures of interest to them in chapter 7."}, {"section_title": "Other (Specify) Variables", "text": "In reviewing \"other (specify)\" responses to questions, there were times when a sufficient number of common responses were given to warrant the addition of a new category to the response options. The categories added after data collection ended, during review of the data, are listed in Exhibit A-1. Users should keep in mind that had these new categories been offered as response options to all respondents during data collection, it is possible that more respondents would have chosen them."}, {"section_title": "A-17", "text": "Exhibit A-1. Variables with too few cases and/or a sparse distribution are suppressed in the K-1 PUF. The values for these variables are set to -2 or -4 and labeled \"suppressed\" in the ECB. The value -2 means that the data for this variable are suppressed to protect the respondent's confidentiality. The value -4 means that the data for this variable are suppressed because of an error in the administration of the instrument; there are only 23 variables with a value -4, and they are all from the kindergarten parent interview."}]