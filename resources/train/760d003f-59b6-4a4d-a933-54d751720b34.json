[{"section_title": "Abstract", "text": "Connectivity studies using resting-state functional magnetic resonance imaging are increasingly pooling data acquired at multiple sites. While this may allow investigators to speed up recruitment or increase sample size, multisite studies also potentially introduce systematic biases in connectivity measures across sites. In this work, we measure the inter-site effect in connectivity and its impact on our ability to detect individual and group differences. Our study was based on real, as opposed to simulated, multisite fMRI datasets collected in N=345 young, healthy subjects across 8 scanning sites with 3 T scanners and heterogeneous scanning protocols, drawn from the 1000 functional connectome project. We first empirically show that typical functional networks were reliably found at the group level in all sites, and that the amplitude of the inter-site effects was small to moderate, with a Cohen's effect size below 0.5 on average across brain connections. We then implemented a series of Monte-Carlo simulations, based on real data, to evaluate the impact of the multisite effects on detection power in statistical tests comparing two groups (with and without the effect) using a general linear model, as well as on the prediction of group labels with a support-vector machine. As a reference, we also implemented the same simulations with fMRI data collected at a single site using an identical sample size. Simulations revealed that using data from heterogeneous sites only slightly decreased our ability to detect changes compared to a monosite study with the GLM, and had a greater impact on prediction accuracy. However, the deleterious effect of multisite data pooling tended to decrease as the total sample size increased, to a point where differences between monosite and multisite simulations were small with N=120 subjects. Taken together, our results support the feasibility of multisite studies in rs-fMRI provided the sample size is large enough."}, {"section_title": "Introduction", "text": "Main objective. Multisite studies are becoming increasingly common in resting-state functional magnetic resonance imaging (rs-fMRI). In particular, some consortia have retrospectively pooled rs-fMRI data from multiple independent studies comparing clinical cohorts with control groups, e.g. normal controls in the 1000 functional connectome project (FCP) (Biswal et al., 2010) , children and adolescents suffering from attention deficit hyperactivity disorder from the ADHD200 Fair et al., 2012) , individuals diagnosed with autism spectrum disorder in ABIDE (Nielsen et al., 2013) , individuals suffering from schizophrenia (Cheng et al., 2015) , or elderly subjects suffering from mild cognitive impairment . The rationale behind such initiatives is to dramatically increase the sample size at the cost of decreased sample homogeneity. The systematic variations of connectivity measures derived using different scanners, called site effects, may decrease the statistical power of group comparisons, and somewhat mitigate the benefits of having a large sample size (Brown et al., 2011; Jovicich et al., 2016) . In this work, our main objective was to quantitatively assess the impact of site effects on group comparisons in rs-fMRI connectivity.\nGroup comparison in rs-fMRI connectivity. In this work, we focused on the most common measure of individual functional connectivity, which is the Pearson's correlation coefficient between the average rs-fMRI time series of two brain regions. To compare two groups, a general linear model (GLM) is typically used to establish the statistical significance of the difference in average connectivity between the groups. Finally a p-value is generated for each connection to quantify the probability that the difference in average connectivity is significantly different from zero (Worsley and Friston, 1995; Yan et al., 2013) . If the estimated p-value is smaller than a prescribed tolerable level of false-positive findings (see for more detail Table 1) , generally adjusted for the number of tests performed across connections, say \u03b1 = 0.001, then the difference in connectivity is deemed significant.\nStatistical power in group comparisons at multiple sites. The statistical power of a group comparison study is the probability of finding a significant difference, when there is indeed a true difference. A careful study design involves the selection of a sample size that is large enough to reach a set level of statistical power, e.g. 80%. In the GLM, the statistical power actually depends on a series of parameters (Desmond and Glover, 2002; Durnez et al., 2014) : (1) the sample size (the larger the better); (2) the absolute size of the group difference (the larger the better), and, (3) the intrinsic variability of measurements (the smaller the better) (4) the rejection threshold \u03b1 for the null hypothesis.\nSources of variability: factors inherent to the scanning protocol. In a multisite (or multi-protocol) setting, differences in imaging or study parameters may add variance to rs-fMRI measures, e.g. the scanner make and model Friedman et al., 2008) , repetition time, flip angle, voxel resolution or acquisition volume , experimental design such as eyesopen/eyes-closed (Yan et al., 2009) , experiment duration (Van Dijk et al., 2010) , and scanning environment such as sound attenuation measures (Elliott et al., 1999) , or head-motion restraint techniques (Edward et al., 2000; Van Dijk et al., 2012) , amongst others. These parameters can be harmonized to some extent, but differences are unavoidable in large multisite studies. The recent work of Yan et al. (2013) has indeed demonstrated the presence of significant site effects in rs-fMRI measures in the 1000 FCP. Site effects will increase the variability of measures, and thus decrease statistical power. To the best of our knowledge, it is not yet known how important this decrease in statistical power may be.\nSources of variability: within-subject. The relative importance of site effects in rs-fMRI connectivity depends on the amplitude of the many other sources of variance. First, rs-fMRI connectivity only has moderate-to-good test-retest reliability using standard 10-minute imaging protocols (Shehzad et al., 2009) , even when using a single scanner and imaging session. Differences in functional connectivity across subjects are also known to correlate with a myriad of behavioural and demographic subject characteristics (Anand et al., 2007; Sheline et al., 2010; Kilpatrick et al., 2006) . Taken together, these sources of variance reflect a fundamental volatility of human physiological signals.\nSources of variability: factors inherent to the site. In addition to physiology, some imaging artefacts will vary systematically from session to session, even at a single site. For example, intensity nonuniformities across the brain depend on the positioning of subjects (Caramanos et al., 2010) . Room temperature has also been shown to impact MRI measures (Vanhoutte et al., 2006) . Given the good consistency of key findings in resting-state connectivity across sites, such as the organization of distributed brain networks (Biswal et al., 2010) , it is reasonable to hypothesize that site effects will be small compared to the combination of physiological and within-site imaging variance. Multivariate analysis. Another important consideration regarding the impact of site effects on group comparison in rs-fMRI connectivity is the type of method used to identify differences. The concept of statistical power is very well established in the GLM framework, which tests one brain connection at a time (mass univariate testing). However, multivariate methods that combine several or all connectivity values in a single prediction are also widely used and likely affected by the site effects. A popular multivariate technique in rs-fMRI is supportvector machine (SVM) (Cortes and Vapnik, 1995) . In this approach, the group sample is split into a training set and a test set. The SVM is trained to predict group labels on the training set, and the accuracy of the prediction is evaluated independently on the test set. The accuracy level of the SVM captures the quality of the prediction of clinical labels from resting-state connectivity, but does not explicitly tell which brain connection is critical for the prediction. The accuracy score can thus be seen as a separability index between the individuals of two groups in high dimensional space. Altogether, the objectives and measures of statistical risk for SVM and GLM are quite different. Because SVM has the ability to combine measures across connections, unlike univariate GLM tests, we hypothesized that the GLM and SVM will be impacted differently by site effects. Even though the accuracy is expected to be lower for the multisite than the monosite configuration, it as been shown that the generalizability of a predictive model to unseen sites is greater for models trained on multisite than monosite datasets as shown by Abraham et al. (2016) .\nSpecific objectives. Our first objective was to characterize, using real data, the amplitude of systematic site effects in rs-fMRI connectivity measures across sites, as a function of within-site variance. We based our evaluation on images generated from independent groups at 8 sites equipped with 3 T scanners, in a subset (N=345) of the 1000 FCP. Our second objective was to evaluate the impact of site effects on the detection power of group differences in rs-fMRI connectivity. To answer this question directly, one would need to scan two different cohorts of participants at least twice, once in a multisite setting and once in a monosite setting. Such an experiment may be too costly to implement for addressing a purely technical objective. As a more feasible alternative, we implemented a series of Monte Carlo simulations, adding synthetic \"pathological\" effects in the 1000 FCP sample. One interesting feature of the \"1000 FCP\" dataset is the presence of one large site of \u223c200 subjects and 7 small sites of \u223c20 subjects per site. We were therefore able to implement realistic scenarios following either a monosite or a multisite design (with 7 sites), with the same total sample size. Our simulations gave us full control on critical aspects for the detection of group differences, such as the amplitude of the group difference, sample size, and the balancing of groups across sites. We evaluated the ability of detecting group differences both in terms of sensitivity for a GLM and in terms of accuracy for a SVM model."}, {"section_title": "Method", "text": ""}, {"section_title": "Imaging sample characteristics", "text": "The full 1000 FCP sample includes 1082 subjects, with images acquired over 33 sites spread across North America, Europe, Australia and China. As the 1000 FCP is a retrospective study, no effort was made to harmonize population characteristics or imaging acquisition parameters (Biswal et al., 2010) . A subset of sites was selected based on the following criteria: (1) 3 T scanner field strength, (2) full brain coverage for the rs-fMRI scan, and, (3) a minimum of 15 young or middle aged adult participants, with a mixture of males and females (4) samples drawn from a population with a predominant Caucasian ethnicity. In addition, only young and middle aged participants (18-46 years old) were included in the study, and we further excluded subjects with excessive motion (see next Section). The final sample for our study thus included 345 cognitively normal young adults (150 males, age range: 18-46 years, mean \u00b1 std: 23.8 \u00b1 5.14) with images acquired across 8 sites located in Germany, the United Kingdom, Australia and the United States of America. The total time of available rs-fMRI data for these subjects ranged between 6 and 7.5 min and only one run was available per subject. See Table 2 for more details on the demographics and imaging parameters at each site selected in the study. The experimental protocols for all datasets as well as data sharing in the 1000 FCP were approved by the respective ethics committees of each site. This secondary analysis of the 1000 FCP sample was approved by the local ethics committee at CRIUGM, University of Montreal, QC, Canada."}, {"section_title": "Computational environment", "text": "All experiments were performed using the NeuroImaging Analysis Kit, NIAK 2 (Bellec et al., 2011) version 0.12.18, under CentOS version 6.3 with Octave 3 version 3.8.1 and the Minc toolkit 4 version 0.3.18. Analyses were executed in parallel on the Mammouth supercomputer 5 , using the pipeline system for Octave and Matlab, PSOM (Bellec et al., 2012 ) version 1.0.2. The scripts used for processing can be found on Github 6 . Prediction was performed using the LibSVM library (Chang and Lin, 2011) . Visualization was implemented using Python 2.7.9 from the Anaconda 2.2.0 7 distribution, along with Matplotlib 8 (Hunter, 2007) , Seaborn 9 and Nilearn 10 for brain map visualizations."}, {"section_title": "Preprocessing", "text": "Each fMRI dataset was corrected for slice timing; a rigid-body motion was then estimated for each time frame, both within and between runs, as well as between one fMRI run and the T1 scan for each subject (Collins et al., 1994) . The T1 scan was itself non-linearly co-registered to the Montreal Neurological Institute (MNI) ICBM152 stereotaxic symmetric template , using the CIVET pipeline (Ad-Dab'bagh et al., 2006) . The rigid-body, fMRI-to-T1 and T1-to-stereotaxic transformations were all combined to re-sample the fMRI in MNI space at a 3 mm isotropic resolution. To minimize artifacts due to excessive motion, all time frames showing a frame displacement, as defined in Power et al. (2012) , greater than 0.5 mm were removed and a residual motion estimated after scrubbing. A minimum of 50 unscrubbed volumes per run was required for further analysis (13 subjects were rejected). The following nuisance covariates were regressed out from fMRI time series: slow time drifts (basis of discrete cosines with a 0.01 Hz highpass cut-off), average signals in conservative masks of the white matter and the lateral ventricles (average Pearson correlation across all subjects is 0.242 between gray matter and white matter signals, and 0.031 between gray matter and ventricles signals) as well as the first principal components (accounting for 95% variance) of the six rigid-body motion parameters and their squares (Giove et al., 2009; Lund et al., 2006) . The fMRI volumes were finally spatially smoothed with a 6 mm isotropic Gaussian blurring kernel. A more detailed description of the pipeline can be found on the NIAK website 11 and Github 12 ."}, {"section_title": "Inter-site bias in resting-state connectivity", "text": "Functional connectomes. We compared the functional connectivity measures derived from different sites of the 1000 FCP. A functional brain parcellation with 100 regions was first generated using a bootstrap analysis of stable clusters (Bellec et al., Jul. 2010b ), on the Cambridge cohort of the 1000 FCP (N = 195), as described in Orban et al. (2015) . For a given pair of regions, the connectivity measure was defined by the Fisher transformation of the Pearson's correlation coefficient between the average temporal rs-fMRI fluctuations of the two regions. For each subject, a 100\u00d7100 functional connectome matrix was thus generated, featuring the connections for every possible pair of brain regions.\nInter-site effects. The inter-site effects at a particular connection were defined as the absolute difference in average connectivity between two sites. In order to formally test the significance of the inter-site effects, we used a GLM including age, sex and residual motion as covariates (corrected to have a zero mean across subjects), as well as dummy variables coding for the average connectivity at each site. For each site, a \"contrast\" vector was coded to measure the difference in average connectivity between this site and the grand average of functional connectivity combining all other sites. A p-value was generated for each connection to quantify the probability that the observed effect using this contrast was significantly different from zero (Worsley and Friston, 1995) . The number of false discovery was also controlled (q = 0.05) using a Benjamini-Hochberg false discovery rate (FDR) procedure (Benjamini and Hochberg, 1995) . To quantify the severity of inter-site effects, we derived Cohen's d effect size measure for each connection: \u03b2 \u03c3 | |/ c , with \u03b2 c being the weight associated with the contrast. The standard deviation from the noise \u03c3 was calculated as \u03c3 e N K = \u2211 /( \u2212 ) 2 , e being the residuals from the GLM, N the sample size and K the number of covariates in the model. As secondary analyses, t-tests were also implemented in the GLM to validate that age, sex as well as residual motion made significant contributions to the model."}, {"section_title": "Simulations", "text": "Data generation process. We implemented Monte-Carlo simulations to assess the detection sensitivity of group differences in rs-fMRI connectivity. The simulations were based on the 1000 FCP sample, with 8 sites totaling 345 subjects. The multisite simulations were sampled from 148 subjects, available across S=7 sites. The monosite simulations were sampled from 195 subjects available at S=1 site (Cambridge). For each simulation, a subset of subjects of a given size N was selected randomly and stratified by site. For each site, a ratio W of the selected subjects was randomly assigned to a so-called \"patient\" group. We focus our analysis on connections showing a fair-to-good test-retest reliability based on a previous study reporting 11 connections likely impacted by Alzheimer's disease, see Orban et al. (2015) for details. For each connection, a \"pathology\" effect was added to the connectivity measures of the subjects belonging to the \"patient\" group. This additive shift in connectivity for \"patients\" was selected as to achieve a specified effect size, defined below.\nEffect size (Cohen's d). The Cohen's d was used to quantify the effect size. For a group comparison, Cohen's d is defined as the difference \u03bc between the means of the two groups, divided by the standard deviation of the measures within each group, here assumed to be equal. For a given connection between brain regions i and j, let y i j , be the functional connectivity measure for a particular subject of the 1000 FCP sample. If the subject was assigned to the \"patient\" group in a particular simulation, an effect was added to generate a simulated connectivity measure y * i j\n. For a specified effect size d, the parameter \u03bc was set to d s \u00d7 i j , , where s i j , is the standard deviation of connectivity between region i and j. The parameter s i j , was estimated as the standard deviation of connectivity measures across subjects in the mono-site sample (Cambridge), without any \"pathological\" effect simulated.\nGLM tests. In order to detect changes between the simulated groups at each connection, a GLM was estimated from the simulated data, using age, sex and frame displacement as confounds (corrected to have a zero mean across subjects). To account for site-specific effects, S \u2212 1 dummy variables (binary vectors coding for each site) were added to the model, with S being the total number of sites used in the study, in addition to an intercept accounting for the global average. Finally, one dummy variable coded for the \"patient\" group. The regression coefficients of the linear model were estimated with ordinary least squares, and a t-test, with associated p-value, was calculated for the coefficient of the \"patient\" variable. A significant pathology effect was detected if the p value was smaller than a prescribed \u03b1 level. The \u03b1 level needs to be adjusted for multiple comparisons (in our case 11 connections, but this would depend on the number of connections selected in a particular study), which can be done in an adaptive manner using FDR. When connections are pre-specified, such as in e.g. Wang et al. (2012) , a more liberal threshold can be applied. In our case, since we wanted to have a constant behavior independent of the effect size, we tested different typical values for \u03b1 in {0.001, 0.01, 0.05}. For each simulation sample b and each connection, we derived a p-value p * b ( ) , and the effect was deemed detected if p * b was less than \u03b1. The sensitivity of the test for a particular connection was evaluated by the frequency of positive detections over all simulation samples. Prediction accuracy. In addition to mass univariate GLM tests, we also investigated a linear SVM (Cortes and Vapnik, 1995) using a Monte-Carlo simulation of the prediction of clinical labels based on cross-validation. For SVM simulations, all possible connections between the 100 brain regions were used simultaneously to predict the presence of the simulated pathology in a given subject. For a participant assigned to the \"patient\" group, a \"pathology\" effect was only simulated in a set percentage of connections, which were randomly selected. The proportion of connections with a non-null effect was denoted as \u03c0 1 . For a given simulation at sample size N, the SVM model was trained on N subjects selected randomly and stratified by site. The accuracy of the model was evaluated on a separate sample consisting of the remaining subjects, unused during training. For example, for a multisite simulation with N=80 subjects for training, the model accuracy was tested on the remaining 68 subjects: 148 (available subjects) minus 80 (subjects in the training set). During training, a 10-fold cross-validation was used to optimize the hyperparameters of the SVM independently for each simulation. The mean and standard deviation of accuracy scores across all samples were derived for each simulation scenario.\nSimulation experiments. All the simulation parameters have been summarized below:\n\u2022 The type of detection method, either GLM or SVM.\n\u2022 For GLM tests, the false-positive rate \u03b1.\n\u2022 For SVM tests, the proportion of \"pathological\" connections \u03c0 1 .\n\u2022 The effect size d.\nFor a given set of simulation parameters, we generated B = 10 3 Monte-Carlo samples to estimate either the sensitivity (for GLM test) or the accuracy (for SVM prediction) of the method. For all experiments, we investigated effect sizes d \u2208 {0, 2} with a step of 0.01 and \u03b1 \u2208 {0.001, 0.01, 0.05}. The number of site(s) was S=1 for the monosite analysis and S=7 for the multisite analysis. We implemented the following experiments:\n\u2022 ( ) 1 Test the impact of the sample size on GLM N \u2208 {40, 80, 120}, with a fixed allocation ratio W=0.5.\nTest the impact of the allocation ratio on GLM W \u2208 {0.5, 0.3, 0.15} for a fixed sample size N=120.\n\u2022 ( ) 3 Test the impact of multisite correction (regressing out the site effects using dummy variables coding for each site) and affected connection volume (\u03c0 1 ) on the prediction accuracy. For the prediction scenario, we used a range of \u03c0 \u2208 {0.1, 1, 5%} 1 , and two sample sizes N \u2208 {80, 120} subjects for training, with model accuracy estimated on N=68 and N=28, respectively."}, {"section_title": "Results", "text": ""}, {"section_title": "Inter-site effects in fMRI connectivity", "text": "Site effects in the default-mode network. We first focused on the connections associated with a seed region located in the posterior cingulate cortex, a key node of the default-mode network (DMN), which is one of the most widely studied resting-state networks (Greicius et al., 2004) . The connections were based on the Cambridge 100 parcellation, and were represented as a connectivity map, (Fig. 1 ). Fig. 1A shows the posterior cingulate cortex connectivity map, averaged across all subjects and all sites. The key regions of the DMN are easily identifiable, and include the posterior cingulate cortex, precuneus, inferior parietal lobule, anterior cingulate cortex, medial prefrontal cortex (dorsal, anterior and ventral), superior frontal gyri and the medial temporal lobe (Damoiseaux et al., 2006; Dansereau et al., 2014; Yan et al., 2013) . The average connectivity map of the DMN was then extracted for each site, Fig. 1A . Qualitatively, the DMN maps were consistent across sites, as expected based on the literature. We then tested for the significance of the site effects (Fig. 1B) , i.e. the difference in average connectivity at a given site and the average connectivity at all remaining sites. The statistical maps were corrected for multiple Fig. 1 . Panel A: map of the DMN obtained using a seed in the posterior cingulate cortex, averaging all subjects and sites together (first row) and then averaging all subjects for each of the 8 sites (subsequent rows). Panel B shows the number of sites with a significant inter-site difference for each brain region (first row) and the significant differences between the average functional connectivity maps of one site versus all the others (subsequent rows). comparisons across the brain with FDR at q \u2264 0.05 (Benjamini and Hochberg, 1995) . A significant site effect for at least one connection could be identified for every site, without exception, Fig. 1B. Fig. 1C shows how reproducible the significant site effects were in connectivity across the brain and sites. The identified significant connections were quite variable across sites, most of them being identified at less than three sites.\nSite effects across the connectome. In order to extend these observations outside of the DMN, we derived the entire connectome using the Cambridge 100 parcellation. Fig. 2A shows the average connectome, pooling all subjects and sites together. The regions have been re-ordered based on a hierarchical clustering with Ward criterion. A network structure is clearly visible as squares of high connectivity on the diagonal of the connectome (as outlined by black lines). Each diagonal square corresponds to the intra-network connectivity for a partition into 7 networks ( Fig. 2A ). These 7 networks 13 were consistent with the major resting-state networks reported using a cluster analysis in previous works (Bellec et al., 2010a,e.g.) : the DMN, visual, sensorimotor, dorsal and ventral attentional networks, mesolimbic and cerebellar networks were identified (Fig. 2B ). Fig. 2C shows how this large-scale connectome organization varied from site to site. The average connectivity per site as well as significant differences with the average of the remaining sites (q \u2264 0.05) is shown in Fig. 2C .\nVisually, consistent with our previous observations in the DMN, the organization of the average connectome into large-scale resting-state networks was preserved across all sites. Some significant site effects were still detected in the connectivity both within each network, as well as between networks. By counting the number of sites showing a significant effect for each pair of regions, it was apparent that significant site effects were quite variable in their localization and spread across the full connectome (Fig. 2D) .\nConcerning the association with the other confounding variables in the model (sex, age and motion) many connections were found to be significantly associated with motion, see Supplementary Material Figure S5 , although very few connections were found to be significantly associated with the sex and age, see Supplementary Material Figure S6 and S7. We also checked that the analysis was not predominantly driven by the larger Cambridge site. We thus ran the same analysis excluding that site (see Supplementary Material Figure S8 ). The number of significant pairs remained very similar, although the spatial location of half of the significant connectivity pairs changed when the large Cambridge site was removed from the analysis. Those findings do not qualitatively change our conclusion, but they influence the location of the significant connections. These differences may be due to the intrinsic variability in the statistical test, and not just the size of the Cambridge site. In summary, those findings support the inclusion of age, sex and motion parameters in a GLM in order to remove their confounding effects in addition to site effects.\nSite effects vs. within-site variations across subjects. We measured the amplitude of inter-site effects, represented as violin plots across connections using either the absolute difference in average connectivity (Fig. 3A ,C) or Cohen's d effect size measures (Fig. 3B,D) . The violin plots include either every connection from the BASC Cambridge parcellation (Fig. 3A,B ), or only the 11 connections selected for Monte-Carlo simulations (Fig. 3C,D) . For absolute differences, the distributions were mostly consistent across sites, with a median around 0.06, 5% percentile near 0 and 95% percentiles in the 0.08-0.1 range. For Cohen's d, the distributions were also consistent across sites, with a median around 0.33, 5% percentile near 0 and 95% percentiles in the 0.4-0.6 range. These effect sizes are typically deemed small-tomoderate (Cohen, 1992) , although such a qualitative assessment needs to be refined based on each application. This result thus suggests that the impact of additive inter-site effects on statistical tests will be limited. Similar findings were observed across all possible connections, or across the 11 pairs of connections selected in the simulation study. Differences in standard deviation across sites. We also investigated the site differences in standard deviation of connectivity across subjects, see Supplementary Figure S1 for the DMN, Supplementary Material S2 for the connectomes. The standard group GLM assumes equal variance of resting-state connectivity across all subjects, or \"homoscedasticity\". Significant differences in across-subject standard deviation between sites violates the homoscedastic assumption, and may jeopardize the validity of the false-positive rates of the model.\nQualitatively, we first observed that the sites showing the larger number of differences were the one with the most temporal variance among connections see Supplementary Figure S3 . We then ran a White's test aimed at rejecting homoscedasticity at each connection, independently. The White's tests resulted in a family of p-values, which was corrected for multiple comparisons using FDR (q < 0.05). The homoscedastic hypothesis was rejected in a large portion of connections. This was expected due to the large overall number of subjects and consequently large statistical power of White's procedure. However, despite reaching significance, the absolute difference in the average standard deviation between two sites was 19% of the grand average standard deviation, on average across pairs of sites. Such a small departure from homoscedasticity likely has only a mild impact on the GLM, which we formally investigated using Monte-Carlo simulations."}, {"section_title": "Multisite monte-carlo simulations", "text": "Validity of the control of false positives in the GLM. An excellent control of the false positive rate was observed at all nominal levels \u03b1 \u2208 {0.001, 0.01, 0.05}, both in monosite simulations or in multisite simulations, when site covariates were included in the GLM, see Fig. 4 . This means that the nominal, user-specified, false positive rate matched precisely with the effective false positive rate measured in the simulations. This observation held for any combination of alloca- tion ratio, W \u2208 {15%, 30%, 50%}, and sample size, N \u2208 {40, 80, 120}. By contrast, when no site covariates were included in the GLM, the false positive rate was not controlled appropriately, sometimes by a wide margin. In the absence of site covariates, the procedure was sometimes too conservative, e.g. W = 50%, and sometimes very liberal, e.g. N W = 120, = 15%. This experiment showed that, despite the mild departure from homoscedasticity reported above, the GLM does control for false-positive rate at each connection very precisely, if and only if site covariates are included in the model.\nStatistical power and effect size. Fig. 5A shows the relationship between effect size and a GLM detection power in experiment ( ) 1 , i.e. for a fixed allocation ratio (W = 50%) and three different sample sizes, N \u2208 {40, 80, 120}. The average and std of detection power was plotted across the 11 selected connections. The variations of statistical power across connections were very small for monosite simulations, as the effect size was adjusted based on the standard deviation of each connection within that sample. As expected, the sensitivity increased with sample size, quite markedly. In multisite simulations (S=7), for a large effect size (d=1), the detection power was 20% with 40 subjects, 80% with 80 subjects and 95% with 120 subjects. The sensitivity was larger with a single site than a multisite sample, yet the difference between the two decreased as sample size increased. With N=40 and d=1, the detection power was close to 30% for a single site sample, compared to 20% for the multisite sample. With N=120 and d=1, the difference in sensitivity was only of a few percent. The same trend was apparent for all tested effect sizes as well as for \u03b1 \u2208 {0.01, 0.05} (not shown). Fig. 4 . Monte-Carlo simulation of the false positive rate in the absence of group differences (d=0), either for a monosite (S=1, left), a multisite (S=7) with (middle) or without (right) site covariates included in the GLM. In panel A, three different \u03b1 values have been tested, \u03b1 \u2208 {0.001, 0.01, 0.05} with a fixed sample size and patient allocation ratio (N W = 120, = 50%). In panel B, three different sample sizes have been tested, N \u2208 {40, 80, 120} with a fixed patient allocation ratio (W = 50%) (Experiment ( ) 1 ). In panel C, three different patient allocation ratios have been tested, W \u2208 {50%, 30%, 15%} with a fixed sample size (N=120) (Experiment ( ) 2 ).\nStatistical power and group allocation ratio. Fig. 5B shows the relationship between effect size and a GLM detection power in experiment ( ) 2 , i.e. for a fixed sample size (N=120) and three different patient allocation ratio, W \u2208 {15%, 30%, 50%}. Overall, we found that the detection power increased with W. For example, with d=1, the detection power was 65% for W = 15%, and increased to 90% with W = 30%, and finally 95% for W = 50%. The impact of W was observed in both monosite and multisite samples, with an optimal allocation ratio of W = 50% for both. This observation was also made for \u03b1 \u2208 {0.01, 0.05} (not shown).\nDetectable effect size, as a function of sample size. An alternative summary of experiment ( ) 1 is to represent the effect size that can be detected with 80% sensitivity, as a function of sample size for monosite and multisite configurations, see Fig. 6 . As a reference, we computed the same curve for parametric t-test comparisons, under assumptions of normality. As expected, the detectable effect size for parametric ttests closely followed the monosite estimation. For a small sample size (N=40), the detectable effect size was notably larger in multisite configurations than in a monosite configuration (difference of about 0.25 in Cohen's d for \u03b1 = 0.001). However, the difference decreased for Fig. 5 . Monte-Carlo simulation of detection power as a function of the effect size d \u2208 [0, 2], either for a monosite (S=1, in red) or a multisite (S = 7, in blue) sample, when testing differences between two groups with a GLM and a false-positive rate \u03b1 = 0.001. The plain curves are the average statistical power across 11 connections, and the shaded area represents \u00b1 1 standard deviation across connections. In panel A, the patient allocation ratio is fixed (W = 50%) and three different sample sizes have been tested, N \u2208 {40, 80, 120} (Experiment ( ) 1 ). In panel B, the sample size is fixed (N=120) and three different patient allocation ratios have been tested W \u2208 {15%, 30%, 50%} (Experiment ( ) 2 ). Fig. 6 . Effect size detectable at 80% sensitivity as a function of sample size, for different false-positive rate \u03b1 \u2208 {0.05, 0.01, 0.001} (experiment ( ) 1 ). All simulations used a balanced patient allocation ratio W = 50%. The monosite performance is shown in red and the multisite in blue. The dotted black line shows the detectable effect size for a classical parametric ttest.\nlarge sample sizes to become smaller than 0.1 with N=120 and \u03b1 = 0.001. The lowest detectable effect size for a sensitivity of 80% at \u03b1 = 0.05 was about d=0.8, achieved in a monosite configuration with N=120. At this sample size, the difference between single and multisite configurations was marginal, with only a few percent's of difference in detectable effect sizes.\nPrediction accuracy. In experiment ( ) 3 , we examined the impact of effect size and the volume of affected connections on prediction accuracy in a SVM, see Fig. 7 . The volume of changes \u03c0 1 had a major impact on prediction accuracy. At \u03c0 = 0.1% 1 (around 5 connections) the accuracy level was at chance level across all tested effect sizes, (Fig. 7A ). With \u03c0 = 1% 1 , accuracy slightly increased, but effect sizes larger than d=2 were still required to reach over 80% accuracy (Fig. 7B ). With \u03c0 = 5% 1 , 95% accuracy was achieved at the same effect size (about d=1.5) for monosite and multisite simulations, although the accuracy in multisite simulations was notably lower than for monosite simulations across most effect sizes (Fig. 7C) . The relationship between effect size and accuracy followed a sigmoidal curve in both settings, yet a sharper, and later transition between very low and very high accuracy was observed in multisite simulations. Interestingly, correcting for site effects by regressing out the dummy variable before running the SVM classifier had no impact on accuracy levels. The sample size (N=80 vs N=120 for training) did have a moderate effect on prediction accuracy: for \u03c0 = 5% 1 and d = 1 and monosite simulations, accuracy was about 85% with N = 120 ( Fig. 7C) and 75% with N = 80 (Fig. 7F )."}, {"section_title": "Discussion and conclusions", "text": "Inter-site effects in rs-fMRI connectivity. Typical resting-state networks, such as the DMN, the attentional, visual and sensorimotor networks, were reliably found across sites. This was strongly expected given the relative consistency of their distribution across individuals, studies, preprocessing approaches or even methods used to extract networks (e.g. Damoiseaux et al., 2006; van den Heuvel et al., 2008; Bellec et al., 2010b; Yeo et al., 2011; Power et al., 2011) . We however found that significant differences in average connectivity existed between sites, as previously reported by Yan et al. (2013) . These site effects in connectivity may undermine the generalization of the results derived at a single site. The inter-subject (intra-site) standard deviation of the connections was found to be more than twice as large as the inter-site absolute effect, on average across brain connections. This effect size measured in Cohen's d would be deemed small-to-moderate, which suggests that the impact of additive inter-site effects on statistical tests will be limited. This is a reassuring finding supporting the feasibility of statistical tests pooling fMRI data across multiple sites. Previous studies (Sutton et al., 2008; Brown et al., 2011) had reported inter-site variance up to 10 times smaller than inter-subject variability, but these studies had much more homogeneous scanning environments than ours and also used different fMRI outcome measures. In our case, we still investigated only 3 T scanners, mostly Siemens, and inter-site effects may be larger when considering other manufacturers or field strengths.\nStatistical power and multisite rs-fMRI. After accounting for siterelated additive effects in a GLM, the multisite simulation pooling 7 sites together showed detection power close to that of a monosite simulation with equivalent sample size. The difference was noticeable for small sample size (total N=40), and became very small for a sample size N=120. Another observation was that, for a given detection power, Fig. 7 . Prediction accuracy of patient vs. controls as a function of effect size. Three simulation settings are presented on each plot: monosite (red curve), multisite with regression of site effects (S = 7, blue curve), and multisite without regression of site effects (S = 7, black curve). Accuracy was estimated over B = 10 3 simulation samples with a patient allocation ratio W = 50% and 3 volumes of affected connections \u03c0 = 0.1% 1 (left column), \u03c0 = 1% 1 (middle column) and \u03c0 = 5% 1 (right column). Two sample sizes were tested: N=120 randomly selected subjects for training, with the remaining N=28 to estimate accuracy (first row), and N=80 randomly selected subjects for training, with the remaining N=68 to estimate accuracy (second row).\nthe lowest effect size that we were able to detect was more variable across connections for a low sample size. We demonstrated that a parametric group GLM does control precisely for the rate of false positive discoveries, even in multisite settings, as long as site covariates are included in the model. Taken together, these observations suggest to use sample sizes larger than 100 subjects for GLM multisite studies. This conclusion may depend on the number of sites pooled in the study and the actual number of subjects in each of those sites, which we could not test in this work due to the size of the available sample.\nModeling site effects as random variables. We modeled the effect of each site on the average connectivity between any given pair of regions as a fixed effect. This means that the proposed GLM inference does apply only to collection of sites included in a given analysis. The linear mixed-effects model (Chen et al., 2013) would allow more powerful inferences: by modeling site effects as random variables, following a specific distribution (e.g. Gaussian), we would be able to generalize observations potentially to any collection of sites, provided our assumptions are accurate. The sample of sites available for this study (7 at most) is however too small in our view to correctly estimate the variability of effects across sites. This work would also require to formulate and investigate empirically as well as on simulations different models for the distribution of inter-site variations of site effects (e.g. Gaussian distribution).\nSite heteroscedasticity. We observed mild heteroscedasticity across sites. Our simulations showed that this does not compromise the control of false positive rate in the GLM, even under homoscedastic assumptions, with the range of contrasts we investigated. Regression models more robust to heteroscedasticity may be investigated in the future, e.g. weighted least squares regression or linear mixed-effects modeling (Chen et al., 2013) .\nStatistical power and sample size. For a medium effect size, e.g. d = 0.5, the sensitivity was low (below 20%), even for monosite simulations with N = 120 subjects. This sobering result supports the current trend in the literature to pool multiple data samples to increase sample size, at the cost of decreased homogeneity. We also found that resting-state studies based on 40 subjects or less, even at a single site, are seriously underpowered, except for extremely large effect sizes (Cohen's d greater than 1.5). Finally, unbalanced patient allocation ratio in site samples greatly reduces sensitivity, even in monosite studies. Balanced datasets, i.e. with equal numbers of patients and controls at each site, should therefore be favored.\nPrediction. Comparing the monosite and the multisite accuracy curves reveals a substantial drop in accuracy from monosite to multisite across a broad range of effect sizes. However, it should be noted that classifiers trained across multiple data sources will likely generalize better to new observations, which is likely a critical feature in most applications and reflects the true potential clinical utility of this type of technique. Our conclusions are consistent with the work of Nielsen et al. (2013) , which compares the prediction of a clinical diagnosis of autism in monosite vs. multisite settings. The authors concluded that the prediction accuracy for the multisite sample was significantly smaller than for the monosite sample. A somewhat surprising observation in our analysis was that linear correction for site-specific effects did not improve accuracy of prediction using SVM. The SVM model seems to learn features that are invariant across sites, maybe focusing on connections with the smallest site effect, or looking at differences between connections similarly impacted by a site effect. Finally, an important conclusion of our simulations was that the volume of brain connections affected by a disease impacts accuracy as much as the effect size per connection. This suggests that feature reduction and/or selection is a very important step to improve sensitivity to small effect sizes.\nBeyond additive site effect. An important limitation to our study is that we only investigated the impact of additive effects in brain connectivity across sites. Areas of future work include interactions between site effects and pathology, possibly in the form of polynomial and non-linear interactions. We hope that, in the future, fMRI data acquired on clinical cohorts at tens of sites will become available, which will enable researchers to test empirically the presence of such interaction effects.\nOther types of multisite data. Another limitation of our study is that we only investigated multisite data featuring roughly equal sample sizes with fairly balanced patient allocation ratios at each site. Multisite studies including a very large number of sites with sometimes only a few subjects per site are however quite common, e.g. the Alzheimer's disease neuroimaging initiative (ADNI) (Mueller et al., 2005) and many pharmaceutical clinical trials at phase II and III 14 . In this type of design, the multisite effect may play a much more pronounced role than in our simulations as it cannot be modeled in the GLM, and will become an intrinsic added source of inter-subject variance (Feaster et al., 2011) . Unfortunately, this type of design could not be tested with the current dataset due to the limited number of sites available. This represents an important avenue of future work.\nUnderlying causes of the site effects. Not all sites seemed to be equally impacted by the site effects, with sites like Berlin or Saint-Louis showing a small number of connections significantly different then the grand average connectivity matrix, while sites like Baltimore, Queensland and Oxford showed many more connections affected by the site effects. Interestingly this can potentially be due to temporal variance of the connections (see Supplementary Figure S3 ) partly explained by the scanner make since Queensland and Baltimore site used scanners from different makers (namely Bruker and Philips) than the rest of the sites used in this study (Siemens scanners). This may suggest that scanners SNR (signal to noise ratio) may partly explain the variance of connectivity. These differences may not be statistically significant, or they may reflect real differences due to protocol, scanner characteristics at these sites or differences in sampling across sites. Multiple causes may be interacting together to produce the site effects, as reported by Yan et al. (2013) , although some of these sources of variance could be better controlled like the scanner parameters, paired with the use of a phantom to promote more homogeneous configurations across sites Glover et al., 2012; Friedman et al., 2008) . Even in standardized experiments, it should be noted that differences in scanner protocols remain (Brown et al., 2011) . A much larger multisite sample with systematically varying parameters could enable a data-driven identification of the critical parameters impacting site effects. The various releases made by the INDI initiative may fill that gap in the literature in the future, as the scanner protocols are much better described in recent releases, such as CoRR (Zuo et al., 2014) , than they were in the initial FCP release. These findings stress the need for more work to find the source of that variance rather than ad-hoc procedures to correct for them. by a salary award from \"Fonds de recherche du Qu\u00e9bec -Sant\u00e9\" and the Courtois Foundation."}]