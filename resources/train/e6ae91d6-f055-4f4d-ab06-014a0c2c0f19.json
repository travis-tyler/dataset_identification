[{"section_title": "Abstract", "text": "Abstract The automated identification of brain structure in Magnetic Resonance Imaging is very important both in neuroscience research and as a possible clinical diagnostic tool. In this study, a novel strategy for fully automated hippocampal segmentation in MRI is presented. It is based on a supervised algorithm, called RUSBoost, which combines data random undersampling with a boosting algorithm. RUSBoost is an algorithm specifically designed for imbalanced classification, suitable for large data sets because it uses random undersampling of the majority class. The RUSBoost performances were compared with those of ADABoost, Random Forest and the publicly available brain segmentation package, FreeSurfer. This study was conducted on a data set of 50 T1-weighted structural brain images. The RUSBoost-based segmentation tool achieved the best results with a Dice's index of 0:88 AE 0:01 (0:87 AE 0:01) for the left (right) brain hemisphere. An independent data set of 50 T1-weighted structural brain scans was used for an independent validation of the fully trained strategies. Again the RUSBoost segmentations compared favorably with manual segmentations with the highest performances among the four tools. Moreover, the Pearson correlation coefficient between hippocampal volumes computed by manual and RUSBoost segmentations was 0.83 (0.82) for left (right) side, statistically significant, and higher than those computed by Adaboost, Random Forest and FreeSurfer. The proposed method may be suitable for accurate, robust and statistically significant segmentations of hippocampi."}, {"section_title": "", "text": "Abstract The automated identification of brain structure in Magnetic Resonance Imaging is very important both in neuroscience research and as a possible clinical diagnostic tool. In this study, a novel strategy for fully automated hippocampal segmentation in MRI is presented. It is based on a supervised algorithm, called RUSBoost, which combines data random undersampling with a boosting algorithm. RUSBoost is an algorithm specifically designed for imbalanced classification, suitable for large data sets because it uses random undersampling of the majority class. The RUSBoost performances were compared with those of ADABoost, Random Forest and the publicly available brain segmentation package, FreeSurfer. This study was conducted on a data set of 50 T1-weighted structural brain images. The RUSBoost-based segmentation tool achieved the best results with a Dice's index of 0:88 AE 0:01 (0:87 AE 0:01) for the left (right) brain hemisphere. An independent data set of 50 T1-weighted structural brain scans was used for an independent validation of the fully trained strategies. Again the RUSBoost segmentations compared favorably with manual segmentations with the highest performances among the four tools. Moreover, the Pearson correlation coefficient between hippocampal volumes computed by manual and RUSBoost segmentations was 0.83 (0.82) for left (right) side, statis"}, {"section_title": "Introduction", "text": "The role of neuroimaging in the study of brain disease and for clinical diagnostic purposes has acquired increasing importance. The possibility of investigating the morphology of specific brain structures relies on their accurate delimitation from the surrounding brain parenchyma and from the other adjacent structures (segmentation). This proves particularly challenging for structures characterized by morphological complexity, such as the hippocampus, a part of the temporal lobe with a prominent role in memory and other cognitive functions. The hippocampus is primarily involved in the pathogenesis of a number of conditions, firstly Alzheimer's disease (AD), the most common type of dementia [1] . Nowadays, a definite diagnosis of AD can only be made if there is histopathological confirmation, either post-mortem or on brain biopsy. However, biomarkers of the disease supportive of the diagnosis are now recognized, and these include structural brain changes visible on Magnetic Resonance Images (MRIs), in particular atrophy of the medial temporal lobe and in particular of the hippocampal formation [2] [3] [4] [5] [6] [7] .\nManual segmentation of hippocampus has been so far considered the gold standard, despite the heterogeneity of anatomical landmarks and protocols adopted [8] ; it is also laborious, time consuming and prone to rater error. Automated segmentation techniques are gaining increasing recognition since, not only they offer the possibility of studying rapidly large databases, for example in pharmaceutical trials or genetic research, but also afford higher test-retest reliability and the robust reproducibility needed for multi-centric studies. In the last few years, state-of-the-art hippocampal segmentation from 3D MRI research has delineated a few major approaches. Multiatlas methods, among which the joint label fusion technique proposed by Wang et al. [9] , are based on information propagation between multiple atlases, and bias correction. Other approaches are based on the active contour models (ACM) [10] , in which a deformable contour is iteratively adapted to the image in order to generate the partition of the ROI. Machine learning approaches, on the contrary, use statistical tools from image processing techniques to perform the segmentation of the hippocampus, by focusing on the delineation of most characterizing features (texture, shape, edges). Among them, Morra et al. [11, 12] showed the validity of this approach for accurate segmentation of the hippocampal region. Hence, building accurate tools for the identification of brain structures in MRI is a promising approach to identify anatomical differences that can be associated with the presence or absence of neurodegenerative diseases, such as AD. The brain images mostly contain noise, inhomogeneity and sometime deviation [13] , therefore accurate segmentation of brain images in a difficult task. Despite numerous efforts described in the literature [11, [14] [15] [16] [17] [18] [19] , segmentation is still commonly performed manually by experts.\nThe main goal of this work was to develop an accurate strategy based on supervised learning algorithms for hippocampal segmentation using 3D brain MRI. The task of a classifier, trained on a set of previously labeled examples (MR images in which the hippocampi had been previously manually segmented), is to classify voxels of a new brain MR image as belonging or not to the hippocampus. In this study, the performance of a novel statistical strategy, based on RUSBoost [20] , was evaluated for hippocampal segmentation. RUSBoost was designed for imbalanced classification problems, combining data random undersampling with boosting. It is an alternative of another data sampling/boosting algorithm called SMOTEBoost [21] which uses an oversampling technique, creating new minority class examples by extrapolating between existing examples, combined with boosting technique. Creating new examples, SMOTEBoost increases model training times. It has been successful in applications [22, 23] where not too big data sets were analyzed. As the training data increases in size, the SMOTE run time increases, incurring the risk of becoming impractical. When a data set is very large, as for 3D MRI data sets, selecting an appropriate sampling method becomes important. Training a model on very large data set would take much less if undersampling is used as for RUSBoost. The drawback associated with undersampling is the loss of information that comes with deleting examples from the training data. Moreover, there is evidence that the RUSBoost algorithm performs favorably when compared to SMOTEBoost, while being a simpler and faster technique that often results in significantly better classification performance [21, 24] . To the best of our knowledge, this is the first application of RUSBoost classifiers to hippocampal segmentation.\nThis work utilizes two datasets, obtained from the Alzheimers Disease Neuroimaging Initiative (ADNI, http:// adni.loni.usc.edu/) database, consisting of MR images and their corresponding expert manual labels produced with a standard harmonized protocol. The first data set, DB1, was used for training the algorithms and estimating evaluation metrics via cross validation. The RUSBoost performances on DB1 were excellent when compared with those of three classifiers, Adaboost [25] , Random forest (RF) [26] and FreeSurfer v.5.1 [15] . Adaboost is a boosting algorithm that sequentially selects weak classifiers and weights each of them based on their error. It has been previously employed as segmentation tools in [11] . RF uses multiple binary decision trees, and recently several brain MRI segmentation systems based on RF classifiers have appeared in the literature [16, 19, [27] [28] [29] . FreeSurfer is a publicly available package and can be considered the stateof-the-art whole-brain segmentation tool, since numerous imaging studies across multiple centers have shown its robustness and accuracy [30] .\nThe second data set, DB2, was employed for an assessment of the performance of the fully trained classifiers. Results on the DB2 data set confirmed those obtained in the previous analysis and showed that the RUSBoost segmentation strategy, trained on DB1, generalized very well on the independent data set, avoiding problems like overfitting. Moreover, the hippocampal volumes obtained with our RUSBoost segmentation showed the best correlation with those segmented manually, which is very important for diagnostic purposes.\nFor all the classifiers, we also evaluated how the Dice's index varied with the training set size, providing practical guidelines for future users."}, {"section_title": "Materials and methods", "text": ""}, {"section_title": "Data set description", "text": "The data used in the preparation of this study were obtained from the Alzheimers Disease Neuroimaging Initiative (ADNI, http://adni.loni.usc.edu) database. The ADNI was launched in 2003 by the NIA, the National Institute of Biomedical Imaging and Bioengineering (NIBIB), the U.S. Food and Drug Administration (FDA), private pharmaceutical companies, and nonprofit organizations. For up-todate information, see http://www.adni-info.org.\nTwo databases of T1-weighted whole-brain MR images, DB1 and DB2, were used in the study, both including normal controls (NC), subjects with mild cognitive impairment (MCI) and patients with Alzheimer's disease (AD). All images were downloaded from the ADNI LONI Image Data Archive (https://ida.loni.usc.edu). Both DB1 and DB2 data sets consisted of 50 subjects each whose demographic details are reported in Table 1 . All the images were acquired on 1.5 Tesla, and 3.0 Tesla scanners which specifications are reported in Table 2 .\nBilateral hippocampi were manually segmented using the Harmonized Hippocampal Protocol (http://www.hippo campal-protocol.net/) [31, 32] which aims to standardize the available manual segmentation protocols. The more inclusive definition of the Harmonized protocol may also limit the inconsistencies due to the use of arbitrary lines and tissue exclusion of the currently available manual segmentation protocols.\nPreprocessing involved a first registration through a sixparameter affine transformation to the Montreal Neurological Institute MNI152 template. Then a gross peri-hippocampal volume was extracted for left and right hippocampi for each scan and for the template; these regions underwent a further affine registration using the template hippocampal boxes as reference images. In this way, two Volumes of Interest (VOIs) of dimension 50 \u00c2 60 \u00c2 60 were obtained. The two registrations and box extraction were fully automated."}, {"section_title": "Features", "text": "The 3D segmentation was performed using for each voxel a vector of 315 elements (Table 1) representing information about position, intensity, neighboring texture, and local filters. Haar-like and Haralick features provide information on image texture, in particular on contrast, uniformity, rugosity, regularity, etc. [33] [34] [35] [36] . A number of 248 Haarlike features were calculated spanning a 3D filter of varying dimensions (from 3 \u00c2 3 \u00c2 3 to 9 \u00c2 9 \u00c2 9) for each voxel and averaging the voxels intensities in each VOI. Forty-eight Haralick features were calculated; in particular energy, contrast, correlation and inverse difference moment were computed based on the calculation of gray level co-occurrence matrix (GLCM), created on the n \u00c2 n voxels (n varying from 3 to 9) projection subimages of the volume centered in each voxel. A study on local Haralik features has been previously carried out showing their successful application to hippocampal segmentation [27] . Number of features used in the data sets is shown Finally, the gradients calculated in different directions and at different distances, and the relative positions of the voxels (x, y, z) were included as additional features."}, {"section_title": "RUSBoost", "text": "RUSBoost is a boosting-based sampling algorithm designed to handle class imbalance. It combines Random UnderSampling (RUS) and Adaboost. RUS is a technique that randomly removes examples from the majority class until the desired balance is achieved. Let x i be a point in the feature space X and y i be a class label in Y \u00bc f\u00c01; \u00fe1g. \nIt is a modified version of Adaboost error function: here an higher cost is assigned to the examples with higher probability of being misclassified by the weak learner. 5. The weight update parameter is calculated: Output the final hypothesis:\n3 Results and discussion\nAll data were analyzed using Matlab (MathWorks, Natick, MA). A cross-validation (CV) technique was used in order to estimate how accurately a predictive model will perform in practice. Figure 1 shows one round of CV which involves partitioning a sample of data into complementary subsets, training and test sets, building the classifier on the first set, and validating the model on the second set. To reduce variability, multiple rounds of CV are performed using different partitions, and the validation results are averaged over the rounds.\nBefore performing the classification, the preprocessing involved a first registration of all the images in the same stereotaxic space and extraction of the gross peri-hippocampal VOI containing 50 \u00c2 60 \u00c2 60 \u00bc 180000 voxels (see Sect. 2.1). Next 315 features suitable for describing complex images were extracted, as reported in Sect. 2. Hence the number of examples in the training (test) set was given by 180000 \u00c2 the number of training (test) images, and the number of components was 315. Internally to each round of the cross validation, a bounding box around the training hippocampi was defined by the logical OR of the training masks. A reduced VOI (rVOI) was identified using this bounding box plus some neighboring voxels obtained applying a cubic kernel of size 2 \u00c2 2 \u00c2 2. The rVOI dimensions increased with the number m of training images (with m \u00bc 5; 10; 15; . . .; 40) and in each round of the CV, the rVOIs changed. The rVOI dimensions over ten rounds of CV were averaged. The resulting mean values, varying m, are shown in the Table 3 . Reduced training set and test set were built based on the training rVOI; their size can be computed multiplying the rVOI size by the number of training/test images.\nThe voxels outside the training rVOI definitely do not belong to the hippocampus. The neighboring voxels were included because they might contain hippocampal voxels of testing images lying outside the bounding box. The percentage of hippocampal voxels in the training rVOIs was in the range of 27-38 % of the total number. The use of rVOIs also reduced the computational time required for training the classifiers. It is worth reporting that in a first attempt, random undersampling of the majority class was used to obtain a desired unbalancing (in the range of 25-40 %) between hippocampus and non-hippocampus sets, combined with the classification task. This procedure results in worsened performances of the classifiers, hence the rVOI extraction was adopted.\nA number of standard metrics, described in Appendix 1, were calculated for each segmentation algorithm: Dice's coefficient, Precision, Recall and Relative Overlap (R.O). In particular Dice's index was used to compare the performances of the methods [12] . Left and right hemispheres were independently analyzed.\nThe RUSBoost performance for automated segmentation on the DB1 MRI data set was studied. Subsequently, we compared the performances of RUSBoost with two classifiers previously used in medical image analysis [11, 27, 28] : Adaboost and RF (see Appendix 1). Figure 2 summarizes the relationship Fig. 1 One round of the CrossValidation technique employed to evaluate the performances of RUSBoost, RF and Adaboost using the data set DB1 Table 5 shows all the metrics values obtained using m \u00bc 30 training VOIs for left and right hemispheres, highlighting a strong concordance of results between the two brain hemispheres. RUSBoost showed higher Recall than RF: the 87 % (86 %) of true left (right) hippocampus was correctly identified by RUSBoost, versus the 85 % (82 %) identified by RF. The Precision with RF was slightly higher than that of RUSBoost: 89 % (91 %) of the voxels that RF predicted as hippocampus for the left (right) side, was true hippocampus. This was 88 % with RUSBoost.\nFinally, in Table 5 the RUSBoost behavior was compared the publicly available segmentation package FreeSurfer v.5.1 (see Appendix 1), highlighting the excellent segmentation performances of the proposed algorithm. FreeSurfer segmentations compared with manual segmentations similarly to Adaboost, with a Dice's coefficient of 0.74 (0.76) for the left (right) side. These numbers should be treated with caution, because FreeSurfers segmentation Table I and are reproduced here for consistency tool uses a probabilistic atlas constructed from training data different from those employed for other algorithms, and an exact comparison is not possible without using the same data. To overcome this drawback, the performances of all the segmentation methods were evaluated on an independent data set. With this aim, we used an external data set DB2, obtained from an ADNI archive. This procedure guarantees a bias-free estimations of metrics for the RUSBoost, Adaboost and RF final model, trained on DB1, since DB2 was not employed to select the final models. For this section of the study, FreeSurfer was used again for comparison. The results (Table 6 ) illustrate the excellent performance of RUSBoost, followed by RF, on DB2, in keeping with the DB1 analysis. Unlike the DB1 analysis, in this case RUSBoots also achieved best Precision (0.89 and 0.88 for left and right side) and Recall (0.86 and 0.85 for left and right side). FreeSurfer and Adaboost gave the worst results. "}, {"section_title": "Conclusions", "text": "The use of automated techniques for image segmentation and analysis is gradually overtaking manual methods, particularly when applied to highly prevalent conditions, such as AD [11] and temporal lobe epilepsy [37] , both disorders in which the hippocampus plays a pivotal role in the pathogenesis of the illness.\nIn this paper, we propose a novel strategy for automated segmentation of the hippocampal region based on the classifier RUSBoost, which produced excellent results when compared with other two learning methods, Adaboost and RF, and the publicly available package, FreeSurfer. For all experiments described in this paper, the classifiers were learning generalizable methods. RUSBoost gave the best results in terms of evaluation metrics; RF was the next best, suggesting that RUSBoost and RF may perform much better than both Adaboost and FreeSurfer.\nRUSBoost proved to be the most accurate, with high sensitivity and precision. Moreover, the hippocampal volumes measured by RUSBoost showed the highest, statistically significant correlation with manually segmented volumes.\nSome of the differences in the results obtained using different segmentation methods may be ascribed to the fact that the tools have been trained and tuned on different databases. Differences in image quality, manual segmentation protocol, clinical status and demographics have been described as possible causes of discrepancy [38] . An advantage of using machine learning algorithms for segmentation is the opportunity of using very large training data sets, shared by the scientific community. This is exemplified by the efforts of the EADC-ADNI working group to develop a standard harmonized protocol for the manual segmentation [8, 31, 32] (http://www.hippocampalprotocol.net) employed in our analysis.\nThis study was performed blindly to subject status. In terms of further developments, future efforts will be devoted to the application of these techniques to multiple data sets and other illness models. This approach could be extended to the study of other anatomical structures that have proved rather elusive to accurate segmentation, such as the thalamus or the putamen, both complex deep gray matter structures.\nOverall, the results obtained with automated segmentation are very promising and a better understanding of the characteristics of the main machine learning methods is necessary for future applications combining multiple biomarkers and different illness sub-types.\nAcknowledgments Data used in preparation of this article were obtained from the Alzheimers Disease Neuroimaging Initiative (ADNI) database (http://adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ ADNI_Acknowledgement_List.pdf. The Principal Investigator of this initiative is Michael W. Weiner, MD, VA Medical Center and University of California, San Francisco. ADNI is the result of efforts of many co-investigators from a broad range of academic institutions and private corporations, and subjects have been recruited from over 50 sites across the USA and Canada. The initial goal of ADNI was to recruit 800 subjects but ADNI has been followed by ADNI-GO and ADNI-2. To date these three protocols have recruited over 1500 adults, ages 55 to 90, to participate in the research, consisting of cognitively normal older individuals, people with early or late MCI, and people with early AD. The follow-up duration of each group is specified in the protocols for ADNI-1, ADNI-2 and ADNI-GO. Subjects originally recruited for ADNI-1 and ADNI-GO had the option to be followed in ADNI-2. For up-to-date information, see http://www.adni-info.org. Data collection and sharing for this project were funded by the Alzheimer's Disease Neuroimaging Initiative (ADNI) (National Institutes of Health Grant U01 AG024904) and DOD ADNI (Department of Defense award number W81XWH-12-2-0012). ADNI is funded by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, and through generous contributions from the following: Alzheimers "}, {"section_title": "Evaluation metrics", "text": "A number of standard metrics described below were used to compare the performances of the four segmentation algorithms. Two binary vectors A and B are considered. A contains the voxel labels as identified by manual tracing and B contains the voxel labels predicted using a supervised learning algorithm. The voxels that the classifier correctly identifies as belonging to the hippocampus represent the true positives (TP) (i.e. intersection of A and B), the voxels correctly identified as background the true negatives (TN); the voxels wrongly identified as belonging to the hippocampus are the false positives (FP), and, finally, the voxels wrongly identified as background are the false negatives (FN). Dice's coefficient, precision, recall and relative overlap are defined as follows: "}, {"section_title": "Classifiers Adaboost", "text": "Adaboost is a meta-algorithm that sequentially selects weak classifiers, and weighs each of them based on their error. A weak classifier is a classifier that performs better than pure chance. The algorithm assigns to each example the weight D 1 \u00f0i\u00de \u00bc The output of the strong classifier on a new example x is:\nThe algorithm tends to concentrate on hard examples, i.e. after selecting an optimal classifier h t for the distribution D t , the examples x i , that were identified correctly by the classifier h t , are given lower weight, and those that were identified incorrectly by h t are given higher weight. Therefore, when the algorithm is testing the classifiers on the distribution D t\u00fe1 , it will select a classifier that better identifies those examples that the previous classifier missed.\nThe final hypothesis y is a weighted majority vote of the T weak hypothesis where a t is the weight to h t , that is the weighted mean of the T weak classification on x."}, {"section_title": "Random forest", "text": "Random Forest uses multiple binary decision trees. Each of the classification trees is built using a sample of the training data, and at each node a randomly chosen set of variables is considered for the best split.\nFor b \u00bc 1; 2; . . .; B the RF algorithms can be briefly described as follows.\n1. A bootstrap sample Z* of size n is drawn from the training set. 2. A random forest tree T b is grown from the bootstrapped data, by recursively repeating the following steps for each terminal node of the tree, until the minimum node size, n min , is reached:\nSelection of q variables at random from the d variables; Choice of the best variable/split-point among q (internal feature selection); Splitting of the node into two daughter nodes. In the experiments here described, q \u00bc ffiffiffi d p and the minimum node size was 1."}, {"section_title": "FreeSurfer", "text": "Cortical reconstruction and volumetric segmentation were performed with the FreeSurfer image analysis suite, which is documented and freely available for download online. 1 The technical details of these procedures are described in prior publications [15, [39] [40] [41] [42] [43] [44] [45] [46] [47] [48] [49] . Briefly, this processing includes motion correction and averaging [50] of multiple volumetric T1 weighted images (when more than one is available), removal of non-brain tissue using a hybrid watershed/surface deformation procedure [48] , automated Talairach transformation, segmentation of the subcortical white matter and deep gray matter volumetric structures (including hippocampus, amygdala, caudate, putamen, ventricles) [15, 42] intensity normalization [51] , tessellation of the gray matter white matter boundary, automated topology correction [41, 52] , and surface deformation following intensity gradients to optimally place the gray/white and gray/cerebrospinal fluid borders at the location where the greatest shift in intensity defines the transition to the other tissue class [39, 40, 49] . Once the cortical models are complete, a number of deformable procedures can be performed for in further data processing and analysis including surface inflation [39] , registration to a spherical atlas which utilized individual cortical folding patterns to match cortical geometry across subjects [44] , parcellation of the cerebral cortex into units based on gyral and sulcal structure [45, 53] , and creation of a variety of surface-based data including maps of curvature and sulcal depth. This method uses both intensity and continuity information from the entire threedimensional MR volume in segmentation and deformation procedures to produce representations of cortical thickness, calculated as the closest distance from the gray/white boundary to the gray/CSF boundary at each vertex on the tessellated surface [40] . The maps are created using spatial intensity gradients across tissue classes and are therefore not simply reliant on absolute signal intensity. The maps produced are not restricted to the voxel resolution of the original data thus are capable of detecting submillimeter differences between groups. Procedures for the measurement of cortical thickness have been validated against histological analysis [54] and manual measurements [55, 56] . Freesurfer morphometric procedures have been demonstrated to show good test-retest reliability across scanner manufacturers and across field strengths [46, 57] ."}, {"section_title": "Example text for longitudinal processing", "text": "To extract reliable volume and thickness estimates, images where automatically processed with the longitudinal stream in FreeSurfer [57] . Specifically an unbiased within-subject template space and image [58] is created using robust, inverse consistent registration [50] . Several processing steps, such as skull stripping, Talairach transforms, atlas registration as well as spherical surface maps and parcellations are then initialized with common information from the within-subject template, significantly increasing reliability and statistical power [57] ."}]