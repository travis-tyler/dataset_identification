[{"section_title": "Abstract", "text": "Abstract. This paper aims to solve a fundamental problem in intensitybased 2D/3D registration, which concerns the limited capture range and need for very good initialization of state-of-the-art image registration methods. We propose a regression approach that learns to predict rotation and translations of arbitrary 2D image slices from 3D volumes, with respect to a learned canonical atlas co-ordinate system. To this end, we utilize Convolutional Neural Networks (CNNs) to learn the highly complex regression function that maps 2D image slices into their correct position and orientation in 3D space. Our approach is attractive in challenging imaging scenarios, where significant subject motion complicates reconstruction performance of 3D volumes from 2D slice data. We extensively evaluate the effectiveness of our approach quantitatively on simulated MRI brain data with extreme random motion. We further demonstrate qualitative results on fetal MRI where our method is integrated into a full reconstruction and motion compensation pipeline. With our CNN regression approach we obtain an average prediction error of 7mm on simulated data, and convincing reconstruction quality of images of very young fetuses where previous methods fail. We further discuss applications to Computed Tomography and X-ray projections. Our approach is a general solution to the 2D/3D initialization problem. It is computationally efficient, with prediction times per slice of a few milliseconds, making it suitable for real-time scenarios."}, {"section_title": "Introduction", "text": "Intensity-based registration requires a good initial alignment. General optimisation methods often cannot find a global minimum from any given starting position on the cost function. Thus, image analysis that requires registration, e.g., atlas-based segmentation [2] , motion-compensation [14] , tracking [13] , or clinical analysis of the data visualised in a standard co-ordinate system, often requires manual initalisation of the alignment. This problem gets particularity challenging for applications where the alignment is not defined by a 3D-3D rigidbody transformation. An initial rigid registration can be achieved by selecting common landmarks [3] . However, many applications, in particular motion compensation techniques, require at least approximate spatial alignment and 3D consistency between individual 2D slices to provide a useful initialisation for subsequent automatic registration methods. Manual alignment of hundreds of slices is not feasible in practice. Landmark-based techniques can mitigate this problem, but is heavily dependent on detection accuracy and robustness of the calculated homography between locations and the descriptive power of the used landmark encoding. 2D slices also do not provide the required 3D information to establish robust landmark matching, therefore this technique cannot be used on applications such as motion compensation in fetal imaging.\nRobustness of (semi-)automatic registration methods is characterised by their capture range, which is the maximum transformation offset from which a specific method can recover good spatial alignment. For all currently known intensitybased registration methods, the capture range is limited. Contribution: We introduce a method that automatically learns slice transformation parameters relative to a canonical atlas co-ordinate system, purely from the encoded intensity information in 2D slices. We propose a Convolution Neural Network (CNN) regression approach that is able to predict and re-orient arbitrarily sampled slices, to provide an accurate initialisation for subsequent intensity-based registration. Our method is applicable to a number of clinical situations. In particular, we quantitatively evaluate the prediction performance with simulated 2D slice data extracted from adult 3D MRI brain and thorax phantoms. In addition, we qualitatively evaluate the approach for a full reconstruction and motion compensation pipeline for fetal MRI. Our approach can naturally be generalised to 3D/3D volumetric registration by predicting the transformation of a few selected slices. It is also applicable to projective images, which is highly valuable for X-ray/CT registration. Related Work: Slice-to-Volume registration is a key step in medical imaging, as it allows single or multiple 2D images to be registered together in a common world co-ordinate system to form a consistent 3D volume. This provides better visualisation for the practitioner to either diagnose or perform operative procedures. Furthermore, it paves the way to exploit 3D medical image analysis techniques.\nIn literature one can distinguish between volume-to-slice and slice-to-volume techniques. The first is concerned with aligning a volume to a given image, e.g., aligning an intra-operative C-arm X-ray image to a pre-operative volumetric scan. This can be manually or artificially initialised and many approaches have been proposed to solve this problem. The most advanced solution to this problem we are aware of uses CNNs to evaluate the spatial arrangement of landmarks automatically [13] . Besides this, methods that can compensate for large offsets usually require the use of fiducial markers [9] , which makes use of either special equipment or invasive procedures.\nWhile our method is also applicable to the volume-to-slice problem, as shown in Exp. 3, here we focus on the slice-to-volume problem. Manual alignment of hundreds of slices to each other is much more challenging than the theoretically possible manual initialisation of volume-to-slice problems.\nOne target application we discuss in this paper is fetal MRI, where maternal breathing and spontaneous movement from the fetus is a major problem, that involves slice-wise re-alignment of randomly displaced anatomy [4, 8, 14, 11] . Existing methods require good initial spatial consistency between the acquired slices to generate an approximation of the target structure. This approximation is used for iterative refinement of slice-to-volume registration. Good initial 3D slice alignment is only possible trough fast acquisition like single-shot Fast Spin Echo (ssFSE) and the acquisition of temporally close, intersecting stacks of slices. Redundant data covering an area of interest cannot be used from all acquired images since the displacement worsens during the course of an examination, thus redundancy has to be high and, generally, several attempts are necessary to acquire better quality data that can be motion compensated. Nevertheless, from the clinical practice, we know that individual 2D slices are well examinable and trained experts are able to virtually realign a collection of slices mentally with respect to their real anatomical localization during diagnostics. The recent advent of deep neural network architectures [12] suggests that such a learning based expert-intuition of slice transformations can also be achieved fully automatically using machine learning. Fig. 1 : Overview over our approach. The core of our method utilises a CNN, called SVRNet, to regress and predict transformation parametersT i , such thatT i = \u03c8(\u03c9 i , \u0398), where \u0398 is the learned network parameters and \u03c9 i \u2208 \u2126 are a series of 2D image slices that are acquired from a moving 3D object \u2126. SVRNet provides a robust initialisation for intensitybased registration refinement by predictingT i for each \u03c9 i (see Fig. 1 ). We also define T i as known ground truth parameters of \u03c9 i during validation.\nOur proposed pipeline consists of three modular components: (I) approximate organ localisation, (II) prediction ofT i , and (III) 3D reconstruction/ intensity-based registration refinement.\nOrgan localisation, which defines a Region of Interest (ROI), can be achieved using rough manual delineation, organ focused scan sequences or automatic methods, such as [10] for example for the fetal MRI use case. For 3D Reconstruction, we use a modified Slice-to-Volume Reconstruction (SVR) method [8] and initialise it with transformed \u03c9 i usingT i . Here on, we focus on the novel part of this pipeline, which is SVRNet. SVRNet needs to be trained accurately on a desired ROI, imaging modality, and use-case scenario. Data Set Generation: \u03c9 i , for training and validation, are generated from n motion free 3D volumes \u2126 train . Each volume encloses a desired ROI, is centred at the origin and re-sampled to a cubic volume of length L, with spacing 1mm \u00d7 1mm \u00d7 1mm. L/4 sampling planes, with spacing of 4mm and size L \u00d7 L, are evenly spaced along the Z-axis.\n\u03c9 i at extremities of \u2126 train may contain little or no content. If the variance of a particular \u03c9 i is below a threshold of t, where\n, then it is omitted. A higher K value will restrict \u03c9 i to the middle portion of the volume. In our experiments, K \u2248 0.2, which samples the central 80% of the volume.\nTo capture a dense permutation ofT i \u2208 \u2126 train , we rotate the sampling planes about the origin whilst keeping the volume static. Ideally, all rotational permutations should be random and evenly spaced on the surface of a unit sphere. Uniform sampling of polar co-ordinates, P (\u03c6, \u03b8), causes denser sampling near the poles. This can lead to an imbalance of training samples. Thus we use Fibonacci sphere sampling [5] , which allows each point to represent approximately the same area. Thus sampling normals can be calculated by P (\u03c6 i , cos \u22121 (z i )), where \u03c6 i = 2\u03c0i/\u03a6 and z i = 1 \u2212 (2i + 1)/n, i \u2208 0, 1, 2, ..., n \u2212 1. \u03a6 is the golden ratio, as \u03a6 \u22121 = \u03a6 \u2212 1, and is defined as \u03a6 = ( \u221a 5 + 1)/2. For both, training and validation, only one hemisphere needs to be sampled due to symmetry constraints. Sampling planes with normals in the one hemisphere result in the same image as sampling planes with normals in the other hemisphere albeit mirrored. Ground Truth Labels:T i can be represented by Euler angles (six parameters: {r x , r y , r z , t x , t y , t z }) or Quaternions (seven parameters: {q 1 , q 2 , q 3 , q 4 , t x , t y , t z }), or by defining three Cartesian landmarks within the plane (nine parameters). Huynh et al. [6] have presented detailed analysis on distance functions for 3D rotations. As they are differentiable, we have implemented them as custom loss layers for regressing on rotational parameters. The loss for Euler angles can be ex-\nwhere d(a, b) = min{|a \u2212 b|, 2\u03c0 \u2212 |a \u2212 b|}, and \u03b1, \u03b3 \u2208 [\u2212\u03c0, \u03c0); \u03b2 \u2208 [\u2212\u03c0/2, \u03c0/2). For quaternions; \u03a8 2 (q 1 , q 2 ) = min { q 1 \u2212 q 2 , q 1 + q 2 }, where q 1 and q 2 are unit quaternions. We have evaluated all of these options and found that the Cartesian landmark approach yielded the highest accuracy. Hence, we use this approach in all our experiments. The landmarks can be arbitrarily selected, as long as their location remains consistent for all \u03c9 i . For our experiments, we have chosen the centres of \u03c9 i , p c , and two corners p l , p r ; where p c = (0,0,z),\n. To take rotation into account, each point is further multiplied by a rotation matrix R to obtain their final position in world co-ordinates. Each \u03c9 i can thus be described by nine parameters: p c (x, y, z), p l (x, y, z) and p r (x, y, z). This approach keeps the nature of the network loss consistent as it only needs to regress in Cartesian co-ordinate space instead of a mixture of Cartesian co-ordinates and rotation parameters. Network Design: SVRNet is derived from the CaffeNet [7] architecture. Experimentation with other architectures has revealed that this approach yields a maximum training performance whilst keeping the training effort feasible. For regression, we define multiple loss outputs; one for each p c , p l , p r . SVRNet employs therefore a multi-loss framework, which avoids over-fitting to one particular single loss [16] . Fig. 1 shows the details of the SVRNet architecture. 3D Reconstruction: The network predictsT i to certain degree of accuracy.\nTo reconstruct an accurate high-resolution, motion free 3D volume for \u2126 from the regression, we integrate an iterative intensity-based SVR motion compensation approach. Conventional SVR methods, e.g. [8] , require a certain degree of correct initial 2D slice alignment in scanner co-ordinate space to estimate an initial approximation of a common volume \u2126. The approximation of \u2126 is subsequently used as a 3D registration target for 2D/3D slice-to-volume registration.\nOur approach does not depend on good initial slice alignment and disregards slice scanner co-ordinates completely. We only use slice intensity information for SVRNet and generate an initialization for \u2126 using the predictedT i . We use regularized Super-Resolution and a Point-Spread-Function similar to [8] to account for different resolutions of low-resolution \u03c9 i and high-resolution \u2126. \u03c9 i -to-\u2126 registration is then individually refined using cross-correlation as cost-function and gradient decent for optimization. Optimization uses three scales of a Gaussian Pyramid representation for \u03c9 i and \u2126. Robust statistics [8] identifies \u03c9 i that have been mis-predicted and excludes them from further iterations."}, {"section_title": "Experiments and Results", "text": "We have tested our approach on 85 randomly selected and accurately segmented healthy adult brains, on a real-world use case scenario with 34 roughly delineated fetal brain MRI scans and on 60 low-dose thorax CT scans with no organ specific segmentation. SVRNet's average prediction error for these datasets is respectively 5.6\u00b11.07mm, 7.7\u00b14.80mm, and 5.9\u00b12.43mm. We evaluate 3D reconstruction performance using the Peak Signal-to-Noise Ratio (PSNR) and T i prediction error as average distance in mm between ground truth locations p c,gt , p l,gt , p r,gt and predicted locations p c,p , p l,p , p r,p , i.e., (||p c,gt \u2212p c,p ||+||p l,gt \u2212 p l,p || + ||p r,gt \u2212 p r,p ||)/3.0. All experiments are conducted using the Caffe neural network library, on a computer equipped with an Intel 6700K CPU and Nvidia Titan X Pascal GPU. Exp. 1: Segmented adult brain data is used to evaluate our network's regression performance with known ground truth T i . 85 brains from the ADNI data set [1] were randomly selected; 70 brains for \u2126 train and 15 brains for \u2126 validation . Fig. 2 shows an example slice of the ground truth and the reconstructed \u2126. Each brain has been centered and re-sampled in a 256 \u00d7 256 \u00d7 256 volume. Using the Fibonacci Sphere Sampling method, a density of 500 unique normals is chosen with 64 sampling planes spaced evenly apart on the Z-axis (giving a spacing of 4mm). This therefore yields a maximum of 32000 images per brain; 2.24M for the entire training set and 345K for the entire validation set. After pruning \u03c9 i with little or no content, this figure drops to approximately 1.2M images for training and 254K for validation. Training took approximately 27hrs for 30 epochs. Reconstructing fromT i initialisation without SVR yields a PSNR of 23.7 \u00b1 1.09; with subsequent SVR the PSNR increases to 29.5\u00b12.43 when tested on 15 randomly selected test volumes after four iterations of SVR. Exp. 2: Fetal brain data is used to test the robustness of our approach under real conditions. Fetuses younger than 30 weeks very often move a lot during examination. Fast MRI sequences allow artifact free acquisition of individual slices but motion between slices corrupts consistent 3D information. Fig. 3 shows that our method is able to accurately predictT i also under these conditions. For this experiment we use \u03c9 i from three orthogonally overlapping stacks of ssFSE slices covering the fetal brain with approximately 20-30 slices each. We are ignoring the stack transformations relative to the scanner and treat each \u03c9 i individually. For \u2126 train , 28 clinically approved motion compensated brain reconstructions are resampled into a 150 \u00d7 150 \u00d7 150 volume with 1mm \u00d7 1mm \u00d7 1mm spacing. A density of 500 unique sampling normals has been chosen via the Fibonacci sphere sampling method with 25 sampling planes evenly spaced between -25 to +25 on the Z-axis. This gives a plane spacing of 2mm, sampling only the middle portion of the fetal brain. Training took approximately 10hrs for 30 epochs. Prediction, i.e., the forward pass through the network, takes approx. 12 ms/slice. Exp. 3: Adult thorax data: To show the versatility of our approach we also apply it to adult thorax scans. For this experiment no organ specific training is performed but the whole volume is used. We evaluate reconstruction performance similar to Exp. 1 andT i prediction performance when \u2126 is projected on an external plane, comparable to X-Ray examination using C-Arms. The latter provides insights about our method's performance when applied to interventional settings in contrast to motion compensation problems. 60 healthy adult thorax scans were randomly selected, 51 scans used for \u2126 train and nine scans used for We use Siddon-Jacobs ray tracing [15] to generate Digitally Reconstructed Radiographs (DRRs) from the above described data. For training, we equally sample DRRs on equidistant half-spheres around 51 CT volumes at distances of 80cm, 60cm, and 40cm, between \u221290\n\u2022 and 90\n\u2022 around all three co-ordinate axes. For validation, we generate 1000 DRRs with random rotation parameters within the bounds of the training data at 60cm distance from the volumetric iso-centre. We trained on healthy volunteer data and tested on nine healthy and ten randomly selected pathological volumes (eight lung cancer and two spinal pathologies). Our approach is able to predict DRR transformations relative to the trained reference co-ordinate system with an average translation error of 106mm and 5.6\n\u2022 plane rotation for healthy patients, and 130mm and 7.0 \u2022 average error for pathological patients. An example is shown in Fig. 4e ,f. Note that these values are good enough to robustly initialize intensity-based registration refinement. SVRNet prediction can be improved by generating a denser training data set, in particular, in more equidistant half-spheres. Discussion & Conclusion: We have presented a method that is able to predict slice transformations relative to a canonical atlas co-ordinate system. This allows motion compensation for highly motion corrupted scans, e.g., MRI scans of very young fetuses. It allows to incorporate all images that have been acquired during examination and temporal proximity is not required for good initialisation of intensity-based registration methods as it is the case in state-of-the-art methods. We have shown that our method performs remarkably well for fetal brain data in presence of surrounding tissue and without organ specific training for low-dose thorax CT data and X-Ray to CT registration.\nOne limitation of our method is that SVRNet requires images to be formatted in the same way the network is trained on. This includes identical intensity ranges, spacing and translation offset removal and can be achieved with simple pre-processing methods. Furthermore, SVRNet has to be trained for a specific region of interest or organ and scenario (e.g., MRI T1, T2, X-Ray exposure, etc.). However, we show that the training region does not need to be delineated accurately and that our method is not restricted with respect to the used imaging modality and scenario. "}, {"section_title": "Appendices", "text": "The prediction performance per slice is shown in absolute numbers in Fig. 10 , Fig. 15 and Fig. 17 ."}, {"section_title": ".1 Data generation illustration", "text": "Figures 5 to 7 illustrate our data generation sampling strategies and shows the new Z-axis, i.e. normals, of the sampling planes with respect to the origin. To calculate the rotation error of the predicted plane, we use linear algebra to create the rotation matrix. The predicted Cartesian points are likely to not form a perfect isosceles triangle formation compared to the ground truth. We exclusively use p c as the plane's origin point in world space with plane rotation calculated by the following method.\ndef c a l c u l a t e R ( p1 , p2 , p3 ) : v1 = p3 \u2212 p1 v2 = p2 \u2212 p1 n1 = np . c r o s s ( v1 , v2 ) n2 = np . c r o s s ( n1 , v1 ) v1 norm = v1 / np . l i n a l g . norm ( v1 ) #x n2 norm = n2 / np . l i n a l g . norm ( n2 ) #y n1 norm = n1 / np . l i n a l g . Any function, given that it's differentiable, can be used as a layer in a neural network. The distance metrics for 3D rotation are differentiable. Therefore, we are able to implement it as a network loss layer for regressing angle parameters.\nEuler Loss Function\nEuler Back Propagation Function\nQuaternion Loss Function\nwhere q 1 = {q\n1 , q\n1 , q\n1 } and q 2 = {q\n2 , q\n2 , q\n2 } .4 Network details Table 1 lists the details of the SVRNet architecture as it is shown in Figure 1 in the paper in a complementary textual way. In these experiments, we present a ground truth (GT) image to the network to estimate the respective transformation parameters needed to reorient the slice in its correct world co-ordinates. Using the transformation parameters, we generated a slice from the 3D atlas in the location where the network has predicted that slice should be (denoted as SVRNet).\nThe slices are compared side-by-side to give a visual representation of \"where the slice really is\" and \"where the network thinks the slice is\". Exp. 1: Slices, extracted from a correctly registered and reconstructed 3D volume, from the testing data set are presented to the network. The predicted slice is extracted from the same volume, using parameters estimated by SVRNet as shown in Fig. 8 and 9 . Exp. 2: Slices, from a motion corrupted MRI stack, are segmented and cropped. Since there is no ground truth for the queried images, an arbitrary fetal atlas is used for visualization in Fig. 11 and 12 . "}, {"section_title": "Exp. 3:", "text": "We replicated the experiment on adult thorax data without specifically segmented organs. This approach was applied to CT acquisition, shown in Fig 13  and 14 , as well as Digitally Reconstructed Radiographs generated using SiddonJacobs Ray Tracing shown in Fig. 16 . "}]