[{"section_title": "Abstract", "text": "Water managers in the western United States (U.S.) rely on longterm forecasts of temperature and precipitation to prepare for droughts and other wet weather extremes. To improve the accuracy of these longterm forecasts, the U.S. Bureau of Reclamation and the National Oceanic and Atmospheric Administration (NOAA) launched the Subseasonal Climate Forecast Rodeo, a year-long real-time forecasting challenge in which participants aimed to skillfully predict temperature and precipitation in the western U.S. two to four weeks and four to six weeks in advance. Here we present and evaluate our machine learning approach to the Rodeo and release our SubseasonalRodeo dataset, collected to train and evaluate our forecasting system.\nOur system is an ensemble of two nonlinear regression models. The first integrates the diverse collection of meteorological measurements and dynamic model forecasts in the SubseasonalRodeo dataset and prunes irrelevant predictors using a customized multitask feature selection procedure. The second uses only historical measurements of the target variable (temperature or precipitation) and introduces multitask nearest neighbor features into a weighted local linear regression. Each model alone is significantly more accurate than the debiased operational U.S. Climate Forecasting System (CFSv2), and our ensemble skill exceeds that of the top Rodeo competitor for each target variable and forecast horizon. Moreover, over 2011-2018, an ensemble of our regression models and debiased CFSv2 improves debiased CFSv2 skill by 40-50% for temperature and 129-169% for precipitation. We hope that both our dataset and our methods will help to advance the state of the art in subseasonal forecasting."}, {"section_title": "INTRODUCTION", "text": "Water and fire managers in the western United States (U.S.) rely on subseasonal forecasts-forecasts of temperature and precipitation two to six weeks in advance-to allocate water resources, manage wildfires, and prepare for droughts and other weather extremes [39] . While purely physics-based numerical weather prediction dominates the landscape of short-term weather forecasting, such deterministic methods have a limited skillful (i.e., accurate) forecast horizon due to the chaotic nature of their differential equations [24] . Prior to the widespread availability of operational numerical weather prediction, weather forecasters made predictions using their knowledge of past weather patterns and climate (sometimes called the method of analogs) [27] . The current availability of ample meteorological records and high-performance computing offers the opportunity to blend physics-based and statistical machine learning (ML) approaches to extend the skillful forecast horizon.\nThis data and computing opportunity, coupled with the critical operational need, motivated the U.S. Bureau of Reclamation and the National Oceanic and Atmospheric Administration (NOAA) to conduct the Subseasonal Climate Forecast Rodeo [28] , a yearlong real-time forecasting challenge, in which participants aimed to skillfully predict temperature and precipitation in the western U.S. two to four weeks and four to six weeks in advance. To meet this challenge, we developed an ML-based forecasting system and a SubseasonalRodeo dataset [14] suitable for training and benchmarking subseasonal forecasts.\nML approaches have been successfully applied to both shortterm (< 2 week) weather forecasting [3, 7-12, 18, 19, 22, 29, 31, 43] and longer-term climate prediction [1, 4, 16, 35, 36] , but mid-term subseasonal outlooks, which depend on both local weather and global climate variables, still lack skillful forecasts [33] . Our subseasonal ML system is an ensemble of two nonlinear regression models: a local linear regression model with multitask feature selection (MultiLLR) and a weighted local autoregression enhanced with multitask k-nearest neighbor features (AutoKNN). The MultiLLR model introduces candidate regressors from each data source in the SubseasonalRodeo dataset and then prunes irrelevant predictors using a multitask backward stepwise criterion designed for the forecasting skill objective. The AutoKNN model extracts features only from the target variable (temperature or precipitation), combining lagged measurements with a skill-specific form of nearest-neighbor modeling. For each of the two Rodeo target variables (temperature and precipitation) and forecast horizons (weeks 3-4 and weeks [5] [6] , this paper makes the following principal contributions:\n(1) We release a new SubseasonalRodeo dataset suitable for training and benchmarking subseasonal forecasts. (2) We introduce two subseasonal regression approaches tailored to the forecast skill objective, one of which uses only features of the target variable. (3) We introduce a simple ensembling procedure that provably improves average skill whenever average skill is positive. (4) We show that each regression method alone outperforms the Rodeo benchmarks, including a debiased version of the operational U.S. Climate Forecasting System (CFSv2), and that our ensemble outperforms the top Rodeo competitor. (5) We show that, over 2011-2018, an ensemble of our models and debiased CFSv2 improves debiased CFSv2 skill by 40-50% for temperature and 129-169% for precipitation.\nWe hope that this work will expose the ML community to an important problem ripe for ML development-improving subseasonal forecasting for water and fire management, demonstrate that ML tools can lead to significant improvements in subseasonal forecasting skill, and stimulate future development with the release of our user-friendly Python Pandas SubseasonalRodeo dataset."}, {"section_title": "Related Work", "text": "While statistical modeling was common in the early days of weather and climate forecasting [27] , purely physics-based dynamical modeling of atmosphere and oceans rose to prominence in the 1980s and has been the dominant forecasting paradigm in major climate prediction centers since the 1990s [2] . Nevertheless, skillful statistical machine learning approaches have been developed for shortterm weather forecasting with outlooks ranging from hours to two weeks ahead [3, 7-12, 18, 19, 22, 29, 31, 43] and for coarse-grained long-term climate forecasts with target variables aggregated over months or years [1, 4, 16, 35, 36] . Tailored machine learning solutions are also available for detecting and predicting weather extremes [23, 25, 30] . However, subseasonal forecasting, with its 2-6 week outlooks and biweekly granularity, is considered more difficult than either short-term weather forecasting or long-term climate forecasting, due to its complex dependence on both local weather and global climate variables [39] . We complement prior work by developing a dataset and an ML-based forecasting system suitable for improving temperature and precipitation prediction in this traditional 'predictability desert' [37] ."}, {"section_title": "THE SUBSEASONAL CLIMATE FORECAST RODEO", "text": "The Subseasonal Climate Forecast Rodeo was a year-long, real-time forecasting competition in which, every two weeks, contestants submitted forecasts for average temperature ( \u2022 C) and total precipitation (mm) at two forecast horizons, 15-28 days ahead (weeks 3-4) and 29-42 days ahead (weeks 5-6). The geographic region of interest was the western contiguous United States, delimited by latitudes 25N to 50N and longitudes 125W to 93W, at a 1 \u2022 by 1 \u2022 resolution, for a total of G = 514 grid points. The initial forecasts were issued on April 18, 2017 and the final on April 3, 2018. Forecasts were judged on the spatial cosine similarity between predictions and observations adjusted by a long-term average. More precisely, let t denote a date represented by the number of days since January 1, 1901, and let year(t), doy(t), and monthday(t) respectively denote the year, the day of the year, and the month-day combination (e.g., January 1) associated with that date. We associate with the two-week period beginning on t an observed average temperature or total precipitation y t \u2208 R G and an observed anomaly\ny t is the climatology or long-term average over 1981-2010 for the month-day combination d. Contestant forecasts\u0177 t were judged on the cosine similarity-termed skill in meteorology-between their forecast anomalies\u00e2 t =\u0177 t \u2212 c monthday(t ) and the observed anomalies:\n(1)\nTo qualify for a prize, contestants had to achieve higher mean skill over all forecasts than two benchmarks, a debiased version of the physics-based operational U.S. Climate Forecasting System (CFSv2) and a damped persistence forecast. The official contest CFSv2 forecast for t, an average of 32 operational forecasts based on 4 model initializations and 8 lead times, was debiased by adding the mean observed temperature or precipitation for monthday(t) over 1999-2010 and subtracting the mean CFSv2 reforecast, an average of 8 lead times for a single initialization, over the same period. An exact description of the damped persistence model was not provided, but the Rodeo organizers reported it relied on \"seasonally developed regression coefficients based on the historical climatology period of 1981-2010 that relate observations of the past two weeks to the forecast outlook periods on a grid cell by grid cell basis. \" period. The SubseasonalRodeo dataset is available for download at [14] , and Appendix A provides additional details on data sources, processing, and variables ultimately not used in our solution.\nTemperature Daily maximum and minimum temperature measurements at 2 meters (tmax and tmin) from 1979 onwards were obtained from NOAA's Climate Prediction Center (CPC) Global Gridded Temperature dataset and converted to \u2022 C; the same data source was used to evaluate contestant forecasts. The official contest target temperature variable was tmp2m tmax+tmin 2 .\nPrecipitation Daily precipitation (precip) data from 1979 onward were obtained from NOAA's CPC Gauge-Based Analysis of Global Daily Precipitation [42] and converted to mm; the same data source was used to evaluate contestant forecasts. We augmented this dataset with daily U.S. precipitation data in mm from 1948-1979 from the CPC Unified Gauge-Based Analysis of Daily Precipitation over CONUS. Measurements were replaced with sums over the ensuing two-week period.\nSea surface temperature and sea ice concentration NOAA's Optimum Interpolation Sea Surface Temperature (SST) dataset provides SST and sea ice concentration data, daily from 1981 to the present [32] . After interpolation, we extracted the top three principal components (PCs), (sst_i) 3 i=1 and (icec_i) 3 i=1 , across grid points in the Pacific basin region (20S to 65N, 150E to 90W) based on PC loadings from 1981-2010.\nMultivariate ENSO index (MEI) Bimonthly MEI values (mei) from 1949 to the present, were obtained from NOAA/Earth System Research Laboratory [40, 41] . The MEI is a scalar summary of six variables (sea-level pressure, zonal and meridional surface wind components, SST, surface air temperature, and sky cloudiness) associated with El Ni\u00f1o/Southern Oscillation (ENSO), an oceanatmosphere coupled climate mode.\nMadden-Julian oscillation (MJO) Daily MJO values since 1974 are provided by the Australian Government Bureau of Meteorology [38] . MJO is a metric of tropical convection on daily to weekly timescales and can have significant impact on the western United States' subseasonal climate. We extract measurements of phase and amplitude on the target date but do not aggregate over the two-week period.\nRelative humidity and pressure NOAA's National Center for Environmental Prediction (NCEP)/National Center for Atmospheric Research Reanalysis dataset [17] contains daily relative humidity (rhum) near the surface (sigma level 0.995) from 1948 to the present and daily pressure at the surface (pres) from 1979 to the present.\nGeopotential height To capture polar vortex variability, we obtained daily mean geopotential height at 10mb since 1948 from the NCEP Reanalysis dataset [17] and extracted the top three PCs (wind_hgt_10_i) 3 i=1 based on PC loadings from 1948-2010. No interpolation or contest grid restriction was performed. NMME The North American Multi-Model Ensemble (NMME) is a collection of physics-based forecast models from various modeling centers in North America [20] . Forecasts issued monthly from the Cansips, CanCM3, CanCM4, CCSM3, CCSM4, GFDL-CM2.1-aer04, GFDL-CM2.5 FLOR-A06 and FLOR-B01, NASA-GMAO-062012, and NCEP-CFSv2 models were downloaded from the IRI/LDEO Climate Data Library. Each forecast contains monthly mean predictions from 0.5 to 8.5 months ahead. We derived forecasts by taking a weighted average of the monthly predictions with weights proportional to the number of target period days that fell into each month. We then formed an equally-weighted average (nmme_wo_ccsm3_nasa) of all models save CCSM3 and NASA (which were not reliably updated during the contest). Another feature was created by averaging the most recent monthly forecast of each model save CCSM3 and NASA (nmme0_wo_ccsm3_nasa)."}, {"section_title": "FORECASTING MODELS", "text": "In developing our forecasting models, we focused our attention on computationally efficient methods that exploited the multitask, i.e., multiple grid point, nature of our problem and incorporated the unusual forecasting skill objective function (1) . For each target variable (temperature or precipitation) and horizon (weeks 3-4 or 5-6), our forecasting system relies on two regression models trained using two sets of features derived from the SubseasonalRodeo dataset. The first model, described in Section 4.1, introduces lagged measurements from all data sources in the SubseasonalRodeo dataset as candidate regressors. For each target date, irrelevant regressors are pruned automatically using multitask feature selection tailored to the cosine similarity objective. Our second model, described in Section 4.2, chooses features derived from the target variable (temperature or precipitation) using a skill-specific nearest neighbor strategy. The final forecast is obtained by ensembling the predictions of these two models in a manner well-suited to the cosine similarity objective."}, {"section_title": "Local Linear Regression with Multitask", "text": "Feature Selection (MultiLLR)\nOur first model uses lagged measurements from each of the data sources in the SubseasonalRodeo dataset as candidate regressors, with lags selected based on the temporal resolution of the measurement and the frequency of the data source update. The y-axis of Fig. 2 provides an explicit list of candidate regressors for each prediction task. The suffix anom indicates that feature values are anomalies instead of raw measurements, the substring shift\u2113 indicates a lagged feature with measurements from \u2113 days prior, and the constant feature ones equals 1 for all datapoints. We combine predictors using local linear regression with locality determined by the day of the year 1 (Algorithm 1). Specifically, the training data for a given target date is restricted to a 56-day (8-week) span around the target date's day of the year (s = 56). For example, if the target date is May 2, 2017, the training data consists of days within 56 days of May 2 in any year. We employ equal datapoint weighting (w t, = 1) and no offsets (b t, = 0).\nAs we do not expect all features to be relevant at all times of year, we use multitask feature selection tailored to the cosine objective to automatically identify relevant features for each target date. The selection is multitask in that variables for a target date are selected jointly for all grid points, while the coefficients associated with those variables are fit independently for each grid point using local linear regression.\nThe feature selection is performed for each target date using a customized backward stepwise procedure (Algorithm 2) built atop \nthe local linear regression subroutine. At each step of the backward stepwise procedure, we regress the outcome on all remaining candidate predictors; the regression is fit separately for each grid point. A measure of predictive performance (described in the next paragraph) is computed, and the candidate predictor that decreases predictive performance the least is removed. The procedure terminates when no candidate predictor can be removed from the model without decreasing predictive performance by more than the tolerance threshold tol = 0.01.\nOur measure of predictive performance is the average leave-oneyear-out cross-validated (LOYOCV) skill on the target date's day-ofyear, where the average is taken across all years in the training data. The LOYOCV skill for a target date t is the cosine similarity achieved by holding out a year's worth of data around t, fitting the model on the remaining data, and predicting the outcome for t. When forecasting weeks 3-4, we hold out the data from 29 days before t through 335 days after t; for weeks 5-6, we hold out the data from 43 days before through 321 days after t. This ensures that the model is not fitted on future dates too close to t. For n training dates, Y training years, and d features, the MultiLLR running time is O(nd 2 + Yd 3 ) per grid point and step. In our experiments in Section 5, we run the per grid point regressions in parallel on each step, d ranges from 20 to 23, and the average number of steps is 13."}, {"section_title": "Multitask k-Nearest Neighbor Autoregression (AutoKNN)", "text": "Our second model is a weighted local linear regression (Algorithm 1) with features derived exclusively from historical measurements of the target variable (temperature or precipitation). When predicting \nweeks 3-4, we include lagged temperature or precipitation anomalies from 29 days, 58 days, and 1 year prior to the target date; when predicting weeks 5-6, we use 43 days, 86 days, and 1 year. These lags are chosen because the most recent data available to us are from 29 days before the target date when predicting weeks 3-4 and 58 days before the target date when predicting weeks 5-6.\nIn addition to fixed lags, we include the constant intercept ones and the observed anomaly patterns of the target variable on similar dates in the past (Algorithm 3). Our measure of similarity is tailored to the cosine similarity objective: similarity between a target date and another date is measured as the mean skill observed when the historical anomalies preceding the candidate date are used to forecast the historical anomalies of the target date. The mean skill is computed over a history of H = 60 days, starting 1 year prior to the target date (lag \u2113 = 365). Only dates with observations fully observed prior to the forecast issue date are considered viable. We find the 20 viable candidate dates with the highest similarity to the target date and scale each neighbor date's observed anomaly vector so that it has a standard deviation equal to 1. The resulting features are knn1 (the most similar neighbor) through knn20 (the 20th most similar neighbor). We find the k = 20 top neighbors for each of n training dates in parallel, using O(knHG) time per date.\nTo predict a given target date, we regress onto the three fixed lags, the constant intercept feature ones, and either knn1 through knn20 (for temperature) or knn1 only (for precipitation), treating each grid point as a separate prediction task. We found that including knn2 through knn20 did not lead to improved performance for predicting precipitation. For each grid point, we fit a weighted local linear regression, with weights w t, given by 1 over the variance of the target anomaly vector. As with MultiLLR, locality is determined by the day of the year. For predicting precipitation, we restrict the training data to a 56-day span s around the target date's day of the year. For predicting temperature, we use all dates. In each case, we use a climatology offset (b t, = c monthday(t ), ) so that the effective target variable is the measurement anomaly rather than the raw measurement. Given d features and n training dates, the final regression is carried out in O(nd 2 ) time per grid point. In our experiments in Section 5, per grid point regressions were performed in parallel, and d = 24 for temperature and d = 5 for precipitation."}, {"section_title": "Ensembling", "text": "Our final forecasting model is obtained by ensembling the predictions of the MultiLLR and AutoKNN models. Specifically, for a given target date, we take as our ensemble forecast anomalies the average of the \u2113 2 -normalized predicted anomalies of the two models: The \u2113 2 normalization is motivated by the following result, which implies that the skill of\u00e2 ensemble is strictly better than the average skill of\u00e2 multillr and\u00e2 autoknn whenever that average skill is positive.\nConsider an observed anomaly vector a and m distinct forecast anomaly vectors (\u00e2 i ) m i=1 . For any vector of weights p \u2208 R m with m i=1 p i = 1 and p i \u2265 0, let\nbe the weighted average of the \u2113 2 -normalized forecast anomalies. Then, sign( m i=1 p i cos(\u00e2 i , a)) = sign(cos(\u0101 (p) , a)) and\nHence, whenever the weighted average of individual anomaly skills is positive, the skill of\u0101 (p) is strictly greater than the weighted average of the individual skills."}, {"section_title": "Proof The sign claim follows from the equalities", "text": "Since the forecasts are distinct, Jensen's inequality now yields the magnitude claim as\nwith strict inequality when m i=1 p i cos(\u00e2 i , a) 0."}, {"section_title": "EXPERIMENTS", "text": "In this section we evaluate our model forecasts over the Rodeo contest period and over each year following the climatology period and explore the relevant features inferred by each model. Python 2.7 code to reproduce all experiments can be found at https://github. com/paulo-o/forecast_rodeo."}, {"section_title": "Contest Baselines", "text": "For each target date in the contest period, the Rodeo organizers provided the skills of two baseline models, debiased CFSv2 and damped persistence. To provide baselines for evaluation outside the contest period, we reconstructed a debiased CFSv2 forecast approximating the contest guidelines. We were unable to recreate the damped persistence model, as no exact description was provided. We first reconstructed the undebiased 2011-2018 CFSv2 forecasts using the 6-hourly CFSv2 Operational Forecast dataset and, for each month-day combination, computed long-term CFS reforecast averages over 1999-2010 using the 6-hourly CFS Reforecast High-Priority Subset [34] . For each target two-week period and horizon, we averaged eight forecasts, issued at 6-hourly intervals. For weeks 3-4, the eight forecasts came from 15 and 16 days prior to the target date; for weeks 5-6, we used 29 and 30 days prior. For each date t, we then reconstructed the debiased CFSv2 forecast by subtracting the long-term CFS average and adding the observed target variable average over 1999-2010 for monthday(t) to the reconstructed CFSv2 forecast. Our reconstructed debiased forecasts are available for download at [15] , and Appendix B provides more details on data sources and processing.\nWhile the official contest CFSv2 baseline averages the forecasts of four model initializations, the CFSv2 Operational Forecast dataset only provides the forecasts of one model initialization (the remaining model initialization forecasts are released in real time but deleted after one week). Thus, our reconstruction does not precisely match the contest baseline, but it provides a similarly competitive benchmark."}, {"section_title": "Contest Period Evaluation", "text": "We now examine how our methods perform over the contest period, consisting of forecast issue dates between April 18, 2017, and April 17, 2018. Forecast issue dates occur every two weeks, so we have 26 realized skills for each method and each prediction task. Table 1 shows the average skills for each of our methods and each of the baselines. All three of our methods outperform the official contest baselines (debiased CFSv2 and damped persistence), and our ensemble outperforms the top Rodeo competitor in all four prediction tasks. Note that, while the remaining evaluations are of static modeling strategies, the competitor skills represent the real-time evaluations of forecasting systems that may have evolved over the course of the competition.\nIn Fig. 1 we plot the 26 realized skills for each method. In each plot, the average skill over the contest period is indicated by a vertical line. The histograms indicate that both of the official contest baselines have a number of extreme negative skills, which drag down their average skill over the contest period. Our ensemble avoids these extreme negative skills. For both precipitation tasks, the worst realized skills of the two baseline methods are \u22120.8 or worse; by contrast, the worst realized skill of the ensemble is \u22120.4."}, {"section_title": "Historical Forecast Evaluation", "text": "Next, we evaluate the performance of our methods over each year following the climatology period. That is, following the template of the contest period, we associate with each year in 2011-2017 a sequence of biweekly forecast issue dates between April 18 of that year and April 17 of the following year. For example, forecasts with submission dates between April 18, 2011 and April 17, 2012 are considered to belong to the evaluation year 2011. To mimic the actual real-time use of the forecasting system to produce forecasts for a particular target date, we train our models using only data available prior to the forecast issue date; for example, the forecasts issued on April 18, 2011 are only trained on data available prior to April 18, 2011. We compare our methods to the reconstructed debiased CFSv2 forecast. Table 2 shows the average skills of our methods and the reconstructed debiased CFSv2 forecast (denoted by rec-deb-cfs) in each year, 2011-2017. MultiLLR, AutoKNN, and the ensemble all achieve higher average skill than debiased CFSv2 on every task, save for MultiLLR on the temperature, weeks 3-4 task. The ensemble improves over the debiased CFSv2 average skill by 23% for temperature weeks 3-4, by 39% for temperature weeks 5-6, by 123% for precipitation weeks 3-4, and by 157% for precipitation weeks 5-6.\nApplied Data Science Track Paper KDD '19, August 4-8, 2019, Anchorage, AK, USA Table 2 also presents the average skills achieved by a threecomponent ensemble of MultiLLR, AutoKNN, and reconstructed debiased CFSv2. Guided by Proposition 1, we \u2113 2 -normalize the anomalies of each model before taking an equal-weighted average. This ensemble (denoted by ens-cfs) produces higher average skills than the original ensemble in all prediction tasks. The ens-cfs ensemble also substantially outperforms debiased CFSv2, with skill improvements of 40% and 50% for the temperature tasks and 129% and 169% for the precipitation tasks. These results highlight the valuable roles that ML-based models, physics-based models, and principled ensembling can all play in subseasonal forecasting.\nContribution of NMME. Interestingly, the skill improvements of AutoKNN were achieved without any use of physics-based model forecasts. Moreover, a Proposition 1 ensemble of just AutoKNN and rec-deb-cfsv2 realizes most of the gains of ens-cfs without using NMME. Indeed, this ensemble has mean skills over all years in Table 2 of (temp. weeks 3-4: 0.354, temp. weeks 5-6: 0.31, precip. weeks 3-4: 0.162, precip. weeks 5-6: 0.147).\nWhile physics-based model forecasts contribute to MultiLLR through the NMME ensemble mean, nmme_wo_ccsm3_nasa alone achieves inferior mean skill (temp. weeks 3-4: 0.094, temp. weeks 5-6: 0.116, precip. weeks 3-4: 0.116, precip. weeks 5-6: 0.107) over all years in Table 2 than all proposed methods and even the temperature debiased CFSv2 baseline. One contributing factor to this performance is the mismatch between the monthly granularity of the publicly-available NMME forecasts and the biweekly granularity of our forecast periods. As a result, we anticipate that more granular NMME data would lead to significant improvements in the final MultiLLR model. Fig. 2 shows the frequency with which each candidate feature was selected by MultiLLR in the four prediction tasks, across all target dates in the historical evaluation period. For all four tasks, the most frequently selected features include pressure (pres), the intercept term (ones), and temperature (tmp2m). The NMME ensemble average (nmme_wo_ccsm3_nasa) is the first or second most commonly selected feature for predicting precipitation, but its relative selection frequency is much lower for temperature."}, {"section_title": "Exploring MultiLLR", "text": "Although we used a slightly larger set of candidate features for the precipitation tasks-23 for precipitation, compared to 20 for temperature-the selected models are more parsimonious for precipitation than for temperature. The median number of selected features for predicting temperature is 7 for both forecasting horizons, while the median number of selected features for predicting precipitation is 4 for weeks 3-4 and 5 for weeks 5-6. Fig. 3a plots the month distribution of the top nearest neighbor learned by AutoKNN for predicting precipitation, weeks 3-4, as a function of the month of the target date. The figure shows that when predicting precipitation, the top neighbor for a target date is generally from the same time of year as the target date: for summer target dates, the top neighbor tends to be from a summer month and similarly for winter target dates. The corresponding plot for temperature (omitted due to space constraints) shows that this pattern does not hold when predicting temperature; rather, the top neighbors are drawn from throughout the year, regardless of the month of the target date."}, {"section_title": "Exploring AutoKNN", "text": "The matrix plots in Fig. 3b show the year and month of the top 20 nearest neighbors for predicting temperature, weeks 3-4, as a function of the target date. In each plot, the vertical axis ranges from k = 1 (most similar neighbor) to k = 20 (20th most similar neighbor). The vertical striations in both plots indicate that the top 20 neighbors for a given target date tend to be homogeneous in terms of both month and year: neighbors tend to come from the same or adjacent years and times of year. Moreover. the neighbors for post-2015 target dates tend to be from post-2010 years, in keeping with recent years' record high temperatures. The corresponding plots for precipitation (omitted due to space constraints) show that the top neighbors for precipitation do not disproportionately come from recent years, and the months of the top neighbors follow a regular seasonal pattern, consistent with Fig. 3a ."}, {"section_title": "DISCUSSION", "text": "To meet the USBR's Subseasonal Climate Forecast Rodeo challenge, we developed an ML-based forecasting system and demonstrated 40-169% improvements in forecasting skill across the challenge period (2017-18) and the years 2011-18 more generally. Notably, the same procedures provide these improvements for each of the four Rodeo prediction tasks (forecasting temperature or precipitation at weeks 3-4 or weeks 5-6). In the short term, we anticipate that these improvements will benefit disaster management (e.g., anticipating droughts, floods, and other wet weather extremes) and the water management, development, and protection operations of the USBR more generally (e.g., providing irrigation water to 20% of western U.S. farmers and generating hydroelectricity for 3.5 million homes). In the longer term, we hope that these tools will improve our ability to anticipate and manage wildfires [39] .\nOur experience also suggests that subseasonal forecasting is fertile ground for machine learning development. Much of the methodological novelty in our approach was driven by the unusual multitask forecasting skill objective. This objective inspired our new and provably beneficial ensembling approach and our custom multitask neighbor selection strategy. We hope that introducing this problem to the ML community and providing a user-friendly benchmark dataset will stimulate the development and evaluation of additional subseasonal forecasting approaches."}, {"section_title": "A SUPPLEMENTARY SUBSEASONALRODEO DATASET DETAILS", "text": "The SubseasonalRodeo dataset is organized as a collection of Python Pandas DataFrames and Series objects [26] stored in HDF5 format (via pandas.DataFrame.to_hdf or pandas.Series.to_hdf), with one .h5 file per DataFrame or Series. The contents of any file can be loaded in Python using pandas.read_hdf. Each DataFrame or Series contributes data variables (features or target values) falling into one of three categories: (i) spatial (varying with the target grid point but not the target date); (ii) temporal (varying with the target date but not the target grid point); (iii) spatiotemporal (varying with both the target grid point and the target date). Unless otherwise noted in Section 3 or below, temporal and spatiotemporal variables arising from daily data sources were derived by averaging input values over each 14-day period, and spatial and spatiotemporal variables were derived by interpolating input data to a 1 \u2022 \u00d7 1 \u2022 grid using the Climate Data Operators (CDO version 1.8.2) operator remapdis (distance-weighted average interpolation) with target grid r360x181 and retaining only the contest grid points. In addition to the variables described in Section 3, a number of auxiliary variables were downloaded and processed but not ultimately used in our approach."}, {"section_title": "A.1 Temperature and Precipitation Interpolation", "text": "The downloaded temperature variables tmin and tmax, global precipitation variable rain, and U.S. precipitation variable precip were each interpolated to a fixed 1 \u2022 \u00d7 1 \u2022 grid using the NCAR Command Language (NCL version 6.0.0) function area_hi2lores_Wrap with arguments new_lat = latGlobeF(181, lat , latitude , degrees_north ); new_lon = lonGlobeF(360, lon , longitude , degrees_east ); wgt = cos(lat*pi/180.0) (so that points are weighted by the cosine of the latitude in radians); opt@critpc = 50 (to require only 50% of the values to be present to interpolate); and fiCyclic = True (indicating global data with longitude values that do not quite wrap around the globe). rain was then renamed to precip."}, {"section_title": "A.2 Data Sources", "text": "The SubseasonalRodeo dataset data were downloaded from the following sources.\n\u2022 "}, {"section_title": "A.3 Dataset Files", "text": "Below, we list the contents of each SubseasonalRodeo dataset file. Each file with the designation 'Series' contains a Pandas Series object with a MultiIndex for the target latitude (lat), longitude (lon), and date defining the start of the target two-week period (start_date). Each file with the designation 'MultiIndex DataFrame' contains a Pandas DataFrame object with a MultiIndex for lat, lon, and start_date. Each file with a filename beginning with 'nmme' contains a Pandas DataFrame object with target_start, lat, and lon columns; the target_start column plays the same role as start_date in other files, indicating the date defining the start of the target two-week period. Each remaining file with the designation 'DataFrame' contains a Pandas DataFrame object with lat and lon columns if the contained variables are spatial; a start_date column if the contained variables are temporal; and start_date, lat, and lon columns if the contained variables are spatiotemporal.\nThe filename prefix 'gt-wide' indicates that a file contains temporal variables representing a base variable's measurement at multiple locations on a latitude-longitude grid that need not correspond to contest grid point locations. The temporal variable column names are tuples in the format ('base variable name', latitude, longitude). The base variable measurements underlying the files with the filename prefix 'gt-wide_contest' were first interpolated to a 1 \u2022 \u00d7 1 \u2022 grid. The measurements underlying the remaining 'gt-wide' files did not undergo interpolation; the original data source grids were instead employed. -Spatiotemporal variables temperature at 2m (tmp2m), average squared temperature at 2m over two-week period (tmp2m_sqd), and standard deviation of temperature at 2m over two-week period (tmp2m_std)\n\u2022 gt-contest_wind_hgt_100-14d-1948-2018.h5 (Series)\n-Spatiotemporal variable geopotential height at 100 millibars (contest_wind_hgt_100)\n\u2022 gt-contest_wind_hgt_10-14d-1948-2018.h5 (Series)\n-Spatiotemporal variable geopotential height at 10 millibars (contest_wind_hgt_10)\n\u2022 gt-contest_wind_hgt_500-14d-1948-2018.h5 (Series)\n-Spatiotemporal variable geopotential height at 500 millibars (contest_wind_hgt_500)"}]