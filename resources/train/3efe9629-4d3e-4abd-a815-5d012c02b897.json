[{"section_title": "LIST OF FIGURES", "text": "Page Fig. 1-1 "}, {"section_title": "INTRODUCTION1", "text": ""}, {"section_title": "Motivation", "text": "Storm surge is an abnormal rise of water generated by a storm, over and above the predicted astronomical tide. In coastal areas, storm surge causes the greatest concentration of death and destruction during a hurricane, more than even the powerful winds and the tremendous amounts of rainfall. Storm surges can be caused either by tropical storms or extra-tropical storms. The focus of this study is the storm surge caused by tropical storms or hurricanes. Since hurricanes are intense form of tropical storms they are discussed in this study. For a hurricane, the surge typically has duration of several hours and affects about 100 miles of coastline from its landfall location. In the great Galveston, Texas, hurricane of 1900, an estimated 6,000 people drowned when the island was almost completely submerged by the storm surge. Hurricane storm surges of over 20 feet have been observed; hurricane Camille in 1969 produced a surge of approximately 7.4 m (24 feet) in the area of Gulfport, Mississippi. The destruction caused by such abnormally high water is truly astounding. In the aftermath of other historic hurricane storm surges, areas of coast have been abandoned completely, as in the case of Indianola, Texas, which was deserted after a storm in 1875. More recently in July of 2003, hurricane Claudette caused severe damage in the state of Texas. The damage was estimated to be about $5 million comprising of damage to public as well as private properties and beach erosion. This damage resulted even though Claudette was not a strong Hurricane, being Category 1 on Saffir-Simpson The thesis follows the style and format of Journal of Waterway, Port, Coastal and Ocean Engineering. scale that is the lowest category of hurricane. A hurricane is a severe tropical storm that forms in the southern Atlantic Ocean, Caribbean Sea, and Gulf of Mexico or in the eastern Pacific Ocean. Hurricanes are also known as typhoons in some regions. Hurricanes need warm tropical oceans, moisture and light winds above them. If the right conditions last long enough, a hurricane can produce violent winds, incredible waves, torrential rains and floods. Hurricanes rotate in a counterclockwise direction around an \"eye\". Storm surges are caused by atmospheric pressure gradients and shear stresses acting on the surface of a body of water. Local water levels are affected day to day by even weak atmospheric disturbances that occur at a great distance, but the greatest impact certainly is from well-developed tropical storms and hurricanes that pass within the intermediate area. There are on average six Atlantic hurricanes each year; over a 3-year period, approximately five hurricanes strike the United States coastline from Texas to Maine (Ho 1987). Though strong winds from hurricanes and tropical storms often have the greatest influence on the level of the storm surge along a coastline, there are sometimes other factors, which contribute significantly to the total storm surge level. The total water level change experienced during a hurricane depends upon the combination of a number of complex influences. These influences include: 1) the storm surge, 2) astronomical tides, which are the normal cause of day to day water level change at the coast, 3) surface wave set-up, and 4) rainfall flooding. Storm surge is the combined effect on the water surface elevation by the reduced pressure, wind shear and wave runup. In the case of wave setup, in some locations as much as 50% of the total surge level can be the result of wind wave set-up (Jelesnianski and Taylor 1973). Two general approaches have been used to forecast hurricane storm surges: statistical modeling and numerical modeling. In statistical modeling, past observations of storm surge heights are correlated statistically to observed or forecasted hurricane characteristics. However, since hurricanes are relatively uncommon and are small scale in nature (compared to meteorological phenomena), insufficient data exist to allow such statistical correlations to be derived to the extent desired. Numerical modeling offers a viable alternative to statistical modeling for hurricane storm surge problem. In computer modeling of storm surge, a set of differential equations governing the flow of water (transport equations) are solved with relevant and pertinent boundary conditions to obtain storm surge. This approach, though effective fails to quantify storm surge value relative to the hurricane characteristics. Hence, a better approach as used in this study would be to utilize both of the above approaches. Knowing that hurricanes in the Atlantic Ocean can be assigned a temporal cycle, a frequency relationship can be performed. For these estimates to be useful, an accurate database needs to be populated with hurricane storm surge levels and their inherent characteristics. This study aims to provide an effective approach for estimating storm surge effects on an area by utilizing both numerical and statistical approaches for storm surge estimating."}, {"section_title": "Approach", "text": "Numerical modeling using the long wave model ADCIRC (Advanced Circulation model; Luettich, Westerlink, and Scheffner, 1992) is used to model historical hurricanes that have affected the area and some of the extreme storms are perturbed to achieve the maximum impact in the area. The numerical simulations help in the generation of a database of storm characteristics like storm surge values, maximum winds, radius of maximum winds, eye pressure etc. This database of storm characteristics is used in the statistical model EST (Empirical Simulation Technique, Scheffner et al., 1999). This procedure uses historic events to generate a large population of life-cycle databases that are post-processed to compute mean value maximum storm surge elevation frequency relationships with statistical error estimates. This study also tries to compare the results with NWS numerical model SLOSH (Jelesnianski et al, 1992). The model SLOSH has been used extensively to delineate coastal areas susceptible to hurricane storm surge flooding."}, {"section_title": "Study Area", "text": "The region of general interest within the Gulf of Mexico for this application consists of the Freeport area as shown in the Fig. 1-1. Freeport is an important industrial center and deepwater port located on the Texas coast. The community has a diversified source of income, but is predominantly dependent on the petro-chemical industry. The principal sources of income are derived from processing petroleum and petroleum byproducts. Brazoria County houses one of the world's largest chemical complexes with the Dow Chemical being the principal employer. Since this area is exposed to storm surges resulting from tropical and extra tropical events, a levee system was constructed at Freeport, TX, in 1982. The levee was constructed for providing flood damage protection to the area and has an elevation of 6.5-7m above sea level. The levee system consists of a series of levees and pumping stations that protect an area of about 68 square kilometers. The project was completed in 1982. The levee system is vital to protection against flooding of the nation's most vital petro-chemical industry worth almost $500 billion."}, {"section_title": "Fig. 1-1: Study area", "text": ""}, {"section_title": "Procedure", "text": "This study first required the development of a computational grid for the study area. The ADCIRC model then used the computational grid to simulate tidal circulation and storm events. The model grid was verified by comparing model-generated tide time series with the corresponding time series reconstructed from existing harmonic analyses and on-site measurements of surface elevation. Storm event simulations were verified by comparing simulated results of water surface elevation with archived storm measurements. Once the model was shown to be capable of reproducing historic events, all storms that significantly impacted the study area since 1886 were simulated. The beginning year 1886 is used as that is the first year in the available database maintained by the National Weather Service. In order to insure that the most severe events were included for all along the coastline, simulations included hypothetical events that could likely occur. Following the numerical simulations for all the selected storms, the database of computed surges and tides was used as input for the statistical procedure EST. Frequency computations are made at 35 locations in the Freeport area. These stations are located at points of interest within the domain and help in establishing the extreme event storm surges along the levee system that was constructed for providing flood damage protection to the area."}, {"section_title": "Computational Grid", "text": "The modeling strategy has been to define the entire Gulf of Mexico as the computational domain and to refine the region of interest using the significant grid flexibility offered by the finite elements and the ADCIRC codes. Using the entire Gulf as the pertinent domain is quite convenient from a variety of perspectives. Most important, two well-defined open ocean boundaries of limited extent can be used to specify the boundary forcing functions that define the interaction between the Atlantic Ocean and the Caribbean Sea with the Gulf. The procedure for generation of the finite element grid required the following steps: 1) Obtaining coastline to serve as boundary for our domain. 2) Generation of model bathymetry. 3) Applying boundary conditions."}, {"section_title": "4)", "text": "Using grid generation software (SMS: Surface Water Modeling System) to generate finite element grid. A problem often encountered in the modeling of near-shore regions in the Gulf of Mexico is that the areas of interest are not well removed from the computational boundaries. The Gulf of Mexico being a semi-enclosed basin has numerous amphidromes that affect both the amplitude and phase of astronomical tide and storm surge. Circulation elements and is shown in Fig. 1-2. Minimum node-to-node spacing in the study is approximately 50 m. The bathymetry for the computational domain is shown in Fig. 1-3. The increased resolution of the study area is shown in Fig. 1-4. Bathymetry in the study area is shown in Fig. 1-5. This large domain approach to specification of boundary conditions virtually eliminates contamination of model results from poorly defined boundary conditions. "}, {"section_title": "Outline", "text": "The thesis consists of 7 sections. Section 1 consists of motivation, approach, study area, general procedure used in the thesis and outline. ADCIRC model and its components, which are related to this thesis, constitute section 2. SLOSH model, its history and methodology are described in section 3. Section 4 gives some insight into the EST method. The implementation of ADCIRC and EST to the study area is further described in section 5. The comparisons of ADCIRC/EST results with SLOSH model are given in section 6. Section 7 consists of conclusions."}, {"section_title": "ADCIRC: MODEL DESCRIPTION", "text": "This section is divided into four parts. First part sheds light on ADCIRC model. The other three parts discuss components of the model relevant to this thesis like tidal forcing, bottom friction and wind forcing."}, {"section_title": "ADCIRC Model", "text": "Water-surface elevations and currents for both tides and storm events are obtained from the large-domain long wave hydrodynamic model ADCIRC (Advanced Circulation model; Luettich et al, 1992). ADCIRC is a finite element (FEM) code that makes use of the Generalized Wave Continuity Equation GWCE The 2-dimensional, Depth Integrated (2DDI) model formulation begins with the depth-averaged shallow-water equations for conservation of mass and momentum subject to incompressibility and hydrostatic pressure approximations. The Boussinesq approximation, where density is considered constant in all terms but the gravity term of the momentum equation, is also incorporated in the model. Using the standard quadratic parameterization for bottom stress and omitting baroclinic terms and lateral diffusion and dispersion, the following set of conservation statements in primitive, non-conservative form and expressed in a spherical coordinate system are incorporated in the model (Flather 1988;Kolar et al. 1994): In order to overcome general stability problems encountered when finite element models depend upon the direct solution of these primitive forms of the governing equations, the ADCIRC code was developed around the Generalized Wave Continuity Equation (GWCE). Combining a time-differentiated form of the momentum equations yields this form of the primitive equations. With the inclusion of a simple eddy viscosity model for closure (Kolar and Gray 1994), the GWCE in Spherical coordinates takes the form:  The ADCIRC-2DDI model solves the GWCE (Equation (4)) in conjunction with the primitive momentum equations given in Equations (2) and (3). The equations are solved using a FEM grid, made up of linear triangular elements (only three nodes per element). The model domain can be as extensive as an entire ocean basin, or more localized, as in the case of a small bay or estuary. The numerical solution of the governing equations presented above follows a two-step procedure in ADCIRC code. First, the GWCE (Equation (4)) is solved. The linear terms in the GWCE are discretized using a Galerkin weighted residual, three time level, and implicit scheme. The non-linear terms, along with Coriolis forcing, atmospheric forcing and tidal potential are solved explicitly (Westerink et al. 1993). The explicit formulation of these terms has the advantage that the solution depends only upon the previous time step. On the other hand, the implicit terms depend upon the solution of a system of equations, arranged in a banded matrix. The second step in the solution of the governing equations, after solving the GWCE, is to solve the momentum equations (Eq. 2 Those used in the model are described in section 5.4 ADCIRC can be forced with: elevation boundary conditions normal flow boundary conditions surface stress boundary conditions earth load/self attraction tide A feature of ADCIRC that makes its application particularly useful in storm surge modeling of bays along the Texas Gulf coast is the capability of wetting and drying in the computational cells. Most of the coastal basins that make up the estuaries along the Texas Gulf coast are very shallow, with depths that are often no more than a meter. In addition to the shallow bay depths, the topography of coastal lands is that of flat coastal plains, with very gentle slopes. The barrier islands in Freeport are just a few meters above mean water level. During extreme meteorological events like hurricanes, it is possible that shallow areas may become dry from \"blow down\" (due to water being driven from the area by storm winds). On the other hand, the surge during a storm can cause extensive inland coastal flooding as is evident historically in all large storms affecting the Freeport area over the time period considered in this study. An element based technique for wetting/drying was developed for implementation in ADCIRC. Conceptually, the algorithm assumes removable barriers exist along the sides of all triangular elements of the grid. Nodes of the elements are designated as \"dry\" nodes, \"interface\" nodes, and \"wet\" nodes. All elements connected to a dry node are assumed to have barriers in place such that there is no flow through the element, i.e. a dry element. An element connected to all wet nodes is a wet element and is included in the full flow domain. Interface nodes connect wet and dry elements. Boundaries connecting interface nodes are considered as standard land boundary nodes at which the water level rises and falls against the element barrier."}, {"section_title": "Tidal Propagation", "text": "Tidal potential forcing, which causes the normal observed periodic water level changes in large bodies of water, is included in ADCIRC. Other popular large-scale hydrodynamic models, like SLOSH and RMA2, do not include tidal potential forcing. ADCIRC determines the magnitude of the tidal potential \u03b7 in equation 4at each grid node and each model time step by the relationship: where:  The values of f ( jn B ) and \u03bd for the constituents used for the tidal potential computations are determined for the specific time that a model run begins using Le Provost database (Westerink, J.J. et al, 1993). LeProvost database (Le Provost et al, 1994) is an atlas of the main components of the tides and has been produced on the basis of a finite element hydrodynamic model, with the aim of offering the scientific community, using satellite altimetric data, a prediction of the tidal contribution to sea surface height variations under the ground tracks of the satellite that is totally independent of altimetric variation. Eight constituents, M 2 , S 2 , N 2 , K 2 , 2N 2 , K 1 , O 1 , and Q 1 have been simulated. Five secondary constituents: Mu 2 , Nu 2 , L 2 , T 2 , and P 1 , required to insure a priori correct predictions, have been deduced by admittance. The admittance is assumed to be a slowly varying function of frequency so that the admittance of the major constituents can be used for determining the response at nearby frequencies for the secondary constituents. The accuracy and precision of these solutions have been estimated by reference to the harmonic constituents' data set available from analysis of the entire collection of the pelagic, plateau and coastal observations made to date, and archived. Over the deep oceans these solutions fit the observations to within a few centimeters for the larger components: M 2 , S 2 , K 1 , O 1 , and a few millimeters for the others. Over the continental shelves the differences are larger, because of the increase in the magnitude of the tidal waves, but the flexibility offered by the finite element technique to refine the discretization mesh of the model over the shallow seas enables detailed cotidal maps to be produced along the coasts. Note that tidal potential was not used during the simulation of tropical storms. Tides were combined after the simulations during the frequency analysis."}, {"section_title": "Bottom Stress", "text": "Bottom stress in the 2DDI version of ADCIRC is generally expressed as: Depending on the form used for \u03c4* the result is either a linear, quadratic or hybrid function of depth-averaged velocity. For most coastal applications, quadratic friction should be used with a drag coefficient, C f \u223c 0.0025. In very shallow water, hybrid friction may be useful with C fmin \u223c 0.0025, particularly when wetting and drying is included since this expression becomes highly dissipative as the water depth becomes small. Linear friction is primarily useful for model testing or when a totally linear model run is desired. In this case the magnitude of \u03c4* should be consistent (at least in order of magnitude) with a value that would have been computed using the quadratic friction expression and not with the value of C f that would normally be used in the quadratic expression. The description of formulation based on the form of \u03c4* is given here. Linear friction: \u03c4* = C f where C f = constant in time (may vary with space), read in model as input, unit s -1"}, {"section_title": "Quadratic friction:", "text": "( ) where C f = constant in time (may vary with space), read in model as input H = water depth Hybrid friction:  and \u03b3 = 1/3, where g is the gravitational constant and n is the Manning Coefficient, the hybrid friction will have a Manning equation frictional behavior for H < H break."}, {"section_title": "Wind Forcing", "text": "In addition to the capability for tidal forcing within ADCIRC, there are provisions to input atmospheric and wind forcing information into the simulations. Several formats for the wind data are supported, including a fleet numeric and National Weather Service (NWS) wind file format. For this study, the Planetary Boundary Layer (PBL) model (Cardone et al. 1992) supplies the atmospheric forcing information. This model was developed to simulate hurricane generated wind fields using basic characteristics about a particular storm that can be easily retrieved from sources such as NWS archives of past hurricanes. The input data to the model consists of location of the eye of the hurricane (latitude and longitude) in degrees, wind speed and pressure measured at the eye for 6hour intervals. Such data are available for all the storms. A sample input data for Hurricane Claudette (2003) is shown in Table 2-1. This distance is not dependent on storm motion, and for any given time it is assumed to be the same in all directions. This parameter controls the horizontal extent of the surge on the coast. If only the value of the peak surge on the coast is desired then the accuracy of this parameter becomes unimportant, and for most purposes a rough estimate of this distance is sufficient. (3) Central pressure of the storm: The pressure difference from the center to the periphery of the storm. For an actual storm, this could be the mean of several differences measured along rays from the storm enter to the first anticyclonically turning isobar. This is the most important storm parameter; it controls the peak surge on the coast. For constant pressure drop, the peak surge on the coast is only weakly dependent on the radius of maximum wind. The pressure drop is not used directly in the model computations; instead it is used as an argument to arrive at a more convenient measure for computations, the stationary-storm-maximum-wind. 4 estimate of the geostrophic wind speed and direction. A two-step process is used to generate wind fields for use by ADCIRC from the storm data. First, a program for the PBL model is used to determine the track of the storm as one our 'snapshots'. These snapshot data include the radius of maximum wind, which is approximated using a nomograph that incorporates the maximum wind speed and atmospheric pressure anomaly (Jelesnianski and Taylor, 1973), which is shown in Fig. 2-1. In the second step, the PBL model computes the wind field and pressure field of the hurricane over the relevant areas of the Gulf of Mexico. The PBL model solves for the surface wind stress and barometric pressure distribution. Wind speeds are computed in the model and then converted to surface wind stress. The PBL model solves the equations of horizontal motion that have been averaged through the depth of the atmospheric boundary layer, following the work of Chow (1971) . Written in general coordinate system fixed to the earth these equations can be expressed as: It is assumed that the vertical advection of momentum is small compared to the horizontal advection and can be neglected and that shearing stress vanishes at the top of the PBL. In addition to the storm winds, the PBL model generates a pressure field. The pressure field is axisymmetric and is defined by exponential law which expresses P c , the pressure at a particular location in the storm, as: where, P eye = the central low pressure at the eye of the storm, p \u2206 = P-P eye , where P is the normal background pressure (\u223c 1013 mb), R p = scale radius, equivalent to the radius of maximum winds, r = radial distance from the eye. These equations are solved using a finite difference formulation, which utilizes a nested rectangular mesh for computations. The computational grid is a rectangular nested grid system consisting of five nests. An example of this type of mesh is shown in Fig.   2-2, where a single quadrant of the complete numerical grid is shown. The grid moves with the storm, therefore the eye of the hurricane is always centered about the point indicated as origin."}, {"section_title": "Fig. 2-2: Nested grid system used for hurricane wind computation", "text": "The PBL model produces a consistent description of the vertically integrated wind, the surface drag and the wind speed and direction at anemometer height in a moving hurricane with asymmetric horizontal wind distribution over water, with the strongest winds occurring at the right-hand side of the storm, when facing the direction of travel of storm. The formulation of the model includes momentum, heat, and moisture flux. Equilibrium PBL theory is used to extend the surface wind description to terrain of specified roughness. The final surface wind stress output of the PBL model is determined from the computed wind speeds using the relationships: where, \u03c4 \u03c6 , \u03c4 \u03bb = the surface stresses applied in the \u03c6 and \u03bb directions \u03c1 air /\u03c1 o = density ration of air and seawater (\u223c 0.001293) V = absolute magnitude of the wind velocity A plot of the wind shear stress in a well-developed hurricane is shown in Fig. 2-3, where it is shown that the greatest winds in this case occur north-east of the eye of this north-westerly moving storm. The wind model is incorporated in a computer program to provide a gridded temporal and spatial history of the surface wind for use in surge calculation. After completing the computations for the wind and pressure fields at each hourly position of the storm, the solution from the nested rectangular grid is superimposed onto a triangular grid for use by ADCIRC. Therefore, when ADCIRC is run with the hurricane wind fields in the PBL format, it is supplied with the components of the surface stress in the \u03c6 and \u03bb directions and barometric pressure at each node in the FEM grid and at every hour."}, {"section_title": "SLOSH", "text": "A numerical-dynamic, tropical storm surge model, was developed for real-time forecasting of hurricane storm surges on continental shelves, across inland water bodies, along coastlines, and for inland routing of water -either from the sea or from inland water bodies. The most valuable application of SLOSH (Sea, Lake, and Overland Surges from Hurricanes) was in the design of evacuation plans for various communities. SLOSH is a two-dimensional finite difference code, which has been programmed to utilize a variety of curvilinear grid formats, such as polar, elliptical, and hyperbolic (Jelesnianski 1973). Such a model cannot consider inundation across terrain or surges across inland water bodies (Jelesnianski, 1972;Wanstrath and Reid, 1976). An earlier shelf model by Bodine (1971) was even more restricted. His model required computations carried out on only one seaward line from the coast. Also the storm track was restricted to being nearly perpendicular to the coastline. invariant fine mesh to a small region or small basin, the SLOSH model's coordinate system begins as fine mesh in the limited area nearest to the pole point of the grid and stretches continuously to a coarse mesh at distant boundaries of a large basin. The geographical area covered by the entire grid is large and there is detailed description over the fine-mesh region. Moreover, in many cases simple boundary conditions are sufficient (Jelesnianski et al, 1992). "}, {"section_title": "SLOSH Methodology", "text": "The transport equations of motion on a Cartesian frame of reference used are: where: "}, {"section_title": "SLOSH Output", "text": "The final output of the SLOSH model runs gives both local information at selected sites in the grid, and global output for the entire modeled domain. Local, timedependent data are collected from as many as 60 individual stations. These time histories present the surge elevation, wind speed and wind direction every 10 minutes of simulated time. In addition to the model station output, SLOSH outputs global (values for each node in the grid) surge elevations. The global data is output at three-hour intervals up to the closest approach of the storm, and then every two hours, up until nine hours beyond the closest approach. There are other factors also that can have a significant influence on the total water level elevation during a storm. In coastal regions, the action of breaking waves can create a quasi-steady-state, long period \"set-up\" (if not set-down) whereby the original storm surge is altered. This wave action can affect bottom stress in shallow waters. Also, exotic effects occur such as an increase of density from suspended sand particles. Along coastal regions, during passage of a tropical storm and onset of inundation, the totality of windwave effects on surge is now well understood or even well observed. Many theoretical studies of an idealized and piecemeal nature, as well as idealized wave tank experiments, have been made. It is not sufficient to correct a computed surge for one or more longterm interactions. Accordingly, the SLOSH model lumps the long-term interactions into an ad hoc generalized calibration to observed surge data generated by a multitude of historical storms: that is, the short term action from wind waves is absent but crude approximations for the long term effects may be present. The SLOSH model does give an indication of inland flooding but not the pulsating action of wind waves, such as shortterm, periodic, sheet flow over barriers (Jelesnianski et al, 1992)."}, {"section_title": "THE EMPIRICAL SIMULATION TECHNIQUE", "text": "The Empirical Simulation Technique (EST) is a procedure for simulating multiple life-cycle sequences of non-deterministic multi-parameter systems such as storm events and their corresponding environmental impacts. Essentially, it is a \"Bootstrap\" resampling-with-replacement, interpolation, and subsequent smoothing technique in which random sampling of a finite length database is used to generate a larger database (Borgman et al. 1992). The only assumption is that future events will be statistically similar in magnitude and frequency to past events. As stated above, EST is a generalized procedure applicable to any cyclic or frequency-related phenomena (Scheffner et al, 1999). For example, if one can parameterize storm events as well as obtain or simulate corresponding historical impacts for these events, EST could be used to investigate lifecycle scenarios of storm conditions. The EST begins with an analysis of historical events that have impacted a specific locale. The selected database of events (the training set) is then parameterized to define the characteristics of the event. The interdependence of parameters is computed directly from the respective parameter interdependencies contained in the historic data. In this manner, probabilities are site-specific; do not depend on fixed parametric relationships or assumed joint probability distributions. The impacts of events may be known or may be simulated by other models (e.g., hurricane events can be characterized by parameters such as central pressure, forward speed, etc. and their impact may be simulated with appropriate hydrodynamic and storm wind models). Parameters that describe an event, i.e., a storm in this discussion, are referred to as input parameters or input vectors. Response parameters or response vectors define event-related impacts such as storm surge elevation, inundation, shoreline and dune erosion, etc. These input parameters and response parameters are then used as a basis for generating life-cycle simulations of hurricane activity with corresponding impacts. The descriptive characteristics of the storm event with respect to the specific location of interest are determined by the input parameters or input vectors. For tropical storms these input parameters are studied at the point when the eye of the hurricane is closest to the station of interest. These vectors are defined as: 1) tidal phase during the event, with 1.0 corresponding to high water slack, 0.0 MSL at maximum ebb, -1.0 low water slack, these represent relative values that are defined for each station 2) radius of maximum wind for the hurricane when the eye is closest to the hurricane in nautical miles. 3) minimum distance from the eye of the storm to the location of interest in nautical miles. 4) pressure at the hurricane eye in millibars (mb) 5) wind speed in the hurricane at the instant of eye hitting the coast, measured in knots. 6) direction of forward propagation of the eye of the hurricane in knots. 7) tidal range during the event: with spring, neap or mid tide conditions. The maximum storm surge elevation reached at specified gauge locations is defined as the response vector of the storm at that location. The specified response vector for this study was determined by simulating the specific storm event via the ADCIRC hydrodynamic model using the computational domain shown in Fig. 1-2. The output vector(s) represents the environmental response to the storm. This response is defined at location X and is a direct consequence of the storm via the storm parameter values defined at the point of nearest proximity of the storm eye to point X. For the case of stage-frequency analyses, maximum surge is assumed to occur when the eye of the storm is nearest to location X."}, {"section_title": "Storm Consistency with Past Events", "text": "The first major requirement for the use of EST is that future events will be statistically similar to past events. This criterion is maintained by insuring that the input vectors for simulated events are similar to those of past events and the input vectors have similar joint probabilities to those historical events of the training set. For example, a hurricane with a large central pressure deficit and low maximum winds is not a realistic event -the two parameters are not independent although their precise dependency is unknown. The simulation of realistic events is accounted for in the nearest-neighbor interpolation-bootstrap-resampling technique developed by Borgman (Scheffner, et al. 1999 andBorgman, et al. 1992). By using the training set as a basis of for defining future events, unrealistic events are not included in the life cycle of events generated by the EST. Events that are output by EST are similar to those in the training set with some degree of variability from the historic/historically based events. This variability is a function of the nearest neighbor: therefore the deviation from historic conditions is limited to natural variability of the system. The basic technique can be described as follows. Let X 1 , X 2 , X 3 , . . . X a be n independent, identically distributed random vectors (historic storm events) each having two components [X i ={xi(1),x i (2)}; I =1,n]. If there are no hypothetical events, each event X i has a probability p i of l/n. If one storm event is used to generate two hypothetical events, then the original storm and each of the two perturbations are assigned a probability of one-third of l/n. A cumulative probability relationship can be developed in which each storm event of the total training set is assigned a segment of the total probability of 0.0 to 1.0. Therefore each event occupies a fixed portion of the 0.0 to 1.0 cumulative probability spaces according to the total number of events in the training set. A random number from 0 to 1 is then used to identify a storm event from the total storm training set population. The procedure is equivalent to drawing and replacing a random sample from the full storm event population. The EST is not simply a resampling of historical events technique, but rather an approach intended to simulate the vector distribution contained in the training set data base population. The EST approach is to select a sample storm based on a random number selection from 0 to 1 and then performs a random walk from the event X i with n number of response vectors to the nearest neighbor vectors. The walk is based on independent uniform random numbers on (-1,1) and has the effect of simulating responses that are not identical to the historical events but are similar to events, which have historically occurred. However it is important to point out that it is possible that the response value of water surface elevation (i.e. tide plus surge) may be greater than the greatest value in the total training set or it could be smaller than the smallest of the training set. The process can be summarized as follows. Select a specific storm event from the training set and proceed to the location in multidimensional input vector space corresponding to that event. From that location, perform a nearest neighbor random walk to define a new set of input vectors. This new input vector defines a new storm, similar to the original storm but with some variability in parameters."}, {"section_title": "Storm Event Frequency", "text": "The second criteria to be satisfied is that the number of storm events selected per year must be statistically similar to the number of historical events that have occurred at the area of concern. Given the mean frequency of storm events for a particular region, a Poisson distribution is used to determine the average number of expected events in a given year. For example, a Poisson distribution can be written in the following form: for s=0,1,2,3\u2026 The probability Pr(s;\u03bb) defines the probability of having s events per year where \u03bb is the historically based number of events per year. In the present study, historical data were used to define \u03bb as: \u03bb = 0.2307 (27 historical events/117 years or one event every 4.33 years) Output from the EST program is N repetitions of T years of simulated storm event responses. For this study, 500 repetitions, N, of a 200 year sequence, T, of storm activity are used. It is from the responses of those 500 life cycle simulations that frequency-ofoccurrence relationships are computed. Because EST output is of the form of multiple time-series simulations, post processing of output yields mean value frequency relationships with definable error estimates. The computational procedure followed is based on the generation of a probability distribution function corresponding to each of the T-year of simulated data. In the following section, the approach adopted for using these storms to develop frequency-of-occurrence relationships is given."}, {"section_title": "Risk-Based Frequency Analysis", "text": "The primary justification for applying the EST to a specific project is to generate risk-based frequency information relating to effectiveness and cost of the project with the level of protection provided. The multiple life-cycle simulations produced by EST can be used for developing design criteria in two approaches. In the first, the actual time series "}, {"section_title": "Frequency-of-Occurrence Relationships", "text": "Estimates of frequency-of-occurrence begin with the calculation of a probability distribution function (pdf) for the response vector of interest. Let X 1 , X 2 , X 3 , . . . , X n be n independent, identically distributed, random response variables with a cumulative pdf given by where Pr [X<x] represents the probability that the random variable X is less than or equal to some value x, and F x (x) is the cumulative probability density function ranging from 0.0 to 1.0. The problem is to estimate the value of F x without introducing some parametric relationship for probability. The following procedure is adopted because it makes use of the probability laws defined by the data and does not incorporate any prior assumptions concerning the probability relationship. Assuming a set of n observations of data, the n values of x are first ranked in order of increasing size. In the following analysis, the parentheses surrounding the subscript indicate that the data have been rank-ordered. The value x(1) is the smallest in the series and x(n) represents the largest value. Let r denote the rank of the value x(r) such that rank r = 1 is the smallest and rank r = n is the largest. An empirical estimate of F x (x(r)), denoted by F x (x (r) ), is given by Gumbel (1954) (See also Borgman and Scheffner (1991) and Scheffner and Borgman (1992)) as: for {x (r) , r = 1, 2, 3, . . ., n}. This form of estimate allows for future values of x to be less than the smallest observation x (1) with a cumulative pdf of 1/(n+1), and to be larger than the largest values with cumulative pdf of n/(n+1). The cumulative pdf as defined by Equation 14is applied to develop stagefrequency relationships as follows. Consider that the cumulative probability for an nyear return period storm can be written as where F(n) is the simulated cumulative pdf for an event with a return period of n years. Frequency-of-occurrence relationships are obtained by linearly interpolating a stage from Equation 14corresponding to the pdf associated with the return period calculated by Equation (15). Equations (14) and (15) are applied to each of the N-repetitions of T-years of storm events simulated via the EST. Therefore, there are N frequency-of-occurrence relationships generated. From these results, the standard deviation is determined to provide an estimate of the variability of the result. The standard deviation is computed for each return period as: where x is the mean value of x."}, {"section_title": "PROJECT IMPLEMENTATION", "text": "The model as stated before required the generation of finite element grid and application of appropriate boundary conditions in order to simulate tides and coupling with the wind model PBL, to simulate hurricanes and tropical storms. The process required the following tasks: "}, {"section_title": "Coastline", "text": "The coastline is required to define the extents of the model domain. This will become the boundary of the computational mesh. The coastline around our area of interest as well as the ocean defines the domain. The coastline for this purpose is obtained in digital format from GEODAS and NOAA databases. The coastline is in the form of World Vector Shoreline subset at 1:1million resolution (altered) format. Since the obtained coastline was ragged in nature, it was smoothened before being used for grid generation."}, {"section_title": "Bathymetry", "text": "The bathymetry in the Gulf of Mexico varies dramatically, as is illustrated in the Fig. 1-3. Bathymetric data in most of the Gulf was obtained from the grid developed by Scheffner et al. (2003), GeoDas (a database developed by National Oceanic and Atmospheric Administration, NOAA), USACE surveys, surveys conducted by Texas A&M University in the area of interest, and USGS terrain data. The terrain data was in the form of 30-meter grid digital elevation models (DEM). These data are based on the USGS 7.5 minute x 7.5-minute quads maps and are interpolated from 5-foot elevation contours."}, {"section_title": "Grid Generation", "text": "The grid was generated as a combination of finite element grid developed by Scheffner et al. (2003) and modified in the area of interest with details added. The grid in the area of interest was developed using SMS (Surface Modeling System). To get a mesh/grid with density radiating from the center of the Freeport channel, size functions in SMS were used along with celerity and wavelength functions so that smaller elements are obtained closer to the shore to correctly model the area of interest."}, {"section_title": "Boundary Conditions and Model Setup", "text": "Boundary conditions are imposed on the solutions of ordinary differential equations and partial differential equations, to fit the solutions to the actual problem.There are many kinds of possible boundary conditions, depending on the formulation of the problem, number of variables involved, and (crucially) the mathematical nature of the equation. The boundary conditions used within ADCIRC for this study were: \u2022 There are several other boundary conditions that can be applied in ADCIRC but they were not needed for the present model. The model was \"spun up\" (started with progressively increasing forcing such as tides or winds) from homogeneous initial conditions using a time ramp to avoid problems with short period gravity modes and vortex modes in the sub internal frequency range. A very smooth hyperbolic tangent ramp function, which acts over approximately one day, was applied to both boundary conditions and direct forcing functions. A 6-day spin-up was determined to be more than adequate for all conditions of interest. A time step of 6 sec was used for tidal propagation and a time step of 2 seconds was used for storm simulation in order to accommodate the strong gradients associated with strong winds for storm conditions.  "}, {"section_title": "Tidal Verification", "text": "Tidal water surface elevation data computed with the ADCIRC model were recorded at locations for verification purposes. These locations are listed in Table 5-1. Storm surge water surface elevations were archived for 35 locations within the area of interest for subsequent computation of frequency-of-occurrence relationships. The verification of the model had to be done to ensure that grid resolution, bathymetry, and boundary conditions were acceptable to properly simulate conditions in the defined domain. For comparison of tidal simulations with observed tides, verification was accomplished using 8-constituents (M 2 , S 2 , N 2 , N 1 , K 1 , O 1 , Q 1 , and P 1 ), as these constituents comprise most of the tidal energy, with tidal elevations calculated using software XTIDE which in turn uses published harmonic series, and NOS published tidal records. The use of fewer tidal constituents resulted in less accurate simulation of tides in the study area. Additionally, tidal potential terms are specified at each node of the computational grid. The ADCIRC model has an internal harmonic analysis option in which individual constituent amplitudes and epochs are computed at user specified locations during the tidal simulation. Verification of tidal circulation was made by comparing both ADCIRC computed harmonic constituents and ADCIRC computed time series with existing constituent data and reconstructed time series at each of the 15 verification locations listed in Table 5-1. Comparisons of ADCIRC versus published Harmonic Analysis (HA) computed constituent amplitudes and Greenwich epochs (G) are shown in Table 5-2 for two locations. Because the Gulf of Mexico is a semi-enclosed body of water, approximately 10 to 15 days of spin up time were required for the tide to come to a dynamic equilibrium, i.e. when the tides are acceptably reproduced. The harmonic analysis used for the comparisons in Table 5-2 were based on a 43-day simulation of tides and during this time the harmonic analysis was computed for the 29-day (one lunar month) period of days 15 through 43. A period with little wind activity was chosen to effectively compare the real-time data and ADCIRC simulated time-series. In order to demonstrate a degree of acceptability for the constituent comparisons shown in Table 5   for all gauges to demonstrate approximate verification in a more global sense with verification efforts concentrated on the study area. However, in order to demonstrate that the overall verification was acceptable, all model to reconstructed prototype tidal time series are presented in Appendix A. As shown, the comparisons are acceptable and demonstrate that the ADCIRC model is properly reproducing tides throughout the general area of interest as well as in the specific study area."}, {"section_title": "Tropical Storm Surge", "text": "The PBL model was used during this study and was coupled with ADCIRC in the form of a wind file, which can be input to the ADCIRC model to simulate wind effects of the storms of interest for the study area. Peripheral atmospheric pressures were assumed equal to the standard atmospheric pressure of 1013 millibars (mb) and the geostrophic wind speeds were specified as 6 knots in the same direction as the moving eye of the storm. All additional data were computed from data contained in the National Oceanic and Atmospheric Administration's (NOAA) Hurricane DATabase (HURDAT) of tropical storm events (Jarvinen, Neumann, and Davis 1988). This database is updated yearly and now contains descriptions of all hurricane, tropical storm, and severe tropical depressions The goal of this component of the study was to compute frequency-of-occurrence relationships for storm surge plus tide in the Freeport area. In order to develop these relationships, it was necessary to identify tropical storms that have historically impacted the study area. This was accomplished by making use of the tropical storm database (Scheffner, et al, 1994) that was generated through simulation of 134 historically based storm events along the east coast, Gulf of Mexico, and Caribbean Sea. The database uses the HURDAT database described above as input. For 486 discrete locations along the U.S. coast, peak storm surge values corresponding to storm events, which produced a surge of at least 0.305 m, were archived and indexed according to event, location, and surge magnitude. The database was used to select 26 storm events for the present study beginning with the hurricane of 1886 and extending through Hurricane Claudette (2003). These events, shown in Table 5-3, represent the selected historical set of storms. An example plot of the storm track and location every 6 hours of Hurricane Claudette is shown in Fig. 5-5. The track for each storm event of the historical training set is shown in Appendix B.  Hurricane Claudette (2003). As is evident from Fig. 5-6, the storm surge is accurately captured; however, the tides are not accurately simulated due to spin-up time required for tidal simulation in the Gulf. In Fig. 5-7, surge-only data is shown, which is computed by subtracting tides and pre-storm datum from the raw signal. All storms shown in  Use of these 10 hypothetical events increases the total to 37 storms that are used as a \"training set\" for the study. Thus the training set is comprised of 27 historical storm events plus the 10 perturbations. "}, {"section_title": "Application of EST", "text": "In order to establish the training set of storms 27 historical events and 10 storm perturbations were used to produce a total of 37 events. Each of the 37 storms was simulated without tide to produce a set of surge-only responses at the stations. The storms are then assumed to have taken place at different phases of the astronomical tides: 1) high tide, 2) mean (MSL = 0.0) tide, and 3) low tide. It is further assumed that the storm could occur during the lunar cycles of: 1) spring tide, 2) between spring and neap, and 3) neap tide. Input vectors representing these phases of the tide are described above. This combination of tide and lunar cycle produces 9 surface elevations for each of the 27 storm events of Table 5-3 and 10 hypothetical storm events at the station locations shown in Fig. 5-11. This procedure produces a total input/response vector training set of 333 (37*9), tide plus surge events for each station location. It is also considered that the mid-tide level would have twice the probability of occurrence on the MHHW or the MLLW. Similarly the mean tidal range would have twice the probability of occurrence as spring or neap tide. The ADCIRC-EST approach can thus be used to estimate and predict life cycle storm surge values at any given site as shown above. These values can hence be used for design purposes or for evacuation and planning purposes. The model SLOSH is currently used by federal authorities in the United States for hurricane evacuation and planning. The next section presents a comparison between this approach and the SLOSH approach."}, {"section_title": "COMPARISON WITH SLOSH", "text": "The SLOSH numerical model is used to prepare an atlas of pre-computed surges. On-site computations with several tracks, tropical cyclone intensities and sizes may require many computer runs. During a real time situation this can overload computer facilities and personnel, and require unacceptably time consuming analysis of the output. To generate a database of pre-computed atlases of storm surges for a particular basin with inland water bodies and complicated terrain features, recourse is made to a tropical cyclone climatology, which gives a broad view of the tropical cyclone types likely to affect a given region. Historical tropical cyclones affecting a region are stratified into preferred track directions, intensities, and sizes. Such families of equally spaced, parallel tracks for surge computations gently curve to reflect climatological data, but should all correspond in the vicinity of the landfall points."}, {"section_title": "Methodology of Storm Atlas", "text": "The family of tracks account for alternate landfall points for a given direction along a coastal area of interest (or else alternate distances from the coast for alongshore moving tropical cyclones). It is recognized that the generated surge normally is strongly dependent on the angle the track makes with the coast, several hours before and after landfall. Thus, the remaining track segments affect the surge only mildly. Thus, although the location of a tropical cyclone far out to sea and its landfall point may be significantly in error, the family, or families, representing the broad approach to land can be used to estimate the likely surge consequences. For simplification, it is assumed that the cyclone translation speed, central pressure and size remain constant along the track. Alternate values of these parameters can be used for each track family to provide a more comprehensive database. The embedded, identical tropical cyclone model in each family of tracks also can be designed to alter the tropical-cyclone central pressure and size with time after landfall to represent any explosive filling and core changes of the tropical cyclone. After a study of potential surges with idealized tracks from the atlas, runs are then made to fine tune a surge forecast that includes relevant details from the actual cyclone."}, {"section_title": "Maximum Envelope of Waters (MEOW)", "text": "An atlas of pre-computed surges can be a bulky document and collating the several possible tropical cyclone conditions from the many into a composite potential of surges is a demanding chore. Since each computer run gives an envelope of highest waters in a basin for the life history of a tropical cyclone, it is a simple computer chore to determine the highest possible surge at all vulnerable coastal locations from a particular family of tracks. The resulting map is called a Maximum Envelope of Waters, or MEOW. Model runs are made using hypothetical tropical cyclones stratified by the Saffir/Simpson scale of intensity categories and sizes. To generate the MEOW, the maximum surge value from the entire family of cyclones at each grid square of a basin is saved; regardless of which cyclone was responsible. The resulting composite of peak surges makes up a MEOW such as that shown in Fig. 6-1. Other MEOWs can be developed for a range of cyclone profiles and conditions. This provides an easily accessible summary of the \"worst case\" surge scenario given the uncertainty in the current forecast situation."}, {"section_title": "Storm Atlas for Harris/ Brazoria County", "text": "The model SLOSH was run for imaginary storms with varying intensities (category 1-5) and the effect of storm making landfall at a relative distance from Galveston channel was studied. These results were produced in the form of Maximum Storm Surge Penetration Maps in the Storm Atlas (College of Architecture 1994). There are five MEOWs, which indicate the worst case MEOWs (or MOMS, Maximum of MEOWs) for each category of hurricane making landfall from 87 nautical miles right to 70 nautical miles left of Galveston channel. For each of these landfall areas, a high and a low MEOW were produced. The high (maximum) is produced by taking all of the various MEOW movement directions and forward movement speeds and using the high surge elevations. The low (minimum) is produced by using the lowest surge elevations. As will be noticed, the differences between high (maximum) and low (minimum) surge estimates for the same category of hurricane are quite extreme. This illustrates the extreme range of possibilities in modeling with SLOSH as only hypothetical events are used. "}, {"section_title": "Comparison with ADCIRC", "text": "For comparison of ADCIRC simulated storm surge with SLOSH predicted surges, the hurricanes used in the study were arranged according to the approach used in the storm atlas that is on the basis of the Saffir-Simpson scale category and the distance from Galveston ship channel as shown in Table 6-1.  Galveston Island, Freeport harbor and Clear Lake and is shown in Fig. 6-2, 6-3 and 6-4. As evident from the Figs, there is a large amount of scatter in the case of SLOSH values, especially in the case for Freeport harbor. This may be because the SLOSH runs with simulated hurricanes whose distances were measured from Galveston ship channel. The distance between Freeport Harbor and the ship channel being 80 miles, the SLOSH analysis is not accurate for Freeport area. In the case of storm surge at Freeport due to Hurricane Alicia (1983), SLOSH estimates that a hurricane of its intensity and at a distance of 38.45 n-miles from Galveston ship channel should produce a maximum surge of 3.96 m and a minimum surge of 1.2192 m. However, the ADCIRC generated surge due to Alicia was 1.023 m. The anecdotal surge due to Alicia (Garcia and Flor, 1984) was reported to be around 1 m. Thus SLOSH over-estimates the surge in this case. This is due to the fact that Alicia made landfall around 50 miles to the right of Freeport looking landward. In the northern hemisphere due to rotation of earth, the effect of storm surge is more pronounced on the right side of hurricane center rather than left side. This effect is not taken into account in the approach used for producing the storm atlas using SLOSH. Hence the storm surge values from the storm atlas are accurate only for centrally located regions in the SLOSH grid. Also this approach assumes that all the storms of same category making landfall within the distance of 46 n-miles right to 23 n-miles left of a location will produce the same surge. This may be true for some ideal cases but imagine an intense hurricane of category 5 having Rmax (Radius of maximum winds) of 29 nmiles making landfall 10 miles right of the central location. In this case areas 35-52 nmiles from the central location will experience high values of surge of the order of 3-4 meters but the areas on the left hand side of landfall location will experience much less storm surge, assuming the bathymetry around the area is similar. Some of the problems associated with SLOSH are thus identified as follows: 1. Although storm-induced flooding along the coast of the United States can be predicted fairly accurately using SLOSH, storm surges in bays and estuaries are often poorly predicted using this method because of irregular coastline and local topography. 3. For the same landfall location and category storms, the storm atlas gives the same results. It is accepted that the peak surge on the open coast is not always monotonically related to the parameter, maximum wind speed of the storm. In fact, the peak surge may increase or decrease according to the manner in which the other storm parameters are affected by the change in maximum wind speed. The pressure drop, size scale and shape of the wind profile also have an effect on the peak surge. For example Hurricane Camille (1969) that was an ordinarily sized storm though it was a category five storm and resulted in storm surge of 6 meters in Gulfport, Mississippi. However, SLOSH gives prominence to maximum wind speeds during analysis. 4. The SLOSH model's polar grid as shown in Fig. 3-2 is generally very coarse compared to the ADCIRC grid. For the study area Freeport, a high-resolution grid was needed to effectively calculate storm surges all along the existing levee system. The SLOSH grid covers the whole Freeport area in just 24 squares. 5. Since we do not completely understand the processes of hurricane formation and hence cannot model these accurately, using empirical relations to simulate them is a wrong approach. Thus the approach of using historical storms to generate database of storm surge and then using a return period concept to calculate maximum storm surges may be more reliable instead of just simulating different storms, where not all the storms have similar features. Using the same coefficients for all these different storms also is not advisable. 6. Risk assessment by definition necessitates a probabilistic approach whereas SLOSH uses an event-driven approach. The SLOSH model results are taken manually from the contour plots provided in the Storm Atlas. Due to difficulties reading the SLOSH values from the contour plots, the values in the table should only be considered approximate (+/-0.5 feet) and only used for a broad and general comparison."}, {"section_title": "Alternate Approach", "text": "The database consisting of 37 storms, which is extended to 333 storms with addition of neap, spring tides, high and low tides, is used for this EST analysis. Two    Table 6-2 are recorded and are shown in Table 6-3. These return periods are then used along with storm surge frequency plots in order to obtain bands for different category of hurricanes. These bands are marked on the plots as shown in Fig. 6-7, 6-8 and 6-9. The expected surge corresponding to different category of hurricanes can be read from these plots. In order to validate this approach some hurricanes were simulated and checked for their acceptability with the return periods predicted. "}, {"section_title": "Validation", "text": "In order to validate this approach some more hypothetical hurricanes were simulated in addition to the existing ones. Since SLOSH model assumes that the size and intensity of a hurricane remain constant throughout its track, Hurricane Carla (1961) was manipulated to obtain medium range category 2,3,4 and 5 hurricanes. Further, the track of Carla was perturbed by shifting it by -1.0 and -1.5 degree longitude from its normal track to obtain the maximum effect around the Freeport and Galveston Bay areas respectively. The storm surges from these hurricanes were plotted on the categorized storm surge return period graphs and are shown in Figures 6-7, 6-8 and 6-9. The return periods for pressure difference are used in this study, as they seem to be more accurate and also conform to the HURISK return periods (Neumann C.J. 1994). The reason for this may be because maximum wind speeds are a function of central pressure difference and radius of maximum winds (Jelesnianski 1972), and since we still can not simulate Radius of maximum winds accurately, there seems to be some flaw in using maximum winds for calculating return periods for categories of hurricanes. Also, the fact that hurricane database consists of wind speeds greater than 64 knots, introduces some error in EST analyses for lower values of wind speeds response vector. The hypothetical hurricanes like Carla (-1.0) and (-1.5) longitude whose maximum effect occurs at the respective locations Freeport and Galveston Bay fall well within the bands for their category as predicted. This is because these hurricanes make landfall within the distance of 30-50 miles left of the respective sites. Hence these hurricanes produce their maximum surge at these locations. As we move away from the landfall location, surge will reduce. However on the right hand side of the landfall location due to opposing wind directions surge is also reduced. Let us consider the case of Hurricane #211. It was a medium range category 4 hurricane according to the Saffir Simpson scale. It made landfall at a distance of 31.2 miles from Galveston ship channel. It produced a surge consistent with the expected value read from Fig. 6-7 for Pleasure Pier. However it has a different impact at Freeport. It made landfall at a distance of 18 miles to the right and resulted in a surge of 1.34 meters. This is much less than its expected value that is within range of 2.6-4.5 meters. This is due to its making landfall right of Freeport. Thus it had an effect of a category 2 hurricane in the vicinity of Freeport. Similarly Hurricane Alicia made landfall 33.5 miles left of ship channel and resulted in a storm surge of 2.7 meters and lies within its expected category in the frequency curves. However with respect to Freeport, it made landfall right of Freeport at a distance of 15 miles. Here again the frequency plots do not capture the storm surge associated due to it. This is again due to its making landfall right of Freeport. The return period bands cover a wide range of storm surge, thus in order to predict a value that a hurricane can result, care must be taken. Storm surge being a complicated process, dependent on a large number of parameters, the predicted value will always have some inherent errors in it. Since the EST approach tries to take into effect all the factors that influence storm surge, the approach is more reliable compared to other approaches. A hurricane belonging to a category can, still be classified further into weaker, modest and stronger based on the ranges. Hence a weaker hurricane would correspond to a lower value of storm surge in that band and so on. The effect of distance of the site from landfall also plays an important role in the storm surge expected at that site. Based on the radius of maximum winds for the hurricane, the value must be selected from the plots considering the fact that surge is greater at a distance of around 30-40 miles from the landfall location. As we move away from the landfall location the storm surge is greatly reduced. Thus relative distance plays a very important role in storm surge expected due to a hurricane. The present prediction and forecasting techniques do not accurately capture the effect of relative distance. Thus a method needs to be established in order to quantify storm surge due to a hurricane of given intensity and size on the basis of its relative distance from the landfall location. The prevalent forecasting techniques do a good job in estimating maximum surge that can be expected based on Saffir-Simpson scale category of storms."}, {"section_title": "CONCLUSIONS", "text": "The results of this thesis illustrate the use of the hydrodynamic model ADCIRC in conjunction with statistical model EST for storm surge simulation and generation of frequency return curves for a location. Comparisons have been presented that show model output from ADCIRC and observed water level data for various hurricanes that have significantly impacted Texas coast during last 117 years. The simulated data for most of the hurricanes compares well with the measured data wherever available. The model was able to accurately simulate the time and duration of the highest storm surge level. However the model is not able to fully capture storm surges due to tropical storms like Allison (2001). This inadequacy most likely results from the simplified vortex flow representation of the wind field by the wind model PBL, which may not accurately model the actual hurricane wind field particularly for a tropical storm as a storm approaches and passes over a coastal boundary. The other cause may be the method used to estimate the radius of maximum winds, which is the nomograph of The study extends the storm surge simulation using the EST model to arrive at life-cycle return period curves for storm surge at a particular location. These values can be used to give a realistic idea of typical \"design\" storm surge values that may help engineers to design new structures and maintain existing structures. EST encompasses all the parameters that affect the storm surge at a location for an individual hurricane like relative distance from landfall. Thus the results give a more realistic picture of the damage potential to the engineers. This approach is also compared with the SLOSH approach used by federal authorities to delineate coastal areas susceptible to hurricane storm surge flooding and evacuation studies. The shortcomings in the model SLOSH and the storm atlas approach are pointed out in the study. An alternate way for estimating storm surge at a location based on Saffir-Simpson categorization scale of hurricanes is presented in the study. This method utilizes the database of historical and hypothetical storms to arrive at return periods for different categories of hurricane and then uses these periods for estimating storm surges due to different category of hurricanes. This approach needs to be applied for localized areas, as there are many local factors that may influence the storm surge. The local bathymetry may play an important role in the final storm surge value; hence the computational grid used for populating the database should have required resolution in order to capture the local bathymetry. The shape of the coastline also has a strong effect on the size of a storm surge. A concave coastline is favored for greater storm surge, as water can be funneled toward the center of the coastline. For example, the coast of North Carolina has many concave areas that can influence the size of a storm surge. For example, in 1996, Hurricane Fran made landfall at Cape Fear, NC and produced a storm surge of 3.2 m at Carolina Beach, NC. Hurricane Hazel (1954) made landfall at the border of North and South Carolina, as a rapidly moving hurricane. This area of North Carolina's coastline to the right of Hazel was concave. Hazel was also a stronger, faster moving storm than Fran and it produced a peak storm surge of 5.5 m on the south facing beaches south west of Carolina Beach, while Carolina Beach had a peak storm surge of less than 3.0 m. Hence these coastline effects again call for a good resolution of the computational grid. There are certain improvements that can be made to the model. One is to include shoaling effect. This refers to water forced from deep water into shallow water, where it converges to the shoreline. The more shoaling that occurs, the higher the potential storm surge. The waves also play an important role in potential storm surge. Strong hurricane winds may result in high wave setups. Hence coupling of the hydrodynamic ADCIRC model with a wave model would be helpful in estimating these factors. Also, high rainfall amounts can lead to fresh water flooding, which can exacerbate the storm surge problem. If a hurricane makes landfall in a location where several rivers empty into the ocean, the runoff from the rivers can increase flooding. An example of this is Bangladesh, which is located in a low-lying flood plain where several rivers empty into the Indian Ocean. Future models may also include the effect of rainfall and river-runoffs to completely calculate and estimate storm surges. After evaluating the limitations of the present storm surge calculation and estimation approach, it still provides useful information about surge levels for sites, particularly for sites that are located to the east of a storm. The good feature of this approach is its relative simplicity, as it can model a storm using only the most basic information. Also as it is based on actual historical storm rather than possible hypothetical ones, it is more credible.  "}]