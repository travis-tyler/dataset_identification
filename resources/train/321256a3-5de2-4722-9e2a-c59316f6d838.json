[{"section_title": "Abstract", "text": "Abstract -Principal component analysis (PCA) is an exploratory tool widely used in data analysis to uncover the dominant patterns of variability within a population. Despite its ability to represent a data set in a low-dimensional space, PCA's interpretability remains limited. Indeed, the components produced by PCA are often noisy or exhibit no visually meaningful patterns. Furthermore, the fact that the components are usually non-sparse may also impede interpretation, unless arbitrary thresholding is applied. However, in neuroimaging, it is essential to uncover clinically interpretable phenotypic markers that would account for the main variability in the brain images of a population. Recently, some alternatives to the standard PCA approach, such as sparse PCA (SPCA), have been proposed, their aim being to limit the density of the components. Nonetheless, sparsity alone does not entirely solve the interpretability problem in neuroimaging, since it may yield scattered and unstable components. We hypothesized that the incorporation of prior information regarding the structure of the data may lead to improved relevance and interpretability of brain patterns. We therefore present a simple extension of the popular PCA framework that adds structured sparsity penalties on the loading vectors in order to identify the few stable regions in the brain images that capture most of the variability. Such structured sparsity can be obtained by combining, e.g., 1 and total variation (TV) penalties, where the TV regularization encodes information on the underlying structure of the data. This paper presents the structured SPCA (denoted SPCA-TV) optimization framework and its resolution. We demonstrate SPCA-TV's effectiveness and versatility on three different data sets. It can be applied to any kind of structured data, such as, e.g., N-dimensional array images or meshes of cortical surfaces. The gains of SPCA-TV over unstructured approaches (such as SPCA and ElasticNet PCA) or structured approach (such as GraphNet PCA) are significant, since SPCA-TV reveals the variability Manuscript"}, {"section_title": "", "text": "within a data set in the form of intelligible brain patterns that are easier to interpret and more stable across different samples.\nIndex Terms-MRI, unsupervised machine learning, PCA, total variation."}, {"section_title": "I. INTRODUCTION", "text": "P RINCIPAL components analysis (PCA) is an unsupervised statistical procedure whose aim is to capture dominant patterns of variability in order to provide an optimal representation of a data set in a lower-dimensional space defined by the principal components (PCs). Given a data set X \u2208 R N\u00d7P of N samples and P centered variables, PCA aims to find the most accurate rank-K approximation of the data: Using K = rank(X) components leads to the singular value decomposition (SVD). A vast majority of neuroimaging problems involve high-dimensional feature spaces (\u2248 10 5 features i.e. voxels or mesh (nodes over the cortical surface) with a relatively limited sample size (\u2248 10 2 participants. With such \"large P, small N\" problems, the SVD formulation, based on the data matrix, is much more efficient than an eigenvalue decomposition of the large P \u00d7 P covariance matrix. In a neuroimaging context, our goal is to discover the phenotypic markers accounting for the main variability in a population's brain images. For example, when considering structural images of patients that will convert to Alzheimer disease (AD), we are interested in revealing the brain patterns of atrophy explaining the variability in this population. This provides indications of possible stratification of the cohort into homogeneous sub-groups that may be clinically similar but with a different pattern of atrophy. This could suggest different sub-types of patients with AD or some other etiologies such as dementia with Lewy bodies. Clustering methods might be natural approaches to address such situations, however, they can not reveal subtle differences that go beyond a global and trivial pattern of atrophy. Such patterns are usually captured by the first component of PCA which, after being removed, offers the possibility to identify spatial patterns on the subsequent components.\nHowever, PCA provides dense loading vectors (patterns), that cannot be used to identify brain markers without arbitrary thresholding.\nRecently, some alternatives propose to add sparsity in this matrix factorization problem [33] , [36] , [43] . The sparse dictionary learning framework proposed by [36] provides a sparse coding (rows of U) of samples through a sparse linear combination of dense basis elements (columns of V). However, the identification of biomarkers requires a sparse dictionary (columns of V). This is precisely the objective of Sparse PCA (SPCA) proposed in [14] , [30] , [31] , [49] , and [51] which adds a sparsity-inducing penalty on the columns of V. Imposing such sparsity constraints on the loading coefficients is a procedure that has been used in fMRI to produce sparse representation of brain functional networks [20] , [45] .\nHowever, sparse PCA is limited by the fact that it ignores the inherent spatial correlation in the data. It leads to scattered patterns that are difficult to interpret. Furthermore, constraining only the number of features included in the PCs might not always be fully relevant since most data sets are expected to have a spatial structure. For instance, MRI data is naturally encoded on a grid; some voxels are neighbors, while others are not.\nWe hypothesize that brain patterns are organized into distributed regions across the brain( [11] , [21] , [41] ). Recent studies tried to overcome this limitation by encoding prior information concerning the spatial structure of the data (see [24] , [29] , [48] ). However, they used methods that are difficult to plug into the optimization scheme (e.g., spline smoothing, wavelet smoothing) and incorporated prior information that sometimes may be difficult to define. One simple solution is the use of a GraphNet penalty ( [18] , [23] , [32] , [38] , [40] ). It promotes local smoothness of the weight map by simply forcing adjacent voxels to have similar weights using an \u03bb 2 penalty on the gradient of the weight map. Nonetheless, we hypothesized that Graph-net provided smooth solution rather than clearly identified regions. In data classification problems, when extracting structured and sparse predictive maps, the goals are largely aligned with those of PCA. Some classification studies have revealed stable and interpretable results by adding a total variation (TV) penalty to the sparsity constraint (see [19] ). TV is widely used as a tool in image denoising and restoration. It accounts for the spatial structure of images by encoding piecewise smoothness and enabling the recovery of homogeneous regions separated by sharp boundaries.\nFor simplicity, rather than solving Eq. (2), we solve a slightly different criterion which results from using the Lagrange form, rather than the bound form, of the constraints on V. Then, we extend the Lagrangian form by adding penalties ( 1 , 2 and TV) to the minimization problem:\nwhere \u03bb 1 , \u03bb 2 and \u03bb are hyper-parameters controlling the relative strength of each penalty. We further propose a generic optimization framework that can combine any differentiable convex (penalized) loss function with: (i) penalties whose proximal operator is known (here \u00b7 1 ) and (ii) a large range of complex, non-smooth convex structured penalties that can be formulated as a \u00b7 2,1 -norm defined over a set of groups G. Such group-penalties cover e.g., total variation and overlapping group lasso. This new problem aims at finding a linear combination of original variables that points in directions explaining as much variance as possible in data while enforcing sparsity and structure (piecewise smoothness for TV) of the loadings.\nTo achieve this, it is necessary to sacrifice some of the explained variance as well as the orthogonality of both the loading and the principal components. Most existing SPCA algorithms [14] , [31] , [49] , [51] , do not impose orthogonal loading directions either. While we forced the components to have unit norm for visualization purposes, we do not, in this formulation, enforce v k 2 = 1. Instead, the value of v 2 is controlled by the hyper-parameter \u03bb 2 . This penalty on the loading, together with the unit norm constraint on the component, prevents us from obtaining trivial solutions. The optional 1 N factor acts on and conveniently normalizes the loss to account for the number of samples in order to simplify the settings of the hyper-parameters:\nThis paper presents an extension of the popular PCA framework by adding structured sparsity-inducing penalties on the loading vectors in order to identify the few stable regions in the brain images accounting for most of the variability. The addition of a prior that reflects the data's structure within the learning process gives the paper a scope that goes beyond Sparse PCA. To our knowledge, very few papers ( [1] , [24] , [29] , [48] ) addressed the use of structural constraint in PCA. The study [29] proposes a norm that induces structured sparsity (called SSPCA) by restraining the support of the solution to be sparse with a certain set of group of variables. Possible supports include set of variables forming rectangles when arranged on a grid. Only one study, recently used the total variation prior [1] , in a context of multi-subject dictionary learning, based on a different optimization scheme [5] .\nSection II presents our main contribution: a simple optimization algorithm that combines well known methods (deflation scheme and alternate minimization) with an original continuation algorithm based on Nesterov's smoothing technique. Our proposed algorithm has the ability to include the TV penalty, but many other non-smooth penalties, such as e.g. overlapping group lasso, could also be used. This versatile mathematical framework is an essential feature in neuroimaging. Indeed, it enables a straightforward application to all kinds of data with known structure such as N-dimensional images (of voxels) or meshes of (cortical) surfaces. Section III demonstrates the relevance of structured sparsity on both simulated and experimental data, for structural and functional MRI (fMRI) acquisitions. SPCA-TV achieved a higher reconstruction accuracy and more stable solutions than ElasticNet PCA, Sparse PCA, GraphNet PCA and SSPCA (from [29] ). More importantly, SPCA-TV yields more interpretable loading vectors than other methods."}, {"section_title": "II. METHOD", "text": "A common approach to solve the PCA problem, see [14] , [31] , [49] , is to compute a rank-1 approximation of the data matrix, and then repeat this on the deflated matrix [34] , where the influence of the PCs are successively extracted and discarded. We first detail the notation for estimating a single component (Section II-A), and its solution using an alternating minimization pipeline (Section II-B). Then, we develop the TV regularization framework (Section II-C and Section II-D). Last, we discuss the algorithm used to solve the minimization problem and its ability to converge toward stable pairs of components/loading vectors (Section II-E) and (Section II-F)."}, {"section_title": "A. Single Component Computation", "text": "Given a pair of loading/component vectors, u \u2208 R N , v \u2208 R P , the best rank-1 approximation of the problem given in Eq. (2) is equivalent [49] to:\nwhere l(v) is the penalized smooth (i.e. differentiable) loss, h(v) is a sparsity-inducing penalty whose proximal operator is known and s(v) is a complex penalty on the structure of the input variables with an unknown proximal operator. This problem is convex in u and in v but not in (u, v)."}, {"section_title": "B. Alternating Minimization of the Bi-Convex Problem", "text": "The objective function to minimize is bi-convex [9] . The most common approach to solve a bi-convex optimization problem (which does not guarantee global optimality of the solution) is to alternatively update u and v by fixing one of them at the time and solving the corresponding convex optimization problem on the other parameter vector.\nOn the one hand, when v is fixed, the problem to solve is\nwith the associated explicit solution\nOn the other hand, solving the equation with respect to v with a fixed u presents a higher level of difficulty that will be discussed in Section II-E."}, {"section_title": "C. Reformulating TV as a Linear Operator", "text": "Before discussing the minimization with respect to v, we provide details on the encoding of the spatial structure within the s(v) penalty.\nIt is essential to note that the algorithm is independent of the spatial structure of the data. All the structural information is encoded in a linear operator, A, that is computed outside of the algorithm. Thus the algorithm has the ability to address various structured data and, most importantly, other penalties than just the TV penalty. The algorithm requires the setting of two parameters: (i) the linear operator A; (ii) a projection function detailed in Eq. (12) .\nThis section presents the formulation and the design of A in the specific case of a TV penalty applied to the loading vector v measured on a 3-dimensional (3D) image or a 2D mesh of the cortical surface.\n1) 3D Image: The brain mask is used to establish a mapping g(i, j, k) between the coordinates (i, j, k) in the 3D grid, and an index g \u2208 [ [1; P] ] in the collapsed image. We extract the spatial neighborhood of g, of size \u2264 4, corresponding to voxel g and its 3 neighboring voxels, within the mask, in the i, j and k directions. By definition, we have\nThe first order approximation of the spatial gradient \u2207(v g (i, j,k) ) is computed by applying the linear operator A g \u2208 R 3\u00d74 to the loading vector v g in the spatial neighborhood of g, i.e.\nwhere v g(i, j,k) is the loading coefficient at index g in the collapsed image corresponding to voxel (i, j, k) in the 3D image. Then A g is extended, using zeros, to a large but very sparse matrix A g \u2208 R 3\u00d7P in order to be directly applied on the full vector v. If some neighbors lie outside the mask, the corresponding rows in A g are removed. Noticing that for TV there is one group per voxel in the mask (G = [[1; P]]), we can reformulate TV from Eq. (6) using a general expression:\nFinally, with a vertical concatenation of all the A g matrices, we obtain the full linear operator A \u2208 R 3 P\u00d7P that will be used in Section II-E."}, {"section_title": "2) Mesh of Cortical Surface:", "text": "The linear operator A g used to compute a first order approximation of the spatial gradient can be obtained by examining the neighboring vertices of each vertex g. With common triangle-tessellated surfaces, the neighborhood size is \u2264 7 (including g). In this setting, we have A g \u2208 R 3\u00d77 , which can be extended and concatenated to obtain the full linear operator A."}, {"section_title": "D. Nesterov's Smoothing of the Structured Penalty", "text": "We consider the convex non-smooth minimization of Eq. (3) with respect to v, where thus u is fixed. This problem includes a general structured penalty, s(\u00b7), that covers the specific case of TV. A widely used approach when dealing with non-smooth problems is to use methods based on the proximal operator of the penalties. For the 1 penalty alone, the proximal operator is analytically known and efficient iterative algorithms such as ISTA and FISTA are available (see [4] ). However, since the proximal operator of the TV+ 1 penalty is not closed form, standard implementation of those algorithms is not suitable. In order to overcome this barrier, we used Nesterov's smoothing technique [39] . It consists of approximating the non-smooth penalties for which the proximal operator is unknown (e.g., TV) with a smooth function (of which the gradient is known). Non-smooth penalties with known proximal operators (e.g., 1 ) are not affected. Hence, as described in [50] , it allows to use an exact accelerated proximal gradient algorithm. Thus we can solve the PCA problem penalized by TV and elastic net, where an exact 1 penalty is used.\nUsing the dual norm of the 2 -norm (which happens to be the 2 -norm too), Eq. (8) can be reformulated as\nwhere\nis a vector of auxiliary variables, in the 2 unit ball, associated with A g v.\nAs with A \u2208 R 3 P\u00d7P which is the vertical concatenation of all the A g , we concatenate all the \u03b1 g to form the\nK is the Cartesian product of 3D unit balls in Euclidean space and, therefore, a compact convex set. Eq. (9) can further be written as\nGiven this formulation of s(v), we can apply Nesterov's smoothing. For a given smoothing parameter, \u03bc > 0, the s(v) function is approximated by the smooth function\nfor which lim \u03bc\u21920 s \u03bc (v) = s(v). Nesterov [39] demonstrates this convergence using the inequality in Eq. (15) . The value of"}, {"section_title": "maximizes Eq. (11) is the concatenation of projections of vectors", "text": "where\nThe function s \u03bc , i.e. the Nesterov's smooth transform of s, is convex and differentiable. Its gradient given by [39] \nis Lipschitz-continuous with constant\nwhere A 2 is the matrix spectral norm of A. Moreover, Nesterov [39] provides the following inequality relating s \u03bc and s\nwhere\nThus, a new (smoothed) optimization problem, closely related to Eq. (3) (with fixed u), arises from this regularization as\n. (16) Since we are now able to explicitly compute the gradient of the smooth part \u2207(l + \u03bbs \u03bc ) (Eq. (18)), its Lipschitz constant (Eq. (19) ) and also the proximal operator of the nonsmooth part, we have all the ingredients necessary to solve this minimization function using an accelerated proximal gradient methods [4] . Given a starting point v 0 and a smoothing parameters \u03bc, FISTA (Algorithm 1) minimizes the smoothed problem and reaches a prescribed precision \u03b5 \u03bc .\nCompute the gradient of the smooth part \u2207(g + \u03bbs \u03bc ) (Eq. (18)) and its Lipschitz constant L \u03bc (Eq. (19)). \nHowever, in order to control the convergence of the algorithm (presented in Section II-E1), we introduce the Fenchel dual function and the corresponding dual gap of the objective function. The Fenchel duality requires the loss to be strongly convex, which is why we further reformulate Eq. (16) slightly: All penalty terms are divided by \u03bb 2 and by using the following equivalent formulation for the loss, we obtain the minimization problem\nThis new formulation of the smoothed objective function (noted f \u03bc ) preserves the decomposition of f \u03bc into a sum of a smooth term l + \u03bb \u03bb 2 s \u03bc and a non-smooth term h. Such decomposition is required for the application of FISTA with Nesterov's smoothing. Moreover, this formulation provides a decomposition of f \u03bc into a sum of a smooth loss L and a penalty term \u03c8 \u03bc required for the calculation of the gap presented in Section II-E1).\nWe provide all the required quantities to minimize Eq. (17), as shown at the bottom of this page, using Algorithm 1. Using Eq. (13) we compute the gradient of the smooth part as\nand its Lipschitz constant (using Eq. (14))\nE. Minimization of the Loading Vectors With CONESTA\nThe step size t \u03bc computed in Line 3 of Algorithm 1, depends on the smoothing parameter \u03bc (see Eq. (19)). Hence, there is a trade-off between speed and precision. Indeed, high precision, with a small \u03bc, will lead to a slow convergence (small t \u03bc ). Conversely, poor precision (large \u03bc) will lead to rapid convergence (large t \u03bc ). Thus we propose a continuation approach (Algorithm 2) which decreases the smoothing parameter with respect to the distance to the minimum. On the one hand, when we are far from v * (the minimum of Eq. (17)), we can use a large \u03bc to rapidly decrease the objective function. On the other hand, when we are close to v * , we need a small \u03bc in order to obtain an accurate approximation of the original objective function.\n1) Duality Gap: The distance to the unknown f (v * ) is estimated using the duality gap. Duality formulations are often used to control the achieved precision level when minimizing convex functions. They provide an estimation of the error f (v k ) \u2212 f (v * ), for any v, when the minimum is unknown. The duality gap is the cornerstone of the CONESTA algorithm. Indeed, it is used three times:\ni As the stopping criterion in the inner FISTA loop (Line 7 in Algorithm 1). FISTA will stop as soon as the current precision is achieved using the current smoothing parameter, \u03bc. This prevents unnecessary convergence toward the approximated (smoothed) objective function. ii In the i th CONESTA iteration, as a way to estimate the current error f (v i )\u2212 f (v * ) (Line 7 in Algorithm 2). The error is estimated using the gap of the smoothed problem GAP \u03bc=\u03bc i (v i+1 ) which avoid unnecessary computation since it has already been computed during the last iteration of FISTA. The inequality in Eq. (15) is used to obtain the gap \u03b5 i to the original non-smoothed problem. The next desired precision \u03b5 i+1 and the smoothing parameter, \u03bc i+1 , are derived from this value. iii Finally, as the global stopping criterion within CONESTA (Line 10 in Algorithm 2). This will guarantee that the obtained approximation of the minimum,\nAlgorithm 2 CONESTA X u, \u03b5\n7:\n9:\nBased on Eq. (17), which decomposes the smoothed objective function as a sum of a strongly convex loss and the penalty,\nwe compute the duality gap that provides an upper bound estimation of the error to the optimum. At any step k of the algorithm, given the current primal v k and the dual \u03c3 (v k ) \u2261 \u2207L(v k ) variables [8] , we can compute the duality gap using the Fenchel duality rules [35] :\nwhere L * and \u03c8 * \u03bc are respectively the Fenchel conjugates of L and \u03c8 \u03bc . Denoting by v * the minimum of f \u03bc (solution of Eq. (17)), the interest of the duality gap is that it provides an upper bound for the difference with the optimal value of the function. Moreover, it vanishes at the minimum:\nThe dual variable is\nthe Fenchel conjugate of the squared loss\nHadj-Selem et al. [25] provide the expression of the Fenchel conjugate of the penalty \u03c8 \u03bc (v k ):\nwhere\nThe expression of the duality gap in Eq. (20) provides an estimation of the distance to the minimum. This distance is geometrically decreased by a factor \u03c4 = 0.5 at the end of each continuation, and the decreased value defines the precision that should be reached by the next iteration (Line 8 of Algorithm 2). Thus, the algorithm dynamically generates a sequence of decreasing prescribed precisions \u03b5 i . Such a scheme ensures the convergence [25] towards a globally desired final precision, \u03b5, which is the only parameter that the user needs to provide.\n2) Determining the Optimal Smoothing Parameter: Given the current prescribed precision \u03b5 i , we need to compute an optimal smoothing parameter \u03bc opt (\u03b5 i ) (Line 9 in Algorithm 2) that minimizes the number of FISTA iterations needed to achieve such precision when minimizing Eq. (3) (with fixed u) via Eq. (17) (i.e., such that f (v (k) \nHadj-Selem et al. [25] provide the expression of this optimal smoothing parameter:\nwhere M = P/2 (Eq. (15)) and L(\u2207(l)) = 2 is the Lipschitz constant of the gradient of l as defined in Eq. (17) . We call the resulting algorithm CONESTA (short for COntinuation with NEsterov smoothing in a ShrinkageThresholding Algorithm). It is presented in detail, with convergence proofs in [25] .\nLet K be the total number of FISTA loops used in CONESTA, then we have experimentally verified that the convergence rate to the solution of Eq. (16) is O 1/K 2 (which is the optimal convergence rate for first-order methods). Also, the algorithm works even if some of the weights \u03bb 1 or \u03bb are zero, which thus allows us to solve e.g., the elastic net using CONESTA. Note that it has been rigorously proved that the continuation technique improves the convergence rate compared to the simple smoothing using a single value of \u03bc. Indeed, it has been demonstrated in [6] (see also [50] ) that the convergence rate obtained with single value of \u03bc, even optimised, is O 1/K 2 + O(1/K ). However, it has recently been proved in [25] that the CONESTA algorithm achieves a O(1/K ) for general convex functions.\nWe note that CONESTA could easily be adapted to many other penalties. For example, to add the group lasso (GL) constraint to our structure, we just have to design a specific linear operator A G L and concatenate it to the actual linear operator A."}, {"section_title": "F. The Algorithm for the SPCA-TV Problem", "text": "The computation of a single component through SPCA-TV can be achieved by combining CONESTA and Eq. (5) within an alternating minimization loop. Mackey [34] demonstrated that further components can be efficiently obtained by incorporating this single-unit procedure in a deflation scheme as done in e.g. [14] , [31] . The stopping criterion is defined as\nAll the presented building blocks were combined into Algorithm 3 to solve the SPCA-TV problem. \n6:\nuntil STOPPINGCRITERION \u2264 \u03b5 8:\nu k+1 = u i+1 10:"}, {"section_title": "III. EXPERIMENTS", "text": "We evaluated the performance of SPCA-TV using three experiments: One simulation study carried out on a synthetic data set and two on neuroimaging data sets. In order to compare the performance of SPCA-TV with existing sparse PCA models, we also included results obtained with Sparse PCA, ElasticNet PCA, GraphNet PCA and SSPCA from [29] . We used the scikit-learn implementation [42] for the Sparse PCA while we used the Parsimony package(https://github.com/ neurospin/pylearn-parsimony) for the ElasticNet, GraphNet PCA and SPCA-TV methods. Concerning SSPCA, we used the MATLAB implementation provided in [29] .\nThe number of parameters to set for each method is different: For Sparse PCA, the \u03bb 1 parameter selects its optimal value from the range {0.1, 1.0, 5.0, 10.0}. ElasticNet PCA requires the setting of the \u03bb 1 and the \u03bb 2 penalties weights. Meanwhile, GraphNet PCA and SPCA-TV requires the settings of an additional parameter, namely the spatial constraint penalty \u03bb. We operated a re-parametrization of these penalty weights in ratios. A global parameter \u03b1 \u2208 {0.01, 0.1, 1.0} controls the weight attributed to the whole penalty term, including the spatial and the 1 regularization. Individual constraints are expressed in terms of ratios: the 1 ratio: \u03bb 1 /(\u03bb 1 + \u03bb 2 + \u03bb), \u2208 {0.1, 0.5, 0.8} and the T V (or G N for GraphNet) : \u03bb/(\u03bb 1 + \u03bb 2 + \u03bb), \u2208 {0.1, 0.5, 0.8}. For ElasticNet, we explore the grid of parameters composed of the Cartesian product of \u03b1 and 1 ratio subsets. For GraphNet PCA and SPCA-TV, we perform a parameter search on a grid of parameters given by the Cartesian product of respectively (\u03b1, 1 G N ) subsets and (\u03b1, 1 T V ) subsets. Concerning SSPCA method, the regularization parameter selects its optimal value in the range {10 \u22128 , . . . , 10 8 } However, in order to ensure that the components extracted have a minimum amount of sparsity, we also included a criteria controlling sparsity: At least half of the features of the components have to be zero. For both real neuroimaging experiments, performance was evaluated through a 5-fold x 5-fold double cross validation pipeline. The double crossvalidation process consists of two nested cross-validation loops which are referred to as internal and external cross-validation loops. In the outer (external) loop, all samples are randomly split into subsets referred to as training and test sets. The test sets are exclusively used for model assessment while the train sets are used in the inner (internal) loop for model fitting and selection. The inner folds select the set of parameters minimizing the reconstruction error on the outer fold. For the synthetic data, we used 50 different purposely-generated data sets and 5 inner folds for parameters selection. In order to evaluate the reconstruction accuracy of the methods, we reported the mean Frobenius norm of the reconstruction error across the folds/data sets, on independent test data. The hypothesis we wanted to test was whether there was a substantial decrease in the reconstruction error of independent data when using SPCA-TV compared to when using Sparse PCA, ElasticNet PCA, GraphNet PCA and SSPCA. It was tested through a related two samples t-test. This choice to compare methods performance on independent test data was motivated by the fact that the optimal reconstruction of the training set is necessarily hindered by spatial and sparsity constraints. We therefore expect SPCA-TV to perform worse on train data than other less constrained methods. However, the TV penalty has a more important purpose than just to minimize the reconstruction error: the estimation of coherent and reproducible loadings. Indeed, clinicians expect that, if images from other patients with comparable clinical conditions had been used, the extracted loading vectors would have turned out to be similar. Therefore, since the ultimate goal of SPCA-TV is to yield stable and reproducible weight maps, it is more relevant to evaluate methods on independent test data.\nThe stability of the loading vectors obtained across various training data sets (variation in the learning samples) was assessed through a similarity measure: the pairwise Dice index between loading vectors obtained with different folds/data sets [16] . We tested whether pairwise Dice indices are significantly higher in SPCA-TV compared other methods. Testing this hypothesis is equivalent to testing the sign of the difference of pairwise Dice indices between methods. However, since the pairwise Dice indices are not independent from one another (the folds share many of their learning samples), the direct significance measures are biased. We therefore used permutation testing to estimate empirical p-values. The null hypothesis was tested by simulating samples from the null distribution. We generated 1 000 random permutations of the sign of the difference of pairwise Dice index between the PCA methods under comparisons, and then the statistics on the true data were compared with the ones obtained on the reshuffled data to obtain empirical p-values.\nFor each experiment, we made the initial choice to retrieve the first ten components. However, given the length constraint, we only present the weights maps associated to the top three components for Sparse PCA and SPCA-TV in this paper. ElasticNet PCA, GraphNet PCA and SSPCA 's weights maps of experiments are presented in the supplementary materials (supplementary materials are available in the supplementary files /multimedia tab)."}, {"section_title": "A. Simulation Study", "text": "We generated 50 sets of synthetic data, each composed of 500 images of size 100 \u00d7 100 pixels. Images are generated using the following noisy linear system:\nwhere V = [V 1 , V 2 , V 3 ] \u2208 R 10 000\u00d73 are sparse and structured loading vectors, illustrated in Fig. 1 . The support of V 1 defines the two upper dots, the support of V 2 defines the two lower dots, while V 3 's support delineates the middle dot. The coefficients u = [u 1 , u 2 , u 3 ] that linearly combine the components of V are generated according to a centered Gaussian distribution. The elements of the noise vector are independent and identically distributed according to a centered Gaussian distribution with a 0.1 signal-to-noise ratio (SNR). This SNR was selected by a previous calibration pipeline, where we tested the efficiency of data reconstruction at multiple SNR ranges running from 0 to 0.5. We decided to work with a 0.1 SNR because it is located in the range of values where standard PCA starts being less efficient in the recovery process.\nWe splitted the 500 artificial images into a test and a training set, with 250 images in each set and learned the decomposition on the training set. Fig. 2 represents the loading vectors extracted with one data set. Please note that the sign is arbitrary. Indeed, if we consider the loss of Eq. (3), u and v can be both multiply by \u22121 without changing anything. We observe that Sparse PCA yields very scattered loading vectors. The loading vectors of SPCA-TV, on the other hand, are sparse; but also organized in clear regions. SPCA-TV provides loading vectors that closely match the ground truth. The reconstruction error is evaluated on the test sets (Tab. I), with its value over the 50 data sets being significantly lower in SPCA-TV than in Sparse PCA (T = 94.5, p = 3.9 \u00b7 10 \u221257 ), ElasticNet PCA (T = 33.2, p = 2.7 \u00b7 10 \u221235 , GraphNet PCA (T = 12.7, p = 3.6 \u00b7 10 \u221217 and SSPCA from [29] (T = 18.9, p = 3.9\u00b710 \u221224 )) methods. Additional details concerning the reconstruction accuracy of both the train and test data is presented in Figure 1 of supplementary materials (supplementary materials are available in the supplementary files /multimedia tab). A different way of quantifying the reconstruction accuracy for each method is to evaluate how closely the extracted loadings match the known ground truth of simulated data set. We computed the mean squared error (MSE) between the ground truth and the estimated loadings. The results are presented in Tab. I. We note that the MSE is significantly lower with SPCA-TV than with Sparse PCA (T = 6.9, p = 8.0 \u00b7 10 \u22129 ), ElasticNet PCA (T = 6.2, p = 1.1 \u00b7 10 \u221207 ), GraphNet-PCA (T = 4.1, p = 1.4 \u00b7 10 \u221204 ) and SSPCA (T = 22.6, p = 1.5 \u00b7 10 \u221227 ).\nMoreover, when evaluating the stability of the loading vectors across resampling, we found a higher statistically significant mean Dice index when using SPCA-TV compared to the other methods ( p < 0.001). The results are presented in Tab. I. They indicate that SPCA-TV is more robust to variation in the learning samples than the other sparse methods. SPCA-TV yields reproducible loading vectors across data sets.\nThese results indicate that the SPCA-TV loadings are not only more stable across resampling but also achieve a better recovery of the underlying variability in independent data than the Sparse PCA, ElasticNet PCA,GraphNet PCA and SSPCA methods.\nOne of the issues linked to biconvex optimization is the risk of falling into locals minima. Conscious of this potential risk, we set up an experiment in which we ran 50 times the optimization of the same problem, with a different starting point at each run. We then compare the resulting loading vectors obtained at each run, and computed a similarity measure, the Dice index. It quantifies the proximity between each independently-run solution with a different starting point. We obtained a Dice index of 0.99 on the 1st component, 0.99 on the 2nd component, and 0.72 on the 3rd component. Off the strength of this indices, we are confident of this algorithm robustness and ability to converge toward the same stable solution independently from the choice of the starting point."}, {"section_title": "B. 3D Images of Functional MRI of Patients With Schizophrenia", "text": "We then applied the methods on 3D images of BOLD functional MRI (fMRI) acquired with the same scanner and pulse sequence. Imaging was performed on a 1.5 T scanner using a standard head-coil. For all functional scans the field-ofview was 206 * 206 * 153 mm, with a resolution close to 3.5 mm in all directions. The parameters of the PRESTO sequence were TE = 9.6 ms, TR = 19.25 ms, EPI-factor = 15, flip angle = 9. Each fMRI run consisted of 900 volumes collected. The cohort is composed of 23 patients with schizophrenia (average age = 34.96 years, 8 Females/15 Males). Brain activation was measured while subjects experienced multimodal hallucinations. The fMRI data was pre-processed using SPM12, (WELLCOME Department of Imaging Neuroscience, London, UK). Data preprocessing consisted of motion correction (realignment), coregistration of the individual anatomical T1 image to the functional images, spatial normalization to MNI space using DARTEL based on segmented T1 scans.\nWe considered each set of consecutive images under prehallucinations state as a block. Since, most of the patients hallucinate more than once during the scanning session, we have more blocks than patients (83 blocks). The activation maps are computed from these blocks. Based on the general linear model approach, we regressed for each block, the fMRI signal time course on a linear ramp function: Indeed, we hypothesized that activation in some regions presents a ramp-like increase during the time preceding the onset of hallucinations. (See example of regression in figure 3 in the supplementary materials,available in the supplementary files /multimedia tab.). The activation maps that we used as an input to the SPCA-TV method are the statistical parametric maps associated with the coefficients of the block regression. (See one example in Figure 4 of supplementary materials, available in the supplementary files /multimedia tab). We obtained a data set of n = 83 maps and p = 63 966 features. We hypothesized that the principal components extracted with SPCA-TV from these activation maps could uncover major trends of variability within pre-hallucination patterns. Thus, they might reveal the existence of subgroups of patients, according to the sensory modality (e.g., vision or audition) involved during hallucinations.\nWe applied all PCA methods under study to this data set except SSPCA. Indeed, the SSPCA method ( [29] ) could not be applied to this specific example since datasets have to be constituted from closed cubic forms without any holes to be eligible for SSPCA method application. It does not support masked data such as the one used here.\nThe loading vectors extracted from the activation maps of pre-hallucinations scans with Sparse PCA and SPCA-TV are presented in Fig. 3 . We observe a similar behavior as in the synthetic example, namely that the loading vectors of Sparse PCA tend to be scattered and produce irregular patterns. However, SPCA-TV seems to yield structured and smooth sources of variability, which can be interpreted clinically. Furthermore, the SPCA-TV loading vectors are not redundant and revealed different patterns.\nIndeed, the loading vectors obtained by SPCA-TV are of great interest because they revealed insightful patterns of variability in the data: the second loading is composed of interesting areas such as the precuneus cortex and the cingulate gyrus, but also areas related to vision-processing areas such as the occipital fusiform gyrus and the parietal operculum cortex regions. The third loading reveals important weights in the middle temporal gyrus, the parietal operculum cortex and the frontal pole. The first loading vector encompasses all features of the brain. One might see this first component as a global variability affecting the whole brain, such as the overarching effect of age. SPCA-TV selects this dense configuration in spite of the sparsity constraint: It is highly desirable to remove any sort of global effect at first, in order to start identifying local patterns in next components, that are not impacted by a global variability of this kind. We can identified a widespread set of dysfunctional language-related or vision-related areas that present increasing activity during the time preceding the onset of hallucinations. The regions extracted by SPCA-TV are found to be pertinent according to the existing literature on the topic ( [7] , [27] , [28] ).\nThese results seem to indicate the possible existence of subgroups of patients according to the hallucination modalities involved. An interesting application would be to use the score of the second component extracted by SPCA-TV in order to distinguish patients with visual hallucinations from those suffering mainly from auditory hallucinations.\nThe reconstruction error is significantly lower in SPCA-TV than in Sparse PCA (T = 13.9, p = 1.5 \u00b7 10 \u22124 ), ElasticNet PCA (T = 7.1, p = 2.1 \u00b7 10 \u22123 ) and GraphNet PCA (T = 4.6, p = 1.0 \u00b7 10 \u22122 ). Moreover, when assessing the stability of the loading vectors across the folds, we found a higher statistically significant mean Dice index in SPCA-TV compared to: Sparse\nand GraphNet PCA ( p = 2.0 \u00b7 10 \u22123 ) as presented in Tab. II. Additional details regarding the reconstruction accuracy on both the train and test sets, and the Dice index, is presented in Figure 5 of supplementary materials (available in the supplementary files /multimedia tab).\nIn conclusion, SPCA-TV significantly outperforms Sparse, ElasticNet and GraphNet PCA in terms of the reconstruction error on independent test data, and in the sense that loading vectors are both more clinically interpretable and more stable.\nWe also evaluated the convergence speed of Sparse PCA, Mini-Batch Sparse PCA (a variant of Sparse PCA that is faster but less accurate), ElasticNet PCA, GraphNet PCA and SPCA-TV for this functional MRI data set of n = 83 samples and p = 63 966 features. We compared the time of execution required for each algorithm to achieve a given level of precision in Tab. III. Sparse PCA and ElasticNet PCA are similar in terms of convergence time, while mini-batch sparse PCA is much faster but does not converge to high precision. As expected, structured methods (GraphNet PCA and SPCA-TV) take longer than other sparse methods because of the inclusion of spatial constraints. Especially, SPCA-TV is much longer than other methods but the convergence time is still reasonable for an fMRI data set with 65 000 voxels."}, {"section_title": "C. Surfaces Meshes of Cortical Thickness in Alzheimer Disease", "text": "Finally, SPCA-TV was applied to the whole brain anatomical MRI from the ADNI database, the Alzheimer's Disease Neuroimaging Initiative, (http://adni.loni.usc.edu/). The MR scans are T1-weighted MR images acquired at 1.5 T according to the ADNI acquisition protocol. We selected 133 patients with a diagnosis of mild cognitive impairments (MCI) from the ADNI database who converted to AD within two years during the follow-up period. We used PCA to reveal patterns of atrophy explaining the variability in this population. This could provide indication of possible stratification of the population into more homogeneous subgroups, that may be clinically similar, but with different brain patterns. In order to demonstrate the relevance of using SPCA-TV to reveal variability in any kind of imaging data, we worked on meshes of cortical thickness. The 317 379 features are the cortical thickness values at each vertex of the cortical surface. Cortical thickness represents a direct index of atrophy and thus is a potentially powerful candidate to assist in the diagnosis of Alzheimer's disease ( [3] , [17] ). Therefore, we hypothesized that applying SPCA-TV to the ADNI data set would reveal important sources of variability in cortical thickness measurements. Cortical thickness measures were performed with the FreeSurfer image analysis suite (Massachusetts General Hospital, Boston, MA, USA), which is documented and freely available for download online (http://surfer.nmr.mgh. harvard.edu/). The technical details of this procedure are described in [2] , [13] , and [46] . All the cortical thickness maps were registered onto the FreeSurfer common template (fsaverage).\nWe applied all PCA methods under study to this data set except SSPCA. Indeed, we could not applied SSPCA method to this data set due to some intrinsic limitations of the method. SSPCA 's application is restricted to N-dimensional array images. It does not support meshes of cortical surfaces such as the data set used here.\nThe loading vectors obtained from the data set with sparse PCA and SPCA-TV are presented in Fig. 4 . As expected, Sparse PCA loadings are not easily interpretable because the patterns are irregular and dispersed throughout the brain surface. In contrast, SPCA-TV reveals structured and smooth clusters in relevant regions.\nThe first loading vector, which maps the whole surface of the brain, can be interpreted as the variability between patients, resulting from a global cortical atrophy, as often observed in AD patients. The second loading vector includes variability in the entorhinal cortex, hippocampus and in temporal regions. Last, the third loading vector might be related to the atrophy of the frontal lobe and captures variability in the precuneus too. Thus, SPCA-TV provides a smooth map that closely matches the well-known brain regions involved in Alzheimer's disease. [22] Indeed, it is well-documented that cortical atrophy progresses over three main stages in Alzheimer disease. ( [10] , [15] ) The cortical structures are sequentially being affected because of the accumulation of amyloid plaques. Cortical atrophy is first observed, in the mild stage of the disease, in regions surrounding the hippocampus ( [26] , [44] , [47] ) and the enthorinal cortex ( [12] ), as seen in the second component. This is consistent with early memory deficits. Then, the disease progresses to a moderate stage; where atrophy gradually extends to the prefrontal association cortex as revealed in the third component ( [37] ). In the severe stage of the disease, the whole cortex is affected by atrophy ( [15] ) (as revealed in the first component). In order to assess the clinical significance of these weight maps; we tested the correlation between the scores corresponding to the three components and performance on a clinical test: ADAS. The Alzheimer's Disease Assessment Scale-Cognitive subscale, is the most widely used general cognitive measure in AD. ADAS is scored in terms of errors, so a high score indicates poor performance. We obtained significant correlations between ADAS test performance and components 'scores in Fig. 5 . r = \u22120.34, p = 4.2 \u00b7 10 \u221211 for the first component, r = \u22120.26, p = 3.6 \u00b7 10 \u22127 for the second component and r = \u22120.35, p = 4.0\u00b74.5 \u221212 for the third component) The same behavior is observable for all three components: The ADAS score grows proportionately to the level to which a patient is affected and to the severity of atrophy he presents (in temporal pole, prefrontal region and also globally). Conversely, controls subjects score low on the ADAS metric and present low level of cortical atrophy. Therefore, SPCA-TV provides us with clear biomarkers, that are perfectly relevant to the scope of Alzheimer's disease progression.\nThe reconstruction error is significantly lower in SPCA-TV than in Sparse PCA (T = 12.7, p = 2.1 \u00b7 10 \u22124 ), ElasticNet PCA (T = 6.8, p = 2.3\u00b710 \u22123 ) and GraphNet PCA (T = 2.83, p = 4.7\u00b710 \u22122 ). The results are presented in Tab. IV. Moreover, when assessing the stability of the loading vectors across the folds, the mean Dice index is significantly higher in SPCA-TV than in other methods. Additional details regarding the reconstruction accuracy on both the train and test sets, and the Dice index, is presented in Figure 7 of supplementary materials (available in the supplementary files /multimedia tab)."}, {"section_title": "IV. CONCLUSION", "text": "We proposed an extension of Sparse PCA that takes into account the spatial structure of the data. The optimization scheme is able to minimize any combination of the 1 , 2 , and TV penalties while preserving the exact 1 penalty. We observe that SPCA-TV, in contrast to other existing sparse PCA methods, yields clinically interpretable results and reveals major sources of variability in data, by highlighting structured clusters of interest in the loading vectors. Furthermore, SPCA-TV 's loading vectors were more stable across the learning samples compared to other methods. SPCA-TV was validated and its applicability was demonstrated on three distinct data sets: we may reach the conclusion that SPCA-TV can be used on any kind of structured configurations, and is able to present structure within the data."}]