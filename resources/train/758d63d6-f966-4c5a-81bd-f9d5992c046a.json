[{"section_title": "Abstract", "text": "Abstract. We present a disease progression model with single vertex resolution that we apply to cortical thickness data. Our model works by clustering together vertices on the cortex that have similar temporal dynamics and building a common trajectory for vertices in the same cluster. The model estimates optimal stages and progression speeds for every subject. Simulated data show that it is able to accurately recover the vertex clusters and the underlying parameters. Moreover, our clustering model finds similar patterns of atrophy for typical Alzheimer's disease (tAD) subjects on two independent datasets: the Alzheimer's Disease Neuroimaging Initiative (ADNI) and a cohort from the Dementia Research Centre (DRC), UK. Using a separate set of subjects with Posterior Cortical Atrophy (PCA) from the DRC dataset, we also show that the model finds different patterns of atrophy in PCA compared to tAD. Finally, our model also provides a novel way to parcellate the brain based on disease dynamics."}, {"section_title": "Introduction", "text": "During the progression of Alzheimer's disease, many biomarkers based on Magnetic Resonance Imaging (MRI) such as cortical thickness become abnormal at Data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wpcontent/uploads/how to apply/ADNI Acknowledgement List.pdf different points in the progression. Finding out the precise temporal evolution of these biomarkers facilitates patient staging in clinical trials. However, the analysis of disease progression is limited by several factors: short number of follow-up visits available, different disease onset and progression speed for every subject and cohort heterogeneity.\nA hypothetical model of disease progression has been proposed by [1] , describing the trajectory of key biomarkers along the progression of Alzheimer's disease. The model suggests that amyloid-beta and tau biomarkers become abnormal long before symptoms appear, followed by brain atrophy measures and cognitive decline. Motivated by this idea, several models such as [2] or [3] have been proposed that reconstruct biomarker trajectories and can be used to stage subjects. However, these models make use of a priori clinical categories, which are noisy, biased and can limit the temporal resolution of the model. This motivates the use of fully data-driven approaches that do not use a priori clinical stages.\nVarious data-driven disease progression modelling techniques have been proposed in recent years. One such model is the Event-Based Model [4] , which models the progression of disease as a sequence of discrete events, representing underlying biomarkers switching from a normal to abnormal state. Other models such as the Disease Progression Score (DPS) [5] or self-modelling regression approaches [6] have been developed, that build continuous trajectories by \"stitching\" together short-term follow-up data. Models estimating linear or logistic trajectories by means of Riemannian manifold techniques have also been recently shown [7] .\nThe main limitation of these data-driven models is that they use a small set of biomarkers that are obtained by averaging MRI or PET measures across all voxels or vertices in a Region of Interest (ROI). This can be problematic, especially if different parts of the ROI, say the hippocampus, are affected at different speeds or timepoints in the disease process. Therefore, moving to a voxel-wise approach would allow one to estimate the fine-grained spatial distribution of atrophy, which could give new insights into the disease process and potentially enable more precise staging. A voxel-wise disease progression model [8] has been recently proposed to mitigate this problem, that uses amyloid measures in each voxel as its input data. However, the model by [8] has two limitations: (1) the biomarker trajectories are assumed to be linear, so cannot capture the plateau effect observed in amyloid-beta or tau and (2) the model uses a spatial correlation function for modelling correlation between voxels; while this is necessary, due to the nature of the imaging data, it has been shown that in different types of dementia atrophy patterns match functional networks, which are not spatially connected [9] .\nIn this work, we present a new disease progression model with single vertex resolution that avoids assumptions on spatial correlation. We combine unsupervised learning and disease progression modelling to identify clusters of vertices on the cortical surface, with no spatial constraints, that show a similar trajectory of atrophy over a particular patient cohort. This formulation enables us to gain new insights into the spatial structure of atrophy in different diseases and also provides a novel parcellation of the brain based on temporal change. Moreover, each cluster of vertices has a corresponding sigmoidal trajectory, which avoids the limitation of linear trajectories in [8] .\nWe first show using simulated data that our model is able to recover known underlying clusters, trajectory parameters and subject stages. We then apply our model to cortical thickness vertex-wise measures using ADNI and the DRC dataset and highlight the new insights the model can give. Finally, we validate our model using cross-validation and by correlating the subject stages with cognitive measures."}, {"section_title": "Methods", "text": ""}, {"section_title": "Model", "text": "We seek to identify groups of image vertices that show a common trajectory during the disease course, while simultaneously placing each visit from each subject within that disease course. In a similar way to [5, 6, 7] , we estimate a time shift and speed (or rate) of progression for each subject. We relate these time shifts and progression speeds by assigning each subject a disease stage which we will refer to as the Disease Progression Score (DPS). In contrast to [5, 6, 7] , which model temporal trajectories for a small set of biomarker measures based on a priori defined ROIs, we model temporal trajectories for each vertex on the cortical surface. Each trajectory is a function of the disease progression score (i.e. disease stage) of a subject. We estimate each subjects' time shift, progression speed and trajectory parameters from the data. The disease progression score s ij for subject i at visit j is defined as a linear transformation of age t ij :\nwhere \u03b1 i and \u03b2 i represent the speed of progression and time shift (i.e. disease onset) of subject i. Our model assumes that the cortical thickness at each vertex on the cortical surface follows a sigmoidal trajectory f (s) given the disease progression score s. We also assume that vertices are grouped into K clusters and we model a unique trajectory for each cluster k \u2208 [1, . . . , K], which will be referred to as cluster trajectories. The sigmoidal function for cluster k is parametrised as\nFor a given subject i at visit j, the value V ij l of its cortical thickness at vertex l is a random variable that has an associated discrete latent variable Z l \u2208 [1, . . . , K] denoting the cluster it was generated from. The value of V ij l given that it was generated from cluster Z l can be modelled as:\nwhere\nrepresents the pdf of the normal distribution that models the measurement noise along the sigmoidal trajectory of cluster Z l , having variance \u03c3 Z l . Next, we assume the measurements from different subjects are independent, while the measurements from the same subject i at different visits j are linked using the disease progression score from equation 1, because we estimate only two parameters (\u03b1 i and \u03b2 i ) using the data from all visits j. Moreover, we also assume a uniform prior on Z l . This gives the following model:\nwhere I = (i, j) represents the set of all the subjects i and their corresponding visits j. Furthermore,\nis the 1D array of all the values for vertex l across every subject and corresponding visit. Vectors \u03b1 = [\u03b1 1 , . . . , \u03b1 S ] and \u03b2 = [\u03b2 1 , . . . , \u03b2 S ], where S is the number of subjects, denote the stacked parameters for the subject shifts.\nwith K being the number of clusters, represent the stacked parameters for the sigmoidal trajectories and measurement noise specific to each cluster.\nWe further assume all vertex measurements to be spatially independent, giving the complete data likelihood:\nwhere\n, L being the total number of vertices on the cortical surface. We recall that we don't want to enforce spatial correlation between vertices as we are interested to see if vertices from distinct areas of the brain are grouped together in the same cluster. Our assumption is also justified by the fact that we smoothed the cortical thickness images in the preprocessing steps. We get the final model log likelihood for incomplete data by marginalising over the latent variables Z:\nTherefore, the parameters that need to be estimated are \u0398 = [\u03b1, \u03b2, \u03b8, \u03c3] where \u03b1 and \u03b2 are the subject specific shifting parameters while \u03b8 and \u03c3 are the cluster specific trajectory and noise parameters."}, {"section_title": "Fitting the Model using EM", "text": "Due to the summing over the latent variables Z, it is not possible to find a closed form solution to the maximum likelihood. Therefore, we fit our model using Expectation-Maximisation, which is suitable given the large number of data points and parameters that need to be estimated."}, {"section_title": "E-step", "text": "In the Expectation step we seek to estimate which cluster has generated each of the L vertices, given the current estimates of the cluster parameters \u03b8 \n, given our independence assumption between vertices. Let us denote by z lk = p(Z l = k|V l , \u0398 old ). We then have:\nIgnoring the normalisation factor, we perform a log transformation and expand the pdfs of the normal distributions. This results in the following update equation for the E-step:\nThe original probabilities can be easily recovered by exponentiating them and then normalising with respect to their sum."}, {"section_title": "M-step In the Maximisation step we try to find", "text": "Since there is no closed-form solution, we perform successive refinements of \u03b8 k for each cluster k and \u03b1 i , \u03b2 i for each subject i until convergence.\nIn order to get the update rule for the trajectory parameters \u03b8 k corresponding to cluster k we need to maximise the expected log likelihood with respect to \u03b8 k . We get the following simplified optimisation problem:\nA similar equation is also obtained for \u03c3 k . After estimating \u03b8 and \u03c3 for every cluster, we use the new values to estimate the subject specific parameters \u03b1 and \u03b2. Let S be the number of subjects and \u03b1 i , \u03b2 i be the rate and shift for subject i \u2208 S. We again maximise the expected log likelihood with respect to \u03b1 i , \u03b2 i independently, and after simplifications we obtain the following problem:\nIn summary, at every single M-step, we iterate between solving for \u03b8, \u03c3 and solving for \u03b1, \u03b2 using numerical optimisation until convergence. Due to the use of numerical optimisation, we are not guaranteed to find the global maxima for the expected log likelihood, but EM still works if we only find an increase in the log-likelihood. This approach that involves a partial M-step is called Generalised EM."}, {"section_title": "Initialisation and Implementation", "text": "Before starting the fitting process, we need to initialise \u03b1, \u03b2 and the clustering probabilities z lk . We set \u03b1 i and \u03b2 i to be 1 and 0 respectively for each subject. We initialise z lk using k-means clustering using vectors V l having |I| number of samples and L features. Furthermore, as already explained in [5] , the scale of the DPS is arbitrary so we standardise the scores at each EM iteration such that the DPS of controls have a mean \u00b5 N of zero and a standard deviation \u03c3 N of 1. This also requires a rescaling of the cluster parameters \u03b8 k .\nIn our implementation, we run the main EM loop until convergence of the clustering probabilities z lk . At each M-step we perform numerical optimisation using the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm that makes use of the first derivative of the objective function. In all datasets analysed, the method converges after maximum 25 EM iterations."}, {"section_title": "Simulation Results", "text": "We first tested the model in a toy scenario using synthetic data, which we generated as follows: (1) sampled age and shift parameters from 300 subjects with 4 timepoints (each timepoint 1 year apart), with t i1 \u223c U (40, 80), \u03b1 i \u223c N (1, 0.05), \u03b2 i \u223c N (0, 10) (2) generated three sigmoids with different center points and slopes (Fig. 1a , red lines) (3) generated a random cluster assignment for L = 1, 000 vertices, i.e. every vertex l was assigned to a cluster a[l] \u2208 {1, 2, 3} (4) sampled a set of L perturbed trajectories \u03b8 l from each of the original trajectories, one for each vertex (Fig. 1a, gray lines) and (5) sampled subject data for every vertex l from its corresponding perturbed trajectory \u03b8 l with \u03c3 l = 0. 5 To fit the data we used a uniform prior on the parameters \u03b8 and \u03c3 and an informative prior on \u03b1 and \u03b2, with p(\u03b1 i ) \u223c \u0393 (49, 70) and \u03b2 i \u223c N (0, 1). We also normalised age to have a mean of 0 and standard deviation of 1 and rescaled the DPS and cluster trajectories accordingly. After convergence, we calculated the agreement between the final clustering probabilities and the true clustering\nThe method also requires us to set the number of clusters a priori, so we optimised the number of clusters using the Bayesian Information Criterion (BIC).\nThe BIC analysis correctly predicted three clusters for this synthetic experiment. Using three clusters, we also obtained a clustering agreement of A = 1.0. Figure 1a shows the original trajectories and the recovered trajectories using our model, plotted against the disease progression score on the x-axis and the vertex value on the y-axis. Moreover, in Fig. 1b we plotted the recovered DPS of each subject along with the true DPS.\nThe results show that the model accurately estimated which clusters generated each vertex. Moreover, the recovered trajectories are close to the true trajectories, with some errors for the trajectories corresponding to clusters 1 and 2. The recovered DPS also shows good agreement with the true DPS, with the exception of a few subjects with DPS greater than 2.5. This is explained by the fact that there is not enough signal in that DPS range in terms of trajectory dynamics (i.e. trajectories are mostly flat). The simulation confirms that our model is able to recover the hidden clusters, trajectory parameters and the subject specific parameters. However, more realistic simulations with varying noise levels and numbers of clusters are required to understand the limitations of the model and find out when it fails to recover the true parameters. "}, {"section_title": "Experimental Results", "text": ""}, {"section_title": "Data Acquisition and Preprocessing", "text": "Data used in this work were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu) and from the Dementia Research Centre, UK. For ADNI, we downloaded all T1 MRI images that have undergone gradwarping, intensity correction and scaling for gradient drift. We included subjects that had at least 4 scans, in order to ensure we get a robust estimate of the subject specific parameters. This resulted in 328 subjects with an average number of 4.95 scans each. The DRC dataset consisted of T1 MRI scans from 31 healthy controls, 32 PCA and 23 typical typical AD subjects with at least 3 scans each and an average of 5.26 scans per subject. On both datasets, in order to extract reliable cortical thickness measures we ran the Freesurfer longitudinal pipeline [10] , which first registers the MRI images to an unbiased within-subject template space using inverse-consistent registration. The longitudinally registered images were then registered to the average Freesurfer template and smoothed at a full-width/half-max (FWHM) level of zero. For each vertex we averaged the thickness levels from both hemispheres. Finally, we standardised the data from each vertex with respect to the values of that vertex in the control population. Each of the final images had a resolution of 163,842 vertices on the cortical surface."}, {"section_title": "ADNI and DRC Results", "text": "Using ADNI and DRC datasets, we were interested to find out the spatial distribution of cortical atrophy, as well as the rate and timing of this atrophy process. In particular, we wanted to find out: (1) if we get similar results using our model on two independent tAD datasets: ADNI and DRC and (2) if we get different patterns of atrophy on distinct diseases (tAD and PCA) that match previous studies.\nBIC analysis predicted that the optimal number of clusters is two for the ADNI cohort and three for both tAD and PCA subjects from the DRC cohort. In order to make the results easily comparable across the different datasets, we ran all experiments using 3 clusters. Fig. 2a shows the results of our model using all ADNI subjects, where we coloured points on the cortical surface according to the cluster they most likely belong to. We assigned a colour to each cluster according to the slope of its corresponding trajectory, ranging from red (high slope suggesting a fast rate of atrophy) to blue (low slope suggesting a slow rate of atrophy). In Fig. 2d we also show the resulting cluster trajectories with samples from the posterior distribution of each \u03b8 k . We repeated the same analysis on the DRC cohort, separately for the tAD subjects ( Fig. 2b and 2e ) and PCA subjects ( Fig. 2c and 2f) .\nWe notice that in tAD subjects using both ADNI and DRC datasets (Fig.  2) , there is widespread atrophy in most temporal, parietal and frontal areas (red cluster), with the notable exception of the motor cortex and the occipital lobe. These patterns of atrophy are similar across the two different datasets. Moreover, the spatial distribution of cortical thinning found with our technique resembles results from previous longitudinal studies such as [11] . However, in contrast to these approaches, our model gives insight into the timing, rate and extent of atrophy and is also able to stage subjects across the disease timecourse.\nIn the PCA subjects (Fig. 2c) , we find that the atrophy is more focused on the posterior part of the brain, mostly the posterior parietal and occipital, with more limited spread in the superior temporal and inferior frontal. This is in contrast with the tAD patterns in the other datasets, that lacks the focus on posterior parietal and occipital regions. This posterior pattern of atrophy also matches previous findings in the literature [12] . For all datasets, we find that the cluster trajectories differ less in timing and more in the slope and minima/maxima values at which they plateau (Fig. 2d, 2e, 2f) . Our model therefore predicts that regions on the cortical surface are all affected roughly at the same time, but the rate and extent to which they are affected is different. The same analysis is shown also for (b, e) tAD subjects from the DRC cohort and (c, f) PCA subjects from the DRC cohort."}, {"section_title": "Model Validation", "text": "We tested robustness of the model by performing 10-fold cross validation (CV) on ADNI. Our motivation was to test the following: (1) Fig 3 shows the clusters that were estimated at each fold from the training data only. Moreover, in Fig. 4 we plot the estimated DPS (i.e. disease stage) of each subject from the test set against their age.\nThe results in Fig. 3 prove that the model is robust in cross-validation, as the estimated clusters are all very similar across folds. The average Dice scores we obtained across all pairs of folds were 0.89, 0.89 and 0.90 for clusters 0, 1 and 2 respectively. Furthermore, 84% of the subjects analysed show increased stages across their follow-up visits, proving that the estimated stages are mostly consistent. Finally, the stages of test subject correlate with clinical measures such as CDRSOB (\u03c1 = 0.41, p < 1e\u221266), ADAS-COG (\u03c1 = 0.40, p < 1e\u221262), MMSE (\u03c1 = 0.39, p < 1e \u2212 58) and RAVLT (\u03c1 = 0.35, p < 1e \u2212 46), demonstrating that the stages have clinical validity. As before, each cluster is coloured according to the slope of its corresponding trajectory, from red (high rate of atrophy) to blue (low rate of atrophy). "}, {"section_title": "Discussion", "text": "We presented a model of disease progression that clusters vertex-wise measures of cortical thickness based on similar temporal dynamics. The model highlights, for the first time, groups of cortical vertices that exhibit a similar temporal trajectory over the population. This provides a new way to parcellate the brain that is specific to the temporal trajectory of a particular disease. The model also finds the optimal temporal shift and progression speed for every subject. We applied the model to cortical thickness vertex-wise data from the ADNI and DRC cohorts. Our model found similar patterns of atrophy dynamics in the tAD subjects using the two independent datasets. Moreover, it also found different patterns of atrophy dynamics on two distinct diseases: tAD and PCA.\nThe model has some limitations. First of all, we assumed that cluster trajectories follow sigmoidal shapes, which might not be the case for many types of biomarkers such as cortical thickness. Another limitation of the model is that it assumes all subjects follow the same disease progression pattern, which might not be the case in heterogeneous datasets such as ADNI or DRC. This can be a concern, as there might be a pattern of atrophy that occurs in a small set of subjects. Moreover, our cluster-based model might miss atrophy patterns that occur is very small regions. Furthermore, the data we analysed has been standardised with respect to controls, which assumes controls don't show any biomarker abnormalities.\nThere are several potential avenues of future research. While we only used the model for studying cortical thickness, one can also apply it to other types of data such as amyloid images or Jacobian compression maps. On the methodological side, the assumption of sigmoidal trajectories can be avoided using nonparametric curves such as Gaussian Processes. Another extension is to model different progression dynamics for distinct subgroups using unsupervised learning methods like the approach of [13] , or incorporate subject-specific deviations from the standard pattern of atrophy using a mixed-effects model.\nOur approach can be used for accurately predicting and staging patients across the progression timeline of neurodegenerative diseases. This is promising for patient prognosis, as well as in clinical-trials for assessing efficacy of a putative treatment for slowing down the degeneration process."}]