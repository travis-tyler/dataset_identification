[{"section_title": "TABLE OF CONTENTS (continued)", "text": "List of Tables (continued)   Table  Page 5-5 IRT theta (ability) means and standard deviations by subpopulation, six data collection rounds plus bridge sample: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 IRT parameters for reading and mathematics proficiency levels, based on items from kindergarten, first-grade, third-grade, and fifth-grade assessments: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 ......\nAppendix A Tables (continued)   Table  Page A34 Probability of proficiency, mathematics level 6: place value (range of possible values: 0.0 to 1.0): School years 1998School years -99, 1999School years -2000School years , 2001School years -02 and 2003 A35 Probability of proficiency, mathematics level 7: rate and measurement (range of possible values: 0.0 to 1.0): School years 1998School years -99, 1999School years -2000School years , 2001School years -02 and 2003 A36 Probability of proficiency, mathematics level 8: fractions (range of possible values: 0.0 to 1.0): School years 1998School years -99, 1999School years -2000School years , 2001School years -02 and 2003 A37 Probability of proficiency, mathematics level 9: area and volume (range of possible values: 0.0 to 1.0): School years 1998School years -99, 1999School years -2000School years , 2001School years -02 and 2003 School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 A39 Percent of children at or above modal mathematics proficiency for each grade: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Appendix B Tables   Table   B1 Reading assessment IRT item parameters: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 "}, {"section_title": "5-12", "text": ""}, {"section_title": "B2", "text": "Mathematics assessment IRT item parameters: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Table  Page   C1 Reading assessment estimated proportion correct: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003"}, {"section_title": ".. C-1 C2", "text": "Reading assessment estimated proportion correct: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 C3 Science assessment estimated proportion correct: School years 2001-02 and 2003-04..................................................................................................... C-10 Appendix D Tables   Table  Page  1. INTRODUCTION This report documents the design, construction, and psychometric characteristics of the assessment instruments used in the spring 2004 data collection of the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K). The ECLS-K is sponsored by the U.S. Department of Education, National Center for Education Statistics. The ECLS-K was designed to assess the relationship between a child's academic and social development and a wide range of family, school, and community variables. Analysis of the cognitive and social skills assessment scores described in this report, along with contextual variables in the ECLS-K database collected from schools, parents, teachers, and children, provides a basis for policy-relevant examination of growth rates, school influences, and subgroup differences in achievement and growth. While the ECLS-K spans kindergarten through fifth grade, this report documents the psychometric results for the sixth round of data collection, in spring 2004, when approximately 90 percent of the sampled children were in fifth grade. Also included is a review of the salient features of the assessments used in kindergarten through third grade. Among these salient features are the selection and design of assessment instruments and selected psychometric characteristics. Two domains are represented by the ECLS-K fifth-grade assessment instruments: cognitive (direct and indirect) and socioemotional. Direct cognitive measures refer to scores based on children's \"direct\" responses to cognitive test items. In fifth grade, direct cognitive tests were administered in reading, mathematics, and science. Indirect cognitive measures were ratings by teachers of the children's cognitive performance in the areas of language and literacy, mathematical thinking, science, and social studies. The socioemotional measures were teachers' ratings of children's social skills and approaches to learning. A questionnaire administered to the children included both indirect cognitive measures (selfratings of competence in reading, mathematics, and all school subjects) and socioemotional questions relating to peer relationships and problem behaviors. The direct cognitive assessments for fifth grade were designed to measure an individual child's knowledge at a given point in time, as well as that same child's academic growth in each subject on vertical score scales based on successive assessments. The score scales for reading and mathematics 1-2 measure growth from fall-kindergarten through fifth grade, while the science assessment was administered only in the third-and fifth-grade rounds. The cognitive assessments were designed not only to make reliable normative comparisons with respect to status and growth, but also to provide criterion-referenced interpretations. That is, in the reading and mathematics content domains, criterion-referenced proficiency scores can be used to describe a given child's mastery of specific knowledge and skills that mark ascending critical points on the developmental growth curve. These multiple criterion-referenced levels serve two functions. First, they help with respect to the interpretation of what a particular attained score level means in terms of what a child can or cannot do. Second, they are useful in measuring change at particular points along the score scale. They provide a means of evaluating the relationship of certain school processes to changes in mastery of specific skills. The development of the direct cognitive battery was carried out in five steps: 1. A background review was carried out of all the currently available psychometric instruments and the constructs that they purported to measure."}, {"section_title": "2.", "text": "Test specifications were developed that were appropriate to the domains and constructs considered relevant for each grade."}, {"section_title": "3.", "text": "Item pools were developed that reflected the test specifications in step 2."}, {"section_title": "4.", "text": "The item pools were field tested in order to gather statistical and psychometric evidence as to the appropriateness of the items for carrying out the overall assessment goals."}, {"section_title": "5.", "text": "The final test forms were assembled consistent with field test item statistics and the test specifications. Chapter 2 of this report describes the objectives and design of the fifth-grade assessment instruments. Differences between the kindergarten-first grade (K-1), third-grade, and fifth-grade assessment batteries are described. For the direct cognitive tests, chapter 2 includes selection of content domains, notes on frameworks, descriptions of field testing, and selection of test items. It describes the criterion-referenced subsets of items in the reading and mathematics tests that were used to mark proficiency levels in kindergarten through third grade and the extension of these levels for fifth-grade skills. Chapter 2 also describes the evaluation of potential gaps in the longitudinal scale for the years in which data were not collected, second and fourth grades, and the steps taken to avoid compromising measurement of gains. For the indirect measures, chapter 2 describes the development and content of the 1-3 instruments used by teachers to rate children's academic and social skills as well as the instrument used by children to rate their own academic ability and interest, and their behavior and relationships with peers. Chapter 3 contains a description of the quality control procedures applied to analysis of the assessment data, as well as an overview of item response theory (IRT) procedures used in computing test scores and the differential item functioning (DIF) procedures used to detect problem items. Chapter 4 presents the psychometric characteristics of the direct cognitive tests given in fifth grade, and chapter 5 describes their role in longitudinal measurement. Chapter 6 describes the development and psychometric characteristics of the Self-Description Questionnaire administered to sampled children, and chapter 7 presents the same information for the teacher indirect cognitive and social rating scale measures. A national probability sample of about 22,000 children in about 800 public and 200 private schools was assessed at entry to kindergarten in fall 1998 (round 1). They were followed up in springkindergarten (round 2), fall-and spring-first grade (rounds 3 and 4, respectively), spring-third grade (round 5), and spring-fifth grade (round 6). The third round (fall-first grade) was a subsample of about 30 percent of the base-year kindergarten schools. The sixth round of data collection described in this report took place in spring 2004, when approximately 90 percent of the children were in fifth grade. The direct cognitive assessments were conducted in all six rounds of data collection, while the indirect cognitive and socioemotional measures were collected from teachers in rounds 1, 2, 4, 5, and 6 (fall-and springkindergarten, spring-first grade, spring-third grade, spring-fifth grade), and from parents in rounds 1, 2, and 4. In rounds 5 and 6, children completed a direct socioemotional measure. More details on the sample design and data collection methods used in the ECLS-K can be found in the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), Combined User's Manual for the ECLS-K Fifth-Grade Data Files and Electronic Codebooks (NCES 2006-032) (Tourangeau et al. forthcoming). Sample counts, completion rates, psychometric characteristics, and score statistics for the fifth-grade assessments are presented in chapter 4 (direct measures) and chapter 6 (indirect measures), with score breakdowns by sex, race/ethnicity, socioeconomic status, and school type in appendix A. Additional information about the sample design, the assessment instruments, and the collection of assessment data can be found in the ECLS-K electronic codebook and data file users' manuals. Statistics presented in this report may differ slightly from those in the data file users' manual. Tables in the users' manual are based on the panel sample, that is, children who participated in all six rounds of data collection, with national estimates computed using the longitudinal panel weight (C1_6SC0). The emphasis in this report is on the psychometric characteristics of the tests at each round, so all children participating in each round are included, and the corresponding cross-sectional weights, (C1CW0-1-4 C6CW0) are used for national estimates. Statistics that report characteristics of the tests rather than national estimates, such as reliabilities or floor and ceiling effects, are unweighted. Detailed information on the assessments used in the earlier rounds can be found in the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), Psychometric Report for the Third Grade (NCES 2005-062) (Pollack et al. 2005). 2-1"}, {"section_title": "DESIGN AND DEVELOPMENT OF THE ASSESSMENT INSTRUMENTS", "text": "The Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS- The National Center for Education Statistics (NCES) and contractor staff assembled school curriculum specialists, teachers, and academicians to consult on the design and development of the assessment instruments. Issues that were addressed included domains to be covered, test specifications, individual item content and presentation, mode of assessments, and time allocation. The advice of these experts guided the decisions necessary to ensure valid representation of domain content and to make efficient use of resources while minimizing burden on teachers and students. The fifth grade direct cognitive assessments built on the structure established in the kindergarten through third-grade rounds of data collection. Individually administered assessments were conducted for the direct cognitive measures, while teachers provided indirect reports of children's academic skills, attitudes, and behaviors. The third-grade assessment battery differed from that of kindergarten and first grade (K-1) in several important respects. The English language screening assessment, parent questionnaire, and psychomotor assessment used in kindergarten and/or first grade were not included in the third grade assessment battery. A questionnaire eliciting children's academic and behavioral self-ratings was added in third grade, and a science assessment replaced the K-1 general knowledge test. The content and components of the fifth-grade instruments were essentially similar to those used in third grade, with ageappropriate increments in the difficulty of test items. Important changes in the assessments during the course of the longitudinal study are described here: No English language screening: In kindergarten and first grade, children who were identified as coming from a language minority background were administered an 2-2 English language screening assessment, the Oral Language Development Scale (OLDS), prior to administration of the direct cognitive assessments. Once each child achieved a score sufficient for assessment in English, the OLDS was not administered to that child in subsequent rounds of data collection. At kindergarten entry, about 15 percent of the ECLS-K participants were found to need screening for English proficiency. By spring of first grade, less than 6 percent of the sample were screened, and nearly two-thirds of the screened children achieved the score required to go on to the rest of the assessment. Since no freshening of the sample occurred after first grade, the number of sampled children who might still lack English proficiency two and four years later, in third and fifth grades, was assumed to be so small that the language screening assessment would be unnecessary. Therefore, an English language screener was not administered after spring-first grade. No parent questionnaire items on children's social behaviors: Parents' ratings of children's behavior and social skills had been collected during kindergarten and firstgrade rounds. These ratings were deleted from parent information collected in third and fifth grades for several reasons: age appropriateness of the instrument, technical issues (low intercorrelations among parent scales), and the need to minimize burden on participants."}, {"section_title": "No psychomotor assessment:", "text": "The fall-kindergarten assessment battery included an evaluation of children's fine and gross motor skills. This assessment was designed as a baseline measure and was not repeated in subsequent rounds of data collection."}, {"section_title": "Self-Description Questionnaire (SDQ):", "text": "In third and fifth grades, children were asked to rate their own academic competence and interest and to report on their relationships with peers. See section 2.3 for more details. Changes in the content and format of the direct cognitive assessment instruments: New reading, mathematics, and science assessment forms were developed for the fifth grade. A science assessment, begun in third grade, replaced the direct cognitive assessment of general knowledge that had been used in kindergarten and first grade. Assessment formats in fifth grade were similar to the earlier rounds, but some modifications were made to accommodate the content of the questions. A Spanish translation of the mathematics assessment, used in kindergarten and first grade, was assumed to be unnecessary for third and fifth grades. 1 Additional scores were defined that targeted fifth-grade skills. Details of these changes are described in sections 2.1 and 2.2. Changes in the indirect cognitive assessment instruments: Separate teacher ratings of science and social studies skills in third grade replaced the K-1 general knowledge ratings. In fifth grade, the social studies section was eliminated in order to reduce teacher burden. Another change in the longitudinal design of ECLS-K was the elimination of the secondand fourth-grade rounds of data collection due to budgetary constraints. The implications of this decision, and the steps taken to minimize its impact on longitudinal measurement, are discussed in sections 2.1.5 and 5.1 of the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), Psychometric Report for the Third Grade (NCES 2005-062) (Pollack et al. 2005)."}, {"section_title": "Direct Cognitive Assessment", "text": "The child development and primary education experts consulted by project staff during the design phase of the ECLS-K recommended that the knowledge and skills assessed by the ECLS-K tests should represent the typical and important cognitive goals of elementary schools' curricula. Therefore, the subject-matter domains of language and literacy skills (referred to hereafter simply as \"reading\" for the direct cognitive assessment), mathematics, and science were selected for the fifth-grade direct cognitive battery. Time constraints and concern about burden on children as well as differences in social studies curricula throughout the states led to a decision not to include a social studies assessment in the direct cognitive battery. The practical difficulties of adequately assessing children's proficiencies in writing, art, and music within the resource constraints of the study precluded assessment in these domains. The nature of the ECLS-K cognitive assessment battery was shaped by its basic objectives and constraints. Foremost among these was the requirement that the test battery accurately measure children's cognitive development in reading and mathematics throughout the whole span of the study, and in science between third and fifth grades. The longitudinal design of the study required the development of vertical scales in each subject to support valid change scores. Such scales would allow comparisons of achievement levels across grades and support estimates of the gains children make from year to year. The goal of minimizing time and burden on students and teachers determined the kinds of test items that could be used, as well as the structure of the tests. Some compromises were necessary to reconcile the goal of using age-appropriate reading passages with the objective of limiting total test time to an average of 75 minutes in fifth grade. The time limitation precluded the use of assessment tasks such as extended reading materials or hands-on science experiments. As noted earlier, the same reading, mathematics, and general knowledge assessment instruments had been used in all four kindergarten and first-grade rounds of data collection. Children were routed to different levels of difficulty within each assessment domain depending on their performance on 2-4 a short routing test in each subject area. For most children, the easiest of two (general knowledge) or three (reading and mathematics) second-stage forms was selected in fall-kindergarten, while by spring of first grade the majority of children were routed to the most difficult forms within the same sets. Because children's academic skills in third and fifth grades could be expected to have advanced beyond the levels covered by the K-1 assessments, new sets of assessment instruments were developed for each round after those for the first grade. Some test items were retained from each round to the next to support development of a longitudinal score scale. The K-1 general knowledge assessment, which included basic natural science concepts as well as concepts in social studies, was replaced by a direct cognitive science assessment administered in third and fifth grades. The science assessment is not comparable to the K-1 general knowledge assessment, so the longitudinal scale in science spans only the last two rounds of data collection. As a result, gains in science can be measured only for third to fifth grade, while general knowledge scores may be compared only between the kindergarten and first-grade rounds. The format of the fifth-grade assessment was similar to that of prior rounds, with some changes to accommodate the more advanced level of the questions. As in the earlier years, an assessor presented the questions to the child and entered responses into a computer for each individually administered assessment. Seven of the mathematics items asked the child to carry out a task, such as completing a graph or diagram, measuring an object, writing a decimal number, or solving a problem requiring a computation. These items were administered in workbook format. Each child received between one and six workbook items, depending on which second-stage form was selected. To accommodate the length of the reading materials used in the fifth-grade assessment, a separate booklet containing both the reading passages and questions was given to the student, with the questions also appearing on the easel handled by the assessor. Section 5.1.2, Evaluating Common Items, describes the procedures used to evaluate common-functioning of items across different assessment rounds. Kindergarten and first-grade children whose English language skills were not sufficiently advanced to be assessed in English and who were Spanish speakers were administered a Spanish translation of the ECLS-K mathematics assessment. No such translation was used for the reading and general knowledge assessments, which were too language-and culture-dependent to yield comparable measurement. More than two-thirds of the children who received the Spanish mathematics assessment in fall-kindergarten were able to take the English version by spring-first grade. The third-and fifth-grade batteries were administered entirely in English."}, {"section_title": "2-5", "text": "The types of scores reported for the fifth-grade direct cognitive assessments are similar to those for kindergarten through third grades, with some modifications for scores representing both broadbased measures and targeted skills. Assessment scores were recalibrated and rescaled for fifth grade, and several new scores were added. The pool of items on which the broad-based scores are estimated was expanded to provide longitudinal measurement of gains in reading and mathematics for kindergarten through fifth grade, and in science for third to fifth grade. As a result, scores in the public-use files for the earlier rounds should not be compared with recalibrated/rescaled scores in the kindergarten through fifthgrade public-use file. Scores from the earlier rounds that are required for longitudinal measurement have been rescaled and appear in the kindergarten through fifth-grade file in a metric that makes comparisons possible. New targeted scores based on clusters of fifth-grade reading and science items are reported, and new proficiency levels are defined that correspond to grade-appropriate skills in reading and mathematics. Descriptions of scores appear in chapter 4."}, {"section_title": "Individually Administered Adaptive Tests", "text": "During the background review prior to the kindergarten year, the project staff, which included experts in child development, primary education, and testing methodology, made the recommendation that the direct cognitive measures be administered individually to each sampled child. Since young children are not experienced test takers, individual administration could provide more sensitivity to each child's needs than a group-administered test. In addition to being individually administered, it was also recommended that the tests be adaptive in nature; that is, each child should be tested with a set of items that is most appropriate for his or her level of achievement. The development of a vertical scale that must span kindergarten to fifth grade and have optimal measurement properties throughout the achievement range calls for multiple test forms that vary in their difficulty. The total pool of assessment items in each grade should reflect core curriculum elements for that grade. Within each grade, multiple test forms of varying difficulty optimize the accuracy of measurement for individuals with different levels of achievement. Overlapping items for forms within a grade as well as across grades link the forms to a vertical scale for measurement of longitudinal gains. A child who is performing essentially on grade level should receive items that span the curriculum for his or her grade. A child whose achievement is above or below grade level should be given tasks in which difficulty level matches his or her individual level of development at the time of testing, 2-6 rather than a grade-level standard. A child who is performing much better in relation to his or her peers, as measured by a brief routing test, would subsequently be given a second-stage form containing test items that are proportionately more difficult, while a child performing below grade level would receive a form with proportionately more easy items. The matching of the difficulties of the item tasks to each child's level of development that can take place in individualized adaptive testing situations increases the likelihood that the child will be neither frustrated by item tasks that are much too hard, nor bored by questions that are much too easy. Psychometrically, adaptive tests are significantly more efficient than \"one form fits all\" administrations since the reliability per unit of testing time is greater (Lord 1980). Adaptive testing also minimizes the potential for floor and ceiling effects, which can impact measurement of gain in longitudinal studies. Floor effects occur when some children's ability level is below the minimum that is accurately measured by a test. This can prevent low-performing children from demonstrating their true gains in knowledge when they are retested. Similarly, ceiling effects result in failure to measure the gains in achievement of high-performing children whose abilities are beyond the most difficult test questions. Adaptive testing uses performance at the beginning of a testing session to direct the selection of later tasks at an appropriate difficulty level for each child. Adaptive testing relies on item response theory (IRT) assumptions in order to place children who have taken different test forms on the same vertical score scale. Additional discussion of IRT may be found in chapter 3, and notes on the ECLS-K longitudinal scales in chapter 5. It is for these reasons that the ECLS-K uses individually administered adaptive tests. A review of commercially available tests indicated that there were no \"off-the-shelf\" tests that matched the domain requirements and were both individually administered and adaptive. Individual administration of assessments was retained in fifth grade, even though children would probably have been able to cope with paper-and-pencil test forms at this time. The success of the adaptive approach in earlier rounds in optimizing measurement characteristics for a diverse sample of children suggested its use in the later grades as well. A change to group administration was considered for third and fifth grades but rejected because it would have been difficult to administer given the two-stage adaptive structure of the assessments. In the kindergarten and first-grade rounds, a concern was expressed that the individual mode of administration may have contributed unwanted sources of variance to the children's performance in the direct cognitive measures. Unlike group administrations, which in theory are more easily standardized, 2-7 variance attributable to individual administrators might affect children's scores. A multilevel analysis of fall-kindergarten and spring-first grade data found only a very small interviewer effect of about 1 to 3 percent of variance. A team leader effect could not be isolated, because it was almost completely confounded with primary sampling unit. Analysis of interviewer effect was not carried out for the third and fifth-grade data for two reasons. First, the effect in K-1 was about twice as large for the general knowledge assessment (which was not used after first grade) than for reading or mathematics. Second, the effect found was so small that it was inconsequential. Refer to the ECLS-K Psychometric Report for Kindergarten Through First Grade (NCES 2002-05) for more details on the analysis of interviewer effects."}, {"section_title": "The ECLS-K Frameworks", "text": "The ECLS-K is charged with assessing cognitive skills that are both typically taught and developmentally important. Neither typicality nor importance is easily determined. Identifying typical curriculum objectives and their relative importance is difficult because of the decentralized control that characterizes the American education system. The difficulties are compounded for the ECLS-K, since curriculum is constantly evolving and the data collection started with the kindergarten year in 1998, 2 years after the design phase, and continued until 2004. The ECLS-K assessment frameworks were derived from multiple sources. A review of national and state performance standards, comparison with state and commercial assessments, and the judgments of curriculum experts and teachers all provided input to the ECLS-K test specifications. For the third-through fifth-grade assessments, national and state performance standards in each of the domains were examined. The scope and sequence of materials from state assessments, as well as from major publishers, were also considered. Some of the ECLS-K panel consultants had been instrumental in developing the fourth-grade National Assessment of Educational Progress (NAEP) content and process frameworks for reading, mathematics, science, and social studies. The NAEP assessment goals are similar to those of the ECLS-K in that both projects aim to assess cognitive skills that schools typically emphasize. The NAEP 1992, 1994, and 1996 frameworks were particularly useful as models for the third-and fifth-grade ECLS-K assessments since they define appropriate sets of skills and understandings at fourth grade. The resulting 2-8 ECLS-K frameworks are similar to the NAEP fourth-grade frameworks, with grade-appropriate modifications, as well as some differences due to ECLS-K formatting and administration constraints. The NAEP frameworks are based on both current curricula and recommendations for curriculum change that have strong professional backing among theorists and teacher associations. NAEP is interested in the recommendations because it is charged with assessing skills and knowledge that reflect \"best practices,\" as well as those that are widely taught. In contrast, the ECLS-K examines the full range of practices rather than concentrating on best practices. Nonetheless, these recommendations represent reasonable predictions about the directions that schools and school systems in the United States are likely to take in the near future and are thus appropriate to the ECLS-K. With respect to current curricula, NAEP relies on advice from panels of curriculum specialists. In addition to often being directly involved in the construction of curricula used in the schools, specialists often hold a wealth of local knowledge about current practices, which is not recorded in publications and thus not otherwise available. Despite these strengths, the NAEP test specifications have some important limitations in their applicability to the ECLS-K. NAEP frameworks define a number of different subscales within subject-matter domains, but test-length constraints forced the ECLS-K to define single proficiency scales for each subject domain. NAEP can measure multiple subscores within a content domain because it administers a large number of different item sets in a spiraled design to children at a given grade level. That design follows from NAEP's primary goal of measuring cognitive status at the aggregate level on a cross-sectional basis. In contrast, the ECLS-K attempts to attain relatively accurate longitudinal measurement (through adaptive test instrumentation and vertical scaling) at the individual level within a more focused cognitive domain. In addition to the conceptual framework identifying the various types of skills and knowledge tested in the ECLS-K, the relative emphasis given to different content strands was designed to reflect typical curriculum emphases. The general rule used in determining allocations is that the composition of the tests should reflect typical curriculum emphases while considering differences in the number of items and length of items needed to adequately measure a given skill, knowledge, or concept. Systematically collected evidence on typical curricular content is not available in most subject areas so the study relied mainly on the advice of curriculum specialists and people with extensive teaching and administrative experience in elementary schools and on the standards published by states and national professional organizations. The overall testing time for each child was expected to consist of comparable time allotted for reading and mathematics, with a lesser amount of time allocated for the science 2-9 assessment. It is important to keep in mind that some content strands can be assessed more quickly than other areas. For example, many single-word decoding items can be administered in a short period of time, while reading questions based on passage comprehension require a greater investment of time. Tables 2-1 to 2-3 present the test specifications for the ECLS-K cognitive battery from kindergarten through fifth grade. The numbers in the cells are the target percentages for each content area; they are at best approximations since the item classifications are somewhat arbitrary. Particularly in third and fifth grades, many items tap more than one area. For example, solving a mathematics problem may require understanding of number concepts as well as skill in interpreting data. The items for the kindergarten and first grade are allocated according to the amount of time items were expected to take. However, the content items for the third and fifth grades are distributed by the percentage of items to match the NAEP frameworks."}, {"section_title": "Reading Test Specifications", "text": "The ECLS-K reading specifications were adapted from the 1992 and 1994 NAEP Reading Frameworks (National Assessment Governing Board [NAGB] 1994a). The NAEP framework is defined in terms of four types of reading comprehension skills: Initial understanding requires readers to provide an initial impression or global understanding of what they have read. Identifying the main point of a passage and identifying the specific points that were drawn on by the reader to construct that main point would be included in this category. Developing interpretation requires readers to extend their initial impressions to develop a more complete understanding of what was read. It involves the linking of information across parts of the text, as well as focusing on specific information. Personal reflection and response requires readers to connect knowledge from the text with their own personal background knowledge. Personal background knowledge in this sense includes both reflective self-understanding, as well as the broad range of knowledge about people, events, and objects that children bring to the task of interpreting texts. Demonstrating a critical stance requires the reader to stand apart from the text and consider it objectively. This would include questions asking about the adequacy of evidence used to make a point or the consistency of someone's reasoning in taking a particular value stance. In kindergarten and first grade, some questions about unrealistic stories were asked to assess the child's notion of \"real vs. imaginary.\" Such 2-10 story types allow us to get information on critical skills as early as kindergarten. Third-and fifth-grade critical stance items might assess children's understanding of literary devices or the author's intention. Because the NAEP framework begins with fourth grade, it had to be modified for the ECLS-K to accommodate adequately the basic skills typically emphasized beginning in kindergarten. Two skill categories were added to the NAEP framework: Basic Skills, which includes familiarity with print, recognition of letters and phonemes, and decoding; and Vocabulary. After first grade, the emphasis on basic skills in the ECLS-K reading framework was decreased, so that the allocations for third and fifth grades are very close to that of the reading comprehension skills of fourth grade NAEP. Literacy curriculum specialists and teachers contributed to development of the framework and reviewed item pools. The conceptual categories shown in table 2-1 combine the recommendations of the literacy curriculum specialists with the NAEP reading framework. Notably absent from the ECLS-K reading framework is any place for writing skills. This absence is a reflection of practical constraints associated with limited amount of testing time and the cost of scoring. Nevertheless, the ECLS-K asks teachers to provide information on each sampled child's writing abilities each year, and on the kinds of activities they use in their classrooms to promote writing skills, with the use of the Academic Rating Scale (see chapter 6 in this report)."}, {"section_title": "2-11", "text": "Table 2-1. Reading longitudinal test specifications for kindergarten through fifth grade: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Reading comprehension skills NOTE: The content strands are identical to the National Assessment of Educational Progress 1994 Reading Framework categories, with the addition of Basic Skills and Vocabulary. Basic Skills include familiarity with print, recognition of letters and phonemes, and decoding. Initial understanding requires readers to provide an initial impression or global understanding of what they have read. Developing interpretation requires readers to extend their initial impressions to develop a more complete understanding of what was read. Personal reflection and response requires readers to connect knowledge from the text with their own personal background knowledge. The focus here is relating text to personal knowledge. Demonstrating a critical stance requires the reader to stand apart from the text and consider it objectively. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004. 2-12"}, {"section_title": "Mathematics Test Specifications", "text": "The mathematics test specifications shown in table 2-2 are primarily based on the Mathematics Framework for the 1996 NAEP (NAGB 1996a), which is in turn derived from the curriculum standards from the Commission on Standards for School Mathematics of the National Council of Teachers of Mathematics [NCTM] (1989). The content strands represented by the column categories in table 2-2 are defined as follows (these correspond closely to NAGB [1996a] definitions for most strands): Number sense, properties, and operations. This refers to children's understanding of numbers (whole numbers, fractions, decimals, and integers), operations, and estimation, and their application to real-world situations. Children are expected to demonstrate an understanding of numerical relationships as expressed in ratios, proportions, and percentages. This strand also includes understanding properties of numbers and operations, ability to generalize from numerical patterns, and verifying results. Measurement. Measurement skills include choosing a measurement unit, comparing the unit to the measurement object, and reporting the results of a measurement task. It includes items assessing children's understanding of concepts of time, money, temperature, length, perimeter, area, mass, and weight. Geometry and spatial sense. Skills included in this content area extend from simple identification of geometric shapes to transformations and combinations of those shapes. The emphasis of the ECLS-K is on informal constructions rather than the traditional formal proofs that are usually taught in later grades. Data analysis, statistics, and probability. This includes the skills of collecting, organizing, reading, and representing data. Children are asked to describe patterns in the data or make inferences or draw conclusions based on the data. Probability refers to making judgments about the likelihood of something occurring based on information collected on past occurrences of the event in question. Students answer questions about chance situations, such as the likelihood of selecting a marble of a particular color in a blind draw when the numbers of marbles of different colors are known. Patterns, algebra, and functions. Consistent with the NCTM kindergarten to fourthgrade curriculum standards, the ECLS-K framework groups pattern recognition together with algebra and functions. Patterns refers to the ability to recognize, create, explain, generalize, and extend patterns and sequences. In the kindergarten test, the items included in this category entirely consist of pattern recognition items. As one moves up to the subsequent grades, algebra and function items are added. Algebra refers to the techniques of identifying solutions to equations with one or more missing pieces or variables. This includes representing quantities and simple relationships among variables in graphical terms. While pattern recognition is heavily emphasized in kindergarten and even first-grade classrooms, the proposed framework tends to deemphasize the assessment allocation since it is not clear what to expect with reference to longitudinal trends in this skill area. Table 2-2. Mathematics longitudinal test specifications for kindergarten through fifth grade: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Content strands  (National Assessment Governing Board, 1996a). The content strand item targets for the third and fifth grades match the NAEP fourth-grade recommendations for the minimum number of \"Number Sense\" items and the maximum numbers for the other strands. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004. "}, {"section_title": "2-13", "text": ""}, {"section_title": "Grade levels Total", "text": ""}, {"section_title": "2-14", "text": "The number sense, properties, and operations content strand represents the dominant emphasis of elementary school mathematics. Additional discussion of the adaptation of the NAEP mathematics framework to ECLS-K, and an appendix listing the NCTM curriculum standards, may be found in the ECLS-K Psychometric Report for Kindergarten Through First Grade (NCES 2002-05)."}, {"section_title": "Science Test Specifications", "text": "The K-1 general knowledge test, a combination of science and social studies items, was replaced by a science test for third and fifth grades. No direct measurement of social studies knowledge was included in third and fifth grades, although teacher ratings of children's proficiency in social studies were collected in third (but not fifth) grade. For a discussion of the design and specifications of the K-1 general knowledge test, refer to the ECLS-K Psychometric Report for Kindergarten Through First Grade (NCES 2002-05). The test specifications for third-and fifth-grade science (table 2-3) were developed largely from recommendations of the ECLS-K advisory group. Similar to the 1996 NAEP Science Framework (NAGB 1996b), the ECLS-K science framework includes two broad classes of science competencies: Conceptual Understanding and Scientific Investigation. Conceptual understanding refers to both the child's factual knowledge base and the conceptual accounts that children have developed for why things occur as they do. Consistent with current curriculum trends, the emphasis in the ECLS-K will be more on the adequacy of accounts than the grasp of discrete facts, particularly as the children move up in grade level. Scientific investigation refers to children's abilities to formulate questions about the natural world, to go about trying to answer them on the basis of the tools available and the evidence collected, and to communicate their answers and how they obtained them. The ECLS-K science assessment includes questions drawn from the fields of earth, physical, and life science. These fields are defined as follows: Earth and space science is the study of the earth's composition, process, environments, and history, focusing on the solid earth and its interactions with air and water. The content to be assessed in earth science centers on objects (soil, minerals, rocks, fossils, rain, clouds, the sun and moon), as well as processes and events that are relatively accessible or visible. Examples of processes are erosion and deposition, and weather and climate; events include volcanic eruptions, earthquakes, and storms. Space science in the elementary grades is usually concerned with the relationships between earth and other bodies in space (e.g., patterns of night and day and the seasons of the year, phases of the moon). Physical science includes matter and its transformations, energy and its transformations, and the motion of light, sound, and physical objects. Physical science concepts in the elementary grades include the physical and chemical transformations of matter such as liquids and solids, and the conduction of heat, sound, and electrical energy. Life science is devoted to understanding and explaining the nature and diversity of life and living things. The major concepts assessed relate to interdependence, adaptation, ecology, and health and the human body. In terms of subject matter emphasis in the elementary grades, the 1996 NAEP Science Framework, American Association for the Advancement of Science (1995) and National Academy of Sciences (1995) recommend roughly equal emphasis on the three strands: earth, life, and physical science. Brace 1995;Holt 1986;Scott-Foresman 1994;and Silver Burdett & Ginn 1991) revealed that coverage of these topics is equally distributed. The ECLS-K advisors concurred with the recommendation of equal representation of the strands at each grade level, and the final item batteries reflect that balance. for a full-scale field test in spring 2002. Fourth-graders were included in the field test sample along with fifth-graders, in the event that a fourth-grade ECLS-K round might prove to be feasible. The field test results, in turn, were used to guide the revision and selection of items for the fifth-grade assessments for the longitudinal sample."}, {"section_title": "Review of elementary text series (Harcourt", "text": ""}, {"section_title": "Field Testing of Direct Cognitive Items", "text": ""}, {"section_title": "Field Test Design", "text": "Preliminary pilot testing of items. Pools of test items in each of the content domains were developed for second through fifth grades. Items were chosen to extend the longitudinal scales initiated in kindergarten and first grade, with grade-appropriate changes in content and format. The majority of reading items for second through fifth grades tapped reading comprehension rather than basic skills. In mathematics, increased emphasis was placed on problem solving. Both of these areas made expanded use of open-ended items (scored right/wrong), and in both, children were asked to provide some of their answers on worksheets instead of orally. Some of the reading passages on which test questions were based were taken from published sources, while others were written for the ECLS-K. All of the mathematics and science questions were prepared by the ECLS-K item writers. Some utilized photographs or diagrams from published sources. Test items were reviewed by elementary school curriculum specialists for difficulty, appropriateness of content, and relevance to the test framework. In addition, items were reviewed for sensitivity issues related to population subgroups. Items that passed these content, construct, and sensitivity screenings were assembled into pairs of booklets for preliminary pilot testing in spring 1999. Approximately 120 to 150 items in each content area were distributed among two reading, two mathematics, and two science forms within each of the four grades. Each pilot test form in each grade, second through fifth, was administered to about 50 children. The results of the pilot testing were used to select and revise test questions for use in full-scale field tests of second-and third-graders in spring 2000, and of fourth and fifth-graders in spring 2002. Field test issues. The operational feasibility of the individualized two-stage assessment procedure with \"on-time\" scoring of the routing test had been established in the ECLS-K kindergarten and first-grade rounds. These data collections had also satisfactorily demonstrated young children's ability to maintain the necessary attention span and to complete the assessments without signs of discomfort or distress. The field test for fourth and fifth grade was designed primarily to gather the 2-17 necessary psychometric data to evaluate the suitability of items for selection for the operational test forms. An additional purpose was the construct validation of the reading and mathematics item pools, by comparison of field test results with scores on selected sections of an established assessment instrument, the Woodcock-McGrew-Werder Mini-Battery of Achievement (MBA) (Woodcock, McGrew, and Werder 1994). MBA subtests measuring letter and word identification, vocabulary, and comprehension were used for validation of the ECLS-K reading item pool, while validation of the mathematics pool was based on scores on the MBA Calculation and the Reasoning and Concepts subtests. The MBA was chosen from several instruments reviewed because the content it covered, the time it took to administer, and its available reliability and validity information best suited its use as a validation instrument for ECLS-K. The MBA subtests were administered according to standard procedures specified by the publisher. Spring 2002 field test. Between 120 and 136 questions in each of the content areas (i.e., reading, mathematics and science) were field tested. Most of the field test items were taken from the 1999 pilot tests, with revisions incorporated as necessary. Four reading passages and the accompanying items, which had been administered to eighth-graders in the National Education Longitudinal Study of 1988 (NELS:88) were also field tested. These were selected to serve two purposes: to supply high-difficulty questions that were based on relatively short reading passages, and to facilitate linking to NELS:88 score scales if ECLS-K were to be extended beyond fifth grade. The items within each of the content areas were divided into two parallel sets of items, A and B, with separate workbooks accompanying the mathematics sets for 12 (Form A) or 10 (Form B) of the 60 mathematics items. Six booklets, each containing two subtests in different content areas, were created (table 2-4). "}, {"section_title": "2-18", "text": "Each A or B set of content area items appeared in one test booklet as the first cognitive set, and in another as the second, so that possible practice effects or fatigue effects would be balanced. For each of the six student test booklets, a corresponding examiner booklet contained the instructions for administering and scoring each test item. Booklet covers were color-coded for ease in matching the examiner to the student forms. Table 2-4 shows the subtests in each of the field test forms. The number of items (or separately scored item parts) is shown in parentheses after the name of the test section. At the end of four of the six booklets, an MBA reading or mathematics test was administered for validation purposes. In each case, the booklet in which a set of science items was paired with reading or mathematics was selected for administration of the corresponding MBA test. The first two booklets, red and orange, did not include an MBA validation section. Each of these two booklets, which paired a reading with a mathematics section, was already quite long and challenging for the children because of the time required for reading passages and mathematics computations. The science sections, with only short-answer questions, were faster to administer and left time for an MBA test at the end. About 1,800 children, approximately evenly divided between fourth-and fifth-graders, participated in the field test of cognitive items in spring 2002. Each child was administered one of the six booklets. Spiralling the forms among test takers resulted in approximately 600 observations on each test question, about half of which came from fourth-graders and half from fifth-graders. Early in the field test period, it became clear that the test forms were too long for some children to complete in a reasonable period of time. Modifications were implemented to minimize burden on the children, while ensuring that sufficient data would be collected for the purposes of selecting test items and validating the item pool. Two reading passages were deleted from the test forms for the remainder of the field test period, one because it was excessively long, and the other because it was too easy. One mathematics item, which had proven to be flawed, was also deleted. Discontinue rules were defined so that if an excessive amount of time had elapsed at certain checkpoints within the booklets, the assessor would then skip to a later item set within the same section, to the next section in the booklet, or to the MBA validation section that followed the field test item sets."}, {"section_title": "2-19", "text": ""}, {"section_title": "Field Test Results and Conclusions", "text": "Analysis of field test data focused on both psychometric characteristics of the test items and operational issues. Psychometric analysis included calibration of item difficulty and discrimination, identification of flawed items that could be revised, and detection of differential item functioning (DIF) with respect to population subgroups. Validation of the ECLS-K reading and mathematics field test item pools was carried out by correlating field test ability estimates with MBA reading and mathematics test scores. Operational issues examined included timing, completion rates, and cooperation. Comprehensive reports from the assessors who administered the field tests complemented the analysis of item response data, and played an important part in the design of the fifth-grade assessments. Psychometric characteristics of test items. Classical item statistics were obtained for each of the field test items. Item difficulty was represented by percent correct, which was computed for fourth and fifth grade participants combined, as well as for each grade separately. Item discrimination, that is, the extent to which each item is consistent with the overall set of items, was measured by r-biserials, which are correlations of total score with item score (right/wrong) for each item. Distractor analysis consisted of evaluating statistics on the percentage of children choosing each response option for multiple-choice items, and the average total test score for those choosing each option. This information provided a basis for identifying items in need of revision, for example, questions that might have more than one potentially correct answer, incorrect response options chosen by children scoring higher, on average, than those choosing the intended correct option, or response options that seemed so implausible that few if any children selected them. Item analysis procedures provided information on the number of children who omitted each item, and their performance on the test as a whole. A high number of omitted items, for children who then went on to answer subsequent test questions, can be an indication that a test item is confusing or otherwise problematic for children. Classical item statistics also included the alpha coefficient, a measure of reliability, for each set of field test items. IRT parameters (Lord 1980) were estimated for all cognitive items in the field test, using the PARSCALE computer program (see section 3.2.2 for details) for the purpose of item selection only. (Parameters were re-estimated later using national sample data.) The IRT parameters were based on the three-parameter model with a parameter for guessing, a parameter for difficulty, and a slope (discrimination) parameter. The IRT slope, or \"a\" parameter, complements the information provided by the r-biserial but relates item discrimination to overall performance at a particular ability level rather than for the whole range of ability. The \"b\" parameter provides a measure of difficulty that is less susceptible 2-20 to distortion, if large numbers of children omitted an item, than is percent correct. Marginal maximum likelihood estimation procedures (Mislevy and Bock 1982;Muraki and Bock 1991) were used to estimate the item parameters. ICCs were inspected for indications of lack of fit. Graphs containing the ICCs also included markers showing percent correct, separately for fourth and fifth graders, at intervals spaced along the ability range. This permitted evaluation of overall fit as well as displaying possible differences in functioning for the two grades. A relatively small percentage of items exhibited overall lack of fit and were removed from consideration for the fifth-grade battery. Examination of ICCs for the poorer fitting items, along with the distractor analysis from the classical item statistics, can suggest possible revisions that might correct a flawed item. In some cases modifications to the response options could be made, and the item kept in the pool. Attempts to modify and retain flawed items were particularly important for items that represented one of the more difficult-to-fill cells in the framework classifications. IRT-based estimates of ability distributions provided a basis for the selection of target difficulty ranges for the fifth-grade test forms. The metric of the IRT ability estimates for field test participants corresponds to the metric of the item difficulty parameters. This allowed the selection of items whose difficulty was matched to the ability levels that could be expected in the fifth-grade assessment. Although the field test sample was not designed to be nationally representative, care was taken to select participating schools such that the sample would include both high and low achievers. Section 2.1.4 describes the use of the item difficulty and ability parameters in the selection of items for the fifth-grade forms. The question of whether the absence of a fourth-grade round of data collection might result in a gap in ability levels that might seriously impact the measurement of gain was addressed. Examination of the field test results showed a considerable overlap in ability distributions between third-and fifthgraders. As a result, no fourth-grade \"bridge\" data collection, analogous to the second-grade sample that had been assessed to bridge the first-to-third grade gap, was necessary. Cognitive test items were checked for DIF for males compared with females. There were too few Hispanic and Asian children in the field test sample for DIF analyses to be carried out for these groups. Sample sizes of Black students were sufficiently large for Black/White DIF to be evaluated for only about half of the field test items. It is not necessarily expected that different subgroups of students will have the same average performance on a set of items. But when students from different groups are matched on overall ability, performance on each test item should be about the same. There should be no relative advantage or disadvantage based on the student's gender or racial/ethnic group."}, {"section_title": "2-21", "text": "The DIF procedure (Holland and Thayer 1986) is designed to detect possible differential functioning for subgroups by comparing performance for a focal group (e.g., females or Black students) with a reference group (e.g., males or White students), while holding ability constant. DIF refers to the identification of individual items on which some population subgroups (the focal groups) perform, on average, relatively better or worse in comparison with members of a reference group who are matched in terms of overall performance on the total pool of items. Items are classified as \"A,\" \"B,\" or \"C\" depending on the statistical significance of subgroup differences, as well as effect sizes. Items identified as having \"C\" level DIF have detectable differences that are both sizeable and statistically significant. Chapter 3 provides a more detailed description of the procedures used to detect DIF levels of items. A finding of differential functioning, however, does not automatically mean that a test item is inappropriate. It simply means that the item is differentially easier or more difficult for some subgroup (focal group) when compared with a reference group. A judgment that an item is inappropriate requires not only the statistical measure of DIF for one or more subgroups, but also a determination that the difference in performance is irrelevant to the construct being measured. In other words, different population subgroups may have differential exposure or skill in solving test items relating to a topic included in the test specifications. If so, the finding of differential performance may be an important and valid measure of the targeted skill, and should be included in the assessment (see section 3.4; also Holland and Thayer 1986). Items that demonstrate differential functioning favoring the reference group were reviewed for inappropriate content by a standing committee on test fairness at Educational Testing Service (ETS), consisting of both majority and minority group members. Items that were judged to have content or presentation that might be problematic for a particular focal group in ways that are not relevant to the construct being measured were dropped from the item pool. For example, a mathematics item requiring students to mark the location of an ordered pair on a grid turned out to be differentially more difficult for Black compared with White students, while a science question based on relative weights of three blocks was differentially more difficult for females compared with males. However, the items that had DIF that was judged to be the result of possible differential skills in some area of the test framework, and not merely due to subgroup membership, were retained. DIF analysis of field test items resulted in a finding of \"C\" level DIF for four reading, three mathematics, and two science items. The mathematics and science \"C\" DIF items, and two of the four reading items, were deleted from consideration for the fifth-grade assessments. The remaining two reading \"C\" DIF items were retained: one had been previously reviewed in the third-grade assessment and found to be acceptable; for the other, statistics suggested that the DIF finding might be merely an artifact of small sample size."}, {"section_title": "2-22", "text": "Correlations between total reading and mathematics scores on the MBA construct validation instrument and the corresponding field test reading and mathematics IRT ability estimates (thetas) were computed. The correlations (.73 for reading, .80 for mathematics) were somewhat lower than the MBA correlations for the corresponding subjects in the second-and third-grade field test (.83 and .84, respectively). Two factors contributed to the lower correlations between reading comprehension measures: Differences in content between MBA and ECLS-K tests increased from the 2000 to the 2002 item pools: While the same MBA tests were used in both field tests, the ECLS-K item pools were different. The MBA tests place a great deal of emphasis on basic skills, while the ECLS-K pools moved toward increasing emphasis on reading comprehension and mathematics problem solving. MBA reading sections contain 28 \"identification\" (decoding items), 22 vocabulary items (opposites), and 23 comprehension items. About two-thirds of the ECLS-K grade 4 to 5 reading field test items were comprehension questions based on reading passages. Similarly, the MBA mathematics test contained 29 calculation questions (most children discontinued the section after item 18 or 19) and 50 \"reasoning and concepts\" items, of which most children answer about 20 because the easiest and hardest items were not administered. The ECLS-K field test grade 4 to 5 mathematics item pool contained very few pure calculation questions and a large majority of word problems. Reduced MBA score variances: The standard deviations of the MBA scores were somewhat lower for the grade 4 to 5 field test than for grade 2 to 3. All other things being equal, reducing variance results in lower correlations. (The ECLS-K ability estimates did not have a lower variance for grade 4 to 5, in fact, variances were increased.) Early in the field test, a proposal to save field test time by deleting the MBA section for children who were taking a long time to complete their ECLS-K item sections was rejected because of the need to preserve the variability of the MBA sample. There is evidence that these lower-scoring children did receive the MBA: the mean ability estimate for the half-sample of MBA takers was similar to the mean for the whole field test sample. Although the correlation coefficients were lower than those found for the grade 2 to 3 field test, the correlations of .73 for reading total and .80 for mathematics total are sufficiently high to support the purpose of validating the ECLS-K item pools. Operational issues. Findings from both quantitative and qualitative analysis of field test data answered questions related to practical and administrative issues, such as timing, fatigue, and cooperation."}, {"section_title": "2-23", "text": "Start and stop times were recorded for each field test section as a whole, and for reading sections, separately for each reading passage as well. As noted earlier, the field tests proved to be too long for the time allotted and for children's ability to function effectively. The deletions and discontinue rules instituted to shorten the test meant that timing data were relevant only for test sections that appeared in the first position in the booklet, before stop rules impacted the collection of second-section data for some children. Item nonresponse rates for sections given first were low: relatively few children either omitted items while answering subsequent questions or failed to reach the end of the test sections. While the timings for mathematics sections were somewhat longer than expected, the reading sections were primarily responsible for the excessive field test times, for several reasons. Unlike the mathematics and science questions, which were independent short answers or computations, most of the reading questions required the additional investment of time to complete the reading passages on which the questions were based. Analysis of timing data showed that roughly half of the total time was used in reading, the other half in answering the questions. Another factor responsible for the excessive time required for the reading sections was the length of the reading passages themselves. In an effort to make the assessments reflect tasks typical of the fifth-grade curriculum, passages of several pages in length were included in the field test. Timing results made it clear that given the time constraints of the assessment, the requirement of curriculum relevance must be satisfied by the difficulty of the reading materials and questions rather than their length. In interpreting results for the longer and more difficult reading passages, it is also important to take into account the difference in method of assigning forms in the field test (random) compared with the fifth-grade operational test (form selection based on ability demonstrated in the routing section). Passages that were too long and difficult for the randomly-assigned field test participants may be suitable for the reading form designed to be administered to the highest achieving fifth-graders. Timings for the mathematics and science sections were recorded for the section as a whole, not for individual items or groups of items. On average, the mathematics sections took under 40 minutes to complete, and science sections under 32 minutes, or about two-thirds of a minute per item for mathematics and one-half minute per item for science. This suggests that 35 to 40 questions per child could be administered in the national test within the target time of 30 minutes for mathematics and 20 minutes for science. Field test assessors participated in debriefing sessions following the spring 2002 field test administration. They provided information on the children's reactions to test questions as well as 2-24 suggestions on revisions of items that might improve item performance. They reported that most of the children were interested and cooperative. The assessors made numerous suggestions about item content, presentation, and scoring. Comments on questions and response options that were confusing, ambiguous, or incomplete were taken into consideration in selecting and revising items for the proposed fifth-grade reading forms. Comments related to performance (such as reports that children found an item too difficult) were, in general, corroborated by analysis of the field test data, although some reading passages that assessors reported that they or the children did not like did have satisfactory statistics. This was particularly true of the long reading passages, which the less able readers clearly found too challenging. The most important point made by the examiners was the need to keep testing time short enough so that children would not get tired and frustrated. The booklet design described earlier, with each test form appearing both early and late in a testing session, was designed to permit analysis of order effects. However, the discontinue rules implemented to shorten the assessment resulted in many children-primarily the lower achievers-failing to complete the second section of the field test booklet. This made comparison of statistics for sets of items given early in the testing session with the same items given in a later position impossible. Assessors' reports clearly indicated that fatigue due to test length was a factor in performance. In the second-and third-grade field tests, when excessive test length was not an issue, neither a practice effect (better performance toward the end of the test) nor a fatigue effect (a drop in performance) was found."}, {"section_title": "Fifth-Grade Test Forms", "text": "The fifth-grade assessments were designed to support measurement of the reading, mathematics, and science domains as accurately as possible, both at all levels of ability found within the ECLS-K fifth-grade round and longitudinally as well. Assembly of the test forms from the field-tested items took into account numerous objectives, including psychometric considerations, framework specifications, and practical issues. The psychometric considerations included item quality and reliability, item difficulty, floor and ceiling effects, and longitudinal measurement. Field-tested items were candidates for selection for final test forms if they had acceptable item analysis statistics and IRT parameters, had no DIF problems related to subgroup membership, and showed some increase in percent correct between fourth-and fifth-graders. Framework specifications, and practical issues such as timing and scoreability of items, placed additional constraints on assessment design. Design of the test forms required some compromises due to competing objectives."}, {"section_title": "2-25", "text": ""}, {"section_title": "Item Quality and Reliability", "text": "To contribute useful information about children's skill levels, test items selected for the final forms should ideally have high r-biserials (.40 or higher) and IRT \"a\" parameters (1.0 or higher), as well as good fits of empirical data to the IRT model. Items with high discrimination parameters permit accurate placement on the ability continuum. A small number of the selected items fell short of these standards but were selected for other reasons such as framework specifications, overlap with third-grade assessments, or links to a selected reading passage. In IRT, the measurement precision for individual examinees is improved by administering the maximum number of items possible in the time available, and including items that function appropriately and measure the same construct. Items found to have DIF for population subgroups were deleted from the item pool except as noted earlier."}, {"section_title": "Item Difficulty", "text": "Accurate measurement at all scale points requires that children receive sets of test items that are close to their ability level. The routing section of each assessment should direct each child to an appropriate set of second-stage items. Within each second-stage form, the item difficulties were selected to match the expected ability levels of the test takers. The distribution of IRT ability estimates for the field test fifth-graders was used to determine item difficulty objectives such that the middle-difficulty form would be suitable for approximately the middle half of fifth-grade test takers, while the low and high second-stage forms would each be taken by about a quarter of the children. Thus, the target difficulties for the majority of the second-stage middle form items were selected to fall within two-thirds of a standard deviation above and below the mean fifth-grade ability estimate, corresponding to 50 percent of the distribution. The low and high second-stage forms consisted primarily of easier and harder items, respectively. The low form items ranged from about two standard deviations to about two-thirds of a standard deviation below the fifth-grade mean, overlapping with some of the easier items in the middle form. Each high second-stage form began with items overlapping the hardest middle form items, at about two-thirds of a standard deviation above the mean, and ranged up to two standard deviations above the mean. The test items taken by each child (routing test plus one second-stage form) were designed to have a rectangular distribution of item difficulties in the target ability range, that is, IRT \"b\" parameters that were approximately equally spaced with no large gaps."}, {"section_title": "2-26", "text": ""}, {"section_title": "Floor and Ceiling Effects", "text": "Floor effects occur when all test items are so difficult that many children must simply guess at random, while ceiling effects are a result of a test that is too easy, with many children achieving a perfect score. Tests that are too hard or too easy for large numbers of test takers do not do a good job of measuring the ability levels of the lowest and highest achieving children. It is particularly important to avoid floor and ceiling effects in a longitudinal study, so that achievement gains may be measured accurately. The fifth-grade assessment forms were designed to have enough easy items that distinctions could be made at the low end of the ability range, and enough hard items to accurately measure the most skilled students. To avoid floor and ceiling effects, each assessment included a few items in the high second-stage form that almost all children would get wrong, and a few in the low second-stage form that almost all children would get right, so that accurate measurement of the extremes of ability could be accomplished. Each of the second-stage test forms contained some items with difficulty levels that extended beyond the target ability range, at both the high and low end. This design feature served two purposes. First, it provided some of the overlapping items required to put all of the test forms on a common scale (in addition to routing items taken by all children). Second, it improved measurement properties for children whose achievement level was very near a routing cut point. There was the possibility that guessing and/or careless mistakes on the routing test could result in children at the margin receiving a second-stage test form that was too easy or too hard. For example, a child whose ability level was half a standard deviation below the mean (i.e., near the low end of the middle ability range) might miss a few routing test items and be assigned to the low second-stage form. Accuracy of measurement in this situation was supported by the overlap of some of the hardest low form items with the easiest middle form items."}, {"section_title": "Longitudinal Score Scale", "text": "Measurement of gain over time requires a longitudinal score scale. The challenge for ECLS-K was to establish a common scale not only for tests given in different grades but also for different forms of the test within each grade. In the four rounds of testing in kindergarten and first grade, this was accomplished by using the same sets of assessments in each round, with alternative overlapping secondstage forms. The third-and fifth-grade assessments used the same overlapping two-stage design but with more advanced sets of items. Putting K-1, third-, and fifth-grade scores on a common scale required 2-27 common items shared between subsequent assessments. Items from the K-1 assessments (22 in reading, and 14 in mathematics) provided the necessary link between K-1 and third grade, with a small \"bridge\" sample of second-graders augmenting the gap in ability levels between first and third grade. Overlapping ability distributions for third-and fifth-grade made a fourth-grade bridge sample unnecessary. Fifth-grade items shared with the third grade assessment (59 common items in reading, 31 in mathematics, and 27 in science) supported the extension of the K-1-3 longitudinal scale through fifth grade."}, {"section_title": "Curriculum Relevance", "text": "Both fourth-and fifth-graders participated in the 2002 field test of cognitive items. Although there was no fourth-grade round of data collection, the fourth-grade field test data did play a role in the design of the test forms for the fifth grade. Analysis of field test data was carried out for both grades combined, as well as separately for fourth grade and fifth grade. In selecting items for the fifth-grade test forms, preference was given to items that showed the largest differences in percent correct between the fourth-and fifth-graders in the field test sample. Although the fourth-and fifth-graders in the field test were different children, not longitudinal measurements of the same children, items with the largest fourthgrade to fifth-grade differences in percent correct could be assumed to be strongly related to fifth grade curriculum. This inference was supported by the finding that not all items showed large differences. Many had close to the same percent correct for fourth-grade and fifth-grade field test participants, suggesting that their content was not emphasized in fifth-grade curriculum materials."}, {"section_title": "Framework Specifications", "text": "Items were selected to match the target percentages specified in the framework tables in section 2.1.2 as closely as possible (see tables 2-5 to 2-7). Some compromises in matching target percentages were necessary to satisfy constraints related to other issues, including linking to the earlier rounds, avoiding floor and ceiling effects, and maintaining item quality. This was especially true for the reading assessment in which several questions based on each reading passage placed an additional constraint on the selection of items to match content strands. Reading items were not selected individually but in sets of four to eight items or more based on the reading passages. Once an investment of time had been made reading a passage, accuracy of measurement per unit of time could be maximized by selecting as many high quality items as possible based on the passage, even if that resulted in overrepresentation of 2-28 a content strand. Conversely, a shortfall in a content strand could result if the available items in the strand were linked to a reading passage that had too few other useful items to justify its selection.    grade. A majority of the items needed for the first to third grade link were decoding items classified as Basic Skills, and these same items served to avoid floor effects for the lowest achieving third-graders. While the decoding items did provide a valuable link between third and fifth grades, the presence of many common items from the reading passages shared between third and fifth grades made this issue less critical. A more important reason for selecting additional Basic Skills items was the need to fill gaps in the distribution of item difficulties. Since these items were not tied to reading passages, they could be selected individually at points where items of a particular difficulty were needed. In fact, the impact of the overrepresentation was minimal for two reasons. First, the 16 Basic Skills decoding items were administered in sets of four, in ascending order of difficulty, and the harder sets were skipped if a performance criterion on an easier set was not met. The last set of four items was administered to only 13 percent of the children. Second, an adjustment was made when scores were calculated. All of the Basic Skills items were utilized in estimating ability levels, but four were deleted from computation of the final scale scores to align the composition of the scores more closely with the framework. Initial Understanding items were overrepresented in comparison to framework targets, while Personal Reflection items were underrepresented. Nearly half of the Initial Understanding items were selected for the lowest fifth-grade form and served the purpose of linking to the earlier rounds. The remaining Initial Understanding items were retained because they accompanied a selected reading passage. The shortage of Personal Reflection items, as for the third-grade assessment, was due to relatively poor psychometric performance for items in this category. Item selections for the mathematics and science assessments closely matched framework target percentages, in large part because the constraint of selecting items in groups was not present. Enough high quality science items were available for selection in each of the content strands to match frameworks exactly, with only minor deviations from targets in mathematics. The deviations from framework targets probably have relatively little impact on the measurement of the domain of interest because there is some ambiguity in the classification of items. Many if not most of the third-grade reading and mathematics items had aspects of more than one content strand. For example, answering a reading comprehension item would require decoding the words in the story, understanding the meaning of words in context, and using personal experience to interpret the reading passage and the question. Even the Basic Skills decoding items were probably affected by children's mastery of vocabulary. Similarly, a graph-reading item in the mathematics assessment could be classified as Data Analysis, Statistics, and Probability but would also require an understanding of 2-30 numbers. Therefore, the designation of a single strand category for each item was somewhat arbitrary. It is unlikely that the necessary compromises in selecting items would have a serious negative impact on measurement of the intended construct."}, {"section_title": "Practical Issues", "text": "The 75-minute time allocation for the fifth-grade direct cognitive assessments was divided into 30 minutes each for reading and mathematics and 15 minutes for science. Analysis of field test timings showed that more time per item was needed for reading, with the extra time required for the reading passages, and for mathematics, which required problem solving, than for science questions. The sets of science items, consisting of short-answer questions, tended to go much more quickly. The number of items in each of the fifth-grade test forms is shown in table 2-8. The number of items in each fifth-grade pool is less than the sum of the items in the test forms because there is some overlap of items across forms. Four fifth-grade reading items were calibrated but deleted from the final score scale to align the scale with the framework, and one was deleted from scoring because of differential item functioning (DIF) in the fifth-grade sample. Two reading items that had not been scored in third grade because they proved to be too difficult to provide useful information for third-graders performed satisfactorily when fifthgrade responses were added to the analysis. These two items, present but not scored in third grade, were added to the longitudinal scale. Similarly, one mathematics item that had unsatisfactory statistics in third grade was added to the longitudinal scale based on the combined third and fifth-grade data. See chapters 4 and 5 for details. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), spring 2004."}, {"section_title": "2-31", "text": "Routing test cut points were determined empirically based on field test IRT ability estimates and item parameters. Using the ability estimates for field-tested fifth-graders, simulations were carried out to predict, for each child, a score on the items selected for the routing test and a predicted score on each of the three proposed second-stage forms. Cross-tabulations of the simulated routing scores against each second-stage score were examined, and routing cut points were selected such that ceiling and floor effects would be minimized. For example, if many of the children with simulated routing scores below 7 would be expected to receive below-chance scores on the middle difficulty item set, but few if any perfect scores on the low second-stage items, children in this routing score range would be assigned to the low form. This procedure was carried out rather than relying on cut points that approximated the planned 25-50-25 percent assignment to second-stage forms because it was more important for children to receive test questions matched to their ability than it was to achieve a particular distribution of test forms. Experts in each of the subject areas reviewed the proposed fifth-grade forms for appropriateness of content and relevance to the assessment framework."}, {"section_title": "Indirect Measures: Teacher Ratings", "text": "Teachers of ECLS-K children in previous data collection cycles received two questionnaires (A and B) asking about their background, training, and classroom practices. They also received a third 2-32 questionnaire (C) that asked the teacher to rate each ECLS-K child on sets of academic and behavioral measures. By fifth grade many schools no longer have self-contained classrooms and students may be taught by two or more teachers. Therefore, the child's reading teacher completed the questions about the child's language and literacy and social development, as well as providing information about the child's classroom experiences in language and literacy. The questions on classroom experiences addressed both the classroom and student peer characteristics, as well as the instructional and curricular aspects of the classroom. Separate questionnaires pertaining to mathematics performance and instruction and science performance and instruction were provided to the child's mathematics teacher and science teacher respectively. To reduce the cost of data collection, children were randomly assigned to two of the three questionnaires. All children were rated on the language and literacy measure. Therefore, children were rated by teachers on language and literacy, social skills, and mathematics OR language and literacy, social skills, and science. All children were rated by the reading teacher on the social-emotional scale. The following two sections describe the indirect assessments of children's academic performance and socialemotional development that the teachers completed."}, {"section_title": "Academic Rating Scale", "text": "The Academic Rating Scale (ARS) indirect cognitive measures were developed for the ECLS-K to measure teachers' evaluations of students' academic achievement in four domains: language and literacy (reading and writing), mathematical thinking, science, and social studies. The social studies domain was not included in the fifth-grade data collection. The ARS was designed both to overlap and to augment the information gathered through the direct cognitive assessment battery. Although three rating scales measure children's skills and behaviors within the same broad curricular domains as the direct measures, some of the constructs they were designed to measure differ in significant ways. The scope of curricular content represented in the indirect measures was designed to be broader than the content represented on the direct cognitive measures. The direct cognitive battery was less able to measure the process of children's thinking, including the strategies they used to read, solve math problems, or investigate a scientific phenomenon. Due to format limitations, the direct cognitive battery was not able to assess writing skills. Unlike the direct cognitive measures, which were designed to measure gain on a longitudinal vertical scale from kindergarten entry through the end of fifth grade, the ARS was targeted to a specific grade level. The questions ranged from criterion-referenced items (e.g., \"Divides multi-digit problems 2-33 with remainders in the quotient\") to others with a more norm-referenced point of view (e.g., \"Uses various strategies to gain information\" or \"Communicates scientific information\" A background review of the literature on the reliability and validity of teacher judgments of academic performance was conducted (see Meisels and Perry 1996). National and state standards as well as the scope and sequence in published mathematics curricula and the literature on the importance of skills at different grade levels were examined to develop the item pool. The following criteria were used in creating and selecting items for the ARS: Skills, knowledge, and behaviors that reflect the most recent state and national curriculum standards and guidelines; Variables identified in the literature as predictive of later achievement; Direct criterion-referenced items with high level of specificity that called for lower levels of teacher inference; Skills, knowledge, and behaviors that were easily observable by teachers; Items broad enough to allow for diverse populations of students to be evaluated fairly; Some items that overlapped with the content assessed through the direct cognitive battery; Some items that expanded the skills tested by the direct cognitive batteryparticularly those that assess process skills that would be difficult to assess directly given the time constraints; Literacy items that targeted speaking, reading, and writing skills; and Items that reflected developmental change across time."}, {"section_title": "2-34", "text": "Teachers were to rate each child's skills, knowledge, and behaviors on a scale from \"Not Yet\" to \"Proficient\" (see exhibit 2-1). If a skill, knowledge, or behavior had not been introduced into the classroom yet, the teacher coded that item as N/A (not applicable). Teachers from both public and private schools and from different regions of the country and content experts familiar with the elementary grades reviewed the items and made recommendations. Items were then piloted and later field tested in order to gather statistical evidence of the appropriateness of the items for carrying out the overall assessment goals. The pilot testing indicated that the difficulty of the items needed to be set rather high in order to capture the range of abilities represented in fifth grade and to avoid a serious ceiling problem. The items were field tested in the spring of 2002, at the same time as the field test of the direct cognitive assessments. One fourth-grade teacher and one fifth-grade teacher in each of the 49 participating field test schools were asked to participate in the field test by completing five child rating forms: one for the highest achieving child in their class, one for the lowest achieving child in their class, and three for children with average achievement, regardless of whether these particular children were participating in the direct assessment. Participating schools were alternatively assigned \"blue\" and \"white\" designations to ensure equitable distribution of the two forms of teacher ratings. The covers of the teacher questionnaire were blue or white. At \"blue\" schools, teachers were asked to rate these children 2-35 using the ARS for the children's current grade level and the grade level below. At \"white\" schools, teachers were asked to rate children using the ARS for the children's current grade level and the grade level above. A total of 545 teachers, 277 fourth-grade and 268 fifth-grade teachers, completed field test forms. Final items were chosen consistent with the item statistics and representativeness of the content."}, {"section_title": "Social Rating Scale", "text": "The Social Rating Scale (SRS) is an adaptation of the Social Skills Rating System (Gresham and Elliott 1990). Teachers use a frequency scale (see exhibit 2-2) to report on how often the student demonstrates the social skill or behavior described. Factor analyses (both exploratory analyses and confirmatory factor analyses using LISREL) were used to confirm the scales. A parent version of the SRS had been administered in the kindergarten and first-grade years as part of a telephone or in-person survey. (See chapter 2 in the ECLS-K kindergarten and first-grade user manuals for a more detailed description of the parent scales.) The factors on the parent SRS were similar to the teacher SRS; however, the items in the parent SRS were designed for the home environment and, thus, were not the same as the teacher items. It is also important to keep in mind that parents and teachers observe the children in very different environments. Results of the K-1 parent SRS are presented in the 2-36 ECLS-K Psychometric Report for the Kindergarten Through First Grade (NCES 2002-05). A parent version of the SRS was not administered during the third-or fifth-grade parent interview."}, {"section_title": "Self-Description Questionnaire", "text": "In the third-grade data collection and again in the fifth grade, students rated their own academic competence and social skills. The SDQ was designed to determine how children feel about After analyzing different combinations of responses, it was found that a three-to four-point response scale worked best. A four-point scale offered the opportunity to get as much variance as possible within the ability of third-and fifth-graders to interpret the response choices. Children appeared hesitant to use the extreme negatively-laden ends of the response scale; thus the response choices used assessed degrees of truth rather than the degrees of truth and untruth used in the original SDQ-I: \"not at all true,\" 2-37 \"a little bit true,\" \"mostly true,\" or \"very true.\" This also reduced the cognitive demand for the students. The same scale was used in third and fifth grades. The SDQ consisted of 42 statements, including self-ratings of children's competence and interest in reading, mathematics, and \"all school subjects.\" The statements also included self-ratings of children's competence and popularity with peers and problem behaviors with which they might struggle. The following scales were used with ECLS-K students in the fifth and sixth rounds of data collection: In addition to the change in response scale and the addition of problem behavior items, the following adaptations were made to the original SDQ-I. The word \"marks\" was changed to \"grades\" in items asking about their performance in reading, mathematics, and all school subjects. Items that were at similar difficulty levels were eliminated, when it did not affect the reliability, to decrease the number of items in the scale. Students had some difficulty understanding \"look forward to\u2026\", so the wording was changed to \"cannot wait to\u2026.\" Items were added to decrease the number of children who rated themselves as very competent (\"very true\") on all items: \"I can do very difficult math problems\"; \"I like reading chapter books.\""}, {"section_title": "2-38", "text": "For additional information about the changes made to the SDQ-I, see the field test report (Atkins-Burnett, Meisels, and Correnti 2000) of the Self-Description Questionnaire-I for the second and third grades. The third-grade instrument was used in the fifth grade. However, in the fifth-grade the item asking about frustration loaded more heavily on the Sad/Lonely/Anxious scale, while in the third grade it had been more closely related to Anger/Distractibility. 3-1"}, {"section_title": "ANALYSIS METHODOLOGY", "text": "This chapter describes the procedures used in processing the ECLS-K fifth-grade assessment data and producing scores for analysis and for inclusion in user files. Quality control steps are described in section 3.1, followed by an explanation of the methodology used to carry out specialized procedures for psychometric analysis. A three-parameter item response theory (IRT) model was used to put scores obtained on different assessment forms on the same scale for the purpose of comparisons within and across assessment years. The Rating Scale model (Wright and Masters 1982), a one-parameter (Rasch) model, was employed for scoring teacher ratings with multiple categories. Differential item functioning (DIF) procedures identified test items that performed differently for subgroups of the population. The development of longitudinal score scales is described in chapter 5."}, {"section_title": "Quality Control Procedures", "text": "Procedures employed to ensure accuracy in the collection of the cognitive test item data are described in section 4.6 of the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), Fifth-Grade Methodology Report (NCES 2006-037) (Tourangeau et al. forthcoming). In the subsequent steps of converting the resulting raw item response data to final scores, procedures were checked to ensure the accuracy and validity of the results. A series of steps were carried out, from converting raw examinee item responses into scores for individual items, to evaluating item functioning using both classical item analysis and IRT methods, to assembling item data into meaningful and interpretable scores. Throughout the process, attention was given both to checking that steps were carried out correctly, and to verifying that results accurately represented the constructs they were designed to measure. Frequency distributions of raw examinee item responses were produced for each test item to serve as a baseline for confirming the accuracy of later processing steps. Each distribution was compared with the text of the corresponding question in the assessment easel, and with the instructions the assessor used in recording responses, to confirm that responses were coded as expected. For example, for a fouroption multiple choice question, the data file would be expected to contain response codes of 1, 2, 3, and 4, while 1 (correct) or 2 (incorrect) was to have been recorded by the assessor for open-ended questions. Missing data codes (8 = refused, 9 = \"I don't know\") were also counted for each item."}, {"section_title": "3-2", "text": "Within each subject area, children who had not responded to enough test items to receive a score were identified. \"Too few items\" was defined as answering fewer than 10 questions in the routing and second stage forms combined. For the purpose of identifying unscoreable cases, codes for \"I don't know\" were not treated as valid responses. Only items actually attempted by the child were counted toward the scoreability threshold. Before being deleted from further analysis, each \"too few items\" data record was reviewed visually to verify that not enough valid item responses were present. who did not answer the item or any subsequent items (\"not-reached\"). The response frequencies from the item analysis procedure were checked, item by item, against the baseline response frequencies initially obtained on the raw data file to confirm that responses and missing data codes had been interpreted correctly. Summary statistics for each item include P+ (percent correct) and r-biserial (the correlation of item score with total test score, adjusted for the item score being dichotomous). These statistics were reviewed to verify that an unambiguous correct answer key was used for each item, meaning not only that the intended right answer was tagged, but that the tagged answer was in fact functioning as an unambiguous right answer. Evidence for the validity of the answer key comes from two sources: the mean average section score for test takers choosing the correct response should be higher than that of the groups choosing incorrect responses; and the r-biserial should be positive, ideally at least .30 or higher. If these conditions are not satisfied, one of two error conditions could be responsible. An incorrect answer key could have inadvertently been applied or the item may be flawed; that is, the intended correct answer may not really be correct, or there may be two or more equally correct response options. analysis of scores because the IRT score calibration tends to shrink extreme scores to reflect the ability distributions for each round (see section 3.2.2 for a discussion of the effect of the Bayesian approach on chance and perfect scores). No evidence of a ceiling effect was found for any of the fifth-grade tests. The next step in processing the raw item responses was preparing scored item files for input to the IRT calibration procedures, that is, replacing raw response option codes (e.g., 1, 2, 3, 4) with standard codes for correct, incorrect, omitted, and not reached items (1, 0, 2, and 3, respectively). Omitted items were defined as unanswered items that were followed by a response to at least one subsequent item, while unanswered items coded as \"not reached\" had no subsequent items answered. The quality control procedure for confirming that this was done correctly consisted of printing, for a spaced sample of every 1000th case, the raw and scored data record, along with the answer keys, and hand-checking the conversions. In some cases, additional records were needed, so that all variations found in the raw data file could be checked. For example, if the spaced sample of quality control records happened to have only cases that were routed to the low and middle second stage forms, additional records were obtained so that high form score conversions could be verified as well. Producing the scored item files entailed reorganizing the order of test items, because some items appeared in more than one second stage form. In order to strengthen the linkage of each set of forms to the same scale, the scores for these common items needed to be relocated from their original separate locations to a single common location. An item map was developed to direct the reordering of the common items. Scores that were simple sums of number correct on a specified set of items (reading and science cluster scores, reading and mathematics proficiency level scores: see sections 4.1.3 and 4.1.4 for definitions) were computed at this time, checked for the same spaced sample, and inserted into the scored item records. Although number-right proficiency scores do not appear in the user files because the sets of items were not taken by all test takers, the number-right counts for the proficiency levels were needed as input to the IRT calibration step. The fifth-grade scored item files were then combined with the scored item files from kindergarten through third grade. Like the test items shared in common across test forms within fifth grade, items shared in common across rounds were positioned together for IRT calibration, and again, frequency counts were checked to confirm the accuracy of the files. Finally, item-by-item frequency distributions were produced for the scored, reordered files; for the common items, the frequency counts were checked against the aggregates of the frequencies for the separate forms and rounds in which the items originally appeared. These frequency counts, and item means computed on the verified scored item file, provided the basis for checking the results of the IRT scaling steps."}, {"section_title": "3-5", "text": "Section 3.2 below describes PARSCALE, the IRT program used for calibrating item parameters and test takers' ability levels on a scale that is then used to produce scale scores on the whole item pool, and probability scores for the proficiency levels. Statistics and graphs produced by the PARSCALE program and its associated graphing program (Parplot) were used not only to verify the accuracy of the computations, but also to evaluate the reasonableness of the results. PARSCALE produces counts, for each test item, of the number of responses, number of omits, number right, and number wrong found in the input scored data file. Percent correct for each item is also computed. These counts and percents were checked, item by item, against the statistics generated from the scored, reordered data file to confirm that the correct input file was used and that the information it contained was interpreted correctly. Another perspective on quality assurance, aside from verifying the accuracy of data and computations, is the extent to which the scoring model appropriately represents the information in the whole item pool. The r-biserials produced in the classical item analysis steps show the relationship of each test item with the rest of the form on which it appears. The IRT \"a\" parameter, and the PARSCALE plots, demonstrate the cohesiveness of the whole set of items used in kindergarten through fifth grade in each subject (or for science, third to fifth grade only). High \"a\" parameters (1.0 or above) mean that items were strongly related to the underlying construct represented by the item pool. Nearly all reading and mathematics items had \"a\" parameters above 1.0. The science test, with more diversity of content, had somewhat weaker \"a\" parameters, as would be expected for a pool of items that are less strongly related to each other. The graphs generated in conjunction with PARSCALE are a visual representation of the fit of the IRT model to the data. The modeled IRT parameters for each item define the shape and location of a logistic function for the item, which is plotted on a graph. Percentages of observed correct responses for grouped points across the range of estimated ability levels are superimposed on the same graph. The closeness of fit of the data to the logistic function can be interpreted as confirming the appropriateness of the IRT model for scoring the tests. More detail on the IRT model is presented in section 3.2, and a full description of the use and evaluation of the IRT procedures in developing the longitudinal scale appears in chapter 5. The final steps in producing the IRT-based scores consisted of aggregating probabilities of correct responses across the whole item pool in each subject for the scale scores, and obtaining weighted 3-6 means of ability estimates for standardized scores that represented population estimates at each round. These were checked by printing a spaced sample of every 1000th data case, including item and ability parameter estimates, and hand-checking computations. As a final checking step, means and standard deviations of the final score record were obtained, and found to be consistent with expectations. For the scale scores, that would be scale score means that increased from round to round, with ranges that were consistent with the number of items in the pool for each subject. The standardized scores could be explicitly checked, since by definition their weighted mean should equal 50.0 and standard deviation 10.0 within each round."}, {"section_title": "Overview: The Three-Parameter Model", "text": "Measuring the extent of cognitive gains at both the group and individual level requires that the various kindergarten through fifth-grade assessment forms be calibrated on the same scale. The most convenient way of doing this is to use IRT. To successfully carry out such a calibration, the sets of test items should be relatively unifactorial within a subject area (reading, mathematics, or science), with the same dominant factor underlying all test forms. This suggests that there should be a common set of anchor items across adjacent forms and that most, but not necessarily all, content strands be represented in all grade forms. Increments in difficulty demanded in ascending grade forms (kindergarten through fifth grade) can be accomplished by (1) increasing the problem-solving demands within the same content areas and (2) including content in the later forms (in particular third and fifth grade) that taps materials normally found in the curriculum for higher grades, and that build on skills learned in earlier grades. As indicated earlier, IRT (Lord 1980) was used in calibrating the various forms within each content area. A brief introduction to IRT follows with additional information on the Bayesian approach taken here."}, {"section_title": "Overview of Item Response Theory", "text": "The underlying assumption of IRT is that a test taker's probability of answering an item correctly is a function of his or her ability level for the construct being measured and of one or more characteristics of the test item itself. The three-parameter IRT logistic model uses the pattern of right, wrong, and omitted responses to the items administered in a test form and the difficulty, discrimination 3-7 power, and probability of guessing correctly, given the lowest level of ability, of each item, to place each test taker at a particular point, \u03b8 (theta), on a continuous ability scale. Figure 3-1 is an example of a graph of the logistic function for a hypothetical test item. The horizontal axis represents the ability scale, theta. Points along the vertical axis represent the probabilities of answering an item correctly given the level of ability (\u03b8).The shape of the curve is given by the following equation describing the probability of a correct answer on item i as where \u03b8 = ability of the test taker; a i = discrimination of item i, or how well changes in ability level predict changes in the probability of answering the item correctly, at a particular point; b i = difficulty of item i; and c i = \"guessability\" of item i, that is, the probability that a very low-ability test taker will answer item i correctly. The \"c\" parameter represents the probability that a test taker with very low ability will answer the item correctly. In figure 3-1, about 20 percent of test takers with a very low level of mastery of the test material guessed the correct answer to the question. The c parameter will not necessarily be equal to 1/(number of options) (e.g., .25 for a four-choice item). Some response options may, for unknown reasons, be more attractive than random guessing, while others may be less likely to be chosen. The IRT \"b\" parameters correspond to the difficulty of the items, represented by the horizontal axis in the ability metric. In figure 3-1, b = 0.0 means that test takers with \u03b8 = 0.0 have a probability of getting the answer correct that is equal to halfway between the guessing parameter and 1. In this example, 60 percent of people at this ability level would be expected to answer the question correctly. The \"b\" parameter also corresponds to the point of inflection of the logistic function. This point occurs farther to the right for more difficult items and farther to the left for easier ones. Figure 3-2 is an example of a graph of the logistic functions for seven different test items, all with the same \"a\" and \"c\" parameters and with difficulties ranging from b = \u22121.5 to b = 1.5. For each of these hypothetical questions, 60 percent of test takers whose ability level matches the difficulty of the item are likely to answer correctly. Fewer than 60 percent will answer correctly at values of theta (ability) that are less than \"b,\" and more than 60 percent at \u03b8 > b.  "}, {"section_title": "3-8", "text": ""}, {"section_title": "3-9", "text": "The discrimination parameter, \"a,\" has perhaps the least intuitive interpretation of the three IRT parameters. It is proportional to the slope of the logistic function at the point of inflection. Items with a very steep slope are said to discriminate well. In other words, they do a good job of discriminating, or separating, people whose ability level is below the calibrated difficulty of the item (who are much less likely to get it right) from those of ability higher than the item \"b,\" who are much more likely to answer correctly. By contrast, an item with a relatively flat slope is of little use in determining whether a person's correct placement along the continuum of ability is above or below the difficulty of the item. This idea is illustrated by figure 3-3, representing the logistic functions for two test items having the same difficulty and guessing parameters but different discrimination. The test item with the steeper slope (a = 2.0) provides useful information with respect to whether a particular test taker's ability level is above or below the difficulty level, 1.0, of the item: if the answer to this item was incorrect, the person very likely has an ability below 1.0; if the answer is correct, the test taker probably has a \u03b8 greater than 1.0, or guessed successfully. A series of many such highly discriminating items, with a range of difficulty levels (b parameters) such as those shown in figure 3-2, will do a good job in narrowing the choice of probable ability level. Conversely, the flatter curve in figure 3-3 represents a test item with a low discrimination parameter (a = 0.3). There is little difference in proportion of correct answers for test takers several points apart on the range of ability. In this example, knowing whether a person's response to such an item is correct or not contributes relatively little to pinpointing his or her correct location on the horizontal ability axis. With respect to evaluating item quality, \"a\" parameters (the discrimination parameter) should each be over 0.50. Items with \"a\" parameters of 1.0 or above are considered very good. As described earlier, the \"a\" parameter indicates the usefulness of the item in discriminating between points on the ability scale. The \"b\" parameters, or item difficulties for the items, should span the range of abilities being measured. Item difficulties should be concentrated in the range of abilities that contains most of the test takers. Test items provide the most information when their difficulty is close to the ability level of the examinees. Items that are too easy or too difficult for most of the test takers are of little use in discriminating among them. Ideally the \"c\" parameters (the probability of a low ability person guessing correctly) tend to be about .25 or less for four-choice items, but they may vary with difficulty, and of course, the number of options. Open-ended items typically have a \"c\" parameter that is close to 0. In general, the ECLS-K item parameters met these standards. Once there is a pool of test items whose parameters have been calibrated on the same scale as the test takers' ability estimates, a person's probability of a correct answer for each item in the pool can be computed as a function of the person's ability estimate, theta, and the \"a\", \"b\", and \"c\" parameters for the item, even for items that may not have been administered to that individual. The IRT-estimated number correct for any subset of items is simply the sum of the probabilities of correct answers for those items. Consequently, the score is typically not a whole number. In addition to providing a mechanism for estimating scores on items that were not administered to every individual, IRT has advantages over raw number-right scoring in the treatment of guessed and omitted items. By using the overall pattern of right and wrong responses to estimate ability, the model gives very little credit for correct answers to hard items by low ability students. Omitted items are treated as if the examinee had guessed at random. Raw number-right scoring, in effect, treats omitted items as if they had been answered incorrectly. While this may be a reasonable assumption in a motivated test for older students, this may not always be the case in the ECLS-K, where behavioral or other factors may contribute to a child's inability to complete all items."}, {"section_title": "3-11", "text": ""}, {"section_title": "Item Response Theory Estimation Using PARSCALE", "text": "The PARSCALE (Muraki and Bock 1991) computer program computes marginal maximumlikelihood estimates of IRT parameters that best fit the responses given by the test takers. The procedure estimates \"a,\" \"b,\" and \"c\" parameters for each test item, iterating until convergence when a specified level of accuracy is reached. Comparison of the IRT-estimated probability of a correct response with the actual proportion of correct answers to a test item for examinees grouped by ability provides a means of evaluating the appropriateness of the model for the set of test data for which it is being used. A close match between the IRT-estimated probabilities and the empirical probabilities means that the theoretical model accurately represents the empirical data. As indicated earlier, a longitudinal growth study by its very nature consists of subpopulations defined by differing ability levels. That is, after all the kindergarten, first-grade, third-grade, and fifth-grade assessments had been completed (six rounds, counting fall and spring administrations in K-1) there are six recognizable subpopulations of different ability levels, which are tied to the time of testing. For example, the fall-kindergarten subpopulation will have, on average, a lower expected level of performance than that found in each of the remaining followups. Similarly, the average performance of the fall-first graders will be lower than that of the same children the following spring. The bridge sample of second-graders, designed to fill in the gap in testing between first and third grade, represents a seventh subpopulation. When the first round of kindergarten data was collected in fall 1998, relatively few children were routed to the middle-level second-stage forms and even fewer to the high level forms. Thus, there were not enough data on the most difficult items to obtain stable item parameter estimates. As the children were retested in spring-kindergarten and fall-and spring-first grade the following year, more and more data were collected that could be used to stabilize the estimates for the middle-and then the highlevel items. The same is true for the most difficult first-grade items that were repeated in third grade, and for third-grade items repeated in fifth grade. As each round of data became available, item responses were pooled and parameters re-estimated. The pooling of all time points and re-estimating the item parameters, of course, results in a remaking of history in a longitudinal study where intermediate results are published before all the data from all the time periods are available. That is, fall-and spring-kindergarten scores that have been reported and analyzed were later modified somewhat when first-grade data became available. Similarly, all kindergarten and first-grade scores were replaced when the scale was extended to incorporate the third-grade assessment items, and now, with the addition of fifth-grade items to the scales, 3-12 all previous rounds were re-estimated. The use of all data points over time is desirable because it can provide updated estimates of both the item and latent ability parameters throughout the entire ability distribution on a vertical scale. This procedure was used in the vertical scaling that was carried out for National Education Longitudinal Study (NELS:88) (Rock et al. 1995) and for High School and Beyond (Rock et al. 1985;Rock and Pollack 1987). A strength of the PARSCALE and other Bayesian approaches to IRT is that they can incorporate prior information about the ability distribution (i.e., the round of data collection from which an observation is taken) in the ability estimates. This is particularly crucial for measuring change in longitudinal studies. It provides an acceptable way of coping with perfect and chance scores (i.e., correct answers to all items administered, or scores at the guessing level or below). For example, a few very advanced individuals who took the high level mathematics form in spring-first grade might get all the items correct. These individuals, while gifted, may not get perfect scores when they eventually are tested on a harder set of items in later grades. Will this mean that they are less skilled in third grade than in first grade? Probably not. Conversely, individuals scoring at or below the chance level at two time periods may have gained skills that are below the level assessed by the test items. Pooling all available information, that is, pooling all item responses for all people at all time points, and re-calibrating all of the item parameters using Bayesian priors reflecting the ability distributions associated with each particular round, provides for an empirically based shrinkage to more reasonable item parameters and ability scores (Muraki and Bock 1991). The fact that the total item pool is used in conjunction with the Bayesian priors leads to shrinking back the extreme item parameters, as well as the perfect and chance scores, which in turn allows for the potential of some gains even in the upper and lower tails of the distribution. Each of the rounds of data collection in kindergarten through fifth grade is treated as a separate subpopulation with its own ability distribution. The amount of shrinkage is a function of the distance from the subgroup means and the relative reliability of the score being estimated. Theoretically this approach has much to recommend it. In practice, it has to have reasonable estimates of the difference in ability levels among the subpopulations in order to incorporate realistic priors. Essentially, the scales are determined by the linking items, and the initial prior means for the subgroups are in turn determined by the differential performance of the subpopulations on these linking items. For this reason the item pool has been designed to have an overabundance of items linking the forms. This approach, using adaptive testing procedures combined with Bayesian procedures that allow for priors on both ability distributions and on the item parameters, is needed in longitudinal studies to minimize ceiling and floor effects."}, {"section_title": "3-13", "text": "A multiple group version of the PARSCALE computer program (Muraki and Bock 1991) that was developed for the National Assessment of Educational Progress (NAEP) allows for both group ability priors and item priors. A publicly available multiple group version of the BILOG (Mislevy and Bock 1982) computer program called BIMAIN Bock 1987, 1991) has many of the same capabilities for dichotomously scored items only. Since the PARSCALE program was applied to dichotomously scored items in the ECLS-K vertical scaling, its estimation procedure is identical to the multiple group version of BILOG or BIMAIN. PARSCALE uses a marginal maximum likelihood estimation approach and thus does not estimate the individual ability scores when estimating the item parameters but assumes that the ability distribution is known for each subgroup. Thus, the posterior distribution of item parameters is proportional to the product of the likelihood of observing the item response vector, based on the data and conditional on the item parameters and subgroup membership, and the assumed prior ability distribution for that subgroup. More formally, the general model in terms of item-parameter estimation is the same as that used in NAEP and described in some detail by Yamamoto and Mazzeo (1992, p. 158) as follows: f g \u03b8 is a population density for \u03b8 in group g . Prior distributions on item parameters can be specified and used to obtain Bayes modal estimates of these parameters (Mislevy 1984). The proficiency densities can be assumed known and held fixed during item parameter estimation or can be estimated concurrently with item parameters. optimal set of points and weights to best approximate the integral in (3.2) for a broad class of smooth functions. For more general population density function f or for data from multiple populations with known densities, other sets of points (e.g., equally spaced points) can be substituted, and the values of may be chosen to be the normalized density at point Maximization of ) L( \u03b2 is carried out by an application of an EM algorithm (Dempster, Laird, and Rubin 1977). When population densities are assumed known and held constant during estimation, the algorithm proceeds as follows. In the E step, provisional estimates of item parameters and the assumed multinomial probabilities are used to estimate expected sample sizes at each quadrature point for each group (denoted N gk ), as well as over all groups (denoted  ). These same provisional estimates are also used to estimate an expected frequency of correct responses at each quadrature point for each group (denoted r gik ), and over all groups (denoted r = r gik g ik\u2211 ). In the M step, improved estimates of the item parameters, \u03b2 , are obtained using maximum likelihood by treating the N gk and rik as known, subject to any constraints associated with prior distributions specified for \u03b2 . The user of the multiple group version of PARSCALE has the option of fixing the priors on the ability distribution or allowing the posterior estimate to update the previous prior and combine with the data-based likelihood to arrive at a new set of posterior estimates after each major EM cycle. If one wishes to update on each cycle, one can continue to constrain the priors to be normal or their shape can be allowed to vary. The ECLS-K approach was to allow for updating the prior but with the normality assumption. The smoothing that came from the updated normal priors led to less jagged-looking ability distributions and did not tend to overfit the item parameters. Lack of fit in the item parameter distribution would simply be absorbed in the shape of the ability distribution if the updated ability distribution were allowed to take any shape. A similar procedure was used in estimating the item parameters in the National Adult Literacy Study (NALS) (Kirsch et al. 1993). It should be remembered that the solution to equation 3.2 finds those item parameters that maximize the likelihood across all seven time points (the six longitudinal ECLS-K rounds plus the second-grade bridge sample). The present version of the multiple group PARSCALE only saves the subpopulation means and standard deviations and not the individual expected a posteriori (EAP) scores. The individual EAP scores, which are the means of the posterior distributions of theta, were obtained from the C-Group conditioning program, which uses the Gaussian quadrature procedure. This procedure is virtually equivalent to conditioning (e.g., see Mislevy et al. 1992) on a set of \"dummy\" variables defining the ability subpopulation from which an observation comes. The one difference is that the group variances are not restricted to be equal as in the standard conditioning procedure. Conditional independence is an assumption of all IRT models, but as Mislevy et al. (1992) point out, it is a strong assumption that is often violated in practice. However, if one thinks of IRT-based scores as a summarization of essentially the largest latent factor underlying a given item pool, then small 3-15 violations are of little significance. To ensure that there were no substantive violations of this assumption, factor analyses were carried out on the field test forms to confirm that there was a large dominant factor underlying each content area. In addition, all graphs were inspected to ensure a good fit throughout the ability range. For each item, the empirical proportion correct in each round was computed, and compared with the model-based estimated proportion correct based on thetas for the same set of students, that is, the subset of students in the round who had received and responded to the item. Discrepancies between predicted and actual item proportion correct were reviewed for each round. No systematic over-or underprediction was found for any round or for any type of item. Tables B1 to B3 in appendix B list the IRT item parameters for the three subject areas. The items are sorted in ascending order of difficulty (the IRT \"b\" parameter). These tables also show the assessment versions in which the items appeared: one set of tests used for the first four rounds, fall-and spring-kindergarten and fall-and spring-first grade, with new versions used in third and fifth grades. Items that appeared in more than one assessment version served to link the scales across rounds (see section 5.1). Appendix B also shows the mean and standard deviation of the IRT ability estimate, theta, within each round. Bands marking two standard deviations below and above the theta mean illustrate the match of assessment difficulty to the range of student ability in each round. Tables C1 to C3 in appendix C show estimates of the proportion of correct responses to each item that would have been expected if all children had answered all of the items in the kindergarten through fifth-grade item pools at every round. Although each child answered only a small subset of the items each time, IRT ability estimates and item parameters make it possible to estimate performance on all of the items in the pool. In appendix D, tables D1 to D3 show the fit of the IRT model to the item response data. The IRT-estimated probability of a correct response was calculated for each item answered by each child. The average of these probabilities is equivalent to the estimated proportion correct predicted by the IRT model for each answered item. These estimates were compared with the actual proportion correct observed for the answered items. The tables in appendix D show the differences for each item (actual minus predicted), for all items used in each round. For nearly all items in nearly all rounds, these discrepancies were small, indicating good fit of the IRT model to the item response data."}, {"section_title": "Rating Scale Model", "text": "A generalization of the simple Rasch model (1960), the Rating Scale model (Wright and Masters 1982) was used to estimate the scores on the Academic Rating Scale (ARS) described in chapter 3-16 6. In Rasch models (also called one-parameter logistic models), the log odds of the probability of a correct response are a function of the difference between the person's ability and the difficulty of the item. The item discrimination power is constant across the items, and there is no guessing parameter. Applying Rasch models to the data allows one to construct invariant linear measures, estimate the accuracy of the measures (standard errors), and determine the degree to which these measures and their errors are confirmed in the data using the fit statistics (Wright 1999). Like the three-parameter IRT models, Rasch models assume unidimensionality, that is, a single dimension is being measured. The Rating Scale Model (Wright and Masters 1982) was used with the ARS data: where \u03c0 nix is the probability that for child n the teacher chooses category x of ARS item i; \u03b2 n is a person measure indicating the location of child n on the variable (e.g., Mathematical Thinking) being measured; \u03b4 i is the \"difficulty\" of ARS item i; \u03c4 j are response thresholds, or \"step difficulties\" for each response category on the rating scale; m is the maximum category number, x is the current category; and j is a subscript that varies between 0 and m. An easier to understand derivation of this model (Wright 1999) is \u03b2 n is comparable to the theta described in the chapter on the three-parameter IRT model used in estimating the scores for the direct measures."}, {"section_title": "Item Response Theory Estimation Using Winsteps", "text": "Winsteps software ( weighted for the variance of the residual and thus is more influenced by unexpected responses close to the person's trait level (Linacre and Wright 2000). The expected value for the mean square is 1.0. For samples larger than 1000, fit statistics greater than 1.1 indicate departures from expected response patterns that should be examined (Smith, Schumacker, and Bush 1998). Results of the IRT scaling of the teacher Academic Rating Scale are presented in chapter 6."}, {"section_title": "Differential Item Functioning", "text": "Differential item functioning (DIF) as defined here attempts to identify those items showing an unexpectedly large difference in item performance between a focal group (e.g., Black students) and a 3-18 reference group (e.g., White students) when the two groups are \"blocked\" or matched on their total score. It should be noted that any such strictly internal analysis (i.e., without an external criterion) cannot detect bias when that bias pervades all items in the test (Cole and Moss 1989). It can only detect differences in the relationships among items that are anomalous in some group in relation to other items. In addition, such approaches can only identify the items where there is unexpected differential performance; they cannot directly imply bias. A determination of bias implies not only that differential performance on the item is related to subgroup membership but also that the difference is unfairly associated with subgroup membership. That is, the difference is due to an attribute not related to the construct being measured. As Cole and Moss (1989) point out, items so identified must still be interpreted in light of the intended meaning of the test scores before any conclusion of bias can be drawn. It is not entirely clear how the term item bias applies to academic achievement measures given to students with different patterns of exposure to content areas. For example, some students may be in schools where the third-through fifth-grade science curriculum emphasizes life science units, while others may have greater exposure to physical science topics. Both groups may have similar total scores in science, but for one group the life science items may be differentially difficult while the reverse is true for the other group. It is Educational Testing Service's practice to carry out DIF analysis on all tests it designs in order to detect test items with differential performance for subgroups defined by gender and ethnicity. The DIF program was developed at ETS (Holland and Thayer 1986) and was based on the Mantel-Haenszel odds-ratio (Mantel and Haenszel 1959) and its associated chi-square. Basically, the Mantel-Haenszel (M-H) procedure forms odds-ratios from two-way frequency tables. In a 20-item test, 21 two-way tables and their associated odds-ratios can be formed for each item. There are potentially 21 of these tables for each item since there will be one table associated with each total number-right score from 0 to 20. Because of the two-stage, multiform design of the ECLS-K tests, children were assessed with different sets of items, so number-right scores are not based on items of comparable difficulty. Instead, the IRT ability estimate, theta, was used as the stratifying variable, divided into 41 equally spaced intervals. The first dimension of each of the 41 tables is population subgroups (e.g., Whites vs. Blacks), and the remaining dimension is passing versus failing on a given item. Thus, the question that the M-H procedure addresses is whether or not members of the reference group (e.g., Whites), who have the same total ability estimate as members of the focal group (e.g., Blacks), have the same likelihood of passing the item in question. Although the M-H statistic looks at passing rates for two groups while controlling for total score, no assumption need be made about the shape of the total score distribution for either group. The chi-square statistic associated with the M-H procedure tests whether the average odds-ratio for a test item, aggregated across all 41 score levels, differs from unity (i.e., equal likelihood of passing).\nSection 3.4 explains the DIF procedures used for identifying test items that perform differentially for population subgroups. Table 4-7 summarizes the results of the DIF analysis of the fifthgrade reading items. The largest number of C-DIF 1 items was found for performance comparisons of White versus Asian children, with some items favoring the focal group (Asian children) and some the reference group (White children). There are several reasons for these numbers to be larger than those for the other subgroup contrasts. First, the field test of fifth-grade items had too few Asian participants for DIF analysis to be carried out on field test data, so that items with the potential for White/Asian DIF were not identified and removed from consideration for the fifth-grade assessments. Second, many of the Asian children came from a language minority background. Two of the three items on which Asian children performed relatively better than expected were difficult decoding items, while the four questions that were relatively harder for Asian children involved inferences based on stories. (Compare these numbers 1 ETS has developed an \"effect size\" estimate that is not sample-size dependent. Associated with the effect sizes is a letter code that ranges from \"A\" to \" C.\" It is ETS's experiences that effects sizes of 1.5 and higher have practical significance. Effect sizes of this magnitude that are statistically significant are labeled with a \"C.\"\n\n5-1"}, {"section_title": "3-19", "text": "The M-H procedure provides a statistical test of whether or not the average odds-ratio significantly departs from unity for each item. If the probability is .05 or less, then one could say that there is statistical evidence for DIF on the item in question. The problem with this interpretation is twofold. First, a very large number of statistical tests are being performed, one for each item for each pair of subgroups, so low probabilities will be found occasionally even if no DIF is present. Second, if there are two relatively large samples involved, statistical significance will be virtually guaranteed. Given these reservations, ETS has developed an \"effect size\" estimate that is not sample-size dependent. Associated with the effect sizes is a letter code that ranges from \"A\" to \"C.\" It is ETS's experience that effect sizes of 1.5 and higher have practical significance. Effect sizes of this magnitude that are statistically significant are labeled with a \"C.\" Items labeled \"A\" or \"B\" either do not show statistically significant differential functioning for the two groups being compared or have differences that are too small to be important. The fact that an item is identified by the DIF procedure does not mean that the item is necessarily unfair to any particular group. The DIF procedure is merely a statistical screening step that indicates that the item is behaving somewhat differently for one or more subgroups. Thus, the formal DIF analysis is the first step in a two-step screening procedure. The second step is a review of the item content for C-DIF items for evidence that the item may be measuring some extraneous dimension not consistent with the test framework. Items that attain C-level DIF in favor of the majority group are routinely submitted to content analysis by reviewers who were not involved in the development of the test. If the reviewers decide that the item is measuring important content consistent with the test framework and does not contain language or context that would be unfair to a particular group, the item is kept in the test. If the committee finds otherwise, the item is removed from the scoring procedures. DIF procedures were carried out for the fifth-grade assessment items for six sets of contrast groups: males (reference group) compared with females (focal group), and White children (reference group) compared with four other racial/ethnic groups: Black, Hispanic, Asian, and \"Other.\" There were too few Native American and Multiracial children for DIF statistics to be evaluated separately for these groups. Statistics were computed for each item for which the minimum number of required responses, 200 observations for the smaller group, was available. The results of DIF analysis for the fifth-grade assessment are discussed in chapter 4."}, {"section_title": "3-20", "text": "This page is intentionally left blank. 4-1"}, {"section_title": "PSYCHOMETRIC CHARACTERISTICS OF THE ECLS-K DIRECT COGNITIVE BATTERY", "text": "This chapter documents the direct cognitive test results for the fifth-grade round of testing. The types of scores derived from each of the assessments will be described, along with the psychometric characteristics of each. (Notes on the development of longitudinal scales appear in chapter 5, along with a discussion of the analysis of gain scores.) Results for the five kindergarten through third-grade rounds are reviewed, to the extent that they are relevant to interpretation of fifth-grade results or to the measurement of gain. The numbers of observations in some of the tables in this chapter may differ slightly from the sample totals in the ECLS-K public-use data file. These analyses were carried out prior to final determination of cases eligible for the public-use file, and a few cases were deleted from the files. The psychometric results presented here may also differ from statistics reported in the users' manual. National estimates in this chapter are based on all children who had been tested at each round, using the corresponding cross-sectional weights, (C1CW0-C6CW0). Tables in the users' manual are based on the panel sample, that is, the subset of children who participated in all six rounds of data collection, and the longitudinal panel weight (C1_6SC0). The emphasis in this chapter is on the psychometric characteristics of the tests at each round, while the users' manual is designed to provide a reference for comparison with statistics obtained from secondary analyses, which may typically employ multiple rounds of data. Score statistics for all direct cognitive scores are presented in appendix A, with breakdowns by gender, race/ethnicity, socioeconomic status, and school type. Intercorrelations among the subject areas, within and across rounds, are presented in the chapter 5 sections on longitudinal measurement and evaluation of the score scales."}, {"section_title": "Types of Scores", "text": "The scores used to describe children's performance on the direct cognitive assessment "}, {"section_title": "Number-Right Scores", "text": "Number-right scores are counts of the raw number of items a child answered correctly. These scores are useful for descriptive purposes only for assessments that are the same for all children. However, when these scores are for assessments that differ in difficulty, they are not comparable to each other. For example, a student who took the middle difficulty mathematics second-stage form would probably have gotten more questions correct if he or she had taken the easier low form and fewer if the more difficult high form had been administered. For this reason, raw number-right scores are reported only for the first-stage (routing) sections of the assessments, which were the same for all children being assessed using a particular set of instruments, either the kindergarten-first grade (K-1), third-grade, or fifth-grade version. The routing test in each subject area consisted of sets of items spanning a wide range of skills. For example, the reading routing test used for the four kindergarten and first-grade rounds emphasized prereading skills, while the routing tests in third and fifth grades contained easy and difficult decoding words, understanding of words in context, and a series of questions based on a reading passage. An analyst might use the routing test number-right scores to report actual performance on these particular sets of tasks. Because the same routing test was used for the fall-kindergarten through spring-first grade data collections, rounds 1 through 4, score comparisons may be made among these rounds. However, scores on the third-and fifth-grade routing tests were each based on different and more difficult sets of items. The third-and fifth-grade routing test number-right scores should not be compared with the kindergarten or first-grade routing test number-right scores, nor with each other."}, {"section_title": "Item Response Theory Scale Scores; Standardized Scores (T-Scores)", "text": "Broad-based scores based on the full set of assessment items in reading, mathematics, and as well as content that is not necessarily equivalent in difficulty. That is, it would not be correct to assume that a child is doing better in reading than in mathematics because his or her IRT scale score is higher for reading than for mathematics.) A description of IRT methodology may be found in chapter 3. Chapter 5 contains a discussion of the application of IRT to creating longitudinal scores for ECLS-K. Standardized scores (T-scores) provide norm-referenced measurements of achievement, that is, cross-sectional estimates of achievement relative to the population as a whole. A high mean T-score for a particular subgroup indicates that the group's performance is high in comparison with other groups. It does not represent mastery of a particular set of skills, only that the subgroup's mastery level is greater than a comparison group. Similarly, a change in mean T-scores over time reflects a change in the group's status with respect to other groups. In other words, T-scores provide information on status compared with children's peers, while the IRT scale scores and proficiency scores represent status with respect to achievement on a particular criterion set of assessment items. The T-scores may be used as an indicator of the extent to which an individual or a subgroup ranks higher or lower than the national average and how much this relative ranking changes over time. The standardized scores reported in the database are transformations of the IRT theta (ability) estimates, rescaled to a mean of 50 and standard deviation of 10 using cross-sectional sample weights for each wave of data. For example, a fall-kindergarten reading T-score of 45 represents a reading achievement level that is one-half of a standard deviation lower than the mean for the fall-kindergarten population represented by the assessed sample of ECLS-K participants. If the same child had a reading T-score of 50 in fifth grade, this would indicate that the child has made up his or her initial deficit and is reading at a level comparable to the national average."}, {"section_title": "4-4", "text": "Appendix A includes tables of subgroup means for the IRT theta (ability) estimates as well as for the IRT scale scores and T-scores. However, because the theta scores may be difficult to use and interpret, except in combination with item parameters, they are not included in the public-use data files."}, {"section_title": "Item Cluster Scores", "text": "Several item cluster scores are reported for the reading and science assessments. These are simple counts of the number right on small subsets of items linked to particular skills. These clusters of items are also included in the broad-range scores described above. Because they are based on very few assessment items, their reliabilities are relatively low. The reading and science item cluster scores are described in sections 4.3.2 and 4.5.2."}, {"section_title": "Proficiency Levels", "text": "Proficiency levels provide a means of distinguishing status or gain in specific skills within a content area from the overall achievement measured by the IRT scale scores and T-scores. Clusters of four assessment questions having similar content and difficulty were included at several points along the score scale of the reading and mathematics assessments. Clusters of four items provide a more reliable assessment of proficiency than do single items because of the possibility of guessing; it is very unlikely that a student who has not mastered a particular skill would be able to guess enough answers correctly to pass a four-item cluster. The proficiency levels were assumed to follow a Guttman model, that is, a student passing a particular skill level was expected to have mastered all lower levels; a failure should be consistent with nonmastery at higher levels. Only a very small percentage of students in kindergarten through fifth grade had response patterns that did not follow the Guttman model, that is, a failing score at a lower level followed by a pass on a more difficult item cluster. Overall, including all six rounds of data collection, less than 7 percent of reading response patterns and about 3 percent of mathematics assessment results failed to follow the expected hierarchical pattern. This does not necessarily indicate a different order of learning for these children; since most of the proficiency level items were multiple choice, many of these reversals may be due to children guessing."}, {"section_title": "4-5", "text": "The nine reading and nine mathematics proficiency levels identified in the kindergarten through fifth-grade assessments are described in sections 4.3.2 and 4.4.2, respectively. No proficiency scores were computed for the science assessment because the questions did not follow a hierarchical pattern. Two types of scores are reported with respect to the proficiency levels: a single indicator of highest level mastered, and a set of IRT-based probability scores, one for each proficiency level. More information on each of these types of scores is provided below."}, {"section_title": "Highest Proficiency Level Mastered", "text": "Mastery of a proficiency level was defined as answering correctly at least three of the four questions in a cluster. This definition results in a very low probability of guessing enough right answers to pass a cluster by chance. The probability varies depending on the guessing parameters (IRT \"c\" parameters) of the items in each cluster, but is generally less than 2 percent. At least two incorrect or \"I don't know\" responses indicated lack of mastery. Questions that were answered with an explicit \"I don't know\" were treated as wrong, while omitted items were not counted. Since the ECLS-K direct cognitive child assessment was a two-stage design (where not all children were administered all items), and since more advanced assessment instruments were administered in third and fifth grades, children's data did not include all of the assessment items necessary to determine pass/fail for every proficiency level at each round of data collection. The missing information was not missing at random; it depended in part on children being routed to second-stage forms of varying difficulty within each round, and in part on the range of difficulty of the assessments at the different grade levels. In order to avoid bias due to the nonrandomness of the missing proficiency level scores, imputation procedures were undertaken to fill in the missing information. Pass or fail for each proficiency level was based on actual counts of correct or incorrect responses, if they were present. If too few items were administered or answered to determine mastery of a level, a pass/fail score was imputed based on the remaining proficiency level scores only if they indicated a pattern that was unambiguous. That is, a \"fail\" might be inferred for a missing level if there were easier cluster(s) that had been failed and no higher cluster passed; or a \"pass\" might be assumed if harder cluster(s) were passed and no easier one failed. In the case of ambiguous patterns (e.g., pass, missing, fail for three consecutive levels, where the missing level could legitimately be either a pass or a fail), an additional imputation step was undertaken that relied on information from the child's performance on all of the domain items answered in that round of data collection. IRT-based estimates of the probability of a 4-6 correct answer were computed for each missing assessment item and used to assign an imputed right or wrong score to the item. These imputed responses were then aggregated in the same manner as actual responses to determine mastery at each of the missing levels. About 67 percent of the \"highest level\" scores in reading and about 80 percent in mathematics were determined on the basis of item response data alone; the rest utilized IRT-based probabilities for some or all of the missing items. Scores were not imputed for missing levels for patterns that included a reversal (e.g., fail, blank, pass) because no resolution of the missing data could result in a consistent hierarchical pattern. Scores in the data file represent the highest level of proficiency mastered by each child at each round of data collection, whether this determination was made by actual item responses alone, or by a combination of item responses and imputed scores. The highest proficiency level mastered implies that children demonstrated mastery of all lower levels and nonmastery of all higher levels. A zero score indicates nonmastery of the lowest proficiency level. Scores were excluded only if the actual or imputed mastery level data resulted in a reversal pattern as defined above. The highest proficiency level mastered scores do not necessarily correspond to an interval scale, so in analyzing the data, they should be treated as ordinal."}, {"section_title": "Proficiency Probability Scores", "text": "Proficiency probability scores are reported for each of the proficiency levels described above, at each round of data collection. The scores estimate the probability of mastery of each level, and can take on any value from zero to one. An IRT model was employed to calculate the proficiency probability scores, which indicate the probability that a child would have passed a proficiency level, based on the child's whole set of item responses in the content domain. The item clusters were treated as single items for the purpose of IRT calibration, in order to estimate students' probabilities of mastery of each set of skills. The hierarchical nature of the skill sets justified the use of the IRT model in this way. The proficiency probability scores differ from the highest level scores in that they can be used to measure gains over time, and from the IRT scale scores in that they target specific sets of skills. The proficiency probability scores can be averaged to produce estimates of mastery rates within population subgroups. These continuous measures can provide a close look at individuals' status and change over time. Gains in probability of mastery at each proficiency level allow researchers to study not only the amount of gain in total scale score points but also where along the score scale different children 4-7 are making their largest gains in achievement during a particular time interval. For example, subtracting the mathematics level 6 probability at third grade from the mathematics level 6 probability at fifth grade would indicate to what extent a student has advanced in mastery of place value during this time interval. Thus, students' school experiences at selected times can be related to improvements in specific skills."}, {"section_title": "Motivation and Timing", "text": "An important issue in a low-stakes testing situation is motivation: whether the test results really represent the best efforts of the test takers. There are several pieces of evidence to support the conclusion that the ECLS-K participants were motivated to try their best. though not all received scores on the cognitive tests. In the early rounds, limited English proficiency was the primary reason for some children being excluded from the cognitive assessments; this was no longer a factor by fifth grade. There were no time limits on test sections; children were able to proceed at their own speed.  (Tourangeau et al. 2004). As the following tables report, only a very small number of children who were assessed answered too few items for scores to be calculated. Very low: Child doesn't try or attempt many items, even with encouragement 1.7 1.6 1.0 1.2 1.4 0.9 Low: Child frequently says \"I don't know\" without even trying, consistent encouragement needed 9.9 10.4 7.5 8.1 6.8 6.3 Average: Child works on most items, says \"I don't know\" or refuses to answer items after s/he has begun doing some work or after making some attempt to figure out the item.   "}, {"section_title": "4-9", "text": ""}, {"section_title": "4-10", "text": ""}, {"section_title": "Reading Assessment", "text": "The fifth-grade reading test emphasized reading comprehension, with the majority of questions based on one of several reading passages. Additional questions tapped basic skills, including decoding and vocabulary. Children began the reading assessment with a routing test of 26 items, 7 of which were based on a short reading selection. Three items tested understanding of vocabulary words in context. The remaining 16 items were decoding words, administered in ascending order of difficulty. Discontinue rules were in place for the routing test: when a child was not able to read a specified number of the decoding words in each progressively more difficult 4-item cluster, subsequent clusters were not administered. The score on the routing test was used to select one of three second-stage forms, of varying 4-11 difficulty, each consisting of 4 (low and middle forms) or 5 (high form) reading passages, each with 4 to 8 associated questions. The low form also contained four individual word-in-context questions repeated from the earlier rounds. The percentages taking the various second-stage forms in reading followed the expected distributions based on the cut points determined by simulations using field test item parameters and estimates of ability distributions. That is, in round 1 about three-quarters of the children were assigned the low second-stage form based on their routing test performance. In rounds 2 and 3, the largest percentages were assigned the middle-level form. By spring-first grade, round 4, more than three-quarters of the students took the highest level of the second-stage forms. The third-and fifth-grade assessments developed for rounds 5 and 6 were designed to route approximately 50 percent of children to the middle form, with the remaining children about evenly divided between the low and high forms.  . These items were not included in the third-grade reading forms because nearly all children had mastered them by the end of first grade."}, {"section_title": "Samples and Operating Characteristics", "text": "\nMore advanced sets of assessment forms, entirely in English, were developed for third and fifth grades. Scores were calculated only for children who attempted at least 10 items in the routing test and secondstage form combined. The fifth-grade assessment developed for round 6 was designed to route approximately 50 percent of children to the middle form, with the remaining children about evenly divided between the low and high forms. Fewer fifth-graders were routed to the middle difficulty second-stage form than anticipated, and more to the low and high forms. This discrepancy may be due to greater variability in the emphasis placed on mathematics skills (compared with reading) by different schools in the early elementary years. Again, the important point here is not matching the anticipated routing percentages, but selecting the test form that best matches each child's ability level. The cutting points for the routing test were selected to minimize floor and ceiling effects rather than to match target distributions. The very low percentages of perfect and below-chance scores observed in the assessments demonstrate that this strategy was successful in avoiding floor and ceiling effects. NOTE: Rounds 1-4 used the same set of assessment forms; rounds 5 and 6 forms were different sets developed for third and fifth grades. Some children in rounds 1-4 received a Spanish translation of the mathematics assessment; in rounds 5 and 6, all assessments were in English. Approximately 90 percent of the round 6 children were in fifth grade during the 2003-04 school year, 9 percent were in fourth grade, and about 1 percent were in third or other grades. \"Too few items\" refers to the number of children who did not attempt a sufficient number of mathematics items to generate a reliable score. 4-22\nFewer children were routed to the low second-stage form, and more to the high form, than had been anticipated based on field test results. As noted above for reading and mathematics, the success of the two-stage procedure is demonstrated by the absence of ceiling effects. Only one child received a perfect score on the routing plus second-stage items combined. The percentage of \"less than chance\" scores in the table is problematic only for the children taking the fifth-grade low form. Although a substantial number of children received less than chance scores on the middle and high fifth-grade second-stage forms, when their item responses were combined with routing test responses, none were below chance. However, about 5 percent of children routed to the low second-stage form, or about half of 1 percent of the sample, found the science assessment too difficult overall. "}, {"section_title": "4-12", "text": ""}, {"section_title": "4-14", "text": "A set of four relatively difficult decoding items is reported for the third-and fifth-grade assessments. These were words that were unlikely to be in most children's everyday vocabulary, but could be sounded out phonetically. Proficiency levels. The following nine reading proficiency levels were defined for the longitudinal assessments. Level 1: Letter recognition: identifying upper-and lower-case letters by name; Level 2: Beginning sounds: associating letters with sounds at the beginning of words; Level 3: Ending sounds: associating letters with sounds at the end of words; Level 4: Sight words: recognizing common words by sight; Level 5: Comprehension of words in context: reading words in context; Level 6: Literal inference: making inferences using cues that are directly stated with key words in text (for example, recognizing the comparison being made in a simile); Level 7: Extrapolation: identifying clues used to make inferences, and using background knowledge combined with cues in a sentence to understand use of homonyms; Level 8: Evaluation: demonstrating understanding of author's craft (how does the author let you know\u2026), and making connections between a problem in the narrative and similar life problems; and Level 9: Evaluating nonfiction: critically evaluating, comparing and contrasting, and understanding the effect of features of expository and biographical texts. The test items on which the proficiency levels were defined were not used in all rounds of data collection, but only in grades for which their difficulty was appropriate. Level 1-3 items appeared only in the K-1 assessments, level 4 in K-1 and third grades, level 5 in all rounds, levels 6-8 in third and fifth grades, and level 9 in fifth grade only. IRT procedures described in sections 3.2 and 5.2 were used to obtain probability estimates for all levels at all rounds so that longitudinal gains in specific skills could be measured. items a test has, and the greater the variance in ability of test takers, the higher the reliability is likely to be. Approximately 90 percent of the round 6 children were in fifth grade during the 2003-04 school year, 9 percent were in fourth grade, and about 1 percent were in third or other grades. Statistics are unweighted. Statistics for IRT-based scores (percent agreement and reliability of theta) may be different from those in earlier reports due to recalibration of longitudinal scales. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004. Internal consistency (alpha) coefficients for fifth grade are comparable to those obtained for K-1 and third grade. The pattern of alpha coefficients for the routing tests is at least in part due to the number of items. For tests with similar characteristics, a larger number of items will result in a higher alpha coefficient. The K-1 reading routing test had 20 items, with 15 items in third grade and 26 in fifth grade, and the resulting reliabilities follow the same pattern. The alpha coefficients for the second-stage forms in each round are generally lower than those for the routing test due to the restriction in range 4-16 among the children sent to the various second-stage forms. Since the children taking each of these forms are a more homogeneous group with respect to reading performance, the score variances, and thus the alpha coefficients, are lower than they would have been if the whole sample of children had taken each set of items. Only for the high level K-1 second-stage form , which had much greater variance than did the other forms, did the alpha coefficients approach or exceed .90. The restriction in range characteristic of the second-stage forms was counteracted in third grade by the greater number of items in the thirdgrade second-stage forms, relative to the number of items in the routing test. The reliabilities of the second-stage forms are presented for the sake of completeness, although scores on the second-stage forms are not reported separately."}, {"section_title": "Reliabilities", "text": "Split-half reliabilities were computed for the scores that are defined by clusters of items: the decoding score and the individual proficiency level scores. Each of these reliabilities is a transformation of the correlation of a subscore based on half of the items in the cluster with the score based on the other half. The decoding score was reported only for third and fifth grades, not for the earlier rounds. In the fifth-grade round, only three of the four items in this cluster were present in the assessment and the fourth item was imputed to produce a score, so a calculation of split-half reliability based on all items was not possible. Split-half reliabilities are presented for the individual proficiency level scores for information only since \"pass/fail\" on the proficiency levels is reported only in the aggregate and not for each level separately. The split-half reliabilities tend to be highest for levels 1-5, where the items are essentially replicates of the same task (e.g., level 1, recognizing letters of the alphabet). Levels 6-9 are based on comprehension of reading passages, where the questions within a level are more loosely related to each other than for the lower levels, resulting in lower internal consistency within levels. The most appropriate estimate of the reliability of the reading assessment is the reliability of the overall IRT ability estimate, theta. This number is based on the variance of repeated estimates of theta, and applies to all of the scores derived from the theta estimate, namely, the IRT scale scores, T-scores, and proficiency probabilities. Error variance was estimated as the within-person variance of repeated estimates of theta, averaged over all data cases. The ratio of this number to the total variance (betweenperson variance of the posterior mean) is the estimated proportion of total variance that is error variance, and 1 minus the proportion is the estimate of true variance that is reported as the reliability of theta. This reliability index differs from the information function primarily in that it is a single estimate for the whole set of scores, rather than a function evaluated at each point along the continuum. This is the most appropriate estimate of the reliability of the assessment since it reflects the internal consistency of performance on the combined first-and second-stage sections, and for the full range of variance found in 4-17 the sample as a whole. The reliability of theta applies to the scale scores and proficiency probabilities as well, since these scores are nonlinear transformations of the thetas that do not affect rank orderings. It was not possible to apply standard measures of reliability to the \"highest proficiency mastered\" score, for the following reasons. The score is not a set of items replicating the same or similar tasks, so an internal consistency measure such as split-half reliability or alpha coefficient cannot be computed. Nor can the reliability be evaluated based on the variance of repeated estimates of overall ability that was appropriate for the IRT-based scores. The definition of reliability-consistency of measurement under different circumstancessuggested an appropriate way to assess the reliability of the \"highest proficiency level mastered\" score. The score denoting the highest level mastered reduces the series of pass/fail scores on the hierarchical set of proficiency levels to a single score. For example, a student demonstrating mastery of the first five reading levels but not the remaining three would be said to have a \"highest proficiency mastered\" score of five. The question to be answered by a reliability estimate is how likely it would be that the same highest level score would be obtained under other circumstances. In this case, the other circumstances available are not a parallel set of items, but two different methods of arriving at the score. A student's highest level mastered could be determined on the basis of actual item response data alone for more than 80 percent of the sample (see section 4.1.4.1). Alternatively, IRT ability estimates and item parameters could be used to generate pass/fail scores, and the composite highest level scores, for these same students. The percent of cases for which these two different methodologies result in identical or adjacent \"highest level mastered\" scores can be considered to be a reliability estimate. Table 4-6 presents reading scale score means for each round. These scores are estimates of the number of correct answers that would have been expected if at every round each child had been given all of the 186 test items. Four additional items, consisting of difficult decoding words, were used for the purpose of calibrating IRT ability, but deleted from the score scale to bring the representation of content strands more closely into alignment with the framework specifications. One tested item was deleted from scoring due to differential item functioning (DIF) (see section 4.3.5). The IRT procedures described earlier allowed the scale score estimates to be computed based on the subset of questions actually administered to each child at each round. As the assessments progressed from kindergarten through fifth 4-18 grade, more and more of the test items relied on comprehension of reading passages. Inspection of the reading scale score means by round shows an accelerated rate of growth between fall and spring of first grade, round 3 to round 4, and much larger gains between first and third grade, round 4 to round 5. These gains correspond to the times when children would be mastering basic technical reading skills, and then later, acquiring the ability to derive meaning from what they read. The greater variability in reading performance in the later rounds, compared with kindergarten and fall first grade, can be interpreted as an increase in the reading skills gap between low and high achievers. Score statistics for all reading scores, with breakdowns by population subgroups, are presented in appendix A. \nSplit-half reliabilities are shown in the table for the items present at each round: levels 1-3 items were present in the K-1 mathematics assessment only, level 4 in K-1 and third-grade, level 5 in all rounds, levels 6 and 7 in third and fifth grades, and levels 8 and 9 only in the fifth-grade forms. There is no split-half reliability presented for proficiency level 6 in third grade because the items on which it is based did not all appear in the same test form, so no complete data cases were available for evaluation of the reliability. The kindergarten and first-grade split-half reliabilities for levels 1 through 5 were substantially lower than for the corresponding levels in the reading test. While the sets of reading items in each of the lowest proficiency levels were essentially replicates of the same task, the mathematics sets were not as homogeneous with respect to content and skill demands. The greater heterogeneity for the mathematics sets may have contributed to their lower split-half reliabilities. Both alpha coefficients and split-half reliabilities tend to be underestimates of \"true\" reliability, and this tendency may be accentuated by greater diversity of content. The relatively low split-half reliabilities for mathematics proficiency levels 8 and 9 in fifth grade are a consequence of their placement only in the high level form, resulting in restriction in the range of ability of children taking these items. Similar to the reading test, the reliabilities of the third-and fifth-grade theta scores were in the mid .90s. Reliabilities for the K-1 rounds were lower than had been reported for earlier versions, because the score scale extended through fifth grade increasingly emphasized problem solving. The reliability of theta applies to the scale scores and proficiency probabilities as well, since these scores are nonlinear transformations of the thetas that do not affect rank orderings. The percentages of agreement between methods in determining the highest mathematics proficiency level mastered were comparable to those for reading, both for percentage of exact agreement, and percentage of agreement within one level. The greater homogeneity of the reading items for the low compared with high proficiency levels resulted in percent agreement of highest level that tended to go down in the later rounds. Conversely, percent agreement for mathematics, with greater heterogeneity in 4-25 the early rounds, tended to go up. See section 4.3.3 for a detailed explanation of how this score was computed and evaluated.\nAlpha coefficients for the routing test and second-stage forms are somewhat lower than those for reading and mathematics because the science assessment had fewer items in the second-stage forms. This is especially true for the fifth-grade science assessment, in which the routing test was lengthened to 21 items (from 15 in third grade) and the second-stage forms shortened to 14 to 17 items (from 20 in third grade) in order that the items designated for the three science cluster scores would be administered to all children. As a result, the alpha coefficient is higher for the routing test, and lower for the second-stage forms, than was the case in third grade. As in reading and mathematics, the second-stage alpha coefficients were depressed in comparison with the routing test because the range of ability within each form was restricted. The children taking each of these forms are a more homogeneous group with respect to science performance, so the score variance, and thus the alpha coefficient, are lower than they would have been if the whole sample of children had taken each form. Scores for the second-stage forms are not reported separately. The split half-reliabilities for the science clusters were somewhat lower than for the decoding cluster in the reading test (.67). Similarly, the reliability of the IRT theta based on all assessment items, and the scores derived from it, is lower than the mid .90s found in reading and mathematics."}, {"section_title": "Score Statistics", "text": "\nThe scale score means presented in table 4-10 represent estimates of the number of correct answers that would have been expected if each child had been given all of the 153 mathematics items in the pool; that is, all items that appeared in any of the K-1, third-grade, and/or fifth-grade test forms, and were scored. The greatest gains are observed between rounds 4 and 5, spring-first grade to spring-third grade. The variance in mathematics achievement increased markedly for each successive round from fallkindergarten through third grade, leveling off in fifth grade. Score statistics for the mathematics scores and breakdowns by population subgroups are presented in appendix A. Three geometry items that had weak statistics in the third-grade assessment were satisfactory in the fifth-grade round. Data for these items from both rounds were pooled, and the items were included in the longitudinal scale. Table 4-11 presents counts of the C-DIF items for the fifth-grade mathematics forms. There were insufficient numbers of Native American and multiracial children in the sample for DIF statistics to be computed for either group alone, and for the two groups combined as \"other\" no C-DIF items were found. All C-DIF mathematics items were reviewed and found to be relevant to the construct being 4-26 measured by the assessment, and all were retained for scoring. See section 3.4 for an explanation of DIF procedures. \nThird-and fifth-grade science scale score statistics are presented in table 4-14 and represent the number of correct answers that would have been expected if each child had been given all of the 92 items in all of the test forms. Despite the diversity of content in the assessment, all items had acceptable fit to the IRT model. Score statistics for all science scores and breakdowns by population subgroups are presented in appendix A.  Table 4-15 summarizes the results of the DIF analysis of the fifth-grade science items. Only two items were identified as having C-DIF, and one of them favored the focal group (Hispanics). There were too few Native American and multiracial children in the sample for DIF to be evaluated for these children. No C-DIF was found for these two groups combined. The C-DIF science items were reviewed and found to be relevant to the construct being measured by the assessment, so all were retained in the scoring procedures. Section 3.4 explains the DIF procedures used for identifying test items that perform differentially for population subgroups."}, {"section_title": "4-19", "text": "with the small number of C-DIF items, favoring either the focal group or the reference group, for Asian children in the mathematics and science assessments described below.) There were insufficient numbers of Native American and multiracial children in the sample for DIF statistics to be computed for either group alone, and, for the two groups combined as \"other,\" no C-DIF items were found. It should be kept in mind that there were 94 reading items in the fifth-grade reading assessment forms and five sets of comparison groups. Even with insufficient sample sizes for some of the DIF statistics to be computed for some groups, several hundred comparisons were made. The large number of contrasts evaluated means that chance alone could result in statistically significant differences for a few items even where no differential functioning actually exists. All C-DIF reading items were reviewed and found to be relevant to the construct being measured by the assessment. However, one item was judged to be differentially more difficult for Asian children because of cultural considerations and was not scored."}, {"section_title": "Mathematics Assessment", "text": "The fifth grade mathematics framework specifications were identical to those for third grade, in terms of percentages of items in each content strand for the whole item pool, and quite similar to those for the kindergarten and first-grade rounds. The easier items in the routing test and low second-stage form tended to focus on number sense, properties, and operations, while the more difficult forms contained a larger proportion of measurement and geometry items. Greater emphasis was placed on problem solving in fifth grade compared with the earlier rounds. Children began the mathematics assessment with a routing test of 18 items. The score on the routing test was used to select one of three second-stage forms, of varying difficulty, each consisting of 18 (low and middle forms) or 19 (high form) items. 4-20 Table 4-8 presents sample counts and operating characteristics of the adaptive test forms in mathematics. Note that the same set of assessment forms was used for rounds 1-4, fall-kindergarten through spring-first grade. A Spanish translation of the mathematics assessment was administered in kindergarten and first grade to children who were Spanish speakers and whose English language fluency was not sufficiently advanced to take the assessments in English. Children who lacked English language fluency but were not Spanish speakers were excluded from the mathematics assessment in those rounds."}, {"section_title": "4-21", "text": ""}, {"section_title": "Scores Unique to the Mathematics Assessment: Proficiency Levels", "text": "The following nine mathematics proficiency levels were defined for the longitudinal assessments. Level 1: Number and shape: identifying some one-digit numerals, recognizing geometric shapes, and one-to-one counting of up to 10 objects. Level 2: Relative size: reading all single-digit numerals, counting beyond 10, recognizing a sequence of patterns, and using nonstandard units of length to compare objects. Level 3: Ordinality, sequence: reading two-digit numerals, recognizing the next number in a sequence, identifying the ordinal position of an object, and solving a simple word problem. Level 4: Addition/subtraction: solving simple addition and subtraction problems. Level 5: Multiplication/division: solving simple multiplication and division problems and recognizing more complex number patterns. Level 6: Place value: demonstrating understanding of place value in integers to the hundreds place. Level 7: Rate and measurement: using knowledge of measurement and rate to solve word problems. Level 8: Fractions: demonstrating understanding of the concept of fractional parts. Level 9: Area and volume: solving word problems involving area and volume, including change of units of measurement. As was the case for reading, the test items on which the mathematics proficiency levels were defined were not used in all rounds of data collection, but only in grades for which their difficulty was appropriate. Levels 1-3 items appeared only in the K-1 assessments, level 4 in K-1 and third grades, level 5 in all rounds, levels 6-7 in third and fifth grades, and levels 8 and 9 in fifth grade only. (One item in each of the two highest proficiency levels had been present in the third grade test, but without the remaining three, third grade pass/fail scores for the levels could not be computed.) IRT procedures described in sections 3.2 and 5.2 were used to obtain probability estimates for all levels at all rounds so that longitudinal gains in specific skills could be measured.  Approximately 90 percent of the round 6 children were in fifth grade during the 2003-04 school year, 9 percent were in fourth grade, and about 1 percent were in third or other grades. The four test items for mathematics proficiency level 6 did not all appear in the same test form in third grade, so no complete data cases were available for evaluation of split half reliability. Statistics for IRT-based scores (percent agreement and reliability of theta) may be different from those in earlier reports due to recalibration of longitudinal scales. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004. All other things being equal (e.g., the psychometric quality of test items), internal consistency coefficients tend to be higher when tests are longer and lower when the ability range of the test takers is restricted. The internal consistency (alpha) coefficients for the third-and fifth-grade mathematics routing tests were slightly higher than that of the K-1 forms, probably partly due to a slightly longer test (17 and 18 items in third and fifth grades, respectively, vs. 16 items in K-1), and partly because of greater variability in the mathematics achievement of third-and fifth-graders compared with 4-24 earlier rounds. The fifth-grade second-stage mathematics forms have lower alpha coefficients than the routing test because of the restricted variance within each form. While the K-1 high second-stage form had many more items than the other forms (31 items, compared with 18 and 23 for the low and middle K-1 forms, respectively) and thus a higher reliability coefficient, the third-and fifth-grade tests all had about the same number of items in each second stage form, and similar alphas. The reliabilities of the secondstage forms are presented for the sake of completeness, although scores on the second-stage forms are not reported separately."}, {"section_title": "4-23", "text": ""}, {"section_title": "Science Assessment", "text": "The fifth-grade science assessment consisted of a 21-item routing test followed by low, middle, and high difficulty second stage forms of 15, 17, and 14 items, respectively. Content of the science questions was approximately equally divided among life science, earth science, and physical science strands. The science assessment was first added to the ECLS-K cognitive battery in third grade; thus the longitudinal score scale spans only third to fifth grades. Table 4-12 presents sample counts and operating characteristics of the fifth-grade science forms. Scores were calculated only for children who attempted at least 10 items."}, {"section_title": "4-27", "text": ""}, {"section_title": "Scores Unique to the Science Assessment: Cluster Scores", "text": "The science assessment does not have sets of proficiency levels in the same sense as the hierarchical levels for reading and mathematics. Different states and different schools may have quite different sequences for teaching science units. Many science topics are independent of each other, so there is no logical interpretation that would imply that mastery of a set of questions would imply mastery of a set based on different topics. The 21 routing form items of the fifth-grade science assessment tapped a range of basic concepts, with 7 questions each in life science, physical science, and earth science: Life Science: a sample of concepts related to anatomy/health, animal characteristics/ behavior, and ecology;"}, {"section_title": "4-28", "text": "Physical Science: a sample of concepts related to states of matter, sound, physical characteristics, and the scientific method; and Earth Science: a sample of concepts related to the solar system, earth, soil, minerals, and weather. The seven-item clusters administered in the fifth-grade routing test each included the five items tested in the corresponding cluster in third grade. Scores consisting of simple counts of number right for the seven items, as well as for the five-item subsets, were computed for each of the three clusters. Children who omitted more than two items in a cluster were not scored. which may be used for subgroup comparisons. Table 4-13 presents reliability coefficients for the third-and fifth-grade science assessments."}, {"section_title": "4-29", "text": ""}, {"section_title": "4-30", "text": ""}, {"section_title": "DIRECT COGNITIVE ASSESSMENTS: LONGITUDINAL MEASUREMENT", "text": "The study of the relationships between children's school experiences and their gains in academic skills requires accurate measurements of achievement on scales that can be linked across years. This chapter discusses issues in the longitudinal measurement of the reading and mathematics skills of ECLS-K children from fall-kindergarten through spring-fifth grade, and of science skills from springthird grade to spring-fifth grade. The development of the longitudinal scales, including analysis of common items, will be described. Evidence supporting the validity of the measures will be presented. The final section of the chapter will focus on applications: choosing the appropriate scores for analysis and interpreting gain statistics."}, {"section_title": "Development of the K-1-3-5 Longitudinal Scale", "text": "The longitudinal scales necessary for measuring gain over time were developed by pooling the four rounds of kindergarten and first-grade data with the data from the ECLS-K third-and fifthgraders. Data from a small sample of second-graders was included to support the development of the scales by bridging the anticipated gap in ability between first and third grades. The link between the assessment forms used in different rounds relied on the presence of common items shared by successive test forms. The scale scores for kindergarten and first grade were based on the pool of items used in the test forms administered in those grades. Items were added to the pools as each successive round of data was collected: a supplementary set of reading items in first grade, and new assessment forms for the thirdand fifth-grade rounds. Thus the kindergarten reading scale scores were estimates based on a pool of 72 items, with the pool expanding to 92 items for kindergarten and first grade combined, and to 154 and then 186 items as the third-and fifth-grade assessments were added. Each time the item pool was expanded, scores were recalibrated for all rounds to make longitudinal comparisons possible. Each recalibration of the scale score represents the estimated number right on a larger and larger set of items that includes all of the items in the current round as well as all administered in previous rounds. As a result, the scale score for the same child in the same grade changes each time a new set of test items is incorporated and the scale on which the score is based is expanded. second-graders who were not part of the ECLS-K longitudinal sample. It documents the characteristics of the second-grade bridge sample, and shows how the data were used to supplement the longitudinal sample data in establishing vertical scales for measurement of gain. Since the purpose of the bridge sample was to obtain data on the performance of the assessment items, rather than track the progress of the children themselves, their assessment scores are not included in released data files. The absence of a fourth-grade round of data collection in ECLS-K also represented a potential gap in abilities that could affect the longitudinal scale. However, examination of field test results for fourth-and fifth-graders compared with third-graders showed that sufficient overlap of ability levels from third-to fifth-grade existed, and that a fourth-grade bridge sample was unnecessary."}, {"section_title": "Evaluating Common Items", "text": "Linking score scales across grades required not only overlapping ability distributions, but also overlapping test forms. The longitudinal score scales relied on common items that were present in more than one set of assessment forms. These common items permitted the development of a vertical scale suitable for measuring gains in the elementary years. Table 5-1 shows the number of items in each subject area shared by more than one set of assessment forms, as well as the number that appeared in only 5-3 one set. Within rounds, the score scale was supported by items taken by all students within the round (the 12 to 25 items on the routing tests) as well as smaller numbers of items overlapping two or all three second-stage forms.  1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004. The first step in developing the longitudinal scale was evaluating the functioning of the common items at different time points. Although the content and presentation of each of the common items were identical in the three versions of the assessments (K-1, third grade, and fifth grade), it was still possible for the items to function differently. Of course, it would be expected that performance on the items would improve as children advance through school and gain skills, and gains in the probability of a correct answer would be observed. However, the relative difficulty of items in the context of the whole assessment should be maintained for the common items used to anchor the scale. For example, an item \"X\" based on content that had not yet been introduced could, in first grade, be the hardest item in the assessment, and could be found to be much more difficult than a particular set of computation items \"Y.\" By third and fifth grades, when children could have had extensive practice in the skills tapped by \"X,\" it could become much easier than the same set of \"Y\" computations. Such an item, showing a large difference in relative difficulty over time, should not be treated as a common item for the purpose of estimating gains."}, {"section_title": "5-4", "text": "In order to assess the common functioning of the overlapping reading, mathematics, and science items, preliminary estimates of an item response theory (IRT) item and ability parameters were obtained, using all items in the K-1, third-grade, and fifth-grade assessment forms. For this purpose, each common item was initially assumed to be common functioning, and then this assumption was tested as follows. Responses for each of the common items were pooled for all rounds, and a single set of item parameters was estimated for each. Then the actual performance on the common items in each round was compared with performance predicted by the IRT item and ability parameters, in order to identify discrepancies that would indicate differential functioning for any items. Tables 5-2 through 5-4 compare the actual with the predicted proportion correct for each of the reading, mathematics, and science items used in more than one assessment version, based on the children who answered each of the items in each round of data collection. Note that the comparisons of observed vs. predicted percent correct for each question can be carried out only for children who For almost all of the items, the difference between the observed and predicted percent correct was very small, indicating common functioning of the items across time periods and good fit to the IRT model. Only one item common to the K-1 and third-grade mathematics assessments had a sufficiently large discrepancy in actual compared with predicted proportion correct to warrant separate calibration. This item was deleted from the common item list used for anchoring the scale, but retained for each (K-1 and third-grade) assessment form, with separate sets of item parameters. No non-commonfunctioning items were found in the reading and science assessments. Positive numbers correspond to actual proportion correct that is higher than predicted by the IRT model, and negative numbers to actual proportion correct that is lower than predicted. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004."}, {"section_title": "5-5", "text": ""}, {"section_title": "5-7", "text": "Table 5-3. Mathematics assessment, actual minus predicted proportion correct: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004. 5-9 5-10"}, {"section_title": "IRT Calibration and Scoring", "text": "IRT calibration was carried out using the PARSCALE program as described in chapter 3. One reading item was deleted from the item pool because of differential item functioning (DIF) for a population subgroup (see section 4.3.5). The estimation of reading item parameters and student abilities was based on the remaining 190 unique items that appeared in all forms of the reading assessments, with the 69 items common to two or more of the assessment versions serving to anchor the scale. Four of the 190 items were deleted from the final scale scores so that the scale would be more closely aligned with framework specifications, leaving 186 items in the final reading scale. No mathematics items were deleted because of differential functioning in grade 5 (two had been deleted for this reason in earlier rounds). The K-1, third-grade, and fifth-grade mathematics scale is based on 153 unique mathematics items in all assessment forms, including 40 common to more than one version of the assessment. The science scale is based on 92 unique items in third and fifth grades, including 27 common to both rounds. For each item, the IRT calibration resulted in a set of three item parameters that define a logistic function associated with the item. The height of the function at any point along an ability range corresponds to the estimated probability of a correct answer on the item for a person at that ability level. The tables in appendix B show the item parameters, in ascending order of difficulty (IRT \"b\" parameter). Each of the rounds of data collection, kindergarten through fifth grade (plus the bridge sample), was treated as a separate subpopulation with its own ability distribution for the purpose of IRT calibration. This feature of PARSCALE and other Bayesian approaches to IRT provides for an empirically based shrinkage toward subpopulation means for extreme ability estimates, low and high. This shrinkage is particularly important for a longitudinal study, where the focus is on measuring gain and it is important to avoid floor and ceiling effects. See section 3.2.1 for additional details. Table 5-5 presents theta (ability) means and standard deviations for the subpopulations of the reading, mathematics, and science calibrations. The theta estimates are standardized to mean = 0.0 and standard deviation = 1.0 for all rounds combined.  1998-99, 1999-2000, 2001-02, and 2003-04, plus bridge sample 2001-02."}, {"section_title": "5-11", "text": "IRT scale scores, T-scores, and proficiency scores were derived from the IRT item parameters and ability estimates. As described above and in section 4.1.2, the set of three parameters for each item defines a logistic function corresponding to the probability of a correct answer for a test taker with a given ability level. At each time point, the ability estimate for each child was used in combination with the item parameters to generate a probability for each item. These probabilities were summed over all items in the assessments to get a scale score representing an estimate of the number of items the student would have answered correctly if he or she had taken all 186 reading items, all 153 mathematics items, or all 92 science items. The T-scores in the database are theta estimates transformed to a metric of mean = 50.0, standard deviation = 10.0 within each round, using cross sectional sample weights. Proficiency scores required an additional IRT calibration step. Section 4.1.4 describes the selection of a hierarchical series of mastery levels in reading, and another series in mathematics, marked by clusters of four items at each level. Nine such levels were defined in each subject, based on items from the K-1, third-grade, and fifth-grade assessments. Children were judged to have passed a level (score = 1) if they answered at least three of the four items correctly, and to have failed if at least two wrong answers were given (score = 0). Children with fewer than three right or two wrong answers (because they omitted items, or because the items defining a particular level were not included in the assessment forms they received) were not scored for the purpose of IRT calibration. The proportion of omitted responses in all subjects in all rounds was negligible, so nearly all children had pass or fail scores on the proficiency 5-12 levels whose items were administered to them. After the initial PARSCALE estimates of item parameters and abilities were obtained, parameters for the proficiency levels were estimated. Ability levels were held constant, and the proficiency level clusters (scored as right, wrong, or not administered) were treated as items for estimating item parameters. In essence, this resulted in prediction of mastery level proficiency from estimates of ability levels derived from all items administered to each child. Extremely close fits of the logistic functions to the proportion correct from item-response-based cluster scores (1 or 0) were observed for all levels in all rounds, for both reading and mathematics. No proficiency levels were defined for the science test because the more diverse curriculum content meant that acquisition of knowledge and skills in science could not be assumed to follow a hierarchical pattern. The parameters for the reading and mathematics proficiency levels are shown in table 5-6. The very high \"a\" parameters are consistent with the assumption that 4-item clusters are more reliable than single items, and do a better job of discriminating among ability levels. It would be very difficult for a low-ability student to pass a 4-item cluster by guessing; the guessing parameters (c) were all fixed at zero. "}, {"section_title": "5-13", "text": "The IRT parameters permit calculation of probability of proficiency at each mastery level in the same manner as described above for individual items. These probabilities are included in ECLS-K user files. Applications of the proficiency probability scores in measuring status and gain are discussed in section 5.3. An additional proficiency score, the highest proficiency level mastered at each round, is described in section 4.1.4.1. Tables A38 and A39 in appendix A present subgroup differences with respect to mastery of the level that represents the modal \"highest level\" score within each round."}, {"section_title": "Evaluating the K-1-3-5 Longitudinal Scale", "text": "Section 5.1 described the construction of the longitudinal score scales and IRT calibration of parameters. This section will address the issue of the validity of the score scales as measures of student achievement and growth between fall-kindergarten and spring-fifth grade. The validity issue will be examined from several perspectives: Do the tests measure the right content? Is the difficulty of the tests suitable for children's ability levels? Do the scores constitute a cohesive scale suitable for longitudinal measurement? What is the relationship of the cognitive test scores to scores in different rounds and different subjects, and to teacher ratings and student self-ratings? How do the ECLS-K results compare with findings from other studies?"}, {"section_title": "Do the Tests Measure the Right Content?", "text": "Evidence for the appropriateness of the tests' content can be obtained from two sources: expert judgments and psychometric results. Chapter 2 describes the design of the tests and development of test frameworks (see section 2.1.2). Curriculum experts and teachers provided input with respect to cognitive skills that are both typically taught and developmentally important. Test frameworks in each subject were developed accordingly, and test items in each set of assessments were selected to conform as closely as possible to framework specifications. Field test item pools and proposed final form item selections were reviewed by experts, and content and presentation of items were modified in response to their recommendations."}, {"section_title": "5-14", "text": "Appendix C illustrates a psychometric perspective on appropriateness of test content. For each item, the assessment version(s) in which it appears are noted: K-1 for the assessment package used for fall-and spring-kindergarten and fall-and spring-first grade (rounds 1 through 4), 3 for the third-grade assessment (round 5) and 5 for fifth-grade (round 6). IRT calibration allows us to estimate performance on each item for all rounds, even rounds in which the item was not used. In general, the largest gains in estimated proportion correct are observed in rounds in which the items were actually administered. For example, for items used only in the K-1 assessments, the greatest gains tend to occur in rounds 1 through 4, with relatively little gain later on. Conversely, for items that were introduced in the third-and fifthgrade forms, IRT estimates show that very little gain would have been observed in these items if they had been presented in the earlier rounds. The common items used to link K-1 with third-grade forms, or third with fifth grade, tend to show gains across a wider range of rounds. (An exception to the general pattern of assessment forms matching gains is found for certain difficult items that were included in a supplementary reading form designed to avoid a possible ceiling effect in first grade. The supplementary form was administered only to first-graders who had performed unusually well on the standard set of K-1 forms. These items were too difficult for the majority of first-graders, and showed little gain until the third-and fifth-grade rounds). The match of assessment forms to estimated performance gains suggests that the content of the tests reflected what children had been learning during the intervening time periods."}, {"section_title": "Is the Difficulty of the Tests Suitable for Children's Ability Levels?", "text": "Chapter 2 describes the development of two-stage adaptive tests in each subject area for kindergarten and first grade, with similar assessments assembled for the third-and fifth-grade rounds. The adaptive tests were designed to maximize reliability per unit of testing time by matching test difficulty to children's ability level, while minimizing frustration or boredom that could occur if children received tests that were much too difficult or much too easy (see section 2.1.1). Separate assessment packages for K-1, third, and fifth grades focused on items of appropriate difficulty for the grade(s) in which they were administered, while containing enough overlapping items to support the longitudinal scale. Psychometric results indicate that this approach, the combination of grade-appropriate assessment versions plus alternative second-stage forms within grade, was successful in selecting items of appropriate difficulty for the test takers. Evidence that the tests contained items that were of appropriate difficulty for both the individual children taking them, and in the aggregate for the rounds in which they were administered, can 5-15 be found in analysis of the test data. Chapter 2 discusses the importance of avoiding floor and ceiling effects, that is, tests that are much too hard (floor effect) or much too easy (ceiling effect) for a substantial number of test takers. Floor and ceiling effects preclude accurate measurement of children at the extremes of the ability distribution. This is particularly important in a longitudinal study, where score scales with floor and ceiling effects can attenuate measurement of gain for the lowest and highest achieving students. Chapter 4 reviews the operating characteristics of the ECLS-K assessment forms, including the percentages of below-chance (floor effect) and near-perfect (ceiling effect) scores (see test probably should have contained a few more of the easiest items suitable for the lowest achieving students."}, {"section_title": "Do the Scores Constitute a Cohesive Scale Suitable for Longitudinal Measurement?", "text": "Evidence presented in appendix D supports the validity of the score scales for longitudinal measurement, in two ways. Examination of IRT \"a\" parameters suggest that the item pools within each subject are strongly related to a single underlying factor that is consistent across rounds from fallkindergarten through spring-fifth grade. The fit statistics in appendix D demonstrate that the IRT model appropriately represents the test data collected in each round. Tables of proportion correct in appendix C provide an additional perspective on the score scales derived from the IRT estimates. If each test taker had answered all of the items in the kindergarten through fifth-grade item pools at every round of data collection, it would be possible to measure the cohesiveness of the scale by observing alpha coefficients and item biserials. Of course, it would have been neither reasonable nor practical to administer the whole item pools to everyone at every round. The IRT \"a\" parameters provide the same type of insight into the cohesiveness of a set of test items (see section 3.2.1). This parameter represents item discrimination, or the ability of an item to discriminate, or separate, people whose ability level is above or below the calibrated difficulty of the item. In other words, the \"a\" parameters indicate how strongly each item is related to the underlying construct being measured by the test, with values of 1.0 or above indicating a strong relationship. Values above 1.0 for most of the items in a test constitute evidence that there is a strong underlying factor. Of the 186 items in the reading scale, only 14 have \"a\" parameter values less than 1.0, and half of those are picture-vocabulary items. The rest are based on either listening comprehension, understanding conventions of print, or difficult vocabulary words. All of the items tapping reading skills, from simple letter recognition and decoding in kindergarten to comprehension of complex reading passages in the later rounds, have \"a\" parameters above 1.0. Results for mathematics were quite similar, with only six of 153 items having \"a\" parameters below 1.0. Of these, four were geometry items, which were identified in the field test as being slightly weaker than the other mathematics categories with respect to cohesiveness of the scale, but were included in the item pool to conform to framework specifications. Examination of the reading and mathematics \"a\" parameters provide evidence that the item 5-17 pools and resulting score scales are strongly related to an underlying construct that spans the kindergarten through fifth grade years. Results for the science assessment are strikingly different, with \"a\" parameters for nearly three-quarters of the items (68 out of a total of 92) falling below 1.0. This is a consequence of the composition of the science item pool, which is a mix of life science, earth science, and physical science topics. Furthermore, the science assessments did not assume a hierarchical structure in the science curriculum comparable to the patterns for reading and mathematics. In other words, it would be possible for children in some schools to master difficult material relating to the life sciences without having been exposed to basic concepts in earth science, or vice versa. That is the reason that proficiency levels within the science assessments were neither hypothesized nor identified. The relatively low \"a\" parameters for the science items do not necessarily, however, make IRT methodology inappropriate for calibration of the science scale. In fact, for all except 14 of the 92 items, \"a\" parameter values were .60 or above. This suggests that although there may be multiple factors influencing item responses, they are all related to each other. Section 5.1.2 explains the use of the fit statistics presented in tables 5-2 through 5-4 in evaluating the functioning of common items tying the score scale together across assessment versions. Appendix D presents the same fit statistics for all items in the assessments. In each round, proportion correct for all children who answered each test item was compared with the proportion correct predicted by the IRT model for the same children. The extremely small differences between actual and predicted percent correct for virtually all items at all rounds-even the science items-support the idea that the IRT model appropriately represents the test data collected in each round. Appendix C shows the proportion correct estimated by the IRT procedures for each item at each round, for all children tested. The increase in proportion correct over time, and the fact that increases took place at the rounds expected given the content and difficulty of the items, provides further evidence that the IRT results appropriately model achievement growth."}, {"section_title": "5-18", "text": ""}, {"section_title": "Relationship of the Cognitive Test Scores to Scores in Different Rounds and Different", "text": "Subjects, and to Teacher Ratings and Student Self-Ratings Table 5-7 shows correlations of test scores in each round with scores in the same subject in other rounds. Note that for both reading and mathematics, correlations are highest near the diagonal, and get progressively lower toward the lower left corner of each set. In other words, scores in each subject appear to be most closely related to the most recent or subsequent score, and least closely related to rounds that are more distant. "}, {"section_title": "5-19", "text": "For example, the highest correlation (i.e., best predictor) for round reading is the round 5 reading measure, with a correlation coefficient of .85. Previous reading scores are also strongly correlated with round 6 reading, but the relationship becomes weaker going back in time. While reading ability at kindergarten entry is a good predictor of fifth-grade achievement (correlation = .58), other factors present in the intervening years presumably have an important influence as well. Measures of family and school circumstances that relate to student achievement are provided in the ECLS-K database. Exploration of the role these variables play in predicting later achievement is beyond the scope of this report. Correlations of scores across subjects within rounds are presented in table 5-8. These statistics are consistent with estimates from numerous studies. The relationship between reading and mathematics achievement tends to be close to .75 at all ages from early childhood through high school.  1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004. A final perspective on construct validity of the assessments is their relationship with concurrent measures within the ECLS-K survey, namely, the teacher ratings and student self-ratings. These are discussed in chapter 7, section 7.2."}, {"section_title": "Comparison of ECLS-K Results With Findings From Other Studies", "text": "An additional way to validate the ECLS-K measures would be to compare ECLS-K results with findings of similar studies. Ideally, these \"similar studies\" would have tests that measure the same content as the ECLS-K tests, and have similar formats, administration procedures, reliabilities, and 5-20 scoring methodology. Children would be sampled from the same population as ECLS-K (children entering kindergarten in the U.S. in fall 1998, with some sample freshening in later rounds), with adequate sample sizes and comparable sampling and weighting procedures. Children would be in the same grades at the same ages as the ECLS-K sample, and the similar studies would have been conducted in the recent past. Definitions of subpopulations to be compared would be the same for ECLS-K and the comparison studies. If all of these conditions were met, a finding that ECLS-K results were similar to those of a similar study would support the validity of the ECLS-K cognitive test scores. Conversely, discrepancies between the results would call into question the validity of the findings of one or both studies. Unfortunately, no published studies could be found that replicate the ECLS-K structure closely enough to expect that findings would be consistent. A key result that would be important to replicate would be estimates of test score gaps between population subgroups. Numerous studies document the existence of score gaps, especially between Black and White students at various ages and in various subjects. A great deal of work has been done on studying correlates of these gaps, and cross-sectional and longitudinal changes in the gaps. While there is general consensus on factors that influence score gaps, there is by no means consensus on the size of the gaps (Jencks and Phillips 1998;Rouse et al. 2005). In fact, there is no truly reliable estimate of the Black-White score gap, for all of the following reasons, and others: Comparability depends on exactly what is being measured: verbal tests that focus primarily on vocabulary seem to find larger gaps than reading tests with more diversity of content. Time frame is important: in recent decades, such factors as desegregation, trends in class sizes, and increased preschool attendance have tended to reduce the size of Black-White score gaps in the early years of school. Findings from recent studies may be quite different from those carried out 10 or 20 years ago (Grissmer et al. 1998). Studies of \"stereotype threat\" show that context and mode of administration may influence performance, especially for Black children (Steele et al. 1998). Many studies are not meant to be nationally representative, but may be based, for example, on children in a certain type of preschool program, or children in a particular city that may not closely resemble the characteristics of the ECLS-K nationally representative sample. A literature review and in-depth study of test score gaps is well beyond the scope of this report. However, a few similarities and differences with other findings may be noted that may aid in the evaluation of the consistency of ECLS-K findings with other studies."}, {"section_title": "5-21", "text": "Several studies reported Black-White score gaps for children age 5 or 6, or in kindergarten or first grade, of about one standard deviation, based on the Peabody Picture Vocabulary Test. Some of these studies noted that vocabulary gaps for children of this age are typically larger than gaps found in measures of early reading (Rock and Stenner 2005;Jencks 1998;Phillips, Crouse, and Ralph 1998;Phillips, Brooks-Gunn, et al. 1998). The Black-White score gap in the ECLS-K reading test, which contained some picture vocabulary items but primarily focused on early literacy, was indeed smaller: about four-tenths of a standard deviation. A consensus finding of several studies was that Black-White gaps tend to widen after children enter school (Grissmer et al. 1998;Ferguson 1998). This was consistent with ECLS-K results. In the ECLS-K, the Black-White reading score gap increased only slightly, from .40 to .42 of a standard deviation (SD), by spring-kindergarten, but then expanded to .52 SD by spring first grade, and .71 SD in rounds 5 and 6, when most children were in third-and fifth-grade. A similar pattern was found for mathematics, with an initial fall kindergarten gap of .61 standard deviations widening to .82 and then .85 SD in rounds 5 and 6. The study that is perhaps most comparable with ECLS-K may be the National Assessment of Educational Progress (NAEP) 2003 assessments in reading and mathematics. Both were large-scale samples representing a national population, in about the same year and similar grades. The content specifications for the ECLS-K tests were derived from NAEP frameworks. Similar IRT methodology was used in producing score scales. Table 5-9 shows reading and mathematics score gaps for selected subgroups for the NAEP 2003 fourth-grade assessment and for ECLS-K rounds 5 and 6, which consisted primarily of third-and fifth-graders. NAEP subgroup differences in reading and mathematics scores were quite similar to the differences found in both ECLS-K rounds for the male/female comparison and for White students compared with Black students. For all of these contrasts except for the male/female difference in mathematics scores the ECLS-K results showed slightly smaller gaps than those found for NAEP fourth-graders. Statistics for White/Hispanic score gaps are included in table 5-9 although race/ethnicity for Hispanic students is defined differently in NAEP and ECLS-K. As similar as the NAEP and ECLS-K assessments are in many respects, there are also some important differences that relate to the comparability of measurements of gaps: NAEP used a cross-sectional sample of children in fourth grade in 2003; the ECLS-K sample was a longitudinal followup of a kindergarten sample. Most of the children tested in round 5 in 2002 were in third grade, and in round 6 in fifth grade, but about 9 percent of children in each round were not yet in the modal grade."}, {"section_title": "5-22", "text": "The NAEP cross sectional sample could be expected to contain more recent immigrants than the ECLS-K longitudinal sample. ECLS-K round 6 children tested in spring 2004 had all joined the sample in kindergarten or first grade, during the 1998-99 or 1999-2000 school year, so they had been attending school in the U.S. for at least four years. The NAEP sample consisted of children in fourth grade in 2003, and included children whose early schooling may have taken place in another country with instruction in a language other than English. Tests in ECLS-K were individually administered, while NAEP used group administration. NAEP had two different sources for race variables: school records and student selfreport (Table 5-9 shows race/ethnicity from school records). ECLS-K used a composite race/ethnicity variable, derived from parent interviews in most cases, and from a variety of other sources when parent reports were unavailable."}, {"section_title": "5-23", "text": "NAEP reported scores for Hispanics as a group, while ECLS-K had separate categories for Hispanic, race specified and Hispanic, race not specified."}, {"section_title": "Applications", "text": "This section describes issues in selection and use of scores for analyzing status and gain in cognitive skills. Appendix A includes breakdowns by gender, ethnicity, socioeconomic status (SES), and school type for all of the fifth-grade direct cognitive measures. For measures that can be compared with the analogous scores in earlier rounds, results for rounds 1 through 5 are included in the tables as well. Examination of similarities and differences, within and across rounds, may suggest research questions that can be addressed by the ECLS-K data and assist with formulation of analysis models."}, {"section_title": "Choosing Appropriate Scores for Analysis", "text": "Each of the types of scores described earlier measures children's achievement from a slightly different perspective. The choice of the most appropriate score for analysis purposes should be driven by the context in which it is to be used: A measure of overall achievement versus achievement in specific skills; An indicator of status at a single point in time versus growth over time; and A criterion-referenced versus norm-referenced interpretation."}, {"section_title": "Item Response Theory-Based Scores", "text": "The scores derived from the IRT model (IRT scale scores, T-scores, proficiency probabilities) are based on all of the child's responses to a subject area assessment. That is, the pattern of right and wrong answers, as well as the characteristics of the assessment items themselves, are used to estimate a point on an ability continuum. This ability estimate, theta, then provides the basis for criterionreferenced and norm-referenced scores. The IRT scale scores are overall, criterion-referenced measures of status at a point in time. They are useful in identifying cross-sectional differences among subgroups in overall achievement level 5-24 and provide a summary measure of achievement useful for correlational analysis with status variables, such as demographic, school type, or behavioral measures. The IRT scale scores may be used as longitudinal measures of overall growth. However, gains made at different points on the scale have qualitatively different interpretations. For example, children who make gains in recognizing letters and letter sounds are learning very different lessons from those who are making the jump from reading words to reading sentences, although the gains in number of scale score points may be the same. Comparison of gain in scale score points is most meaningful for groups that started with similar initial status. The standardized scores (T-scores) are also overall measures of status at a point in time, but they are norm-referenced rather than criterion-referenced. They do not answer the question, \"What skills do children have?\" but rather, \"How do they compare with their peers?\" The transformation to a familiar metric with a mean of 50 and standard deviation of 10 facilitates comparisons in standard deviation units. T-score means may be used longitudinally to illustrate the increase or decrease in gaps in achievement among subgroups over time. T-scores are not recommended for measuring individual gains over time. The IRT scale scores or proficiency probability scores may be used for that purpose. Proficiency probability scores, derived from the overall IRT model, are criterion-referenced measures of proficiency in specific skills. Because each proficiency score targets a particular set of skills, they are ideal for studying the details of achievement, rather than the single summary measure provided by the IRT scale scores and T-scores. They are useful as longitudinal measures of change because they show not only the extent of gains but also where on the achievement scale the gains are taking place. Thus, they can provide information on differences in skills being learned by different groups, as well as the relationships of skill gains with processes, both in and out of school, that correlate with learning specific skills. For example, high SES kindergarten children showed very little gain in the lowest reading proficiency level, letter recognition, because they were already proficient in this skill at kindergarten entry. At the same time, low SES children made big gains in basic skills, but most had not yet made major gains in reading words and sentences by the end of kindergarten. Similarly, the best readers in fifth grade may be working on learning to make evaluative judgments based on reading material, which would show up as large gains in reading level 8. Less skilled readers may show their largest gains between third and fifth grade at levels 6 or 7, literal inference and extrapolation. The proficiency level at which the largest change is taking place is likely to be different for children with different initial status, background, and school setting. Changes in proficiency probabilities over time may be used to identify the process variables that are effective in promoting achievement gains in specific skills."}, {"section_title": "5-25", "text": ""}, {"section_title": "Scores Based on Number Right for Subsets of Items (Non-IRT Based Scores)", "text": "The routing test number-right and item cluster scores do not depend on the assumptions of the IRT model. They are derived from item responses on specific subsets of assessment items, rather than estimates based on patterns of overall performance. Highest proficient level mastered also, in theory, is derived from item responses, although a relatively small number of IRT-based estimates were substituted for missing data. Routing test number-right scores for the fifth-grade reading, math, and science assessments are based on 25, 18, and 21 items respectively (15, 17, and 15 items for the same subjects in grade 3; and 20, 16, and 12 items for the K-1 reading, math, and general knowledge assessments). They target specific sets of skills and cover a broad range of difficulty. These scores may be of interest to researchers because they are based on a specific set of assessment items, which was the same for all children who took the fifth-grade assessment. Note that comparisons of routing test number-right scores may be made within rounds 1 through 4, because the same set of assessment forms was used in those rounds, and all children received the same sets of routing items. However, scores on the third-and fifthgrade routing tests were each based on different and more difficult sets of items. The fifth-grade routing test number-right scores should not be compared with the routing test number-right scores for earlier rounds. Item cluster scores in reading (e.g., Decoding Score Gr 5) and science (e.g., Life Science Gr 5) are based on a count of the number correct for a particular set of items. Users may wish to relate these scores to process variables to get a perspective that is somewhat different from that of the hierarchical levels of skills. However, with only three to seven items in each of these item cluster scores, reliabilities tend to be relatively low (see sections 4.3.2, 4.3.3, 4.5.2, and 4.5.3). Highest proficiency level mastered is based on the same sets of items as the proficiency probability scores but consists of a set of dichotomous pass/fail scores, reported as a single highest mastery level. Pass/fail on each of the individual levels in the set is based on whether children were able to answer correctly at least three out of four actual items in each cluster. Over all rounds of data collection, for about 33 percent of these scores in reading, and about 20 percent in mathematics, the item data were supplemented with IRT-based estimates to avoid complications associated with non-random missing data. The highest proficiency level mastered should be treated as an ordinal variable."}, {"section_title": "5-26", "text": ""}, {"section_title": "Choosing the Correct Sample Weight", "text": "The ECLS-K database contains several versions of sample weights, designed to identify students participating in selected rounds and produce national estimates accordingly. Cross sectional weights should be used only when analyzing data from a single round of data collection. forthcoming)."}, {"section_title": "Notes on Measuring Gains", "text": "This section outlines approaches to measuring gains that rely on multiple criterionreferenced points to identify different patterns of student growth. It describes how analysts might use the proficiency probability scores to address policy questions dealing with subgroup differences in achievement growth over time. Individual and group differences in the amount of gain given a fairly standard treatment (e.g., a year or two of schooling) can be relatively trivial compared with individual and group differences in where the gains take place. It is more likely that one will see substantial subgroup differences in initial status than in scale score point gains, suggesting that the gains being made by individuals at different points on the score scale are qualitatively different. Thus, analysis of the total IRT scale score without explicitly taking into consideration where the gain takes place tells only part of the story. The ECLS-K design utilized adaptive assessments to maximize the accuracy of measurement and minimize floor and ceiling effects and then to develop an IRT-based vertical scale with multiple criterion-referenced points along that scale. These points, the nine reading and nine mathematics proficiency levels that were described in chapter 4, model critical stages in the development of skills. Criterion-referenced points serve two purposes at the individual level: (1) they provide information about changes in each child's mastery or proficiency at each level, and (2) they provide information about where on the scale the child's gain is taking place. This provides analysts with two options for analyzing 5-28 achievement gains and relating them to background and process variables. First, gains in probability of proficiency at any level may be aggregated by subgroup, and/or correlated with other variables. Second, the location of maximum gain may be identified for each child by comparing the gains in probability for all of the levels, and focusing on the skills the child is acquiring during a particular time interval. The probabilities of proficiency at any level may be averaged to estimate the proportion of children mastering the skills marked by that level. For example, the spring-first grade mean for mathematics level 5, \"Multiply/Divide,\" was 0.22, analogous to 22 percent of the first-grade population demonstrating mastery of this set of items. The mean probability at the end of third grade, 0.75, is equivalent to a population mastery rate of 75 percent (see table A33). While most children were making their largest gains between first and third grade at level 5, a small number of children were advancing their skills in solving word problems based on rate and measurement, level 7. The mastery rate for level 7 advanced from near zero at the end of first grade to 13 percent at the end of third grade (shown in Another approach to the analysis of gain entails computing differences in probabilities of proficiency between any two rounds for all of the proficiency levels. The largest difference marks the mastery level where the largest gain for a given child is taking place: the \"locus of maximum gain. "}, {"section_title": "5-30", "text": "This page is intentionally left blank. 6-1"}, {"section_title": "PSYCHOMETRIC CHARACTERISTICS OF THE SELF-DESCRIPTION QUESTIONNAIRE", "text": "Chapter 6 describes the selection and development of the Self-Description Questionnaire (SDQ), which asked children to rate their competence and interest in school subjects and relationships with peers as well as behaviors that might interfere with their academic and social competence. This chapter provides details of the psychometric characteristics of this instrument."}, {"section_title": "Self-Description Questionnaire (SDQ)", "text": "Beginning in the third-grade data collection in the ECLS-K, children were asked to provide self-assessments of their academic and social skills. In the SDQ, fifth-grade students rated their perceived competence and interest in reading, mathematics, and all school subjects. 1 They also rated their perceived competence and popularity with peers and reported on problem behaviors with which they might struggle. The Externalizing Problems scale included questions about anger and distractibility, while the Internalizing Problems scale included items on sadness, loneliness, and anxiety. For further detail on the development and content of the SDQ, see chapter 2. Students rated whether each item was \"not at all true,\" \"a little bit true,\" \"mostly true,\" or \"very true.\" Six scales were produced from the SDQ items. The scale scores on all SDQ scales represent the mean rating of the items included in the scale. Students who responded to the SDQ answered virtually all of the questions, so treatment of missing data was not an issue. As with most measures of social-emotional behaviors, the distributions on these scales are skewed (negatively skewed for the positive social behavior scales, and positively skewed for the problem behavior scales). The reliability for scores is lower for scales with only six items, and for the Internalizing Problem Behaviors (see table 6-1). This is consistent with other research because internalizing problems are less visible and more difficult to rate. Weighted means and standard deviations for these scales are shown in table 6-2. The mean score in each academic area (Perceived Interest/Competence in Reading, Math, All Subjects) was lower in the fifth-grade data collection than in third grade, as were the mean scores for the Externalizing and Internalizing Problems scales. The mean score for Peer Relations increased slightly from third to fifth grade. 6-2  SDQ score statistics for subpopulations are presented in tables 6-3 through 6-8. Children who had been retained (third-or fourth-graders in this round) rated themselves lower in the academic interest/competence areas and rated themselves as having more behavior problems, both internalizing and externalizing problems. Their mean self-rating on peer competence was similar to that of their fifth-grade peers. 6-3 6-4 6-5 6-7 6-8 Intercorrelations with other scales are presented in table 7-13 in chapter 7. 7-1"}, {"section_title": "PSYCHOMETRIC CHARACTERISTICS OF THE INDIRECT MEASURES", "text": "Chapter 7 describes the selection and development of the fifth-grade indirect measures. The indirect measures were teacher evaluations of children's academic and social skills. This chapter provides details of the psychometric characteristics of these instruments. In addition, the relationships between the direct and indirect cognitive measures are explored."}, {"section_title": "Teacher Measures", "text": "In the spring-fifth grade data collection (round 6), teachers of the sampled children were asked to evaluate each child's academic and social skills. The fifth-grade teacher measures were similar in design to those administered in kindergarten, first, and third grades, and shared some common items with the earlier instruments.  (Pollack et al. 2005). Differential item functioning (DIF) analysis of the Academic Rating Scale (ARS) was not appropriate for several reasons. First, the ratings were produced by the teacher, not by direct observation of the child. Therefore, there is a confounding source of difference, namely the teacher's attitudes or potential bias, that cannot be separated from the child's performance. Second, even if it could be determined that teachers' ratings were completely accurate and unbiased, DIF would also be impossible for the ARS because there is no satisfactory criterion for matching. DIF analysis depends on the assumption that, for subsets of individuals matched on overall ability level, performance on each test item should be about the same. The ARS scales are too short to provide a matching criterion (i.e., each item represents too big a part of the total score needed for matching), and there is no independent measure of 7-2 the same construct that could be used for this purpose. The direct cognitive score would not be an appropriate criterion because the ARS includes process questions that are not represented in the direct cognitive tests. Third, factor analysis of the ARS scales found a very strong first factor, which suggests that a \"halo\" effect is operating. This suggests that DIF analysis using the total ARS score as the criterion would probably find no evidence of DIF simply because a teacher who rated a child high on one item would tend to rate the same child high on all items. It was probably not the items that were functioning differently, but it may have been teachers differentially rating children. This is not a psychometric characteristic of the scale itself. The order of item difficulties was examined by subgroup to check for any major problems in how the items were interpreted in relation to the students who were rated. No problems were identified. The ordering of item difficulties was similar with all subgroups, and the item difficulties clustered closely together. DIF analysis of the Social Rating Scale (SRS) was not carried out because DIF assumptions are not relevant to behavioral and attitudinal measures. The basic premise of DIF is that for subsets of individuals matched according to a criterion (such as a score on the total set of items or some external criterion), similar item performance for different subgroups should be observed. Significant deviation from this could indicate that an item is measuring differently for different groups. For behavioral measures such as SRS, there can be no expectation that ratings should be the same for different groups. Any group differences in ratings may reflect either legitimate real differences in the group's attitude or behavior on an item or set of items, or factors having to do with the standards or attitudes of the rater (teacher), not differential functioning or flaws in the items. It is possible that the interaction between teachers' attitudes and demographic characteristics, and the demographic characteristics, cognitive ability, and behavior of children may influence the social and academic ratings assigned to children. Secondary analysis of these relationships may reveal differences in the standards used in the academic (ARS) and social (SRS) ratings."}, {"section_title": "Indirect Cognitive Assessment Using the Academic Rating Scale (ARS)", "text": "The ARS evaluated achievement in the three domains that are also assessed in the direct cognitive assessment battery: language and literacy (reading), mathematical thinking, and science. For each of the scales, the child's primary teacher in the area completed the ratings."}, {"section_title": "7-3", "text": "The ARS was designed both to overlap and to augment the information gathered through the direct cognitive assessment battery. Although the direct and indirect instruments measure children's skills and behaviors within the same broad curricular domains with some intended overlap, several of the constructs they were designed to measure differ in significant ways. Most importantly, the ARS includes items designed to measure both the process and products of children's learning in school, whereas the direct cognitive battery assesses only the products of children's achievement. The scope of curricular content represented in the indirect measures was designed to be broader than the content represented on the direct cognitive measures. Unlike the direct cognitive measures, which were designed to measure gain on a longitudinal scale spanning kindergarten entry through the end of fifth grade, the ARS is targeted to a specific grade level. The questions range from criterion-referenced items (e.g., \"reduces fractions to lowest denominator\") to others with a more norm-referenced point of view (e.g., \"uses various strategies to gain information\"). Teachers evaluating the children's skills were instructed to rate each child compared with other children of the same age/grade level. Response options for each item ranged from 1 (\"not yet\") to 5 (\"proficient\"). See section 2.3 of this report for additional details on the design and development of the ARS instrument. The Rating Scale model used to estimate ARS scores is described in detail in chapter 3. The reliability of scores for each of the scales is very high (table 7-1). The summary fit statistics for persons and items are acceptable for all the scales (table 7-2). The fit statistics for the step calibrations indicate that the lowest category (\"Not yet\") was used less than expected. The ARS scores were scaled to have a low of \"1\" and a high of \"5\" to correspond to the 5-point rating scale that teachers used in rating children on these items, but they should not be interpreted as mean scores. The item difficulties and student scores are placed on a common scale. Students have a high probability of receiving a high rating on items below their scale score and a lower probability of receiving a high rating on items above their scale score. For example, a child whose scale score is 4.0 would have a greater than 50 percent probability of having received a rating of \"5\" on all items whose difficulty is below 4.0 on the scale. Students who received maximum ratings on all the items or minimum ratings on all the items were assigned an estimated score. The ARS scales were designed to provide information on children's abilities at a given point in time, rather than provide a measure of change over time. The sets of items developed for the fifth-grade ARS ratings were different from the items used in the kindergarten, first-grade, and third-grade instruments. Although the fifth-grade item stems have some similarities to those used in the earlier forms, the extended item descriptions include grade-appropriate performance criteria that describe the level of proficiency a child should have reached in order to receive the highest rating. For example, \"demonstrates an understanding of place value\" appeared in the versions of the ARS, first through fifth grade. In springfirst grade, this item was described as \"by explaining that fourteen is ten plus four, or using two stacks of ten and five single cubes to represent the number 25\" while the fifth-grade ARS described this item stem as \"compares decimals to the thousandths place (1.04 > 1.009).\" Obviously, a spring-first grade rating with respect to the first description does not represent the same level of skill as the same rating based on the fifth-grade criterion. As a result, the ARS score metric is different at each point in time, and change scores should not be used to compare fifth-grade ratings with those from earlier rounds. Covariance 7-5 models may be used to compare teachers' ratings of performance in different grades. Before using these variables in such analyses, the distribution of the samples should be assessed to determine if the assumption of normal distribution is met. On the ARS, teachers indicated \"not applicable\" when the knowledge, skill, or behavior has not been introduced to the classroom. Because some children might already have had this skill (from home or other opportunities for learning), the \"not applicable\" ratings were treated as missing data and the child's score was estimated based on the items on which the child was rated. Although the Rasch program estimates scores for all children based on the information provided, scores estimated on a limited number of responses are less reliable than scores with more ratings. ARS scores were computed only if at least 60 percent of the items in the scale were given ratings. In other words, if more than 40 percent of the items in a scale were not rated, the score was set to missing. The weighted means and standard deviations for the fifth-grade ARS scores are shown in table 7-3. Score breakdowns for population subgroups are presented in tables 7-18 through 7-21 at the end of this chapter. "}, {"section_title": "Floor and Ceiling", "text": "As noted in the section on the development of the ARS, the criteria for some of the items was set very high to avoid serious ceiling problems and some items were included at a level designed to avoid most floor problems. Because teachers could not be expected to respond to items far outside the range of grade-level performance (they would have little opportunity to observe this as well), it was unavoidable in this type of measure that some children would have perfect scores.  Tables 7-5 to 7-7 provide the estimates of difficulty for each of the items. Higher difficulty values mean that teachers rated fewer students as proficient on those items. The ordering of the items in difficulty is consistent with what would be expected based on reviews of curriculum. The range of difficulty is more limited than expected.  Tables 7-8 to 7-10 provide standard errors (SE) for each of the ARS scores for fifth grade."}, {"section_title": "7-7", "text": "The \"Score\" column is the sum of the raw score ratings. \"Measure\" is the score estimated using the Rating Scale model. The column labeled \"SE\" is the corresponding standard error of measurement for those scores. These standard errors can be used in analytic models to correct for the heteroskedasticity of scores. NOTE: E = estimated extreme score. The \"Score\" column is the sum of the raw score ratings. \"Measure\" is the Rasch-based score. The column labeled \"SE\" is the corresponding standard error of measurement for those scores. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99, spring 2004. .14 \u2020 Not applicable. NOTE: E = estimated extreme score. The \"Score\" column is the sum of the raw score ratings. \"Measure\" is the Rasch-based score. The column labeled \"SE\" is the corresponding standard error of measurement for those scores. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), spring 2004. .17 \u2020 Not applicable. NOTE: E = estimated extreme score. The \"Score\" column is the sum of the raw score ratings. \"Measure\" is the Rasch-based score. The column labeled \"SE\" is the corresponding standard error of measurement for those scores. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), spring 2004."}, {"section_title": "7-9", "text": ""}, {"section_title": "7-10", "text": ""}, {"section_title": "Social Rating Scale (SRS)", "text": "The Social Rating Scale (SRS) is an adaptation of the Social Skills Rating System (Gresham and Elliott 1990). As part of a self-administered questionnaire, fifth-grade reading teachers were asked to judge how often students exhibited certain social skills and behaviors. (In kindergarten and first grade, SRS questions had been asked of both teachers and parents.) Teachers used a frequency scale to report on how often the student demonstrated the social skill or behavior described (1 = never to 4 = very often). The 24 SRS items used in kindergarten and first grade were included in the third-and fifth-grade SRS, and two new items were added. The same form was used in third and fifth grades. included interactions with both adults and peers. In both third and fifth grade, students who were rated higher on self-control were also rated higher on interpersonal skills that involved peers. Thus, in addition to the Self-Control and Interpersonal social abilities scale scores, a Peer Relations scale score was included. This additional scale combines responses on both the interpersonal and self-control scale items that relate to peers. Although 24 of the 26 fifth-grade SRS items were the same as items in the kindergarten-first grade (K-1) instrument and the fifth-grade form was identical to the third-grade form, teachers might 7-11 place different interpretations on the meaning of the items at different time points. Therefore these scores would be most appropriately used as covariates rather than as change scores. The score on each SRS scale is the mean of ratings on the items included in the scale. Scores were computed only if the student was rated on at least two-thirds of the items in that scale. Exploratory factor analyses were used to provide evidence of the validity of the scales with this sample. The split-half reliabilities for the scores of the teacher SRS scales were high (table 7-11). Reliabilities are nearly identical for fifth-graders in round 6 and for children who were not yet in fifth grade, so the table contains only reliabilities for the whole sample. These reliabilities are also nearly identical to round 5 results. Weighted means and standard deviations for these scales are shown in table 7-12. About 90 percent of the children whose teachers provided social ratings data were in fifth grade during the round 6 data collection, and about 10 percent were in third or fourth grade. Numbers in the table are for fifthgraders, with scores for children who at round 6 were still in third or fourth grade shown in parentheses. The number of children who had advanced to sixth or seventh grade by round 6 was too small (less than 0.5 percent) to be analyzed separately. SRS score statistics for subpopulations are presented in tables 7-14 through 7-22 at the end of this chapter, with scores for fifth-graders shown separately from those of children in third and fourth grade. Care should be taken when entering these scales into the same analysis due to problems of multicollinearity who were still in third or fourth grade in the fifth-grade round were very similar to patterns for the ongrade-level children, and were also consistent with results in earlier rounds. The correlations between the internalizing problem scale and the other scales were weaker for third-and fourth-graders, ranging from .22 to .32. "}, {"section_title": "Discriminant and Convergent Validity of the Direct and Indirect Measures", "text": "As indicated earlier, the patterns of correlations among selected measures provide evidence for their construct validity, that is, whether they measure what they purport to measure. Systematic evidence for construct validity is often described in terms of convergent and discriminant validity. Convergent validity means that two different measures of the same trait or skill ought to have relatively high correlations with each other. Conversely, discriminant validity means that two measures that are designed to measure two different traits or skills should show lower correlations with each other than each does with its matching measure. (An exception to this model is high correlations that may be found for different measures that constitute a predictive relationship.) More complete discussions of construct validity may be found in Campbell and Fiske (1959) and Campbell (1960). Correlations among 12 fifth-grade measures were examined for evidence of convergent and discriminant validity. These measures included three teacher ratings of children's achievement (ARS), Theta, Math Theta, and Science Theta, the direct cognitive assessment scores. It is instructive to compare the discriminant validity within each of the two sets of cognitive measures (the extent to which scores measuring different constructs should be different), as well as the convergent validity across sets (the extent to which scores should be closely related to other measures of the same construct). The correlation for the direct cognitive measure of reading with mathematics is .75. The correlation between the direct reading and mathematics scores had a slight but steady decline from the kindergarten through third-grade round, suggesting the possibility of some divergence of the two skills over time. However, the correlations are higher among the direct measures in fifth grade than they were in third grade. The direct cognitive mathematics and science measures were read to the children so that, as much as possible, the reading demands were removed from the content areas. However, the text was available for children to review and thus children who were better readers may have had additional support from the text. Alternatively, children with more limited literacy skills may spend more of their day working on literacy skills with less time available for the content areas. Children with stronger literacy skills might have the opportunity to read more widely in content areas, and the increased exposure to mathematics and science content might increase the development of the concepts and vocabulary needed for success in the content areas. From kindergarten through the third-grade data collection, the corresponding correlations for ARS were consistently high. In kindergarten through third grade, the same teacher responded to all areas of the ARS (ARS language/literacy, the ARS mathematical thinking, and the ARS science measure). Thus, there was additional method variance in the correlation. In the fifth grade, the teachers who taught reading, mathematics, and science rated the children on the relevant ARS form; thus, the ARS ratings may have been completed by different teachers. The correlations of the ARS language/literacy with the ARS mathematics scale and with the ARS science scale were lower for this data collection period when compared with previous data collections and when compared with the relationships among the direct measures. When one examines the cross-correlations from a convergent validity perspective, patterns are similar to those found in third grade. Relationships are stronger within measures than across measures of similar constructs. One would expect that the direct score in each subject area would be more closely related to the indirect measure of the same subject than to measures of the other subjects. This is true for language/literacy (ARS with direct reading) and mathematical thinking (ARS with direct math), although the differences are relatively small. This represents an improvement in convergent validity compared with kindergarten and first-grade results, where correlations of the ARS mathematical thinking score with the direct cognitive reading were almost exactly the same as those with the direct mathematics score. In both third and fifth grade, the ARS science scale was more highly correlated with both reading and mathematics direct scores than it was with the direct science measure that should have been a closer match. The direct science measure showed similar correlations with all three ARS measures in the fifth grade. In the third grade, the correlation of the direct science was greater with the ARS Literacy than with ARS Mathematics or ARS science. The indirect ARS measures show consistently higher relationships with teacher-rated behavioral scales such as teacher SRS ratings of approaches to learning, interpersonal behavior, and selfcontrol than do the comparable direct cognitive measures (table 7-13). The higher intercorrelations of the SRS with the indirect cognitive measures may be partly due to the fact that they do indeed measure process in addition to products. Teachers' views of children's attitudes and behavior may also influence their ratings of all content domains. Only the reading teacher completed the SRS ratings. The SRS scales show the strongest relationships with the literacy related scores, although this is true of both the indirect ARS language and literacy scale and the direct cognitive measure in reading. The differences in relationships are smaller than in previous rounds. As was found in previous rounds, among the teacher rated scales of social skills, the SRS approaches to learning scale has the strongest relationship with each of the ARS scales and direct cognitive scores. Correlations of children's self-ratings with other measures, while still low, are stronger than in the third grade. In third grade, only the self-rating of reading competence with the teacher rating of 7-16 language/literacy and the self-rating of competence in all school subjects with the teacher rating of approaches to learning, reached correlations of .20. The slightly stronger correlation in fifth grade suggests an increased awareness of academic performance. Nevertheless, it continues to appear that children use different criteria than teachers use when rating academic competence. Teachers are more knowledgeable about national standards and had more specific criteria to use when rating academic competence. Children's self-perceptions reflect not only the feedback that they receive from others about their performance, but may also be influenced by self-comparison with peers in their environments. Thus, some children's scores may reflect the \"big fish, little pond\" phenomenon described by Marsh and his colleagues (Marsh et al. 1995). As noted earlier, score breakdowns for population subgroups for the indirect measures are presented in tables 7-14 through 7-22. The means for the ARS will be presented first, followed by the SRS.          Wright, B.D., and Masters, G.N. (1982). Rating Scale Analysis: Rasch Measurement. Chicago, IL: MESA Press."}, {"section_title": "7-17", "text": ""}, {"section_title": "7-20", "text": ""}, {"section_title": "7-21", "text": "Yamamoto, K., and Mazzeo, J. (1992). Item Response Theory: Scale linking in NAEP. Journal of Education Statistics, 17: 155-173."}, {"section_title": "R-6", "text": "This page is intentionally left blank. A-1   A-4 Table A4. Reading IRT scale score, K-5 scale (range of possible values: 0 to 186): School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Round  Table estimates are based on cross-sectional weights within each round (C1CW0, C2CW0, C3CW0, C4CW0, C5CW0, C6CW0). Estimates for kindergarten through fifth grade have been put on a common scale to support comparisons. Subgroup counts do not sum to total sample because demographic variables are missing for some cases. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004. A-5 Table A5. Mathematics IRT scale score, K-5 scale (range of possible values: 0 to 153): School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 (C1CW0, C2CW0, C3CW0, C4CW0, C5CW0, C6CW0). Estimates for kindergarten through fifth grade have been put on a common scale to support comparisons. Subgroup counts do not sum to total sample because demographic variables are missing for some cases. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004. A-6 Table A6. Science IRT scale score, 3-5 scale (range of possible  A-7 Table A7. Reading T-scores, standardized within round (range of possible values: 0 to 96): School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Round  Table estimates are based on cross-sectional weights within each round (C1CW0, C2CW0, C3CW0, C4CW0, C5CW0, C6CW0). Estimates for kindergarten through fifth grade have been put on a common scale to support comparisons. Subgroup counts do not sum to total sample because demographic variables are missing for some cases. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004. A-8 Table A8. Mathematics T-scores, standardized within round (range of possible values: 0 to 96): School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Table estimates are based on cross-sectional weights within each round (C1CW0, C2CW0, C3CW0, C4CW0, C5CW0, C6CW0). Estimates for kindergarten through fifth grade have been put on a common scale to support comparisons. Subgroup counts do not sum to total sample because demographic variables are missing for some cases. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004. A-9 A-10 Table A10. Reading IRT theta score,K-5 scale (range of possible values: -5 to 5): School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Round  1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004. A-11 Table A11. Mathematics IRT theta score,K-5 scale (range of possible values: -5 to 5): School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Round  Table estimates are based on cross-sectional weights within each round (C1CW0, C2CW0, C3CW0, C4CW0, C5CW0, C6CW0). Estimates for kindergarten through fifth grade have been put on a common scale to support comparisons. Subgroup counts do not sum to total sample because demographic variables are missing for some cases. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004 A-12 Table A12. Science IRT theta score, 3-5 scale (range of possible values: -5 to 5): School years 2001-02 and2003-04 Round    A-20 Table A20. Probability of proficiency, reading level 1: letter recognition (range of possible values: 0.0 to 1.0): School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Round  Table estimates are based on cross-sectional weights within each round (C1CW0, C2CW0, C3CW0, C4CW0, C5CW0, C6CW0). Estimates for kindergarten through fifth grade have been put on a common scale to support comparisons. Subgroup counts do not sum to total sample because demographic variables are missing for some cases. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004 A-21 Table A21. Probability of proficiency, reading level 2: beginning sounds (range of possible values: 0.0 to 1.0): School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Round  Table estimates are based on cross-sectional weights within each round (C1CW0, C2CW0, C3CW0, C4CW0, C5CW0, C6CW0). Estimates for kindergarten through fifth grade have been put on a common scale to support comparisons. Subgroup counts do not sum to total sample because demographic variables are missing for some cases. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004 A-22 Table A22. Probability of proficiency, reading level 3: ending sounds (range of possible values: 0.0 to 1.0): School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Round  Table estimates are based on cross-sectional weights within each round (C1CW0, C2CW0, C3CW0, C4CW0, C5CW0, C6CW0). Estimates for kindergarten through fifth grade have been put on a common scale to support comparisons. Subgroup counts do not sum to total sample because demographic variables are missing for some cases. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004 A-23 Table A23. Probability of proficiency, reading level 4: sight words (range of possible values: 0.0 to 1.0): School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Round  Table estimates are based on cross-sectional weights within each round (C1CW0, C2CW0, C3CW0, C4CW0, C5CW0, C6CW0). Estimates for kindergarten through fifth grade have been put on a common scale to support comparisons. Subgroup counts do not sum to total sample because demographic variables are missing for some cases. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004 A-24 Table A24. Probability of proficiency, reading level 5: words in context (range of possible values: 0.0 to 1.0): School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Round  Table estimates are based on cross-sectional weights within each round (C1CW0, C2CW0, C3CW0, C4CW0, C5CW0, C6CW0). Estimates for kindergarten through fifth grade have been put on a common scale to support comparisons. Subgroup counts do not sum to total sample because demographic variables are missing for some cases. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004 A-25 Table A25. Probability of proficiency, reading level 6: literal inference (range of possible values: 0.0 to 1.0): School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Round  Table estimates are based on cross-sectional weights within each round (C1CW0, C2CW0, C3CW0, C4CW0, C5CW0, C6CW0). Estimates for kindergarten through fifth grade have been put on a common scale to support comparisons. Subgroup counts do not sum to total sample because demographic variables are missing for some cases. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004 A-26 Table A26. Probability of proficiency, reading level 7: extrapolation (range of possible values: 0.0 to 1.0): School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Round  Table estimates are based on cross-sectional weights within each round (C1CW0, C2CW0, C3CW0, C4CW0, C5CW0, C6CW0). Estimates for kindergarten through fifth grade have been put on a common scale to support comparisons. Subgroup counts do not sum to total sample because demographic variables are missing for some cases. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004 A-27 Table A27. Probability of proficiency, reading level 8: evaluation (range of possible values: 0.0 to 1.0): School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Round  Table estimates are based on cross-sectional weights within each round (C1CW0, C2CW0, C3CW0, C4CW0, C5CW0, C6CW0). Estimates for kindergarten through fifth grade have been put on a common scale to support comparisons. Subgroup counts do not sum to total sample because demographic variables are missing for some cases. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004 A-28 Table A28. Probability of proficiency, reading level 9: evaluating nonfiction (range of possible values: 0.0 to 1.0): School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Round  Table estimates are based on cross-sectional weights within each round (C1CW0, C2CW0, C3CW0, C4CW0, C5CW0, C6CW0). Estimates for kindergarten through fifth grade have been put on a common scale to support comparisons. Subgroup counts do not sum to total sample because demographic variables are missing for some cases. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004 A-29 Table A29. Probability of proficiency, mathematics level 1: count, number, shape (range of possible values: 0.0 to 1.0): School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Round  Table estimates are based on cross-sectional weights within each round (C1CW0, C2CW0, C3CW0, C4CW0, C5CW0, C6CW0). Estimates for kindergarten through fifth grade have been put on a common scale to support comparisons. Subgroup counts do not sum to total sample because demographic variables are missing for some cases. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004 A-30 Table A30. Probability of proficiency, mathematics level 2: relative size (range of possible values: 0.0 to 1.0): School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Round  Table estimates are based on cross-sectional weights within each round (C1CW0, C2CW0, C3CW0, C4CW0, C5CW0, C6CW0). Estimates for kindergarten through fifth grade have been put on a common scale to support comparisons. Subgroup counts do not sum to total sample because demographic variables are missing for some cases. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004 A-31 Table A31. Probability of proficiency, mathematics level 3: ordinality, sequence (range of possible values: 0.0 to 1.0): School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Round  Table estimates are based on cross-sectional weights within each round (C1CW0, C2CW0, C3CW0, C4CW0, C5CW0, C6CW0). Estimates for kindergarten through fifth grade have been put on a common scale to support comparisons. Subgroup counts do not sum to total sample because demographic variables are missing for some cases. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004 A-32 Table A32. Probability of proficiency, mathematics level 4: add/subtract (range of possible values: 0.0 to 1.0): School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Round  Table estimates are based on cross-sectional weights within each round (C1CW0, C2CW0, C3CW0, C4CW0, C5CW0, C6CW0). Estimates for kindergarten through fifth grade have been put on a common scale to support comparisons. Subgroup counts do not sum to total sample because demographic variables are missing for some cases. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004 A-33 Table A33. Probability of proficiency, mathematics level 5: multiply/divide (range of possible values: 0.0 to 1.0): School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Round  Table estimates are based on cross-sectional weights within each round (C1CW0, C2CW0, C3CW0, C4CW0, C5CW0, C6CW0). Estimates for kindergarten through fifth grade have been put on a common scale to support comparisons. Subgroup counts do not sum to total sample because demographic variables are missing for some cases. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004 A-34 Table A34. Probability of proficiency, mathematics level 6: place value (range of possible values: 0.0 to 1.0): School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Round  Table estimates are based on cross-sectional weights within each round (C1CW0, C2CW0, C3CW0, C4CW0, C5CW0, C6CW0). Estimates for kindergarten through fifth grade have been put on a common scale to support comparisons. Subgroup counts do not sum to total sample because demographic variables are missing for some cases. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004 A-35 Table A35. Probability of proficiency, mathematics level 7: rate and measurement (range of possible values: 0.0 to 1.0): School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Round  Table estimates are based on cross-sectional weights within each round (C1CW0, C2CW0, C3CW0, C4CW0, C5CW0, C6CW0). Estimates for kindergarten through fifth grade have been put on a common scale to support comparisons. Subgroup counts do not sum to total sample because demographic variables are missing for some cases. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004 A-36 Table A36. Probability of proficiency, mathematics level 8: fractions (range of possible values: 0.0 to 1.0): School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Round  Table estimates are based on cross-sectional weights within each round (C1CW0, C2CW0, C3CW0, C4CW0, C5CW0, C6CW0). Estimates for kindergarten through fifth grade have been put on a common scale to support comparisons. Subgroup counts do not sum to total sample because demographic variables are missing for some cases. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004 A-37 Table A37. Probability of proficiency, mathematics level 9: area and volume (range of possible values: 0.0 to 1.0): School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Round  Table estimates are based on cross-sectional weights within each round (C1CW0, C2CW0, C3CW0, C4CW0, C5CW0, C6CW0). Estimates for kindergarten through fifth grade have been put on a common scale to support comparisons. Subgroup counts do not sum to total sample because demographic variables are missing for some cases. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004 A-38 Table A38. Percent of children at or above modal reading proficiency for each grade: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 (C1CW0, C2CW0, C3CW0, C4CW0, C5CW0, C6CW0). Estimates for kindergarten through fifth grade have been put on a common scale to support comparisons. Subgroup counts do not sum to total sample because demographic variables are missing for some cases. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004 A-39 Table A39. Percent of children at or above modal mathematics proficiency for each grade: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Table estimates are based on cross-sectional weights within each round (C1CW0, C2CW0, C3CW0, C4CW0, C5CW0, C6CW0). Estimates for kindergarten through fifth grade have been put on a common scale to support comparisons. Subgroup counts do not sum to total sample because demographic variables are missing for some cases. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004. B-1 Table B1. Reading assessment IRT item parameters: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 IRT  Table B1. Reading assessment IRT item parameters: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 B-3 Table B1. Reading assessment IRT item parameters: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 Table B1. Reading assessment IRT item parameters: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 B-5 Table B1. Reading assessment IRT item parameters: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 3 Parameter for guessing. 4 Mean and standard deviation of theta ability estimate NOTE: Item responses from kindergarten through fifth grade were pooled for IRT calibration to produce parameter estimates on a common scale. Items are sorted in estimated ascending order of overall difficulty (IRT \"b\" parameter). The grades in which items appeared on assessment forms are noted. Mean and standard deviation of theta ability estimates are based on cross-sectional weights within each round (C1CW0, C2CW0, C3CW0, C4CW0, C5CW0, C6CW0). Asterisks mark the range corresponding to 2 standard deviations below and above the mean ability for the round. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004. B-6 Table B2. Mathematics assessment IRT item parameters: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 IRT 2 . 1 4 -0 . 9 8 0 . 0 0 K 1 * * * 0 0 0 X 1 . 1 9 -0 . 9 2 0 . 1 9 K 1 * * * N U M B E R 2 3 2 . 1 4 -0 . 8 3 0 . 0 0 K 1 * * * 3 R D L I N E 2 . 0 6 -0 . 8 0 0 . 0 0 K 1 * * * 3 + 2 C A R S 1 . 4 3 -0 . 7 9 0 . 0 0 K 1 * * * _ 7 8 9 1 0 2 . 0 0 -0 . 7 7 0 . 0 0 K 1 * * * H A L F O V A L 1 . 0 1 -0 . 7 7 0 . 2 2 K 1 * * * 2 + 3 S T I C K 1 . 6 0 -0 . 7 2 0 . 0 0 K 1 * * * * # B U G S 1 . 5 6 -0 . 6 8 0 . B-7 Table B2. Mathematics assessment IRT item parameters: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 B-8 Table B2. Mathematics assessment IRT item parameters: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 B-9 Table B2. Mathematics assessment IRT item parameters: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 4 Mean and standard deviation of theta ability estimate NOTE: Item responses from kindergarten through fifth grade were pooled for IRT calibration to produce parameter estimates on a common scale. Items are sorted in estimated ascending order of overall difficulty (IRT \"b\" parameter). The grades in which items appeared on assessment forms are noted. Mean and standard deviation of theta ability estimates are based on cross-sectional weights within each round (C1CW0, C2CW0, C3CW0, C4CW0, C5CW0, C6CW0). Asterisks mark the range corresponding to 2 standard deviations below and above the mean ability for the round. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004. B-10  4 Mean and standard deviation of theta ability estimate NOTE: Item responses from third and fifth grade were pooled for IRT calibration to produce parameter estimates on a common scale. Items are sorted in estimated ascending order of overall difficulty (IRT \"b\" parameter). Science was not tested in kindergarten/first grade. The grades in which items appeared on assessment forms are noted. Mean and standard deviation of theta ability estimates are based on cross-sectional weights within each round (C5CW0, C6CW0). Asterisks mark the range corresponding to 2 standard deviations below and above the mean ability for the round. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), spring 2002 and spring 2004."}, {"section_title": "APPENDIX A SCORE STATISTICS FOR DIRECT COGNITIVE MEASURES FOR SELECTED SUBGROUPS", "text": ""}, {"section_title": "APPENDIX B ECLS-K ITEM PARAMETERS BY ROUNDS", "text": "C-2 Table C1. Reading assessment estimated proportion correct: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 C-3 Table C1. Reading assessment estimated proportion correct: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 C-4 Table C1. Reading assessment estimated proportion correct: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 C-5 Table C1. Reading assessment estimated proportion correct: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004. C-7 Table C2. Mathematics assessment estimated proportion correct: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 C-8 Table C2. Mathematics assessment estimated proportion correct: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 C-9 Table C2. Mathematics assessment estimated proportion correct: School years 1998School years -99, 1999School years -2000School years , 2001School years -02, and 2003 0.00 0.00 0.00 0.00 0.01 0.10 MEASDIAM 5 0.00 0.00 0.00 0.00 0.02 0.09 CARPET 5 0.00 0.00 0.00 0.00 0.00 0.04 PRISMVOL 5 0.00 0.00 0.00 0.00 0.02 0.08 TILESCOV 3,5 0.00 0.00 0.00 0.00 0.01 0.04 NOTE: IRT-estimated proportion correct for each item in each round. Estimates for kindergarten through fifth grade have been put on a common scale to support comparisons. Items are sorted in estimated ascending order of overall difficulty (IRT \"b\" parameter). Not all items appeared in test forms for all rounds. Table estimates are based on crosssectional-weights within each round (C1CW0, C2CW0, C3CW0, C4CW0, C5CW0, C6CW0). SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), fall 1998, spring 1999, fall 1999, spring 2000, spring 2002, and spring 2004. C-10 Table C3. Science assessment estimated proportion correct: School years 2001-02 and 2003-04"}, {"section_title": "Grades", "text": "Round 5 Round 6 R B U L B 3 0 . 9 0 0 . 9 5 R E N R G Y 3 0 . 9 4 0 . 9 8 R P L A N T 3 0 . 9 4 0 . 9 IRT-estimated proportion correct for each item in each round. Estimates for third and fifth grade have been put on a common scale to support comparisons. Items are sorted in estimated ascending order of overall difficulty (IRT \"b\" parameter). Not all items appeared in test forms for both rounds. Science was not tested in kindergarten/first grade. "}]