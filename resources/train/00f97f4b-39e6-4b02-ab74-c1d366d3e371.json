[{"section_title": "ABSTRACT", "text": "This thesis describes the development of a high-resolution storm-surge hazard database, which can be used for estimating the long-term storm surge hazard at any given site along the eastern coast of the United States (US). A stochastic hurricane track model is used to generate a set of one hundred thousand years of synthetic hurricane tracks. The SLOSH (Sea, Lake and Overland Surges from Hurricanes) model is used to simulate the storm surge in the Atlantic basin caused by selected synthetic hurricane tracks. The study domain covers a region of about 20 miles from the coastline containing more than 220,000 grid points (or observation points) for recording the peak storm surges of each synthetic hurricane track. A uniform grid of resolution of 1.1 km is proposed for this study. Hind cast simulation of a set of 16 hurricanes was performed to quantify the modeling error of SLOSH model in terms of its ability to predict the surge height that occurred along the US coastline. The SLOSH predicted surges for the 16 historical hurricanes were validated against historical storm surge data obtained from various tide stations and post-hurricane high watermarks along the eastern coast of the US. These modeling errors were then quantified for each SLOSH study region (basin). The simulated surge heights for each basin were then adjusted for systematic error (bias) to assist in the development of more robust, reliable and accurate hazard maps. The biased adjusted surge heights were used to generate (1) storm surge hazard curves (surge height versus return period) for the 220,000 grid points, and (2) storm surge hazard maps for different return periods. iii A hazard visualization tool was developed to view the surge hazard footprint. The availability of this information of long-term hazard for more than 220,000 locations along the US coast can be a useful tool for coastal city developers and planners, decision makers, risk analysts and engineering firms responsible for designing structures for hurricane induced storm surge hazards. Also, such a database and the visualization tools (maps, "}, {"section_title": "II.", "text": "HURRICANE which was expected to make landfall as a category 4 storm in North Carolina but instead slowed down and made landfall as a category 2 storm. Hurricane Michael, which was expected to make landfall as a category 2 storm, suddenly strengthened within 24 hours giving little or no time for evacuation planning and made landfall as a category 4 storm with wind speeds of 155 miles/hour. Hurricane Harvey hit Texas as a category 4 storm in the last week of August 2017 causing a damage of more than $125 billion, making it the second costliest storm in the US history. The 2017 Hurricane Harvey was followed by another hurricane Irma that brought waves of about 20 feet higher than normal tides whereas Harvey was a rain-dominated storm with more than 50 inches of rain. Three of the costliest tropical storms, namely Katrina 2005, Harvey 2017, Sandy 2012, resulted in a total of $360 (USD) billion loss (NCEI, 2018). Much of the United States' densely populated Atlantic and Gulf Coast coastlines lie less than 15 feet above the mean sea level (National Hurricane Center). The historical storm surge data alone is not enough to assess the risk from catastrophic events since a weak storm has diverse stochastic aspects as compared to a strong storm, which has a lower frequency and longer return period. As a result of increase in sea surface temperature, an increase in the frequency of such events is expected (Rego and Li, 2009). A storm surge hazard database needs to be developed to capture probable potential losses due to such events along with the randomness associated with such meteorological events and better map out locations of critical infrastructure such as hospitals, airports, etc. This storm surge geo-spatial database will help improve understanding the risk of property damage due to flood at any location in the United States. With more accurate data, stakeholders including developers, city planners, and insurance providers can modify their planning to reflect more accurate property damage risk."}, {"section_title": "Background", "text": "During a hurricane event, when the water is pushed towards the shore by the wind forces it causes storm surge and the water level rises above the normal tide resulting in a storm tide. The total storm tide depends on the interaction of tidal stage, inverted barometer effects, wind stress, coastal topography, bathymetry and geology. The height of storm surge is a function of maximum winds' radius, intensity, translational speed, and direction. Being one of the major forces in a hurricane, storm surge can pose a potential hazard in the form of severe coastal flooding and devastation inland. Coastal geomorphology, vegetation (or lack of it), levees and sea-walls also affect storm surge. Within an embayment, surge may be further concentrated, whereas at a cape, it is spilled away. Steep slope results in breaking waves whereas gentle slopes can result in long wave run-ups, waves can cause significant damage to lower elevation buildings near the coast and in open bays, even without flooding. The first component is the tidal stage; this can profoundly affect the storm tide during the time of hurricane landfall. However, ignoring the tidal stage does not bias storm surge results positively or negatively as tidal variation occurs with the same frequency and magnitude. Instead, they merely present an envelope of best and worst case scenarios (Phan et al., Slinn et al., 2010). In the United States, the Gulf Coast sees mixed or semidiurnal micro tidal (< 2 m magnitude) events, while the Atlantic coast has diurnal mesotidal (2-4 m magnitude) events (Pinet, 1998). The storm tides are highly correlated with the tidal effect during a hurricane's landfall. A hurricane making a landfall at the time of high tides will cause more flooding and damage as compared to a hurricane making a landfall at the time of low tides. The second component is the response of sea surface to the pressure drop near the eye of the storm creating a vacuum, pushing the water into the hurricane's eye, which results in the rise of the sea surface. It is one of the least contributing component to storm surge when compared to the other components. Ambient atmospheric pressure fluctuates around 1012 millibars. Every millibar drop in pressure causes a centimeter rise in the local sea level in deep water (Anthes, 1982). It contributes about 5% of the total storm surge. Another component is the wind stress tide, caused by wind forces acting on the water surface. The high-speed winds from the hurricane push the water to the shore as it makes landfall (Ingargiola et al., 2013), Interaction between the sea and atmosphere boundary causes energy to be transferred from a hurricane's winds to the water column, causing a build-up near the shoreline. SLOSH (Sea Lake and Overland Surges from Hurricanes) uses an empirically derived constant drag coefficient. Approximate wind stress per unit mass is given by equation 1.1 Where: The variation in local bathymetry near the coast is one of the major factors affecting the storm surge. The US coast has a varying bathymetry with as low as 25 meters in Gulf of Mexico making it prone to storm surge. Figure 1 and 2 shows the variation in the bathymetry along the US coastline. The resulting storm surge varies by \u00b110% for amplitude with variations less than \u00b140% of the initial bathymetry. Fluctuations up to \u00b160% in bathymetry would generate a difference at the coast of at most \u00b120% (Weaver and Slinn, 2010). "}, {"section_title": "Objectives", "text": "The present study has three primary objectives: (i) Develop a uniform and fine resolution storm surge hazard database by coupling a synthetic hurricane model with a hydrodynamic model (SLOSH) to capture the randomness of such future events and estimate the long term storm surge hazard for the eastern coast of the US. (ii) Validate the model by simulating surges from historical storms and compare the results of the simulated storm surges with that of the observed water levels at different locations along the coastline to quantify modeling errors. These errors were then used to adjust the Mean Return Interval (MRI) curves for systematic modeling errors and quantify the uncertainty in return periods and respective surge heights. (iii) Create a tool to visualize the surge footprints and prepare risk maps for an example coastal location (Miami, FL). This storm surge geo-spatial database will help improve understanding of the risk of property damage due to flood at any location along the United States coast. The visualization tool can be a valuable asset to improve the risk awareness in the community."}, {"section_title": "Overview of Analysis Approach", "text": "A stochastic hurricane track model (Liu, 2014) was used to generate synthetic storms which varied in their intensity and direction. The simulated hurricane database by Liu (2014) contains 100,000 years of simulated hurricanes with more than 1 million tracks. Since storm surge simulation using a finite element or finite difference model is computationally intensive and not all hurricanes cause significant surges to the coastal regions, a methodology to select candidate hurricanes that may yield appreciable storm surge was utilized. Any simulated hurricane with its closest distance between the hurricane eye and the coastline dj less than 2Rmax (dj < 2Rmax) where Rmax is the radius of the maximum wind of the hurricane, was considered as a candidate hurricane based on a prior sensitivity study by Pei (2015). Figure 6 presents the criteria of selection of candidate hurricane tracks. Numerical model SLOSH was used to calculate the storm surge for the Atlantic and Gulf of Mexico coasts of the United States using selected candidate hurricane tracks. The model divides the US coast into a set of 33 domains called as basins. For each candidate hurricane, the storm surge from SLOSH was interpolated in the study area using the uniform and high-resolution 0.01\u00b0x0.01\u00b0 grid representing the study region. The grid extends up to 20 miles inland to capture surge inundation as well as inland flooding due to storms. A high-resolution storm surge database was developed for the entire eastern coastline of the United State coast."}, {"section_title": "Study Area", "text": "SLOSH has been applied to the entire Atlantic and Gulf of Mexico coasts of the United States. The model subdivides the US coast into different regions referred as basins as shown in Figure 3. This subdivision of the coastal areas into different basins has been performed based on the population density at the coast and topography of these coastal areas. These basins are continuously expanding polar grids in the areas of interest along the coastline and implement simple boundary conditions (Jelesnianski et. al.1992) with coarse mesh in deep waters. These grids were later refined to a uniform size with a high resolution of 0.01\u00b0x0.01. A statistical (stochastic) hurricane model was used in this study to generate synthetic tropical cyclones under given large-scale atmospheric and ocean environments, which may be estimated from observations or climate modelling. This method overcomes the limitation of having limited historical track database by generating synthetic storms using Monte Carlo simulation method such that they are in statistical agreement with observations, and it agrees well with various other methods used to simulate hurricanes (Vickery et al.,2000;Huang et al.,2001;Emanuel et al., 2006;Vickery et al., 2009). In this study, the stochastic simulation program by Liu (2014) was used. An overview of the simulation methodology is provided in Figure 5. The simulation process begins with the genesis model where the number of events per simulation year are determined and hurricane parameters are randomly selected for the first time step using the historical database (HURDAT). After generating the initial location of the hurricane, the tracking model generates the position of the storm based on the heading direction and the forward speed in the next time step. This is followed by calculation of the storm intensity based on the location of the hurricane track and other environmental parameters such as the sea surface temperature. If the hurricane made landfall, a decay model is used to simulate the hurricane intensity for the next time step. However, if the hurricane is still on the ocean, the hurricane intensity is calculated using a relative intensity model. Once the location of the storm and the value of the storm intensity based on the location of the track is determined, the radius to maximum winds (Rmax) and the Holland B parameter are determined at every 6-hour time step as a function of the latitude and central pressure and are then interpolated linearly to 1-hour increment. If the value of the central pressure exceeds the atmospheric pressure the track simulation is terminated. This process is repeated for every event in the total number of simulation years. The Figure 4 below shows a hurricane track with various parameters. First for each simulated hurricane, the Radius of maximum wind (Rmax) was calculated and closest distance (Dj) from the eye of the hurricane to the coastline. For any synthetic hurricane track to be a candidate of a given domain, a threshold was set to be as twice of the radius to maximum winds (2Rmax,j) at every time step j (1-h interval). Any simulated hurricane at time step j with Dj < 2Rmax is considered as a candidate hurricane and is selected for storm surge calculation. This approach was adopted based on a sensitivity study, which verified that 2Rmax threshold is adequate to capture the hurricanes that can produce significant surge (Pei et al., 2014). The 100,000-year hurricane simulations resulted in 1,038,322 simulated hurricanes originated from the North Atlantic Ocean out of which 504,826 events were considered to be as candidate hurricanes. For these selected candidate hurricanes, track parameters like the position of hurricane eye, translational speed, heading angle, central pressure deficit, and radius to maximum wind were calculated with a 1-h time interval. These parameters are used as an input for the SLOSH model to calculate maximum surge levels. For the SLOSH simulations, the wind and pressure are calculated using a semi parametric wind field model within the SLOSH model. The differences between the wind field computed by SLOSH's parametric wind field model and surface wind field observations were within a range of 6% or less (Lin et al., 2010). The Surge heights predicted by the model has a reported accuracy of \u00b120% (Jelesnianski et al., 1992). boundaries and land boundaries. We can also specify the tidal constituents, normal flow, wind and/or wave radiation stresses, atmospheric pressure and outward radiation of waves (meteorological forcing). It is capable of full wetting/drying elements (Luettich, 2004). Nodes of the elements are designated as \"dry nodes\", \"interface nodes\", and \"wet nodes\". A node which is connected to dry elements which have barriers in place making sure that the flow through the element to be zero is called as \"dry node\". A node which is connected to all wet elements is a wet node and is included in the full flow domain. Continuity Equation (GWCE) form (Scheffner et al., 1994).  (Blumberg and Mellor, 1987)). POM is a 3-D and fully nonlinear hydrodynamic model that has been in development and used by a wide range of users over the past three decades. It has been applied for use in tropical storm surge modeling in North Carolina (Xie et al.,2003;Peng et al., 2004;Pietrafesa et al., 2007). In this application storm surge has been coupled various models such as meteorological (wind, pressure, and precipitation), wave, and river discharge models to estimate the impact and damage due to storm surge more accurately and more close to reality."}, {"section_title": "CH3D", "text": "Another well-developed Finite Difference model that has been applied to predict the storm surge is the \"Curvilinear grid Hydrolidynamics model in 3D\". CH3D is capable of handling large-scale simulations as it can run on MPI and high performance computers (Sheng and Alymov, 2002). CH3D uses a structured curvilinear grid and has been applied on domains with the resolution reaching less than 20 meters. It is a fully nonlinear model that has been coupled with wave models and with larger scale circulation models. Various wind models have been used to provide input."}, {"section_title": "FVCOM", "text": "The fully nonlinear 3-D model FVCOM has been applied to storm surge simulations in Florida (Weisberg and Zheng, 2006). This modeling application demonstrates a massconserving 3-D simulation on an unstructured grid extending past the continental shelf (Dube et al., 2010 (Glahn et al., 2009). In the SLOSH model, the governing equations of motion for the Cartesian coordinate were first developed by (Platzman, 1963) and later modified by (Jelesnianski, 1967) with a bottom slip coefficient. SLOSH uses a curvilinear grid system to allow higher resolutions inland and a coarser resolution in deep waters. It computes surges over bays and estuaries and includes sub-grid features such as channels and barriers. For storm surge heights of individual hurricanes, SLOSH has been reported with an accuracy of +-20% (NWS, 2011). Figure 7 shows the SLOSH basins coverage for east and gulf coastlines of the US. "}, {"section_title": "SLOSH Methodology", "text": "The transport equations of motion are given as where, u and v are the components of transport; g is the acceleration due to gravity; D is the depth of still water relative to a common datum; h is the water height above the datum; h0 is the hydrostatic water level; f is the Coriolis parameter; and are the components of surface stresses; and Ar, Ai, Br, Bi, Cr, Ci are the bottom stress terms. The model applies the law of conservation for mass relating the horizontal transports (u, v) to the sea level rise. eq. 1.4 shows the continuity equation with Boussinesq's approximation for incompressible flow. Vertical velocity is assumed to be negligible and SLOSH's form of the continuity eq. 1.5 is obtained, note that x and y in this equation are the real and imaginary components of the complex plane, respectively. For each time step, u and v are solved, leading to a new surge level, h, at every grid location predefined in SLOSH. The model uses a simplified wind model based on pressure and radius of the maximum winds to calculate the storm surge for each grid point. complete derivation of the SLOSH equations of motion can be found in (Jelesnianski, 1967) and Appendix A of the SLOSH Technical Report (Jelesnianski, Chen and Shaffer, 1992)."}, {"section_title": "SLOSH Computational Grid", "text": "SLOSH has been applied to the entire U.S. Atlantic and Gulf of Mexico coasts of the U.S. The model is subdivided into different regions referred as basins. This subdivision of the coastal areas into different basins us done depending upon population density at the coast, topography of the areas. These basins have a finer resolution particularly near susceptible features such as inlets and channels. The polar grid helps in implementation of simple boundary conditions. These basins are a continuously expanding polar grids to allow for both finer resolution in the areas of interest along the coastline, and implementation of simple boundary conditions (Jelesnianski et al.,1992) and stretching to coarse mesh in deep waters or areas of lesser interest. The shape of the grids helps the SLOSH model to be computationally efficient. A typical simulation of a storm using SLOSH takes about 3 minutes which is about 50 times faster when compared with other storm surge models when ran on a typical desktop. This made SLOSH an ideal choice for simulation of a 100000-year event catalog. An example of a SLOSH basin is the New York NY3 It should be noted that all of the inputs and output values of SLOSH are based on real space (longitudinal and latitudinal coordinates for the polar grid points). A grid interpolation was performed for all computations to transform the storm surge data from polar grid to rectangular grid of a uniform and finer resolution about 1 km near the coastline and about 5 km coarser resolution after 20 miles inland, to keep the model computationally efficient. Figure 9 represents the coverage of uniform surge grid for the US. SLOSH implements four boundary conditions for this computational grid (Phan et al; \u2022 Surface gradients are replaced by the nearest interior, contiguous point in shallow water. \u2022 Surface gradients are replaced by the storm's hydrostatic gradient in intermediate depths. \u2022 Hydrostatic height of the storm is set at height points of boundary squares in deep water. \u2022 Zero transport over dry land.  hourly progression of the hurricane. An example of a TRK file used in our study can be found in Appendix A. At the storm's center and far away from the hurricane, wind speed tapers off to zero. A vector correction for the storm's motion is added to the stationary wind speed to arrive at the wind vector used in surface shear stress computations (Jelesnianski, 1967) SLOSH Output SLOSH gives the output in the form of a REX file which is SLOSH specific animation file containing the storm surge results for each time step respectively. These files contains information about the surge elevation, wind speed and wind direction at every 10 minutes of simulation. SLOSH refers the results of storm surge heights relative to the vertical datum, NAVD88. For estimating the losses, one might want to determine the flood depth of surge flooding at a location, this can be done by subtracting the ground elevation (relative to NAVD88) at that location from the potential surge height. Within the SLOSH model, an average elevation is assumed within each grid square. As mentioned before, all of the inputs and output values of SLOSH are based on real space (longitudinal and latitudinal coordinates for the polar grid points). Interpolation was performed for the SLOSH output to transform the Surge depth corresponding to a polar grid into rectangular grid of a uniform and finer resolution about 1 km near the coastline and about 5 km coarser resolution after 20 miles inland from the coastline to keep the model computationally efficient. SLOSH Model was not used to calculate the flood depth as it uses an average grid elevation. Whereas, a true terrain height may vary significantly within a SLOSH grid square. The depth of surge flooding above terrain at a specific site in the grid square can be calculated more precisely by subtracting the DEM data from the model-generated storm surge height at that site. Figure 10 shows the schematic workflow of SLOSH model with required inputs and the resulting outputs. The major advantage of creating the hazard database is that it can help in reducing the uncertainties in the storm surge levels by simulating a large number of synthetic storms. The variations in the storm track parameters can significantly affect the surge levels. For instance, the storm size can have a major contribution in the rise of water levels above predicted levels of normal tide than intensity of the storm (Irish et al., 2008) and storms moving faster will produce high surges as compared with storms moving slowly (Irish et al., 2008;Rego and Li, 2009). These uncertainties are taken into account by the large number of simulations of the synthetic storms and developing a more robust hazard database."}, {"section_title": "Methodology", "text": "For the current study, Figure 11 outlines the framework of the approach applied for the development of Storm Surge Hazard Database. "}, {"section_title": "Synthetic Storm Simulation", "text": "A limitation of using historical data is that the number of events in the historical database is scarce and a very low number of hurricanes make landfall in a selected study region making it unsuitable for long-term probable hazard estimation. In addition, it is difficult to make meaningful extrapolation for such spatially variable and large size of the domain from a small set of historical data. A statistical hurricane model was used in this study to generate synthetic tropical cyclones under given large-scale atmospheric and ocean environments. The stochastic simulation comprised of 4 modules, namely (i) Genesis module (ii) Tracking module (iii) Central pressure module and lastly (iv) decay module. The full database contains a set of 100,000 years of simulated tracks which were created using empirical track method with 7 key variable parameters. The tracks varied in the Location of the eye (latitude and longitude) of hurricane, radius of maximum winds, forward speed of the storm, central pressure, heading angle and Holland B parameter which were calculated on 6-hour basis These parameters were linearly interpolated to an hourly increment."}, {"section_title": "Candidate Hurricanes", "text": "Any simulated hurricane which is physically realistic and that came within a distance Dj less than 2Rmax from the coastline (Dj < 2Rmax) where Rmax is the radius of the maximum wind of the hurricane, was considered as a candidate hurricane. A sensitivity study conducted by Pei(2015) showed that the 2Rmax was an adequate threshold for the purpose of selecting the candidate hurricanes. The 100,000-year hurricane simulations resulted in 1,038,322 simulated hurricanes originated from the North Atlantic Ocean out of which 504,826 events were considered to be as candidate hurricanes. Figure 12 shows a set of 500 such candidate hurricanes in the Atlantic basin. Jelesnianski (Jelesnianski et al., 1992) was used to simulate the surge heights associated with each candidate hurricane event respectively. First, the surface wind stresses are computed. Second, these wind stresses are used as an input to the SLOSH model for computing the time histories of storm surge heights. However, some modeling limitations do exist. Out of three main contributors to overall storm surge namely pressure setup, wind setup and wave setup (Zachry et al., 2015). The storm surge has been modeled considering first two. The wave effect was not included in the study. However, it has been reported that waves can significantly result in a water level rise in the range of 10 -50% (Dean and Walton, 2009). Dynamic variations in tide are ignored in the model. The initial water level was set to zero in this analysis and the surge heights were calculated with respect to NAVD88. With these known limitations, the data can still be used appropriately for various applications. Figure 10 shows the schematic workflow of the SLOSH model with required inputs and the resulting outputs."}, {"section_title": "Fine Resolution Hazard Database Development", "text": "Lin et al. studied the long term storm surge risk in NYC using synthetic hurricanes and compared the surge spatial pattern of ADCIRC and SLOSH. Their study showed that the surge spatial pattern agrees well between the two models in most simulations. However, it was concluded that SLOSH was unable to capture some local features due to its coarser resolution and ADCIRC simulations were inefficient in terms of computation time (Lin et al., 2012). Therefore, surge was calculated using SLOSH to make the simulations efficient and opted for a uniform fine resolution grid to overcome these drawbacks. SLOSH has predefined structured grids (polar, elliptical or hyperbolic) with variable resolution having fine resolution close to shoreline and coarser offshore resolution. There exist a considerable overlap between the SLOSH basins to cover the complete coast for storm surge calculations. For each candidate hurricane, the surge depth was computed in the given study area. If a hurricane was passing through the overlapping region of two basins and produces significant surge then the larger value of the two simulated basins was selected as the water depth resulted from the storm for that region. The results from the SLOSH model were interpolated to obtain a uniform and high-resolution database for a 0.01\u00b0x0.01\u00b0 grid representing the study region, which extends 20 miles inland to capture inland flooding, and surge inundation. The details of the hazard database size for each SLOSH basin are summarized in the table below. Simulation of tens of thousands of storms over the study domain is computationally intensive. A high performance computer was used to simulate the storm surge, which supported in parallel run of multiple storms at the same instance. Making it possible to run 100,000 simulations and handle a big database efficiently.  (Jelesnianski et al.,1992;Jarvinen, 1999;Glahn et al., 2009;Forbes and Rhome, 2012;Kerr et al., 2013;Forbes et al., 2014). For an individual hurricane simulation, SLOSH has been reported to be within +/-20% range (NWS, 2011). 79% of the time, for best track data the storm surge model errors can be less than 2 feet (Jarvinen, 1999). High errors were found for low surge levels and can show significant dispersion (Glahn et al., 2009). Modeled surge tends to scatter significantly at low levels of increase in water heights. Jelesnianski studied a total of 16 historical storms and simulated storm surge using best track data at 9 locations and reported that SLOSH model can have +/-20% error range (Jelesnianski et al., 1992). A validation study was performed using best track data for hurricane Sandy for New York basin and it was reported that root mean squares of errors of storm surge was less than 1.4 feet at 13 water stations and the correlation between modeled and observations was more than 0.8. (Forbes et al., 2014). In this chapter, a brief discussion on the validation and performance of the storm surge modeling system is presented. The main criterion for validation of the storm surge model was to find the error between the measured and modeled data by comparing the simulated surge with measured water levels for different SLOSH basins at NOAA tide gauge stations, USGS storm surge sensors and high water marks. To test the accuracy of the modeled surge heights, a set of 16 historical storms were considered and the track details for each storm were obtained using the HURDAT and IBTrACK database and each historical hurricane track was then converted into a format required by SLOSH. The numerically simulated storm surge using SLOSH basins was obtained. Water levels from these historical hurricane simulations were then compared to a set of NOAA water stations, high water marks, and storm surge sensors. A database with 508 observations was developed. The aim of these simulations was to analyze the performance of the model and quantify the error in the model for each basin. It was observed that simulated surge matches well with the measured data. However, there are some modeling errors in storm surge, which needs to be adjusted. Modeling errors need to be quantified, as it is important for an accurate estimation of long-term storm surge probability using stochastic simulation.   As Hurricane Sandy made a turn towards the mid-Atlantic coast and continued to grow in size, storm affected major parts of the US coast starting from North Carolina to New England, higher storm effects were observed across New Jersey, New York and Connecticut. The maximum observed storm surge was measured at Kings Point, NY 3.856 m above normal predicted tide levels, which was 30 minutes prior to landfall (Blake et al., 2013;Fanelli, Fanelli and Wolcott, 2013). The storm resulted in a surge of 2.87 m at battery, NY and destroyed over 100,000 homes. Figure 14 represents the path of hurricane Sandy. The black line represents the SLOSH NY3 basin boundary.   Table 2 summarizes the model performance in terms of relative error of the NOAA, HWM and SSS vs SLOSH respectively. More than 85% of the observations had errors of less than or equal to 30%. Nearly half of the time, SLOSH was able to capture the surge effectively with an error less than 10%. The mean error in modeled observations as compared to NOAA observation data was 0.1 m.    Bias Correction Approach for SLOSH Basins Figure 18 and Figure 22 represents the tracks of selected candidate historical hurricanes for validation study for the Mobile basin and New York basin respectively. Mapping of NOAA tide station locations has been displayed in Figure 19 and Figure 23. Hind-cast simulations were driven for the candidate hurricanes for analysis and verification. The results of the analysis were then compared to the observed values. Figure   20 and Figure 24 portrays the scatter plots and relationship between the observed and the modeled data in mobile (emo2) and New York (ny3) basin respectively. It can be noted that the simulated surge values fall within a range of 10%-30% error. It is evident that the observed surge and the modeled surge do not have a linear 1:1 relationship indicating existence of modeling errors. Also, It is apparent that SLOSH seems to exhibit under prediction of the surge in New York basin, these aleatory and random uncertainties in the SLOSH model surge depths needs to be quantified and bias corrected. The quantification of errors is addressed by adjusting the model for systematic and random errors. The systematic errors may arise due to limitations in modeling physics using model parameters and inability to replicate the actual meteorological conditions. For instance, bottom friction calculation, Coriolis effect, numerical representation, and wind speed calculations, which may indirectly/directly influence the storm surge calculations. The random errors may occur due to the randomness in the hurricane track in terms of speed, size, heading angle and severity. By accounting these errors, we can make the model more representative of the real world conditions. In order to remedy the systematic bias in the storm surge heights least squares regression can be used (power model) (Pei et al., 2013). After adjusting the model for systematic errors using 0 ( ) 1 , the random errors ( ) can be taken into account by dividing the measured surge height by systematic error adjusted surge height. The random errors ( ) represents the scattermess around the fitted power curve and are independent of . The adjustment of systematic errors using non-linear regression will result in two basin specific constants namely 0 and 1 . The power equation is given by: Taking natural log on both sides of the equation, we have the following equivalent equation. This equation has the form of linear regression model. The random errors are assumed to follow lognormal distribution and to find the parameter of the distribution; the random error is fitted to lognormal distribution. The total number of observation points for each basin, the adjusted power regression constants 0 and 1 along with location and scale parameters of the lognormal distribution of random errors and are listed in table 2. It can be noted from table 2 that the mean of the random error is always equal to zero, supporting the assumption that random errors follow lognormal distribution. That is, the power equation can be used to remove the modeling biases and the resulting systematic-error-adjusted surge simulations only suffer the random errors. The PDF and CDF of lognormal distribution is given in equation 4.3 and 4.4. Table  3 lists details of estimated parameters for both systematic and random errors for each SLOSH basin. Probability density function (PDF): (4.3) Cumulative distribution function (CDF): Where \u03bc is the location parameter, \u03c3 is the scale parameter and \u03bb is the threshold parameter. Figure 21 and Figure 25 shows the lognormal distribution fit of random errors for two different basins.  Several studies have been performed in the last decade to estimate the long-term surge probabilities and return intervals. (Walton, 2000;Keim, Muller and Stone, 2007;Legg et al., 2010;Lin et al., 2010Lin et al., , 2012Lin et al., , 2014Apivatanagul et al., 2011;Needham and Keim, 2012;Shrestha et al., 2014;Sota and Mori, 2014). A comprehensive historical storm surge database was developed consisting of 195 surges along US coast since 1880 (Needham and Keim, 2012). This database was limited as it represented surge return periods based on a particular location rather than the extent of storm surge inundation. This limited observation data can be fitted using extremal fitted probability distributions. However, it may result in under prediction in case of large extremes of the data (Walton, 2000). This makes it difficult to accurately assess the risk from extreme events with long return periods and may lead to incorrect portrayal of the variability in surge hazard for a large spatially variable extent. Also, the historical data is unreliable due to its limited size and can be bias in predicting the risk of extremes for any coastal areas (Lin et al., 2014). Another approach to estimate the long-term surge probability is empirical simulation technique (EST) using a non-parametric analysis of historical data (Borgman, 1992). However, it was reported that EST overestimates the expected values (Agbley and Basco, 2008). In the current study, synthetic hurricanes are generated where the genesis point of the storm is sampled from the known spawn locations of historical hurricanes recorded in HURDAT (Liu, 2014;Pei et al., 2014). The main goal is to derive a hazard curve that may assist in best estimate the frequency and captures uncertainty of certain storm surge. A twostep approach is proposed to better capture the extreme event frequency. Firstly, derive the surge hazard curve by using stochastic hurricane simulation and SLOSH model that represents a good understanding of the risk. Secondly, consider the effect of modeling errors during the storm surge simulation and adjust the hazard curve for these errors. As previous studies suggest that SLOSH might under predict or over predict surge (Lin et al., 2012;Kerr et al., 2013;Forbes et al., 2014). The following steps were used to estimate the long-term probability of surge at a specific site. First, for a given study domain, a set of candidate hurricanes were selected based on the criteria (Dj <2Rmax) from the stochastic hurricane track database containing one hundred thousand year of simulated tracks. Second, the SLOSH model was used to run storm surge simulations for the selected set of selected candidate hurricanes and a hazard database was developed at every 0.01-degree uniform distance in the study domain extending from Texas to Maine. Third, at any given location of interest, the maximum storm surge for each candidate track was recorded. Fourth, the recorded maximum surge from each candidate hurricane was sorted and ranked in the order of lowest to highest. Fifth, a histogram of peak surge at any given site of interest was created. Sixth, the probability of exceedance and the mean recurrence interval were calculated using equations 5.3 and 5.4. Finally, hazard curve was adjusted for systematic modeling errors using the coefficients provided in table 3 and generate the confidence bounds for return periods. The probability of surge height (si) greater than certain surge threshold value(S) can be described as: Where ( \u2264 | ) is the occurrence probability for surge ( \u2264 ) for given number of candidate events(x) and ( ) is the probability of x events that occur over a period of time (t). Assuming ( ) follows a Poisson distribution the can be expressed as: where, Ht is the hazard value from the distribution for t-year probability of exceedance, where n is the total number of hurricanes meeting the condition ( > ) and Y is the total number of years. If is very small then the occurrence probability is given by: The site-specific long-term probabilistic surge heights and respective return periods can be calculated using the following equation where is the mean annual occurrence rate The hazard curve was then adjusted for the systematic modeling errors. The adjusted surge for a given return period can be determined using the following equation: where is the systematic adjusted surge height 0 , 1 are the power regression constants listed in table 3 for each basin. As a case study for non-parametric approach, I considered long-term risk assessment of storm surge at the Battery Park, NY. Hurricane simulation model was made to run and storm surge was simulated for resulted candidate hurricanes. A total of 9880 simulated storms caused a significant amount of surge. Only 99 simulated storms had surge height higher than 3 meter, with a scarce dataset of 19 storms causing an extreme surge of greater than 4 meter. The largest simulated surge caused by a hurricane was observed to be 5.77 meter. Figure 26 (a) shows the distribution of storm surge into different bins. As it can be seen that this site has low number of recurrences of high intensity surge showing that the data is heavily tailed. For the heavy tailed extreme surge heights, probability of exceedance and return intervals were calculated using 5.6, 5.7 and 5.8. The hazard curve can then be adjusted for systematic modeling errors using the coefficients provided in table 3 and generate the confidence bounds for return periods. The quantile-quantile (Q-Q) shows that surge distribution has a heaver tail than exponential as expected in case of extreme surges. However, this tail is important to determine the risk of extreme storm surges in any storm surge prone areas. Figure 26 (b) shows the exponential Q-Q plot for simulated storm surge. This should be a 1:1 line if the data follows an exponential distribution. A best fit was found to estimate this heavy upper tail of the simulated storm surge. Generalized Pareto distribution with peaks over threshold (POT) was used to estimate this upper tail. The Peaks over threshold (POT) approach generates a subset of data points from a simulated surge data by only considering those events above a defined threshold in this case we set 95 th percentile of the surge data as the threshold (1.77 m). Simulated surge is more likely to be from a same distribution when we consider set of the data above a certain threshold. In addition to this, data peaks can also be considered statistically independent, the distribution of the peak events is indeed following a Generalized Pareto distribution.  The probability of storm surge exceeding a threshold is given as: For large u, GPD is given as where is the shape parameter and is the scale parameter. The threshold is selected based on 90 th percentile of the simulated surge dataset. The scale, shape and location parameters were estimated using maximum likelihood estimation (MLE) method and the cumulative probability distribution of surge data is shown in figure 26 (d)."}, {"section_title": "26(d) Cumulative Distribution and estimated parameters of the GPD fit", "text": "The Annual exceedance frequency of a given storm surge is calculated using the method described by Lin (Lin et al., 2010). And is given as the product of the surge height exceedance probability and the annual frequency of the storm and can be written as: where { > } is the chance that the storm surge will exceed the threshold (1.77 m) and is calculated to be 0.0328 for the present scenario. Annual storm frequency is given as ratio of total storms occurring at the site to the total length of simulation years. Annual frequency of the storm is calculated to be 0.9882. The mean return period of the storm surge is estimated using equation (5.4) which is the reciprocal of the annual exceedance frequency. The obtained hazard curve was then adjusted for the systematic modeling errors due in the storm surge using equation (5.5) and a new adjusted hazard curve was estimated. From Figure 27, it can be noted that the adjustment in the storm surge hazard for modeling errors has increased the surge estimates for longer return intervals. It can be interpreted that developing the surge hazard using the raw data from SLOSH can under predict the long-term risk at Battery Park, NY. Also, for a given return period, the storm surge estimates can still be improved and made more accurate by adding wave run up and consider climate change. Figure 27 shows the storm surge return level plot at Battery Park, NY. The dashed black line is the storm surge without adjustments and the red line is the surge after adjustment. This strengthens the hypothesis that the modeling errors does have an impact on long term estimates of storm surge and must be considered in estimation of long term return periods of extreme events. The 90% confidence bounds were estimated by using 5 th and 95 th percentile of the shape and scale parameter estimates of the GPD fit shown in Figure 28."}, {"section_title": "Historical Storm Surge Return Period Estimation", "text": "After generating a hazard database, one of its important application can be prediction of surge return periods for historical storms. We considered Battery Park, NY as our candidate site to estimate the return period of hurricane Sandy. Sandy produced a Surge of 3.5 meters (circled in Figure 28) causing a catastrophic flooding. The total damage from Hurricane Sandy was over $70 billion USD (NCEI, 2018). We tried to estimate the frequency of Hurricane Sandy storm-surge using our hazard curve to assess the risk of such storm hitting the NY coast and the return period of a particular storm surge intensity at Battery Park, NY. A number of studies exist discussing about the assignment of a return period to hurricane Sandy (Lin et al., 2010(Lin et al., , 2012Jay and Talke, 2013;Sweet et al., 2013;Forbes et al., 2014;Shrestha et al., 2014). years (Sweet et al., 2013). A study was performed GPD analysis using bootstrapping technique to infill unknown data between 1821 and 1843 and considering 53 events with storm tide greater than 1.25 m (Jay and Talke, 2013). They reported the Hurricane Sandy return level to be approximately 300 Years (in a range of 200-400 years). We estimated the return period of sandy and found it to be in strong agreement with the study done by Jay and Talke indicating surge return interval of Sandy to be in a range of 240 to 360 years. To assess the risk of long term surge, we estimated a range for a 100 year storm surge and 500 year storm surge. The 100 year storm surge can possible be anywhere between 2.65 m to 2.9 m. A once in 500 year storm surge can be in between 3.74 to 4 m. These estimates appear to closely match the values of long term hazard reported as 2.62 m for 100-year return interval and slightly on a higher side as compared with 3.26 m for 500 year return interval (Rosenzweig and Solecki, 2010). This risk assessment methodology can be adopted for all other historical storms and all other coastal regions as well. It is important to be able to translate the accurate hazard risk information to nonprofessionals and to common people effectively and with least effort. Hazard maps can be a useful tool for interpretation of the hazard information (Kockelman, 1980) and helps understand how vulnerable an area is to a particular hazard. When used correctly, hazard maps can contribute to improving the risk communication in the community and help in mitigating the losses (monetary and life) due to storm surge and also help in focusing on the hazard prone areas during the early planning stages for appropriate mapping locations for critical infrastructures. To develop these maps, we calculate the surge height exceedance at each grid point. At each grid point, we derive an empirical CDF representing the probability of surge exceeding a certain threshold in any given year. The CDF can then be inverted in order to get the 10-year, 100-year, 300-year, 1000-year surge exceedances which are the surge with a probability of occurring once in 10-year, 100-year, 300-year, 1000-year or in other words the surge with probability of occurrence in a given year exceeding 0.1, 0.01, 0.003, 0.001 respectively. Next, we integrate this information to create a seamless raster layer for each return period using ArcGIS. Each of the resulting raster represents the different hazard layer for the United States. The following figure 29(a-d) shows the extent and variation in the surge inundation areas for different return periods. These maps can play an important role in identification of the areas for safe placement of critical infrastructure such as nuclear power plants, oil refineries, and electricity generating facilities. These maps can also help in assessing the exposure of population along the coast to storm surge inundation at different probable surge return intervals and can be a useful tool for decision making and future planning. Another application of the hazard database can be visualizing the risk of certain surge height at a location. Figure 30 shows a map with varying return periods of 5 feet surge for Miami, FL. It can be noticed that as we get away from the coast the return period of 5 feet surge keeps getting lower as the locations away from the coast has a lower risk of storm surge as compared to the locations right on the coast. From the map, it can be read that the Miami beach can see a surge of 5 feet once in every 500 years. Also, it can be noted that Carol City which is 8 miles from the coast has a higher return interval of 2000 years for the surge of 5 feet which means that it has a lower risk of having a high surge. In other words, the probability of having a surge of 5 feet in carol city is as low as 0.0002 whereas the Miami beach has a probability of 0.002 for the same surge height. Miami beach has ten times higher chance of having surge exceeding 5 feet as compared with Carol city. This information is useful for identification of surge hazard vulnerable areas and also guide in selection of appropriate locations for critical infrastructures, future siting, and improve future investment decisions.   Figure 30 shows the variation in surge heights for a return period of 1000 years in Miami, FL. It can be noticed that, after error adjustment the surge height between 10 to 12 feet can increase by more than 500 percent while the areas with lower surge heights seem to be in decreasing order. Again, emphasizing on the importance of including the model errors in long term estimation of the storm surge hazard. The bias in SLOSH model is undermining our ability to capture the long-term variability in the return periods and accurately estimate the storm surge risk. This new approach, based upon quantification of model errors can possibly be a more accurate way to assess the uncertainty in long-term risk and generate more robust risk maps. Assignment of a region based error coefficient makes it possible to correct the bias in the SLOSH model over a spatially variable large extent. This method being flexible can easily be modified and applied to any region given it has enough historical data to quantify the model errors."}, {"section_title": "CHAPTER SEVEN FUTUTRE WORK", "text": "It is to be noted that no dynamic tide signals were used, also wave-induced setup was not included into the present study. Further work can be carried out to account for the climate change and combine the present study with sea level rise, tide and wave effects. SLOSH model can be downscaled to capture the surge variation to a higher accuracy. A high-resolution dynamic storm surge model can be used to simulate the surge. Currently used hurricane simulation model generates storms randomly but spawns from known historical locations, which limits its robustness when the historical data is insufficient. New methods for storm population generation such as Joint probability method (optimal sampling) can be investigated. A detailed data analysis on category wise inundation and find the maximum envelopes of water for each category can be performed. This work can be extended to calculate the losses due to hazard consistent hurricanes using the surge hazard for a specific return period and calculate the losses using HAZUS. Effect of the hurricane parameters on losses for a set of hazard consistent hurricanes can be studied and a sensitivity analysis of losses to these parameters can be performed. while broadening the range of category IV wind speeds by 2 mph. The main aim of this modification was to solve the rounding issues of the wind speed so that the wind intensity can be converted correctly from one unit to another and keep storms in the correct category regardless of the units used.  ; end % find steps within slosh domain boundary idxin=find(inpolygon(hur(:,1),hur(:,2),boundary(:,1),boundary(:,2))); % calculate Longitude(+), Vt, Delta_P and Rmax hur(:,2)=-hur(:,2); hur(:,3)=hur(:,3)/0.44704; % Vt_mps -> mph hur(:,5)=1013-hur(:,5); % dp hur(:,6)=hur(:,6); % mi % create track file mkdir ([num2str(Hur.Year),'_',num2str(Hur.StormNoOfYear)]); cd([num2str(Hur.Year),'_',num2str(Hur.StormNoOfYear)]); fid=fopen(string(basin)+'_'+string(storm_data(1:end-7))+'.trk','w'); fprintf(fid,'\\n'); fprintf(fid,'\\n'); for j=1:100 if j==lf_step fprintf(fid,'%17s%3d%8.4f%8.3f%8.2f%8.2f%8.2f%8.2f%5d %s\\n',... 'NAP-----',j,hur(j,:),j,'---NAP'); else fprintf (fid,'%17s%3d%8.4f%8.3f%8.2f%8.2f%8.2f%8.2f%5d\\n',... ' ',j,hur(j,:),j); end end % determine start and end step if ~isempty(idxin) if nargin < 3 || isempty(basin_name) basin_name ='acy'; if nargin < 2 || isempty(storm_year) storm_year ='2004_3'; if nargin < 1 || isempty(text_file) text_file = 'acy_Charley.txt'; end end end end tic cd(string(storm_year)+'/'+string(basin_name)+'/'+string(basin_name)) load('latlondpth.mat') Y=max(latlondpth(:,2)); X=length(latlondpth)/Y; vcells = zeros(Y,1); wind = zeros(Y,1); % for I = 0 : X-1 % for J = 1 : Y % % vcells(J+(I*313),1)=I+1; % vcells(J+(I*313),2)=J; % % end % end %% % calls the function that reads the data data lines cd .. cd .. cd .. fid1 = fopen(string(pwd)+'\\'+string(storm_year)+'\\'+string(text_file), 'r'); % Opens file and defines as fid fopen(fid1); % Opens file fid z= fgetl(fid1); % skips the first 4 values because they will all be 0 for the null set fgetl(fid1); fgetl(fid1); fgetl(fid1); it=0; L=1; % L is preallocated while L ~= 0 a = fscanf( fid1, '%f'); fgetl(fid1); L = logical(a); it = it + 1; end dataPts = it; fclose(fid1); %closes the file % latd=zeros(dataPts); % lond=zeros(dataPts); if exist('vcells.mat', 'file') load('vcells.mat') load ('stormnum.mat') [size1,size2]=size(vcells); else size2 = 0; end %% reading the file ------------------------------------------------------fid=fopen(string(pwd)+'\\'+string(storm_year)+'\\'+string(text_file), 'r'); % open the file fopen(fid); a = zeros(70000,1); b = zeros(70000,1); it = 1; %make sure the file is not empty finfo = dir(string(storm_year)+'/'+string(text_file)); fsize = finfo.bytes; if fsize < 20 Max(1) = (-55); else tic z=fgetl(fid); % opens and skips first line while ~feof(fid) % runs while the file is open fseek(fid, 1, 'cof'); grid = fscanf( fid, '%d'); a(it) = grid(1,1); %I b(it) = grid (2,1); %J fseek(fid, 7, 'cof'); latd(it)=fscanf( fid, '%f'); fseek(fid, 6, 'cof'); lond(it)=fscanf( fid, '%f'); fgetl(fid); fgetl(fid); FormatString=repmat('%f',1,5); Data = textscan(fid,FormatString,'delimiter',','); % Read data block Data=cell2mat(Data); for remove = 1:length (Data(:,4)) if (Data(remove,4)) > 50 Data(remove,4)=0; else ; end end vcells(b(it)+(a(it)*Y) ,1+size2 ) = max(Data(:,4)); it = it + 1; end %% select only the cells we want to validate for our experiment stormnum{size2+1}=z; latlon(:,1)=latd; latlon(:,2)=lond; save(string(storm_year)+'/'+string(basin_name)+'/'+string(basin_name)+'/'+string(opfolde r)+'/'+string(text_file(5:end-4))+'vcells.mat','vcells') save(string(storm_year)+'/'+string(basin_name)+'/'+string(basin_name)+'/'+string(opfolde r)+'/'+string(text_file(5:end-4))+'stormnum.mat','stormnum') % save('latlon.mat','latlon') end toc fclose('all'); %closes the file                                  arcpy.CheckOutExtension(\"spatial\") Layer_1 = \"Layer_1\" # Making XY Event Layer arcpy.MakeXYEventLayer_management(\"Event_\"+event_id + \".csv\", \"lon\", \"lat\", Layer_1,GEOGCS ['GCS_WGS_1984',DATUM['D_WGS_1984',SPHEROID['WGS_198 4',6378137.0,298.257223563]],PRIMEM['Greenwich',0.0],UNIT ['Degree',0.0174532925 199433]];-400 -400 1000000000;-100000 10000;-100000 10000;8.98315284119522E-09;0.001;0.001;IsHighPrecision\", \"\") # Converting Feature Class to Feature Class arcpy.FeatureClassToFeatureClass_conversion(Layer_1, work_dir, \"eveid\"+event_id, \"\", \"event_id \\\"event_id\\\" true true false 4 Long 0 0 ,First,#,Layer_13,event_id,-1,-1;lon \\\"lon\\\" true true false 8 Double 0 0 ,First,#,Layer_13,lon,-1,-1;lat \\\"lat\\\" true true false 8 Double 0 0 ,First,#,Layer_13,lat,-1,-1;wind_speed \\\"wind_speed\\\" true true false 4 Long 0 0 ,First,#,Layer_13,wind_speed,-1,-1;storm_surge \\\"storm_surge\\\" true true false 8 Double 0 0 ,First,#,Layer_13,storm_surge,-1,-1\", \"\") # Interpolation of surge data to a surface using IDW -----arcpy.gp.Idw_sa( \"eveid\"+event_id+\".shp\", \"storm_surg\", \"s_int\"+event_id, \"0.001548\", \"2\", \"VARIABLE 12\", \"\") idw_time = time.time() print(\"-----------Surge Interpolation time --------------\") print(str(idw_time -start)+\" \"+\"seconds\") elevRaster_s = arcpy.sa.Raster('s_int'+event_id) extent_s = elevRaster_s.extent Envelope_s = \"%f %f %f %f\" %(extent_s.XMin, extent_s.YMin, extent_s.XMax, extent_s.YMax) # Clip Surge to exact extent of the footprint arcpy.Clip_management(\"s_int\"+event_id, \"%f %f %f %f\" %(extent_s.XMin, extent_s.YMin, extent_s.XMax, extent_s.YMax), \"sfinal\"+event_id, \"Event_\"+event_id+\".shp\", \"0\", \"ClippingGeometry\", \"NO_MAINTAIN_EXTENT\") # Returning the License back to the ArcGIS to be used for next time arcpy.CheckInExtension(\"spatial\") # Creating a layer with the same color classifcation and scale arcpy.MakeRasterLayer_management(in_raster=\"sfinal\"+event_id, out_rasterlayer=\"MakeRas_sfinal\"+event_id, where_clause=\"\", envelope=Envelope_s, band_index=\"\") arcpy.SaveToLayerFile_management(in_layer=\"MakeRas_sfinal\"+event_id, out_layer=\"slayer\"+event_id+\".lyr\", is_relative_path=\"RELATIVE\", version=\"CURRENT\") arcpy.ApplySymbologyFromLayer_management(in_layer=\"slayer\"+event_id+\".lyr\", in_symbology_layer=\"color_s.lyr\") layer_time = time.time() print(\"-----------Layer creation time --------------\") print(str(layer_time -idw_time)+\" \"+\"seconds\") ## Creating a KML file to view it in google earth arcpy.LayerToKML_conversion(layer=\"slayer\"+event_id+\".lyr\", out_kmz_file=\"s\"+ event_id+\"GE.kmz\", layer_output_scale=\"0\", is_composite=\"NO_COMPOSITE\", boundary_box_extent=\"DEFAULT\", \\ image_size=\"1024\", dpi_of_client=\"96\", ignore_zvalue=\"ABSOLUTE\") googleearthfile_time = time.time() print(\"-----------Google earth File creation time --------------\") print(str(googleearthfile_time -layer_time)+\" \"+\"seconds\") for f in glob(\"eveid\"+event_id+\".*\"): os arcpy.CheckOutExtension(\"spatial\") Layer_1 = \"Layer_1\" # Making XY Event Layer arcpy.MakeXYEventLayer_management(\"Event_\"+event_id + \".csv\", \"lon\", \"lat\", Layer_1,\"GEOGCS ['GCS_WGS_1984',DATUM['D_WGS_1984',SPHEROID['WGS_19 84',6378137.0,298.257223563]],PRIMEM['Greenwich',0.0],UNIT['Degree',0.017453292 5199433]];-400 -400 1000000000;-100000 10000;-100000 10000;8.98315284119522E-09;0.001;0.001;IsHighPrecision\", \"\") # Converting Feature Class to Feature Class arcpy.FeatureClassToFeatureClass_conversion(Layer_1, work_dir, \"eveid\"+event_id, \"\", \"event_id \\\"event_id\\\" true true false 4 Long 0 0 ,First,#,Layer_13,event_id,-1,-1;lon \\\"lon\\\" true true false 8 Double 0 0 ,First,#,Layer_13,lon,-1,-1;lat \\\"lat\\\" true true false 8 Double 0 0 ,First,#,Layer_13,lat,-1,-1;wind_speed \\\"wind_speed\\\" true true false 4 Long 0 0 ,First,#,Layer_13,wind_speed,-1,-1;storm_surge \\\"storm_surge\\\" true true false 8 Double 0 0 ,First,#,Layer_13,storm_surge,-1,-1\", \"\") # Interpolation of surge data to a surface using IDW ----arcpy.gp.Idw_sa( \"eveid\"+event_id+\".shp\", \"wind_speed\", \"w_int\"+event_id,\"0.001548\", \"2\", \"VARIABLE 12\", \"\") idw_time = time.time() print(\"-----------Wind Interpolation time --------------\") print(str(idw_time -start)+\" \"+\"seconds\") elevRaster_w = arcpy.sa.Raster('w_int'+event_id) extent_w = elevRaster_w.extent Envelope_w = \"%f %f %f %f\" %(extent_w.XMin, extent_w.YMin, extent_w.XMax, extent_w.YMax) # Clip Surge to exact extent of the footprint arcpy.Clip_management(\"w_int\"+event_id, \"%f %f %f %f\" %(extent_w.XMin, extent_w.YMin, extent_w.XMax, extent_w.YMax), \"wfinal\"+event_id, \"Event_\"+event_id+\".shp\", \"0\", \"ClippingGeometry\", \"NO_MAINTAIN_EXTENT\") # Returning the License back to the ArcGIS to be used for next time arcpy.CheckInExtension(\"spatial\") # Creating a layer with the same color classifcation and scale arcpy.MakeRasterLayer_management(in_raster=\"wfinal\"+event_id, out_rasterlayer=\"MakeRas_wfinal\"+event_id, where_clause=\"\", envelope=Envelope_w, band_index=\"\") arcpy.SaveToLayerFile_management(in_layer=\"MakeRas_wfinal\"+event_id, out_layer=\"wlayer\"+event_id+\".lyr\", is_relative_path=\"RELATIVE\", version=\"CURRENT\") arcpy.ApplySymbologyFromLayer_management(in_layer=\"wlayer\"+event_id+\".lyr\", in_symbology_layer=\"color_w.lyr\") layer_time = time.time() print(\"-----------Layer creation time --------------\") print(str(layer_time -idw_time)+\" \"+\"seconds\") ## Creating a KML file to view it in google earth arcpy.LayerToKML_conversion(layer=\"wlayer\"+event_id+\".lyr\", out_kmz_file=\"w\"+ event_id+\"GE.kmz\", layer_output_scale=\"0\", is_composite=\"NO_COMPOSITE\", boundary_box_extent=\"DEFAULT\", image_size=\"1024\", dpi_of_client=\"96\", ignore_zvalue=\"ABSOLUTE\") googleearthfile_time = time.time() print(\"-----------Google earth File creation time --------------\") print(str(googleearthfile_time -layer_time)+\" \"+\"seconds\") for f in glob(\"eveid\"+event_id+\".*\"): os.remove(f) for j in glob(\"w_int\"+event_id+\".*\"): os.remove(j) process_time = time.time() print(\"------------------Processing time -------------------\") print(str(process_time -start)+\"seconds\") # Creating a Geodatabase to store the surge footprints OutGDB = arcpy.CreateFileGDB_management(work_dir, \"wind_footprints.gdb\") myWindList = arcpy.ListRasters(\"wfinal*\", \"\") newList = [] for x in myWindList: z1 = os.path.join(work_dir, x) newList.append(z1) inList = (\";\".join([i for i in newList])) arcpy.RasterToGeodatabase_conversion(inList, OutGDB) try: os.mkdir(\"Google_Earth_files_wind\") os.mkdir(\"ArcGIS_Layer_files_wind\") except OSError: # The directory already existed, nothing to do pass src_fldr = work_dir kmz_dst_fldr = work_dir+\"\\\\Google_Earth_files_wind\" lyr_dst_fldr = work_dir+\"\\\\ArcGIS_Layer_files_wind\" for kmz_file in glob(src_fldr+\"\\\\*.kmz\"): shutil.copy2(kmz_file, kmz_dst_fldr); for lyr_file in glob(src_fldr+\"\\\\w*. arcpy.CheckOutExtension(\"spatial\") Layer_1 = \"Layer_1\" # Making XY Event Layer arcpy.MakeXYEventLayer_management(\"Event_\"+event_id + \".csv\", \"lon\", \"lat\", Layer_1, # Clip Surge to exact extent of the footprint arcpy.Clip_management(\"s_int\"+event_id, Envelope_s, \"sfinal\"+event_id, \"Event_\"+event_id+\".shp\", \"0\", \"ClippingGeometry\", \"NO_MAINTAIN_EXTENT\") # Creating a layer with the same color classifcation and scale arcpy.MakeRasterLayer_management(in_raster=\"wfinal\"+event_id, out_rasterlayer=\"MakeRas_wfinal\"+event_id, where_clause=\"\", envelope=Envelope_w, band_index=\"\") arcpy.SaveToLayerFile_management(in_layer=\"MakeRas_wfinal\"+event_id, out_layer=\"wlayer\"+event_id+\".lyr\", is_relative_path=\"RELATIVE\", version=\"CURRENT\") arcpy.ApplySymbologyFromLayer_management(in_layer=\"wlayer\"+event_id+\".lyr\", in_symbology_layer=\"color_w.lyr\") wind_layer_time = time.time() print(\"-----------Wind Layer creation time --------------\") print(str(wind_layer_time -idw_time_surge)+\" \"+\"seconds\") # Creating a layer with the same color classifcation and scale arcpy.MakeRasterLayer_management(in_raster=\"sfinal\"+event_id, out_rasterlayer=\"MakeRas_sfinal\"+event_id, where_clause=\"\", envelope=Envelope_s, band_index=\"\") arcpy.SaveToLayerFile_management(in_layer=\"MakeRas_sfinal\"+event_id, out_layer=\"slayer\"+event_id+\".lyr\", is_relative_path=\"RELATIVE\", version=\"CURRENT\") arcpy.ApplySymbologyFromLayer_management(in_layer=\"slayer\"+event_id+\".lyr\", in_symbology_layer=\"color_s.lyr\") surge_layer_time = time.time() print(\"-----------Surge Layer creation time --------------\") print(str(surge_layer_time -wind_layer_time)+\" \"+\"seconds\") ## Creating a KML file to view it in google earth arcpy.LayerToKML_conversion(layer=\"wlayer\"+event_id+\".lyr\", out_kmz_file=\"w\"+ event_id+\"GE.kmz\", layer_output_scale=\"0\", is_composite=\"NO_COMPOSITE\", boundary_box_extent=\"DEFAULT\",image_size=\"1024\", dpi_of_client=\"96\", ignore_zvalue=\"ABSOLUTE\") googleearthfile_w_time = time.time() print(\"-----------Google earth files for wind --------------\") print(str(googleearthfile_w_time -surge_layer_time)+\" \"+\"seconds\") ## Creating a KML file to view it in google earth arcpy.LayerToKML_conversion(layer=\"slayer\"+event_id+\".lyr\", out_kmz_file=\"s\"+ event_id+\"GE.kmz\", layer_output_scale=\"0\", is_composite=\"NO_COMPOSITE\", boundary_box_extent=\"DEFAULT\", image_size=\"1024\", dpi_of_client=\"96\", ignore_zvalue=\"ABSOLUTE\") googleearthfile_s_time = time.time() print(\"-----------Google earth files for surge --------------\") print(str(googleearthfile_s_time -googleearthfile_w_time)+\" \"+\"seconds\") # Removing the intermediate files for f in glob(\"eveid\"+event_id+\".*\"): os.remove(f) for g in glob(\"w_int\"+event_id+\".*\"): os.remove(g) for j in glob(\"s_int\"+event_id+\".*\"): os.remove(j) # Returning the License back to the ArcGIS to be used for next time arcpy.CheckInExtension(\"spatial\") process_time = time.time() print(\"------------------Processing time -------------------\") print(str(process_time -start)+\" \"+\"seconds\") OutGDB = arcpy.CreateFileGDB_management(work_dir, \"footprints.gdb\") mySurgeList = arcpy.ListRasters(\"sfinal*\", \"\") myWindList = arcpy.ListRasters(\"wfinal*\", \"\") newList = [] for x in mySurgeList: z1 = os.path.join(work_dir, x) newList.append(z1) for y in myWindList: z2 = os.path.join(work_dir, y)"}]