[{"section_title": "Abstract", "text": "In this paper, we focus on joint regression and classification for Alzheimer's disease diagnosis and propose a new feature selection method by embedding the relational information inherent in the observations into a sparse multi-task learning framework. Specifically, the relational information includes three kinds of relationships (such as feature-feature relation, response-response relation, and sample-sample relation), for preserving three kinds of the similarity, such as for the features, the response variables, and the samples, respectively. To conduct feature selection, we first formulate the objective function by imposing these three relational characteristics along with an 2, 1 -norm regularization term, and further propose a computationally efficient algorithm to optimize the proposed objective function. With the dimension-reduced data, we train two support vector regression models to predict the clinical scores of ADAS-Cog and MMSE, respectively, and also a support vector classification model to determine the clinical label. We conducted extensive experiments on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset to validate the effectiveness of the proposed method. Our experimental results showed the efficacy of the proposed method in enhancing the performances of both clinical scores prediction and disease status identification, compared to the state-ofthe-art methods."}, {"section_title": "Introduction", "text": "Alzheimer's Disease (AD) is characterized as a genetically complex and irreversible neurodegenerative disorder and often found in persons aged over 65. Recent studies have shown that there are about 26.6 million AD patients worldwide, and 1 out of 85 people will be affected by AD by 2050 ( Brookmeyer et al., 2007; Zhou et al., 2011; Zhu et al., 2014a; 2014b ) . Thus, there have been great interests for early diagnosis of AD and its prodromal stage, Mild Cognitive Impairment (MCI) .\nIt has been shown that the neuroimaging tools, including Magnetic Resonance Imaging (MRI) ( Fjell et al., 2010 ) , Positron Emission Tomography (PET) Morris et al., 2001 ) , and functional MRI , help understand the neurodegenerative process in the progression of AD. Furthermore, machine learning methods can effectively handle complex patterns in the observed subjects for either identifying clinical labels, such as AD, MCI, features in a data-driven manner. It has been shown that the feature selection helps overcome both problems of high dimensionality and small sample size by removing uninformative features. Among various feature selection techniques, manifold learning methods has been successfully used in either regression or classification ( Cho et al., 2012; Cuingnet et al., 2011; Liu et al., 2014; Zhang et al., 2011; Suk et al., 2015 ) . For example, Cho et al. (2012) adopted a manifold harmonic transformation method on the cortical thickness data. Meanwhile, while most of the previous studies focused on separately identifying brain disease and estimating clinical scores ( Jie et al., 2013; Liu et al., 2014; , there also have been some effort s to t ackle both tasks simultaneously in a unified framework. For example, proposed a feature selection method for simultaneous disease diagnosis and clinical scores prediction, and achieved promising results. However, to our best knowledge, the previous manifold-based feature selection methods considered only the manifold of the samples, not manifold of either the features or the response variables.\nFor better understanding of the underlying mechanism of AD, our interest in this paper is to predict both clinical scores and disease status jointly, which we call as Joint Regression and Classification (JRC) problem. In particular, we devise new regularization terms to reflect the relational information inherent in the observations and then combine them with an 2, 1 -norm regularization term within a multitask learning framework for joint sparse feature selection in the JRC problem. The rationale for the proposed regularization method is as follows: (1) If some features are related to each other, then the same or similar relation is expected to be preserved between the respective weight coefficients. (2) Due to the algebraic operation in the least square regression, i.e., matrix multiplication, the weight coefficients are linked to the response variables via regressors, i.e., feature vectors in our work. Therefore, it is meaningful to impose the relation between a pair of weight coefficients to be similar to the relation between the respective pair of target response variables. (3) As considered in many manifold learning methods ( Belkin et al., 2006; Fan et al., 2008; Zhu et al., 2011; 2013b; 2013c ) , if a pair of samples are similar to each other, then their respective response values should be also similar to each other. By imposing these three relational characteristics along with the 2, 1 -norm regularization term on the weight coefficients, we formulate a new objective function to conduct feature selection and further solve it with a new computationally efficient optimization algorithm. Then, we can select effective f eatures to build a classifier for clinical label identification and two regression models for ADAS-Cog and MMSE scores prediction, respectively."}, {"section_title": "Image preprocessing", "text": "In this work, we used the publicly available ADNI dataset for performance evaluation."}, {"section_title": "Subjects", "text": "We selected the subjects satisfying the following general inclusion/exclusion criteria 1 : (1) The MMSE score of each NC is between 24 and 30. Their Clinical Dementia Rating (CDR) is of 0. Moreover, the NC is non-depressed, non MCI, and non-demented. (2) The MMSE score of each MCI subject is between 24 and 30. Their CDR is of 0.5. Moreover, each MCI subject is an absence of significant level of impairment in other cognitive domains, essentially preserved activities of daily living, and an absence of dementia. (3) The MMSE score of each Mild AD subject is between 20 and 26, with the CDR of 0.5 or 1.0.\nIn this paper, we use baseline MRI and PET obtained from 202 subjects including 51 AD subjects, 52 NC subjects, and 99 MCI subjects.\n1 Please refer to ' http://adni.loni.usc.edu/ ' for up-to-date information. "}, {"section_title": "Image processing", "text": "We downloaded raw Digital Imaging and COmmunications in Medicine (DICOM) MRI scans from the ADNI website 3 . All structural MR images used in this work were acquired from 1.5T scanners. Data were collected across a variety of scanners with protocols individualized for each scanner. Moreover, these MR images were already reviewed for quality, and automatically corrected for spatial distortion caused by gradient nonlinearity and B1 field inhomogeneity. Moreover, PET images were acquired 30-60 min post Fluoro-DeoxyGlucose (FDG) injection. They were then averaged, spatially aligned, interpolated to a standard voxel size, intensity normalized, and smoothed to a common resolution of 8 mm full width at half maximum.\nThe image processing for all MR and PET images was conducted by following the same procedures in . Specifically, we first performed anterior commissure-posterior commissure correction using MIPAV software 4 for all images, and used the N3 algorithm ( Sled et al., 1998 ) to correct the intensity inhomogeneity. Second, we extracted a brain on all structural MR images using a robust skull-stripping method ( Wang et al., 2013 ) , followed by manual edition and intensity inhomogeneity correction. After removal of cerebellum based on registration ( Tang et al., 2009; Wu et al., 2011; Xue et al., 2006 ) and also intensity inhomogeneity correction by repeating N3 for three times, we used FAST algorithm in the FSL package ( Zhang et al., 2001 ) to segment the structural MR images into three different tissues: Gray Matter (GM), White Matter (WM), and CSF. Next, we used HAMMER ( Shen and Davatzikos, 2002 ) to register the template into subject specific space for preserving local image volume of each subjects. We then obtained the Region-Of-Interest (ROI) labeled images using the Jacob template, which dissects a brain into 93 ROIs ( Kabani, 1998 ) . For each of all 93 ROIs in the labeled image of a subject, we computed the GM tissue volumes in ROIs by integrating the GM segmentation result of the subject. For each subject, we aligned the PET images to their respective MR T1 images using affine registration and then computed the average intensity of each ROI. Therefore, for each subject, we obtained 93 features for MRI and 93 features for PET."}, {"section_title": "Method", "text": ""}, {"section_title": "Notations", "text": "In this paper, we denote matrices as boldface uppercase letters, vectors as boldface lowercase letters, and scalars as normal italic letters, respectively. For a matrix X = [ x i j ] , its i -th row and j -th , and\n, respectively. We further denote the transpose operator, the trace operator, and the inverse of a matrix X as X T , tr ( X ), and X \u22121 , respectively."}, {"section_title": "Relational regularization", "text": "Let X \u2208 R n \u00d7d and Y \u2208 R n \u00d7c denote, respectively, the d neuroimaging features and c clinical response values of n subjects or samples 5 . In this work, we assume that the response values of clinical scores and clinical label 6 can be represented by a linear combination of the features. Then, the problems of regressing clinical scores and determining class label can be formulated by a least square regression model as follows:\n(1) 5 In this work, we have one sample per subject. 6 In this paper, we represented the class label with 0-1 encoding.\nwhere W \u2208 R d\u00d7c is a weight coefficient matrix and \u02c6 Y = XW . While the least square regression model has been successfully used in many applications, it is shown that the solution is often overfitted to the dataset with small samples and high-dimensional features in its original form, especially, in the field of neuroimaging analysis. To this end, a variety of its variants using different types of regularization terms have been suggested to circumvent the overfitting problem and find a more generalized solution Yuan and Lin, 2006; , which can be mathematically simplified as follows:\nwhere R (W ) denotes a set of regularization terms.\nFrom a machine learning point of view, a well-defined regularization term can produce a generalized solution to the objective function, and thus results in a better performance for the final goal. In this paper, we devise novel regularization terms that effectively utilize various pieces of information inherent in the observations. Note that since, in this work, we extract features from ROIs, which are structurally or functionally related to each other, it is natural to expect that there exist relations among features. Meanwhile, if two features are highly related to each other, then it is reasonable to have the respective weight coefficients also related. However, to the best of our knowledge, none of the previous representation (or regression) methods in the literature considered and guaranteed this Fig. 2 . An illustration of the relational information that can be obtained from the observations. The red solid rectangles, the blue dash rectangles, and the green dotted rectangles denote, respectively, the 'sample-sample' relation, 'feature-feature' relation and 'response-response' relation. characteristic in their solutions. To this end, we devise a regularization term with the assumption that, if some features, e.g., x i and x j in the blue dash rectangles of Fig. 2 , are involved in regressing the response variables and are also related to each other, their corresponding weight coefficients ( i.e., w i and w j ) should have the same or similar relation since the i -feature x i in X corresponds to the i th row w i in W in our regression framework. We call this relation as the ' featurefeature ' relation in this work. To utilize the ' feature-feature ' relation, we penalize the loss function with the similarity between x i and x j ( i.e., m ij ) on w i \u2212 w i 2 2\n. Specifically, we impose the relation between columns in X to be reflected in the relation between the respective rows in W by defining the following embedding function:\nwhere m ij denotes an element in the feature similarity matrix M = [ m i j ] \u2208 R d\u00d7d which encodes the relation between features in the samples. With respect to the similarity measure between vectors of a and b , throughout this paper, we first use a radial basis function kernel as defined as follows:\nwhere \u03c3 denotes a kernel width. As for the similarity matrix M , we first construct a data adjacency graph by regarding each sample as a node and using k nearest neighbors along with a heat kernel function defined in Eq. (4) to compute the edge weights, i.e., similarities. For example, if a sample x j is selected as one of the k nearest neighbors of a sample x i , then the similarity m ij between these two samples or nodes is set to the value of f ( x i , x j ); otherwise, their similarity is set to zero, i.e., m i j = 0 . In the meantime, given a feature vector x i , in our joint regression and classification framework, we use a different set of weight coefficients to regress the elements in the response vector y i . In other words, the elements of each column in W are linked to the elements of each column in Y via feature vectors. By taking this mathematical property into account, we further impose the relation between column vectors in W to be similar to the relation between the respective target response variables ( i.e., respective column vectors) in Y , which is called as ' response-response ' relation as defined below:\nwhere g ij denotes an element in the matrix G = [ g i j ] \u2208 R c\u00d7c which represents the similarity between every pair of target response variables ( i.e., every pair of column vectors).\nWe also utilize the relational information between samples, called as ' sample-sample ' relation. That is, if samples are similar to each other, then their respective response values should be also similar to each other. To this end, we define a regularization term as follows:\nwhere s ij is an element in the matrix S = [ s i j ] \u2208 R n \u00d7n which measures the similarity between every pair of samples. We should note that this kind of sample-sample relation has been successfully used in many manifold learning methods ( Belkin et al., 2006; Zhu et al., 2013b; 2013c ) . The elements of the matrices G and S can be computed similarly as in the computation of M as described above. We argue that the simultaneous consideration of these newly devised regularization terms, i.e., feature-feature relation, samplesample relation, and response-response relation, can effectively reflect the relational information inherent in observations in finding an optimal solution. Fig. 2 illustrates these relational regularizations in a matrix form. Regarding feature selection, we believe that due to the underlying brain mechanisms that influence both the clinical scores and a clinical label, i.e., response variables, if one feature plays a role in predicting one response variable, then it also devotes to the prediction of the other response variables. To this end, we further impose to use the same features across the tasks of clinical scores and clinical label prediction. Mathematically, this can be implemented by an 2, 1 -norm regularization term on W , i.e., W 2 , 1 = i w i 2 . Concretely, w i 2 , the 2 -norm of the i th row vector in W , is equally imposed on the i th feature across different tasks, which thus forces the coefficients that weight the i -th feature for different tasks to be grouped together. Earlier, considered the same regularization term in their multi-task learning and validated its efficacy in AD/MCI diagnosis.\nFinally, our objective function is formulated as follows:\nwhere \u03b1 1 , \u03b1 2 , \u03b1 3 , and \u03bb denote control parameters of the respective regularization terms, respectively. It is noteworthy that unlike the previous regularization methods such as local linear embedding ( Roweis and Saul, 20 0 0 ), locality preserving projection ( He et al., 2005; Zhu et al., 2013a; 2014c ) , and high-order graph matching ( Liu et al., 2013 ) that focused on the sample similarities by imposing nearby samples to be still nearby in the transformed space, the proposed method utilizes richer information obtained from the observations for finding the optimal weight coefficients W . The matrices X and Y are used to obtain the similarities, where X and Y are composed of MRI/PET features and target values, respectively. According to the previous work in Zhu et al. (2014a ) , theoretically the loss function in Eq.\n(1) can be designed to expect that the predictions of the model should be correlated for the similar subjects. But, in practice, it is not guaranteed due to unexpected noises in features. In this regard, we explicitly impose such correlational characteristic ( e.g., the proposed three kinds of relations) in the final objective function. Thus, it is expected that the proposed method can find a generalizable solution robust to noise or outlier."}, {"section_title": "Optimization", "text": "With respect to the optimization of parameters W , due to the use of the similarity weights of m ij in Eq. (3) , g ij in Eq. (5) , and s ij in Eq. (6) , it is beneficiary to transform the respective regularization terms to the trace forms using Laplacian matrices ( Belkin et al., 2006; Zhu et al., 2012; 2015a ) . Let H M , H G , and H S , respectively, be diagonal matrices with their diagonal elements being the column-wise or row-wise sum of the similarity weight matrices of M, G , and\nThe regularization terms can be rewritten as follows:\nwhere \nNote that Eq. (11) is a convex but non-smooth function. By setting the derivative of the objective function in Eq. (11) with respect to W to zero, we can obtain the form of\nQ \u2208 R d\u00d7d is a diagonal matrix with the i -th diagonal element set to\nHere, we should note that due to the possibility of being zero for w i in Eq. (13) , we add a small constant to the denominator in implementation, by following Nie et al. 's work ( Nie et al., 2010 ) . In solving Eq. (12) , it is not trivial to find the optimum solution due to the inter-dependence in computing matrices of W and Q . To this end, in this work, we apply an iterative approach by alternatively computing Q and W . That is, at the t -th iteration, we first update the matrix W ( t ) with the matrix Q (t \u2212 1 ), and then update the matrix Q ( t ) with the updated matrix W ( t ). Refer to Algorithm 1 and Appendix A , respectively, for implementation details and the proof of convergence of our algorithm. Input : X \u2208 R n \u00d7d , Y \u2208 R n \u00d7c , \u03b1 1 , \u03b1 2 , \u03b1 3 , \u03bb;\nOutput : W ; 1 Initialize t = 0 and set Q (t) a random diagonal matrix; Factorize matrices A = P T \u00d7 P and B = R \u00d7 R T ;"}, {"section_title": "5", "text": "Perform singular value decomposition on P and R ; Although there exists a general solver with this iterative approach 7 , its computational complexity is known to be cubic. In this paper, we propose a simple but computationally more efficient algorithm. In Eq. (12) , since both A and B are positive semi-definite, we can decompose them into two triangular matrices by Cholesky factorization ( Golub and Van Loan, 1996 ) :\nBy applying a Singular Value Decomposition (SVD) on each of the triangular matrices, P and R , we can further decompose them as follows:\nwhere 1 and 2 are diagonal matrices whose elements correspond to eigenvalues, and U 1 , U 2 , V 1 , and V 2 are unitary matrices, i.e., \nBy multiplying V T 1 and U 2 to both sides of Eq. (14) , we can obtain\nZU 2 , then we obtain the form of\nNote that both\n] \u2208 R c\u00d7c are diagonal matrices. Therefore, it is straightforward to obtain \u02dc W = \u02dc w i j \u2208 R d\u00d7c as follows:\nwhere e ij denotes the ( i, j )-th element in E . From the matrix \u02dc W , we can obtain W by\nIt is noteworthy that, thanks to the decomposed diagonal matrices obtained by Cholesky factorization and SVD, we can greatly reduce the computational cost in solving the optimization problem."}, {"section_title": "Feature selection and model training", "text": "Because of using the 2, 1 -norm regularization term in our objective function, after finding the optimal solution with Algorithm 1, we have some zero or close to zero row vectors in W . In terms of least square regression, the corresponding features are not necessary in regressing the response variables. Meanwhile, from the prediction perspective, the lower the 2 -norm value of a row vector, the less informative the respective feature in our observation. To this end, we first sort rows in W in a descending order based on the 2 -norm value of each row, i.e., w j 2 , j \u2208 { 1 , . . . , d} , and then select the features that correspond to the K top-ranked rows 8 .\nWith the selected features, we then train support vector machines, which have been successfully used in many fields ( Suk and Lee, 2013; . Note that the selected features are jointly used to predict two clinical scores and one clinical label. Specifically, we build two Support Vector Regression (SVR) ( Smola and Sch\u00f6lkopf, 2004 ) models to predict ADAS-Cog and MMSE scores, respectively, and one Support Vector Classification ( Burges, 1998 ) model to identify a clinical label, via the public LIBSVM toolbox 9 ."}, {"section_title": "Experimental results", "text": ""}, {"section_title": "Experimental setting", "text": "We considered three binary classification problems: AD vs. NC, MCI vs. NC, and MCI-C vs. MCI-NC. For MCI vs. NC, both MCI-C and MCI-NC were labeled as MCI. For each set of experiments, we used 93 MRI features or 93 PET features as regressors, and 2 clinical scores along with 1 class label for responses in the least square regression model.\nDue to the limited small number of samples, we used a 10-fold cross-validation technique to measure the performances. Specifically, we partitioned the data of each class into 10 disjoints sets, i.e., 10 folds. Then we selected two sets, one from each class, for testing while using the remaining 18 sets ( e.g., 9 sets from AD and 9 sets from NC in the case of AD vs. NC classification) for training in the binary classification task. We repeated the process 10 times to avoid the possible bias occurring in dataset partitioning. The final results were computed by averaging the repeated experiments. For model selection, i.e., tuning parameters in Eq. (11) and SVR/SVC parameters 10 , we further split the training samples into 5 subsets for 5-fold inner cross-validation. In our experiments, we conducted exhaustive grid search on the parameters with the spaces of \u03b1 i \u2208 { 10 \u22126 , . . . , 10 2 } , i \u2208 { 1 , 2 , 3 } , and \u03bb \u2208 {10 2 , \u2026 , 10 8 }. We empirically set k = 3 and \u03c3 = 1 to calculate three kinds of similarity, such as m ij in Eq.\n(3) , g ij in Eq. (5) , and s ij in Eq. (6) . The parameters that resulted in the best performance in the inner cross-validation were finally used in testing.\nTo evaluate the performance of all competing methods, we employed the metrics of Correlation Coefficient (CC) and Root Mean Squared Error (RMSE) between the target clinical scores and the predicted ones in regression, and also the metrics of classification ACCuracy (ACC), SENsitivity (SEN), SPEcificity (SPE), Area Under Curve (AUC), and Receiver Operating Characteristic (ROC) curves in classification."}, {"section_title": "Competing methods", "text": "To validate the effectiveness of the proposed method, we performed extensive experiments comparing with the state-of-the-art methods. Specifically, we considered rigorous experimental conditions: (1) In order to show the validity of the feature selection strategy, we performed the tasks of regression and classification without precedent feature selection, and considered them as a baseline method. Hereafter, we use the suffix \"N\" to indicate that no feature selection was involved in. For example, by MRI-N, we mean that either the classification or regression was performed using the full MRI features. (2) One of the main arguments in our work is to select features that can be jointly used for both regression and classification. To this end, we compare the multi-task based method with a single-task based method, in which the feature selection was carried out for regression and classification independently. In the following, the suffix \"S\" manifests a single-task based method. For example, MRI-S represents single-task based feature selection on MRI features. (3) We compare with two state-of-the-art methods: HighOrder Graph Matching (HOGM) ( Liu et al., 2013 ) and Multi-Modal Multi-Task (M3T) . The former used a samplesample relation along with an 1 -norm regularization term in an optimization of single-task learning. The latter used multi-task learn- ing with an 2, 1 -norm regularization term only to select a common set of features for all tasks of regression and classification. Note that M3T is a special case of the proposed method by setting \u03b1 1 = \u03b1 2 = \u03b1 3 = 0 . Table 2 shows the classification performances of the competing methods. We also compare the ROC curves of the competing methods on three classification problems in Fig. 3 . From these results, we can draw three conclusions. First, it is important to conduct feature selection on the high-dimensional features before training a classifier. The baseline methods with no feature selection, i.e., MRI-N, and PET-N, reported the worst performances. The simple feature selection method, i.e., MRI-S and PET-S, still helped increase the classification accuracy by 1.7% (AD vs. NC), 8.4% (MCI vs. NC), and 4.2% (MCI-C vs. MCI-NC) compared to MRI-N, and by 1.7% (AD vs. NC), 4.8% (MCI vs. NC), and 3.9% (MCI-C vs. MCI-NC) compared to PET-N, respectively. The other more sophisticated methods further improved the accuracies. Note that the proposed method maximally enhanced the classification accuracies by 4.8% (AD vs. NC), 11.4% (MCI vs. NC), and 11.5% (MCI-C vs. MCI-NC) with MRI, and by 5.6% (AD vs. NC), 10.2% (MCI vs. NC), and 9.0% (MCI-C vs. MCI-NC) with PET, respectively, compared to the baseline method."}, {"section_title": "Classification results", "text": "Second, it is beneficial to use joint regression and classification framework, i.e., multi-task learning, for feature selection. As shown in Table 2 , M3T and our method, which utilized the multitask learning, achieved better classification performances than the single-task based method. Specifically, the proposed method showed the superiority to the single-task based method, i.e., MRI-S and PET-S, improving the accuracies by 2.5% (AD vs. NC), 3.0% (MCI vs. NC), and 7.3% (MCI-C vs. MCI-NC) with MRI, and by 3.9% (AD vs. NC), 10.2% (MCI vs. NC), and 9.0% (MCI-C vs. MCI-NC) with PET, respectively.\nLastly, based on the fact that the best performances over the three binary classifications were all obtained by our method, we can say that the proposed regularization terms were effective to find class-discriminative features. It is worth noting that compared to the state-of-the-art methods, the accuracy enhancements by our method were 5% (vs. HOGM) and 4.7% (vs. M3T) with MRI, and 4.6% (vs. HOGM) and 4.2% (vs. M3T) with PET for MCI-C vs. MCI-NC classification, which is the most important for early diagnosis and treatment."}, {"section_title": "Regression results", "text": "Regarding the prediction of two clinical scores of MMSE and ADAS-Cog, we summarized the results in Table 3 and presented scatter plots of the predicted ADAS-Cog scores with MRI against the target ones in Fig. 4 . In Table 3 , we can see that, similar to the classification results, the regression performance of the methods without feature selection (MRI-N and PET-N) was worse than any of the other methods with feature selection. Moreover, our method consistently outperformed the competing methods for the cases of different pairs of clinical labels.\nIn the regression with MRI for AD vs. NC, our method showed the best CCs of 0.669 for ADAS-Cog and 0.679 for MMSE, and the best RMSEs of 4.43 for ADAS-Cog and 1.79 for MMSE. The next best performances in terms of CCs were obtained by M3T, i.e., 0.649 for ADASCog and 0.638 for MMSE, and those in terms of RMSEs were obtained by HOGM, i.e., 4.53 for ADAS-Cog and 1.91 for MMSE. In the regression with MRI for MCI vs. NC, our method also achieved the best CCs of 0.472 for ADAS-Cog and 0.50 for MMSE, and the best RMSEs of 4.23 for ADAS-Cog and 1.63 for MMSE. For the case of MCI-C vs. MCI-NC with MRI, the proposed method improved the CCs by 0.092 for ADAS-Cog and 0.053 for MMSE compared to the next best CCs of Table 2 Comparison of classification performances (%) of the competing methods. (ACCuracy (ACC), SENsitivity (SEN), SPEcificity (SPE), and Area Under Curve (AUC)). "}, {"section_title": "Effects of the proposed regularization trms", "text": "In order to see the effects of each of the proposed regularization terms, such as sample-sample relation, feature-feature relation, and response-response relation 11 , we further compared the performances of the proposed method with those of its counterparts that consider one of the terms or a pair of them. We present the performances of the counterpart methods and the proposed method in 11 For example, we considered the feature-feature relation by setting \u03b1 1 = 0 and \u03b1 2 = 0 in Eq. (11) . Fig. 5 . For better understanding, we also presented the performances of M3T as baseline that doesn't consider any of three regularization terms. From the figure, we can observe the following that: (1) A method that utilizes any one of the three regularization terms is still better than M3T; (2) The inclusion of more than two regularization terms into the objective function resulted in better performances than a single regularization, and ultimately the full utilization of the three relational characteristics achieved the best performances."}, {"section_title": "Multiple modalities fusion", "text": "With respect to multi-modal fusion, it is known that different modalities can provide complementary information, and thus can enhance the diagnostic accuracy ( Cui et al., 2011; Hinrichs et al., 2011; Kohannim et formed experiments using both MRI and PET (MP for short). We constructed a new feature matrix X with a concatenation of MRI and PET features at each row, but used the same response matrix Y as the above-described experiments. Tables 4 and 5 summarize the results of clinical label identification and clinical scores estimation, respectively. In line with the previous researches, the modality fusion helped improve performances in both classification and regression. Moreover, all the methods with the modality fusion selected the aforementioned brain regions with higher 'Frequency' than the corresponding methods with a single modality, such as on average 99.2%, 93.1%, and 92.7%, respectively, for our method, HOGM and M3T, on the data with the modality fusion.\nFinally, to check statistical significance, we conducted the paired t -tests ( Dietterich, 1998 ) (at 95% significance level) on the classification and regression performances of our method and the competing methods (including the experiments in Sections 4.3-4.6 ). Tables 2  and 4 show the p -values obtained from the values of ACC, while Tables 3 and 5 show the p -values computed from the values of CC. All these resulting p -values indicate that our method is statistically superior to the competing methods on the tasks of either predicting clinical scores ( i.e., ADAS-Cog and MMSE) or identifying class label."}, {"section_title": "Conclusions", "text": "In this work, we proposed a novel feature selection method by devising new regularization terms that consider relational information inherent in the observations for joint regression and classification in the computer-aided AD diagnosis. In our extensive experiments on the ADNI dataset, we validated the effectiveness of the proposed method by comparing with the state-of-the-art methods for both the clinical scores (ADAS-Cog and MMSE) prediction and the clinical label identification. The utilization of the devised three regularization terms that consider relational information in observation, i.e., sample-sample relation, feature-feature relation, and response-response relation, were helpful to improve the perfor- It should be noted that while the proposed method was successful to enhance the performances for AD/MCI diagnosis, the current method considered only the linear relationships inherent in the observations. Therefore, it will be our forthcoming research issue to extend the current work to the nonlinear formulation via the kernel methods.\n\u2265 0 . Proof. In Algorithm 1 , we denote part of Eq. (11) (t ) ). We also denote Q ( t ) as the optimal value in the t -th iteration for Q . According to ( Nie et al., 2010 ) According to Lemma 1 , the third term of the left side in Eq. (A.5) is non-negative. Therefore, the following inequality holds\n. (A.6) "}]