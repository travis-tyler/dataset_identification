[{"section_title": "", "text": "III. HAZARD CHARACTERIZATION............................................................................23 IV. NETWORKS AND NETWORK SCIENCE...............................................................47 "}, {"section_title": "V. NEW YORK CITY AS", "text": ""}, {"section_title": "INTRODUCTION", "text": "Resilience is an indicator of the preparedness and adaptability of a civil infrastructure system; it is useful to a range of management teams such as policy makers, engineers and emergency service workers. Civil infrastructure systems, such as transportation networks, power transmission systems and water distribution networks, constitute the backbone of a functioning society and affect entire populations if disrupted. The importance of resilience in networks has grown parallel to the increasing traffic volumes traveling highways and roads, and the continued construction of urban and suburban areas, trends that will continue well into the future. Additionally, there is a lack of long-term investment to elongate, if not simply guarantee, the lifespan of current infrastructure systems. With more vehicles on the roads, and the increase of truck loads, the consequences and delays resulting from a seemingly minor accident or failure can propagate through the system over a wide radius (FHWA, 2010); a developed area will suffer greater economic damage from an extreme event than a more rural region. Infrastructure resilience can be improved after assessing its current state, thereby reducing the vulnerability of civil infrastructure networks to disruptions and extreme events, allowing plans for possible failures, flexibility during probable disruptions, and post-event response and eventual repairs. These components correspond to the preparedness, absorptiveness, adaptation, and recovery of a resilient infrastructure system. This can be done by analyzing the resilience of the infrastructure system, allowing a holistic approach that takes into account several different aspects to improve the network's overall performance against disturbances. Of the expected extreme events, the urban networks located on the northeastern coast of the US are most susceptible to hurricanes, their hazardous wind speeds and storm surges. Hurricanes are so deadly because of their combination of destructive forces; namely, high wind speeds, powerful storm surges and the resulting flooding. As a result of climate change, the intensity of hurricanes and extreme climatic events is expected to worsen (IPCC, 2014). Storm surges will be the concentration of this investigation and the Sea, Lake, Overland Surges from Hurricanes (SLOSH) program by NOAA will be used to develop storm surge projections. Simultaneously, the predictions for sea level rise released in the 2014 IPCC report were more severe than those in previous reports. The Atlantic coast of North America is expected to experience accelerated sea level rise because of regional factors increasing the effects of global sea level rise. These include the spatial distribution of Earth's gravitational mass, the climate of the Atlantic coast and geological processes along the shore. The northeastern coast of the US will thus be one of the most affected regions in the world, necessitating improvements and preparations for worsened conditions. To effectively and efficiently manage the planning and finances of these improvements, decision makers need to know where vulnerable areas and the most important links of the transportation network are located. In a dense highway network, this often translates to identifying the roads and intersections which are highly vital to the adequate connectivity of a transportation network. This can be done using network performance indicators such as topological properties that describe, quantitatively, the connectivity and redundancy present in the system. Topological graph theory is the study of the physical layout and structure of a network; this is especially relevant to transportation networks, which are entirely physical systems. The New York City metropolitan transportation network is chosen as a case study with which to illustrate the measured performance and resilience of a major urban transportation network. Modeling the region's highways and roads as a simple network made of links and nodes allows the mathematical determination of network properties. With these measurements, the resiliency of the current transportation network can be quantified. The text is arranged in a format to allow a cohesive, sequential ordering of research tasks and results. In Chapter 2, the definition of the term resilience is examined. Resilience is a widely used concept to describe properties of a range of systems and objects in science, technology, and engineering. It was first defined, within the academic world, as a term relative to ecological systems. Since then, although the concept of resilience has been applied in different ways, there are significant commonalities in modern uses that derive from this first definition. Resilience is then discussed as it relates to engineering and, more specifically, systems of infrastructure. A thorough literature review of the researched methods to analyze, describe, and quantify resilience is synopsized. To provide a foundation for the rest of the thesis, resilience is differentiated from terms commonly used in similar ways-namely, risk assessment and vulnerability. The exhaustive study of resilience from its origins to the many forms it takes now was useful to develop the most appropriate definition and method to measure resilience in this study of highway networks, which is described in detail in the last subsection of Chapter 2. Chapter 3 summarizes the hazard characterization as it pertains to this case study. Specifically, hazards highly relevant to the northeastern coast of the U.S. are discussed. In Section 3.1, a review of the available hurricane and wind field models is done. This describes the models most applicable to analyzing the effects of a hurricane, and how wind field models may differ among studies. Of most interest in this study are the storm surge models, described in Section 3.2. The two most commonly used models, SLOSH and ADCIRC, are compared and their advantages and disadvantages are outlined. In Section 3.3, climate change is addressed and its projected effects are related to the worsening of hurricane intensity, frequency, and storm surge consequences. Then a review of climate change as it specifically relates to transportation networks is performed (Section 3.4). Preparing and developing infrastructure networks for the effects of climate change is a challenge across the globe, so this review is critical for exploring applications of this study. Section 3.5 of Chapter 3 concentrates on the studies performed on New York City, allowing a better understanding of its uniqueness as an urban development and of its regional hazards. The last section in Chapter 3 described the hazard characterization in this study. In Chapter 4, graph theory is reviewed and its high relevancy to measuring resilience is outlined. To adequately understand why graph theory and its principles can be used to describe resilience and applied to a study of highway networks, an in-depth review of graph theory literature is performed. Despite the overwhelming nature of graph theory and its principles, the first subsection of Chapter 4 covers the concepts of graph theory necessary to comprehend the analysis in this study. This subsection, in addition to several definitions and terms, includes a short synopsis of select models, such as the Erdos-Renyi graph, that are used in network science to develop conclusion of graph property behaviors. Although these models are not employed in this thesis, analysis of network properties is performed which is derived from such initial network models. Chapter 4, Section 3 relates the study of resilience to graph theory, reviewing researchers who have used graph properties in the study of resilience. Section 4 examines how network response to failures is studied, both by differentiating between removal techniques and by defining types of failures known to researchers. More specific to this thesis is Section 5, which reviews the available studies that have employed graph theory to study transportation networks. The concluding section of Chapter 4 explains how the expansive graph theory is applied in the approach of this thesis. Chapter 5 provides a brief discussion of the methods and a comprehensive synopsis of the results of this case study. The highway network of New York City was chosen as a case study because of its location on the northeastern coast of the U.S., its high density of development and population, as well as its existing hurricane hazard. To study the network behavior in a general sense, the highway network, after being modeled as a combination of links and nodes, was subject to random node removal. Described in Chapter 5.2, the random node removal involves increasing the fraction of nodes which are removed after several trials of removal have been run at each fraction. During this random removal, graph properties are measured to observe any trends or transitions that occur during increasing nodal failure. The node removal technique is then supplemented with SLOSH results, meaning that hurricane storm surge predictions determine which nodes are removed from the network, and this is summarized in Chapter 5.3. The next section, Chapter 5.4, described the results of considering node elevations in the node removal based on SLOSH scenarios. Using these results, the same network properties that were measured during random removal are again quantified and trends are described. Chapter 6 is the concluding chapter of this thesis."}, {"section_title": "CHAPTER II", "text": ""}, {"section_title": "DEFINITION OF RESILIENCE", "text": ""}, {"section_title": "Introduction", "text": "Before developing a topological-based method to measure the resilience of transportation networks, it is necessary to first define resilience in this context. Specifically, the focus of this investigation will be the resilience of critical infrastructure. The damage or destruction of critical infrastructure impacts security, economic flow and public health and safety (The White House, 2013). The critical infrastructure systems of which we wish to consider the resilience are those that most govern community life, sometimes referred to as lifelines. The goal of this investigation is to formulate a standardized approach to measuring the resilience of critical infrastructure systems with an algorithm describing the process. Specifically, a transportation network will be evaluated from a holistic perspective, considering all components on which the network function relies. The motivation of this project is to improve the resilience of existing infrastructure by finding a method to first evaluate it and, secondly, by applying this method to identify the most efficient approach to bolstering network resilience. Additionally, the effects of climate change will be considered as they relate to the vulnerability and the level of exposure of infrastructure. The resilience literature reviewed is from a variety of fields and approaches, enabling a comprehensive perspective of the concept or resilience, its derivations, and applications.\nRecent natural disasters that have hit critical northeastern urban areas, such as Hurricanes Sandy and Irene, exposed the vulnerabilities of current infrastructure to extreme events of climatic nature. While manmade hazards, such as terrorist attacks, are worthy of their own analyses, the random nature of natural disasters creates a dangerous situation for which it is difficult to prepare. Furthermore, the increasing awareness and worsening projects of climate change have rendered previous characterizations of many hazards as underestimates of the actual risk. This is of interest as the northern Atlantic coast of the U.S., the study concentration area, is expected to be especially affected by changes in hurricane patterns and rising sea levels, which would only worsen the effects of a hurricane (Sallenger et al., 2012). Additionally, higher percentages of the population are migrating towards the coasts at an unprecedented rate, which increases the number of people affected by the adverse effects of climate events (Wilson et al., 2010). Because of the regionally-specific nature of hazard characterization, a review of the risks most relevant to the urban infrastructure of the northeastern coast of the United States was necessitated. The hazard characterization, described in this chapter, is a review of hurricane risk and a synopsis of potential climatic changes which would exacerbate the consequences of a hurricane.\nTransportation networks, due in large part to their physical layout at the most fundamental level, consist essentially of nodes and links. Their properties can thus be adequately measured with graph theory, or the study of networks comprised of nodes and links. Many researchers have chosen to use the principles of graph theory in studies of networks (Adachi, 2007;Buhl et al., 2004;Cohen et al., 2000;Hu et al., 2011). The topological properties used are derived directly from the fundamental theories of graph theory to achieve consistency and reliability. While shortest path length, efficiency-were each found to behave differently as nodes were randomly removed from the network. These studies are highly relevant to this thesis, which uses graph theory to study resilience. However, to understand the basis of graph properties holistically, a complete literature review of the theory and range of applications of graph theory was performed.\nThe highway network of New York City was chosen as a case study for several reasons. Its location is situated on the coast, specifically the northeastern coast of the US, and its highly developed, urban infrastructure renders the consequences of an extreme event devastating and costly. Currently, its main structural protection against hurricanes is a seawall on the border of Manhattan rising just 1.5m above sea level (Lin et al.., 2008). While the northeastern coast is not as at risk for a high frequency of hurricanes as the Gulf Coast is, its risk of high intensity hurricanes is substantial. Aside from hurricane risk, the number of days of extreme precipitation in New York City is also predicted to double under future climate scenarios (Ntelekos, 2010). On a larger scale, the importance of New York City to the economy of its region and the nation cannot be overstated. The potentially devastating consequences of climate change will not be confined to the relatively small set of counties analyzed, instead inducing possible widespread damage to the U.S. economy (Rosenweig and Solecki, 2001). By concentrating on vital urban centers, an investment is made not only for the individuals inside the city but for those connected to it. Of course, the inherent diversity present in such environments makes implementing and enforcing policy changes challenging for governing bodies. The risks pertinent to the New York City metropolitan area are mainly hurricanes which involve the simultaneous occurrence of high winds, torrential rains, and large storm surges."}, {"section_title": "Resilience in other fields", "text": "The first documented appearance of the concept of resilience as a scientific term is found in Holling (1973), where it is used to describe the adaptive behavior of ecological systems: \"Resilience [is] a measure of the ability of these systems to absorb changes of state variables, driving variables, and parameters, and still persist. In this definition resilience is the property of the system and persistence of probability of extinction is the result.\" This primary meaning served as a foundation for future ecologists and biologists. Several scientists and researchers have approached the topic of resilience from an economic standpoint, choosing to highlight the financial and business effects of an extreme event or disaster. This perspective is certainly relevant to our investigation of critical infrastructure resilience, as the economic losses may outweigh the physical losses to the system. In determining economic resilience, although it is perhaps more tangible and easier to quantify the physical asset loss immediately after an extreme event, this often only corresponds to a short-term economical consequence. A longer period of loss occurs with respect to business operations, both indirectly and directly (Rose, 2004). Of course, in order to appropriately suggest strategies through which resilience and vulnerability of critical infrastructure can be improved, the economic factors and costs must be taken into account. In analyzing the economic resilience of a system, two different types of resilience are recognized: the inherent resilience, which occurs in normal situations, and the adaptive resilience, which occurs in situations of crisis. Adaptive resilience is separated because it is characterized by an ingenuity only elicited in an extreme event, such as conserving water after a devastating earthquake (Rose, 2011). These branches of resilience may be applied to a more engineered approach. Finally, excluding economic resilience will actually result in an underestimation of post-event measurements of loss. Considering the economic resilience is also significant to legislators and decision-makers because it allows an evaluation of mitigation strategies primarily concerned with the minimization of financial losses. In a more specific study, several of these strategies for businesses, such as using less water by recycling and production rescheduling, were found possible to implement with little to no cost (Rose, 2007). Resilience is separated by magnitude: microeconomic, mesoeconomic, and macroeconomic levels. At the microeconomic level, resilience reflects individual behavior of firms, households, and organizations. Resilience of the economic sector, individual market, or cooperative group is represented at the mesoeconomic level. And at the macroeconomic level, resilience is that of all individual units and markets combined. The three levels of resilience is a concept that could be easily adapted to infrastructure resilience by breaking an infrastructure system down into level of subsystems and components. Resurreccion et al. (2013) uses a stochastic inventory model to maintain above-minimum levels of inventory as a resilience strategy that could effectively reduce the onset of disruption. The cascading effect, which is the domino-like propagation of a failure from its origin outward throughout the network, and the disruption in complex infrastructure and economic systems are discussed with the assumption that preparedness and prevention are the two principals to increasing the resilience of a system. By building crucial adaptability into a system to foster disaster resilience and recovery, despite the clear costs involved, the response to such failures can be mitigated. Risk assessment and management are differentiated: risk assessment is defined as a process that addresses the possible hazards and their respective probabilities, as well as their consequences; risk management is a separate process, which addresses the options for managing the risk. Input-output data is utilized to build relationship of inventory, demand, and resilience, while distribution functions are used to model inventory of the manufacturing and retail and trade sectors. The dynamic inoperability input-output model (DIIM) is used to perform event simulations and project amassed economic losses; the stochastic-based dynamic inoperability input-output model (SIDIIM) is specific to an individual sector and estimates the consequences of a disaster. Both the SIDIIM and DIIM are calculated across a predetermined recovery period. Economic loss and inoperability are the two identifiable consequences of disaster for each economic sector. Although hazard analysis is often dependent on the long tail of low likelihood, high damage events, most economic loss estimations in the resulting simulations were largely representative of the high likelihood, mid-level damage events. The study provided an alternative perspective on hazard consideration and suggested that high probability events were important in analysis."}, {"section_title": "Resilience in engineering", "text": "Resilience as an engineering concept was differentiated also by the ecologist C.S. Holling in 1996. The differences between the established definitions are important. In the ecological definition of resilience, the term reflects the ability of the system to absorb changes of state variables, driving variables, and parameters, and still persist. Engineering resilience, however, is not inherently adaptive. For instance, while an organism may benefit from changing in response to an environmental event, a structure or system of structures is not designed to be dynamic; instead of returning to an enhanced state post-event, as an ecological system may, a building would ideally return to its original state ( Figure 1). Several fundamental characteristics are common to the definition of resilience across ecological, economic, and hazard-based research. The ability of a system to anticipate and prepare for these types of likely disruptions is usually considered in the resilience. Once the network-damaging event has occurred, the network absorptiveness, the ability of the system to reduce and adjust to shock, is the commonly identified next stage in a time-dependent study of resilience. This is followed by a measurement of adaptability, a concept used in varying ways, but the root of remains the ability to change, adapt, and reform in the face of an extreme event. Perhaps derived from the ecological origins of resilience, Ortiz et al (2009) defines a specific measure of adaptability as adaptive capacity, which measures how efficiently a network can accommodate extra disturbances in flow. After the initial disruptive event has occurred, the system's ability to recover completely and in a reasonable time frame contributes to its resilience. The speed and completeness of recovery is mentioned in a number of texts, whether referring to a structure or economic market. In total, these compose four important \"abilities\" of a system which are used to measure resilience: ability to anticipate, to absorb, to adapt and to recover (Carlson et al., 2012). Additionally, across disciplines, the idea of resilience almost always includes both pre-event and post-event tasks. Bruneau et al. (2003) outline four aspects of the resiliency concept: robustness, redundancy, resourcefulness and rapidity. Robustness is a measure of strength, describing how adept the system is at simply resisting the forces of disruption. The existence of alternative paths and options in a network is represented by a level of redundancy; in a transportation network this may refer to the range of combinations of roads and highways allowing a vehicle to travel between the same origin and destination. Post-event, the ability to efficiently direct resources and repairs reflects the system resourcefulness and the rapidity is the speed at which recovery is accomplished. Encompassing these concepts, resilience of a transportation network is defined as the ability of the system to withstand, adapt to and rapidly recover from the consequences of disruptive events (Turnquist and Vurgin, 2013). Resilience of a transportation network is defined similarly as the ability to absorb minor disruptions and, after more extreme events, to return back to full serviceability in a short time (Ortiz et al., 2009)."}, {"section_title": "Risk assessment, vulnerability and resilience", "text": "Although often used interchangeably with risk, resilience considered varying with the dimension of time, the time-dependent occurrence of the event and the related resilience measures, resilience assessment can be distinguished from risk assessment. Risk is inherently a simpler concept and can be defined as the likelihood of experiencing damage or loss. Risk assessment is the analysis and determination of this probability. Several researchers use the measurement of risk to describe some aspect of system resilience because they are so closely interconnected (Furtado and Alipour, 2013;Bocchini and Frangopol, 2011;Shinozuka et al., 2003Shinozuka et al., -2004. However, the entirety of resilience is more of a process, not solely determined by the system response to a disruption, but also by its preparation and recovery. Another term referenced simultaneously, and incorrectly interchanged, with risk and resilience is vulnerability. While risk describes the likelihood of threats and hazards, vulnerability is the proneness of a system or object to experiencing adverse effects (IPCC, 2012). Vulnerability, like risk and resilience, is complex to measure and can vary across dimensions such as the economy and time. Gitz and Meybeck (2012) postulate that the first step to improving resilience is reducing the vulnerability."}, {"section_title": "Quantifying Resilience", "text": "While there have been attempts to qualitatively report resilience, at the current time, there is no efficient method to quantify the resilience with regards to natural disasters. This difficulty in accomplishing an accepted method of quantification has as much to do with the ubiquity of the term resilience to describe a variety of properties as it has to do with the inherent complexity of the concept. As a consequence, there is no appropriate way to compare the resilience of different communities and civil infrastructure systems (Simonovic, 2012). This is to the point that some researchers reject the possibility of accurately measuring resilience with a single number, citing the range of different sectors exposed to different risk intensities, and propose its representation as a multi-dimensional concept with a range of contributing factors (Haimes, 2009). Resilience can be represented as the performance of a system, which is a quantifiable quantity, in both time and space paths. There are several challenges that arise when considering resilience as a function of temporal and spatial variables, namely finding these functions with respect to a number of different sub-systems (i.e. the spatial distribution of hazards, location-dependent economic representations, time-varying health data, and energy consumption over time). There is widespread emphasis on the probabilistic nature of resilience. It is accepted that the probabilistic nature both of extreme event occurrence and the variability inherent in the nature of a civil infrastructure system necessitates the application of probabilistic theories in quantifying resilience. Furthermore, to increase the validity of resilience analysis with respect to different events or threats, it is crucial to include its probabilistic characteristics. The robustness of a system is thus determined by the vulnerability and exposure conditions of several smatter network components, and is expressed with regard to PHEOS. PHEOS is an acronym which stands for the five units of community resilience, as outlined by Simonovic (2012): physical, health, economic, organizational and social components. In order to successfully and completely consider each of these units, multi-disciplinary research and investigations must form the basis of the resilience model. Measuring and analyzing resilience may be made significantly easier by utilizing existing models of these systems (i.e. water lines, transportation networks, economic flow) and observing the effects of an extreme event. A systematic, quantitative approach to measuring resilience would allow for such comparisons and a more comprehensive evaluation of resilience which, in turn, would lead to more effective strategies to reduce loss in extreme events. It is also necessary to note that, in a resilient community, events and changes can allow for growth and ingenuity (Simonovic, 2012). Adachi (2007) compares seismic event effects and network resiliency modeled from a Probabilistic Hazard Assessment (PSHA) to the results of a single earthquake scenario. The stark difference in results clearly motivates relying on probabilistic methods over those which are uniformly distributed. The difficulties and challenges in quantifying resilience is a ubiquitous theme investigated by many researchers (Haimes, 2009;Bruneau et al., 2006;Ortiz et al., 2009). The complexity and multi-faceted nature of resilience, in addition to the lack of a central and concise definition, form an unbalanced and complicated foundation on which a variety of uncertainties exist. Motivation for pinning down one reliable process to quantify resilience is high because of its implications. With a systematic and simple measurement of resilience, comparative studies could more easily be carried out on different communities and methods used to currently enhance resilience. The goal of a resilience measurement is to represent the ability of a system to react to the stresses which challenge its performance, specifically in response to a disaster. After comparing resilience studies, the commonality among these is the idea that measuring resilience requires an understanding of system structure and operations, in addition to the depth and range of holistic uncertainties. Qualitative methods are for the most part rejected for universal use because of their objective bases. Resilience quantification dependent on a multitude or set of data values and measurements (i.e. Bruneau et al., 2003) is less desirable than a process which results in one number, but which single metric to use is another discussion. Graph theory used with GPS navigation data and GIS is more comprehensive and adaptable to different regions and networks (Leu et al, 2010;Berche et al., 2009). By marking nodes and links as important, then simulating attack scenarios with graph indicators, Berche et al. (2009) were able to gather a quantifiable rank for the system resilience. Overall, despite some breakthrough theories and approaches, current methods for measuring resilience are too dependent on specific concepts and approaches. The quantification of resilience is focused on earthquakes, justified by their high economic losses and community disruptions. Bruneau et al. (2003) recognized the need for quantitative measures of resilience to more efficiently evaluate and compare the options for improving systems and identifying weaknesses. Enhancing seismic resilience, similar to the improvement of any type of resilience, means minimizing any reduction in the quality of life from earthquakes. The most critical organizations (those that will have the largest and most immediate impacts on quality of life) are identified: utility companies, hospitals and emergency management companies. System performance is approached as varying over time, a path through a multi-dimensional space of performance measures ( Figure 2)."}, {"section_title": "Q(t)", "text": "is defined as the quality the infrastructure of a community which varies with time. Resilience, R, is the size of the degradation in quality and functionality over time and is a modified integral of the function Q(t). This is sometimes referred to as the \"resilience triangle\" (Reinhorn et al., 2011). But outlining what exactly Q(t) is and its definition is somewhat avoided. The concept of functionality is made more concrete in Reinhorn et al. (2011). A performance function is such defined as a relationship which adequately describes the usefulness of the system and dependent on usage, integrity, costs and other specific parameters. In the commonlyused example of a hospital, this would translate to the function of providing better health to individuals with the total population, unhealthy population, medical staff, medical infrastructure, and built infrastructure as parameters. Bruneau and Reinhorn (2006) then used sets of intersecting axes to account for the relationship between robustness, redundancy, resourcefulness and rapidity. By expanding the two-dimensional plot of quality vs. time (robustness and rapidity) into a third dimension, resourcefulness and redundancy can be connected. The inclusion of a temporal variable to measure resilience by is also a defining point in the Ortiz et al. (2009) investigation. Using the dependent variable to represent some measure of performance (i.e. cost or travel time), which may differ depending on the system, the resilience plot is split into two phases of (i) disruption and (ii) recovery to quantify the resilience of freight transportation networks. To quantify the seismic resilience, the concept of Sliding an Overlaid Multidimensional Bell-curve of Response for Engineering Resilience Operationalization (SOMBRERO) is introduced in an Orthogonal Limit-space Environment (OLE). The OLE is made of floor accelerations (PSA) and inter-story drifts (S d ), and the limits of both are indicated by dotted lines. The response surface is a probability distribution surface expressed with a series of contours. Dependent on different situations, resilience-enhancing measures and disasters, the surface can move along the axes defined by the OLE. When the surface crosses one of the limits previously defined by dotted lines, the area under the curve represents the probability that response exceeds the limit. It is thus visually (and quantitatively) clear when measures taken enhance the resilience or when disasters weaken the structure. The methods proposed in Bruneau and Reinhorn (2006) provide a relationship between seismic performance, fragility curves and resilience functions. By first defining resilience and identifying dimensions, ways to measure the dimensions of resilience are more easily found. Three complementary measurements of resilience make up the general framework: \"Reduced failure probabilities\", \"Reduced consequences from failures\", and \"Reduced time to recovery\". There are two sets of dimensions identified; one set-robustness, resourcefulness, redundancy and rapidity--reflects the properties of resilience in any situation while the second set--technical, organizational, social and economic--represents four interrelated dimensions encompassed by resilience. The \"four Rs\" each have definitions for technical, organization, social and economic dimensions. (Two distinguishing notes: redundancy and rapidity are the \"means\" by which resilience can be improved to achieve the \"ends\" of robustness and resourcefulness -these are also represented in a plot of Q(t) and time; the resilience of companies like utility corporations is represented by technical and organizational performance measures while community resilience is better represented by social and economic measures.) One of the problems in this postulation is that these four dimensions of resilience cannot be accurately measured by one number representing performance. Attempting to build on the work done by Bruneau et al. (2003), Cimellaro et al. (2010) defined resilience as the normalized area underneath Q(t). Although similarly focused on seismic resilience, the goal is admittedly to develop a function or process which is adaptable to all types and forms of resilience. The resilience properties outlined by Bruneau et al. (2003), and clarified by Bruneau and Reinhorn (2006), are also carried through Cimellaro et al. (2010). Two new variables are introduced to better define these dimensions: control time (T lc ), and recovery time (T re ). T lc is a period usually decided by owners of the system under consideration and is used to normalize the resilience to calculate the Resilience Index. T re is the period necessary to restore the structure functionality to a desired level (the same, close to, or better than the baseline). The value of T re is largely uncertain and usually smaller than the T lc . Using loss and recovery functions, the factors of Q(t) are quantified and the resulting resilience value is dimensionless. "}, {"section_title": "Resilience to climatic effects", "text": "Perhaps of especial interest to our investigation is Simonovic and Peck (2013), which evaluated climate change resilience as it applies to coastal megacities. Urban resilience, as defined in the investigation, is the ability of physical and social urban systems to absorb disturbance while still being able to continue functioning. Like Bruneau et al. (2003), Simonovic and Peck (2013) identified four interconnected areas from which to define urban resilience: the physical environment (both constructed and natural), metabolic flows (production, supply and consumption chains), governance networks (institutional and organization), and social dynamics (demographics, human capital and inequity). They built on the idea that resilience is made of inherent and adaptive qualities (Rose, 2003) and noted its applicability to the four interconnected areas outlined above. A concentration on critical facilities is taken in this process, like most other quantification framework outlines. Generally defined in Equation (1.1), resilience is represented in Equation (1.2) as a function of the dimensions vulnerability (V), exposure (E), and adaptive capacity (AC). Resilience is taken as a definite integral or sum over all impacts which characterize the impacts of disasters on a community (physical, health, economic, social, and organizational). What is developed is referred to as an ST-DRM, a space-time dynamic resilience measurement, which defines the system performance at a specific point in space relative to time, mathematically shown in Equation (1.3). These impacts are quite comparable to the dimensions of resilience outlined in Bruneau et al. (2003) and are measured using different metrics such as GDP for economic impact, and age for social impact. Resilience is calculated from the change in system performance. A coastal megacity is approached as a system made up of three interdependent subsystems: the natural subsystem, the socio-economic subsystem and the administrative institutional subsystem. The resilience framework described is then implemented in a system dynamics model called the Coastal Megacities Resilience Simulator (CMRS) and integrated with GIS software. This results in ST-DRM values for each point in a region, and a dynamic map that illustrates the changes in resilience. As a case study, Simonovic and Peck (2013) suggested that adding medical emergency response teams would increase resilience in a coastal Canadian community and proved this by using their ST-DRM in combination with the CMRS. Another interesting case study was performed on two communities in Chennai, India by developed. The goal of the study was to understand resilience as it applies to communities and to find ways in which community resilience can be enhanced. Like other types of resilience, community resilience is defined here as the ability of a group of individuals to absorb, manage and bounce back after a climate-related disaster. Like the Boston metropolitan area studied in Suarez et al. (2005), Chennai is a low-lying area near the sea, making it especially susceptible to coastal flooding caused by storm surge. Quite clearly, several individuals who reside near rivers and canals are more at risk of flooding. Less clear, however, was the finding that despite their flood experience, these individuals do not show an increase in adaptive capacity as an aspect of resilience to flooding. As a baseline, and to determine their existing physical capacity of resilience as coping capacity, the current supply of electricity, water and other necessities was assessed among households. Joerin et al. (2012) quantified resilience as a combination of adaptive and coping capacity. Not surprisingly, with fewer resources, less privileged residents were also found to be less resilient. However, households with disaster experience, which were expected to have more resilience, are not more resilient compared to other households. This was attributed to the lack of adaptive capacity needed to increase coping capacity. Although the survey techniques adapted by Joerin et al. (2012) were extensive and encompassed many facets of resilience, their qualitative and anthropological approach to resilience is not especially helpful in pursuing a quantitative method, but does provide insight into the humanitarian impact of pursuing resilience improvements to natural disasters."}, {"section_title": "Resilience approach in this thesis", "text": "By performing an exhaustive literature review encompassing the variety of applications and definitions of the concept of resilience, resilience as defined in this study will incorporate some of the ideas previously discussed. Specifically, properties of graph theory will be applied to effectively quantify quantities such as robustness, redundancy, resourcefulness, and rapidity, the \"four Rs\" of resilience originally outlined in Buneau et al. (2003). These are components that can be appropriately applied to the study of transportation networks and infrastructure engineering, perhaps more so than the types of resilience identified in economic studies, like adaptive and inherent resilience (Rose, 2004)  In this particular study of transportation networks, disruptive events be non-targeted and originate from climatic sources. This poses unique challenges in improving the separate, temporal states of resilience described in Carlson et al. (2012)-anticipation/preparedness, absorptiveness, adaptability, and recovery-especially considering the uncertainties associated with extreme events of climatic nature. In a highway network, the resilience will be broken down into these separate abilities of the entire system to withstand, adapt to, and rapidly recover from the consequences of disruptive events such as floods and surges. As measuring resilience requires some concept of network performance, and applying the principles used in Bruneau et al. (2003), where Q(t) is representative of the quality of infrastructure, the network performance in the highway network will be viewed as relative to the serviceability of highway drivers and vehicles using the road networks. Because delays, cancellations, and traffic accidents can cause significant financial and economic losses, the usability, connectivity, and network flow will be of utmost concern in this study.  "}, {"section_title": "Literature review", "text": ""}, {"section_title": "Hurricane and wind models", "text": "Although storm surge is the main concern of this study, wind causes a large portion of hurricane damages. Additionally, storm surge is worsened by winds and the most accurate analyses of hurricane damages include a joint simulation of wind and storm surges. One of the reasons it is difficult to perform these types of modeling conditions is the lack of adequate wind speed data, motivating several studies of synthetic wind databases. Rosowsky and Lee (2007) published one of the first papers on the development of a synthetic hurricane wind speed database. Because wind speed data is highly useful but largely inconvenient to measure, a \"short-term hurricane wind field model\" was created to estimate surface wind speed time histories at each site for the historical hurricanes which lack this format of wind speed data. Wind speeds can be correlated to physical and economic damage, two consequences that are of high interest to predict and measure. The \"long-term hurricane wind field model\" was developed to replicate discrete hurricanes stochastically. In contrast to the short-term model, the long-term model was employed to create synthetic wind speed records, which can be used in structural risk assessment, to compose maps of wind speeds and to analyze hurricane hazards. For the gradient wind field model, there are limited historical records. Georgiou's model, which includes pressure and the maximum wind speed radius, was used instead of historical records (Georgiou, 1985). The model from Rosowsky and Lee (2007) was validated after comparing observed and simulated hurricane parameters along the east coast. Predicting potential extreme events, such as hurricanes, involves so much uncertainty that thousands, if not hundreds of thousands, of events must usually be considered. The efficiency and time required to perform hazard analyses on such a large scale can hamper the benefits of a thorough investigation. Estimation of losses resulting from hurricanes over the long term is often completed by simulating thousands of historical or synthetic regional hurricanes and estimating the losses caused by each. Legg and Nozick (2010) applied the hazard-consistent scenario approach, first introduced by Chang et al. (2000) for earthquake simulation, to choosing a small set of hurricanes to simulate. This approach reduces an expansive set of events by calculating new annual occurrence probabilities for the smaller set of events, such that one event can mathematically represent all like events (Legg and Nozick, 2010). Legg and Nozick (2010) compare available methods to loss estimation for long-term regional hurricane events to illustrate the advantages of their proposed approach. The first method, simulating events using historical or synthetic catalogs, has different disadvantages dependent on which type of database is used. If historical hurricanes are considered, the method is more direct but requires the often-disproved assumption that the future will reflect the past. Using synthetic databases, on the other hand, requires a much higher level of computation but can account for storms not yet recorded. Instead of simulating the storm database more than once, the sample return period event method selects one storm with a 1/r probability of exceedance after performing only one full synthetic database simulation. The hazard and loss associated with this one storm is then considered to be the r-year return period hazard and loss maps. The downside of this method is that it does not include all possible events so, if the study area extends beyond that which is affected by the one selected event, it may not be representative of the hazard. In the maximum likelihood approach, all historical storms are simulated and a Weibull distribution is determined for the wind speeds at each specified site. These Weibull distributions are then used to calculate r-year return period wind speeds, which are applied to damage and loss models. Unlike the previous method, the maximum likelihood approach does not depend on a synthetic database of catalog; the disadvantage is that, over the course of analysis, the spatial information for individual events and the spatial correlation between assets is lost. The method outlined in Legg and Nozick (2010), the hazard-consistent probabilistic scenario method, is different because it can guarantee the lowest possible error between regional hazards and does not necessitate the user to identify which events belong in the reduced set. The larger set of \"candidate\" hurricanes, when applying the approach of Legg and Nozick (2010), should be both realistic and span the range of possible wind speeds for an area. It can be a set of historical hurricanes, a synthetic database, or taken from HAZUS-MH. The required input data includes this candidate set as well as the wind speed for each census tract and return period, the wind speed at each census tract caused by each hurricane, and user-defined parameters for wind speed threshold and the maximum number of hurricanes in the smaller set. Davidson et al. (2011) expanded upon this method considering both wind and storm surge for hurricane hazard analysis. Usually, wind and storm surge are not both considered in the same study. Even when they are, they are assessed independently. The method proposed by Davidson et al. 2011estimates long-term probabilistic regional hurricane hazard with storm surge and wind simultaneously. Nominally an optimization-based probabilistic scenario (OPS) approach, the method is based off of that described in Legg and Nozick (2010). After collecting a large set of potential hurricane scenarios, wind speeds are estimated across the area of interest, using a wind metric such as sustained wind speed, as well as the coastal surges for each storm scenario, done using a coarse or low-resolution grid. Using mixed-integer linear optimization, a smaller set of hurricanes is identified and the annual occurrence probability is matched to equal the true wind speeds and surges. The mixed-integer linear optimization minimizes the weighted sum of the errors over point locations between true hazard curves and the curves from the set of reduced hurricanes. Both the wind speed and surge depth errors are divided based on intensity and the population density at the control point. This is important because errors in high density or high wind speed/surge points are considered more important and are weighted, a technique that could be adapted to reflect different study priorities. Each error is thus the difference between the true annual exceedance probability 1/r and the annual exceedance probability estimated using the reduced set of hurricanes. The mixed-integer linear optimization requires a heuristic solution to first identify the smaller set of hurricanes and then a standard linear program solver to estimate the annual occurrence probabilities. For this smaller set, the inland surges are estimated across the area of interest using a high-resolution surge grid. Finally, loss estimations can be determined for each hurricane and, when applying the specified annual occurrence probabilities, loss exceedance curves can be generated. The OPS method probabilistically accounts for the entire range of possible events and their probabilities of occurrence, and then provides approximate calculations of storm surge and wind speeds. Davidson et al. 2011included a literature review of the wind hazard and storm surge hazard for long-term hurricane modeling. For the wind hazard, they categorize two methods: analysis which models the full hurricane track and analysis that separates the most important hurricane statistics and assumes a linear decay model after landfall. The full track analysis, or the empirical track method (ETM), was first introduced by Vickery et al. (2000) and involves updating key hurricane parameters every six hours using a wind field model. This method, although computationally demanding, is appropriate for large regions, allowing for the full probability distribution of losses and maintains the spatial correlation for hurricane scenarios. Because there are thousands of events, the damage and loss estimations are not found for each scenario; instead, the sample return period event method is followed. In storm surge assessment, the five methods are discussed. One of these, using design storm events to represent the true hazard, is found to sometimes omit important storm events, thereby not capturing the variability of a regional hurricane hazard. The joint probability method (JPM) steers clear of the errors inherent to using historical storms, but is requires high levels of computation. In response, the joint probability method with optimal sampling (JPM-OS) was created to reduce the number of storm simulations such that storm surges could be estimated for each event."}, {"section_title": "Surge model", "text": "Storm surge is defined as an abnormal rise of water precipitated by both low pressure and wind speeds of a hurricane moving over shallow coastal waters (Figure 3). Despite the inherent dangers of high wind speeds and torrential rainfall, storm surges are the most costly and deadly consequences of hurricane events for coastal developments (Pielke and Pielke, 1997). The risk of hurricanes is often all too dependent on the value of the Saffir-Simpson hurricane category; this, however, does not correlate well to the risk of storm surge (Pei, 2012). Hurricanes grouped in the same category can cause dramatically different storm surges. When considering the growing risks of climate change, the increase in severity and frequency of hurricanes renders the accurate prediction of storm surge heights more important than ever. Simpson (1981) published a table which correlates storm parameters, such as wind, storm surge height, and damage, to the Saffir-Simpson Category number (Table 1). Although the Saffir-Simpson hurricane scale is useful to categorize storm intensity for some parameters, it does not correlate well to the severity of storm surges. In response, there are two main platforms used to calculate storm surge, Sea, Lake, and Overland Surge Heights (SLOSH) and the ADvanced CIRCultation model (ADCIRC). SLOSH basins (in the U.S. Eastern Coast and the Gulf of Mexico) are low resolution grids (~1 km) on which storm surge predictions are made using a simplified parametric wind field model. Other storm surge models provide greater resolution, such as ADCIRC (~100 km), which models with a wind field that uses a dynamic atmospheric model to fully account for the physical processes that cause storm surges. Both have advantages and disadvantages, the most prominent of which is computation power required. Because of the slight approximations in modeling physical phenomenon and its coarser grid, the SLOSH program runs much faster than the ADCIRC model (Jelesnianski et al., 1992). It is thus more useful for analysis of large sets of hurricanes and storms. It should also be noted that SLOSH does not account for astronomical tides or waves, but that ADCIRC can approximate both. However, neither ADCIRC nor SLOSH can account for rainfall runoff or specific sea level rise scenarios, although the mean sea level can be adjusted in ADCIRC (Lin et al., 2012). Normally, the maximum storm surge occurs before the high of river flooding does. While the levees and barriers protecting some flooded areas may suffer structural damage from surges, this possibility is also not considered in SLOSH or ADCIRC. Because the focus of the study is transportation consequences not necessarily storm modeling, SLOSH is relied on for adequate predictions at a fraction of the computational power required by ADCIRC. To model the flooding and inundation due to storm surges, SLOSH uses a bathtub model, which is a simple elevation model that can somtimes inaccurate and overestimate flooding values (Murdukhayeva et al., 2013). However, its simplicity cuts down computation time and SLOSH results are estimated to be within 20% of the true storm surge; errors are largely due to incorrect track and intensity inputs (Jelesnianski et al., 1992;Aggarwal, 2004, Blain, 1997. The most accurate SLOSH-generated storm surge predictions are the storm surges over 12 feet (Glahn et al., 2009), which works to benefit the analysis in this particular study. Clearly, infrastructure systems experience most hurricane-related damage from high storm surges, so these are the levels of most interest. The computational speed of SLOSH offsets the reasonable error, especially in regards to this specific study. In addition to the SLOSH display program, NOAA offers a probabilistic SLOSH analysis interface called P-surge. P-surge is available in the hours leading up to an anticipated hurricane event. By assuming a normal distribution, the average along-track, across-track and maximum wind speed error distributions can be found. The error distribution of the radius of maximum winds is calculated by using the wind field model under the assumption of a constant pressure. Although P-surge does not currently include data to determine structural design criteria for wind, surges, and waves, this may be a reasonable part of a future version of P-surge. The importance of real, usable data for structural engineers is highly relevant to researchers working to predict such hazards. It should be noted that P-surge was able to provide accurate data during the approach of Hurricane Katrina, but its limited availability renders it inapplicable to the current study. Pei 2012includes both storm surge modeling and wind field modeling in investigating hurricane storm surge simulation. Hurricane wind field modeling used the HURDAT database in parallel with a wind field model to generate wind velocities and pressure values. These were included in the storm surge model through meteorological parameters and combined with a finite element mesh and boundary conditions. The storm surge model presented in Pei 2012generated water elevations. As most storm surge investigations do, Pei (2012) employs the use of SLOSH and ADCIRC models. Their hurricane wind field model is based off of the discrete time model of Georgiou (1985) then applied to the finite element grid of the ADCIRC model. Parameters, the hurricane track, translational speed, heading angle, and central pressure, available through HURDAT from historical storms are collected with values in six-hour intervals. ADCIRC then linearly interpolates the input time histories, converting them to 15minute intervals. The resulting simulated water elevations are compared to those measured during historic hurricanes and, although the differences are not significant, there were definite systemic and random errors. As in Pei (2012), Lin et al. (2012) uses both ADCIRC and SLOSH models. ADCIRC requires surface wind estimations, projected by employing an analytical hurricane wind profile along with other added parameters. SLOSH is used first, because of its computation efficiency in analyzing large sets of storms, to identify the most hazardous surge events. The probability density function of surge events is then calculated by statistical analysis. ADCIRC is used to refine the nine highest surge-generating storms for in-depth probability distributions. The ADCIRC wind inputs are those generated by the SLOSH wind field model and the pressure field model is calculated using a Holland pressure distribution. The ADCIRC analysis matched closely to SLOSH with a few exceptions that are most likely due to its coarser resolution grid, showing that SLOSH is acceptable for an NYC-focused analysis if storm surge is the controlling parameter of the risk assessment."}, {"section_title": "Effects of Climate Change and Its Consequences", "text": "The undeniable occurrence of climate change is attributed to a number of factors, but the correlation between increasing temperatures, the melting of the polar ice caps, and rapidly changing climatic patterns is thoroughly reported by the IPCC (2014). Design of civil infrastructure is based on an unchanging set of weather and climate patterns of the past which, scientists have shown, are not necessarily representative of future events (Wright et al., 2013). This motivates an investigation which relies on future climate change predictions, whether they are of carbon dioxide levels, precipitation rates or temperature changes. While weather (shortterm variations) and climate (average weather conditions) are important, we will focus on the resilience of extreme events, which are weather events that are rare at a particular place and time of year. By considering the available data of climate change, the system vulnerability is more accurate and useful. However, the uncertainties associated with these changing conditions must also be noted. Specifically, the growing number of natural hazards attributed to climate change will be of interest, especially those that affect coastal urban environments. These include: rising sea level, increased intensity and frequency of storms, flooding, corrosion, erosion and increased vulnerability. Coastal cities are notable urban systems off of which to base resilience models because of their high population density, economic importance, social connectedness and exposure to coastal weather patterns (i.e. hurricanes, storms and flooding). As predicting the climate changes and atmospheric carbon dioxide levels in the future is highly uncertain, studies evaluating specific cases of climate-induced hazards modeled different greenhouse gas emission scenarios to encompass possible future cases. Global climate change models have predicted varying degrees of changes in maximum daily precipitation, therefore affecting peak flow rates for thousands of watersheds. These conditions create more severe environments for bridges, which will exhibit different scour vulnerability depending on the climate. As eastern coastal communities are more susceptible to hurricanes and storm surges, their bridges are significantly more vulnerable to scour as climate conditions worsen, possibly necessitating major repairs such as raising bridge heights and designing structures for higher water levels and flooding. It is vital to consider scour when estimating the service life of a bridge and, thus, when evaluating the resilience of a critical infrastructure system that includes bridges. In a study by Wright et al. 2012 billion (Bjarnadottir, 2011). These numbers reflect not only the direct costs of immediate flooding and emergency response but the indirect costs such as bridge repairs and infrastructure clean-up. The intensities of such hurricanes are projected to worsen with the irrefutable changing climate, necessitating an even greater focus on preparing coastal communities for climatic events (Emanuel, 2006). was employed along the RCP 8.5 projection. From the simulations, monthly average SST values were gathered and stored for hurricane simulations. The annual occurrence rate of hurricanes in the Atlantic basin is found to be 13.9, larger than the 2005 occurrence rate of 8.4 by more than 5 storms ( Figure 4). This was calculated after following a Poisson process to simulate hurricane events, for which thousands of years of storms were simulated to construct a database such that probabilities of exceedance could be extracted. Further proving the theory that changing climate conditions will affect hurricane intensity and occurrence, Mudd et al. (2014) underlines the need to prepare coastal communities for storm hazards. The combined effects of melting ice caps and thermal expansion of ocean water are gradually causing the rise of global sea levels. Relative sea level describes the local measurements and depends also on regional properties and activities such as uplift, tectonic plate movement, and geologic processes. The current rate of rising sea levels is due to both land subsidence in the Atlantic Ocean and the increase in global average temperature (IPCC, 2014). This has deleterious consequences for, among many areas, coastal developments, wetlands and any structures along the waterfront. Rising sea levels can cause a number of events such as inundation, increased salt concentrations in groundwater, more forceful waves and erosion of structures and roads. Of interest to this study is Kirshen et al. (2008a), which presents a study of the impacts of climate change along the coast of Boston and its surrounding areas along with suggested adaptation strategies. These are categorized into three types of responses: protection, accommodation, and retreat, to be determined in a site-specific manner. Boston was chosen because of its coastal exposure, densely populated areas, and an aging infrastructure that is struggling to meet the growing transit demands of the area. The results of investigating two possible sea level rise scenarios-relative SLRs of 0.6 and 1.0 m by 2100-was shown to affect the storm risk assessment. It was also shown that, by 2100, an increase in total SLR of 0.6 m causes the 10-year storm elevation to reach what is now considered the 100-year storm elevation. SLR is not modeled in this study, but the possibility that it may change a northeastern storm risk assessment is relevant to the importance of analyzing the effects of hurricanes on coastal communities. As hurricane surge levels are dependent on the regional sea level, it is reasonable to suggest that incorporating the climate effect of sea level rise is significant, but complex. Emanuel et al. (2006) simulated hurricane tracks based on best-track tropical storm data from the National Hurricane Center (NHC) to evaluate storm surges in the New York City area. In order to investigate the stress imposed by climate change, sea level rise was manually incorporated into the storm surge projections by changing the mean sea level in ADCIRC. Lin et al. (2012) chose four climate models to predict climate conditions for the New York region, leaving a wide range of future storm predictions that attest to the uncertainty inherent in climate change models. Lin et al. (2012) is the first study to model surge events under projected climates, examining how SLR conditions affect hurricane properties and patterns, instead of focusing on the effects of SLR on flooding. A Global Climate Model (GCM) and a statistical/deterministic hurricane model with hydrodynamic surge models were combined to simulate storm surges. It is assumed in this investigation that storm surge height is determined both by the properties of the storm and by the physical properties of the coast. Considering astronomical tides and SLR was a motivation as SLOSH does not account for either. The study showed that the combined effects of SLR and storm surge heights can drastically change both hazard assessments and storm occurrence intervals. While SLR scenarios are beyond the scope of this project, understanding their projections emphasizes the importance of hurricanes storm surges as a growing and worsening hazard."}, {"section_title": "Climate Change and Transportation", "text": "Because of the long-term analysis and quantity of data needed, relatively little information is known regarding how climate-related extreme weather events affect transportation networks as systems. While researchers have focused efforts on mitigating the climate impact from transportation networks (National Research Council Panel, 2010;Dedinec et al., 2013;Sperling and Cannon, 2007), other, more recent research argues that these efforts should partially be redirected into adapting to the impacts of climate change (Peet, 2014;Cooper and Pile, 2013;Wall and Meyer, 2013). There is enough evidence to safely assume that some effects of climate cannot be completely prevented at this point. Towards this motivation, attention must be directed to assessing the impacts of climate change on current transportation systems and the future hazards of climatic events. Flooding events are costly for the government; any increase in their damage may significantly strain the federal budget. Coastal urban flooding was analyzed as a cause of increased travel time and trip cancellations in the metropolitan area of Boston (Suarez et al., 2005). As a case study, the Boston Metro area is chosen because of its dense transportation network and coastal location. Massachusetts, specifically, is at a higher risk of flooding on the coasts and near rivers because of its extensive coastline, river, and stream network as well as its highly developed neighborhoods. Suarez et al. (2005) describe their modeling approach, first by outlining possible reasons for transportation disruptions: cancellations because either endpoint of a trip is flooded, cancellations because a link in the trip is flooded, or delays because of altered routes or congestion. They used the Urban Transportation Modelling System (UTMS) to simulate the flow of metro road traffic under different situations. First, the model is run to gather reference data in normal circumstances. Then it is run with links and nodes \"flooded\" or marked unusable. Their process of assuming flooding links and nodes unusable is replicated in this study of New York City. The results, which found a doubling in delays and cancellations, were interesting but largely specific to the Boston metropolitan area. Similar to the preface made by Suarez et al. (2005), Jaroszweski et al. (2010) noted that, like delays and cancellations, traffic accidents can cause significant financial and economic losses for a government. Also noted is the undeniably strong relationship between weather and transport, which of all industries, is constantly subjected to and severely affected by the weather conditions. While some may argue that the effects of climate change do not need to be considered for current structures, Jaroszweski et al. (2010) notes that the impacts of climate change are well within the lifetime of newly-built or recently-constructed structures. Based on the climate-related weather changes outlined in Peterson et al. (2008), Jaroszweski et al. (2010) drew each climate effect to its specific impact in transport. As an example, increased heavy precipitation brought about by climate change would have four main effects on transportation: road submersion and underpass flooding; increased landslides and undercutting; poor visibility; exceedance of the existing 100-year flood. The presented concise table is an eye-opening, direct way to examine the effects of climate change on transportation and serves to validate ongoing research into this sector. But perhaps the most interesting objective of Jaroszweski et al. (2010) was to approach the effects of climate change with the knowledge that the future transportation network will be different than it is now. Similar studies, such as Mills and Andrey (2002), account for a changing climate but not an altered transportation system nor new technology. Jaroszweski et al. (2010) postulated that there will also be changes in transportation patterns, both human and freight. Although the exact future situation cannot clearly be predicted, scenarios can be used to account for the uncertainty in socio-economic pathways. The analysis process is then repeated for a range of socio-economic scenarios. Two specific scenarios are outlined: a \"World Markets regime\" where societal values are centered around consumption, leading to a greater demand on transport but also more resources dedicated to bettering the efficiency; a \"Global Sustainability\" society where the community values sustainability and energy efficiency which increases the cost of transport. Using scenarios alongside climate change predictions is a viable and more accurate method to create realistic modeling conditions. Koetse and Rietveld (2009) published an overview of current empirical findings on the effects of climate change and weather conditions on transport. Traffic disruptions and road conditions of transportation systems, highly affected by the climate and weather, will worsen parallel to the changes in climate. Many of these predictions were played out in reality when Hurricane Sandy hit the eastern coast in 2012. The corresponding instances of inundation and vehicle delays were caused by hazardous storm surges from the coast. Of particular interest and uniqueness is this study was the concentration on the shifts that would occur in both tourism and agricultural production as a result of increasing temperatures. These shifts would affect transportation by similarly shifting passenger and freight transport. By accepting that temperatures will rise in Northern communities, which account for the majority of travelers, fewer people will be persuaded to migrate south. Regarding the freight sector of transportation, it follows that farming will be less suited for countries at low longitudes. Both of these climaterelated effects will alter the transportation industry and demands. Additionally, Koetse and Rietveld (2009) note the increase in the frequency (with a decrease in the severity) of automobile accidents that would occur as a result of increased precipitation. While uncertainty is the largest regarding predicting changes in precipitation patterns and severity, it is nonetheless important to consider. Communities on the East Coast are the most at-risk regions for the effects of sea level rise and flooding. Here, even small network disruptions have large consequences, economic and social. As temperatures inland increase more severely than those in coastal areas, rivers and waterways vital to inland transportation will undergo lowering water levels. Thus, economic losses will be experienced as shipping costs are increased for more difficult and risky traveling. Despite this comprehensive overview of the effects climate change may have on transportation, Koetse and Rietveld (2009) conclude that the impacts are ambiguous and region-specific, a relatively common denominator in similar investigations."}, {"section_title": "Hazard studies in New York City", "text": "A case study of the New York City metropolitan area by Rosenzweig and Solecki (2001) is dated but remains relevant to this investigation. Rosenzweig and Solecki (2001)  Higher average temperatures, a greater number of heat waves in the summer, increasing sea levels, shorter recurrence intervals for severe storms and more frequent droughts and flooding were all expected effects in New York City. The MEC Regional Assessment studied the effects of climate change for seven sectors: coasts, infrastructure, wetlands, water supply, public health, energy demand and institutional decision making (Rosenzweig and Solecki, 2001). To estimate the general change in temperature, GCMs were used before interpolating specifically for New York City. Along with the baseline costs, increased stress on energy resources, urban infrastructure, and utilities were found to magnify any damage resulting from climate change. As a city situated on the coast, SLR is of clear interest. Rising sea levels are hazardous to urban and developed areas for a plethora of reasons, the most costly and disastrous of which were determined to be the resulting heightened storm surges. The longstanding approach to placing unappealing but necessary infrastructure (airports, railways, highways and water quality treatment facilities) near waterways could backfire; in the event of coastal storm surges, repair and recovery would be even more costly and, for water quality treatment facilities, the risk of salt-water intrusion is significant. In addition to being a vital regional ecosystem, wetlands were cited as providing protection to developed coastal areas; SLR would accelerate the loss of crucial wetlands which are already endangered by erosion and development. These risks were described in detail and their potential effects briefly explored in Rosenweig and Solecki (2001), which pointed further research to new focuses. As part of a holistic risk assessment of the New York City transportation system focusing on climate change, ClimAid outlined three goals of their work: to collect the frequency of historic storm surge events in New York City and to identify a hazard distribution regarding the maximum daily and yearly storm surges; second, to predict storm surges based on a probabilistic model; third, to assess infrastructure based on reliability (ClimAid, 2010). In Hwang (2013), once past storm tide data is collected, the data was analyzed to determine a probability distribution of storm surges and, eventually, a risk assessment for uncertain future events was developed. Because the Saffir-Simpson scale is primarily based on wind strength, the intensity of hurricanes causing the analyzed storm surges was measured in an alternative way. A region consisting of an 80-mile radius around NYC was selected to be considered in the study. The four critical historical hurricanes, for which tide and surge data was specifically examined, were determined to be Donna (1960), Gloria (1985), Irene (2011), Sandy (2012). Hurricane Donna had the highest recorded surge with 13.3 feet occurring in 10 hours until Hurricane Sandy hitits storm surge was recorded at 17.3 feet in 14 hours. For tide reference levels, four points of observation were placed in New York and six in New Jersey. In the analysis of the maximum daily water heights in NYC, Hwang (2013) found a linearly increasing trend of daily water heights with an 8.1 feet mean value. In order to accurately predict future daily maximum water heights, a linear regression is used for a basis. The corresponding statistical analysis on the daily water height data showed that the standard deviation is actually constant across about 90 years of water heights, but that the mean value is linearly increasing. To find the parent probability distribution, which was determined to be a logistic distribution, goodness-of-fit testing was performed on maximum daily water height data. A similar process was performed on the maximum yearly storm surge data for NYC, which resulted in a determination of a generalized extreme value parent distribution. To develop distributions of maximum storm surges in a specified period of time, the extreme value theory, a principle useful in the risk assessment of extreme events, and the effect of including Hurricane Sandy tides in the dataset, is more closely studied. Two cases for maximum water height analysis are considered: one that includes Hurricane Sandy data and one that excludes it. The resulting storm surges for specific return periods are significantly different (i.e. a 50-year storm has a storm surge of 12.68 if Sandy data is excluded and 13.38 feet if Sandy data is included) (Hwang, 2013). From this, Hurricane Sandy is concluded to be a singular event. For comparison, the data from Hwang (2013) is compared to that from Lin et al. (2010), which utilized SLOSH-generated surges coupled with a Pareto distribution. The storm surges for each return period were comparable, and the Hwang (2013) empirical data excluding Sandy matched the synthetic data from Lin et al. (2010) more closely. During Hurricane Sandy, seven subway tunnels positioned below the East River were flooded. Motivated by the widespread and devastating flooding in the subway network of NYC, Hwang (2013) chose to concentrate on the potential applications of storm surge predicting to subway stations. Subway flooding can be due to both storm surges and heavy rainfall, and Hwang (2013) traced the path of water vertically through grates and stairwells. The time history of a storm surge is modeled, modifying the equation to match the generated general extreme value distribution for peak storm surge height; 100,000 time histories were simulated. These time histories were combined with underground infrastructure dimensions to estimate the total volume of water flooding underground. To perform a stochastic analysis of vulnerability, iterations of storm surges were simulated varying the return period and elevation of infrastructure to generate random peak heights. The time history of storm surges was generated and the volume of water calculated. All of this data was applied to find exceedance and fragility curves, which predict the probability of reaching or passing various damage states, after 1 million simulations (Hwang, 2013). These can be used to direct resources to appropriate subway stations and gauge the risks for New York City."}, {"section_title": "Hazard characterization in this thesis", "text": "The New York City metropolitan area is chosen as a case study for its dense urban population, highly developed infrastructure, and its criticality to the national economy. Because of the shared dependence on the road system in New York City, especially by out-of-city and out-of-state commuters, the highway network will be modeled and analyzed. The New York City metropolitan area is prone to typical hazards characteristic of communities along the northeastern Atlantic coast: hurricanes, high winds, flooding, nor'easter storms, snow, high humidity, heat waves, among other events of climatic sources. The occurrence of Hurricanes Sandy and Irene has brought the vulnerability of New York City to such hazards to national attention, motivating the concentration in this thesis on hurricanes. In these two recent events, storm surges were responsible for millions of dollars of damages. With the knowledge of potential projections of sea level rise, the importance of examining the current hazard of hurricane storm surge is high. - While scenarios of climate change will not be directly applied to this analysis, in future studies, this could increase the value of such analyses to communities. Along those lines, the correlation between rising SSTs and hurricane power dissipation reflects the possibility of more frequent and intense hurricanes (IPCC, 2014). Storm surges will be characterized using the MEOW enveloped scenarios in the SLOSH modeling program. In contrast to alternative storm surge models like ADCIRC, the simplifications and grid resolution allow for more efficient computations and analysis. As resources continue to be directed towards improving the resilience of New York City to future events, the results of this hazard characterization can be used to more efficiently improve highway infrastructures.    "}, {"section_title": "CHAPTER IV", "text": "NETWORKS AND NETWORK SCIENCE"}, {"section_title": "Fundamental concepts of graph theory", "text": ""}, {"section_title": "Definitions and terms", "text": "Knowledge of the most fundamental concepts of graph theory is necessary to investigate the topological parameters of a network. For this reason, an overview of essential terms and definitions in graph theory will be discussed. Graphs are composed of two types of elements-links (edges) and nodes (vertices)-and can be uniquely defined by the set of links and nodes they contain. Networks and graphs are two different terms to describe the same concept, although networks are usually examples of graph systems that exist in reality. The connections and locations of nodes and links determine the majority of identifying properties of the graph. A network is a wide-inclusive term; it can include links and nodes of different types, weights and (for links) directions. Directed graphs-digraphs-can be cyclic, with closed loops, or acyclic, without closed loops. Hypergraphs are graphs with hyperlinks, or links that connect more than two nodes. Bipartite graphs contain two different types of nodes and, generally, links connect only nodes of the same type (Figure 7). Several properties of graphs are actually graphs in themselves-collections of links and/or nodes of the main graph. These, or any defined subsets of graph elements within a larger graph, are called subgraphs ( Figure 6). In highway network studies, properties of a path between two nodes are of special importance. A path is an ordered sequence of links necessary to travel from the origin node to the destination node; the length of a path is the number of links it includes. Distance is expressed as a length, but it is always the length of the shortest path between two nodes. If there is no path connecting two nodes, the distance is defined as infinite. Topological graph theory is based on the physical layout, or structure, of the graph and relies on several measures of connectivity. Topological graph parameters are sometimes combined with properties measuring network flow and referred to as topology-flow effects. Connectivity, a reflection of the adjacency and ease of flow between nodes via links, is closely tied with the intra-dependencies of the network. There are various levels of connectivity, but the highest exists is that of a complete graph, where for every pair of nodes there exists a link between them. For a graph to be connected at all, at least one pair of its nodes must be linked by a path. When evaluating the response of a network to a node or link failure, the interruption of flow or failure of paths is contingent on the connectivity. If a link fails, flow may still be possible between two nodes if there are other available paths along connected nodes. Thus maintaining a certain level of connectivity is necessary to preserve the functionality, or operability, of the network. Node degree is a network measure which compares the number of nodes to the number of links per node. Average nodal degree is a quantification of network connectivity, taking the mean of the number of links connected to each node. For one node, the degree, d(v) is defined as: where |E(v)| is the number of links at node v. The average degree of the network is simply the total number of degrees in the network summed over all nodes, divided by the number of nodes in the network. Topological graph characterizations often rely on finding the law followed by the degree distribution. These rely on p k , the fraction of nodes that have degree k and the probability that an arbitrary node has degree k. By compiling the node degrees into a histogram, a plot of the degree distribution can be created (Figure 8). A histogram can lose its analytical value for large networks, so illustrating the degree distribution can be modified by creating exponentially increasing bin sizes for the histogram or by mapping a cumulative distribution function. More inclusive bin sizes reduce the noise prevalent in a large-network histogram while the cumulative distribution function simply shows the probability that the degree is greater than or equal to k. Power-law and exponential distributions are simple to identify if their cumulative distributions are plotted on log or semi-log scales. It is important to note that directed networks essentially have two sets of degrees, the in-degree and out-degree, which must both be considered in finding the degree distribution for the network. Likewise, bipartite graphs with two types of nodes and links running only between nodes of the same type have a degree distribution for each node type (Newman, 2003). The maximum degree value is generally dependent on the size of the network and can be found through different manipulations on the cumulative degree distribution.  Of course, often clusters are not random groups of nodes but instead predictable based on certain network element characteristics. In social networks, for example, assortative mixing, or homophily, is very apparent by race, age and income. Cluster analysis of social networks involves assigning a connection strength value to pairs of nodes (which represent persons) and adding links in decreasing strength order. From here, the nodes and links are separated into communities (Barabasi et al., 2012). The shortest path is especially relevant to studies of transportation systems. The flow of a highway or road network is the traffic which moves along its links, dependent on the trips taken by vehicles. Likewise, trips are dependent on origin-destination data as well as the shortest path between the origin and destination pairs. There are several methods for calculating the shortest path between two nodes, but most revolve around a similar basic procedure."}, {"section_title": "(4.4)", "text": "The variable n in Equation 4.4 is the number of nodes in the network. Starting at one node, i, and all other nodes marked as unoccupied, the distances to every neighboring node by a link is marked. The algorithm moves to this set of neighboring nodes and considers the distances to the neighbors of the neighbors, marking each with a new distance. If a node can be reached with a shorter path, its marked distance is replaced with a smaller number. The \"shortest\" path is eventually found as all nodes of the network are evaluated to get from node i to node j. Betweenness centrality is used to identify the most important, critical nodes to the network flow and is a measure of the centrality of a node. It is defined as: where g (i,v,j) is the number of shortest paths from node i to node j that pass through v. The denominator within the summation represents the total number of shortest path from i to j. The longest distance from any node of a central node is that which is as small as possible. The betweenness centrality quantity can be compared for all nodes in the network to decide where to ( 1 direct resources and make resourcefulness decisions. It essentially provides a measure from which to rank the network links and nodes according to their roles in topology and flow. By depending on the betweenness centrality quantity to rank nodes instead of other measures, such as nodal degree, the possibility of missing important nodes that may form vital bridges between peninsulas or interstates is avoided. The ability to rank network elements is the first step in predicting response to the removal or failure of elements."}, {"section_title": "Random graphs", "text": "The development of the Erdos-Renyi random graph ( Figure 10) was a turning point in graph theory science. Undirected with a fixed number of nodes, the random graph is one of the fundamental graphs analyzed for simulations or properties. The existence of a specific node is dependent on a probability p. The degree distribution is binomial or, if the number of nodes n is large, follows a Poisson distribution. The study of random graphs often involves creating a random graph in stages of increasing p values. A specific graph property is tracked and the corresponding p value at its phase transition is the concern of research. It should be noted that there are different modified clustering coefficients in a random graph, where the clustering coefficient is equal to the probability that two nodes are adjacent, p. In most real networks, however, the clustering coefficient is much higher. Random graphs can be modified to be more realistic, which is often done to create a base model for a network, but these generalized random graphs lack the transitivity property. Incorporating transitivity into random graphs is difficult because of the presence of loops, which is another barrier in using them to model complex networks. At first, random graphs were used to study the science of complex networks because such networks had no clear organizing pattern and the links seemed to be randomly distributed. Unfortunately, complex real world networks are not similar to random graphs. Many of the graph properties studied in real networks were not apparent in equal form in random graphs. The smallworld effect, however, is one of the few properties of real world networks reproducible in random networks and describes the theory that most pairs of network nodes are connected by a relatively short path. The small-world model is used to investigate various path processes such as percolation. In calculating the average shortest path for the network, nodes which are not connected by any path are excluded from the mean. Networks exhibit the small-world effect if the value of the average shortest path scales logarithmically or slower with network size for a specific average degree (Newman, 2003). Albert and Barabasi (2002) prove that the average shortest path is similar for real complex networks and random networks with the same number of nodes. Although critical to understanding the basis of many graph properties, the emergence of random graph theory was more relevant to the development of probabilistic mathematics to prove graph properties. Complex graph theory necessitated further types of analysis (Albert and Barabasi, 2002), which are described in Section 4.2.3. In a random network, the probability that a link exists is equal for all links; this is an example of a binomial degree distribution. The degree distributions for most real world networks, however, are highly right-skewed. Because the degree distribution of the small-world model does not match those of real world models, it is problematic for those using a random network to simulate realistic conditions. In fact, empirical data shows that most real systems have nodes that are abnormally highly connected with the vast majority of nodes having only a few connections (Duenas-Osorio, 2005). The exceptions to this observation are power and transportation networks, which exhibit exponential tails in the degree distributions."}, {"section_title": "Studies of complex networks", "text": "The dramatic increase in the size and complexity in networks which were desirable to analyze, such as the Internet (Figure 11), necessitated a change in traditional analytic methods. Graph theory, up until recently, was mainly concerned with the study of traditional networks, or networks where nodes all had similar levels of connectivity and degrees. Real networks, however, did not follow this pattern and, as exhibited by the Internet network, exhibited hubsnodes of extremely high connectivity-and nodes with minimal connectivity. The difference in degree distributions between traditional and real networks is sometimes referred to as homogeneous and heterogeneous graphs, where homogeneous graphs have a uniform degree distribution and heterogeneous graphs have a power law degree distribution. Traditional graph theory has since grown to the study of complex networks to incorporate dynamic properties, irregular topology and to allow the comparison of networks from separate fields of study (Boccaletti et al., 2006). Additionally, common methods of observing the graphics of a network quickly became useless to large, expansive networks. The analysis of complex networks necessitates a different set of properties and characteristics than those which describe traditional graphs. Accurately modeling network topology in complex networks is a challenge but has been shown, in several studies, to lead to more accurate determination of network properties. Network topology is vital to complex networks mainly because it determines dynamic properties, such as the network response to failure (Boccaletti et al., 2006). Complex networks exhibit a slightly different topology than traditional networks. Because of the size of several complex networks, the graph can sometimes be described as sparse if K, the total number of degrees, is tiny in comparison to the total number of nodes, N, to the second power: K << N 2 . Alternatively, it can be dense if both K and N 2 are of the same order of magnitude. A component is a maximally connected induced subgraph of the network and is often referred to in analyzing failures and attacks on the network. In some graph models, a giant component may form which has a size the same order of magnitude as N. Like traditional networks, the degree distribution determined the properties of complex networks; more specifically, the degree distribution determines the properties of uncorrelated networks (Boccaletti et al., 2006). In the real world, highly researched networks mostly fall into four categories: social, information, technological, and biological. Most models try to incorporate observed properties of real world networks into an artificial model. However, as pointed out by Newman (2003), it would be more insightful to learn how networks grow to have these properties. One of the most common examples (and earliest used example) of a network is the citation network of papers (Price, 1965). This is a directed network and a link connects a cited paper to the paper which cites it. It cannot be cyclic, that is, papers which are already written cannot cite papers that have cited it. Quite quickly, the in-and out-degree distributions of the paper citation network were found to follow power laws. The most studied networks are those with highly skewed degree distributions. The challenge is thus to explain how such skews develop in real networks. Without significant computation equipment or analyses, Price (1965) explained the power-law degree distributions he found by preferential attachment. This was followed up by an investigation on a modified, undirected model but still assuming linear preferential attachment . It was shown that the probability a link will exist between an added node j and an existing node i is linearly proportional to the degree of i. Furthermore, the number of links added with each new node is a fixed quantity. The first model which successfully reproduces the power-law distribution was a scalefree network. Scale-free is a term used to describe the graph that has the same functional form at all scales. It references the mathematical concept of function scaling and is expressed accordingly: f(ax) = bf(x). There is a strong desire to create networks with a power law distribution because of its appearance in real world networks. Real networks are not homogenous and can be classified as scale-free networks. Networks of biology, sociology and technology are more often heterogeneous with small paths and high local clustering. Studies have shown that power grids, highway, social, and cellular networks are more accurately modeled by small world networks and scale-free networks (Cimellaro et al., 2006). Evolving scale-free networks (Boccaletti et al., 2006) were developed to incorporate dynamic network properties to the static scale-free network. Although highway networks are not dynamic, there are many networks which have dynamically evolving structures. The Barabasi-Albert model was modeled after the development of the World Wide Web (WWW). On the WWW, it is much more likely a web page will contain a link to a popular page than to a little-known, rarely visited site. The conclusion follows that popular web pages are more attractive. Translated to graph theory, nodes prefer to connect with nodes of high degrees.  observed that highly connected websites acquire new links at higher rates than low-degree networks. Generalizations of the Barabasi-Albert model have been developed to make it more realistic (i.e. non-linear preferential attachment, allowing the mean degree to change over time and adding and removing links). In one modification, Krapivsky and Redner (2001) considered a directed version of Barabasi-Albert model, which turns out to have dramatically different properties than the original model. Gastner and Newman (2004) investigated the topological structure of three complex networks: the highway system, the Internet and flight network. All of these networks exhibit a preference to shorter links. Especially relevant to this study is their study of the highway network, applying shorter links, lower degrees and a larger average shortest to the model. These properties arise mostly because the highway network is restricted to the surface of the earth (it is planar), which limits the shortest paths that it can have."}, {"section_title": "Resilience measurement using network properties", "text": "Resilience is characterized by a variety of factors which can be quantified by the parameters described in the previous section. Nodal and average degrees, which allow the determination of degree distribution, are crucial to the vulnerability of a network to nodal or link failure. The degree distribution across all nodes relates to the network robustness, or how well the network absorbs a shock without failure in service or functionality after an extreme event. By categorizing the degree distribution, specifically as homogeneous or heterogeneous, the level of damage after a catastrophic event can be estimated. Separately, the algorithm to calculate nodal degrees is often the first step in the calculations of other parameters, such as betweenness. The betweenness centrality measure underscores the importance of nodes relative to links in a network and becomes central to a discussion on network resilience. The removal of a node potentially affects several links, certainly all the links of which it is an end. The removal of a link, however, only affects the functionality of one link. Therefore, the failure of a network node is more harmful to network performance than the failure of a link. By ranking nodes based on betweenness centrality, the resilience of a network to extreme events can be quantified. Events that result in the failure of important nodes are clearly more devastating than those on nodes of lower importance. Real networks are usually composed of several smaller networks, called clusters. Parameters specific to each cluster provide revealing information of the resilience and redundancy of a network, especially locally. In transportation networks, evaluating clusters allows for the efficient concentration of improvements and funds to the most needy network areas. The clustering coefficient is a measure of connectivityspecifically, how connected the network is locally. This is sometimes referred to as the \"regularity\" or \"locality\" of the network. For a grid-like network, a large clustering coefficient corresponds to a high average path length. In fact, however, most real networks have high clustering coefficients but low average path lengths. With regard to the factors of resilience, the clustering coefficient is best representative of redundancy. In preparing a network for disaster, policy makers would be better equipped to make decisions about the locations of resources and where more improvements could be made. With measures of graph parameters contextualized by factors relevant to resilience, the resilience of the network is better understood. Holme et al. (2002) brought about an important point regarding network topology that challenged the accepted significance of node degree with respect to resilience measurement. While nodes with the highest degrees are vital, nodes with small degrees may connect two important clusters of a network, acting as bridges. These should not be overlooked in evaluating resilience and how node removal will affect the average shortest path value. Especially relevant for targeted attacks, node betweenness provides a measurement on which nodes can be ranked by importance in the network. From this logic, the relevance of measures of betweenness centrality has evolved. Betweenness centrality, like degree, is specific to each node and depends on the number of shortest paths within the network which travel through the considered node. Goh et al. (2002) showed that the distribution of betweenness centrality in scale-free networks follows a power law. One would expect that the nodes with the most degrees have the highest betweenness. Zio and Sansavini (2011b) employ betweenness measures to the maximum load which can be distributed by a node in a power network, constantly comparing the value to the component capacity. Although, in every network, there is a correlation between node degree and betweenness centrality, there are sometimes low-degree nodes which have the largest betweenness centrality. This is proven by plotting the correlation between degree and betweenness across several network models. Scott et al. (2005) found the same pattern from another similar measure, network robustness index. The greatest difference between network robustness index and capacity is in networks with low connectivity. The difference in ranking nodes based on degree and ranking based on betweenness may explain the difference in response to targeted and random attacks. Initial betweenness-based removal of nodes is IB removal and the recalculated removal is RB removal. Generally, Scott et al. (2005) found attacks based on the nodal degree are local while those based on betweenness are global, resulting in more inefficient algorithms. Redundancy, a critical part of network resiliency, depends on the availability of alternative paths to traverse from one node to another. Duenas-Osorio et al. (2005) uses the number of paths between a node and the neighborhood of its neighborhood to calculate the node's redundancy within a graph: Determined by a fairly complex equation, the redundancy ratio is specific to one node. I(i,j) is the number of paths between nodes i and j which share only i and j as nodes. I(v,j) is thus the number of node-independent paths between nodes v and j. \u0413 v 2 is the neighborhood of all of the nodes in the neighborhood of v. So the number of node-independent paths is taken between the node under consideration and each node in the neighborhood of neighbors. This sum is then divided by the number of independent paths between v and the complete graph connecting v, \u0413 v and \u0413 v 2 . To achieve one redundancy ratio representative of the entire network, the collective set of redundancy ratios for all nodes are ordered and the median is taken as the redundancy ratio for the network. Another method used to measure redundancy is by measuring network connectivity indices. Because these describe the number of cycles, links, nodes, and maximum links, they can reveal information about the redundancy of graph connectivity using simple graph characteristics. There are three indices-alpha, beta, and gamma-which comprise the set of connectivity indices. where, u is the number of independent cycles in the graph, l is the number of links, and n is the number of nodes. The simplicity of these algorithms makes them convenient to calculate for several trials. Instead of human-made networks, a study by Buhl et al. (2004) "}, {"section_title": "Targeted and random removal in graph theory", "text": "Real networks function and grow under what are often less than ideal circumstances and environments, motivating specific studies of real network resilience to removal techniques. Albert et al. (2000) and  attributed this resiliency to the inherent redundancy built into these complex systems and refer to it as error tolerance. When a network exhibits this type of tolerance, however, they are also especially vulnerable to targeted attacks. Node removal was investigated in two real-world networks: Internet topology and its subset of the World Wide Web, both of which have approximately power-law degree distributions . The mean node-node distance was measured as a function of the number of nodes removed. Interestingly, it was concluded that mean distance was unaffected by random node removal but dramatically affected by targeted removal. To further study this observance, Albert et al. (2000) compared the average shortest path between any two network nodes of two network models (Erdos-Renyi and scale-free) as fractions of nodes were removed. First, a random attack was simulated. Because, in a homogenous network like the Erdos-Renyi network, the effect of removing any node is about the same throughout the system, removing each node causes the same amount of failure. Under a targeted attack of the most connected nodes, the Erdos-Renyi model actually exhibited a similar response; the average shortest path of the network changed by a comparable amount. In the scale-free network, on the other hand, this pattern did not hold true. Instead, the network was able to tolerate a number of random removals and its diameter was, for the most part, maintained. Under a targeted attack, the network diameter increased dramatically, illustrating that the connectivity of a scale-free, heterogeneous network is highly dependent on a small number of well-connected nodes. The removal of these nodes also completely changes the network topology. Barrat et al. (2008) explored the resilience and robustness of heterogeneous networks; these types of systems often exhibit high robustness to random attacks but are vulnerable to targeted attacks on the nodes and links most vital to functionality. To model network response to both of these attacks, different node removal techniques are employed and the results are observed and measured. In a simulated targeted attack, the node with the highest degree is removed first, followed by the node with the second highest degree, and so on. It should be noted that as nodes are removed, the initial ranked order may actually shift because the network structure is changing. Choosing to remove nodes based on the initial order or based on this changing, dynamic order will certainly affect the response. This attack is referred to as initial degree distribution removal, or ID removal, whereas removal which follows the recalculated order is recalculated degree removal, or RD removal (Holme et al., 2002). The percolation theory has become a fundamental part of graph theory as it relates to resilience because it is a method to simulate network disturbances. Percolation is a process in which nodes or links are randomly designated \"occupied\" or \"unoccupied\", referring to their ability to function or fail, respectively. Site percolation and bond percolation are the two types of network percolation, where site refers to nodes and bonds indicate links. It first considers a random network where the probability of nodes being occupied is denoted by p and links connect only occupied nodes. Thus the probability of a link being occupied is also p. Most commonly, the percolation theory is demonstrated by a lattice or grid network. As the probability p increases, the number of clusters also increases. There is then a value of probability which is critical to the network, defined as p c , at which there are clusters of all sizes and an infinite cluster. If p<p c , there is no global connectivity and an infinite cluster appears; if p>p c , there is global connectivity and all clusters have a finite size. A percolating cluster is one which spans the entirety of the network by allowing for a path between two extreme nodes. While a series of integrals and summations are required to evaluate p c and other percolation theory parameters, the theoretical basis is significant when studying any type of network resilience. Percolation is strongly tied to the concept of fragility, defined by Duenas-Osorio (2005) to be the susceptibility of a network to fail given a certain level of disruption. Fragility provides a way to connect representative nodes and links to their real-world counterparts with physical and mechanical properties. Robustness of real networks was first reported by Albert et al. (2000), who studied how the properties of the Internet changed when a fraction of nodes were removed randomly and also when specific nodes were targeted. Boccaletti et al. (2006) then discussed static and dynamic robustness in their overview of complex networks. Robustness was, in this later study, defined as the ability of a network to maintain functionality when one or several of its components are disturbed. There were two types of robustness identified: static, when the flow of the network does not need to be redistributed, and dynamic, when the redistribution of flow is necessary. First, static tolerance to attacks was considered in an approach that depends on percolation theory. Cohen et al. (2000) found that, by simulating site percolation in real world networks, there is always a giant component. Using this conclusion, Boccaletti et al. (2006) measure the size and appearance of the giant component to investigate as a function of the probability of percolation. In another portion of the study of static robustness, global and local efficiency were studied for BA and KE scale-free networks. Global and local efficiency values are alternative properties that measure clustering for scale-free networks and consider the harmonic mean of the shortest path length over all network node pairs (Equations 4.13 and 4.14, where N is the total number of nodes and d ij is the shortest path from node i to j). Both local and global efficiencies were found to be unaffected by the random removal of nodes. For a targeted attack, however, both efficiency values rapidly decrease. The results elicit a valid point that efficiency may be better quality to consider as nodes are removed in scale-free networks. Dynamic tolerance to attacks measured the network flow at various points as nodes and links were removed. This agreed with the  finding that scale-free networks are robust to random removal. Zio and Sansavini (2011b) also used percolation theory to study the response of random and scale-free graphs to random and targeted attacks, but it was to set a percolation threshold for simulating cascading failures (Section 4.4.2). Above this designated threshold, cascading failures are assumed to propagate in interdependent systems. Something which would later be confirmed by other studies like Boccaletti et al. (2006) and Zio and Sansavini (2011b), Albert et al. (2000) were the first to have found that, while random attacks produce similar results for both random and scale-free graphs, targeted attacks produce very different results. Although the presence of both resilience to random removal but vulnerability to targeted removal was thought to appear only in scale-free networks, other networks have exhibited the same coincident properties. For example, food-webs, which are not scale-free, hold up well to random node failures but lose function suddenly when nodes are removed by decreasing degree (Dunne et al., 2002). In comparing the size of the largest component to the fraction of nodes disconnected, Buhl et al. (2004) found that the size decreases slowly during random node removal but rather quickly during targeted removal. This differs from similar experiments on random graphs and spanning tree graphs. Results from the ant network simulations also show that robustness increases with the skewedness of the degree distribution."}, {"section_title": "Types of network failures", "text": "There are, of course, different types of failures that can occur in a system. Rinaldi et al. (2001) categorized these into three classes: escalating failures, cascading failures, and common cause failures. Escalating failures can happen when one failure or disturbance in a facility adversely affects the function of another facility (Adachi, 2007). When two network components fail at the same time from disturbance caused by the same event a common cause failure occurs. Perhaps the most destructive failure scenario is that which is referred to as a cascading failure. When a component of a network is damaged or fails, it is sometimes necessary to redistribute the load (or flow) that was carried by the component to undisturbed parts of the network to preserve functionality. This can cause overloading in other components, resulting in more failures, although only the original failed component was directly damaged. The cascade carries on until the load cannot be redistributed. All types of networks are not necessarily at risk for cascading failures; if relationships are cut off in social networks, in one example, this does not call for a redistribution of a friendship or acquaintanceship to a new individual. But cascading failures have gained significant attention in studies of power transmission networks and water distribution systems (e.g., Sansavini, 2011a, 2011c;Shuang et al., 2014) and their effects can be costly and disastrous for systems. Heterogeneous networks with a relatively uniform distribution of flow are shown to be most vulnerable to cascading failure, especially if the failed component is one which carries a high flow (Barabasi et al., 2012). Power networks provide a unique example because their components are generally working at or close to full capacity, meaning that taking on more loading can quickly cause failure. Because of the known heterogeneous nature of transportation networks (Jiang et al., 2005), cascading failures are valuable to consider and evaluate. As the number of links in the network increases, the maximum probability of failure, f \u03bc , (Equation 3.15, where sup notates the supremium of the subset of failed nodes) increases. Like real cascading failures, the failure of simulated networks is shown to depend heavily on the way which the failure spreads through the graph. This is likewise dependent on network topology. However, cascading failures, and other types of propagating network failures, have a strong tie to network flow properties. Shuang et al. (2014), which is a study of cascading failures in water distribution systems, pointed out that, along with topological measures of connectivity and centrality, it is equally important to incorporate measures of network flow such that failures in demand can be pinpointed. For these reasons, cascading network flow is beyond the scope of this purely topological study of highway networks. In future work, simulating and analyzing cascading failure in a transportation network would complement the current study."}, {"section_title": "Graph theory in transportation studies", "text": "The study of road and transportation systems will be the focus of this investigation. Properties relevant to road systems are abundant in graph theory literature, as is the direct study Although it should be noted this is only based on damage. Another measure of distance was introduced to measure minimum network travel distances to measure accessibility changes post-extreme event. Liu and Frangopol (2006) created a method to measure bridge network performance using network connectivity, user satisfaction and structural reliability. Bocchini and Frangopol (2011) defined an index for bridge network functionality assuming that for every closed bridge, there exists at least one route to bypass the closed bridge. Research focusing on the performance of transportation networks at times relies on origin-destination (OD) data to measure the functionality of a network over time. Origin-destination data, although not used in this thesis, is a significant metric in studies of traffic flow. Specifically, the networks used in Bocchini and Frangopol (2011) and Furtado and Alipour (2014a) are constructed according to graph theory with directed links and nodes representing highways and intersections or where highway properties change, respectively. As common practice in transportation studies, each node is both an origin and a destination, thus requiring both sets of travel data in terms of originating and destined vehicles with respect to time. Each link is assigned a \"congestion function\" to quantify the time needed to transverse the section of highway; this value is dependent on a measure of ordinary and critical traffic flow (vehicles/time). Bridges are characterized by their specific links and their damage level, assigned by the random field sample. After the extreme event has been simulated, the capacity of each bridge needs to be modified in accordance with the damage level assigned; each damage level is assumed to have varying reductions in capacity expressed as a percent flow decrease. For each pair of origin-destination nodes, the shortest path is computed. The OD matrix is estimated using a gravitational model; it is assumed that network users will modify their original destinations to those closer if an extreme event has occurred. Each element in the OD matrix ij represents the number of vehicles that travel from i to j in the unit time. In redistribution, new shortest paths are found, along with new times to travel across highway segments, allowing for an updated OD matrix. To measure network performance, total travel time and the Fully Connected Ratio, FCR, if all network nodes are reachable, were employed in addition to the summation of time required by all users to travel from origin to destination. Because of scenarios where the network is disconnected, the total travel time was concluded to be the more realistic measure of network performance. Also expressing resilience as a measure of costs, both indirect and direct, Furtado and Alipour (2014a) combined transportation and regional economic data to analyze the effects of bridge failures. This allowed for a recommendation of repair methods and preparation procedures to improve bridge network resilience (Furtado and Alipour, 2014b). In this particular study, resilience will be measured in terms of network properties. The range and set of four damage states is common throughout hazard analysis literature and is used to evaluate the lifetime resilience of highway networks considering the effect of component aging (Alipour, 2010). The probabilistic results are combined into an average value of damage. This two-dimensional space is reduced to a one-dimensional problem by neglecting the difference in x-and y-distance. The correlation coefficient of damage is calculated knowing the damage state and location. In the random field simulation, the simulated variable is the damage level of the bridges. It follows that the marginal probability and correlation structure must be estimated and, in this investigation, it is assumed that the marginal probability follows a continuous and uniform distribution varying from 0 (no damage) to 4 (collapse). The advantage of the stochastic approach, or simulating damage as a random field, lies in the reduction in computational power. Instead, there are a minimal number of structural fragility analyses required to be run. On the other hand, the disadvantage, which can be great, is that many of the structural properties of the bridges considered are averaged and generalized. If the bridges in the study have vastly different structural characteristics, the use of a single variable field should be modified or reconsidered. Alipour 2010 property also seen in Duenas-Osario (2005). A method to analyze the effects of seismic events on civil infrastructure systems is introduced and considers the functional interaction among infrastructure systems. In assessing the network serviceability, the damage to network components is estimated individually from specific fragility curves. Fragility curves are also employed to compute the probability that a node fails; the probability of a link failing is dependent on the nodes it connects (Duenas-Osario, 2005). Ortiz et al. (2009) explored the resiliency of the freight transportation network. The freight transportation network is made not only of highways, roads, ports and railway infrastructure but also the carriers and shippers that transport goods. Although notably larger and more inclusive than the highway and roadway system, the fundamentals of analyzing resilience are common. Resilience here is defined as the ability to absorb smaller disturbances as well as the ability to quickly return to full functionality after large disasters. The benefits of experiencing small disruptions in a real network are found in identifying where the critical network components seem to be post-event. In this way, DOTs can be better prepared against larger disruptions. Ortiz et al. (2009) first proved the costly nature of both small and large disruptions, which cost not to just directly repair routes and reroute traffic but also ripple through the economy in more subtle ways. The economic importance of the transportation and freight system is undeniable as almost all sectors of the economy depend on it in some way. Resilience of the network is also highly dependent on its redundancy and the availability of alternative paths in the event of a component failure or close. The distinguishing factor in the logical use of alternative paths is their relative capacity to that of the main road; in most cases, in fact, their capacity is quite limited. Resilience was presented as a measure of disturbance to the network as a function of time in Ortiz et al. (2009). It is split into two phases: disruption and recovery. Disruption includes the time in which damage and failure to the network occurs as well as when the disruptions which propagate through the network. Recovery starts as the damaged components are repaired and its function starts returning to normal. By representing resilience as a function of time, the area under the graph is representative of a total physical property, often cost. Ortiz et al. (2009) note that state agencies should be cautious of out-of-date infrastructure and be aware of increases in congestion; both of these conditions can easily aggrandize the detrimental effects of extreme events. Much of the study is devoted to recommending the integration and collaboration of public and private agencies to bolster network resilience."}, {"section_title": "Graph theory in this study", "text": "The use of topological network properties, more ubiquitous in graph theory, to measure resilience and performance allows for a concrete quantification of resilience based off of traditional, accepted units of measurement. Topological performance measures, reviewed in depth in Newman (2003), are convenient for their versatility in describing both overall network properties and the contribution of each component. The studies Duenas-Osorio (2005) and Cimellaro et al. (2010) were both referenced to identify which network properties would best describe resilience aspects. Average nodal degree, average shortest path, measures of betweenness centrality, clustering coefficient, and redundancy ratio will be measured across simulated network scenarios. After reviewing node removal techniques used in Albert et al. (2000) and , random node removal was chosen to first examine the network response to a non-targeted attack, perhaps the closest node removal technique to a natural disaster. The aim of this study is to highlight the sensitivity of the network to different levels of node removal and highlight its impact on different aspects of the network characteristics. Furthermore, the probable storm surge heights under specific scenarios have been evaluated and the performance of the network under these conditions is measured using graph theory.  Figure 10 An Erdos-Renyi random graph (Barabasi et al., 2012)."}, {"section_title": "Figure 11", "text": "The Internet, an example of a complex network (Barabasi et al., 2012)."}, {"section_title": "CHAPTER V NEW YORK CITY AS A CASE STUDY", "text": ""}, {"section_title": "Random node removal", "text": "The New York City metropolitan highway network was reduced to a combination of 3,698 nodes connected by 8,494 unique links. This model was manipulated by randomly removing a fraction of nodes to simulate random events that would lead to failure of different links or nodes. Figure   14 compares an intact network diagram to networks with 10, 30 and 60% node removals. To investigate the trends of system properties, each topological network property was measured as the fraction of nodes removed was gradually increased. Using this method enabled the evaluation of the effects of different random failure scenarios through topological network properties. The changing, often decreasing, values of the network properties represent the degradation of different aspects of system resilience. Degree distribution can reveal many network properties. As described in Chapter 4, it dependent on the probability p(k) that a randomly chosen node as a degree k. The plot which is used to determine degree distribution is often displayed as logarithmic and has an x-axis of k and a y-axis variable of p(k). The degree distribution was determined for the original network of the New York City highway system and is plotted in Figure 12. It is clear the degree with the highest probability is 2-that is, most nodes in the network have a degree two. Degree distributions of a random graph and of a scale-free graph were also plotted. A random graph has a Poisson degree distribution (Equation 5.1), where z is the average nodal degree. The degree distribution of a scale-free network follows a power law, Equation 5.2, where \u03b3 and its zeta function are determined empirically. To determine which distribution the network was better fit to, a goodness of fit test was performed on the data using a normalized mean square error (NMSE). The results of the NMSE values, which range from negative infinity to 1 (a perfect fit), indicated a better match to the One of the most fundamental properties to track is the average nodal degree, the trend of which is shown in Figure 15. The average degree is representative of the connectivity of the network and when compared to the undisturbed network value, it can reveal the difference in ease of travel and commute for vehicles. Instead of paralleling the linear decrease in number of nodes, the nodal degree declines exponentially from a starting value of 2.3, which is the average number of links to which each network nodes is connected in the original network. After a seemingly reasonable removal of one-fourth of the network nodes, the average degree has already decreased by half its original value. There is no detectable variation in the average nodal degree among simulations, attesting to the fact that it is entirely dependent on the number of nodes removed, which is constant across trials, instead of which nodes are removed, which is random. Many studies use the average nodal degree to rank network elements based on importance. The logic that follows is that nodes which are connected to large numbers of links are the most critical to the function of the network. This is true in most cases, but there are instances where nodes of low degree may play vital connectivity roles in the network, such either end of a water-spanning bridge (Holme et al., 2002). These nodes would connect two otherwise isolated transportation systems. Average betweenness centrality is dependent not on the number of links connected to a node, but instead on the number of shortest paths which contain the node. Because of this, its value has strong correlation to the importance of a node to the network performance. During random node removal, the average network betweenness rapidly drops ( Figure 17). The error is high and in some simulations, the average betweenness centrality increased after node removal. In some scenarios, the failure of specific nodes will cause the formation of more isolated clusters and a network which depends on a small number of elements. These elements have large betweenness values, sometimes with differences of orders of magnitude, which increase the network average. But after just 20% of nodes are removed, the average betweenness value has dropped to almost zero and the error has similarly decreased. This dramatic trend reflects the potential detrimental effects of a random failure affecting the seemingly minor fraction of points that are very important to the functionality of the network. The network property average shortest path is measure of the system redundancy. It becomes especially critical in transportation systems, where users direct resources to finding the shortest route from an origin to a desired destination. For a graph network, the shortest path is the smallest combination of links from one node to another. Using graph theory algorithms, each pair of nodes in a system is considered and after measuring all possible paths between them, the length of the shortest one is deemed the shortest path. If there is no possible path between two network nodes, the value of the shortest path is infinite. To limit the range of values for illustrative purposes, the infinite upper limit is reset to 1000. It is still large enough that an occurrence of a disconnected pair will force the average to dramatically increase. The large error bars in Figure 18 indicate the high variability of the average shortest path and its dependency on which nodes are removed during the simulation and where they are located, corresponding to the behavior of the average betweenness centrality. In fact, the average shortest path follows an inverted betweenness trend; after 20% of links are removed, the number of infinite shortest paths is adequate to cause the convergence to an upper limit of 1000. The pattern observed would cause serious delays and cancellations in a real transportation network with relatively low percentages of nodes damaged. The computational time required for the redundancy ratio algorithm limited the number of trials to four with an undisturbed network (no links removed) and networks with 10%, 30% and 60% removal. The results ( Figure 19) show an immediate drop off after just 10% node removal, followed by less drastic decreasing trend. A transportation facing a climatic event may reasonably experience damage to 10% of nodes. The difference in the redundancy ratio between the undisturbed network and one after 10% node removal is problematic and infers the need for more alternate routes to bolster resilience. Alternatively, it implies that the network flow depends on too few routes for paths from node to node. Another, more efficient method that can be used to measure redundancy is by use of connectivity indices. These indices, which include the alpha index, beta index, and the gamma index, are dependent on the number of links and nodes as well as the number of cycles present in a graph. Under random removal (Figure 20), the plots compare quite well to the limited results from measuring the redundancy ratio. All three indices exhibit a decreasing, nonlinear trend, signifying the disintegration of network redundancy."}, {"section_title": "Node removal with SLOSH results", "text": "The analysis described above was repeated with consideration of results from the storm surge modeling program SLOSH. Initially, 12 events were selected to run as a worst case scenarios set of storms. These storms were all modeled in the SLOSH display program as MEOWs, an enveloped case described in Chapter 4.3, and they were set to occur at high tide conditions with 60 mph average wind gusts-these are two of the four conditions that can vary in modeling a MEOW; the remaining conditions, direction and Saffir-Simpson category, were varied to create a set of 12 storms. Storms were either a category three or four, and the directions ranged from north, northeast, north-northeast, northwest, north-northwest, and west-northwest. After running these through the SLOSH display program, storm surge values for each cell in the New York basin grid were output. The data was manipulated to develop contours specific to each storm, delineated according to storm surge value. For every storm scenario, the contours were overlaid with the highway network of New York City. Using MATLAB, the contours were searched for network nodes, and a list of flooded nodes could be created for each storm. Translated to a physical network, this meant that no other node was adjacent to a flooded node; the node is effectively unreachable. After creating a new adjacency matrix for each storm scenario, an analysis of network properties could be performed. This was completed by determining the average degree, local clustering, average shortest path, and betweenness centrality values of the disturbed network. Unlike in the random node removal process previously described, this analysis required only one new set of network properties for each storm; in other words, the stages of node removal during the storm were not tracked stepby-step. The resulting data shows noticeable reductions in network connectivity from the original, undisturbed New York City highway network. Perhaps the most severe cases were both storms (category 3 and 4) making landfall from the west-northwest direction (Table 2, last row). In this scenario, with a category three storm, the total number of links and nodes came to only about 53% of the original network, meaning that 47% of the network was flooded. In examining a map of the remaining nodes and contours, it is clear that the coast of Long Island is among the worst areas hit by this potential storm. There are no visible nodes left in this area of the highway network. The percentages in Table 2 are comparisons to the corresponding network property in the original network. The linear relationship between links, nodes and average network degree causes the similarity among their percentages for each scenario. The largest difference between storms headed in the same direction with different categories is in the north-northwest scenarios. In the north-northwest category 3 storm, 80% of the original network remains intact. But in the category 4 storm of the same direction, only 56% of the network is undisturbed. In this case, because of the direction the hurricane lands, the surges are highly dependent on the wind speed; the higher sustained wind speeds characteristic of category 4 storms caused a dramatic increase in surge heights, resulting in a more damaged network. Although the average shortest path has no correlation to the average degree, all disturbed networks exhibit a dramatic increase in average shortest path value, reflecting a worsening level of connectivity in the network. Interestingly, the clustering coefficient increases in all hurricane scenarios. This indicates an occurrence of the same trend exhibited during the random node removal procedure where some scenarios had an increase in clustering. An increase in local clustering after node removal can be attributed to the presence of more defined clusters as nodes are removed. To examine more closely what is occurring in the network during each storm scenario, the behavior of the most critical nodes were investigated. The relative node importances, like other network characteristics, will be gauged by measures of graph properties. The nodes most critical to network performance were determined by the magnitude of betweenness centrality for each node. As described in Chapter 4, the algorithm for determining the betweenness property involves calculating how many of the network's shortest paths between any two nodes contain the node under consideration. Referring to the original, undisturbed network of NYC, the betweenness centrality was calculated for each of the 3698 nodes. The 20 maximum betweenness values were chosen as the 20 most critical nodes and their node IDs were extracted (Figure 21). Node 1015 has the highest betweenness centrality measure by more than 300,000. The 12 worst case storm set and its corresponding data of flooded nodes were searched to find which of the most important nodes were flooded in each scenario. A resulting list of each storm and the set of important nodes flooded was compared to the network behavior analysis of each storm. If a highranked network node is flooded in an event, the properties of the network resulting from the storm would be severely impacted. The differences between the original results with a surge cut-off and the results with elevation data considered are significant. Immediately, it is clear that the percentages of nodes and links damaged are much lower, indicating that consideration of elevation data is necessary to develop more realistic network analysis in the event of a hurricane. One also must note that these scenarios are enveloped, so in the event of a singular hurricane with similar characteristics to the MEOW, less severe consequences would be expected. However, the MEOW results are indicative of the most vulnerable nodes and all potential effects of an oncoming storm. The worst case scenario in both the original data and the analysis considering elevation was the same MEOW: category 4, west-northwest, 60 mph sustained winds making landfall at high tide."}, {"section_title": "Battery Park Surges", "text": "To further investigate the behavior of the New York City highway network under hurricane stressors, the resulting surges of these 24 storm scenarios at Battery Park, New York City, were studied. The scenarios are each shown along with the damaged New York City Networks in Figures 22-45. Battery Park, a frequented and popular destination for city-dwellers and tourists, is a low-lying public park located on the southernmost tip of Manhattan. It was chosen as a point study for its location and the surge levels experienced during Hurricane Sandy, which reached almost 14 feet. The data points in the Figure 46 plot are differentiated by Saffir-Simpson category (3 or 4) and direction (north, northeast, north-northeast, northwest, north-northwest, west-northwest). Aside from these parameters, the studies vary in wind speed (30 mph or 60 mph) and tide level at landfall (mean or high). While the category 4 storms were expected to all exceed the surges of the category 3 storms, this was not the case. The highest surges, all above 25 feet, are category 4 storms. However, below this, the category 3 and 4 surges are interspersed with each other. Instead, a clearer trend is apparent in the direction of the storm, implying, in this specific case, the direction may be more indicative of storm surge levels. The northeast-directed storms all fall below a surge level of 15 feet; the north-northeast storms result in slightly higher surges, but still remain below 17 feet. The higher surges are caused by storms heading westnorthwest, northwest, and north-northwest. North-directed storms seem to cause surges ranging between these two groups. In New York City, the storm direction may play an especially significant role because of the geometry of New York Harbor. The movement of surge-causing waters into the inlet bordered by Staten Island, northern New Jersey, and western Long Island can trap waves to create powerful surges. Storm scenario results were grouped into three categories by storm surge height at Battery Park, NYC: surges less than 18 feet; surges from 18 to 24 feet; surges above 24 feet. The behavior of network properties was then studied with a focus on the comparison among these three surge categories. First, the average degree of the damaged networks was plotted ( Figure   47). Generally, storms causing a higher surge experienced a lower average degree. Two storm scenarios in the 18-24 ft group are exceptions, both are north-directed scenarios making landfall at high tide with 60 mph winds-one is a category 3 storm, the other is a category 4, and their average degrees (2.27 and 2.23, respectively) are higher than those of the remaining scenarios in the same 18-24 ft surge group. This is explained by the dependence of average degree on the number of nodes damaged or removed from the network. While other properties depend on which nodes are flooded, the average network degree is reflective of only the quantity. In these two storms, there are not many flooded nodes with 99% and 97% of nodes remaining, respectively, allowing for high average degree values. The plot in Figure 47 shows a noticeable gap-there are no damaged networks with average degrees between 2.00 and 2.14. There are storms from the 18-24 ft and > 24 ft ranges with degree values higher and lower than this gap. Another two storms in the 18-24 ft group are outliers, but their average network degrees fall notably below the trend: both make landfall at high tide, but one is a category 4 storm directed west-northwest with 30 mph winds, the second is a category 3 storm headed northwest with 60 mph winds. In the > 24 ft range, two storms have average degrees above the value gap: a west-northwest, category 3 storm making landfall at high tide with 60 mph winds and a north-northwest, category 4 storm making landfall at mean tide with 60 mph winds. The gap is also evident when studying the percentage of nodes flooded in the scenarios. No storm scenarios result in node percentages between 88-92%. The storm scenarios which flood more than 12% of nodes are the 7 storms with the lowest average degree. The two storm scenarios resulting in a Battery Park surge greater than 24 ft have fewer flooded nodes than the other five nodes in the same group, both less than 18%. The storm scenario resulting in the network with the lowest average degree also results in the most flooded nodes-15% of the original network. The average network shortest path and betweenness centrality values were plotted in Figures 48 and 49, respectively. Similar to the comparison found during the random node removal, the plots of average shortest path and betweenness centrality are inverses of each other. The focus of these results are the five extreme points of both properties, all which are storm scenarios that result in a Battery Park surge above 24 feet. As expected, these are all category 4 storms and they are also characterized by 60 mph wind speeds. Higher intensity hurricanes cause higher surges, which flood more nodes and decrease the average betweennes value. However, they differ in direction and tide level: two are northwest storms, one at high tide and one at mean tide, two are west-northwest storms, one at high tide and one at mean tide, and the remaining scenario is north-northwest at high tide. The average betweenness values for these five scenarios are close in range, only varying by 2000. In fact, three storms have the same betweenness value of 57,835. After investigating which nodes were flooded in these scenarios, specifically which of the nodes with the highest betweenness centrality values, it was determined that the extreme difference was due to the effective removal of four top nodes. These five storms, unlike the other 19 scenarios, cause the flooding of the 2 nd , 5 th , 7 th , and 14 th most important network nodes, measured by betweenness centrality. The explanation stands for the average shortest path plot, where the same five storms exhibit average shortest path values in close range to each other, but over 300 from the next highest value. Without access to these critical intersections (flooded nodes), travel between origins and destinations is negatively impacted. The value of the average shortest path jumps significantly once these four top nodes are removed. The plots of average shortest path and betweenness centrality behavior for networks under random node removal also illustrate dramatic increases in value between 0 and 20% node removal. The average clustering coefficient of the networks after the hurricane scenarios were simulated was then investigated (Figure 50). There is no clear correlation displayed in the plot, although the data does seem to trend downwards towards a lower clustering coefficient, with high error among trials. One of these outliers, the highest clustering coefficient of a damaged network, was found in a storm scenario with a northwest direction, category 3, 60 mph winds making landfall at high tide. Observing the SLOSH results from this specific scenario, the distribution of surge heights is noticeably different than the SLOSH results from the other scenarios. There are higher surges concentrated around Manhattan, which may be the cause of node flooding that increases the local clusters. As can be seen from the clustering coefficient behavior of the network under random node removal, the value of the clustering coefficient is highly dependent on which nodes are removed and there is high error for the general decreasing trend seen. These outlying storm scenarios in Figure 50 are indicative of this high variability. After selecting the highway network of the New York City metropolitan area, for a range of reasons, as a case study for this research, the network behavior was studied as fractions of random nodes were removed from the network. During these simulations of random attacks, topological network properties were measured and plotted against the fraction of node removal to observe trends and make comparisons. To develop a more realistic node removal method, SLOSH was used to simulate enveloped hurricane scenarios on the New York City basin. These results were used to determine which nodes were flooded, then to find which nodes were removed from the network. In these scenarios, the specific surge heights at Battery Park were used to categorize the results into groups. Network properties were then measured in each scenario-based damaged network, which were plotted to observe the degradation of network connectivity and redundancy. Overall, these results provide important information which can be closely related to the resiliency of the New York City network to both random and scenariobased removals.              "}, {"section_title": "CHAPTER VI", "text": ""}, {"section_title": "CONCLUSIONS AND RECOMMENDATIONS", "text": "After reviewing resilience and its many definitions as an academic term, a form specific to this study was outlined. This definition is that of engineering resilience, comprised of aspects of robustness, resourcefulness, redundancy, and rapidity. Although it can be measured in different ways, in this thesis research, resilience was quantified by topological graph properties of the network under consideration. The hazards most relevant to coastal urban networks were then examined, and hurricanes, specifically storm surges, were identified as the focus of this study. Before modeling hazard scenarios, the original network was investigated and general graph properties were determined. Of these characteristics, the most significant one was finding the degree distribution, which is closest to a Poisson distribution, or the distribution of a random graph. Interestingly, random graphs are less robust to failures of a random nature, and more robust to targeted attacks, providing some insight to the results of the failure simulations of the New York City network. These failure simulations were random, and the failures subjected to the network occurred at increasing fractions of node failures to observe how the network behaved during random node removal. Topological graph properties were then used to illustrate this behavior of system resilience, specifically degree correlation, local and global connectivity, and redundancy. The evaluated graph properties evaluated showed varying trends reflecting overall degradations in network performance and connectivity. SLOSH software was employed to simulate enveloped hurricane scenarios of different categories, wind speeds, tide levels, and directions. The storm surge heights were combined with elevation data of the nodes to simulate the results of a real hurricane. The SLOSH results were then used to determine which nodes were effectively removed from the network in scenario-based node removals. By using results from SLOSH investigations, nodes can be more accurately identified as disrupted and then removed. Similar to the method applied in the random removal scenarios, topological graph properties were measured in each hurricane scenario. These were grouped with respect to surge height at Battery Park, Manhattan, and then compared among each other and to the random removal results. The results in this investigation can be replicated to evaluate and compare network resilience measures between different systems or after improvements and developments have been made to existing infrastructure. The large uncertainty associated with extreme climatic events forces transportation agencies to respond with over-engineered designs or in many cases, to not prepare at all because of the overwhelming and expensive nature of the potential hazards. Supplied with this type of data, however, decision-makers and legislators are better able to direct resources to the most vital locations, allowing a more reasonably budgeted response to climatic hazards. Transportation agencies can pinpoint the specific network components which are most critical to maintaining network flow. Instead of attempting a complete system overhaul, agencies can easily achieve prioritization of replacements and structural updates. The approach which references hurricane storm surge simulation results allows transportation agencies to identify not only the nodes which are most critical to network flow, but also those which are most vulnerable to a specific hazard. While a range of methods exist to measure the resilience of a network, the use of topological graph properties to track network response are shown to be useful in investigations of this transportation network. Examining the nodes most affected by the envelope simulation, planners can incorporate these vulnerabilities into long-term planning."}]