[{"section_title": "Abstract", "text": "Cognitive monitoring and screening holds great promises for early detection and intervention of Alzheimer's disease (AD). A critical enabler is the personalized degradation model to predict the cognitive status over time. However, estimating such a model using individual's data faces challenges due to the sparsity and fragmented nature of the cognitive data of each individual. To mitigate this problem, we propose novel methods, called the collaborative degradation model (CDM) together with its extended network regularized version, the NCDM, which can incorporate useful domain knowledge into the degradation modeling. While NCDM results in a difficult optimization problem, we are inspired by existing non-negative matrix factorization methods and develop an efficient algorithm to solve this problem and further provide theoretical results that ensure that the proposed algorithm can guarantee non-increasing property. Both simulation studies and the real-world application to AD are conducted across different degradation models and sampling schemes, which demonstrate the superiority of the proposed methods over existing methods."}, {"section_title": "INTRODUCTION", "text": "Alzheimer's disease (AD) is a neurodegenerative disorder that is characterized by progressive memory loss and other cognitive impairments. Convergent evidences have shown that disease-modifying therapies will be most effective (e.g., for maintaining cognitive health or delaying disease progression) at earliest stages of the disease. With the rapidly developing biomarker measurement technologies, such as the MRI images, the PiB (Pittsburgh Compound B) PET scan, and CSF measurements, the delineation of the early states of AD has emerged as a major scientific priority in the hope to discover therapeutic interventions that can delay progression to AD. For example, one major disease-modifying therapy that holds great promises of preventing AD is anti-amyloid preventative treatment [14, 23] .\nDespite the promises offered by the biomarkers, the identification of the pre-symptomatic individuals who are going to experience abnormal cognitive decline poses a great challenge in terms of feasibility and cost. For example, although the PiB-PET scan offers a non-invasive in vivo method to detect and quantify brain amyloid deposition, this approach for pre-symptomatic detection is economically challenging for routine use given the current cost and restrictions on reimbursement [27] . Similarly, the clinical use of other useful biomarkers such as A\u03b2 and phosphorylated tau in cerebral spinal fluid (CSF) is also limited, since lumbar puncture carries risks and is met with resistance in elderly subjects. Furthermore it is unlikely to be used in primary health care centers to routinely screen a large number of participants. Given the cost and limited availability of these brain amyloid measurement techniques, more cost-effective first-line approaches for screening participants at risk of AD are needed [3] .\nRecently, there is increasing awareness in the AD research community regarding the importance of cognitive monitoring and screening as a cost-effective tool for early detection of AD. The primary objective of the cognitive monitoring and screening for elders is to identify subjects who may have unrecognized cognitive impairment or undiagnosed dementia. After decades of developments on the instruments that can be used to measure cognitive status, the Mini Mental State Examination (MMSE) and the Alzheimer's Disease Assessment Scale Cognitive subscale (ADAS-cog) have been the gold standard for clinical diagnosis of AD and drug effect evaluation in AD drug trials. Recent studies have also revealed the correlation of these cognitive measurements with the underlying biomarker measurements [19, 38] . While those instruments are undergoing continual refinement and development, to operationalize the idea of cognitive monitoring and screening in primary care or community setting, there is a pressing need for developing a personalized degradation model to predict the cognitive status over time. These personalized cognitive degradation models will lay the foundation for optimally allocating the limited healthcare resources to effectively detect the subjects who may experience abnormal cognitive decline [15, 16] . For instance, such a prediction model will enable the design of the repeated measurements over time to be able to detect trajectories of cognitive change, or it will enable the selection of at-risk subjects for early screening, etc.\nIn this paper, we investigate the statistical challenges and the corresponding modeling solutions that lay in the development of the personalized cognitive degradation models for routine cognitive monitoring and screening. Particularly, we recognize that the major challenge is the sparsity and fragmented nature of the cognitive data of each individual. This is not uncommon in many healthcare studies where it is difficult to collect an abundance of longitudinal data that can cover the whole spectrum of disease progression [12, 41] . Considering the significant complexity of the progression trajectory of the cognitive status that has been reported in the literature [5, 33] , without enough data points that cover the whole spectrum of disease progression, it will result in significant bias when predicting in the area with sparse or no training data. An example is shown in Figure 1 . Thus, one common approach to mitigate this problem is to merge all the individual data together and estimate a group-level prediction model [26] . However, this model reflects only average effect, failing to capture the personalized variation on the progression trajectory. In this paper, our solution is to incorporate the domain knowledge into the cognitive degradation modeling in the hope of reducing the bias. Particularly, we will focus on two forms of domain knowledge, including 1) the latent cluster structure: it has been discovered in the AD research community that there is a latent cluster structure in the AD population [15, 9, 40] . A rough definition of the cluster structure consists of three groups: the diseased group, the normal aging group (NC), and the mild cognitive impairment group (MCI). Previous studies demonstrated that these three groups have exhibited distinct progression rates on cognitive deterioration [40] . Recent research findings have further revealed that there are more subgroups in the MCI group [9, 34] , suggesting a more refined cluster structure. While the subjects between groups may follow significantly different progression trajectories, it is reasonable to believe that the subjects within the same group may follow similar progression trajectories. Therefore, to utilize the cluster structure, we develop a novel model, that is called the collaborative degradation model (CDM). CDM assumes that there are latent subgroups, while each subgroup has a distinct cognitive degradation model. Since it is usually unknown that which subgroup a subject may belong to, CDM assigns a membership vector (with elements) to each subject, e.g., denoted as for subject , while represents the probability of subject belongs to subgroup . Then, the degradation model of subject can be characterized by a weighted combination of the degradation models of the latent subgroups.\n2) The similarity information between subjects: we also extend the CDM formulation to incorporate more domain knowledge, represented as similarity information between subjects. This domain knowledge is common, e.g., it is usually available in AD research to have the individual measurements of some risk factors, including the demographic, social-economical, and even genetic and imaging information, etc. Thus, the similarity between two subjects can be quantified by comparing their profiles on these risk factors. More details of how to obtaining the similarity information based on risk factor measurements will be provided in Section 3. Thus, by incorporating these two forms of domain knowledge together, we expect that the proposed CDM can more effectively utilize the available sparse cognitive degradation data for better modeling. While CDM provides nice intuitive interpretations, it presents a challenging constrained optimization problem. Inspired by existing non-negative matrix factorization methods [6, 10] , we then develop corresponding computational algorithm to solve this optimization problem, and we further provide theoretical results that ensure that the proposed algorithm can guarantee the non-increasing property. This paper is organized as follows: in Section 2, we will provide details of the proposed CDM method; in Section 3, we will derive the corresponding computational algorithm that solves the optimization problem of CDM, and provide theoretical results that ensure that the proposed algorithm can guarantee non-increasing property; in Section 4, we will conduct comprehensive simulation studies to demonstrate the efficacy and superiority of the CDM method over a number of existing methods, and demonstrate the CDM method on a real-world dataset on AD; in Section 5, we will provide a conclusion and brief discussion of future work."}, {"section_title": "PROBLEM STATEMENT AND MODEL FORMULATION", "text": "In this paper, we concern the problem of building the prediction model for each of the subjects who are participants in a cognitive monitoring and screening program. We will propose the novel model, CDM, which can exploit the latent structure in the AD population and further incorporate the similarity information between subjects. As mentioned in Section 1, these prediction models are critical for operationalizing the idea of cognitive monitoring and screening in primary care or community setting. For instance, accurate prediction of cognitive status of healthy normal subjects may enable timely detection of the subjects that are subject to risk of cognitive decline. Also, accurate prediction model can help the health care provider to prioritize the screening efforts on the subjects.\nTo build these prediction models, specifically, with the cognitive status (e.g., measured by ADAS-cog or MMSE) as the outcome of interest, the prediction model aims to capture the predictive relationships between the risk factors with the cognitive status. For each subject , we assume that there are longitudinal measurements at time points that include the longitudinal cognitive status measurements, denoted as [ ] , and the longitudinal measurements of the risk\nFor simplicity, here, we focus our methodology development on linear models, e.g., we assume that the prediction model of subject , denoted as ( ), is a linear model on , i.e., ( ) , where is the coefficient vector. Linear models have been found successful in characterizing the dynamic progression of cognitive status, such as the individual growth Curves (IGC) in [13] . Linear models can also be easily extended to capture the nonlinear dynamics by using nonlinear basis functions of as demonstrated in these applications [21, 28] .\nConventional approaches for building the prediction models individually have found difficult in this problem. As mentioned in Section 1 and demonstrated in Figure 1 , the sparsity and fragmented nature of the cognitive measurements of individuals elevates the risk of being seriously biased in some areas where measurements are sparse or even null. Thus, the basic idea of CDM is to utilize the underlying cluster structure that has been discovered in many AD studies [7, 15, 20, 31, 37] . Thus, let ( ) be the degradation model of the latent subgroup and be the corresponding regression parameters. Recall that each subject has a membership vector , where represents the probability of subject belongs to subgroup . To predict for subject with any given risk profile, i.e., calculate for ( ), it is reasonable to use the weighted combination of the predictions of the degradation models \u2211 ( ) , i.e., ( ) \u2211 ( ) . Apparently, the more similar between subject with subgroup (i.e., larger ), the more important the degradation model ( ) will have in determining the value of ( ) . Furthermore, the following Theorem reveals that, on linear models, this modeling assumption also leads to an interesting property as the coefficient vector of the model ( ) can also be represented as a linear combination of the coefficient vectors of the degradation models."}, {"section_title": "Proof: please see the Appendix for a detailed proof.", "text": "Based on this model, if we use the least square loss function to measure the goodness-of-fit of the model and consider the constraint that the is normalized and has nonnegative elements, we will have the following optimization formulation of the CDM:\nHere, is imposed due to neuropsychological constraints that the predicted cognitive score should stay nonnegative.\n, \u2211 is imposed due to its definition as a membership vector. We then extend this formulation to incorporate the domain knowledge that is represented as similarity information between subjects, i.e., denoted as for subjects and . The similarity can be calculated based on some commonly available measurements of some risk factors, such as the demographic, social-economical, and even genetic and imaging information, etc. More details will be provided in Section 3 for how to calculate using these information. Here, to incorporate the similarity knowledge, we add a regularization term, \u2211 || || , into the objective function of (2.1), which leads to the NCDM formulation:\nApparently, the regularization term is added to encourage the membership vectors more similar if the similarity between the two subjects is larger. is the tuning parameter that is used to control the degree to which the regularization term will affect the parameter estimation. While solving (2.2) will lead to optimal parameter estimation, it is a constrained optimization problem with non-convex objective function, which has no closed form solution and is difficult to solve by regular gradient-based algorithms. We will present the details of our proposed algorithm in Section 3."}, {"section_title": "Remark 1:", "text": "In the literature, a number of transfer learning and multitask learning approaches [25, 26, 45] have been proposed to jointly learn multiple prediction models by treating these models as related. Although the CDM can be generally categorized as a multitask learning approach, one distinct difference of CDM from them is that CDM explicitly exploits the cluster structure embedded in many applications. By allowing the prediction model of each individual to be a weighted combination of the degradation models of the latent subgroups, CDM essentially enables automatic determination of the relatedness of the prediction models. Furthermore, as a byproduct, the CDM also reveals the clustering structure of the underlying problem and produces the subgroup-level degradation models of the latent subgroups, leading to valuable domain insights not available in many existing multitask learning methods."}, {"section_title": "PARAMETER ESTIMATION", "text": "In this section, we present the proposed algorithm for estimating the parameters and of NCDM (CDM is a special case of NCDM with ). First, we rewrite the formulation in (2.2) in matrix form:\nHere, we used the fact that \u2211 || || \u2211 \u2211 ( ) , where \u2211 , is a diagonal matrix with entries { }, and that is also called the graph Laplacian matrix [17] . We observe that by decoupling the estimation of and as two sub-optimization problems, we can derive a simple but efficient iterative algorithm. This is because that the constraints are imposed on and separately. Within each iteration, with the latest estimation of , we can derive a solution of optimizing problem with respect to ; while with the latest estimation of , we can derive an efficient updating algorithm for ."}, {"section_title": "1) Solve for at fixed parameters :", "text": "With given , the optimization problem (3.3) can be rewritten as This quadratic programming problem can be solved by existing algorithms [32, 39] ."}, {"section_title": "2) Solve for at fixed latent models :", "text": "The objective function is\nBy introducing the Lagrange multiplier for constraint , the Lagrangian is\nThe partial derivative of L with respect to is\nUsing the complementarity condition to enforce the nonnegativity of , , we get the following equation for\nSumming order and using the primary feasibility and , we have\nUsing the expression of multiplier in (3.7), (3.6) can be written as\nThis equation leads to the following updating rule:\nThus, we derive an algorithm that iteratively optimize for and . Figure 2 provides a summary of the overall algorithm that consists of the two steps.\nAs demonstrated in the numeric studies in Sections 4 and 5, the proposed algorithm is efficient and easy to converge. Besides this empirical evidence, the following Theorem 2 also shows that the objective function is non-increasing by using the proposed algorithm. Note that we need to define a similarity matrix to implement the algorithm presented in Figure 2 . While sometimes it can be readily available through querying prior knowledge or expert opinion, here, we also adopt some existing approaches that have been found effective in the literature [30] , including the 0-1 weighting, Heat Kernel Weighting and Dot-Product Weighting. In our numerical studies on both synthetic datasets and real-world datasets, we found that the heat kernel weighting method consistently lead to satisfactory results. One reason for the superior performance of the heat kernel weighting method is probably because that, comparing with the 0-1 weighting and dot-product weighting, the heat kernel weighting method allows optimal tuning of the similarity by introducing a scaling parameter , i.e., it is defined as | | for subjects and . Thus, the similarity between subjects can be automatically determined by model selection methods such as cross-validation, AIC and BIC. Other implementation issues include how to obtain initial values ( ) and ( ) . Empirical evidence in the simulation studies (Sec 4.1) and the real-world data analysis (Sec 4.2) shows that solutions from mixed effect models [1] can provide good initial values. "}, {"section_title": "NUMERICAL STUDIES", "text": ""}, {"section_title": "Simulation Studies", "text": "We investigate the performances of the proposed CDM, the CDM with networked regularization term (NCDM), the mixed effect models (MEM) [1] (i.e., MEM assumes that comes from a multivariate normal distribution), and the trivial method that builds the model for each individual separately (IGM) (i.e., estimate independently). We compare these models across various settings of some parameters, such as the number of latent subgroups ( and ), the sparsity of the samples (sparse sampling and dense sampling), and the types of degradation models (type 1 model and type 2 model), etc. Here, the sparsity of the samples controls how many samples we can collect in each individual. Throughout our simulation studies, for each individual, we always simulate longitudinal observations on 25 consecutive time points regardless of any other factors such as or the type of degradation model, etc. Then, in the \"dense sampling\" scenario, we randomly select observations (i.e., ( )) from the first 20 observations as the training data for each individual; while in the \"sparse sampling\" scenario, we randomly select observations (i.e., , we can randomly generate the matrix and . Specifically, to encourage the clustering structure between the subjects, we tend to generate the membership vector that has a dominant element. Thus, for example, considering the case that there are three clusters. We design three multivariate normal distributions as below:\nFor generating , we first use a multinomial distribution to randomly select which multivariate normal distribution will be used, and then, use the selected one to generate a random sample. This random sample can be further normalized to obtain . Apparently, the larger the magnitude of in , the more dominant the element in . The results reported in what follows correspond to , while the simulation results are robust to other choices of . One example of the degradation models (for type 1 model) generated using our simulation procedure is illustrated in Figure 3 .\nWe evaluate the performances of the models based on two major criteria, the parameter estimation and prediction accuracy. For parameter estimation, we calculate the difference between the estimated coefficients with the true coefficients, e.g.,"}, {"section_title": "\u2211 ( \u0302 )", "text": ".\nFor prediction accuracy, we compare the normalized mean square error (nMSE) on the testing set, e.g.,\n, where is the true coefficient of risk factor and subject , \u0302 is estimated value of , are the true cognitive measurements (including all the subjects in the testing dataset) at a single time point, \u0302 are the corresponding predicted values of , and is the number of observations at that time point. Figure 3: The curves of the randomly generated degradation models of 100 subjects\nWe summarize the simulation results in Table 1 , which correspond to . Similar results can be obtained for other choices of . Overall, we are able to draw the following observations: 1) the proposed CDM is effective on exploiting the latent structure as demonstrated by its better performance than IGM in terms of both parameter estimation and model prediction. This verifies our hypothesis as demonstrated in Figure 1 that CDM can effectively borrow strengths from subjects that tend to exhibit similar degradation patterns. 2) the NCDM performs best in all the models, which demonstrates that the NCDM can effectively incorporate the similarity information between the subjects to further enhance the model estimation. 3) the advantage of NCDM is larger in the sparse sampling scenario, indicating that the incorporation of prior knowledge will be more preferred when there is a lack of observations."}, {"section_title": "Application to Alzheimer`s Disease (AD)", "text": "While simulation studies have demonstrated the efficacy and accuracy of the proposed CDM and NCDM, here, we demonstrate our proposed methods on a real-world dataset that is collected in the Alzheimer's Disease Neuroimaging Initiative (ADNI) [44] . In this study, we identified a set of 478 subjects who have longitudinal measurements of MMSE (collected at baseline, 12 th month, 24 th month, 36 th month, 48 th month and 60 th month), including 104 cognitively normal older individuals (NI), 261 patients with mild cognitive impairment (MCI), and 133 AD patients (AD). Figure 4 depicts the cognitive degradation curves of those 478 subjects, which clearly reveal the cluster structure of these subjects: the MMSE measurements of the NI subjects at different time points maintain at a high level with small fluctuations (blue line), while the MMSE measurements of AD patients degrade dramatically (red line), and the degradation of the MMSE measurements of MCI patients is faster than NI but slower than AD (black line). Among these subjects, 21 have 3 observations, 156 have 4 observations, 244 have 5 observations. Thus, we believe this dataset provides a good example for demonstrating our proposed methods. Particularly, we remove the group information before implementing the proposed methods to demonstrate that the CDM and NCDM can effectively recover the latent structure. We also use the measurements in the 48 th month and 60 th month as testing data, and the others as training data. In addition, besides the MMSE measurements, for each subject there are also some other baseline measurements of some risk factors. Specifically, we use the ApoE genotypes, the baseline MMSE score, and the baseline regional brain volume measurements extracted from MRI via FreeSurfer [8] . A total of 34 features are used. This information is used in our model to calculate the similarity between the subjects, using the heat kernel method mentioned in Section 3 by cross-validation. In our case study, we formulate the cognitive degradation process of subject using the following model [22] :\nwhere is the MMSE measurement of subject at time . Then, we first investigate if the proposed method can automatically identify the latent structure. Thus, we employ the CDM model and use AIC to automatically select the best model. This model selection procedure is shown in Figure 5 (a) , where the AIC value reaches minima when the number of latent model is 3, which is consistent with our prior knowledge of the AD dataset. Actually, the quality of this minimum is well as it is quite different from the AIC values of the neighbor values of .\nBesides the selection of , we can also observe good performances on the selection of some other parameters in NCDM such as the and the scaling parameter in the heat kernel function by cross-validation. Furthermore, Figure 5 (b) also shows that the algorithm for NCDM converges quickly, i.e., with less than 50 iterations. To compare the performance of the methods (IGM, CDM, MEM, and NCDM), besides the use of the normalized mean square error (nMSE) that has been used in our simulation study, we also use the weighted correlation coefficient (wR) as employed in the AD literature [28, 43] The prediction results are summarized in Table 2 . It is clear that the NCDM is the best approach on all the performance evaluations, demonstrating that the proposed method is capable of providing more accurate cognitive prediction models. Also, we can observe that the CDM is better than MEM and IGM, indicating that the CDM is indeed effective on utilizing the cluster structure that is embedded in the AD dataset. It is also clear that the NCDM just slightly deteriorates on predicting on the 60 th month, where the performance of the IGM drops dramatically. This shows that the proposed methods are particularly advantageous if long-term prediction/monitoring is desirable. Overall, both our simulation results and the real-world data analysis demonstrate that our methods have better prediction accuracy, and the gain of prediction accuracy has the potential of being translated into higher detection rate of early AD and more cost-effectiveness of the routine cognitive screening and monitoring programs that are currently launching in many primary health care centers."}, {"section_title": "CONCLUSION", "text": "In this paper, we propose CDM and NCDM to incorporate domain knowledge into the cognitive degradation modeling of AD. We also develop an efficient computational algorithm to estimate the models, and we further provide theoretical results that ensure that the proposed algorithm can guarantee the non-increasing property. Both simulation studies and AD cognitive data analysis show that the proposed methodology can lead to significantly better performance on both parameter estimation and prediction over existing methods."}, {"section_title": "Supplementary File", "text": ""}, {"section_title": "Proof of Theorem 1:", "text": "The result is straightforward to have:\nwhere and . Thus,"}, {"section_title": "Proof of Theorem 2:", "text": "Our proof follows similar ideas used in [Ding et al 2006; Cai et al 2011] . It's obvious that the objective function in (3.3) is bounded from below by zero, so the Lagrangian is also bounded from below. The solution will converge if the Lagrangian is monotonically nonincreasing. To prove Theorem 3, firstly, we need to show that the Lagrangian is nonincreasing in each step of the iterative algorithm, then, prove the iterative updates converge to a stationary point. Since in each iteration, the estimated minimizes the objective function, we only need to prove that the Lagrangian will be nonincreasing under the updating rule of in (3.8) .\nSince the updating rule (3.8) is essentially element wise, it is sufficient to show that each ( ) ( ) is monotonically nonincreasing under the update step of (3.8) . Note that ( ) only depends on when other values in are given. To prove this, we need to use the concept of auxiliary function, which is similar to that used in the expectation-maximization algorithm [Dempster et al., 1977] . A function (\n) is an auxiliary function of ( ) if\n(1) ( ) ( ) ( ) ( ).\nBy constructing ( ), we define (2) ( ).\nThus, we have ( )\n). This leads to the monotonicity of under the iterative updating rule of (2). To construct an auxiliary function, we write ( ) using Taylor expression,\nThen we have ( )\nThis is because \nThus, we can show the following function is an auxiliary function of (\nThis is because ( ) satisfies the conditions in (1): the last term in ( ) ( ); and the equality holds when . The minimum for (2) can be obtained by setting the gradient to zero. That is ( )\n.\nThis leads to the following solution:\nBy substituting the Lagrange multiplier as shown in (3.7) in the equation above, we recover the updating rule (3.8).\nThen we prove the iterative updates converge to a stationary point that satisfies the KKT conditions."}, {"section_title": "Lemma 1.", "text": "Starting from an arbitrary feasible nonzero point and , the iterative updates based on updating rule in (3.8) converge to a point that satisfies the KKT conditions for the optimization problem:\n."}, {"section_title": "Proof of Lemma 1:", "text": "We first write the KKT conditions for the optimization problem in ( .\nIt is straightforward to observe that, starting with nonnegative nonzero , the updating rule in (3.8) always keeps non-negative nonzero since the nonnegative assumption of the cognitive measures, and . Assume converge to , we have\nThe equation (4) implies:\n] ."}, {"section_title": "Therefore", "text": ""}, {"section_title": "\u2211 ( [", "text": ") .\nHence, we have:\nThis leads to \u2211 =0, because is nonnegative nonzero under the updating rule. Thus, the first term in the equation is nonzero. The updates converge to a feasible solution. By (3.7) it is known that ( )\nwith the equation (5), we have: "}]