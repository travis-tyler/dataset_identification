[{"section_title": "Abstract", "text": "Gaussian Graphical Models (GGM) are often used to describe the conditional correlations between the components of a random vector. In this article, we compare two families of GGM inference methods: the nodewise approach of [1] and [2] and the penalised likelihood maximisation of [3] and [4] . We demonstrate on synthetic data that, when the sample size is small, the two methods produce graphs with either too few or too many edges when compared to the real one. As a result, we propose a composite procedure that explores a family of graphs with a nodewise numerical scheme and selects a candidate among them with an overall likelihood criterion. We demonstrate that, when the number of observations is small, this selection method yields graphs closer to the truth and corresponding to distributions with better KL divergence with regards to the real distribution than the other two. Finally, we show the interest of our algorithm on two concrete cases: first on brain imaging data, then on biological nephrology data. In both cases our results are more in line with current knowledge in each field. ! \u2022 T. Lartigue is with the CMAP, CNRS,\u00c9cole polytechnique and Aramis project-team, Inria. \u2022 Data used in the preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (http://www.loni.ucla.edu/ADNI). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at https://adni.loni.usc.edu/wpcontent/uploads/how to apply/ADNI Acknowledgement List.pdf so slightly, any two occurrences. As a result, regular correlation networks tend to be fully connected and mostly uninformative. On the other hand, when intermediary variables explain the totality of the co-variations of two vertices, then these two are conditionally uncorrelated, removing their edge from the conditional correlation graph. The conditional correlation structure captures only the direct, explicit interactions between vertices. In our analyses, these interactions are the ones of most interest.\nA Gaussian Graphical Model (GGM) is a network whose values on the p vertices follow a Centred Multivariate Normal distribution in R p : X \u223c N (0 p , \u03a3). This assumption is almost systematic when studying conditional correlation networks for three main reasons. First, it ensures that each conditional correlation corr(X i , X j |(X k ) k =i,j ) is a constant and not a function of the p \u2212 2 dimensional variable (X k ) k =i,j ; a crucial property allowing us to talk about a single graph and not a function graph. Second, it equates the notions of independence and un-correlation, in particular:\nThis makes interpretation much clearer. Finally, under the GGM assumption, we have the explicit formula:\nis the inverse of the unknown covariance matrix. This means that the conditional correlations graph between the components of X is entirely described by a single matrix parameter, K. Moreover the graph and K have the exact same sparsity structure. With this property in mind, the author of [6] introduced the idea of Covariance Selection which consists of inferring -under a Gaussian assumption -a sparse estimation K of K and interpreting its sparsity structure as a conditional dependency network. Subsequently, many authors have proposed their own arXiv:2003.05169v1 [cs.\nLG]"}, {"section_title": "INTRODUCTION", "text": "D EPENDENCY networks are a prominent tool for the representation and interpretation of many data types as, for example, gene co-expression [2] , interactions between different regions of the cortex [5] or population dynamics.\nIn those examples, the number of observations n is often small when compared to the number of vertices p in the network. Conditional correlation networks are graphs where there exists an edge between two vertices if and only if the random variables on these nodes are correlated conditionally to all others. This structure can be more interesting than a regular correlation graph. Indeed, in real life, two phenomena, like the atrophy in two separate areas of the brain or two locations of bird migration, are very likely to be correlated. There almost always exists a \"chain\" of correlated events that \"link\", ever estimators K. In [1] , a local edge selection approach that solves a LASSO problem on each node is introduced. It was noticeably followed by [2] , [7] , who developed the GGMselect algorithm, a practical implementation of this approach coupled with a model selection procedure. We call these methods \"local\", since they focus on solving problems independently at each node, and evaluating performances with an aggregation of nodewise metrics. Other works within the local paradigm have proposed Dantzing selectors [8] , constrained l 1 minimisation [9] , scaled LASSO [10] , or merging all linear regression into a single problem [11] . On a different note, the authors of [3] and [4] considered a more global paradigm where the estimator is solution of a single l 1 -penalised log-likelihood optimisation problem, that has the form of Eq. (1) .\nWe call this point of view \"global\" since the likelihood estimates at once the goodness of fit of the whole proposed matrix. The introduction of problem (1) generated tremendous interest in the GGM community, and in its wake, many authors developed their own numerical methods to compute its solution efficiently. A few notable examples are block coordinate descent for the Graphical Lasso algorithm (GLASSO) of [12] , Nesterov's Smooth gradient methods [13] , Interior Point Methods (IPM) [14] , Alternating Direction Methods of Multipliers (ADMM) [15] , [16] , Newton-CG primal proximal point [17] , Newton's method with sparse approximation [18] , Projected Subgradient Methods (PSM) [19] , and multiple QP problems for the DP-GLASSO algorithm of [20] . The theoretical properties of the solutions to Eq. (1) are studied in [21] , [22] and in [23] . Other methods within the global paradigm include [24] , with penalties other than l 1 in (1), and [25] , with a RKHS estimator. More recent works have proposed more involved estimators, defined as modifications of already existing solutions and possessing improved statistical properties, such as asymptotic normality or better element-wise convergence. The authors of [26] and [27] adapted solutions of local regression problems including [1] , whereas [28] modified the solutions of (1). In [29] , the two approaches are unified with a de-biasing method applied to both local and global estimators.\nIn our applications -where the number of observations n is a fixed small number, usually smaller than the number of vertices p -we did not find satisfaction with the state of the art methods from either the local or the global approach. On one hand, GGMselect yields surprisingly too sparse graph, missing many of the important already known edges. On the other hand, the only solutions from the penalised likelihood problem (1) that are a decent fit for real distribution have so many edges that the information is hidden. To interpret a graph, one would prefer an intermediary number of edges. Additionally, the low sample size setting requires a method with non-asymptotic theoretical properties.\nIn this paper, we design a composite method, combining the respective strengths of the local and global approaches, with the aim of recovering graphs with a more reasonable amount of edges, that also achieves a better quantitative fit with the data. We also prove non-asymptotic oracle bounds in expectation and probability on the solution.\nTo measure the goodness of fit, many applications are interested in recovering the true graph structure and focus on the \"sparsistency\". In our case, the presence or absence of an edge is not sufficient information. The correlation amplitude is of equal interest. Additionally, we need the resulting structure to make sense as a whole, that is to say: describe a co-variation dynamic as close as possible to the real one despite being a sparse approximation. This means that edgewise coefficient recovery -as assessed by the l 2 error K \u2212 K 2 F = i,j (K i,j \u2212 K i,j ) 2 for instance -which does not take into account the geometric structure of the graph as a whole is not satisfactory either. We want the distribution function described by the proposed matrix to be similar to the original distribution. The natural metric to describe proximity between distribution functions is Cross Entropy (CE) or, equivalently, the Kullback-Leibler divergence (KL). In the end, the CE between the original distribution and the proposed one -N 0, K \u22121 -is our metric of choice. Other works, such as [30] and [31] , have focused on the KL in the context of GGM as well.\nIn the following, we quantify the shortcomings of the literature's local and global methods when the data is not abundant. The GGMselect graphs are very sparse, but consistently and substantially outperform the solutions of Eq. (1) in terms of KL, regardless of the penalisation intensity \u03c1. In the KL/sparsity space, the solutions of GGMselect occupy a spot of high performing, very sparse solutions that the problem (1) simply does not reach. Additionally, the better performing solutions of (1) are so dense that they are excessively difficult to read. Subsequently, we demonstrate that despite its apparent success, the GGMselect algorithm is held back by its model selection criterion which is far too conservative and interrupts the graph exploration process too early. This results in graphs that are not only difficult to interpret but also perform sub-optimally in terms of KL. With those observations in mind, we design a simple nodewise exploration numerical scheme which, when initialised at the GGMselect solution, is able to extract a family of larger, better performing graphs. We couple this exploration process with a KL-based model selection criterion to identify the best candidates among this family. This algorithm is composite insofar as it combines a careful local graph construction process with a perceptive global evaluation of the encountered graphs. We prove non-asymptotic guarantees on the solution of the model selection procedure. We demonstrate with experiments on synthetic data that this selection procedure satisfies our stated goals. Indeed, the selected graphs are both substantially better in terms of distribution reconstruction (KL divergence), and much closer to the original graph than any other we obtain with the state of the art methods. Then, we put our method to the test with two experiments on real medical data. First on a neurological dataset with multiple modalities of brain imaging data, where n < p. Then on biological measures taken from healthy nephrology test subjects, with p < n. In both cases, the results of our method correspond more to the common understanding of the phenomena in their respective fields."}, {"section_title": "COVARIANCE SELECTION WITHIN GGM", "text": ""}, {"section_title": "Introduction to Gaussian Graphical Models", "text": "Let S + p and S ++ p be respectively the spaces of positive semi-definite and positive definite matrices in R p\u00d7p . We model a phenomenon as a centred multivariate normal distribution in R p : X \u223c N (0 p , \u03a3). To estimate the unknown covariance matrix \u03a3 \u2208 S ++ p , we have at our disposal an iid sample X (1) , ..., X (n) assumed to be drawn from this distribution. We want our estimation to bring interpretation on the conditional correlations network between the components of X. No real network is truly sparse, yet it is natural to propose a sparse approximation. Indeed, this means recovering in priority the strongest direct connections and privileging a simpler explanation of the phenomenon, one we can hope to infer even with a small amount of data. Sparsity in the conditional correlations structure is equivalent to sparsity in the inverse covariance matrix K := \u03a3 \u22121 . Namely\nAs a consequence, our goal is to estimate from the dataset a covariance matrix \u03a3 \u2208 S ++ p with both a good fit and a sparse inverse K. We say that \u03a3 := K \u22121 is \"inverse-sparse\".\nIn the following, we use the Cross Entropy to quantify the performances of a proposed matrix K. The CE, H(p, q) = \u2212E p [logq(X)] = x \u2212p(x)ln(q(x))\u00b5(dx), is an asymmetric measure of the deviation of distribution q with regards to distribution p. The CE differs from the KL-divergence only by the term H (p, p), which is constant when the reference distribution p is fixed. In GGM, the score H(f \u03a3 , f \u03a3 ) represents how well the normal distribution with our proposed covariance \u03a3 is able to reproduce the true distribution N (0, \u03a3). We call this score the True CE of \u03a3. This metric represents a global paradigm where we explicitly care about the behaviour of the matrix as a whole. This is in contrast to a coefficient-wise recovery, for instance, which is a summation of local, nodewise, metrics. After removal of the additive constants, we get the simple formula (2) for the CE between two centred multivariate normal distributions N (0, \u03a3 1 ) and N (0, \u03a3 2 ).\n(2) In the general case, the CE between a proposed distribution f \u03b8 and an empirical distributionf n = 1 n n i=1 1 x=X (i) defined from data is the opposite of the log-likelihood: H(f n , f \u03b8 ) = \u2212 1 n log p \u03b8 (X (1) , ..., X (n) ). In the GGM case, we denote the observed data X := X (1) , ..., X (n) T \u2208 R n\u00d7p , and set S := 1 n X T X \u2208 S + p , the empirical covariance matrix. The opposite log-likelihood of any centred Gaussian N (0, \u03a3 2 ) satisfies:\nsimilar to Eq. (2) . As a result, we adopt an unified notation. Details on calculations to obtain these formulas can be found in Section 7.1.\nWe use the following notations for matrix algebra, let A be a square real matrix, then: |A| denotes the determinant, A * := tr A T A 1 2\nthe nuclear norm, = \u03bb max (A) the spectral norm (operator norm 2) which is also the highest eigenvalue. We recall that when A is symmetrical positive, then A * = tr(A) and A F = tr(A 2 ) 1 2 . We also consider the scalar product A, B := tr B T A on R p\u00d7p ."}, {"section_title": "Description of the state of the art", "text": "After its introduction, problem (1) became the most popular method to infer graphs from data with a GGM assumption. Reducing the whole inference process to a single loss optimisation is convenient. What is more, the optimised loss is a penalised version of the likelihood -which is an estimator of the True CE -hence the method explicitly takes into account the global performances of the solution. However, even though the l 1 penalty mechanically induces sparsity in the solution, it does not necessarily recover the edges that best reproduce the original distribution, especially when the data is limited. Indeed, the known \"sparsitency\" dynamics of the solutions of (1), see [22] , always involve a large number of observations tending towards infinity. We demonstrate in this paper that, when the sample size is small, other methods recover consequently more efficient sparse structures, inaccessible to the l 1 penalised problem (1). On the other hand, the local approach of [1] carefully assesses each new edge, focusing on making the most efficient choice at each step. We confirm that the latter approach yields better performance by comparing the solutions of problem (1) and GGMselect [2] on both synthetic and real data (Sections 4 and 5). However, the loss optimised in GGMselect, Crit(G), see (4) , is an amalgam of local nodewise regression score, with no explicit regard for the overall behaviour of the matrix:\nwhere pen is a specific penalty function, d a (G) is the degree of the node a in the graph G, X a are all the observed values at node a, such that X = (X 1 , ..., X p ) \u2208 R n\u00d7p is the full data, and:\u03b8\nwhere \u039b G is the set of p \u00d7 p matrices \u03b8 such that \u03b8 i,j is non zero if and only if the edge (i, j) is in G, and \u039b a G is the set of vectors \u03b8 a \u2208 R p such that (\u03b8 a ) i is non zero if and only if the edge (i, a) is in G . Note that by convention, auto-edges (i, i) are never in the graph G, and, in our work, G is always undirected. The full expression of pen can be found in Eq. 3 of [2] . It depends on a dimensionless hyperparameter called K which the authors recommend to set equal to 2.5. We first tried other values without observing significant change, and decided to use the recommended value in every later experiment. The expression (5) illustrates that each nodewise coefficients \u03b8 G a in the GGMselect loss are obtained from independent optimisation problems which each involve only the local sparsity of the graph in the vicinity of the node a, as seen in the definition of \u039b a G . In each parallel optimisation problem argmin \u03b8a\u2208\u039b a G X a \u2212 X\u03b8 a 2 2 , the rest of the graph is not constrained, hence is implicitly fully connected. In particular, the solutions of such problems involve an estimation of the covariance matrix between the rest of the vertices that is not inverse-sparse. This can bias the procedure towards the sparser graphs since it actually implicitly measures the performances of more connected graphs. Finally, the GGMselect model selection criterion (GGMSC) explicitly penalises the degree of each node in the graph making it so that string-like structures are preferred over hubs. Empirically, we observe that with low amounts of data, graphs with hubs are consistently dismissed by the GGMSC. Overall, we expect the selected solutions to be excessively sparse, which experiments on both synthetic and real data in Sections 4 and 5 confirm."}, {"section_title": "Graph constrained MLE", "text": "Even though a covariance matrix \u03a3 uniquely defines a graph with its inverse K, the reciprocal is not true. To a given graph G := (V, E), with vertex set V and edge set E, corresponds a whole subset \u0398 G of S ++ p :\nWhen data is available, the natural matrix representing G is the constrained MLE:\nThe existence of the MLE is not always guaranteed (see [6] , [32] ). When n < p, no MLE exists for the more connected graphs. However, in this paper, we design a procedure that can propose a MLE for any n and any graph without computation errors. To tackle the issue of existence, we add a very small regularisation term to the empirical covariance matrix S. This leads to solving:\n\u03bb is not a true hyper parameter of the model. Its value is set once and for all, and as small as possible as long as the machine still recognises S + \u03bbI p as invertible. Typical values range between 10 \u22127 and 10 \u22124 . This trick changes little for the already existing solutions. Indeed, if \u03a3 G solution of Eq. (6) exists, we observe empirically that for small values of \u03bb: \u03a3 G \u03a3 G,\u03bb . On the other hand, if no solution \u03a3 G to Eq. (6) exists, then we now are able to propose a penalised MLE \u03a3 G,\u03bb , thus avoiding degenerated computations. From now on, the MLE we use are always solutions of (7) . We will omit the index \u03bb and keep the notation \u03a3 G for the sake of simplicity."}, {"section_title": "Our composite algorithm", "text": "The exploration steps of our method are a variation of the local paradigm of [1] . First, we use the GGMselect solution as initialisation. Then we add edges one by one: at each step, for each vertex independently, we run a sparse linear regression using as predictors the vertices that are not among its neighbours yet, and as target the residual of the linear regression between the value on the vertex and its neighbours. With these regressions, each vertex proposes to add to the current graph an edge between them and their new best predictor. Here however, we deviate from the local paradigm by using a global criterion -the out of sample likelihood of the whole resulting new matrix -to evaluate each proposition and select one edge among these candidates. We end this exploration procedure after a fixed number of steps, the result is a family of gradually more connected graphs. The final selection step is done with a global metric: we pick, among the so constructed family, the graph minimising the Cross Validated (with fresh data) Cross Entropy. See Fig. 1 for the details. In the spirit of [26] , [27] , [28] , [29] , this method is designed to complete an already existing efficient, but sparse, solution. As a result, it is sensitive to the initial graph."}, {"section_title": "ORACLE BOUNDS ON THE MODEL SELECTION", "text": ""}, {"section_title": "PROCEDURE", "text": "In this Section, we give non-asymptotic guarantees on the model selection step of our algorithm. We prove these results in Section 7. Using the statistical properties of our model selection criterion, in particular the absence of bias and convergence towards the oracle criterion, we describe the difference between the performance of the selected model and the oracle best performance (\"regret\"). This regret is dependent on the convergence of a Wishart random variable towards its expectation. As a result, we are able to prove non-asymptotic upper bounds in expectation and probability for the regret."}, {"section_title": "Inputs:", "text": "The train set are all the observations available for graph inference, Nb of steps T fixed in advance."}, {"section_title": "Start:", "text": "\u2022 Run GGMselect on the train set to get the initial graph G 0 = (V, E 0 ); Partition the train set into a validation set and exploration set; for t = 1, .., T do Partition randomly the exploration set into a learning set and an evaluation set; Compute the empirical covariance S t eval from the evaluation set; # We then \"ask\" each node for its desired next neighbour:\n\u2022 Run on the learning set the linear regression with the vector X a of the values on a as the target, and the vectors {X s |s \u2208 N t\u22121 (a)} on the neighbour nodes as predictors. LetX a be the residual of this regression;\n\u2022 Run on the learning set one step of the LARS algorithm of [33] , withX a as the target, and the re-\nthe index of the feature chosen by LARS; end for # We now have p potential new edges {(a, c t (a))} a\u2208V some of which can be identical # We give priority to mutual selections: when c t (c t (a)) = a if {(a, c t (a))} ct(ct(a))=a = \u2205 then Let C = {(a, c t (a))} ct(ct(a))=a be our set of candidate edges; # We keep only the mutual selections else\n\u2022 G t := G c * t ; Compute, with the exploration set, the MLE \u03a3 t from G t ; end for Compute, with the exploration set, the MLE \u03a3 0 from G 0 ; Compute the empirical covariance S val from the validation set;\n\u2022 t * := argmin t=0,...,T H S val , \u03a3 t ;\n\u2022 G := G t * ; Return: Inferred graph G. Fig. 1 . Composite GGM estimation. We respectively identify with green \u2022 or orange \u2022 bullets the steps adhering to a local or global paradigm. Comments are in italics."}, {"section_title": "Framework", "text": "In this Section we define or recall the relevant concepts and notations. We recall and rephrase the definition, given in Eq. (7), of the constrained Maximum Likelihood Estimator we build from a given graph G:\nWe use the Cross Validated Cross Entropy (CVCE) H S val , \u03a3 G (S expl ) as a criterion to pick a graph G CV among the ones encountered. This Cross Validated criterion uses the partition of the training set into a validation set -used to build the estimation S val of the true matrix \u03a3 -and an exploration set -used for the graph exploration process and to build the constrained MLE \u03a3 G (S expl ) for each encountered graph G. We compare the graph G CV selected with CVCE with G * selected with the True Cross Entropy H \u03a3, \u03a3 G (S expl ) of the matrix \u03a3 G (S expl ). We define formally those graphs: in Eq. (8) and (9):\nwhere we call M the family of graphs uncovered by the Composite algorithm.\nRemark With the data available, the ideal model selection would be made with True Cross Entropy H \u03a3, \u03a3 G (S train ) of the matrix \u03a3 G (S train ) built from the whole train set. Comparing ourselves to this criterion would allow to quantify the importance of having a balanced split between validation and exploration set. This is outside the scope of this Section. We just compare our H S val , \u03a3 G (S expl ) to H \u03a3, \u03a3 G (S expl ) . In this case, the convergence of S val towards \u03a3 is the only dynamic that matters."}, {"section_title": "Basic control", "text": "In this Section, we show a general upper bound on the regret, using only the properties of the model selection criterion, and not yet the properties of the estimators. From this point on, we generally do not highlight the dependency of \u03a3 G in S expl to simplify notation. First of all, note that by definition we always have the lower bound on the difference of CE:\nThe rest of the guarantees focus on the upper bounds for this difference. From the observation that H \u03a3, \u03a3 = H S, \u03a3 + 1 2 \u03a3 \u2212 S, K , we get the control (10) on the regret\nwhere all the MLE \u03a3 G depend only on G and S expl . The random variable G * is a function of S expl only, whereas G CV depends on both S val and S expl . Since S val and S expl are independent, then:\nIn the end, with e := E H \u03a3, \u03a3 G CV \u2212 H \u03a3, \u03a3 G * the expected regret, we have:"}, {"section_title": "Control in expectation", "text": "In this Section, we use the sparsity properties of the estimator K G CV as well as the statistical properties of \u03a3 \u2212 S val to obtain a more explicit control on the expected regret. In addition, we use a known concentration result to obtain an alternative control in expectation. The result (11) is completely agnostic of the way the matrices K G \u2208 S ++ p are defined as long as they depend on S expl only. To get an order of this control, however, we use the assumption that \u03a3 G is the graph constrained MLE defined in (7) . Let us first notice that we can ensure K G * \u2264 p \u03bb thanks to our penalised definition of (7) .\nWe call E max the union of the maximal edge sets in M, and d max = |E max | \u2264 p(p\u22121) 2 its cardinal. We underline here that, by convention, conditional correlation graphs do not contain self loops, hence the edge sets E never include any of the pairs {(i, i)} i=1,...,p . We then get the control (12) by using Cauchy-Schwartz's inequality in (11) ."}, {"section_title": "Proposition:", "text": "With the previously introduced notations, if the set E max is independent of the exploration empirical matrix S expl , we have:\nIn the case of our Composite procedure, by construction E max is a random variable depending on the exploration set. However (12) still holds by replacing\nWe can get an alternative order of the control by using known concentrations inequalities.\nProposition: By using the Theorem 4 of [34] , we get:\nWhere c is a constant independent of the problem.\nIn the end, with (13) and (14), we have two different upper bounds on e and can use the minimum one depending on the situation."}, {"section_title": "Control in probability", "text": "In this Section, we use the sparsity properties of the estimator K G CV as well as the concentration properties of \u03a3 \u2212 S val around 0 to obtain a control in probability (concentration inequality) on the regret. In addition to the controls in expectation we got in (11) and (12), there is in the CVCE a concentration dynamic based on the convergence rate of a Wishart random matrix towards its average. We call \u03a0 max the orthogonal projection on the set\n. That is to say, for any matrix\nis a standard Wishart random variable depending only on the validation data, hence independent of every matrix K G . Let P := P H \u03a3, \u03a3\u011c CV \u2212 H \u03a3, \u03a3\u011c * \u2264 \u03b4 be the probability that the regret is small. We get two different lower bounds (15) and (16) on P .\nProposition: With the previously introduced notations, the two following inequalities hold:\nMoreover, the results (15) and (16) hold when every probability is taken conditionally to the exploration data or, equivalently here, conditionally to S expl .\nIf we work conditionally to the exploration data, then max\nand E max are constants of the problem. In that case, the lower bound in (15) only depends on the dynamic of a standard Wishart W p (n val , I p ).\nSimilarly, the lower bound in (16) only depends on the convergence dynamic of some coefficients of S val towards the corresponding ones in \u03a3.\nThe bound in (16) has a less general formulation than (15) , since the S val \u2192 \u03a3 is a more specific dynamic than W \u2192 I p .\nOn the other hand, only the diagonal coefficients and those in E max need to be close, which can make a huge difference if p is very large and M contains only sparse graphs and make the bound (16) tighter."}, {"section_title": "EXPERIMENTS ON SYNTHETIC DATA", "text": "We show in this Section the shortcomings of the global problem (1) of [3] and [4] and of the local approach of [1] and [2] on synthetic data. We demonstrate that -when the data is not abundant -the solutions of GGMselect consistently reproduce the true distribution much better than any solution of the global problem (1) . In addition to being outperformed in KL divergence, the best solutions of (1) are also very connected, consequently more than the real graph. However, we also illustrate that the solutions of GGMselect are always very sparse, regardless of the real graph. In the end, we demonstrate that our selection criterion improves both the distribution reproduction and the graph recovery of the previous two methods."}, {"section_title": "The solutions missed by the global paradigm: a comparison of GLASSO and GGMselect", "text": "We start by comparing the two state of the art global and local paradigms, and show that the global paradigm misses crucial solutions when the number of observations is small. We use the scikit learn, see [35] , implementation of the GLASSO of [12] to solve problem (1) for any penalisation level \u03c1 and the R implementation of GGMselect, see [2] , to represent the [1] approach. We use an inverse-sparse covariance matrix \u03a3 fixed once and for all to generate a matrix of observations X. The same observations are provided to the two methods. On Fig. 2 , we compare the True CE H(\u03a3, \u03a3) of each estimated matrix as a function of the number of non-zero, off diagonal coefficients in their inverse K (complexity of the model).\nThe green dot is the MLE -computed as in (7) -under the constraints of the GGMselect graph. In the case of GLASSO, different solutions are obtained by changing the level of penalisation \u03c1 in Eq. (1). We call those solutions \u03a3 \u03c1 , indexed by their penalisation intensity \u03c1. They are represented by the blue curve on Fig. 2 . All of them are inverse-sparse and define a graph we call G(\u03c1). The orange curve is the path of the MLEs \u03a3 G(\u03c1) -computed as in (7) -refitted from those same graphs without the l 1 penalty of problem (1). They have the same inverse-sparsity as their raw solution counterparts, but do not have the extra-penalisation on the non-zero coefficients that every LASSO solution bears. The three columns correspond to graphs with different connectivity -illustrated by a random example on top of each column -and the two rows have different graph sizes, p = 30 and p = 50 respectively. For each simulation, the two methods were given the same n = 30 observations to work with, and each figure represents the average and standard deviation of 100 simulations.\nWe notice that the GGMselect solution is always very sparse. When the true graph is sparse, GGMselect outperforms the penalised likelihood problem (1) regardless of the penalty intensity. For large connected graphs, the most connected solutions of (1) can perform better than the GGMselect solution. However GGMselect is consistently better than the equally sparse problem (1) solution. The failure of GLASSO to reach the spot of GGMselect in the performances/complexity with any penalisation intensity -even when the MLE is refitted from the GLASSO graph without penalty -indicates that when n is small, the l 1 penalised likelihood problem (1) has difficulties selecting the most efficient edges. Additionally, the better performing solutions of GLASSO have many edges -usually much more than the real graph -which draws the focus away from the relevant ones and makes it difficult to get a qualitative reading of the graph. When the number of observations is small, it seems that GGMselect's numerical scheme allows it to find high performing sparse graphs that problem (1) never can. This is the type of solution we want, and the main reason why we choose to initialise our composite method from this point."}, {"section_title": "Conservativeness of the GGMselect criterion: an example with a hub", "text": "We identified that GGMselect produced high quality, very sparse solutions. We argue here that they might be too sparse for their own good. As discussed in Section 2.2, the numerical scheme of the GGMselect algorithm is based on a nodewise approach, and so is its model selection criterion. It penalises independently the degree of every node in the proposed graph. This makes it very unlikely to select graphs with a hub, i.e. a central node connected to many others. However recovering hubs is very important in conditional correlation networks. Genetic regulation networks for instance often feature hubs. With synthetic data, n = 30, p = 30, we encounter a \"soft cap\" effect, where it becomes very hard for GGMselect to propose a graph including a node of degree higher than 3. The penalty for such a node being too large to be compensated by the improved goodness of fit. On the other hand, we see on Fig. 3 that the Cross Validated Cross Entropy selects a graph which features the entire hub, and is in addition closer to the real graph regarding the remaining edges. Indeed, in the example of Fig. 3 , other edges than the ones forming the hub are also ignored by GGMselect. With such a behaviour of the model selection criterion when the number of observations n is small, the GGMselect graphs are hard to interpret, with many key connections potentially missing. Such observations motivated us to replace the GGMselect criterion with the Cross Validated Cross Entropy for graph selection. The next subsection proposes a quantitative comparison of the graphs selected by these two metrics."}, {"section_title": "The short-sightedness of the local model selection: a comparison of the GGMselect criterion and the CVCE", "text": "In this Section, we compare solely the model section metrics -and not the graph exploration schemes -on a fixed, shared, family of graphs. We demonstrate that our global approach to model selection yields graphs much closer to the original one and that reproduces the true distribution much better than the GGMselect criterion, which rejects the better, more connected graphs. We compare the graphs selected by our Cross Validated CE (CVCE) and the GGMSC when shown the same family of candidate graphs. We consider a given true graph (p = 30). We compute once and for all one GGMselect solution with n = 30 observations drawn from this graph. With these key graphs in hand, we build manually (without the exploration scheme of Fig. 1 ) a deterministic sequence of graphs. Starting from the Fully Sparse with no edges, we add one by one, and in an arbitrary order, the edges needed to reach the GGMselect graph. From there, in the same manner, we add the missing edges and remove the excess edges to reach the true graph. Finally, we add -still one by one, still in an arbitrary order -the remaining edges until the Fully Connected graph, with all possible edges. All the encountered graphs in this sequence constitute the fixed family of candidates to be assessed by the model selection criteria. For each simulation, we generate n observations and use them to compute the GGMSC and CVCE along the path. We make 1000 of those simulations. The GGMSC uses the full data freely, while the CVCE must split the n points best H( , ( ) ) Fig. 2 . Average performances as a function of the complexity for: the MLE from the GGMselect graph (green), the GLASSO solutions (blue) and the MLEs from the GLASSO graphs (orange). The average is taken over 100 simulations. In each simulation, n = 30 data points are simulated from a given true graph, different for each subfigure. The two rows of subfigures correspond to two different graph sizes, p = 30 and p = 50 vertices respectively. The three columns correspond to true graphs with different connectivity. At the top of each column, a graph illustrates the typical connectivity of the true graphs in said column.\nTrue graph Graph selected with the GGMselect criterion Graph selected by Cross Validation Fig. 3 . Graph selection in the presence of a hub. The first figure is the true graph. The second and third are the graphs respectively selected by the GGMSC and CVCE on the same fixed graph path going from the fully sparse to the fully connected, via the GGMselect graph and the true graph into the exploration covariance S expl , to compute the graph constrained MLE \u03a3 G (S expl ), and a validation covariance S val to evaluate them. This leads to different results depending on the split size. Let S train be the empirical covariance matrix built with the full data. We assess the performances of each graph G with the True CE (TCE) of the MLE built from S train under the constraints of G: H(\u03a3, \u03a3 G (S train )).\nSince there is a known true \u03a3 we actually compute the True KL KL(\u03a3, \u03a3 G (S train )). This metric differs from the TCE only by a constant, hence is equivalent when ranking methods, but offers a sense of scale since the proximity to 0 in KL is meaningful. Fig. 4 illustrates the behaviour on one simulation. The most noticeable trend is that the GGMSC (in green) advocates a much earlier stop than the CVCE (in red), which stops almost on the same graph as the TCE (in blue). Additionally, on that run, the graph selected by the CVCE is actually the true graph (in grey). Fig. 5 represents the results over all simulations. We compare the average and standard deviation of the performances (true KL, on the y axis) and complexity (number of edges, x axis) of the models selected by the CVCE with different exploration/validation splits (in shades of red), GGMSC (in green) and with the TCE (in blue). The three columns represent different number of available observations (n = 25, 40, 100) and the second row is a zoomed in view of the first. This quantitative analysis confirms that the GGMSC selects graphs that are way too sparse even when shown more complex graphs with better performances. With the performances measured in KL, relative improvement is meaningful, and we see the CVCE improving the GGMSC choice by a factor from 2 to 5, and being much closer to the oracle solution in terms of KL. Additionally, the graphs selected by CVCE are also much closer to the original one. This is especially true when a large fraction of the data (35% or 40% of the training data) is kept in the validation set. The same results are observed with two other oracle metrics: the l 2 recovery of the True \u03a3, \u03a3 \u2212 \u03a3 G (S train ) F , and the oracle nodewise regression\n(the oracle metric of the GGMselect authors [2] ). Those metrics also reveal that when the validation set is small (20%), the variance of the performances of CVCE increases and it can become less Fig. 4 . On a single simulation: evolution of and model selected by GGMSC (green), CVCE (red) and TCE (blue) along the fixed deterministic path. The true graph's position on that path is represented by a vertical grey line. GGMSC stops early whereas CVCE selects the true graph (the vertical grey line and the dashed red one are the same). Moreover, the CVCE graph is very close to the best graph in terms of True Cross Entropy.\nreliable depending on the metric. The Figures and details on these two metrics can be found in supplementary materials. This experiment illustrated how the model selection criterion of GGMselect can actually be very conservative, and even though the numerical scheme of the method explores interesting graph families, the model selection criterion might dismiss the more complex, better performing ones on them. This leads us to believe we can make substantial improvements by using the CVCE on a path built using the GGMselect solution as initialisation."}, {"section_title": "Execution time comparison", "text": "In this Section, we compare the runtimes of GLASSO, GGMselect and the Composite method for several values of p. For each p, 20 simulation are made, with n = p/2 observations each. This number of observations is an arbitrary heuristic to have both n < p and n increasing with p. TABLE 1 synthesises the results. The runtime and complexity of the Composite method depend linearly on the number of steps chosen by the user. As seen in Fig. 1 , this number of steps is the number of graphs that are constructed and evaluated. Ideally, this sequence of graphs should be just long enough to see the Oracle (or Out of Sample) performance improve as much as they can, and stop when they start deteriorating, when the point of overfitting is reached. In this experiment, the number of steps is chosen according to an heuristic depending on the number of edges in the initialisation graph with regards to p. The average number of steps over the simulations is also recorded in TABLE 1. The Composite method and GGMselect both include a model selection step, however GLASSO just returns one solution of Eq. (1) for one given value of the penalty parameter \u03c1. As a result, all three methods are not strictly comparable. This was corrected in this experiment: for every simulation, the GLASSO is run on a grid of \u03c1 with as many values as the number of estimated graphs by the Composite method. We call this the \"grid GLASSO\". TABLE 1 shows that GGMselect is faster than the other two methods by 1 and 2 orders of magnitude in average. The Composite method is faster than the grid of GLASSOs when the dimension is small, but suffers when the dimension goes above p = 100. The Composite algorithm has indeed a high complexity in p, it runs p \u00d7 n steps ordinary linear regression with p \u2212 2 features and computes then evaluates (p + 1) \u00d7 n steps graph constrained MLE of size p \u00d7 p each.\nThe algorithmic of GGMselect and GLASSO were very well optimised by their respective authors. This shows in the very fast GGMselect computations, making it a very efficient initialisation for our Composite method. However, the implementation of the Composite, see Fig. 1 , is naive and sequential. By running the linear regressions and LARS in parallel, and not re-calculating the MLE for the same graph several times, the performance would be greatly improved and closer to GLASSO."}, {"section_title": "EXPERIMENTS ON REAL DATA WITH THE COM-POSITE GGM ESTIMATION ALGORITHM", "text": "In this Section, we present two experiments with our composite method on real data. First, we demonstrate on brain imaging data from a cohort of Alzheimer's Disease patients that it recovers the known structures better than the classical local and global methods, while also having a better Out of Sample goodness of fit with the data. Then, we showcase how it is able to describe known dynamics between factors involved in Adrenal steroid synthesis on a database of Nephrology test subjects."}, {"section_title": "Experiment on Alzheimer's Disease patients", "text": "We first confirm our previous observations and demonstrate the performances of the complete numerical scheme of our composite procedure on real medical data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. We have p = 343 features, n = 92 different patients. The first 240 features are measures of atrophy (MRI) and glucose consumption (PET) in the 120 areas of the cortex defined by the AAL2 map. The next 98 are two descriptors of the diffusion, fractional anisotropy and mean diffusivity, followed in the 49 regions of the JHU ICBM-DTI-81 white matter atlas. The rest of the features are basic descriptions of the patient. the inferred matrices. To replace the TCE, we keep n = 18 patients aside as a test set to define a test empirical covariance matrix S test , whereas the n = 74 patients left constitute the train set, used to define S train . We evaluate an inversesparse covariance matrix built from S train with the negative Out of Sample Likelihood (OSL): H(S test , \u03a3 G (S train )). The OSL is less absolute than the True CE, but still quantifies with no bias the goodness of fit for real data. Additionally, we cannot use a KL divergence for scale reference anymore, see Section 7.1 for more details. The experiment run on the ADNI database is very simple: we compute the GGMselect solution and build our Composite GGM estimation procedure from it. To be fair, we also evaluate every graph our procedure encounters with the GGMSC, giving GGMselect a chance to change its mind if one of the new graphs were to fit its criterion better. In addition, we used the GLASSO algorithm of [12] to get the solutions of (1) for different penalty intensity."}, {"section_title": "Experiment", "text": ""}, {"section_title": "Comparison of GLASSO and GGMselect", "text": "We confirm the observations and conclusions of Section 4.1. Fig. 6 shows that, even with varying penalty intensity, GLASSO does not encounter any solution with an OSL as good as GGMselect. This indicates that the optimisation problem (1) cannot find high-performing sparse graphs in this concrete setting either. The path of GLASSO is interrupted before its completion as we have computational error with the scikit learn package at low penalty levels. We encounter such errors eventually no matter how we regularise and precondition the empirical covariance S. This means we do not get to see the more connected solutions of the GLASSO. This is not a problem since we already go far enough in the GLASSO path to reach unacceptably complex graphs: 6% of the \u223c 59000 possible edges, i.e. 3500 edges for a graph with 343 nodes. By stopping early, we only consider the reasonable solutions of the GLASSO. In that case, GGMselect has a clear advantage, proposing a solution with a better Out of Sample fit with the data and only 281 edges."}, {"section_title": "Comparison of GGMselect and the Composite GGM estimation algorithm", "text": "We represent the selected graphs on left panel of Fig. 7 , with the same conventions as Fig. 5 . Once again the GGMSC (green) selects a sparse model, with 281 edges over the \u223c 60k possible. All the reasonable validation fractions (from 10% to 30%) of the CVCE (shades of red) select one out of two graphs, with both better OSL than the GGMSC one and closer to the OSL-optimum on the path (blue). Those two graphs have 589 or 813 edges respectively. This indicates that many conditional correlations were potentially missed by GGMSC, and that the CVCE graphs may propose a more complete interpretation.\nFor a full comparison of the thee methods, the right panel of Fig. 7 is a zoomed out view that also includes the best model obtainable with problem (1) in terms of OSL (purple point). As we have seen, it is a very complex model with many edges. We visualise the successive improvements in Out of Sample Likelihood made first by GGMselect, with a sparser solution, then with our Composite GGM estimation procedure, with a more complete model. This experiment demonstrates the quantitative benefits of running the Composite algorithm in a High Dimension Low Sample Size setting.\nIn addition to those quantitative improvements, our method allows for a better qualitative interpretation of the disease. Fig. 8 represents, using the Colin 27 brain image of [36] and the MRView software of [37] , the graphs selected by GGMSC and CVCE (589 edges version), as well as the best GLASSO graph in OSL (\u223c 3500 edges). We recall that each of the methods estimates a large graph with p = 343 vertices, a mix of different modalities measured in different areas of the cortex. The full graph cannot be displayed on an image of the cortex. For the sake of clarity, we only represent sub-parts of this one graph. On Fig. 8 , only edges in-between the 120 MRI measures are represented. Additional views of the cortex can be found in supplementary materials. The GGMselect network is mostly composed of inter-hemispheric connections between symmetrical areas (hidden by the perspective in Fig. 8 , see the supplementary materials for different views). These mainly reflect the symmetry of the atrophy pattern and are less informative for understanding disease process. The intra-hemispheric connections have a better interpretation potential to explain the pathology. Our algorithm reveals many more of these correlations -for instance in parietal areas, which are thought to be key hubs in the disease process -promising a more interesting description of the pathology. The GLASSO solution on the other hand, proposes many edges, making even this simple subgraph unreadable. Similar observations can be made for connections in-between PET measures (see supplementary materials).\nAdditionally, Fig. 9 shows that the GGMselect graph features absolutely no edge between MRI and PET measures, effectively proposing a model in which there is no correlation whatsoever between anatomical and functional variables, a very unlikely and unsatisfactory description. Our method on the contrary recovers a reasonable amount of edges between those two modalities. GLASSO recovers a similar number of edges in this sub-part of the graph. However, Fig. 8 shows that it does so while having an extremely large number of edges in other regions of the graphs. Sparser GLASSO solution on the other hand, behave similarly to GGMselect and recover no edge linking MRI and PET measures, see supplementary materials. Of all these solutions, the Composite method proposes the most balanced.\nThese results suggest that our approach could be an interesting tool to study inter-regional and inter-modality dependencies in Alzheimer's Disease. This would need to be confirmed with larger populations of patients and more extensive experiments, which is out of the scope of the present paper and is left for future work."}, {"section_title": "Experiments on neprhology patients", "text": "In this Section, we compare qualitatively the methods in an environment with p < n. Although the Composite procedure was developed specifically for the case n < p, we demonstrate here that it still holds up to the state of the art outside of its intended application framework. We use a dataset of variables relevant to the adrenal steroidogenesis on a cohort of healthy test subjects. Adrenal steroid synthesis in childhood is a complex process involving an enzymatic cascade that transforms cholesterol into mineralocorticoids, glucocorticoids or androgens, depending on the enzymatic equipment of each zona of the adrenal gland. Even though most important ways of adrenal steroidogenesis are known, we now assess new related metabolite that may ask new questions regarding adrenal steroidogenesis. Thus, we analysed a pediatric cohort of n = 172 healthy volunteers aged from 3 months to 16 years old with blood count and LC-MS/MS adrenal steroid profile analysis (p = 35). The other solutions contain many more edges than any of the three matrices here. The models proposed by the three matrices have been compared to literature data for hematological parameters and steroidogenesis analysis. Regarding hematological analysis, both the Composite and GGMselect models confirm well known relations such as strong direct positive links between hemoglobin concentration (Hb) and red cells count (RBC); between hemoglobin concentration and mean corpuscular volume (WCV); between white cells (WBC) and platelet counts (PC); and a strong negative link between red cells count and mean corpuscular volume; between white cells count and age. The GLASSO solution did not show any of them. Regarding steroid metabolism, 11-\u03b21 hydroxylase (11 Ohase B1) and 21 hydroxylase (21 Ohase) activities, the Composite method and GGMselect reach the same conclusion: there is a strong positive direct link between enzymatic activities and the concentration of their corresponding alternate product. This is in accordance with common description of adrenal steroidogenesis process: decreased activity leads to an accumulation product of the alternative pathway. The GLASSO solution failed to show these relations. In the same way, GGMselect and the Composite method exhibit a negative link between the lack of 11-\u03b2 HSD type 2 (11b HSD2) activity (that catabolizes cortisol into cortisone) and the concentration of its product, cortisone (e). The sparse GLASSO fails to underline this link. All these data tend to show a better interpretation of steroids profile with the GGMselect and Composite solutions. Interestingly, these models also underline a new link: a strong positive link between 18-hydroxycorticosterone (18ohb) and 18-hydroxycortisol (18ohf) concentrations, two steroids that are supposed to be independently produced in two different zonas of the adrenal gland. This result could imply an alternative pathway in adrenal steroidogenesis that needs to be explored. The GGMselect and Composite graphs are mostly identical, although some of the conditional correlations are weaker in the Composite matrix. Among the subtle differences, two edges that are coherent with the state of the art, and are present in the GGMselect graph, were alleviated in the Composite matrix (resulting in invisible connections in Fig. 10 ): the link between the 18-oxocortisol (18oxof) and cortisol (f) concentrations, and the very strong negative link between the ratio cortisol/18-oxocortisol (F/18oxof) and 18-oxocortisol. The other very few additions and removals in the Composite model are hard to validate or disprove with the current state of the art. From a medical analysis point of view, all these results are preliminary and will have to be confirmed by more in depth studies. From a purely machine learning point of view, this example illustrates that the Composite method behaves appropriately when p < n. In this example, the GGMselect solution seems already acceptable, and the Composite procedure does not deviate too much from it.\nTo summarise these experimental studies, Section 5.1 showed the quantitative and qualitative improvements made by the Composite method on real data, in the High Dimension Low Sample size setting (n < p) the method was designed for. In this Section, with enough data available (p < n), hence outside the intended area of application, the qualitative analysis suggests that, running the Composite procedure does not provide additional benefits, but does not cause any loss either."}, {"section_title": "CONCLUSION", "text": "When it came to inferring conditional covariance graphs from a small number of observations, we were dissatisfied with the state of the art GGM methods. In this paper, we quantified the shortcomings in terms of goodness of fit, distribution reconstruction and interpretability of the local approach of [1] and the global optimisation problem of [3] , [4] .\nWe proposed a method composed of a structure learning algorithm coupled with model selection criterion. In the latter, the structure learning steps are a variation of the parallel nodewise linear regressions of [1] and the model selection steps guided by out of sample versions of the likelihood optimised in [3] and [4] . The validity of our method was demonstrated on synthetic and real data when n < p. Quantitatively, it consistently reached consequently lower KL divergences and better sparsistency than the aforementioned state of the art paradigms. A qualitative analysis on a neurological data set of real data, revealed that it better recovered the known dynamics of the field. An additional real data experiment, with p < n, suggested that the method did not cause any loss when used outside the intended scope of application.\nIn the future, optimising the numerical scheme will allow us to make further quantitative improvements. Such as lower execution times and better performances with less reliance on the initialisation."}, {"section_title": "PROOFS OF THE MAIN RESULTS", "text": ""}, {"section_title": "Basic Cross Entropy calculus for Gaussian vectors", "text": "In this Section, we offer details and commentary on the Cross Entropy manipulation with normal distributions and prove (2) and (3). The formula of the Cross Entropy H (p, q) is given by:\nThe likelihood p \u03b8 of a parametric distribution f \u03b8 with iid observations (X (1) , ..., X (n) ) is given by:\nf \u03b8 (X (i)) ) .\nLetf n = 1 n n i=1 1 x=X (i) be the empirical distribution of the sample (X (1) , ..., X (n) ), we see the connection between CE and likelihood: omit the constant p 2 ln(2\u03c0) from the calculations: In the end, we get (2):\nWith the observed data X := (X 1 , ..., X n ) T \u2208 R n\u00d7p , let S := 1 n X X T \u2208 S + p , the empirical covariance matrix. The log likelihood of any centred Gaussian distribution f \u03a32 is given by:\nwhere, as in (2), we omit the constant term p 2 ln(2\u03c0) from the calculations. In the end, we get (3):\nThe likelihood H f n , f \u03a32 follows a similar formula as the Cross Entropy between two normal distributions (2) . When S defines a non degenerate normal distribution, what we actually have is H f n , f \u03a32 = H (f S , f \u03a32 ). However, when n < p, S is singular and the density f S is not defined. The formula (3) still holds though, and we write H (S, \u03a3 2 ) := H f n , f \u03a32 since the formula is the same as (2) for H (\u03a3 1 , \u03a3 2 ).\nRemark When the density f S does exists, we have equality in the CE H f n , f \u03a32 = H (f S , f \u03a32 ), but not in the Entropies H f n ,f n = H (f S , f S ), as a consequence the KL divergences are different as well:\nand KL f n , f \u03a32 will never reach 0, since a normal distribution will tend to be closer to another normal distribution than to an empirical one, this is particularly true with n small and \u03a3 2 close to S. As a result, KL f n , f \u03a32 offers a poor sense of scale, since the value 0 cannot be used as a reference. For this reason, when we represent H (f Stest , f \u03a32 ) as we do in Fig. 7 , we do not use it under the form of a KL with 0 as its minimum for scale reference -as we do on synthetic data in Fig. 5 -since the only KL we can compute is the mostly irrelevant KL f n , f \u03a32 ."}, {"section_title": "Preliminary results for the model selection guarantees", "text": "To prove the controls we stated in Sections 3.2, 3.3 and 3.4, we need the two following lemmas. Lemma 1. Let S (\u03bb) := S + \u03bbI p . With K G := \u03a3 \u22121 G , where \u03a3 G is defined as in (7), we have:\nProof: Let \u03a0 G be the orthogonal projection on the edge set E G \u222a {(i, i)} p i=1 . That is to say, for any matrix M \u2208 R p\u00d7p , \u03a0 G (M ) i,j = M i,j 1 (i,j)\u2208E G \u222a{(i,i)} p i=1 . A property of the MLE is that \u03a0 G ( \u03a3 G ) = \u03a0 G (S (\u03bb) ), i.e. the matrices have the same values on the diagonal and the edge set, see [6] . Additionally, note that, because of the sparsity of K G , for any matrix M , we have M, K G = \u03a0 G (M ), K G . Then:\nSince K G * is defined from S expl uniquely, and independently of S val , we get\nFrom (18) and (19) we get:\nWhich is exactly the result of Eq. (11):\nAs we discussed in Section 3.3, to obtain Eq. (11), we only used the definitions of G CV for the upper bound and G * for the lower bound. Since we assume nothing on the model family M, those bounds are somewhat optimal in terms of the available information. Additionally, (11) is actually independent of how the symmmetric positive matrices { \u03a3 G } G\u2208M are defined as long as they are function only of S expl . They do not need to be associated with a different graph each, or with any graph for that matter. They do not need to be solutions of the MLE problem (7) and could be for example all the solutions on the path of solution of the l 1 \u2212penalised likelihood optimisation problem (1).\nTo get a more explicit control on the CVCE however, we need the assumption that \u03a3 G is the constrained MLE defined in (7) . Let \u03a3 \u221e := max i,j |\u03a3 ij |. We call E max the union of the maximal edge sets in M, d max = |E max | \u2264 p(p\u22121) 2 its cardinal and \u03a0 max the orthogonal projection on E max \u222a {(i, i)} p i=1 . We have: From which we finally get the result of (12):\nIf E max is dependent on the exploration data -because the graph family M was built from S expl for instance -we have:\n.\nWe get the control (13), the same as (12) but with an additional expectation term:\nIn order to prove (14) , we start by showing how the regret is bounded by operator norm \u03a3 \u2212 S val 2 . By tracial matrix Holder inequality:\nThen, using (11), we get:\nTo prove (14), we first recall Theorem 4 of [34] :\nTheorem 4 of [34] . Let X 1 , X 2 , ..., X n be i.i.d. weakly square integrable centered random vectors in a separable Banach space with norm . and \u03a3 be their covariance operator. If X is Gaussian, then there exist an absolute constant c, independent of the problem, such that:\nwhere . for operators denotes the operator norm associated with the vector norm . , that is to say:\n\u03a3u .\nIn our case, X \u223c N (0 p , \u03a3) is a Gaussian vector in the Banach space R p , with the euclidean norm X 2 , that verifies the integrability properties of the Theorem and whose covariance operator is the covariance matrix \u03a3. Hence the theorem can be applied. The operator norm for a symmetric positive matrix \u03a3 associated with the euclidean norm is also called the spectral norm, since it corresponds to the highest eigenvalue: \u03a3 2 = \u03bb max (\u03a3). For a Gaussian vector: Z \u223c N (0 p , I p ), we have:\nwhere n val W \u223c W p (I p , n val ) p is a standard Wishart matrix. Then we have:\nWe plug this result into (23) to obtain:\nWe end up with the control (15) by taking the probability in the previous expression:\nFor the second result, let \u03a0 G and \u03a0 max be the orthogonal projections on the edge sets E G \u222a {(i, i)} p i=1 and E max \u222a {(i, i)} p i=1 respectively. We have:\nHence we get, from (23), the logical implication:\nFrom which we get the control (16) by taking the probability of the events:\nWe underline that we obtain the two controls (15) and (16) directly from logical implications. Hence, they remain true when every probability is taken conditionally to any random variable, for instance the exploration data set, or the sufficient statistic built from it: S expl . "}, {"section_title": "Remark", "text": ""}]