[{"section_title": "Abstract", "text": "Abstract: Understanding structural changes in the brain that are caused by a particular disease is a major goal of neuroimaging research. Multivariate pattern analysis (MVPA) comprises a collection of tools that can be used to understand complex disease efxcfects across the brain. We discuss several important issues that must be considered when analyzing data from neuroimaging studies using MVPA. In particular, we focus on the consequences of confounding by non-imaging variables such as age and sex on the results of MVPA. After reviewing current practice to address confounding in neuroimaging studies, we propose an alternative approach based on inverse probability weighting. Although the proposed method is motivated by neuroimaging applications, it is broadly applicable to many problems in machine learning and predictive modeling. We demonstrate the advantages of our approach on simulated and real data examples."}, {"section_title": "Introduction", "text": "Quantifying population-level differences in the brain that are attributable to neurological or psychiatric disorders is a major focus of neuroimaging research. Structural magnetic resonance imaging (MRI) is widely used to investigate changes in brain structure that may aid the diagnosis and monitoring of disease. A structural MRI of the brain consists of many voxels, where a voxel is the three dimensional analogue of a pixel. Each voxel has a corresponding intensity, and jointly the voxels encode information about the size and structure of the brain. Functional MRI (fMRI) also plays an important role in the understanding of disease mechanisms by revealing relationships between disease and brain function. In this work we focus on structural MRI data, but many of the concepts apply to fMRI studies.\nOne way to assess group-level differences in the brain is to take a \"mass-univariate\" approach, where statistical tests are applied separately at each voxel. This is the basic idea behind statistical parametric mapping (SPM) [1] [2] [3] and voxel-based morphometry (VBM) [4, 5] . Voxel-based methods are limited in the sense that they do not make use of information contained jointly among multiple voxels. Figure 1 illustrates this concept using toy data with two variables, X 1 and X 2 . Marginally, X 1 and X 2 discriminate poorly between the groups, but perfect linear separability exists when X 1 and X 2 are considered jointly. Thus, there has been a shift away from voxel-wise methods to multivariate pattern analysis (MVPA) in the neuroimaging community. In general, MVPA refers to any approach that is able to identify disease effects that are manifested as spatially distributed patterns across multiple brain regions .\nThe goal of MVPA is often two-fold: (i) to understand underlying patterns in the brain that characterize a disease, and (ii) to develop sensitive and specific image-based biomarkers for disease diagnosis, the prediction of disease progression, or prediction of treatment response. Although the MVPA literature often uses terminology that suggests a causal interpretation of disease patterns in the brain, little has been done to formalize a causal framework for neuroimaging, with the notable exception of recent work by Weichwald et al. [62] . In this paper, we elucidate subtle differences between the two goals of MVPA and provide guidance for future implementation of MVPA in neuroimaging studies. We focus attention on the consequences of confounding on goal (i) and give a few remarks regarding goal (ii).\nConfounding of the disease-image relationship by non-imaging variables such as age and gender can have undesirable effects on the output of MVPA. In particular, confounding may lead to identification of false disease patterns, undermining the usefulness and reproducibility of MVPA results. We discuss the implications of \"regressing out\" confounding effects using voxel-wise parametric models, a widely used approach for addressing confounding, and propose an alternative based on inverse probability weighting.\nThe structure of this paper is the following. Section 2 provides a brief overview of the use of MVPA in neuroimaging with focus on the use of the support vector machine (SVM) as a tool for MVPA. In Section 3, we address the issue of confounding by reviewing current practice in neuroimaging and proposing an alternative approach. In Section 4, we illustrate our method using simulated data, and Section 5 presents an application to data from an Alzheimer's disease neuroimaging study. We conclude with a discussion in Section 6."}, {"section_title": "Multivariate pattern analysis in neuroimaging", "text": ". . , n, denote n independent and identically distributed observations of the random vector \u00f0Y, X T , A T \u00de T , where Y 2 f \u2212 1, 1g denotes the group label, e.g., control versus disease, X 2 R p denotes a vectorized image with p voxels, and A 2 R r denotes a vector of non-image variables such as age and gender. Suppose Y and A both affect X . For example, Alzheimer's disease is associated with patterns of atrophy in the brain that are manifested in structural MRIs. It is well known that age also affects brain structure [28] . Our primary aim is to develop a framework for studying multivariate differences in the brain between disease groups that are attributable solely to the disease and not to differences in non-imaging Marginally, X 1 and X 2 discriminate poorly between the groups, but perfect separability is attained when X 1 and X 2 are considered jointly.\nvariables between the groups. Thus, we advocate for creating balance between the groups with respect to non-imaging variables before performing MVPA. More formal details are given in the next section. A popular MVPA tool used by the neuroimaging community is the support vector machine (SVM) [29, 30] . This choice is partly motivated by the fact that SVMs are known to work well for high dimension, low sample size data [31] . Often, the number of voxels in a single MRI can exceed one million depending on the resolution of the scanner and the protocol used to obtain the image. The SVM is trained to predict the group label from the vectorized set of voxels that comprise an image. Alternatives include penalized logistic regression [32] as well as functional principal components and functional partial least squares [33, 34] . Henceforth, we focus on MVPA using the SVM.\nThe hard-margin linear SVM solves the contrained optimization problem\nWhere b 2 R, and v 2 R p are feature weights that describe the relative contribution of each voxel to the classification function. When the data from the two groups are not linearly separable, the soft-margin linear SVM allows some observations to be misclassified during training through the use of slack variables \u03be i with associated penalty parameter C. In this case, the optimization problem becomes\nwhere C 2 R is a tuning parameter that penalizes misclassification, and \u03be = \u00f0\u03be 1 , \u03be 2 , . . . , \u03be n \u00de T . For details about solving optimization problems (1) and (2) we refer the reader to Hastie et al. [35] .\nIn high-dimensional problems where the number of features is greater than the number of observations, the data are almost always separable by a linear hyperplane [36] . Thus, MVPA is often applied using the hard-margin linear SVM in (1) . For example, this is the approach implemented by: Bendfeldt et al. [37] to classify subgroups of multiple sclerosis patients; Cuingnet et al. [7] and Davatzikos et al. [8] in Alzheimer's disease applications; and Liu et al. [38] , Gong et al. [39] , and Costafreda et al. [40] for various classification tasks involving patients with depression. This is only a small subset of the relavant literature, which illustrates the widespread popularity of the approach.\n3 Multivariate pattern analysis and confounding"}, {"section_title": "Causal framework for descriptive aims", "text": "When the goal of MVPA is to understand patterns of change in the brain that are attributable to a disease, the ideal dataset would contain two images for each subject: one where the subject has the disease and another at the same point in time where the subject is healthy. Of course, this is the fundamental problem of causal inference, as it is impossible to observe both of these potential outcomes [41, 42] . In addition, confounding of the disease-image relationship presents challenges. Figure 2 depicts confounding of the Y-X relationship by a single confounder, A. Training a classifier in the presence of confounding may lead to biased estimation of the underlying disease pattern. This occurs when classifiers rely heavily on regions that are strongly correlated with confounders instead of regions that encode subtle disease changes [43] .\nFailing to address confounding in MVPA can lead to a false understanding of image signatures that characterize the disease and a lack of generalizability of the estimated classifier.\nLet X i \u00f0y\u00de denote the image that would have been observed had subject i been observed with group status Y i = y, possibly contrary to fact. Let F X\u00f0 \u2212 1\u00de and F X\u00f01\u00de denote the distributions of the counterfactual images X\u00f0\u2212 1\u00de and X \u00f01\u00de, respectively. Assume there exists a unique hyperplane in R p that maximally separates the counterfactural distributions in the sense that the centers of the two distributions lie on opposite sides of the hyperplane and the total combined mass on the \"wrong\" side of the hyperplane is minimized. The following notation will be useful for defining our target parameter. Let S be a map from the space of two distributions with the same support to this unique separating hyperplane, S : \u00f0F D , F D\u2032 \u00de7 !R p for distributions D and D\u2032. Define \u03b8 * = S \u00f0F X\u00f0 \u2212 1\u00de , F X\u00f01\u00de \u00de. Thus, \u03b8 * is the hyperplane that maximally separates the counterfactual image distributions, assuming this unique hyperplane exists, and is our target parameter. The target parameter \u03b8 * is inherently of interest in MVPA, which aims to understand patterns of change associated with a disease in a population of interest. Due to their cost, imaging studies often focus on populations that are at-risk for a particular disease. One example is the Alzheimer's Disease Neuroimaging Initiative (ADNI, http://www.adni.loni.usc.edu), which studied patients with mild cognitive impairment (MCI) and who were therefore at-risk for Alzheimer's disease. The MCI group was comprised of male and female patients across a wide age range with various other heterogeneities. Although non-imaging covariates are often easy to collect, the marginal parameter \u03b8 * is usually of interest in MVPA, as opposed to a parameter that is conditional on the non-imaging variables. Modeling conditional changes in the brain due to the disease would require more assumptions or stratification; the latter case reduces the sample size which may already be limited by budget constraints. We do not directly observe samples from F X\u00f0 \u2212 1\u00de and F X\u00f01\u00de , but under certain identifying assumptions, we can estimate the counterfactual distributions using the observed data. In particular, assume\n, \u00f0ii\u00de fX i \u00f01\u00de,\nAssumption \u00f0i\u00de is the usual consistency assumption, and \u00f0ii\u00de is the assmption of no unmeasured confounding, i.e., ignorability of exposure given measured confounders. Using \u00f0i\u00de and \u00f0ii\u00de,\nNote that the expectation is over the marginal distribution of A rather than the conditional distribution of A given Y = y. Thus, we reweight the integrand as follows:\nThe relationship between Y (disease) and X (image) is confounded by A (e.g., age), which affects both Y and X .\nwhere F * XjY = y is the conditional distribution of X given Y = y that results from averaging over a weighted version of the distribution of A . The weights are the inverse of the probability of being in observed group Y = y given confounders A, multiplied by the normalizing constant, pr \u00f0Y = y\u00de. We assume positivity, meaning pr \u00f0Y = yjA \u00de is bounded away from zero for all possible values of A. We have shown under assumptions \u00f0i\u00de and \u00f0ii\u00de that F X\u00f0 \u2212 1\u00de and F X\u00f01\u00de are identifiable from the observed data. Thus, our target parameter corresponds to \u03b8 * = S\u00f0F\nTo illustrate the effects of confounding on MVPA, consider a toy example with a single confounder A. Let X consist of two features, X = \u00f0X 1 , X 2 \u00de T , and define the corresponding potential outcomes, X \u00f0Y\u00de = fX 1 \u00f0Y\u00de, X 2 \u00f0Y\u00deg T . In the study of Alzheimer's disease, A might be age, Y an indicator of disease group, and X 1 and X 2 gray matter volumes of two brain regions. We generate N = 1, 000 independent observations from the generative model\nNote that model (3) has the property that Y and A are independent, so that A is not a confounder of the Y \u2212 X relationship. Next, we generate an additional N = 1, 000 independent observations from model (3) except with Y = 2Y * \u2212 1, where Y * \u2053 Bernoulli\u00f0A\u00de, so that A is a confounder of the Y \u2212 X relationship in this second sample. The first sample is plotted in the top three panels of Figure 3 and the linear SVM decision boundary estimated from the unconfounded data is drawn in gray in the top right panel. The Y \u2212 X relationship is confounded by A in second sample which is displayed in the bottom three panels of larger values of A a higher probability of being observed with Y = 1, and (ii) A has a decreasing linear effect on X 2 . The decision boundary estimated from the confounded sample is shown in black in the bottom right panel. Confounding by A shifts the estimated decision boundary and obscures the true relationship between the features X 1 , X 2 and outcome Y.\nThere is some variation in the definition of confounding in the imaging literature, making it unclear in some instances if, when, and why an adjustment is made. For example, some researchers recommend correcting images for age effects even after age-matching patients and contols [44] . In an age-matched study, age is not a confounder, and adjusting for its relationship with X is unnecessary. To address confounding, one approach proposed in the neuroimaging literature is to \"regress-out\" the effects of confounders from the image X . This is commonly done by fitting a (usually linear) regression of voxel intensity on confounders separately at each voxel and subtracting the fitted value at each location [44, 3] . The resulting \"residual image\" is then used in MVPA. Formally, the following model is fit using least squares, separately for each j = 1, . . . , p:\nwhere the j are assumed to be independent for all j. The least squares estimates b \u03b2 0, j and b \u03b2 1, j define the j th residual voxel,\nCombining all residuals gives the vector e X = \u00f0 e X 1 , e X 2 , . . . , e X p \u00de which is used as the feature vector to train the MVPA classifier. We henceforth refer to this method as the adjusted MVPA.\nA similar procedure is to fit model (4) using the control group only [44] . We refer to this approach as the control-adjusted MVPA. In applications where there is not a clear control group, i.e., comparing two disease subclasses, a single reference group is chosen. Let b \u03b2 Figure 4 . The first two plots of Figure 4 show the original feature X 2 and the adjusted MVPA feature, e X 2 . Although the residuals e X 2 are orthogonal to A by definition of least squares residuals, separability of the classes by e X 2 alone is much less than marginal separabilty of the classes on the original feature X 2 . This implies that using adjusted features for marginal MVPA may have undesirable consequences on discrimination accuracy and the estimated disease pattern. The right two plots in Figure 4 show that the contol-adjusted MVPA fails to remove the association between X 2 and A. Higher e X Figure 4 suggests that regression-based methods for addressing confounding are ineffective, motivating our proposed method described next. Dashed lines are the least squares fit of X 2 on A using the full and control-group data, respectively."}, {"section_title": "Inverse probability weighted classifiers", "text": "Having formally defined the problem of confounding in MVPA, we now propose a general solution based on inverse probability weighting (IPW) [45] [46] [47] [48] . We have already shown that weighting observations by the inverse probability of Y given A relates the observed data to the counterfactual distributions F X\u00f0 \u2212 1\u00de and F X\u00f01\u00de . The idea of weighting observations for classifier training is not new and in practice, applying IPW in this way is similar to weighting approaches that address dataset shift, a well-established concept in the machine learning literature [see, for example: [49] [50] [51] .\nThe inverse probability weights are often unknown and must be estimated from the data. One way to estimate the weights is by positing a model and obtaining fitted values for the probability that Y = 1 given confounders A, also known as the propensity score [52, 53] . Logistic regression is commonly used to model the propensity score, however, more flexible approaches using machine learning have also received attention [54] . Using logistic regression, the model would be specified as logit \u00bd pr\u00f0Y = 1j A\u00de = \u03b3 0 + A T \u03b3 1 .\nThen, the estimated inverse probability weights would follow as b w\nwhere expit \u00f0x\u00de is the inverse of the logit function, expit \u00f0x\u00de = e x =\u00f01 + e x \u00de, and z is the indicator function that takes value 1 if condition z is true and 0 otherwise. IPW can be naturally incorporated into some classification models such as logistic regression. Subjectlevel weighting can be accomplished in the soft-margin linear SVM framework defined in expression (2) by weighting the slack variables. Suppose the true weights w i are known. To demonstrate how IPW can be incorporated in the soft-margin linear SVM, we first consider approximate weights, T i , defined as subject i's inverse probability weight rounded to the nearest integer. For example, suppose subject i's inverse weight is 1=w i = 3.2; then, T i = 3. Next, consider creating an approximately balanced pseudo-population which consists of T i copies of each original subject's data, i = 1, . . . , n. This pseudo-population has n * = P n i = 1 T i observations. The soft-margin SVM in the pseudo-population is then\nHowever, in the approximately balanced pseudo-population, some of the \u00f0y \nIn fact, assuming all observations in the original n samples are unique, there are n unique constraints of the form y i \u00f0v T x i + b\u00de \u2265 1 \u2212 \u03be i and \u03be i \u2265 0, corresponding to the original i = 1, . . . , n samples. In addition, it is straightforward to show that (5) is equivalent to the original data soft-margin linear SVM with weighted slack variables in the objective function:\nsuch that :\nThe previous argument suggests one could use the true weights w i , rather than the truncated weights, T i . To our knowledge, an implementation of the SVM in R [55] that enables weighting the slack variables at the subject level does not exist. Subject-level weighting is available in the popular library libSVM [56] . Practitioners familiar with C++, MATLAB, or Python can implement the weighted SVM directly or by calling one of these languages from R using tools such as the \"Rcpp\" or \"rPython\" packages (rcpp.org, rpython.rforge.r-project.org). We are currently working on an R implementation of the inverse probability weighted SVM (IPW-SVM) that uses the true weights, w i . Development code is available at www.github.com/kalinn/ weightedSVM, and a full example is given at www.github.com/kalinn/IPW-SVM. The IPW-SVM algorithm only works when the data are not linearly separable. Otherwise, there are no slack variables in the optimization problem to weight. To provide intuition, suppose we are trying to separate two points in two-dimensional space. The optimization problem is then the hard-margin linear SVM formulation:\nsuch that :\nAdding copies of the data only adds redundant constraints that do not affect the optimization. This is a major issue in neuroimaging because the data often have more features than observations and are thus almost always linearly separable. When p \u2265 n, one idea would be to preprocess the data using a variable selection or other dimension reduction technique that accounts for possible confounding in the data. Then the IPW-SVM could be implemented on the reduced feature space. We are currently exploring alternatives to address confounding when p \u2265 n that retain the original interpretability of the features."}, {"section_title": "Simulation study", "text": "In this section we evaluate the finite sample performance of the IPW-SVM relative to the regression methods discussed in Section 3.1. We simulate training data from the following generative model with p = 100:\nwhere AE 1 is a p \u00d7 p identity matrix, and AE 2 is a p \u00d7 p matrix with 1s on the diagonal and 0.2s on all offdiagonal elements.\nFor each of M = 1, 000 iterations, we generate a sample of size N = 300 of the trajectory \u00f0A, X \u00f0\u2212 1\u00de\nT , X \u00f01\u00de T \u00de T from model (7). We train a SVM using the features X i \u00f0\u2212 1\u00de and X i \u00f01\u00de, i = 1, . . . N, and take the resulting SVM weights to be the \"true\" weight vector. Next, we simulate confounding by setting X = X \u00f0Y obs \u00de, where Y obs = 2Y * \u2212 1, Y * = Bernoulli \u00f0\u00c3 2 \u00de and\u00c3 = 0.5 A < 0.5 + A A \u2265 0.5 . Thus, subjects\nwith larger values of A are more likely to be observed with Y = 1. Finally, we create a test set with no confounding by A by generating a separate sample of N = 300 trajectories from model (7) and setting\nWe compare the performance of the IPW-SVM (IPW) to an unadjusted SVM (Unadjusted), a SVM after \"regressing out\" A from each feature separately using a linear model (Adjusted), and a SVM after \"regressing out\" A from each feature separately using a linear model fit in the group observed with Y obs = \u2212 1 (CN-Adjusted).\nSection 3.1 gives details about the regression-based adjustment methods. The full estimated inverse probability weights (i.e., non-truncated weights) are used for training the IPW-SVM. We use L 2 distance between the true and estimated weight vectors as one criterion for comparison. Figure 5 displays boxplots of the test accuracy and L 2 distance from the true weights for M = 1, 000 iterations. Results are presented relative to the unadjusted SVM for each iteration. That is, the results in the left plot of Figure 5 are obtained by dividing the L 2 distance of the IPW-SVM weights from the true weights by the L 2 distance of the unadjusted SVM weights from the true weights at each of the M iterations, and similarly for the other SVM methods. Similarly, the right plot in Figure 5 is obtained by dividing the test accuracy of the IPW-SVM by the test accuracy of the unadjusted SVM at each iteration, and similarly for the other SVM methods. Thus, improved performance over the unadjusted SVM is indicated by values below one in the left plot and values above one in the right plot of Figure 5 . Overall, the IPW-SVM performs the best with respect to the relative distribution of L 2 distance and attains the highest median test accuracy. The left plot of Figure 5 has been zoomed-in to better compare the interquartile ranges. The IPW-SVM resulted in more outliers than the regression-based adjustment methods. The IPW-SVM seems sensitive to very large weights which occurred by chance in several iterations. Using stabilized weights provided modest improvement (results not presented here) in this simulation study."}, {"section_title": "Application", "text": "The Alzheimer's Disease Neuroimaging Initiative (ADNI) (http://www.adni.loni.usc.edu) is a $60 million study funded by public and private resources including the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, the Food and Drug Administration, private pharmaceutical companies, and non-profit organizations. The goals of the ADNI are to better understand progression of mild cognitive impairment (MCI) and early Alzheimer's disease (AD) as well as to determine effective biomarkers for disease diagnosis, monitoring, and treatment development. MCI is characterized by cognitive decline that does not generally interfere with normal daily function and is distinct from Alzheimer's disease [57] . However, individuals with MCI are considered to be at risk for progression to Alzheimer's disease. Thus, studying the development of MCI and factors associated with progression to Alzheimer's disease is of critical scientific importance. In this analysis, we study the effects of confounding on the identification of multivariate patterns of atrophy in the brain that are associated with MCI. We apply the IPW-SVM to structural MRIs from the ADNI database. Before performing group-level analyses, each subject's MRI is passed through a series of preprocessing steps that facilitate betweensubject comparability. We implemented a multi-atlas segmentation pipeline [58] to estimate the volumes of p = 137 regions of interest (ROIs) in the brain for each subject. Each region is divided by the subject's total intracranial volume to adjust for differences in individual brain size, and these volumes are used as features for SVM training. The data we use here consist of n = 551 subjects, where n \u2212 1 = 224 are healthy controls and n 1 = 327 are patients diagnosed with MCI between the ages of 69 and 90. Neurodegenerative diseases are associated with atrophy in the brain, and thus the MCI group has smaller volumes on average in particular ROIs compared to the control group.\nAlthough the ADNI study was approximately matched on age and gender, a logistic regression of disease group on age in our sample returns an estimated odds ratio of 1.06 with 95% confidence interval \u00f01.02, 1.09\u00de, indicating that age is a possible confounder of the disease-image relationship. In this analysis, our focus is on identifying multivariate patterns in the brain that represent differences between the MCI and control groups, rather than predictive performance of the MVPA classifier. We perform four separate multivariate pattern analyses: (i) an unadjusted SVM, (ii) the adjusted SVM described in Section 3.1, (iii) the control-adjusted SVM described in Section 3.1, and the IPW-SVM described in Section 3.2 with estimated weights. We compare the results from these four methods to the estimated weight pattern from a SVM trained on a one-to-one age-matched subsample of the data. Figure 6 displays the top 10 weighted SVM features from the one-to-one age-matched data. Blue (red) regions correspond to negative (positive) weights. From top to bottom, Figure 7 displays the top 10 weighted SVM features from the IPW-SVM, unadjusted SVM, control-adjusted SVM, and adjusted SVM.\nIn general, all four methods perform similarly and return patterns that closely resemble the pattern learned from the matched data. Table 1 gives the L 2 distance between the estimated patterns and the matched-data SVM weight pattern. The IPW-SVM results in the least-biased weight pattern, and the regression-based adjustments demonstrate improvement over the unadjusted SVM.\nIt should be noted that although there is a significant disease-age relationship in the observed data, it is unlikely representative of the true disease-age relationship in the population because the MCI cases are over-sampled. Thus, MVPA classifiers trained to study disease patterns in the brain may demonstrate suboptimal performance when classifying new subjects in the population. Dataset shift methods, or models that integrate imaging biomarkers with knowledge of the true disease-age relationship in the target population, may be applied to improve any MVPA imaging biomarkers derived from the ADNI data. "}, {"section_title": "Method", "text": "Distance "}, {"section_title": "Discussion", "text": "We have proposed a framework for addressing confounding in MVPA that weights individual subjects by the conditional probability of observed class given confounders, i.e., inverse probability weighting (IPW). When the goal of MVPA is to estimate complex disease patterns in the brain, using IPW to address confounding is more principled that the current practice of \"regressing out\" confounder effects separately at each voxel without regard to the correlation structure of the data. When machine learning predictive models such as the SVM are used to perform MVPA, the IPW approach can recover underlying patterns in the brain associated with disease in the presence of measured confounding. We believe there are several advantages to addressing confounding in MVPA using IPW. First, as demonstrated by simulation results, IPW better estimates the target parameter of interest, which is the disease pattern that would be present under no confounding. In cases where a matched study is too expensive or otherwise infeasible, IPW methods will enable researchers to perform MVPA and obtain correct, reproducible results. Finally, IPW is simple and intuitive, and the general idea is well-established in the causal inference and statistics communities. Thus, future research aiming to perform inference on the estimated disease patterns can rely on existing theory. We are currently working on extending existing inference methods for MVPA [14, 59] to account for confounding.\nFurther exploring the effects of confounding on high-dimensional classification models is imperative for neuroimaging research and may greatly impact current practice in the field. An interesting avenue for future research would be to develop dimension reduction techniques that could be applied before or concurrently with MVPA that account for possible confounding in the data. Developing sensitivity analysis methods for assessing the role of confounding in MVPA also merits attention in future work.\nAlthough we have focused on the use of SVMs for binary classification problems, the idea of subjectlevel weighting to address confounding applies more generally to machine learning techniques for a variety of classification problems. In practice, incorporating subject-level weights into black box machine learning methods may not always be straightforward, and implementation of IPW might require specific tailoring to each problem. For example, generalizied versions of the propensity score exist for exposures with more than two groups and continuous exposures [60, 61] . Intuitively, it seems that applying generalized propensity score methods to multiclass classification problems or support vector regression for a continuous exposure is a natural extension of the methods proposed in this work. We believe these extensions are non-trivial and warrant focused attention in future research."}]