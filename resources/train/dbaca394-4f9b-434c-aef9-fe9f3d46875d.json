[{"section_title": "", "text": "In PISA 2006, U.S. 15-year-old students' average mathematics literacy score of 474 was lower than the OECD average of 498, and placed U.S. 15-yearolds in the bottom quarter of participating OECD nations, a relative position unchanged from 2003. Fifteen-year-old students in 23 of the 29 other participating OECD-member countries outperformed their U.S. peers. There was no measurable change in U.S. 15-year-olds' average mathematics literacy score between 2003 and 2006, in its relationship to the OECD average, or in its relative position to the countries whose scores increased or decreased."}, {"section_title": "Science", "text": "The 2007 TIMSS results showed that U.S. students' average science score was 539 for 4th-graders and 520 for 8th-graders. Both scores were above the TIMSS scale average, which is set at 500 for every administration of TIMSS at both grades, but neither was measurably different than the respective U.S. score in 1995. Fourth-graders in 4 of the 35 other countries that participated in 2007 (Singapore, Chinese Taipei, Hong Kong, and Japan) scored above their U.S. peers, on average; and 8th-graders in 9 of the 47 other countries that participated in 2007 (Singapore, Chinese Taipei, Japan, Korea, England, Hungary, the Czech Republic, Slovenia, and the Russian Federation) scored above their U.S. peers, on average. While there was no measurable change in the average score of U.S. 4th-graders or 8th-graders in science between 1995 and 2007, among the other 15 countries that participated in the 1995 and 2007 TIMSS at grade 4, the average science score increased in 7 countries and decreased in 5 countries; and among the other 18 countries that participated in both the 1995 and 2007 TIMSS at grade 8, the average science score increased in 5 countries and decreased in 3 countries. In PISA 2006, U.S. 15-year-old students' average science literacy score of 489 was lower than the OECD average of 500, and placed U.S. 15-year-olds in the bottom third of participating OECD nations. Fifteen-year-old students in 16 of the 29 other participating OECD-member countries outper-\nTwo international assessments measure aspects of science skills: TIMSS, which focuses on students' content knowledge of the science that they are likely to have been taught in school by grades 4 and 8, and PISA, which focuses on the cognitive skills or abilities of 15-year-old students to apply science knowledge and skills to a variety of materials with a real-life context. Whereas TIMSS is closely linked to the curricula of the participating countries, PISA assesses 15-year-olds' scientific literacy, which it defines as An individual's scientific knowledge and use of that knowledge to identify questions, to acquire new knowledge, to explain scientific phenomena, and to draw evidence-based conclusions about science-related issues, understanding of the characteristic features of science as a form of human knowledge and enquiry, awareness of how science and technology shape our material, intellectual, and cultural environments, and willingness to engage in science-related issues, and with the ideas of science, as a reflective citizen (OECD, 2006, p. 5). On account of these different aims, the two assessments ask students to perform different tasks. TIMSS asks 4th-and 8th-graders to complete a range of multiplechoice and constructed response questions that test their knowledge of specific science topics or content domainslife science, physical science, and Earth science at grade 4 and biology, chemistry, physics, and Earth science at grade 8. 35 In contrast, PISA poses multiple-choice questions and constructed response questions that ask students to identify scientific issues (e.g., recognize issues that are possible to investigate scientifically), explain phenomena scientifically (e.g., describe or interpret phenomena scientifically and predict changes), and use scientific evidence (e.g., identify the assumptions, evidence, and reasoning behind conclusions). PISA presents students with a range of exercises, based on materials that they are likely to encounter as young adults, such as a discussion of acid rain, a picture of erosion at the Grand Canyon, or the results of a controlled experiment. 36 It is important to note that PISA's science assessment was revised in 2006 to (1) more clearly distinguish knowledge about science as a form of human inquiry from knowledge of science, and (2) add to the framework components on the relationship between science and technology. In addition, to more clearly distinguish scientific literacy from reading literacy, the PISA 2006 science test items required less reading, on average, than did the science items used in earlier PISA surveys. Because of these changes, it is not possible to compare science learning outcomes from PISA 2006 with those of earlier PISA assessments as is done for reading and mathematics. The differences in science performance that readers may observe when comparing PISA 2006 science scores with science scores from earlier PISA assessments may be attributable to changes in the nature of the science assessment as much as to changes in actual student performance."}, {"section_title": "Introduction", "text": "The National Center for Education Statistics (NCES) is congressionally mandated to report on the state of education in the United States and other countries. 1 To carry out this mission, NCES participates in several international assessments to measure how the performance of U.S. students and adults compares with that of their counterparts in other countries. This special analysis looks closely at the information NCES has gathered from recent international studies that U.S. students have participated in: the Progress in International Reading Literacy Study (PIRLS), the Program for International Student Assessment (PISA), and the Trends in International Mathematics and Science Study (TIMSS). 2 This special analysis describes the most recent results from these international studies as well as trends in the results, when possible. It is organized by subject area into three parts-reading, mathematics, and science. For each subject area, the following topics are addressed: How does the performance of U.S. students compare with their peers in other countries? Which countries' students outperform U.S. students, and which have done so consistently? How has the performance of U.S. students changed over time? To what extent has the performance of U.S. students changed over time relative to their peers in high-performing countries? The three international studies examined in this special analysis periodically measure one or more dimensions of the performance of students at different ages or grade levels. PIRLS, sponsored by the International Association for the Evaluation of Educational Achievement (IEA) and first conducted in 2001, assesses the reading performance of 4th-graders every 5 years. PISA, sponsored by the Organization for Economic Cooperation and Development (OECD) and first conducted in 2000, assesses the reading, mathematics, and science literacy of 15-year-old students every 3 years. 3 And TIMSS, 1 Most recently mandated in the Education Sciences Reform Act of 2002. 2 This special analysis does not examine the results of international assessments of adult literacy, in which the United States has also participated. The reason for this is that the results of the 2002 Adult Literacy and Lifeskills Survey (ALL), the last assessment of adult literacy, have already been described in The Condition of Education 2006 special analysis (see http://nces.ed.gov/programs/coe/2006/analysis/index.asp), and the next assessment, the Program for the International Assessment of Adult Competencies (PIAAC), is not scheduled to be conducted until 2011. 3 While PISA assesses each subject area every 3 years, each assessment cycle focuses on one particular subject. In 2000, the focus was on reading literacy; in 2003, on mathematics literacy; in 2006, on science literacy. In 2009, the focus is on reading literacy again. sponsored by the IEA and first conducted in 1995, assesses the mathematics and science performance of both 4th-and 8th-graders every 4 years. 4 Although organized and run by two different international organizations, these three assessments all provide score results on a scale of 0 to 1,000, with a standard deviation of 100. 5 However, scores from different assessment studies (e.g., PISA and TIMSS) cannot be compared with each other directly because of differences in each study's purpose, subject matter, and assessed grade or age. Thus all comparisons in this special analysis are between countries that participated in the same study. It is important to point out here that the term \"country\" is used for simplicity's sake throughout this special analysis as a common name for the range of political entities that have participated in each study. In most cases, participating countries represent an entire nation state, as in the case of the United States. However, in some studies, participating countries represent parts of nation states. For example, several Canadian provinces participated separately in PIRLS 2006, instead of Canada. Likewise, England and Scotland regularly participate separately (instead of the entire United Kingdom) and Belgium regularly participates as two units (Flemish-speaking and French-speaking Belgium) in PIRLS and TIMSS. Similarly, Hong Kong and Macao, which are special administrative regions (SAR) of China, also participate independently. 6 Not all countries have participated in all three studies or in all administrations of a single study's assessments. 7 Table 1 lists the participating countries in the most recent administration of each assessment, and the supplemental tables 1-8 list participating countries in all administrations of the assessments. All three studies include both developed and developing countries; however, TIMSS and PIRLS have a larger proportion of developing countries participating than PISA does because PISA is principally a study of the member countries of the OECD-an intergovernmental organization of 30 developed countries.\nDifferences in the set of countries that participate in an assessment can affect how well the United States appears to do internationally when results are released. One reason for this is that average student performance in developed countries tends to be higher than in developing countries. As a result, the extent to which developing countries participate in an assessment can affect the international average of participating countries as well as the relative position of one country compared with the others. 8 To deal with this problem, none of the international assessments calculates an international \"average\" score based on results of all participating countries. Instead, PISA calculates an OECD average, for each PISA subject area, that is based only on the results of the OECD-member countries. All OECD-member countries participate in PISA; therefore, PISA ostensibly calculates this average based on a consistent group of countries. 9 TIMSS and PIRLS, on the other hand, do not calculate an average based on the results of any of the participating countries; they report results relative to the midpoint of each assessment's reporting scale, which they call the \"scale average.\" 10 All differences reported in this special analysis are statistically \"measurable\" or significant at a 95 percent level of confidence. All t-tests supporting this special analysis were done without adjustments for multiple comparisons. It is also important to note that the purpose of this special analysis is to provide descriptive information; thus, complex interactions and causal relationships have not been explored. Readers are cautioned not to make causal inferences based on the results presented here. 9 While all OECD-member countries' results are used to calculate PISA's OECD average, the number of countries used to calculate this average has actually increased. For example, in 2000, results for The Netherlands were not used to calculate the average because of its low response rates. In addition, between 2000 and 2003, the total number of countries in the OECD increased from 28 to 30 when the Slovak Republic and Turkey joined the OECD. 10 Although the IEA uses the label \"scale average,\" this is not actually a calculated average: it equals 500 because that is the \"average\" value on the assessment's 1,000-point scale. For a more detailed explanation of scale scores and scale averages, see appendix A.    1 Four Canadian provinces (Alberta, British Columbia, Ontario, and Quebec), as well as the Basque region of Spain, Dubai of the United Arab Emirates, and Massachusetts and Minnesota of the United States, participated in TIMSS 2007 as benchmarking participants and are not included in the total counts shown. 2 Although Mongolia and Morocco participated at both grades, the quality of the data for Mongolia was not well documented at both grades and there was a problem with the participation rates for Morocco at the eighth grade. For more information, see Olson, J.F., Martin, M.O., & Mullis, I.V.S. (Eds.). (2008). TIMSS 2007 Technical Report. Chestnut Hill, MA: TIMSS & PIRLS International Study Center, Boston College. 3 There are a total of 30 countries in the Organization for Economic Cooperation and Development (OECD). An OECD country is counted in the OECD total if it participated as a single/entire country (which is generally the case) or if it participated as one or more component jurisdictions of the country (e.g., England and Scotland as representing the United Kingdom). 4 PISA 2006 reading literacy results were not reported for the United States because of an error in printing the test booklets. NOTE: A bullet \"\uf097\" indicates participation in the particular assessment. An open bullet \" \" indicates jurisdictions that participated as \"benchmarking participants. "}, {"section_title": "Are International Assessment", "text": "Results Reliable, Valid, and Comparable? Since the United States began participating in comparative international assessments in the 1960s, the number and scope of international assessments have grown. In addition, the quality of the data they collect has improved because of the international adoption of ever more rigorous technical standards and monitoring, along with growing expertise in the international community relating to assessment design (National Research Council 2002, p. 9). The international organizations that sponsor international student assessments-the OECD and the International Association for the Evaluation of Educational Achievement (IEA)-go to great lengths to ensure that their assessment results are reliable, valid, and comparable among participating countries. 11 For each study, the sponsoring international organization verifies that all participating countries select a nationally representative sample of schools and, from those schools, randomly select either classrooms of a particular grade or students of the particular age or grade targeted by the assessment. To ensure comparability, target grades or ages are clearly defined. For example, in TIMSS, at the upper grade level, countries are required to sample students in the grade that corresponds to the end of 8 years of formal schooling, providing that the mean age of the students at the time of testing is at least 13.5 years. Moreover, comparisons by age are carefully chosen to ensure that students at the target age are enrolled in school at comparable rates across countries. For example, PISA elected to study 15-year-old students because 15 is the oldest age at which enrollment rates remain around 90 percent or higher in most developed countries, including the United States (OECD 2008, table C2.1). For students 16 and older, attendance is not universally compulsory. Not all selected schools and students choose to participate in the assessment; and certain students, such as some with mental or physical disabilities, may not be able to take the assessment. Thus the sponsoring international organizations check each country's participation rates (for schools and students) and exclusion rates (at the school level and within schools) to ensure they meet established target rates in order for the country's results to be reported. 12 In addition to international requirements and verification to ensure valid samples, the sponsoring international organizations require compliance with standardized 11 For complete details on the methods instituted to ensure data quality and comparability, see OECD 2008;Martin et al. 2007;and Olson, Martin, and Mullis 2008. 12 The United States also conducts its own nonresponse bias analysis if school participation rates are below 85 percent. For more details about nonresponse bias analysis, see appendix A. procedures for the preparation, administration, and scoring of assessments. Countries are required to send quality-control monitors to visit schools and scoring centers to report on compliance with the standardized procedures. Furthermore, independent international quality-control monitors visit a sample of schools in each country to ensure that the international standards are met. Results for countries that fail to meet the required participation rates or other international requirements are footnoted with explanations of the specific failures (e.g., \"only met guidelines for sample participation rates after substitute schools were included\"), shown separately in the international reports (e.g., listed in a separate section at the bottom of a Every participating country is involved in a thorough process of developing the assessment. The national representatives from each country review every test item to be included in the assessment to ensure that each item adheres to the internationally agreed-upon framework (the outline of the topics and skills that should be assessed in a particular subject area), and that each item is culturally appropriate for their country. Each country translates the assessment into their own language or languages, and external translation companies independently review each country's translations. A \"field test\" (a small-scale, trial run of the assessment) is then conducted in the participating countries to see if any items were biased because of national, social, or cultural differences. Statistical analyses of the item data are also conducted to check for evidence of differences in student performance across countries that could indicate a linguistic or conceptual translation problem. Problematic items may be dropped from the final pool of items or scaled differently. When this process is complete, the main assessment instruments are created. Each assessment \"instrument\" consists of the instructions, the same number of \"blocks\" of items (each block is a small set of selected items from the final pool of items), and a student background questionnaire. (Additional questionnaires are often prepared and administered to the students' teachers, parents, and/or school principal.) The instruments are then administered to the sampled students in each of the participating countries at comparable times. For more details on the development and administration of the international assessments, see the Technical Reports produced for each assessment. This section presents key findings from PIRLS, PISA, and TIMSS and is organized, by subject area, into three parts: reading, mathematics, and science. For each subject area, the assessments in that subject are described and their similarities and differences are highlighted. Then for each assessment in that subject, the U.S. average (mean) student score is compared with those of the other participating countries; the threshold or cutpoint score for \"the top 10 percent\" of U.S. students (technically the score of U.S. students at the 90th percentile) is compared with the cutpoint score for the top 10 percent of students in other countries; the cutpoint score for the bottom 10 percent of U.S. students is compared with the cutpoint score for the bottom 10 percent of students in other countries; the percentage of students reaching the highest international benchmark or highest level of proficiency set by each assessment is compared; and changes in these measures over time for the United States and for the top-scoring countries are examined, when possible. These data are described to provide a broader understanding of the performance of U.S. students compared to their peers around the world than is gained by just knowing average scores. Specifically, knowing the cutpoint scores for the top and bottom 10 percent of students tells us how well the highest and lowest performing students do in each country and how wide a range there is in student performance within each country. This range, in turn, provides important contextual information to understand whether a country that outperforms the United States scores higher on account of the performance of its students overall, of mostly its top-performing students, or of mostly its low-performing students. In contrast, comparing the percentage of students who reach the same international benchmarks or levels of proficiency provides information on the extent to which a country's education system brings student performance up to standardized levels that have been internationally established. After these data have been described for each assessment, you will find references for more detailed information and a brief synthesis of all the assessment results in the subject area."}, {"section_title": "Reading", "text": "Both PIRLS and PISA assess aspects of reading skills, but they differ in terms of whom they assess and what they assess. PIRLS assesses 4th-graders and is designed to reflect the curriculum of participating countries. PIRLS asks students to read two texts-either two literary texts (narrative fiction, generally drawn from children's books), two informational texts (typically excerpts from biographies, step-by-step instructions, or scientific or non-fiction materials), or one of each type. It then asks students about a dozen questions (both multiple-choice and open-ended \"constructed response\") about the texts that range from identifying the place, time, and actions of the main characters or events to interpreting how characters might feel, why events occurred, or what the passage means overall (e.g., does the story teach a lesson?). 13 PISA assesses 15-year-old students and does not explicitly focus on curricular outcomes; rather it focuses on cognitive skills and the application of reading to problems within a real-life context. Thus it presents students with a range of texts that they are likely to encounter as young adults, such as excerpts from government forms, brochures, newspaper articles, instruction manuals, books, and magazines. For each text, it then usually asks each student 3-5 questions (both multiple choice and constructed response) to measure the extent to which students can retrieve information, interpret a text, reflect on a text, and evaluate its author's rhetorical choices. 14 In years when PISA focuses on reading, students receive between 12 and 24 reading texts (depending on the particular cluster of items in their particular test booklet); when PISA focuses on mathematics or science, students receive about 7 reading texts."}, {"section_title": "Reading results for 4th-graders", "text": "In PIRLS 2006, the average U.S. 4th-graders' reading literacy score (540) was above the PIRLS scale average of 500, but below that of 4th-graders in 10 of the 45 participating countries, including 3 Canadian provinces (Russian Federation, Hong Kong, Alberta, British Columbia, Singapore, Luxembourg, Ontario, Hungary, Italy, and Sweden) 15 (table 2). The top 10 percent of U.S. 4th-graders scored 631 or higher, a cutpoint score below that of the top 10 percent of students in 8 countries. The bottom 10 percent of U.S. 4th-graders scored 441 or lower, a cutpoint score below that of the bottom 10 percent of students in 13 countries. 13 Examples of PIRLS items can be viewed at http://nces.ed.gov/ pubs2008/2008017_2.pdf. 14 Examples of PISA reading items can be viewed at http://www.oecd.org/ dataoecd/30/17/39703267.pdf, pages 288 to 291. 15 Countries are listed in rank order from highest to lowest score for countries outperforming the United States. 3 Did not meet guidelines for sample participation rates after substitute schools were included. NOTE: Jurisdictions are ordered on the basis of average scores, from highest to lowest. Reading literacy scores are reported on a scale from 0 to 1,000. A cutpoint score is the threshold score for an established level of performance. The cutpoint scores for students in the top 10 percent is the 90th percentile score within the jurisdiction. The cutpoint score for students in the bottom 10 percent is the 10th percentile score within the jurisdiction. The tests for significance take into account the standard error for the reported difference. Thus, a small difference between the United States and one country may be significant while a large difference between the United States and another country may not be significant. SOURCE: Mullis, I.V.S., Martin, M.O., Kennedy, A.M., and Foy, P. (2007) "}, {"section_title": "U.S. Students Compared to Peers in Other Countries", "text": ""}, {"section_title": "PIRLS has developed four international benchmarks", "text": "to help analyze the range of students' performance in reading within each participating country, with the highest, or Advanced, benchmark set at 625 score points. 16 For PIRLS 2006, students reaching the Advanced benchmark could interpret figurative language; integrate ideas across a text to provide interpretations of a character's traits, intentions, and feelings; and provide full text-based support for their interpretations. 17 In 2006, twelve percent of U.S. 4th-graders reached this benchmark (figure 1). Eight participating countries, including 3 Canadian provinces, had a higher percentage of 4th-graders reaching this benchmark, ranging from 19 to 15 percent: Singapore, Russian Federation, Alberta, Bulgaria, British Columbia, Ontario, England, and Luxembourg. Among the countries with a greater percentage of students than the United States reaching the Advanced benchmark, two did not have average student scores higher than the United States: Bulgaria and England. 18"}, {"section_title": "Change over time", "text": "Among the 28 countries that participated in both the 2001 and 2006 PIRLS assessments, the average reading literacy score increased in 8 countries and decreased in 6 countries (figure 2). In the rest of these countries, including the United States, there was no measurable change in the average reading literacy score between 2001 and 2006. The number of these countries that outperformed the United States increased from 3 in 2001 to 7 in 2006. 19 Three of the countries that outperformed the United States in 2006 (Hong Kong, the Russian Federation, and Singapore) had scored below the United States, on average, in 2001. In contrast, in 2 of the 6 countries where 4th-graders showed measurable declines (England and The Netherlands), 4th-graders outperformed their U.S. peers in 2001, but were not measurably different than their U.S. peers in 2006. PIRLS will be offered again in 2011. Results from the PIRLS 2006 assessment can be found in Baer et al. (2007;16 See figure 1 for the cut scores established for the other three international benchmarks. For details about all the international benchmarks, see , chapter 2. 17 The IEA set international benchmarks for PIRLS based on an analysis of score points. The score points for each benchmark remain the same across assessments; however, the configuration of items that define what students reaching a benchmark can do may vary slightly from one assessment to the next. For more details on the IEA's benchmarks and how they differ from PISA's levels of proficiency, see appendix A. 18 There was no measurable difference between the average student scores in the United States and in Bulgaria and England. 19 Luxembourg and the Canadian provinces of Alberta and British Columbia also outperformed the United States in 2006, but they did not participate in 2001. available at http://nces.ed.gov/pubsearch/pubsinfo. asp?pubid=2008017) and  available at http://timss.bc.edu/pirls2006/intl_rpt.html). For more information on PIRLS, see http://nces.ed.gov/surveys/pirls/. Reading results for 15-year-olds PISA 2006 reading literacy results are not reported for the United States because of an error introduced when the test booklets were printing. 20 Thus the reading literacy results described here come from the PISA 2000 and 2003. In PISA 2003, U.S. 15-year-old students' average reading literacy score of 495 was not measurably different than the OECD average of 494, and placed U.S. 15-year-olds in the middle third of participating OECD nations (table 3). Fifteen-year-old students in 9 of the 29 other participating OECD-member countries outperformed their U.S. peers (as did 15-year-olds in 2 of the 11 non-OECD countries that participated) in terms of average scores. U.S. 15-year-olds in the top 10 percent scored 622 or higher, a cutpoint score below that of the top 10 percent of students in 7 countries (all OECD countries). The bottom 10 percent of U.S. 15-year-olds scored 361 or lower, a cutpoint score below that of the bottom 10 percent of students in 9 OECD countries and 3 non-OECD countries. PISA has developed five levels of proficiency to help analyze the range of students' performance in reading within each participating country. 21 The highest level of proficiency identifies students who can complete sophisticated reading tasks, such as managing information that is difficult to find in unfamiliar texts; showing detailed understanding of such texts and inferring which information in the text is relevant to the task; and being able to evaluate critically and build hypotheses, draw on specialized knowledge, and accommodate concepts that may be contrary to expectations. For PISA 2003, the highest level of proficiency corresponds with a score at or above 625 score points. 22 20 In various parts of the U.S. PISA 2006 reading literacy assessment test booklet, students were incorrectly instructed to refer to the passage on the \"opposite page\" when students actually needed to turn back to the previous page to see the necessary passage. 21 See figure 3 for the cut scores for all five levels of proficiency in 2003. For details about all five levels, see OECD 2004, pp.272-79. 22 PISA has defined levels of proficiency based on specific student proficiencies. These specific student proficiencies remain the same across assessments; however, the score point threshold for students who demonstrate these specific student proficiencies may vary slightly from assessment to assessment. For more details on PISA's levels of proficiency and how they differ from the IEA's benchmarks, see appendix A. 9 0 9 9 7 8 4 9 7 7 7 3 9 3 6 9 1 9 9 6 8 3 9 7 6 7 6 9 4 5 8 6 9 9 5 7 6 9 6 5 7 2 9 4 4 6 1 8 4 3 7 2 9 3 3 6 6 9 2 3 6 7 9 1 2 4 0  Figure 2. Change in average PIRLS reading literacy scores of fourth-grade students in selected jurisdictions, by jurisdiction: 2001 to 2006 Average score was higher than the U.S. average score. Average score was lower than the U.S. average score. # Rounds to zero. 1 Hong Kong is a Special Administrative Region (SAR) of the People's Republic of China. 2 Met guidelines for sample participation rates only after substitute schools were included. 3 Did not meet guidelines for sample participation rates after substitute schools were included. 4 National Defined Population covers less than 95 percent of National Target Population. 5 National Target Population does not cover all of International Target Population because coverage falls below 65 percent. 6 National Defined Population covers less than 80 percent of National Target Population. 7 Nearly satisfied guidelines for sample participation rates after substitute schools were included. NOTE: Selected jurisdictions include those that participated in both PIRLS 2001 and PIRLS 2006. Average reading literacy scores are reported on a scale from 0 to 1,000. Tests for statistical significance take into account the standard errors for scores in both years. Because the size of standard errors can vary, a small difference between the scores of one jurisdiction may be significant while a larger difference between the scores of another jurisdiction may not be significant. Detail may not sum to totals due to rounding. SOURCE: Baer, J., Baldi, S., Ayotte, K., and Green, P. (2007)  In 2003, nine percent of U.S. 15-year-old students performed at this level (figure 3). The same 7 countries whose top 10 percent of students outperformed U.S. students had a higher percentage of 15-year-old students who performed at this level. The percentages of students performing at this level in countries that outperformed the United States ranged from 16 percent in New Zealand to 11 percent in Sweden.\nAmong the 32 countries that participated in both PISA 2000 and PISA 2003, the average reading literacy score increased in 4 countries and decreased in 5 countries (figure 4 \nAmong the 16 countries that participated in both the first TIMSS in 1995 and the most recent TIMSS in 2007, at grade 4, the average mathematics score increased in 8 countries, including in the United States, and decreased 30 See figures 5 and 6 for the cut scores established for all the international benchmarks. For details about the international benchmarks, see Mullis et al. (2008a), chapter 2. 31 The IEA set international benchmarks for TIMSS based on an analysis of score points. The score points for each benchmark remain the same across assessments; however, the configuration of items that define what students reaching a benchmark can do may vary slightly from one assessment to the next. For more details, see appendix A. in 4 countries (figure 7). Among the 20 countries that participated in both the 1995 and 2007 TIMSS at grade 8, the average mathematics score increased in 6 countries, including in the United States, and decreased in 10 countries (figure 8). Between 1995 and 2007 the average score of U.S. 4th-graders increased 11 score points (from 518 to 529). In 4 countries, the average score of 4th-graders increased more than in the United States during this time: England, Hong Kong, Slovenia, and Latvia. Increases in England (57 points) and Latvia (38 points) moved their 4th-graders from scoring below their U.S. peers in 1995 to scoring higher than their U.S. peers in 2007. Increases in Slovenia (40 points) and Hong Kong (50 points) did not change their standing relative to the United States. Scores decreased during this time for 4th-graders in Hungary (12 points), The Netherlands (14 points), Austria (25 points), and the Czech Republic (54 points). As a result, the performance of U.S. 4th-graders showed improvement relative to their peers in these countries. At grade 8, the U.S. average score increased 16 score points (from 492 to 508) between 1995 and 2007. In 2 countries, the average score of 8th-graders increased more than in the United States during this time: Colombia (47 points) and Lithuania (34 points). Neither of these countries outperformed the United States in 2007. Scores decreased during this time for 8th-graders in 10 countries, with the decreases ranging from 10 points in Hungary to 63 points in Bulgaria. The decreases in Australia (13 points), Sweden (48 points), and Bulgaria (63 points) were large enough that their 8th-graders' average scores in 2007 were below those of their U.S. peers, whereas in 1995 their students outperformed their U.S. peers. The next TIMSS assessment will be administered in 2011. More detailed results for TIMSS 2007 can be found in Gonzales et al. (2008; \nBecause  3 In Hungary, the percentage of eighth graders who had scores of 625 or higher was 9.825, which is reported as 10 percent when rounded. However, the top ten percent of eighth-grade performers actually had scores of 624 or higher.  Figure 7. Change in average TIMSS mathematics scores of fourth-grade students in selected jurisdictions, by jurisdiction: 1995 to 2007 Average score was higher than the U.S. average score. Average score was lower than the U.S. average score. 1 Hong Kong is a Special Administrative Region (SAR) of the People's Republic of China. 2 Met guidelines for sample participation rates only after substitute schools were included. 3 National Defined Population covers less than 90 percent of National Target Population (but at least 77 percent). 4 Did not satisfy sampling participation rates. 5 National Target Population did not include all of the International Target Population defined by the Trends in International Mathematics and Science Study (TIMSS). 6 Only Latvian-speaking schools (LSS) are included. 7 Nearly satisfied guidelines for sample participation rates only after substitute schools were included. 8 National Defined Population covers 90 percent to 95 percent of National Target Population. 9 Estimates for New Zealand have been computed for students taught in English only, which represents 98 to 99 percent of the student population. NOTE: Selected jurisdictions include those that participated in both TIMSS 1995 and TIMSS 2007. Average mathematics scores are reported on a scale from 0 to 1,000. Tests for statistical significance take into account the standard errors for scores in both years. Because the size of standard errors can vary, a small difference between the scores of one jurisdiction may be significant while a larger difference between the scores of another jurisdiction may not be significant.  Figure 8. Change in average TIMSS mathematics scores of eighth-grade students in selected jurisdictions, by jurisdiction: 1995 to 2007 Average score was higher than the U.S. average score. Average score was lower than the U.S. average score. 1 Met guidelines for sample participation rates only after substitute schools were included. 2 Hong Kong is a Special Administrative Region (SAR) of the People's Republic of China. 3 National Defined Population covers less than 90 percent of National Target Population (but at least 77 percent). 4 National Defined Population covers 90 percent to 95 percent of National Target Population. 5 National Target Population did not include all of the International Target Population defined by the Trends in International Mathematics and Science Study (TIMSS). 6 Sampling issues identified by TIMSS. See TIMSS 1995 report for details. 7 Nearly satisfied guidelines for sample participation rates only after substitute schools were included. 8 Did not satisfy guidelines for sample participation rates. NOTE: Selected jurisdictions include those that participated in both TIMSS 1995 and TIMSS 2007. Average mathematics scores are reported on a scale from 0 to 1,000. Tests for statistical significance take into account the standard errors for scores in both years. Because the size of standard errors can vary, a small difference between the scores of one jurisdiction may be significant while a larger difference between the scores of another jurisdiction may not be significant. Detail may not sum to totals due to rounding. SOURCE: International Association for the Evaluation of Educational Achievement (IEA), Trends in International Mathematics and Science Study (TIMSS), 1995 and 2007.   12* 9* 9* 7* 6* 6* 6* 6* 6* 5* Figure 9. Percentage distribution of 15-year-old students on PISA mathematics literacy scale, by proficiency level and jurisdiction: 2006 *p < .05. Percentage at proficiency Level 6 is measurably higher than U.S. percentage. # Rounds to zero. NOTE: Jurisdictions are ordered by the percentage of students scoring 669.30 or above on a scale from 0 to 1000. Students with a score of 420.07 or lower appear on the left side of the percentage distribution. These students performed at proficiency level 1 or below. At Level 1 students can answer questions involving familiar contexts where all relevant information is present and the questions are clearly defined. They are able to identify information and to carry out routine procedures according to direct instructions in explicit situations. They can perform actions that are obvious and follow immediately from the given stimuli. The Program for International Student Assessment (PISA) has defined six levels of proficiency based on specific student proficiencies. These specific student proficiencies remain the same across all PISA assessments; however, the score point threshold for students who demonstrate these specific student proficiencies may vary slightly from assessment to assessment. Because OECD proficiency levels are anchored by specific student proficiencies (i.e., by items not scores), the percentage distribution by level can be reported. Apparent differences may not be statistically significant. SOURCE: Organization for Economic Cooperation and Development (OECD), Program for International Student Assessment (PISA), 2006.   Figure 10. Change in average PISA mathematics scores of 15-year-old students in selected jurisdictions, by jurisdiction: 2003 to 2006 Average score was higher than the U.S. average score. Average score was lower than the U.S. average score. # Rounds to zero. 1 The Republics of Montenegro and Serbia were a united jurisdiction for the PISA 2003 assessment. NOTE: Selected jurisdictions include those that participated in both PISA 2003 and PISA 2006. Mathematics results from PISA 2000 cannot be compared with mathematics results from PISA 2006 due to changes in the assessment framework; as such, mathematics results from PISA 2003 served as the basis of comparison. Average mathematics literacy scores are reported on a scale from 0 to 1,000. Tests for statistical significance take into account the standard errors for scores in both years and the linking error across assessments. For more information on linking error, see appendix A. Because the size of standard errors can vary, a small difference between the scores of one jurisdiction may be significant while a larger difference between the scores of another jurisdiction may not be significant. \nAmong the 16 countries that participated in both the first TIMSS in 1995 and the most recent TIMSS in 2007, at grade 4, the average science score increased in 7 countries and decreased in 5 countries (figure 13). Among the 19 countries that participated in both the 1995 and 2007 TIMSS at grade 8, the average science score increased in 5 countries and decreased in 3 countries (figure 14). 38 The IEA set international benchmarks for TIMSS based on an analysis of score points. The score points for each benchmark remain the same across assessments; however, the configuration of items that define what students reaching a benchmark can do may vary slightly from one assessment to the next. For more details, see appendix A. Between 1995 and 2007, there was no measurable change in average score of U.S. 4th-graders in science. Average scores increased during this time in England, Hungary, Hong Kong, Slovenia, Iran, Latvia, and Singapore. Increases in Singapore (63 points) and Hong Kong (46 points) moved their 4th-graders from scoring below their U.S. peers in 1995 to scoring higher than their U.S. peers in 2007. Increases in Latvia (56 points), Hungary (28 points), and England (14 points) moved their 4th-graders from scoring below their U.S. peers in 1995 to being not measurably different than their U.S. peers in 2007. Scores decreased during this time for 4th-graders in Japan (5 points), Austria (12 points), Scotland (14 points), the Czech Republic (17 points), and Norway (27 points). Of these countries, only Austria changed its position relative to the United States; its 4th-graders moved from being not measurably different from their U.S. peers in 1995 to scoring below their U.S. peers in 2007. At grade 8, the U.S. average score in science did not measurably change between 1995 and 2007. Average scores increased during this time in Korea, Hong Kong, Slovenia, Colombia, and Lithuania. Two of the countries with increases in the average scores of their 8th-graders changed their position relative to the United States: Lithuania and Slovenia. An increase in Lithuania (55 points) moved their 8th-graders from scoring below their U.S. peers in 1995 to being not measurably different from their U.S. peers in 2007. An increase in Slovenia (24 points) moved their 8th-graders from being not measurably different from their U.S. peers in 1995 to scoring higher than their U.S. peers in 2007. Scores decreased during this time for 8th-graders in the Czech Republic (16 points), Norway (28 points), and Sweden (42 points). The decrease in Norway moved their 8th-graders from being not measurably different from their U.S. peers in 1995 to scoring below their U.S. peers in 2007. The decrease in Sweden moved their 8th-graders from scoring above their U.S. peers in 1995 to scoring below their U.S. peers in 2007. The next TIMSS assessment will be administered in 2011. More detailed results for TIMSS 2007 can be found in Gonzales et al. (2008; Figure 14. Change in average TIMSS science scores of eighth-grade students in selected jurisdictions, by jurisdiction: 1995 to 2007 Score is higher than U.S. score. Score is lower than U.S. score. # Rounds to zero.\nBecause of the revisions to the PISA science assessment for PISA 2006, direct comparisons of 2006 scores with those from 2000 and 2003 are not possible. Thus we cannot reliably say whether any country's scores increased, decreased, or were not significantly different in 2006 from the earlier administrations. Further details on the PISA science literacy results can be found in Baldi et al. (2007; available at http:// nces.ed.gov/pubsearch/pubsinfo.asp?pubid=2008016) and OECD (2007; available at http://www.oecd.org/ dataoecd/30/17/39703267.pdf). For more information on PISA, see http://nces.ed.gov/surveys/pisa/.    Figure 15. Percentage distribution of 15-year-old students on PISA science literacy scale, by proficiency level and jurisdiction: 2006 *p < .05. Percentage reaching advanced benchmark is measurably higher than U.S. percentage. # Rounds to zero. NOTE: Jurisdictions are ordered by the percentage of students scoring 633.33 or above on a scale from 0 to 1000. Students with a score of 409.54 or lower appear on the left side of the percentage distribution. These students performed at proficiency level 1 or below. At level 1, students have such a limited scientific knowledge that it can only be applied to a few familiar situations. They should be able to present scientific explanations that are obvious and follow concretely from given evidence. The Program for International Student Assessment (PISA) has defined six levels of proficiency based on specific student proficiencies. These specific student proficiencies remain the same across all PISA assessments; however, the score point threshold for students who demonstrate these specific student proficiencies may vary slightly from assessment to assessment. Because OECD proficiency levels are anchored by specific student proficiencies (i.e., by items not scores), the percentage distribution by level can be reported. Apparent differences may not be statistically significant In science, results from the most recent TIMSS assessment show that U.S. 4th-graders have fallen behind their peers in several countries, even though their average scores in science have not declined since the first TIMSS assessment in 1995. At the 8th grade, U.S. scores on the most recent assessment were also not measurably different than in 1995, but they fell behind those in the Russian Federation and they continue to lag behind those in Chinese Taipei, the Czech Republic, England, Hungary, Japan, Korea, and Singapore. The most recent PISA assessment suggests that U.S. 15-year-olds are not able to apply scientific knowledge and skills to realworld tasks as well as their peers in the majority of other OECD countries: in the most recent science assessment of 15-year-olds, the United States continued to perform below the OECD average.    Table A-3. Average TIMSS mathematics scores of fourth-grade students on combined mathematics scale, by  jurisdiction : 1995: , 2003: Jurisdiction 1995: 2003 Average score"}, {"section_title": "Synthesis of reading results", "text": "Only one country, Sweden, has consistently outperformed the United States in reading at grade 4 (see supplemental . NOTE: Jurisdictions are ordered by the percentage of students scoring 625 or above on a scale from 0 to 1,000. Students with a score of 407 or lower appear on the left side of the percentage distribution. These students performed at proficiency level 1 or below. At level 1, students can complete only the simplest reading tasks, such as locating a single piece of information, identifying the main theme of a text or making a simple connection with everyday knowledge. The Program for International Student Assessment (PISA) has defined five levels of proficiency based on specific student proficiencies. These specific student proficiencies remain the same across all PISA assessments; however, the score point threshold for students who demonstrate these specific student proficiencies may vary slightly from assessment to assessment. Because OECD proficiency levels are anchored by specific student proficiencies (i.e., by items not scores), the percentage distribution by level can be reported. Apparent differences may not be statistically significant. SOURCE: Organization for Economic Cooperation and Development (OECD), Program for International Student Assessment (PISA), 2003."}, {"section_title": "Mathematics", "text": "TIMSS and PISA both assess aspects of mathematical skills; however, they differ in terms of whom they assess and what they measure. TIMSS assesses 4th-and 8th-graders' knowledge of specific mathematical topics and cognitive skills that are closely linked to the curricula of the participating countries. PISA assesses 15-year-old students' mathematics literacy, which it defines as An individual's capacity to identify and understand the role that mathematics plays in the world, to make well-founded judgments and to use and engage with mathematics in ways that meet the needs of that individual's life as a constructive, concerned, and reflective citizen (OECD, 2006, p. 12). On account of these different aims, the two assessments ask students to perform different tasks. TIMSS asks 4thand 8th-graders to complete a range of multiple-choice and short constructed response questions that test their knowledge of specific mathematics topics or content domains-numbers (manipulating whole numbers and place values; performing addition, subtraction, multiplication, and division; and using fractions and decimals), geometric shapes and measures, and data display at grade 4; and numbers, algebra, geometry, and data and chance at grade 8. 27 In contrast, PISA does not focus exclusively on outcomes that can be directly linked to curricula, but instead emphasizes real world applications of mathematical knowledge. PISA's content domains are defined in terms of the manner in which mathematical knowledge is likely to be encountered in the larger world: space and shape, change and relationships, and uncertainty. Thus, PISA presents students with a variety of situations or problems in which, as young adults, they are likely to encounter numbers and mathematical concepts. These can range from estimating an area or comparing the best buy for a product to interpreting the statistics in a news report or government document. Most questions are multiplechoice, but there are some constructed response questions as well which could ask students to explain a mathematics principle, show their calculations, or explain their reasoning. 28 PISA complements information obtained from studies such as TIMSS because it addresses whether students can apply what they have learned both in and out of school. 29 It is important to note that PISA's mathematics assessment was revised in 2003 to broaden the content domains covered. (The PISA 2000 assessment did not cover uncertainty.) Because of this change, comparing mathematics literacy scores from PISA 2000 with those of later PISA assessments is inappropriate. Mathematics results for 4th-and 8th-graders The 2007 TIMSS results showed that U.S. students' average mathematics score was 529 for 4th-graders and 508 for 8th-graders (tables 4 and 5). Both scores were above the TIMSS scale average, which is set at 500 for every administration of TIMSS at both grades. The U.S. 4th-grade average score reflects the fact that U.S. 4th-graders performed above the TIMSS scale average in all three mathematics content domains (numbers, geometric shapes and measures, and data display). The U.S. 8th-grade average score reflects the fact that U.S. 8th-graders performed above the TIMSS scale average in two of the four mathematics content domains (numbers and data and chance) in 2007 (Gonzales et al. 2008, table 6). In algebra they did not score measurably different from the TIMSS scale average, and in geometry they scored 20 score points below the TIMSS scale average (Gonzales et al. 2008, table 7). Comparing the performance of U.S. students with the performance of their peers in other countries, 4th-graders in 8 countries (Hong Kong, Singapore, Chinese Taipei, Japan, Kazakhstan, Russian Federation, England, and Latvia) scored above their U.S. peers, on average (table  4). The top 10 percent of U.S. 4th-graders scored 625 or higher, a cutpoint score below that of the top 10 percent of students in 7 countries (all of which had higher average scores than the United States), while the bottom 10 percent scored 430 or lower, a cutpoint score below that of the bottom 10 percent of students in 6 countries (5 of which had higher average scores than the United States). Eighth-graders in 5 countries (Chinese Taipei, Korea, Singapore, Hong Kong, and Japan) scored above their U.S. peers, on average (table 5). The top 10 percent of U.S. 8th-graders scored 607 or higher, a cutpoint score below that of the top 10 percent of students in 6 countries, including the 5 countries with average scores higher than the U.S. average scores. The bottom 10 percent of U.S. 8th-graders scored 408 or lower, a cutpoint score below that of the bottom 10 percent of students in 4 countries (all of which had average scores higher than the United States).   Average score was higher than the U.S. average score. Average score was lower than the U.S. average score. # Rounds to zero. NOTE: Selected jurisdictions include those that participated in both PISA 2000 and PISA 2003. The Netherlands participated in the Program for International Student Assessment (PISA) in 2000, but its data are omitted from the PISA report because of technical problems with its sample. Average reading literacy scores are reported on a scale from 0 to 1,000. Tests for statistical significance take into account the standard errors for scores in both years and the linking error across assessments. For more information on linking error, see appendix A. Because the size of standard errors can vary, a small difference between the scores of one jurisdiction may be significant while a larger difference between the scores of another jurisdiction may not be significant. Detail may not sum to totals due to rounding.  Score is higher than U.S. score. Score is lower than U.S. score. Score is higher than U.S. score. Score is lower than U.S. score."}, {"section_title": "TIMSS has developed four international benchmarks", "text": "to help analyze the range of students' performance in mathematics within each participating country. 30 The Advanced benchmark is set at 625 score points for both grades. 31 Fourth-graders reaching the Advanced benchmark demonstrate a developing understanding of fractions and decimals and the relationship between them. They can select appropriate information to solve multi-step word problems involving proportions. They can formulate or select a rule for a relationship. They show understanding of area and can use measurement concepts to solve a variety of problems. They show some understanding of rotation. They can organize, interpret, and represent data to solve problems. Eighth-graders reaching the Advanced benchmark can organize information, make generalizations, solve non-routine problems, and draw and justify conclusions from data. They can compute percentage change and apply their knowledge of numeric and algebraic concepts and relationships to solve problems. They can solve simultaneous linear equations and model simple situations algebraically. They can apply their knowledge of measurement and geometry in complex problem situations. They can interpret data from a variety of tables and graphs, including interpolation and extrapolation. In 2007, ten percent of U.S. 4th-graders and 6 percent of U.S. 8th-graders reached the Advanced benchmark (figures 5 and 6). In comparison, 7 participating countries had a higher percentage of 4th-graders who reached this benchmark (ranging from 41 to 16 percent): Singapore, Hong Kong, Chinese Taipei, Japan, Kazakhstan, England, and the Russian Federation (the same 7 countries with higher cutpoints for their top 10 percent of students). A slightly different set of 7 participating countries had a measurably higher percentage of 8th-graders who reached this benchmark (ranging from 45 to 8 percent): Chinese Taipei, Korea, Singapore, Hong Kong, Japan, Hungary, and the Russian Federation (6 of these 7 countries had higher cutpoints for their top 10 percent of students)."}, {"section_title": "Mathematics results for 15-year-olds", "text": "In PISA 2006, U.S. 15-year-old students' average mathematics literacy score of 474 was lower than the OECD average of 498 (table 6), and placed U.S. 15-yearolds in the bottom quarter of participating OECD nations. Fifteen-year-old students in 23 of the 29 other participating OECD-member countries outperformed their U.S. peers (as did 15-year-olds in 8 of the 27 non-OECD countries that participated) in terms of average scores. A comparable pattern is evident when looking at the results of U.S. 15-year-olds in the top 10 percent of performance. The top 10 percent of 15-year-olds in the same 23 OECD countries and in 6 of the 8 non-OECD countries scored higher than the top 10 percent of U.S. 15-year-olds, who scored 593 or higher. In comparison, students in the top 10 percent in Chinese Taipei scored 677 or higher; in Hong Kong, 665 or higher; and in Korea, 664 or higher. Comparing the performances of the bottom 10 percent of students in each country, 18 OECD countries and 8 non-OECD countries scored higher than the United States, where the bottom 10 percent of 15-year-olds scored 358 or lower. PISA has developed six levels of student achievement to help analyze the range of student performance in mathematics within each participating country. 32 The highest level of proficiency in mathematics (above 669 score points for PISA 2006) identifies students who are capable of advanced mathematical thinking and reasoning and who demonstrate a mastery of symbolic and formal mathematical operations and relationships. They can conceptualize, generalize, and use information based on their investigations and modeling of complex problem situations. They can link different information sources and representations and can flexibly translate among them. They can develop new approaches and strategies for attacking novel situations. In 2006, one percent of U.S. 15-year-olds performed at this level (figure 9). Twenty-seven countries had a higher percentage of 15-year-olds who performed at this level; Chinese Taipei had the largest percentage of students at this level (12 percent). The percentage of students at this level in Korea, Hong Kong, Switzerland, Belgium, Finland, the Czech Republic, Liechtenstein, New Zealand, The Netherlands, and Japan ranged from 9 to 5 percent."}, {"section_title": "How Much Variation Is There Between Low and High Performers in Different Countries?", "text": "The variation between low and high performers within countries provides important contextual information to understand average assessment results by providing a measure of the range or inequality of scores within a country. A common way to examine such variation is to measure the difference between cutpoint scores at the 10th and 90th percentiles for a particular subject area. The cutpoint score at the 10th percentile is the highest score achieved by the bottom 10 percent of students and the cutpoint score at the 90th percentile is the lowest score achieved by the top 10 percent of students. Figures A-1 and A-2 show the dispersion of PISA 2006 mathematics literacy scores for 15-year-olds at the 10th and 90th percentiles. These are arranged by jurisdiction from smallest to largest gap and are shown separately for the OECD and non-OECD jurisdictions. Among the OECD countries, the differences in cutpoint scores ranged from 208 in Finland to 270 in Belgium, with the United States at 234 and an OECD average of 235. Among the non-OECD jurisdictions, the differences in cutpoint scores ranged from 117 in Azerbaijan to 277 in Israel. As shown in figures A-1 and A-2, there is no consistent relationship between a country's average score and the variation between its low-performing 15-year-olds (i.e., those scoring at or below the 10th percentile) and high-performing 15-year-olds (i.e., those scoring at or above the 90th percentile) in mathematics literacy. Some countries with relatively high average scores have a relatively large gap between their low and high performers (e.g., Chinese Taipei and Switzerland), while others have a relatively small gap (e.g., Finland and Canada). Similarly, some countries with relatively low average scores have a relatively large gap between their low and high performers (e.g., Argentina and Bulgaria), while others have a relatively small gap (e.g., Indonesia and Mexico). Finland and Chinese Taipei were among the highestperforming countries in mathematics literacy. Fifteenyear-olds in these two countries (along with Korea and Hong Kong-China) scored higher, on average, than all other countries on the mathematics literacy scale but did not measurably differ from each other. However, the relationship of low and high performers in each country was different. In Finland, the cutpoint scores at the 10th and 90th percentile were 444 and 652, respectively, for a difference of 208 points. In Chinese Taipei, the cutpoint scores at the 10th and 90th percentile were 409 and 677, respectively, for a difference of 268 points. Thus, relative to Finland, the high overall average score of 15-year-olds in Chinese Taipei can be attributed more to the performance of its very high performing students; whereas Finland's high average score can be attributed more to the performance of students across the distribution of low to high performing students."}, {"section_title": "Science results for 4th-and 8th-graders", "text": "The 2007 TIMSS results showed that U.S. students' average science score was 539 for 4th-graders and 520 for 8th-graders (tables 7 and 8). Both scores were above the TIMSS scale average, which is set at 500 for every administration of TIMSS at both grades. The 4th-grade average score reflects the fact that U.S. 4th-graders' performed above the TIMSS scale average in all three science content domains (life science, physical science, and Earth science) in 2007 (Gonzales et al. 2008, table 14). The 8th-grade average score reflects the fact that U.S. 8th-graders performed above the TIMSS scale average in three of the four science content domains (biology, chemistry, and Earth science) but in physics they scored not measurably different from the TIMSS scale average in 2007 (Gonzales et al. 2008, table 15). Fourth-graders in 4 countries (Singapore, Chinese Taipei, Hong Kong, and Japan) scored above their U.S. peers, on average (table 7). The top 10 percent of U.S. 4th-graders scored 643 or higher, a cutpoint score below that of the top 10 percent of 4th-graders in Singapore and Chinese Taipei, while the bottom 10 percent scored 427 or lower, a cutpoint score below that of the bottom 10 percent of students in 7 countries. These 7 countries include the 4 with higher average scores, 2 countries with average scores that are not measurably different than the U.S. score, and 1 country with an average score lower than the U.S. score. Eighth-graders in 9 countries (Singapore, Chinese Taipei, Japan, Korea, England, Hungary, the Czech Republic, Slovenia, and the Russian Federation) scored above their U.S. peers, on average (table 8). The top 10 percent of U.S. 8th-graders scored 623 or higher, a cutpoint score below that of the top 10 percent of 8th-graders in 6 countries (all of which had higher average scores), while the bottom 10 percent scored 410 or lower, a cutpoint score below that of the bottom 10 percent of students in 8 countries (all of which had higher average scores)."}, {"section_title": "TIMSS has developed four international benchmarks to", "text": "help analyze the range of students' performance in science within each participating country. 37 As in mathematics, 37 See figures 11 and 12 for the cut scores established for all the international benchmarks. For details about the international benchmarks, see Mullis et al. (2008b), chapter 2.   5 National Defined Population covers less than 90 percent of National Target Population (but at least 77 percent). 6 Kuwait tested the same cohort of students as other countries, but later in 2007, at the beginning of the next school year. NOTE: Jurisdictions are ordered on the basis of average scores, from highest to lowest. Science scores are reported on a scale from 0 to 1,000. A cutpoint score is the threshold score for an established level of performance. The cutpoint scores for students in the top 10 percent is the 90th percentile score within the jurisdiction. The cutpoint score for students in the bottom 10 percent is the 10th percentile score within the jurisdiction. The tests for significance take into account the standard error for the reported difference. Thus, a small difference between the United States and one country may be significant while a large difference between the United States and another country may not be significant. SOURCE: Gonzales, P., Williams, T., Jocelyn, L., Roey, S., Kastberg, D., and Brenwald, S. the Advanced benchmark is set at 625 score points for both grades. 38 Fourth-graders reaching the Advanced benchmark demonstrate the knowledge and skills for beginning scientific inquiry. They demonstrate some understanding of Earth's features and processes and of the solar system. They can communicate their understanding of structure, function, and life processes in organisms and classify organisms according to major physical and behavioral features. They demonstrate some understanding of physical phenomena and properties of common materials. Eighth-graders reaching the Advanced benchmark demonstrate a grasp of some complex and abstract scientific concepts. They can apply knowledge of the solar system and of Earth features, processes, and conditions, and apply understanding of the complexity of living organisms and how they relate to their environment. They show understanding of electricity, thermal expansion, and sound, as well as the structure of matter and physical and chemical properties and changes. They show understanding of environmental and resource issues. Students at this level understand some fundamentals of scientific investigation and can apply basic physical principles to solve some quantitative problems. They can provide written explanations to communicate scientific knowledge. In 2007, fifteen percent of U.S. 4th-graders and 10 percent of U.S. 8th-graders reached the Advanced benchmark (figures 11 and 12). In comparison, 2 participating countries had a higher percentage of 4th-graders reaching this benchmark (Singapore, with 36 percent reaching this benchmark, and Chinese Taipei, with 19 percent); and 6 participating countries had a higher percentage of 8th-graders reaching this benchmark (ranging from 32 to 13 percent): Singapore, Chinese Taipei, Japan, England, Korea, and Hungary."}, {"section_title": "How Much Does Performance Within the United States Vary by School Poverty?", "text": "As a measure of school poverty, TIMSS asked principals at public schools to report the percentage of students at the school eligible to receive free or reduced-price lunch through the National School Lunch Program. This is a federally assisted meal program that provides nutritionally balanced, low-cost or free lunches to eligible children each school day. TIMSS compares mathematics and science achievement results of students from schools with various poverty levels with the TIMSS scale average and the U.S. national average. In both mathematics and science, the average score of U.S. 4th-graders in the highest poverty public schools (at least 75 percent of students eligible for free or reducedprice lunch) in 2007 (479 in mathematics and 477 in science) was lower than the TIMSS scale average (500); the average scores of 4th-graders in each of the other categories of school poverty were higher than the TIMSS scale average (data not shown). The average score of U.S. 4th-graders in the lowest poverty public schools (less than 10 percent of students eligible for free or reduced-price lunch) in 2007 (583 in mathematics and 590 in science) was also higher than the U.S. national average (529 in mathematics and 539 in science). At 8th grade for both mathematics and science, the average score of U.S. students in the highest poverty public schools in 2007 (465 in mathematics and 466 in science) was lower, on average, than the TIMSS scale average (500) (data not shown). On the other hand, U.S. 8th-graders attending public schools with fewer than 50 percent of students eligible for free or reduced-price lunch scored higher than the TIMSS scale average. The average score of U.S. 8th-graders in the lowest poverty public schools in 2007 (557 in mathematics and 572 in science) was also higher than the U.S. national average (508 in mathematics and 520 in science)."}, {"section_title": "Science results for 15-year-olds", "text": "In PISA 2006, U.S. 15-year-old students' average science literacy score of 489 was lower than the OECD average of 500 (table 9), and placed U.S. 15-year-olds in the bottom third of participating OECD nations. Fifteenyear-old students in 16 of the 29 other participating OECD-member countries outperformed their U.S. peers (as did 15-year-olds in 6 of the 27 non-OECD countries that participated) in terms of average scores. U.S. 15-year-olds in the top 10 percent scored 628 or higher, a cutpoint score below that of the top 10 percent of students in 9 OECD and 4 non-OECD countries. In these 13 countries, cutpoint scores for the top 10 percent of students ranged from 673 in Finland to 640 in Estonia. The bottom 10 percent of U.S. 15-year-olds scored 349 or lower, a cutpoint score below that of the bottom 10 percent of students in 21 OECD and 9 non-OECD countries. PISA has developed six levels of student achievement to help analyze the range of student performance in science within each participating country. 39 For PISA 2006, the highest two levels of proficiency in science (above 633 score points) denote students who can identify the scientific components of many complex life situations, apply both scientific concepts and knowledge about science to these situations, and can compare, select and evaluate appropriate scientific evidence for responding to life situations. They can use well-developed inquiry abilities, 39 See figure 15 for the cut scores for all six levels of proficiency. For details about all six levels, see OECD 2007, pp. 42-44. link knowledge appropriately and bring critical insights to situations. They can construct explanations based on evidence and arguments based on their critical analysis. In 2006, nine percent of U.S. 15-year-olds performed at the highest two levels (figure 15). Thirteen countries had a higher percentage of 15-year-olds who performed at the highest two levels, with the largest percentage in Finland (21 percent). The percentage of students at the highest two levels in New Zealand, Hong Kong, Japan, Chinese Taipei, Australia, Canada, United Kingdom, The Netherlands, Slovenia, Liechtenstein, Germany, and the Czech Republic ranged from 18 to 12 percent."}, {"section_title": "Supplemental Tables", "text": ""}, {"section_title": "Standard error", "text": "Average score\nAverage score\nTIMSS scale average 500 0.0 500 0.0 500 0.0 Table A-4. Average TIMSS mathematics scores of eighth-grade students on combined mathematics scale, by  jurisdiction : 1995: , 1999: , 2003: Jurisdiction 1995: 1999: 2003 Average score  1995, 1999, 2003-Continued Jurisdiction 199519992003 Average score   1995, 1999, 2003Jurisdiction 199519992003 Average score  Table A-7. Average TIMSS science scores of eighth-grade students on combined science scale, by jurisdiction : 1995, 1999, 2003-Continued Jurisdiction 199519992003 Average score These technical notes describe the various issues that are important to keep in mind when interpreting sampling data and the various procedures governing the collection and analysis of assessment data. For detailed information about NCES datasets, see http://nces.ed.gov/surveys/."}, {"section_title": "A.1 Limitations of sampled data", "text": "Estimating the achievement of the total population or subpopulations from an assessment based on a sample of the entire population requires consideration of several factors before the results become meaningful. However conscientious an organization may be in collecting assessment data from a sample of a population, there will always be the possibility of nonsampling errors (errors made in the collection and processing of data) and some sampling errors (the margin of error in estimating the achievement of the actual total population or subpopulation because the data are available from only a portion of the total population)."}, {"section_title": "Nonsampling errors", "text": "\"Nonsampling error\" is a term used to describe variations in the estimates that may be caused by population coverage limitations, nonresponse bias, and measurement error, as well as data collection, processing, and reporting procedures. The sources of nonsampling errors are typically problems such as unit and item nonresponse, the differences in respondents' interpretations of the meaning of questions, response differences related to the particular time the assessment was conducted, and mistakes in data preparation. Sections A.2 through A.5 describe the international policies and procedures put in place to minimize nonsampling errors. Section A.11 describes NCES's policy of nonresponse bias analysis."}, {"section_title": "Sampling errors", "text": "Sampling errors occur when a discrepancy between a population characteristic and the sample estimate arises because not all members of the target population are sampled for the survey. The margin of error or the magnitude of sampling error depends on several factors, such as the amount of variation in the responses, the size and representativeness of the sample, and the size of the subgroup for which the estimate is computed. The magnitude of this margin of error is measured by what statisticians call the standard error of an estimate. The standard error for each estimate in this special analysis was calculated in order to determine the \"margin of error\" for each estimate. An estimate with a smaller standard error provides a more reliable estimate of the true value than an estimate with a higher standard error. The standard errors for all the estimated average scores, cutpoint scores, and percentages reported in the figures and tables of the special analysis can be found on The Condition of Education website at http://nces.ed.gov/ programs/coe."}, {"section_title": "Analysis and interpretation", "text": "Due to standard errors, caution is warranted when drawing conclusions about the achievement results estimated for one population in comparison to another or whether a time series of achievement results is increasing, decreasing, or staying about the same. Although one estimate of the achievement results may be larger than another, a statistical test may reveal that there is no measurable difference between the two estimates due to their uncertainty. Whether differences in averages (means) or cutpoint scores are statistically significant can be determined by using the standard errors of the estimates. When differences are statistically significant, the probability that the difference occurred by chance is usually small; about 5 times out of 100. For this special analysis, differences between means or cutpoint scores (including increases or decreases) are stated only when they are statistically significant. To determine whether differences reported are statistically significant, two-tailed t tests, at the .05 level of significance, were used. In addition, the t test formula for determining statistical significance was adjusted when a linking error term needed to be accounted for (see below for more on linking errors, under A.8). No multiple comparisons (Bonferroni adjustments) were used in this special analysis (see below for more on past significance tests, under A.8 and A.9)."}, {"section_title": "A.2 International requirements for sampling, data collection, and response rates", "text": "To provide valid estimates of student achievement and characteristics, the sample of students for each assessment is selected in a way that represents the full target population in each jurisdiction. The international desired population or full target population in each jurisdiction is carefully defined for each study. This international desired population includes all students in the target grade or age range. All three assessments require a minimum of 150 schools to participate and have minimum student sample sizes: PIRLS and PISA require a minimum of 4,500 students, and TIMSS requires a minimum of 4,000 students per grade. To realize these target sample sizes for a nationally representative sample of students, samples are drawn with two substitute schools identified for each sampled school. Substitute schools can replace a sampled school if the sampled school refuses to participate. The specific procedures for drawing the sample and for using substitute schools differ by study and even by administration: see the appropriate assessment's technical manual for specific details. Each jurisdiction collects its own data, following international guidelines and specifications. International guidelines and specifications require that testing for each assessment occur within a set time period. They define response rate targets for schools (typically 85 percent) and students (typically 85 or 80 percent) as well as minimum response rates for results to be reported. International guidelines and specifications also define maximum rates of exclusion. Schools and students excluded from the national defined target population are referred to as the excluded population. Exclusions can occur at the school level, with entire schools being excluded (e.g., schools that are extremely small or remote), or within schools, with specific students (e.g., functionally or intellectually disabled students) or entire classrooms excluded (e.g., classrooms for non-native language speakers). See the appropriate technical manual for each study's specific policy on exclusion and student accommodations. To ensure that testing procedures are carried out in a consistent manner, international quality monitors visit a sample of schools in every jurisdiction."}, {"section_title": "A.3 Test development", "text": "The development of the assessment instruments is an interactive multi-step process that involves representatives of the participating jurisdictions, various expert committees, and an international consortium of contractors. Generally, the first step is to develop or revise a framework to guide the construction of the assessment. Items that fit within the framework are submitted by participating jurisdictions as well as developed by each study's international consortium of test developers. Potential items are reviewed by representatives of each jurisdiction for possible bias and relevance to the study's framework. All items are meant to reflect the national, cultural, and linguistic variety among participating jurisdictions. Approved items are field-tested, and items that perform well are identified for inclusion in an \"item pool\" that is used to create the actual or \"main\" assessment instruments."}, {"section_title": "Design of instruments", "text": "The main assessment instruments consist of (1) test booklets made up of instructions and blocks or clusters of items (small sets of items from the final pool of items) and (2) questionnaires for students, schools, teachers (in PIRLS and TIMSS), and parents (in PIRLS and PISA, although the United States has not administered the parent questionnaire). The test booklets for PIRLS, PISA, and TIMSS are constructed such that no student responds to all of the items. This construction is consistent with other large-scale assessments, such as the U.S. National Assessment of Educational Progress (NAEP). To keep the testing burden to a minimum, and to ensure broad subject-matter coverage, the assessments use a \"rotated block design\" that includes items from different content domains and, in the case of PISA and TIMSS, different subject areas. The number of booklets for each assessment varies as does the amount of time students have to complete each booklet they receive. In each assessment, a portion of the items are identical to items used in the prior administration of the assessment. These items allow a study to \"maintain trend\" (i.e., be able to make reliable comparisons among the results of different administrations of an assessment over time) and to provide for corrections through equating, if necessary."}, {"section_title": "Translation", "text": "Source versions of all instruments are prepared in English and, in the case of PISA, in French as well. These are translated into the primary language or languages of instruction in each participating jurisdiction. In addition, it is sometimes necessary to adapt the instrument for cultural purposes, even in nations such as the United States that use English as the primary language of instruction. (For example, British terms such as \"lift\" might be replaced with \"elevator\" in the U.S. version.) The national translation and adaptation of all instruments are reviewed and approved by the sponsoring organization of each international study to ensure that the translations and adaptations did not change the substance or intent of the question or answer choices."}, {"section_title": "A.4 Scoring", "text": "PIRLS, PISA, and TIMSS assessment items include both multiple-choice and \"open-ended\" or \"constructedresponse\" items (i.e., items that require students to write their answer in the space provided, usually in the form of a list or short sentences). To score each item, a scoring guide (or rubric) is created to train test-scorers and anchor the scoring in each jurisdiction. The scoring guides are carefully written and reviewed by the national research coordinators and other experts as part of the field test of items, and revised accordingly. For each test item, the scoring guide describes the intent of the question and how to score students' responses-full credit, partial credit, or no credit-for the range of possible responses. In addition, the scoring guides include real examples of students' responses accompanied by a rationale for their classification for purposes of clarity and illustration."}, {"section_title": "A.5 Data entry and cleaning", "text": "Once all items have been scored, the results are entered into data files using a common international software and format. The software facilitates the checking and correction of data by providing various data consistency checks. The data are then generally sent to a central international data processing center (DPC) for cleaning. The DPC checks that the international data structure is followed; checks the identification system within and between files; corrects single case problems manually; and applies standard cleaning procedures to questionnaire files. Results of the data cleaning process are documented by the DPC. This documentation is shared with the national research coordinator so that specific questions can be addressed. The national research coordinator then provides the DPC with revisions to coding or solutions for anomalies. The DPC will then compile background univariate statistics and preliminary test scores based on classical and Rasch item analyses. For more detailed information on the data entry, cleaning process, and scoring of each assessment, see the appropriate assessment's technical manual."}, {"section_title": "A.6 Weighting and scaling", "text": "Before the data are analyzed, responses from students are assigned sampling weights to ensure that the proportion or representation of different subgroups of assessed students (e.g., public/private, census region, urban/suburban/rural, race/ethnicity) match the actual percentage of that subgroup among the school population of the target grade or age. The use of sampling weights is necessary for the computation of sound, nationally representative estimates. The basic weight assigned to a student's responses is the inverse of the probability that the student would be selected for the sample. Adjustments to weights are also made by the international consortium for various situations (such as school and student nonresponse) because data cannot be assumed to be randomly missing. (NCES may conduct a nonresponse bias analysis after these adjustments are made to see how much bias still exists, compared with the original sample frame. For more details, see A.11.) Once these sampling weights are in place, item response theory (IRT) procedures are used to deduce the difficulty of each item, using information about how likely it is for students to get some items correct versus other items. Once the difficulty of each item is determined, the items are assigned a value on a standardized logit scale of item difficulty. Scaling items in this way makes it possible for the ability of groups of students to be estimated or scored, even though not all students were administered the same items."}, {"section_title": "Scale scores", "text": "In order to make the estimated scores more meaningful and to facilitate their interpretation, the scores are transformed to a new scale with a mean of 500 and a standard deviation of 100. These scale scores are what are reported in PIRLS, PISA, and TIMSS reports and throughout this special analysis. Strictly speaking, scale scores are specific to a given assessment and cannot be compared across assessments even within the same study. However, statistical equating procedures are commonly employed to allow comparisons over time between assessments within a study. For example, the scales from TIMSS 1999 (the scales established for each subject and grade in 1999) were statistically equated with the scales from TIMSS 1995 (the scales established for each subject and grade in 1995) so that the TIMSS 1999 results could be placed on the TIMSS 1995 scales. The scales of each subsequent TIMSS assessment, in turn, have been statistically equated with the 1995 scale for the respective subject and grade. Thus, a TIMSS 8th-grade mathematics score of 500 in 1995, for instance, is equivalent to a TIMSS 8th-grade mathematics score of 500 in 2007. In PISA, the three subject matter scales were developed successively in the year that each subject was first assessed in depth as the major subject matter domain (i.e., reading in 2000, mathematics in 2003, and science in 2006), and all subsequent assessment scales have been statistically equated with those scales. It is also important to keep in mind that the procedures used to determine scale scores were developed to produce accurate assessment results for groups of students while limiting the testing burden on individual students. They are not intended to produce assessment results for individual students. However, the procedures to determine scale scores provide data that can be readily used in secondary analyses that is done at the student level. An accessible treatment of the derivation and use of plausible values can be found in Beaton and Gonz\u00e1lez (1995). A more technical treatment can be found in the TIMSS 2007 Technical Report (Olson, Martin, and Mullis 2008). A.7 Cutpoint scores and achievement levels"}, {"section_title": "International benchmarks", "text": "The IEA has developed international benchmarks for achievement on TIMSS and PIRLS based on cutpoint scores that describe what students-who have reached each benchmark's threshold or \"cutpoint\" score-know and can do in regard to the subject assessed. For example, 4th-grade students who have reached the TIMSS Intermediate benchmark in mathematics (scored 475 or better) demonstrate an understanding of whole numbers. They can extend simple numeric and geometric patterns. They are familiar with a range of two-dimensional shapes. They can read and interpret different representations of the same data. (Gonzales et. al. 2008, p. 13) The IEA describes student achievement in this manner at four points on its assessment scales: Advanced International Benchmark (cutpoint score of 625), High International Benchmark (550), Intermediate International Benchmark (475), and Low International Benchmark (400). With these four equally spaced benchmarks serving as touchstones for reference, it is possible to interpret what the scores on the PIRLS and TIMSS achievement scales mean more concretely (i.e., understand what knowledge and skills may be demonstrated with a scale score of 513 versus 426). To describe student performance at the selected points or benchmarks along the TIMSS and PIRLS achievement scales, the IEA uses scale anchoring. Scale anchoring involves selecting a cutpoint score that will \"anchor\" a benchmark and then identifying items that students scoring within plus or minus 5 scale score points of these anchor points are likely to answer correctly. (The range of plus and minus 5 points around a benchmark's anchor point is intended to provide a sample that is adequate to analyze the items defining student performance at each benchmark, yet one that is small enough so that performance at each benchmark anchor point is clearly distinguishable from the next.) Subsequently, these items are grouped by content area within benchmarks and reviewed by subject matter experts. These experts focus on the content of each item and describe the kind of knowledge demonstrated by students answering the item correctly. The experts then provide a summary description of performance at each anchor point leading to a contentreferenced interpretation of the achievement results. (Detailed information on the creation of the benchmarks is provided in Mullis, Martin, and Foy 2008aand 2008b.)"}, {"section_title": "Levels of proficiency", "text": "The OECD has identified levels of proficiency for each of the subject areas of PISA to describe concretely what particular ranges of scores mean. Unlike benchmarks, which are anchored by scale scores, levels of proficiency are anchored by items, which reflect particular proficiencies. Specifically, the knowledge and skills that students are asked to demonstrate in the assessment are classified into one of five or six levels, and the items associated with those specific knowledge and skills become the basis both for classifying students into one of these levels of proficiency and for determining the cutpoint scores for each level. In PISA, all students within a level are expected to answer at least half of the items from that level correctly. Students at the bottom of a level are able to provide the correct answers to about 52 percent of all items from that level, have a 62 percent chance of success on the easiest items from that level, and have a 42 percent chance of success on the hardest items from that level. Students in the middle of a level have a 62 percent chance of correctly answering items of average difficulty for that level (an overall response probability of 62 percent). Students at the top of a level are able to provide the correct answers to about 70 percent of all items from that level, have a 78 percent chance of success on the easiest items from that level, and have a 62 percent chance of success on the hardest items from that level. Students just below the top of a level would score less than 50 percent on an assessment at the next higher level. Students at a particular level demonstrate not only the knowledge and skills associated with that level but also the proficiencies classified at lower levels. Thus, all students proficient at level 3 are also proficient at levels 1 and 2. Patterns of responses for students below level 1 suggest that these students are unable to answer at least half of the items from level 1 correctly. Given that items are the basis for classifying students into the levels of proficiency, the cutpoint scores for particular levels vary from assessment to assessment. For more details about the PISA levels of proficiency, see the PISA 2006Technical Report (OECD 2008. A.8 Comparing results from PISA 2000, 2003The PISA 2000, 2003 assessments of reading, mathematics and science are linked assessments. That is, the sets of items used to assess reading, mathematics, and science in PISA 2000PISA , 2003PISA , and 2006 include a subset of common items. For example, there were 20 common mathematics items that were used in PISA in 2000 and 2003. To establish common reporting metrics for PISA, the difficulty of each link items is measured on different occasions and compared. Using procedures that are detailed in the PISA 2006 Technical Report (OECD 2008), the change in the difficulty of each of the individual link items is used in determining a score transformation that allows the reporting of the data on a common scale. As each item provides slightly different information about the link transformation, it follows that the chosen sample of link items will influence the estimated transformation. Thus, if an alternative set of link items had been chosen, the resulting transformation would be slightly different. The consequence is an uncertainty in the transformation due to the sampling of the link items, just as there is uncertainty in values such as country means due to the sampling of students."}, {"section_title": "Linking error", "text": "Such uncertainty that results from the link-item sampling is referred to as linking error and this error must be taken into account when making certain comparisons between PISA 2000PISA , 2003PISA , and 2006 Just as with the error that is introduced through the process of sampling students, the exact magnitude of this linking error can only be estimated. As with sampling errors, the likely range of magnitude for the errors is represented as a standard error. The standard errors of linking are as follows: When comparing two country means from PISA taken at different times (e.g., 2000 and 2003), the calculation of the standard error of the difference includes the standard errors of the two individual scores in addition to the linking error, making the resulting statement of statistical significance more conservative than if there were no linking error. For example, to calculate the standard error on the difference between scores obtained for a country in 2000 and 2003, the following formula is applied when \u03c3 2 (\u00b52000) and \u03c3 2 (\u00b52003) represent the standard errors for the results of PISA 2000 andPISA 2003, respectively, and\u03c3 2 (linking error) represents the linking error between PISA 2000 and PISA 2003: Because linking error should be taken into account when comparing means from different PISA assessment cycles, the results of simple t-tests that do not include the linking error will differ from the results published in the official PISA reports and this special analysis. For example, without adjusting for linking error, significance tests comparing reading literacy scores between PISA 2000 and PISA 2003 indicate that 15 jurisdictions measurably changed. However, after adjusting for linking error, only 9 jurisdictions are shown to have measurably changed at the .05 level of significance."}, {"section_title": "PISA tests of significance in 2000", "text": "Results from PISA 2000 summarized in this special analysis have been updated from what was presented in the PISA 2000 U.S. report (Lemke et al. 2001). Some country differences discussed in this report were not reported as statistically significant in the PISA 2000 U.S. report. In that report, a Bonferroni adjustment was used in all multiple comparisons of countries. This was not the case when PISA 2003 and 2006 data were analyzed and reported, which makes it difficult to compare results from the PISA 2000 U.S. report with results from the PISA 2003 and 2006 U.S. reports. The use of the Bonferroni adjustment for multiple comparisons was discontinued in order to avoid the possibility that comparisons of achievement between countries could be interpreted differently depending on the numbers of countries compared. A.9 Comparing results from TIMSS 1995 and 1999 TIMSS 1995 scale scores TIMSS 1995 utilized a one-parameter item response theory (IRT) model to produce score scales that summarized the achievement results in the original reports. The TIMSS 1995 data were rescaled using a three-parameter IRT model to match the procedures used to scale the 1999the , 2003 The three-parameter model was preferred to the one-parameter model because it can more accurately account for the differences among items in their ability to discriminate between students of high and low ability. After careful study of the rescaling process, the International Study Center concluded that the fit between the original TIMSS data and the rescaled TIMSS data met acceptable standards. However, as a result of rescaling, the average achievement scores of some countries changed from those initially reported in 1996 and 1997 (Peak 1996;NCES 1997). The rescaled TIMSS scores are included in this special analysis."}, {"section_title": "TIMSS tests of significance in 1995 and 1999", "text": "Tests of significance used in this special analysis to make multiple country comparisons for TIMSS 1995 and TIMSS 1999 differ from those presented in their respective U.S. reports (NCES 1997, NCES 2000. Some country differences discussed in this special analysis were not reported as statistically significant in the TIMSS 1995 and 1999 U.S. reports. The reason for this is that a Bonferroni adjustment was used in all multiple comparisons of countries in those reports. However, the TIMSS 2003 and 2007 U.S. reports discontinued use of the Bonferroni adjustment. To maintain the comparability of results across all four TIMSS assessments, none of the tests of significance presented in this report used the Bonferroni adjustment for multiple comparisons."}, {"section_title": "A.10 Confidentiality and disclosure limitations", "text": "In accord with NCES statistical standard 4-2-6 (Seastrom 2003), confidentiality analyses for the United States are implemented to provide reasonable assurance that public-use data files issued by the IEA and OECD do not allow the identification of individual U.S. schools or students when compared against publicly available data collections. Disclosure limitations include the identification and masking of potential disclosure risks for schools and adding an additional measure of uncertainty of school, teacher, and student identification by randomly swapping a small number of data elements within the student, teacher, and school files."}, {"section_title": "A.11 Nonresponse bias analysis", "text": "In accord with NCES statistical standard 4-4-1 (Seastrom 2003), nonresponse bias analyses are conducted for any survey stage of data collection with a unit or item response rate less than 85 percent. Estimates of survey characteristics for nonrespondents and respondents are compared on sampling frame variables, for which data on respondents and nonrespondents are available, so as to assess the potential nonresponse bias. Comparisons are made before and after weight adjustments for nonresponse bias, and these comparisons examine both bias and relative bias. Bias is calculated as the difference between the value of a survey characteristic for all schools (or students) that were sampled and the value of that characteristic for just those schools (or students) that actually responded. Relative bias reports this difference as a percentage of the value of the survey characteristic for all schools (or students). For example, in PIRLS 2006, of the 214 U.S. schools in the original sample eligible to participate in PIRLS, 120 agreed to participate (for a response rate of 57 percent, weighted). An additional 63 substitute schools agreed to participate, bringing the total up to 183 schools for a weighted response rate, using final adjusted weights, of 86 percent of eligible schools (NCES 2009-050). With these substitute schools, the United States met the international guidelines for school response rates. However, a nonresponse bias analysis was conducted to determine what amount of bias and relative bias existed in the original and final sample. Such an analysis is considered to have found potential bias in a survey characteristic if either (a) a test of statistical significance indicates a significant difference between the responding and nonresponding sample percentages for that survey characteristics or (b) the relative bias for that survey characteristic is greater than 10 percent. The PIRLS 2006 nonresponse bias analysis identified four variables as either (a) significant in bivariate or multivariate bias analysis or (b) having a relative bias of greater than 10 percent. The following list summarizes these variables and the difference found, after weight adjustments, between the value of the variable in the final sample of responding schools (which includes substitute schools) and value in the full sample from which the schools were drawn: percentage of White, non-Hispanic students (53.9 percent of the students in schools in the final sample vs. 53.5 percent of the students in schools in the full sample) In TIMSS 2007, of the 290 U.S. schools in the original sample eligible to participate in TIMSS at 4th grade (TIMSS-4) and 287 at 8th grade (TIMSS-8), 202 schools at grade 4 and 197 schools at grade 8 agreed to participate (or, respectively, 70 and 68 percent, weighted). An additional 55 substitute schools for TIMSS-4 and 42 substitute schools for TIMSS-8 agreed to participate, bringing the total up to 257 schools at grade 4 and 239 schools at grade 8 for weighted response rates, using final adjusted weights, of 89 and 83 percent, respectively, of eligible schools (NCES 2009-012). With these substitute schools, the United States met the international guidelines for school response rates. However, a nonresponse bias analysis was conducted to determine what amount of bias and relative bias existed in the original and final sample. The TIMSS 2007 nonresponse bias analysis identified several variables as either (a) significant in bivariate or multivariate bias analysis or (b) having a relative bias of greater than 10 percent. The following list summarizes these variables (in the 4th and 8th grade sample) and the difference found, after weight adjustments, between the value of the variable in the final sample of responding schools (which includes substitute schools) and value in the full sample from which the schools were drawn:  NCES 2009-055). With these substitute schools, the United States met the PISA standards for a minimum participation rate (65 percent of original sample schools). However, since the U.S. response rate level did not meet the NCES standard of 85 percent, the nonresponse bias analysis was conducted to shed light on the quality of the data. The PISA 2006 nonresponse bias analysis identified eight variables as either (a) significant in bivariate or multivariate bias analysis or (b) having a relative bias of greater than 10 percent. The following list summarizes these variables and the difference found, after weight adjustments, between the value of the variable in the final sample of responding schools (which includes substitute schools) and the value in the full sample from which the schools were drawn: percentage of American Indian/Alaska Native students (1.4 percent of the students in the final sample vs. 2.2 percent of the students in schools in the full sample) percentage of Hispanic students (12.0 percent of the students in schools in the final sample vs. 10.8 percent of the students in schools in the full sample) percentage of White, non-Hispanic students (61.5 percent of the students in schools in the final sample vs. 63.5 percent of the students in schools in the full sample) percentage of students of Other race/ethnicity (2.9 percent of the students in the final sample vs. 2.5 percent of the students in schools in the full sample)  Tables  Table S- Tables  Table S- Gonzales, P., Williams, T., Jocelyn, L., Roey, S., Kastberg, D., and Brenwald, S. (2008). Highlights From TIMSS 2007: Mathematics andScience Achievement of U.S. Fourth-andEighth-Grade Students in an International Context (NCES 2009-001), tables 3 and 9. National Center for Education Statistics, Institute of Education Sciences, U.S. Department of Education. Washington, DC.    Gonzales, P., Williams, T., Jocelyn, L., Roey, S., Kastberg, D., and Brenwald, S. (2008). Highlights From TIMSS 2007: Mathematics andScience Achievement of U.S. Fourth-andEighth-Grade Students in an International Context (NCES 2009-001), tables 11 and 17. National Center for Education Statistics, Institute of Education Sciences, U.S. Department of Education. Washington, DC."}, {"section_title": "Standard Error", "text": "Table S-8. Standard errors for the average TIMSS scores of eighth-grade students in science and cutpoint scores for bottom and top 10 percent of students in each jurisdiction, by jurisdiction: 2007   "}]