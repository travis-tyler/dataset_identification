[{"section_title": "", "text": "CRESST Final Deliverable education, type of community, type of school, region of the country, and so forth. Take as a specific example comparisons of ethnicity subgroup performance on algebra among 8th graders in the 1992 NAEP main assessment. It is well known that the performance differences are large ':among these subgroups when other factors are not taken into account. One may ask to which extent the performanCe differences are reduced when taking across -group variation in OTL into account: The across-group variation is quite large .for a key. OTL variable: 37% of Black and 33% of Hispanic students report that they are in prealgebra or algebra classes, whereas 53% of Whites and 63% of Asians report that they are in such 8th-grade classes. Should NAEP report OTL-conditioned results in addition, to the usual unconditioned results? For discussiOn of some issues of Comparing performance when attempting to control for curriculum, see the Baker (1993) and Westbury (1993) debate in the context of comparing Ainerican and Japanese 8thgrade math performance. Figure 1 shows what happens in the context of NAEP algebra performance when controlling for algebra On. Box-and-whiskers plots show the subgroup distributions both overall (left part of the figure) and for students who are in prealgebra or algebra classes (right part of the figure). The figure shows that subgroup differences are not reduced but remain almost the same when conditioning on OM. All four subgroups have about the same increase in algebra performance when changing the study from all students to students in prealgebra or algebra classes. This result does not change when one excludes the prealgebra group and focuses on the algebra group. One may consider several hypotheses for this finding. First, the prealgebra/ algebra students may not have the same 7th-grade math 'ability across subgroups, or may be otherwise different in their preparation prior to 8th-grade algebra studies. This means that although the OTL effects are the same across subgroups, the starting points are different. The mechanisms for selection into prealgebra/algebra classes would then be-different across subgroups; see alSo Kifer (1992). Second, the quality -Of OTL-May differ across subgroups: For example, the starting points may be -the--same; 'but the on quality may differacross subgroups. Third, the OTL reporting may-differ -across -SubgrOupS iii feiiiis of reliability and validity. This example illuStrates the 'diffiabity in analyzing performance relates to OTL. Prior performance and other relevant background factors need to be controlled for and OTL needs to be measured in detail and with precision. This can provide for interesting analyses of the various hypotheses. All students Prealgebra/algebra students Figure I. NAEP 1992 8th-grade algebra proficiency related to ethnicity (White, Black, Hispanic, Asian) and algebra class type. This article presents some ideas for studying OTL effects using latent variable modeling. An important theme is the notion of a general factor influencing achievement. A general factor notion is introduced for two reasons. First, OM effects can be clearly described as effects that go beyond what is expected by the general factor. Such added effects can be described in terms of outcome variables corresponding to specific factorsvariables affecting more narrow components of the test performance. Second, it is more likely that OTL effects can be found with respect to specific factors as opposed to a general factor (e.g., an overall math score). Controlling for initial ability is possible with longitudinal achievement data such as the National Education Longitudinal Study of 1988 (NELS)  given data from a cross-sectional assessment such as NAEP. This article considers to which extent the general factor is a good proxy for prior performance. To assess the effects of OTI, as well as possible, this. :article: considers. analyses with multivariate information on OTL, performance, and prior performance, as well as information from multiple occasions. Not only can _OTL effects be studied with the help of such multivariate modeling, but scores adjusted for OTI, or lack thereof can also be derived for reporting of assessments. With better OTI, measures in future large-scale -assessments, the -OTL_ effects :on they specific factors can provide useful indicators of the performance effects of population changes in OTL. The article reports on analytic work with the-1992 NAEP and the 1988-1990 NELS mathematics data, divided into analyses of OTL, analyses of performance, and relating the two, constructs by multivariate modeling."}, {"section_title": "Analyses of Multivariate OTL Information Analysis Goals", "text": "As a first step in studying OTL effects on mathematics achievement, the OTI, information itself needs to be carefully analyzed. In line with Burstein _ (personal communication, April 1994), we argue that multivariate information should be used to more fully characterize types Of math classes rather than using isolated pieces of information based Oir single questionnaire items. To illustrate this, we present analyses based on student and teacher reports for both NAEP and NELS data. In the Second International Mathematics Study (SIMS; see, e.g., Burstein, 1992), four 8th-grade class typesremedial, typical, enriched, -and algebra were created from teacher questionnaire data and-information-on textbooks used. These class-type variables provided; -_valuable -information achievement analyses. A corresponding classification is not, however, readily__ _ available based on NAEP and NELS We-will attempt to derive a class-type -I I a a variable similar to the one used in SIMS -using teacher-reported information-on math emphasis variables in NAEP and NELS. 9 I a NAEP TRP Task 3c, Opportunity-to-Learn Effects on Achievement 5"}, {"section_title": "Methods", "text": "Data from the National Assessment of Educational Progress (NAEP) are obtained as a multistage, national probability sample for Grades 4, 8, and 12. The 1992 main assessment data that will be used here cover five content areas in mathematics: numbers and operations (arithmetic), measurement, geometry, data analysis and statistics, and algebra. Test results were obtained for almost 10,000 students per grade. The analyses in this article will focus on 8th-grade and 12th-grade data. The National Education Longitudinal Study (NELS) is another nationally representative achievement study. In it, over 20,000 students are tested. The survey was first administered in 1988 for 8th graders with follow-up tests in the 10th and 12th grades. NAEP 1992 8th-grade math data contain student-reported information that classifies the student as belonging to a class where no or regular 8th-grade math is taught (50%), where prealgebra is taught (27%), or where algebra is taught (23%). In addition, there are teacher-reported math emphasis variables corresponding to the five content areas of numbers and operations, measurement, geometry, data analysis and statistics, and algebra, as well as emphasis on learning skills/procedures, reasoning/analysis, communication, appreciating math, and teacher-determined class ability level. NELS data contain similar information. These variables have the potential of giving more information on class type than the student-administered question. Factor analyses are carried out using maximum-likelihood estimation and oblique rotations using the promax method.\nOTL-DIF analyses can be carried out -.for. -the NAEP 8th-grade math items with respect to the three algebra class type categories reported by students (no or 8th-grade math, prealgebra, and algebra) and with respect to membership in enriched classes using the teacher-based composites discussed above. Such analyses can be carried out in several ways. A visually instructive way is to plot each item's proportion correct for the two groups (see, e.g., Bejar, 1980). We present the proportions here in a logit scale to improve linearity. A line can be placed through the scatter of item values and items deviating from the line show especially strong advantage (or disadvantage) in performance by one of the.groups over the other. This may indicate deviations from invariance of measurement characteristics (i.e., DIF) and possibly lack of unidimensionality. It should be noted, however, that it is well known that this intuitively u4eful method for' _ identifying items with DIF is not optimal for teas with items showing large amounts of guessing or strong variations-in the item discrimi 'nation values. To take such features into account, it is better to carry out Item Response Theory (IRT) analyses with three-parameter logistic models applied to the two groups. As an alternative, the Mantel-Haenszel DIF method, which is the standard DIF method used by ETS for NAEP, may be used. As is the practice at ETS, in this article each item is analyzed as it appears in six different blocks, using the block score as matching variable. For a discussion of DIF methods, see Holland and Wainer (1993). Figure 2 shows a plot of the 28 multiple-choice and short constructedresponse algebra items in the 1992 NAEP main math test. Proportion correct is computed using the \"grade only\" sample. It is important to note that the line is fitted using all 183 8th-grade math items, so that the algebra item performance is related to the overall math performance. Here, the group of algebra students is compared to the nonalgebra group, excluding the prealgebra students. Items would lie along the broken line if nonalgebra \"stn-derits--On the whole-performed as well on the math test as the algebra Ancients. Compared torthe broken_ line, the solid line shows that the algebra group performs better overall. The nonalgebra _ group has an average p value of 0.52 on the 183 items, while the algebra group has an average p value of 0.71. What is of most interest is that the majority of the items are above the solid line and only about a fourth are below it. The items  Figure 2. NAEP 1992 8th-grade algebra item performance related to algebra and nonalgebra class type (plot of 28 algebra items).\nLogistic regression for an ordered categorical response variable is carried out for each extended constructed-response math item in the 1992 NAEP. The background variables used in these analyses are shown in. Table 6. Each analysis is carried out using a subset of the total sample based on data from 6 of the 26 booklets in which the item appears. Four of the six items had sufficient numbers of Grade 8 students in the partial or better categories and could be analyzed: item C\"reason to maximize difference\" (numbers and operations); item I\"find probability and explain\" (data analysis, statistics and probability); item L\"extend pattern to find term\" (algebra); item M\"partition figures to find area\" (measurement). Table 7 shows the results of the four logistic regressions. In the top panel the student-based algebra class type information is used. In the middle panel, teacherbased class type (remedial and typical, enriched, algebra) as well as the factor score for the \"NCTM\" factor of Table 1 are used. In the bottom panel both the student-based and teacher-based information are used.\nFor all three analyses, structural equation modeling with latent variables and maximum-likelihood estimation will be used. Multivariate analysis of NAEP's proficiency scores. The latent variable model is shown in diagram form in Figure 5. In it, the general factor is similar to NAEP's overall math performance score and to a large extent represents skills related to arithmetic, particularly as represented by the numbers and operations items. Specific factors represent student variation in content-area scores, which I NAEP TRP Task 3c, Opportunity-to-Learn Effects on Achievement differ from that of general arithmetic tasks and are more directly related to topics, definitions, and skills specific to the content,area.-A specific factor is not included for numbers and operations so that the general factor is more clearly_ defined in terms of such arithmetic skills. This type of modeling was-successfully 'applied to NAEP math data in Muthen, Khoo, and Goff (1994) to demonstrate multidimensionality in math performance. In that_ application, however,_specially______ derived testlets were used corresponding to item content and format, whereas_ in the present case the usual content-specific proficiencies produced by NAEP are used. Using the model of Figure 5, the performance in a certain content area is BEST COPY AVAiLABLE 34 30 CRESST Final Deliverable decomposed into effects from a general factor and a content-specific factor, and each of these two factors is related to OTL and other background. The analysis of the Figure 5 model is carried out using the five multiple imputation values that are provided in NAEP for each of the five content areas. In comparison to the true (\"theta\") proficiency score for each of the five content areas, these imputed proficiency values have been created such that they have the same mean, variance, and covariance with background variables (Mislevy, 1991(Mislevy, , 1993, as well as having the same covariances between content areas (Mislevy, personal communication, August 1994). A latent variable regression model for the true proficiency scores can therefore be analyzed using the imputed values as observed variables. This implies that there are no measurement error components for the observed variables in the model of Figure 5. The residuals for the specific factors are defined to be uncorrelated with the residual for the general factor. The model has six degrees of freedom if the specific factor residuals are taken to be uncorrelated among themselves, although this is not a necessary assumption for identifying the model. The input for structural equation modeling software is given in the appendix. Input for the LISCOMP program is used here, but LISREL input is very similar. As is customary with multiple imputations, the model is analyzed for each imputed value, and the estimates are averaged over the five analyses. Standard errors of estimates are calculated by the usual imputation formula using the combination of average standard error over imputations and between-imputation estimate variability (see, e.g., Mislevy, Johnson, & Muraki, 1992). The \"grade only\" main 1992 NAEP samples are here analyzed for Grade 8 and Grade 12. In each case, the student-based and the teacher-based OTI, variables considered earlier are used together. Multivariate analysis of process scores in NAEP. These scores are not provided by NAEP but are generated as parcels of items classified into the NAEP ability categories conceptual understanding, procedural knowledge, and problem solving. For this analysis, three types of testlets were formed corresponding to the three ability categories, ignoring math content. The item classification was obtained from ETS (Pashley, personal communication, 6 October 1994). The testlets were formed for items in each block for all of the 26 booklets. On average, six testlets per block were formed with most testlets consisting of two items. There were 29 testlets for conceptual understanding, 29 for problem solving, and 17 for procedural knowledge. A multifactorial model in line with Muthen a al. (1994) was used also here. The analyses reported here are limited to considering a problem:solving ;factor in _ _ addition to a general factor. The general factor' as definelThy.,having procedural knowledge and conceptual understanding variables load only on this. The problemsolving variables, however, were allowed to load not only on the general factor but also on a specific problem-solving factor. In line with Muthen -et al. (1994), a simultaneous structural modeling analysis was carried out for the 26 groups of students defined by the 26 NAEP booklets. The same set of background variables as shown in Table 6 were used. Here, results will only be reported for Grade 8. Longitudinal analysis of NELS first follow-up data. The model is shown in Figure 6. The analysis was carried out on testlets created from the ithms. Ali-algebra-specific factor was defined in addition to a general factor at both Grade 8 and 10. Invariance of factor loadings across the grades was not imposed, allowing\" for changes in measurement characteristics over time. At Grade 10, three mathematics test forms were used depending on the math performance in Grade 8. Here, 2,413 students from the middle group and 1,418 students from the high group were included in the analysis. A simultaneous analysis of these two groups was carried out. To reflect the fact that the analysis pertains to a single population, parameters representing the same quantities were held equal across the two groups."}, {"section_title": "Results", "text": "The teacher-reported variables for NAEP are described in Table 1. As shown in Table 1, factor analysis of the teacher-reported variables in NAEP indicates a clear four-factor solution with one factor corresponding to emphasis on reasoning, communication, and appreciating math (a factor related to NCTM goals), and three other factors corresponding to membership in remedial or typical classes, enriched classes, or algebra classes. The factor loading pattern agrees with usual notions: in remedial and typical classes there is an emphasis on numbers and operations (arithmetic) and facts and skills; in enriched classes there is an emphasis on measurement, geometry, and data analysis and statistics; and in algebra classes there is an emphasis on algebra and reasoning (here the teacher typically also assesses the class ability level as high). The Table 1 factor solution can be used to classify students into three class types: remedial and typical, enriched, and algebra. We do_so by using standardized factor score values for each of these three factors_to classify astudent into: a class type for which his or her value is the largest. Table 2 shows that this classification appears to have at least a minimal amount of validity in the sense that performance on geometry, measurement, and data analysis and statistics is higher for the enriched group than for the remedial and typical group, and performance on algebra is highest for the algebra group.\nfurthest above the line are especially OTL-sensitive in that performance on these items is more enhanced by belonging to an algebra class than performance on other algebra items. The items below the line indicate that performance on them is enhanced by belonging to an algebra class in that the points are above the broken line, but not enhanced as much as the overall performance advantage for algebra classes would predict. CRESST., Final: In Figure 3 the corresponding plot is shown for the prealgebra group compared to the nonalgebra group. In comparing Figures 2 and 3, it is clear that prealgebra has considerably less effect than algebra in enhancing algebra item performance. To get a more detailed assessment of OTL sensitivity in the algebra items, a three-parameter IRT analysis was also carried out. An advantage of such an analysis is that it is possible to take into account group differences in the guessing and discrimination parameters. It may, for example, be the case that on the whole, the nonalgebra group has lower slopes than the algebra group because the topics are less familiar to the nonalgebra group and therefore elicits more measurement error, or random responses. Table 5 gives the 28 algebra items. The estimated item parameters from each group were linked to a common scale using-an analysis of all 183 math items. This takes into account that the two greiips differ in their math achievemeht mean and variance. Although, the algebra group has a higher mean than the nonalgebra group, one might expect that for a given math achievement leifel, the algebra students would have an advantage on some or most of the algebra items. This would result in different item characteristic curves. Figure 4 shows the item characteristic curves. The remarkable finding is that the curves are almost the same across the algebra and nonalgebra groups. Chi-square testing of invariance of the three parameters across the two groups shows only one significant item (item 18). This says that contrary to expectation, for given achievement level, algebra students do not have a higher probability of giving a correct answer to any of these algebra items. Beyond the higher math achievement mean for algebra students, there is no added advantage of algebra class membership for performance on these algebra items and the higher achievement mean may be at most a result of selection effects. There is therefore no clear evidence of specific algebra learning for algebra students. This may be because the algebra items that were chosen for the 1992 NAEP were quite general and did not require algebra-specific training. These algebra items may therefore be-_viewed-as `-`0TLinsensitive\" items. On the other hand, the gro-uping of students into algebra and nonalgebra class types may give too coarse of an indicator of algebra OTL given that there may be a great deal of variation in the actual OTL that a student Nonalgebra grouplow OTL 4.00 5.00 Figure 3. NAEP 1992 8th-grade algebra item performance related to prealgebra and nonalgebra class type (plot of 28 algebra items).      The 1992 NAEP main math assessment for Grade 8 contained six extended constructed-response items which were rated on a graded scale corresponding to mathematical reasoning judged incorrect, minimal, partial, satisfactory, or extended. It is of interest to study these items in more detail with respect to OTL because they represent an item type that will presumably become more and more 6 20 CRESST Final Deliverable common in NAEP and other assessments. Plans for the 1996 NAEP math test are that about 40% of the test will be open ended. The goal is to relate the probability of doing well on each extended constructed-response item to the set of OTL variables _discussed previously as well as to a set of key background variables-that have proven to -be important performance covariates in other analyses. It is of interest to see if the partial effect of OTL is significant, holding other factors constant. Note, however, that prior performance level is not controlled for. This analysis also serves as an indirect validity check for the OTL information in that good OTL measures should have an effect on the performance.\nConsidering the top panel, student-reported algebra class = membership has a significant influence on all items, as expected. Note again, however, that this effect is confounded with ability given the selection of algebra--sttidents'based on _ prior performance. The teacher-based algebra class membership Variable -is-also significant for all items, whereas the teacher-based enriched class type is significant only for item I (data analysis, probability and statistics) and item M  (measurement). The latter finding is interesting given that enriched classes are characterized by an emphasis on measurement, geometry, and data analysis and statistics in line with the factor solution presented in Table 1. As shown in the bottom panel, these two effects are still significant when combining student-based and teacher-based information. This supports the validity of the teacher-based enriched class type derived from the factor analysis. The teacher-based algebra class type variable, however, appears to only add significant information beyond the student-based information for item M. Further methodological approaches are possible on the item level when there is more detailed OTL information. As an example, the SIMS data provided itemspecific OTh information which was utilized in analyses presented in Muthen (1994) and Muthen, Kao, and Burstein (1991). 27 a a a 1 I Table 7 Items C, I, L, and M   \nMultivariate analysis of NAEP's proficiency scores. The latent variable model estimates are shown in Tables 8-11. The analyses showed that the results were almost exactly the same whether the specific factor residuals were specified as uncorrelated or not, although nonzero correlations were indicated (for Grade 8, for example, the residual co-AelatIdzisiI;aiike=-fidth 23 for data analysis --and statistics and measurement to-.:46 for geometry and measurement as well as for algebra and geometry, while ftr Grade 17.2 th4resiaualCOrretatioda range fra 'm .21) for data analysis and statistics and measurement to .64 for geometry and measurement). Because the model with correlated residuals is just ideiitified, no chi-square measure of model fit is provided. BEST COPY MAU 11119 LE  Note. Empty entries correspond to loadings fixed at zero. Note. Empty entries correspond to loadings fixed at zero. BEST COPY AVAILABLE"}, {"section_title": "CRESSY' Final Deliverable", "text": "The teacher-based classification can be compared to that of the student-,. based class membership with the three categories: no math this. year or 8th-grade math; prealgebra; algebra. Table 3 shows that the two classification schemes give partially different results, calling into question the -reliability and validity of the information. It is worthwhile to explore both schemes in relation to performance.  "}, {"section_title": "11.", "text": ""}, {"section_title": "CRESST-Final Deliverable", "text": "Turning to NELS88 8th-grade teacher-reported data, Table :4 suggests a similar pattern of emphases as for the NAEP data: In remedial classes common and decimal fractions are emphasized; in typical classes ratio, proportion, and percent problems are emphasized; enriched classes emphasize-ineasuremerit, geometry, probability and statistics and are characterized by high textbook coverage; algebra .classes emphasize algebra and integer problems. Using a similar factor-score based classification as for NAEP, we obtain an -almost similar percentage distribution over the class types remedial and typical (44%), enriched (25%), and algebra (31%). It is clear that the information about 8th-grade class .type is not as, precise_ as would be desirable in NAEP and NELS. Given the well-known problem of attenuation in regression estimates resulting from unreliability of predictors, this imprecision will make it harder to show_.0m effects on_performance.Muchnaore__, detailed information is also needed. The analytical approaches used here can,_ . however, serve as suggestions for meth-odologies that_ can be.,applied to future data "}, {"section_title": "13", "text": "BEST COPY AVAILABLE '' -----------NAEP TRP Task 3c, Opportunity-to-Learn Effects on Achievement 9 and that will be more likely to show interesting effects the better the OTL measures."}, {"section_title": "Analyses of Test Items: OTL-Sensitive items Analysis Goals", "text": "For assessments such as NAEP and NELS, differential item functioning (DIF) is typically investigated for every item, answering the question \"Do the items function differently for different gender, ethnicity, and so forth?\" If they do, items are discarded. This investigation is not customarily done with respect to groupings based on OTL categories. We suggest, however, that such an analysis is of great interest when the focus is on understanding 0Th effects. For example, 8th-grade algebra class membership should make most algebra items easier. This advantage is, however, confounded with the higher math ability of students in algebra classes that customarily results from tracking and other selection mechanisms. DIF analysis takes into account such group differences and can describe item performance differences resulting from OTL, properly conditioned on ability. What DIF analysis searches for are algebra items that are particularly sensitive (or insensitive) to algebra instruction, so that for these items algebra students have an advantage (or disadvantage) that is larger than that on the overall math test. It is of interest to search for such OTL-DIF items and characterize them. This can be helpful in terms of test construction. OTL-sensitive items may be desirable or undesirable depending on the purpose of the test. If there are few OTL-sensitive items in a test, the test makes for a fair comparison of students who differ in their OTL. However, with few OTL-sensitive items, the test is an insensitive indicator to change in OTL. Further issues arise related to OTL-sensitive items. Instead of discarding items showing DIF, one could entertain the provocative idea of using adjusted scores by allowing these items to have different difficulty parameters. We are then attempting to measure \"potential\": What can these students do given opportunities to learn? One could argue that with persons getting such items right, the ones not in algebra classes should get higher scores than those in algebra classes. And, if students not in algebra classes do not get all such items right, their scores should not be as low as students in algebra classes with the same responses. This information is useful in addition to knowing the actual proficiency."}, {"section_title": "S Analysis Goals", "text": "We will now return to the issue of controlling for prior peiformance when attempting to assess OTL effects on performance. The analysis is now on the score level. Our idea is to use a general math factor that influences performance in all content areas. As mentioned earlier, the introduction of such a general factor notion has two advantages: OTL effects can be clearly described as effects that . go beyond what is expected by the general factor, and it is more likely that interesting OTL effects can be found with respect to performance -that is over and above that expected by a general,factoi: Three types of analyses are of-interest: analysis of NAEP proficiency --scores; analysis of NAEP process `scores, and longitudinal analysis of NELS Multivariate analysis of NAEP proficiency scores. The first analysis ----concerns the NAEP-provided___Proficlency._ scores, which are imputed in five versions for each of five math content areas. A latent variable model is of interest for these proficiency scores, where a general and several specific factors can be identified and regressed on background information including OTL. In the main NAEP assessment, proficiency scores are produced -for each of the five content areas: numbers and operations, measurement, geometry, data analysis and statistics, and algebra. We will formulate a multivariate response model for these five areas and relate it to the same set of OTL and background variables as studied earlier. The response model is formulated with-the aim -of-separating out a general factor influencing perfoimaride oh: all content -areasffrom-specific-, factors---influencing only one content area. The partial effects of OTL, given other background variables, on the general factor often correspond to ability differences due to selectiOn effects as with 8thgrade tracking for algebra classes.-OM effects on the content-specific factors, however, correspond perhaps morerclosely to t he :question initially posed, -What students know and can do as a result of their_ educational experienCes.7_ The main interest of an OTL analysis is in studying the partial OTL effect on the specific factors, where effects of the general fa'ctor as well as -Other VaCkgrOund are held constant. If one can assume that in comparison to skills in each content area, the general skills are relatively stable from, say, Grade 7 to Grade 8, the general BEST COPY AMAMI factor provides a reasonable proxy for prior performance. This is a hypothesis that needs to be tested, however, perhaps using longitudinal data such as NELS. If this is found to be a reasonable approximation, the general factor would become an important variable to control for when studying OTL effects on performance in particular content areas. Multivariate analysis of NAEP process scores. A similar latent variable analysis is applied to scores related to processes or categories of mathematical abilities. The NAEP 1992 Technical Report (Johnson & Carlson, 1994), refers to three mathematical ability categories within which the math items can be organized: conceptual understanding, procedural knowledge, and problem solving (see page 52). Given the NCTM Standards, it is of particular interest to study to which extent a problem-solving factor can be identified. A discussion of the NAEP 1990 Grade 8 math items in the context of NCTM standards is given in Silver and Kenney (1993). NAEP 1992 defines problem-solving items as follows (Johnson & Carlson, 1994): In problem solving, students are required to use their reasoning and analytic abilities when they formulate problems; determine the sufficiency and consistency of the data; use strategies, data models, and relevant mathematics; generate, extend, and modify procedures; use reasoning (i.e., spatial, inductive, deductive, statistical, and proportional); and judge the reasonableness and correctness of solutions. (p. 52) Longitudinal analysis of NELS first follow-up data. Longitudinal achievement data have the potential of more clearly disentangling effects of OTh from effects of prior performance level. Using the first follow-up NELS data from Grades 8 and 10, it is of interest to model 10th-grade performance as a function of both 8th-grade and 10th-grade OTh while conditioning on 8th-grade math ability. In this article reading performance is also available as a covariate."}, {"section_title": "39", "text": ". -.,-,:.;   Tables 8 and 9 show the measurement part of the model for Grades 8 and 12 (cf. Figure 5). With the exception of the numbers and operations proficiency, all five observed variables load on two factors, the general and a specific factor. The factor variances are estimated but not shown here. To facilitate understanding, the information on these values is instead given as the percentage variance that each specific factor accounts for in the corresponding observed proficiency variable. It is, for example, seen that the geometry proficiency has the lowest loading on the general factor and has the largest percentage of its variance explained by the geometry-specific factor. Consider next the Table 10 structural modeling results for Grade 8. The first column contains the effects of the background variables including OTL on the general factor. Strong effects of algebra OTL are seen on the general factor, to a large extent reflecting the selection of students into such classes based on prior performance. Student-based algebra class type has a large effect, while the effect of prealgebra is considerably smaller. This is in line with the item performance shown earlier in Figures 2 and 3, where the difference between the solid and the broken line, describing the average math item performance difference of students in (pre) algebra classes and nonalgebra classes, was large for the algebra group but not for the prealgebra group. We note that the teacher-based class type variable has effects on the general factor beyond that of the student-based OTL information. The results for the specific factors are of particular interest. There is an especially large effect of student-based algebra class type on the algebra-specific factor. This is in line with Figure 2 where many algebra items were found above the solid line describing the average item performance of students in algebra classes. With the exception of the effects of the teacher-reported NCTM factor and algebra class type on the algebra-specific factor, however, the remaining OTL variables do not show any effects. In particular, the absence of effects from the teacher-based enriched class type on the content-specific factors measurement, geometry, and data analysis and statistics is noteworthy. Either this class type variable is not well enough defined by the teacher-based emphasis variables or the increased emphasis is not sufficiently relevant for these test items. Finally, it is of interest to consider how this table describes ethnicity differences in performance. This may be related to the introductory discussion in connection with algebra performance shown in Figure 1. In line with Figure 1, the results in the general column of Table 10 indicate that there are still large ethnicity differences for the general factor even when OTL is taken into account. In several instances, the content-specific columns also show significant ethnicity coefficients. In all cases except one, however, these ethnicity coefficients are smaller than for the general factiiic For example, the Black and Hisparc coefficients for the algebra-specific factor are considerably reduced compared to those for the general factor. An algebra-specific coefficient describes an effect on algebra performance that conditions on the general factor. This type of reduction in coefficients may therefore indicate that the conditioning on the general factor, in addition to conditioning on OTL, to some degree accomplishes the desired conditioning on prior performance. The Table 11 results for Grade 12 also indicate content-specific effects of OTL variables. As expected, there are particularly strong OTL effects of geometry-trigonometry studies on geometry performance and of algebra-calculus studies on algebra performance. A surprising result is that studying trigonometry has a larger effect on algebra-specific performance than algebra or calculus studies. Multivariate analysis of process scores in NAEP. Table 12 gives the estimates of the structural model. This model was arrived at as follows. In the measurement part of the model (not reported), the general and specific factors were first allowed to have free loadings. This allowed for a check of the appropriateness of the NAEP classification of items into the problem-solving category. Problem-solving variables with negative loadings on the problem-solving factor were dropped as indicator of this factor and only allowed to load on the general factor. This occurred for 7 of the 29 variables, resulting in the solution shown in Table 12. The variance contribution for the problem-solving factor can be measured as the percentage of the reliable variance that it contributes to problem-solving testlets. The measurement error variance is in this way not involved, which is desirable given that each testlet consists of few items and is therefore quite unreliable. The percentage variance contribution is 63% on average over problem-solving testlets.;__-:Flien _conditioning on the baCkki-ound variables, it is 71%. This indicates that the factor is very important. Table 12 shows that the background variables have about the same effect on the general factor as in Table 10. For the problem-solving factor, student-reported algebra class type has a large positive effect. The problem-solving factor also BEST COPY AVMLABLE  in Table 10 for the algebra factor,. but. a.Type of .Community effect for specific , factors was not found in Table 10. The fact that different effects are found for the,. problem-solving factor than for the general factor or content-specific factors motivates a further investigation. of _such achievement components. Related research on reasoning components of math achievement -has been carried out using NELS data by Kuppermintz, Ennis, Hamilton, Talbert, and Snow (1994). Longitudinal analysis of NELS first follow-up data Table 13 shows -that for the general factor at Grade 10, the general factor at Grade 8 is -the most -; important predictor as expected. Despite the inclusion of this prior performance variable, however, student-reported algebra OTL has an additional effect. This effect is present even with reading achievenient in Grade 10 taken into account. For the algebra-specific factor in Grade 10, student-reported algebra OTL also has an effect that goes beyond the effect of the prior performance represented-by-the  Grade 8 general factor. The partial effects of student-reported algebra OTL are, however, somewhat lower than the corresponding values in Tables 10 and 11 where conditioning on prior performance was not carried out. In part, this may indicate that the conditioning on the general factor carried out in the crosssectional data analysis of NAEP does not fully accomplish controlling for prior performance as is possible in the longitudinal analysis of NELS. Nevertheless, the conditioning on the general factor is useful also in cross-sectional studies because the general factor is relatively stable over time. In these data, the correlation between the general factor at Grade 8 and Grade 10 is .83. In comparison, we may note that the correlation for the algebra-specific factor at Grade 8 and Grade 10 is only .26."}, {"section_title": "Discussion", "text": "This report suggests a set of analytic approaches that may be used to further the understanding of the relationship between OTL and achievement, and it illustrates them using NAEP and NELS math data. The findings point to three major issues that should be considered in future large-scale educational assessments: OTL sensitivity in items; scoring and reporting of achievement components; and instructions for measuring OTL indicators and relating them to achievement outcomes. First of all, the concept of OTL-sensitivity in items appears important for future analyses of achievement items where OTL is of interest. In the analysis of the 8th-grade algebra items, it was surprising to find that for a given math achievement level, algebra students did not have a significantly higher probability than nonalgebra students of giving a correct answer to any of the 28 algebra items. The 1992 NAEP Grade 8 algebra items appear not to be sensitive to algebra OTL. This may be a desired effect for a test such as NAEP, which is designed to measure performance in areas of math that are most commonly treated in schools. However, if there is a wish to also be able to use the test as an indicator of effects of curricular change over time, the lack of OTL sensitivity in the items is a deficiency. For example, it may be desirable to add more difficult algebra items or items more specifically geared towards problem solving to capture movements toward better adherence to NCTM standards. If more OTLsensitive items are added to the test, three important issues arise. First, OTL DIF analysis becomes an important part of test construction to ensure that the items 46 I 41 have the intended characteristics. Second, it becomes necessary to formulate a model for the analysis that incorporates parameters corresponding to thS Third, it becomes important to, think_, of .ways. to _report ,achievement_ results covering both OTL-sensitive and OTL-insensitive Second, the analyses point to new possibilities in terms of chdice of scoring _ and reporting of achievement components. The fact that different effects are found for the content-specific factors and for the problem-solving factor than for the general factor motivates fUrther investigations of such achievement components. Interesting analysis possibilities would open up if NAEP \"theta\" values (latent variable values) could -be-produced for -these additional 'dimensions,-7 for instance relating algebra-specific skills and problem-solving skills to claSsroom processes and NCTM reform efforts. Given the sparse matrix sampling of items and an otherwise complex data struciiiie, itis-diffieult faFie6diirdary analysts to prepare reliable performance scores from these various types =of items:There is need for NAEP to provide this. Onihidtli-Si'haiikitlifliot-Straightforward--for-----------NAEP to produce and report such thetas (Mislevy, personal communication, August 1994). For example, the assumption of the IRT model may not be well approximated if content area is ignored, multidimensional IRT modeling for content and process jointly may be cumbersome, and there may be difficultieS in reporting that much more detailed information. Perhaps a better place for studiesof process dimensions is in surveys such as NELS. It seems worthwhile, however, to consider whether a multidimensional IRT model can be applied either to produce process-related thetas in addition to content-related -thetas, or perhaps jointly producing a general theta, content-specific thetas, and process-specific thetas. Third, it is clear from the previous analyses that it is highly desirable to produce better OTL measures to understand how different opportunities relate to achievement outcomes. These measures need to be both more reliable'and more detailed. Multilevel information from students, classrooms, and schools is needed. The careful monitoring of OTL Standards:and the ,linking:-dt_suchInfOrm nation to achievement outcomes make _ for '1 more__ iisdfur -largesoale-educaticrnal assessments. BEST COPY MLA = LE 4 7"}]