[{"section_title": "Abstract", "text": "Phrase-based statistical machine translation systems can generate translations of reasonable quality in the case of language pairs with similar structure and word order. However, if the languages are more distant from a grammatical point of view, the quality of translations is much behind the expectations, since the baseline translation system cannot cope with long distance reordering of words and the mapping of word internal grammatical structures. In our paper, we present a method that tries to overcome these problems in the case of English-Hungarian translation by applying reordering rules prior to the translation process and by creating morpheme-based and factored models. Although automatic evaluation scores do not reliably reflect the improvement in all cases, human evaluation of our systems shows that readability and accuracy of the translations were improved both by reordering and applying richer models."}, {"section_title": "Introduction", "text": "Phrase-based statistical machine translation systems rely on statistical observations derived from phrase alignments automatically extracted from parallel bilingual corpora. The main advantage of applying SMT is its language-independence. The phrase-based model works well for language pairs with similar syntactic structure and word order.\nHowever, phrase-based models fail to handle great word-order differences adequately. We describe our attempt to improve performance by transforming source language (English) sentences to a structure similar to that of the corresponding target (Hungarian) sentence. We also describe our approach for handling data sparseness due to the inadequate coverage of linguistic structures by the limited training corpus. It is a common problem in the case of translation to agglutinating languages like Hungarian, where a much greater amount of training data would be necessary to provide adequate statistics than what is necessary for closely related language pairs involving only morphologically less complex languages."}, {"section_title": "Machine Translation from English to Hungarian", "text": "English and Hungarian are rather distant regarding morphological and syntactic structure and word order. Hungarian, like Finish or Turkish, is an agglutinating and compounding language, which morphological processes yield a huge number of different word forms. This, combined with free word order of main grammatical constituents and systematically different word order in NP's and PP's, results in poor performance of traditional phrase-based SMT systems. In order to have an SMT system produce correct translations of high quality, it is required to have a relevant statistical model acquired from bilingual corpora. Thus, even if a corpus of a substantial size were available (which is not the case), both the alignment phase of constructing a translation model and translation itself would be compromised by the high number of seldom or never seen word forms."}, {"section_title": "Related work", "text": "For language pairs having very different syntactic structure and word order, research has shifted towards using hierarchical models or the use of hybrid methods, such as augmenting purely statistical approaches by handmade rules as a preprocessing step. Such extensions have proved to improve results significantly in systems translating from English to German, Arabic or Turkish and several other languages (Yeniterzi and Oflazer, 2010; Go-jun and Fraser, 2012; Collins et al., 2005) . The hybrid models applied to English-Hungarian machine translation that we present in this paper belong to the latter line of research. We applied both reordering and morphological segmentation in order to handle both word order problems and data sparseness caused by agglutination. Luong et al. (2010) applied only morphological analysis in the case of translation from English to Finnish. On the other hand, Yeniterzi and Oflazer (2010) described an approach for English to Turkish translation, in which they applied both syntactic source-side reordering and morphological segmentation. In their work, morphemes constructing a single word were joined during the translation process, but in our experiments, this method increased data sparseness in the training set, decreasing the quality of the final translation rather than improving it. Another difference between Yeniterzi and Oflazer (2010) 's and our work is that they applied the morphological generator integrated in the SMT system, while we used our computational morphology on SMT output as a word form generator, generating final word forms in cases, where the SMT system was not able to find it.\nRelying on recent trends and results of research in the field of machine translation, we believe that neither a purely rule-based nor a statistical method by itself is an optimal way to handle the problem. Our work reflects this attitude by applying handmade language-specific rules. Some works, such as (Jiang et al., 2010; Holmqvist et al., 2012; Genzel, 2010) have also tried deriving such reordering rules automatically.\nA further method to apply would be using a hierarchical tree-based translation system, also augmented by reordering rules and morphological segmentation. Such a method is presented in (Gao et al., 2011) , but focusing on a narrower problem and applying it to Chinese to English translation."}, {"section_title": "Hybrid morpheme-based machine translation system with reordering rules", "text": "In order to mitigate the aforementioned difficulties regarding word order and data sparseness, we created a hybrid system with different preprocessing and decoding solutions. First we applied reordering rules in order to transform the source sentence to a structure more appropriate for word alignment and phrase extraction. The problem of lexical granularity (i.e. the relatively substantial difference in the number of words in the corresponding sentences, see Table 1 ) was also to be solved. We explored two approaches: a) increasing the number of tokens on both sides using morphemes instead of words and b) decreasing the number of word tokens on the English side to approximate that of the corresponding Hungarian sentences."}, {"section_title": "Reordering rules", "text": "In order to augment the phrase-based SMT system, we defined reordering rules as a preprocessing step. The goal of these transformations is to move words in the English source sentence to positions that correspond to their place in the Hungarian translation. Fig. 1 illustrates the transformation process on the phrase the sons of the many merchants living in the city. E.g., the subphrase living in the city is transformed to the order the city in living corresponding to the Hungarian translation \"a v\u00e1ros+ban\u00e9l\u0151\" as shown in Fig. 1a . Our rules apply only to those word order differences, which are systematically present between the two grammars (e.g. prepositions vs. case endings/postpositions). We did not intend to handle free word order variations of Hungarian, where the same meaning can be expressed with several different orderings, since the actual word order in a sentence is not only determined by syntactic rules, but also by pragmatic factors. Dependency structure: Reordering rules are guided by dependency relations. After generating a context-free parse, these relations are extracted by the Stanford parser (Marneffe et al., 2006 ) that we used in our experiments. The dependency structure of our example is shown in Fig. 1b .\nThus the example phrase merchants living in the city is transformed along the relations PART- Fig. 1c . The resulting word order the city in living merchants corresponds to the Hungarian structure \"a v\u00e1ros+ban\u00e9l\u0151 keresked\u0151k\".\nSince these levels of analysis depend on each other, errors arising at each phase propagate and cumulate through the whole process having a significant effect on reordering. Even though we used the lexicalized version of the Stanford parser, which is reported to work more accurately, it still very often generates agrammatical parses with agreement errors and odd PoS sequences as shown in Table 2 (showing only the generated PoS tag sequences here).\n1 PARTMOD=participial modifier, PREP=prepositional modifier, POBJ=object of preposition.\nThe full list of dependency relations can be found in http://nlp.stanford.edu/software/ dependencies_manual.pdf -/: 100/CD million/CD sound/NN good/JJ to/TO me/PRP ./. For/IN airline/NN personnel/NNS ,/, we/PRP cash/NN personal/JJ checks/VBZ up/RP to/TO $/$ 100/CD ./. Morpheme-based restructuring: Due to the agglutinating nature of Hungarian, many function words in English are expressed as suffixes in the Hungarian translation. In order to enable the phrase-based system to have them correspond to each other, we applied morphological analysis on the Hungarian sentences segmenting each word to their morphological constituents. To annotate the Hungarian side of the corpus, we used the PurePos automated morphological annotation system (Orosz and Nov\u00e1k, 2012) . A simple example is a phrase like in my house, which is transformed to the form house my in corresponding to the single word \"h\u00e1zamban\" in Hungarian. The morphological segmentation of this word is\nDefining and applying the rules for such short phrases is not particularly difficult. However, related words in longer sentences can be much further separated from each other and they may be involved in more than one relation, which often results in an interaction of word order constraints. In a similar manner, some rules insert morphological elements corresponding to those present in the Hungarian sentence, but not explicitly expressed in English, such as the accusative case suffix or subject agreement of verbs. These pieces of implicit structural information can be induced from the dependency relations. For example, in the English phrase giving/VBG a/DT present/NN, the word present is tagged as acc (based on its object role) corresponding to the Hungarian accusative -t suffix resulting in the reordered phrase of giving a present+acc now perfectly aligning to the Hungarian structure of \"adni egy aj\u00e1nd\u00e9k+ot\""}, {"section_title": "Lexical granularity", "text": "The number of words is often rather different in a pair of Hungarian and English sentences enforcing the alignment module of the SMT system to create one-to-many or many-to-many alignments, or simply leave tokens unaligned. Such alignments often result in missing or 'hallucinated' words in the translation. Table 1 shows the differences in the average number of words and morphemes in our parallel corpus. The average number of words is smaller in Hungarian than in the English sentences. On the other hand, at least at the granularity of the morphological analysis we applied to our data, the number of morphemes is higher in Hungarian than in English. The number of tokens on both sides can be made more similar by either decreasing the number of words on the English side by joining function words corresponding to Hungarian suffixes or by increasing the number on both sides using morphemes as tokens.\nAs the difference is primarily due to the fact that some English function words are represented as suffixes in Hungarian, the relative difference between the number of morphemes in the corresponding sentences is lower than that of the words. So one possible approach to solving the The difficulty of aligning very frequent functional morphemes is illustrated by the fact that in the Giza++ alignments created from our training corpus, 39% of the nominal plural ([PL]) morphemes remained unaligned, 13% was not attached to the noun it should have been attached to, because the alignment was not monotone, while 1% was aligned to several (up to eight) instances of the corresponding morpheme. Alignment is not the only problem: some indivisible morpheme sequences (like noun+plural) should always stay together but we had concerns that, unless it is constrained to monotone decoding, the baseline distortion model of the decoder will often scatter suffixes throughout the sentence instead. A lexicalized reordering model can be expected to solve this problem, thus we used lexical reordering in our models but for comparison we also tested how each model performs when the decoder is constrained to monotone decoding.\nAnother approach we tested was fusing separate words on the English side that correspond to a single word in the Hungarian sentence (modeling English as an agglutinating language) to avoid the aligner connecting these morphemes to some other words on the Hungarian side and using a factored model to try to solve the data sparseness issues this move results in. For example, possessive determiners are attached to the head noun as suffixes in this model like the corresponding possessive suffixes in Hungarian : the phrase my/PRP$ own/JJ mother/NN is transformed to the form own/JJ mother/NN my/PRP$, which corresponds to the Hungarian phrase saj\u00e1t any\u00e1 m.\nBy applying either of the morpheme-token-based or the factored morphosyntactic-featurebased solution, the translations generated by the SMT system contain sequences of lemmas and morphosyntactic tags, thus, in order to get the final form of the translated sentence, the surface form of the words have to be generated from the morpheme sequence. In our experiments, we applied the word form generator module of the Humor morphological analyzer to the output of the decoder (Nov\u00e1k, 2003; Pr\u00f3sz\u00e9ky and Kis, 1999) ."}, {"section_title": "Factored translation", "text": "The Moses SMT toolkit , which we used in our experiments, is suitable for implementing factored translation models. Instead of relying on just the surface form of the words, further annotations such as morphological analysis can be used in the process of a factored translation. Translation factors might be the surface form of each word, its lemma, its main PoS tag and its morphosyntactic features. During factored translation, there is an opportunity to use multiple translation models, generation models or contextual language models. Since the system has the possibility to use any combination of these, in theory it is able to generate better translations using sparse linguistic data than a word-based baseline system. This feature is vital in cases where some abstraction is necessary, because some words in the sentence to be translated or generated are missing from the training set.\nTo see how well a factored model performs in the case of translation to an agglutinating language, we also trained a factored translation system combined with our reordering rules. The factors in our case were of the form: lemma/PoS | PoS+morphtags, where PoS is the main partof-speech tag and morphtags are the rest of the morphological features and extra morphemes attached to the word as described in Section 4.2. Training the system with this combination of factors to handle data sparseness issues seems reasonable in theory; however, translation of lexical and grammatical factors is compromised by a serious weakness of the factored translation implementation in Moses. If the two factors are treated as connected at training time, then if a certain combination of a lemma and its morphology is not present in the translation models, which is very frequent in the case of an agglutinating language, then it can not be translated even if both the lemma and the morphological feature set are represented in the training corpus separately. In such cases none of the factors are translated and the source word is copied to the output untranslated.\nAnother method of training a factored model is to translate factors independently. This could indeed solve data sparseness problems, but, as we noted during our experiments, another problem arises in this case: at translation time, translations of morphological tags often land at wrong lemmas. This is due to the fact when translating a phrase, the system selects a translation having one word order, e.g. [Det N V], for one factor (the lemmas) and another, e.g. [V Det N] for the other (the morphosyntactic tags). This results in ill-formed structures, such as nominal morphosyntactic features landing on verbs and verbal morphosyntactic features landing on nouns etc., thus, although the translation might contain the relevant translations regarding both lemmas and morphological features, the final sentence will be an inconsistent mixture of them, making generation of the right word forms impossible. Due to word order variations in Hungarian, this situation turned out to be rather frequent, affecting 21% of our 1000 test sentences.\nIn order to improve translations compromised by inconsistent mapping of lemmas and morphology, we introduced a postprocessing step extracting and restoring the proper positions of the morphological tags in the result of factored translations. Relying on the alignment information, the proper position of each morphological tag in the sequence can be found. At translation time, Moses can output which source words each target phrase was translated from. We introduced two auxiliary factors to the phrase table that represent alignments of our two main factors. If the alignments in the two factors mismatch, we can realign them using the auxiliary alignment factors (using the word order in the lemma factor as pivot). Once having the factors rematched, the two factors of the target translation are unified and the morphological generator can be applied to generate the final word forms. As it is evident from the evaluation data presented in Section 5, the realignment of factors consistently improved the quality of translations produced by all factored models."}, {"section_title": "Experiments and results", "text": "We performed experiments on word-based, morpheme-based and factored translations from English to Hungarian with and without applying our reordering rules as a preprocessing step. We also contrasted the performance of our experimental systems with that of some commercial systems: the rule-based MetaMorpho (Nov\u00e1k et al., 2008; Nov\u00e1k, 2009 ) and the major commercial translation services, Google Translate and Bing Translator, which apply their language independent statistical systems trained on huge parallel corpora. Low BLEU scores of translations generated by these systems (compared to those usually obtained for other languages) indicate that machine translation to Hungarian is indeed a difficult task.\nIn all of our experiments, the Moses toolkit was used for building the translation models and performing the translation task itself, using IRSTLM (Federico et al., 2008) to build language models. Wherever it was necessary, PurePos (Orosz and Nov\u00e1k, 2012) was used for morphological analysis and generation, and the Stanford Parser (Marneffe et al., 2006) for constituent and dependency parsing."}, {"section_title": "Datasets", "text": "As training data, we used the Hunglish (Varga et al., 2005) corpus, created by BME MOKK 2 and the Research Institute for Linguistics of the Hungarian Academy of Sciences. This corpus contains parallel texts from the following domains: literature and magazines, legal texts and movie subtitles. There is a great degree of variation in the quality of different parts of the corpus. We automatically eliminated sentence pairs from the corpus that caused technical problems, but overall translation quality was not checked.\nThe corpus we used for training the system consists of 1,026,836 parallel sentences with 14,553,765 words on the English side and 12,079,557 on the Hungarian side. For testing purposes, a 1000-sentence-long portion was selected from the same corpus with one reference translation. Automatic evaluation was performed on this set using the BLEU evaluation metric. Results for each system are listed in Table 3 ."}, {"section_title": "Baseline systems", "text": "We built a word-based, a morpheme-based, and a factored baseline system (featured as w, m and f in Table 3) , not using the reordering rules described in Section 4.1, each trained using Moses.\nFor the word-based baseline model w, the only preprocessing we applied was standard tokenization and lowercasing. A phrase table with a phrase length limit of 7 was extracted, and a 5-gram language model was built. A lexicalized reordering model with a distortion limit of 6 was used in this baseline model (and all other models with nonmonotone decoding).\nWe evaluated this system using two automatic metrics: the usual word-based BLEU (w-BLEU) and, in order to have a relevant base of comparison to the other systems, a morpheme-based score (mm-BLEU), which in the case of the word-based baseline was computed applying morphological analysis to the translations. mm-BLEU is based on counts of identical abstract morpheme sequences in the generated and the reference translations instead of identical word sequences. Note that this differs from m-BLEU as used in e.g. (Clifton and Sarkar, 2011) , which is BLEU applied to pseudomorphs generated by an unsupervised segmenter. mm-BLEU measures the ability of the system to generate the correct morphemes in the translations.\nThe second baseline system m was trained on morphologically segmented sentences, thus the output of the decoder is a sequence of morphemes. A BLEU score computed on the output of the decoder in this case is mm-BLEU. The morphological generator was applied to the output of the Moses decoder in order to acquire the final word forms. The morpheme-based system m performed better in terms of mm-BLEU, although it got a lower w-BLEU score.\nThe third, factored baseline model f was outperformed by the two other models both in terms of w-BLEU and mm-BLEU, even when the problem caused by a different word order in the factors was fixed as described in Section 4.3 (the system fx)."}, {"section_title": "Reordered models", "text": "Based on considerations described in Sections 4.1 and 4.2, we performed reordering as a preprocessing step both at training and translation time. Models using this configuration were also evaluated applying the same w-BLEU and mm-BLEU metrics. We implemented various morphemebased, factored and word-based reordered models. The two word-based setups performed the same transformations moving function words, the difference between the two was only whether the moved words were kept as distinct words (wre) or joined to the target word as suffixes to form a single word form (wre ). The models allowed further reordering during decoding using a lexicalized reordering model. The morpheme-based (mre) and the factored models (fre and frex, the latter with factor misalignment fixed) were contrasted with alternative setups where the decoder was constrained to monotone decoding (mrem, frem, fremx). We had concerns that in the case of the morphemebased model the decoder might move suffixes to incorrect positions. However, using a lexicalized reordering model prevented these problems and the systems with reordering during decoding performed consistently better. Monotone decoding blocked the decoder from fixing word order in the preverbal field of the comment part of Hungarian sentences, where strict word order constraints apply in contrast to the free word order of the topic and the postverbal part of the comment. While our reordering rules did not capture these constraints depending on various subtle features of the actually selected translation that cannot be reliably inferred from the English original, the lexically constrained reordering performed by the decoder did manage to generate translations that conformed to them at least to some extent.\nThe results presented in Table 3 show that the reordered wre, mre and frex models obtained consistently higher BLEU scores than the corresponding baseline models (the only exception being the mm-BLEU score of the wre model). Although the BLEU scores do not show this clearly, the translations generated by the wre model are far worse than the output of any other system due to a high number of untranslated \"agglutinating English\" words with function words attached to content words as suffixes. Figure 4 shows the translation results of our different systems. As it can be seen, mre performed the best, regarding fluency and reflecting the original meaning."}, {"section_title": "Human evaluation", "text": "It has been shown that system rankings based on single reference BLEU scores often do not correspond to how humans evaluate the translations. For this reason, automatic evaluation has for a long time not been used to officially rank systems at Workshops on Statistical Machine Translation (WMT) . In our work, we presented results of automated evaluation using a single reference BLEU metrics, but we also investigated translations generated by each system using human evaluation, applying the ranking scheme used at WMT workshops to officially rank systems. 300 sentences were randomly chosen from the test set for the purpose of human evaluation. Five annotators evaluated translations generated by each of the above described systems plus the reference translation in the corpus with regard to translation quality (considering both adequacy and fluency in a single quality ranking). The order of translations was randomized for each sentence and a balanced number of comparisons was performed for each system pair. The systems were ranked based on a score that was defined as the number of times the output of a system was deemed not worse than that of the other in pairwise comparisons divided by the number of pairwise comparisons. The aggregate results of human evaluation are listed in Table 5 .\nManual investigation of the translation outputs revealed that the system incorporating morphological and syntactic information are better at capturing grammatical relations in the original text and rendering them in the translation by generating the Miut\u00e1n felvett\u00e9k mag\u00e1t a tengeren , hallgatta a hely\u00fcnk , hogy m\u00e1lta\u00e1llta ezt a faxot . back-translation After you were picked up at sea, our listening post caught the fax in Malta. baseline translation Azut\u00e1n , hogy felvette a tengeren , a m\u00e1ltai hallgatta az emelked\u0151 , hogy fax . back-translation After you, he picked it up at the sea, and that Malta were caught, that it is a fax. Hungarian reference Miut\u00e1n\u00f6nt kihal\u00e1szt\u00e1k , ezt fogt\u00e1k el egy m\u00e1ltai post\u00e1n . back-translation After you were fished out, this was caught at a post in Malta. Table 5 : Human evaluation ranking of systems measured as percentage of generating a translation not worse than the other in pairwise comparisons appropriate inflected forms. Rule-based reordering also improved quality when using linguistically rich models. The only ones that performed worse than the baseline were the word-based reordered solutions, especially the one based on \"agglutinating English\", the poor performance of which came as no surprise. BLEU scores do not correspond well to human judgments. Of our models, the wre system had the highest BLEU score, however, human evaluation ranked that worse than any of the morpheme-based systems. Moreover, MetaMorpho, the commercial system having highest rank had by far the lowest BLEU score.\nConsidering all the systems in the ranking procedure, it can be observed that the reference translation used also for measuring BLEU score does not always represent the best translation either according to our evaluators. It is worth noting though that there was a rather significant variance in the ranking of reference translations due to some evaluators ranking them much less favourably than others (75.29% vs. 92.98%)."}, {"section_title": "Conclusion", "text": "We performed several experiments on EnglishHungarian machine translation. Automatic evaluation consistently scored models including rulebased reordering higher than systems not including it. Human evaluation confirmed that applying reordering and morphological segmentation does improve translation quality in the case of translating to an agglutinating language like Hungarian.\nOur models are not yet on par with commercial systems. The rather limited amount of training corpus that also has serious quality problems is certainly one factor playing a role in this. Our future plans include enlarging and improving our training corpus, improving alignment and components of the syntactic annotation and reordering chain as well as experimenting with combination of morpheme-based and factored models."}]