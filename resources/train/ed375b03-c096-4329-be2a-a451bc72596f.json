[{"section_title": "List of", "text": "4  of schools in the fall first-grade school sample with base-year respondents, by selected characteristics: "}, {"section_title": "INTRODUCTION", "text": "This manual provides guidance and documentation for users of the longitudinal kindergarten-first grade (K-1) data file of the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011. It mainly provides information specific to the first-grade rounds of data collection. Users should refer to the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), User's Manual for the ECLS-K:2011 Kindergarten Data File andElectronic Codebook, Public Version (NCES 2015-074) (Tourangeau et al. 2015) for information about the general study methodology and the kindergarten rounds of data collection. Data for the ECLS-K:2011 are released in both a restricted-use and a public-use version. This manual, which has been developed for public dissemination and use with the public version of the data, is almost identical to the manual released with the kindergarten-first grade restricted-use file. 1 Edits have been made to round or remove unweighted sample sizes that cannot be generated with the public-use file (PUF). Estimates such as means that are presented in the tables throughout the manual were calculated with the restricted-use file. Some estimates may not be able to be reproduced exactly with variables in the PUF because the variables have been masked to make them suitable for public release. Appendix B provides information about the ways in which data were masked on the PUF and includes tables that list all variables that have been masked or suppressed. Also, throughout this manual references are made to materials that are on the restricted-use CD-ROM. Public-release versions of these materials are available under \"Data Products\" on the ECLS-K:2011 website, http://nces.ed.gov/ecls/kindergarten2011.asp. This chapter provides an overview of the ECLS-K:2011. Subsequent chapters provide details on the first-grade data collection instruments and methods, including a description of how the first-grade data collections differ from the kindergarten rounds; the direct and indirect child assessments; the sample design; weighting procedures; response rates; and data file content, including composite variables. The ECLS-K:2011 is following a nationally representative sample of children from kindergarten through their elementary school years. It is a multisource, multimethod study that focuses on children's early school experiences. It includes interviews with parents, self-administered questionnaires completed by teachers and school administrators, and one-on-one assessments of children. During the kindergarten year, it also included self-administered questionnaires for nonparental before-and afterschool care providers. The ECLS-K:2011 is sponsored by the National Center for Education Statistics (NCES) within the Institute of Education Sciences (IES) of the U.S. Department of Education."}, {"section_title": "Background", "text": "The ECLS-K:2011 is the third and latest study in the Early Childhood Longitudinal Study (ECLS) program, which comprises three longitudinal studies of young children: the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K); the Early Childhood Longitudinal Study, Birth Cohort (ECLS-B); and the ECLS-K:2011. The ECLS program is broad in its scope and coverage of child development, early learning, and school progress. It draws together information from multiple sources, including school administrators, parents, teachers, early care and education providers, and children, to provide data for researchers and policymakers to use to answer questions regarding children's early educational experiences and address important policy questions. The ECLS-K:2011 provides current information about today's elementary school children and data relevant to emerging policy-related domains not measured fully in the previous studies. Also, coming more than a decade after the inception of the ECLS-K, the ECLS-K:2011 allows for cross-cohort comparisons of two nationally representative kindergarten classes experiencing different policy, educational, and demographic environments. The three studies in the ECLS program provide national data on children's developmental status at birth and at various points thereafter; children's transitions to nonparental care, early education programs, and school; and children's home and school experiences, growth, and learning. The ECLS program also provides data that enable researchers to analyze how a wide range of child, family, school, classroom, nonparental care and education provider, and community characteristics relate to children's development and to their experiences and success in school. Together these cohorts provide the range and breadth of data needed to more fully describe and understand children's education experiences, early learning, development, and health in the late 1990s, 2000s, and 2010s. More information about all three of these studies can be found on the ECLS website (http://nces.ed.gov/ecls). 1-2"}, {"section_title": "1.2", "text": ""}, {"section_title": "Periods of Data Collection", "text": "The ECLS-K:2011 is following a cohort of children from their kindergarten year (the 2010-11 school year) through the 2015-16 school year, when most of the children are expected to be in fifth grade (exhibit 1-1). The sample includes both children who were in kindergarten for the first time and those who were repeating kindergarten during 2010-11. Although the study refers to later rounds of data collection by the grade the majority of children are expected to be in (that is, the modal grade for children who were in kindergarten in the 2010-11 school year), children are being included in subsequent data collections regardless of their grade level. 2 During the 2010-11 school year when both a fall and a spring data collection were conducted, approximately 18,170 kindergartners from about 1,310 schools 3 and their parents, teachers, school administrators, and before-and after-school care providers participated in the study. Fall and spring data collections were also conducted during the first-grade year. While the fall kindergarten collection included the full ECLS-K:2011 sample, the fall first-grade collection was conducted with children in approximately one-third of the sample of primary sampling units (PSUs) selected for the study. These children are referred to as the fall subsample. The planned data collection schedule for second grade is similar to the schedule for first grade, with a fall collection that includes the fall subsample of children and a spring collection that includes the full sample. For third through fifth grade, spring data collections with the entire sample of children who participated in the base-year data collection are planned. 4 1.4"}, {"section_title": "ECLS-K:2011 Kindergarten-First Grade (K-1) Data File", "text": "The ECLS-K:2011 K-1 data file includes both the base-year and first-grade data, encompassing both the fall and spring rounds of data collection in kindergarten and first grade. The data file includes all cases that participated during the kindergarten year even if they did not participate during the first grade rounds. First-grade data for cases that did not participate are set to \"system missing\" for the first-grade round or rounds in which they are nonrespondents. The K-1 data file is intended to replace the previously released base-year data file; the K-1 file includes all of the cases included on the base-year file and has some important corrections and updates to previously released data, including the child assessment scores. In preparing data files for release, NCES takes steps to minimize the likelihood that individual schools, teachers, parents, or students participating in the study can be identified. Every effort is made to protect the identity of individual respondents. The process of preparing the files for release includes a formal disclosure risk analysis. Small percentages of values are swapped across cases with similar characteristics to make it very difficult to identify a respondent with certainty. The modifications used to reduce the likelihood that any respondent could be identified in the data do not affect the overall data quality. Analysts should be aware that the ECLS-K:2011 data file is provided as a child-level data file containing one record for each child who participated in the base year. The record for each child contains information from each of the study respondents: the child, as well as his or her parent, teacher(s), school administrator and, if applicable, before-or after-school care provider. The ECLS-K:2011 K-1 data are provided on CD-ROM in an electronic codebook (ECB) that permits analysts to view the variable frequencies, tag selected variables, and prepare data extract files for analysis with SAS, SPSS, or Stata."}, {"section_title": "Contents of Manual", "text": "The remainder of this manual contains more detailed information on the first-grade data collection instruments (chapter 2) and the direct and indirect child assessments (chapter 3). It also describes the ECLS-K:2011 sample design and weighting procedures (chapter 4), response rates and bias 1-5 analysis (chapter 5), and data preparation procedures (chapter 6). In addition, this manual describes the structure of the K-1 data file and the composite variables that have been developed for the file (chapter 7). Additional information about the ECLS-K:2011 study design, methods, and measures can be found in the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011, Kindergarten Year Methodology Report (Tourangeau et al. forthcoming) "}, {"section_title": "and the Early Childhood", "text": "Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011, Kindergarten Psychometric Report (Najarian et al. forthcoming). Also, as noted earlier, additional information about the ECLS program can be found online at http://nces.ed.gov/ecls. 1-6"}, {"section_title": "DATA COLLECTION INSTRUMENTS AND METHODS", "text": "This chapter describes the data collection instruments used in the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011) first-grade rounds of data collection, including the child assessments, parent interview, school administrator questionnaires, and teacher questionnaires. 1 Differences between the kindergarten and first-grade rounds in the study instruments and data collection procedures are discussed."}, {"section_title": "Data Collection Instruments", "text": "The design of the ECLS-K:2011 and its survey instruments is guided by a conceptual framework of children's development and learning that emphasizes the interaction among the various environments in which children live and the resources within those environments to which children have access. A comprehensive picture of children's environments and experiences is created by combining information from children themselves, their parents, their school administrators, their teachers, and their kindergarten before-and after-school care providers. Exhibit 2-1 presents a listing of the ECLS-K:2011 data collection instruments and the rounds of data collection in which they were used. The instruments for the kindergarten and first-grade collections are included on the ECLS-K:2011 kindergarten-first grade (K-1) CD-ROM and are available online at http://nces.ed.gov/ecls, with the exception of copyrighted materials or items adapted from copyrighted materials that cannot be publicly distributed without copyright holder and NCES permission.  "}, {"section_title": "Direct Child Assessment", "text": "In the first-grade data collections, children were assessed in reading, mathematics, and science in both the fall and the spring. 2 The majority of the items included in the first-grade assessments had been included in the kindergarten assessments. However, to ensure that the assessments adequately measured the knowledge and skills of the children as they progressed through school, new, more difficult items were added to the assessments in first grade, and easier items reflecting lower level kindergarten skills were omitted. All children received the assessments designed for the first-grade collections, regardless of their actual grade level. In both the fall and the spring, students' executive function skills were assessed with the same measures fielded in kindergarten. Finally, children's height and weight were 2 During the kindergarten year, children were assessed in science only in the spring. 2-2 measured again in both fall and spring. The assessment was administered directly to the sampled children on an individual basis by trained and certified child assessors. It was designed to be administered within about 60 minutes per child. 3 Child responses were entered by the assessors into a computer-assisted interviewing (CAI) program. Two-stage assessment. The first-grade direct cognitive assessment included two-stage assessments for reading, mathematics, and science. For each assessment domain, the first stage of the assessment was a routing section that included items covering a broad range of difficulty. A child's performance on the routing section of a domain determined which one of three second-stage tests (low, middle, or high difficulty) the child was next administered for that domain. The second-stage tests varied by level of difficulty so that a child would be administered questions appropriate for his or her demonstrated level of ability for each of the cognitive domains. The purpose of this adaptive assessment design was to maximize accuracy of measurement and minimize administration time. Language screener for children whose home language was not English and routing through the assessment battery. The components of the ECLS-K:2011 assessments administered to children who spoke a language other than English at home depended on the children's performance on a language screener. The screener consisted of two tasks from the Preschool Language Assessment Scale (preLAS 2000). 4 The \"Simon Says\" task required children to follow simple, direct instructions given by the assessor in English. The \"Art Show\" task was a picture vocabulary assessment that tested children's expressive vocabulary. In the fall and spring kindergarten rounds, all children were administered the language screener as the first component of the direct cognitive assessment, regardless of their home language. 5 For children whose home language was English, the screener primarily served as a warm-up or practice for the rest of the assessment since the items were of low difficulty. While the screener also served as a warm-up for children whose home language was one other than English, it also determined whether the children understood English well enough to receive the full direct child assessment in English. In contrast to the procedures used in kindergarten, the screener was not administered to all children in the first-grade collections. The two preLAS 2000 tasks were administered only to children who spoke a language other than English at home who had not passed the screener in the most recent round in which they were assessed. 6 For example, children who spoke a language other than English at home who were assessed most recently in the spring of kindergarten and did not pass the preLAS screener at that time were administered the screener the next time they were assessed. Such children who were part of the fall first-grade subsample were administered the preLAS screener in the fall of first grade. If they did not pass the screener in the fall, it was administered to them again in the spring. Children who were not part of the fall subsample, who spoke a language other than English at home, and who did not achieve at least a minimum score on the screener in the spring of kindergarten were administered the preLAS screener in the spring of first grade. Children who were not administered the language screener either because they did not speak a language other than English at home or because they passed the screener in a previous round were asked only two of the preLAS \"Art Show\" items as a warm-up; they were not administered any of the other preLAS items. In first grade, all children routed to the English version of the assessment were then administered the 30-item reading routing test. Depending on the number of correct responses a child provided to items on the reading routing test, he or she was routed to one of three second-stage reading tests. Those children whose scores routed them to the low or middle second-stage tests in reading first received 18 items that contribute to the calculation of an English basic reading skills (EBRS) score. 7 After administration of these 18 items, students proceeded into the low or middle second-stage test. Children who were routed to the high second-stage test based on their scores on the 30-item router were not administered the 18 items that contribute to the EBRS because these items were considered too easy for their demonstrated ability level. Once the reading assessments were complete, the mathematics, science, and executive function measures were administered in English, followed by measurements of height and weight. Children who were administered the preLAS 2000 in first grade and did not achieve at least the minimum score on the language screener were administered the 18 EBRS items after the screener. Once the EBRS items were administered, the cognitive assessments in English ended for children whose home language was not English. Spanish-speaking children who did not achieve at least the minimum score on the screener were then administered a short reading assessment in Spanish that measured Spanish early reading skills (SERS), as well as the mathematics and executive function assessments that had been translated into Spanish. Children whose home language was one other than English or Spanish and who did not achieve at least the minimum score on the screener were not administered any of the remaining cognitive assessments, although all children had their height and weight measured.  illustrates how the first-grade assessments taken by children depended on their home language and on their performance on the language screener. No 1 Home language designation was identified in the kindergarten rounds of data collection. 2 The EBRS was administered in the English reading battery only to children who were routed to the low and middle second-stage reading forms. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2011 and spring 2012."}, {"section_title": "2-5", "text": "Cognitive domains. The cognitive assessment focused on four domains in the fall and spring first-grade rounds: reading (language use and literacy), mathematics, science, and executive function (working memory and cognitive flexibility). For the reading, mathematics, and science assessments, assessors asked the children questions related to images (such as pictures, words, or short sentences for reading or numbers and number problems for mathematics) that were presented on a small easel. For the reading assessment, children were also asked questions about short reading selections they were asked to read in a passages booklet developed for the assessment. Children could respond by pointing or telling the assessor their answers. They were not required to write their answers or explain their reasoning. The executive function component included a card sort task that required children to sort cards into trays, and a numbers reversed task for which children provided verbal responses; both of these tasks are discussed further below. A brief description of all the components of the cognitive assessment follows. Reading (language and literacy). The reading assessment included questions measuring basic skills (print familiarity, letter recognition, beginning and ending sounds, rhyming words, and word recognition), vocabulary knowledge, and reading comprehension. Reading comprehension questions asked the child to identify information specifically stated in text (e.g., definitions, facts, supporting details), make complex inferences within and across texts, and consider the text objectively and judge its appropriateness and quality. As noted above, the first 30 items in the reading assessment make up the routing form. Scores on the routing form determined if the EBRS was administered and which second-stage test (low, middle, or high) the child received. Spanish speakers who were routed out of the English cognitive assessment after the EBRS were administered an assessment that measured Spanish early reading skills (SERS). The SERS consisted of 31 items included in the English reading assessment (in the low or middle second-stage test) that had been translated into Spanish. Mathematics. The mathematics assessment was designed to measure skills in conceptual knowledge, procedural knowledge, and problem solving. The assessment consisted of questions on number sense, properties, and operations; measurement; geometry and spatial sense; data analysis, statistics, and probability; and patterns, algebra, and functions. A set of 17 routing items was administered to all children, and the children's score on these items determined which second-stage test (low, middle, or high difficulty) the child received. Most of the text that the children could see on the easel pages, for example, question text for word problems or graph labels, was read to the children to reduce the [2][3][4][5][6] likelihood that their reading ability would affect their mathematics assessment performance. 8 Paper and pencil were offered to the children to use for the mathematics assessment, and children were periodically reminded of their availability as part of the assessment protocol. Some second-stage mathematics assessment forms also contained several items for which wooden blocks were available for children to use in solving the problems. However, children were not required to use blocks. Spanish-speaking children who did not pass the language screener completed the full mathematics assessment administered in Spanish. Science. The science assessment domain included questions about physical sciences, life sciences, environmental sciences, and scientific inquiry. The science assessment included 15 routing items that all children who were administered the science assessment received, followed by one of three second-stage forms (low, middle, or high difficulty). As with reading and mathematics, the second-stage form children received depended on their responses to the routing form items. The questions, response options, and any text the children could see on the easel pages (for example, graph labels) were read to the children to reduce the likelihood that their reading ability would affect their science assessment score. To measure cognitive flexibility, children were administered the Dimensional Change Card Sort (DCCS) (Zelazo 2006). In this task, children were asked to sort a series of 22 picture cards into one of two trays according to different rules. Each card had a picture of either a red rabbit or a blue boat; one tray had a picture of a red boat and the other had a picture of a blue rabbit. Children were asked to sort the cards first by color and then by shape. If the child correctly sorted four of the six cards by shape, then he or she moved on to a third sorting rule: if the card had a black border, the child was to sort by color; if the card did not have a black border, the child was to sort by shape. After the card sort, children were administered the Numbers Reversed task. In this task, they were asked to repeat increasingly long strings of orally presented numbers in reverse order. When children responded incorrectly to a certain number of items in a row, the task ended so that they would not be asked to continue at a level that was too difficult. 8 Numbers were read to the child only when the question text referenced the number."}, {"section_title": "2-7", "text": "Height and weight measurement. In addition to the cognitive domains described above, children's height and weight were measured at each data collection point. Assessors recorded the children's height (in inches to the nearest quarter inch) and weight (in pounds to one decimal place). A Shorr board (a tall wooden stand with a ruled edge, used for measuring height) and a digital scale were used to obtain the measurements, which were recorded on a height and weight recording form and then entered into a laptop computer by field staff. 9 Each measurement was taken and recorded twice to ensure reliable measurement."}, {"section_title": "Parent Interview", "text": "As in the base (i.e., kindergarten) year, a parent interview was conducted during the fall and spring first-grade data collections. While the spring first-grade parent interview was about the same length as the spring kindergarten parent interview and captured much of the same information that was asked about in the base year, the fall first-grade parent interview was relatively short and focused on children's experiences during the summer. Parents provided information about various educational and enrichment activities the child participated in during the previous summer, including educational activities in the home; use of a computer for educational purposes; reading books from summer book lists provided by the school; going to the library or bookstore; playing outside; outings with family members; camps; summer school; tutoring; therapy services or special education programs; hours spent watching television and playing video games; and nonparental child care. In addition, information about children's demographic characteristics was collected if it had not been collected in kindergarten. The spring first-grade parent interview included many of the same questions that were included in the kindergarten rounds of the study, for example, questions about parent involvement in the child's school, children's participation in out-of-school activities, household food security, and child health and well-being. Questions about homework, time children spent playing video games, school tardiness, parenting stress, social support, inconsistent discipline, how often the respondent or spouse attended religious services, and whether there had been a change in the relationship of one of the parent figures to the child (e.g., adoption) that were not asked in the base year were added to the spring firstgrade parent interview. 9 The Shorr board is manufactured by Weigh and Measure, LLC, and is model ICA. The digital scale was Seca Bella model 840."}, {"section_title": "2-8", "text": "Exhibit 2-3 shows the content areas included in the parent interview in the fall and spring kindergarten and fall and spring first-grade rounds. While many of the same topics were addressed across rounds, there were differences in the specific questions asked for each topic. For example, questions about home activities in the fall of first grade included questions about reading to the child during a typical week of the previous summer, participation in camps, and attendance at summer school, whereas questions in that section in the spring first-grade round asked about current reading to the child in a typical week, number of books in the home, and the child's extracurricular activities outside of school hours. The average length of the parent interview was approximately 11 minutes in the fall of first grade and 43 minutes in the spring of first grade. The respondent to the parent interview, which was conducted by telephone for most cases, was usually a parent or guardian in the household who identified himself or herself as the person who knew the most about the child's care, education, and health. During the fall and spring first-grade data collection rounds, interviewers attempted to complete the parent interview with the same respondent who completed the parent interview in the kindergarten rounds, although another parent or guardian in the household who knew about the child's care, education, and health was selected if the previous respondent was not available. The parent interview was fully translated into Spanish before data collection began and was administered by bilingual interviewers if parent respondents preferred to speak in Spanish. The parent interview was not translated into other languages because it was cost prohibitive to do so. However, interviews were completed with parents who spoke other languages by using an interpreter who translated from the English during the interview."}, {"section_title": "2-9", "text": "Exhibit 2-3. Parent interview topics, by round of data collection in the ECLS-K:2011: School years 2010 -11 and 2011-12 Parent interview content"}, {"section_title": "Fall kindergarten", "text": "Spring kindergarten"}, {"section_title": "Fall first grade", "text": "Spring first grade Child care arrangements 1 X X X X Child demographic characteristics Child disabilities and services 2 X X X X X X X Child health and well-being X X X Child social skills, problem behaviors, and approaches to learning Country of origin of parent and child X X Family structure X X X Food sufficiency and food consumption X X Household roster X X X Home environment, activities, resources, and cognitive stimulation 3 X X X X Home language 4 X X X Involvement of nonresident parent X X X Neighborhood safety X X Parent characteristics Parent-child relationship X X X X Parenting stress X Parent education 4 X X X Parent employment X X Parent income and assets X X Parent involvement with the child's education X X X Parent marital history 4 X X Parent respondent's psychological well-being and health X Parent social support X Parental beliefs and expectations related to education X Parental discipline, warmth, and emotional supportiveness X X Welfare and other public transfers X X X 1 In the fall of kindergarten, questions were asked about current child care and child care in the year before kindergarten. In the spring of kindergarten, questions about child care in the year before kindergarten were asked if information had not been collected in the fall. In the fall of first grade, questions were about child care during the previous summer. In the spring of first grade, questions asked about current child care. 2 Questions in the fall first-grade interview were about services for special needs or participation in a special education program over the previous summer. Questions about disabilities and services in other rounds of the study were not limited to the past summer. 3 Questions in the fall first-grade interview were about home activities, outings with family members, camps, and summer school during the previous summer. Questions in other rounds of the study were not limited to the summer. 4 Asked in the spring of kindergarten if information had not been collected in the fall. "}, {"section_title": "General Classroom Teacher Questionnaires", "text": "During the first-grade year, the general classroom teachers of children in the study completed a self-administered hard-copy questionnaire about themselves and their classrooms as well as child-level questionnaires for each child in their classrooms who was participating in the ECLS-K:2011. The purpose of the teacher-level questionnaire was to collect information about children's classroom experiences that may relate to children's academic and social development. It included questions about the classroom and student characteristics, class materials, instructional practices and curricula, evaluation practices, and parent involvement. It also included questions on the teacher's background, teaching experience, and attitudes about teaching and the school climate. The purpose of the child-level questionnaires was to collect information specifically about each study child's experiences and performance in the classroom. In both rounds of collection, information was collected in the child-level questionnaires about the children's academic and cognitive abilities, behavior, social skills, and achievement group placement in mathematics and reading, if applicable. In the fall of the first-grade year, teachers were asked to complete only a short child-level teacher questionnaire; in contrast to procedures used in kindergarten, there was no fall teacher-level questionnaire. The fall first-grade version of the child-level questionnaire contained some of the same items as the fall kindergarten version, namely a small set of indicators that were useful measures early in the school year, including achievement group assignment and social skills. The differences between the kindergarten and first-grade child-level questionnaires were as follows: The fall first-grade Academic Rating Scale for language and literacy skills contained only two items, and these were updated to reflect appropriate skills for first grade; there was no Academic Rating Scale for science or mathematical thinking. A question about half-or full-day kindergarten program attendance was omitted. Items were added about the child's grade level placement, whether the child had been given assignment(s) to complete over the summer and, if so, what those assignment encompassed and the extent to which the child completed the assignment. Similar to the spring kindergarten collection, both a teacher-level questionnaire and a childlevel questionnaire were included in the spring first-grade data collection. However, in first grade two versions of each type of teacher questionnaire were used: one for teachers of participating students who were in first grade during the data collection (titled \"Spring 2012 Teacher Questionnaire\") and another for teachers of participating students who had been retained in kindergarten for the 2011-12 school year (titled \"Spring 2012 Kindergarten Teacher Questionnaire\"). This was done so that the items describing use of class time, instructional activities, curricular focus, and other aspects of the classroom would focus on the appropriate grade level. The teacher-level questionnaires used in the spring of the first-grade year were very similar in content and length to the questionnaire that had been used in the spring kindergarten collection. In the questionnaire given to teachers of students in first grade, wording changes were made where necessary to make the questions applicable for first grade. Questions that were only or mostly applicable only to kindergarten were dropped. For example, teachers were not asked to report separately about morning, afternoon, or full-day classes. Both versions of the spring first-grade teacher-level questionnaire for this data collection period included new questions addressing aspects of Response to Intervention (RtI) Programs, 10 such as identification of students who were struggling with language arts and/or mathematics instruction, the provision of more intensive instruction for struggling students, and tracking students' progress. Other new items in both versions included: the time children spend working independently, in small groups, and in a large group (which replaced a kindergarten item on time spent in teacher-directed vs. student-directed activities); availability of computers and Internet access; the use of technology such as computers, smart boards, and DVD players; and an item on school climate. The spring teacher-level questionnaire for students who were in kindergarten in spring 2012 was nearly identical to the one used in spring kindergarten. Omitted from both versions were items that had been asked in kindergarten about regular meetings with other teachers; the number of children with disabilities, by specific disability; and standards used for evaluation of children. Omitted from the questionnaire for teachers of first-grade students were items on interest or activity areas in the classroom, kindergarten transition activities, and additional reading instruction services. In addition, the first-grade version of the teacher-level questionnaire included new items about classroom instruction and curricula that were aligned with Common Core State Standards 11 and focused on skills taught rather than on the instructional activities used to teach those skills. This change was made at the recommendation of members of the study's Technical Review Panel. Similar to the design of the teacher-level questionnaire, there were two versions of the child-level questionnaire used in the spring first-grade collection: one for teachers of study children who were in first grade, titled \"Spring 2012 Teacher Questionnaire Child Level,\" and one for teachers of 10 Response to Intervention (RtI) can be defined as a system for general, remedial, and special education that integrates assessment, evidence-based intervention, and student monitoring within a multitiered system designed to maximize student achievement and reduce behavior problems by tailoring the type and intensity of interventions based on individual student performance. RtI can also be used to identify students with learning disabilities. 11 See the website of the Common Core State Standards Initiative: http://www.corestandards.org/. 2-12 study children who had been retained in kindergarten, titled \"Spring 2012 Kindergarten Teacher Questionnaire Child Level.\" The two versions were very similar, with some minor wording differences to refer to the appropriate grade level. Items relevant only to kindergarten were omitted from the questionnaire for first-grade students (for example, prekindergarten services the child had received). Compared with the spring kindergarten child-level questionnaire, the child-level instruments used in the spring of 2012 for both ongrade and retained students included two new items: (1) one asking for which subjects the respondent was the child's primary teacher and (2) one asking for the teacher's estimation of how far the student would progress in his or her education. An item was added to the set of social skills items to be consistent with the social skills items that were asked in the ECLS-K in first grade. The mathematics and language and literacy Academic Rating Scales included in the questionnaire for teachers of children in first grade were modified from those used in kindergarten to make the scales reflect first-grade skills and knowledge. The mathematics and language and literacy Academic Rating Scales for teachers of students retained in kindergarten were the same as those used in spring of the kindergarten year. A science Academic Rating Scale was included for the students in first grade, with items similar to those used in fall of the kindergarten year data collection, but updated to reflect firstgrade skills and knowledge. In addition, an item about the type of language instruction English language learner (ELL) students received was revised. Exhibits 2-4 and 2-5 show the topics addressed in the kindergarten and first-grade teacher-level questionnaires and child-level questionnaires, respectively, by data collection round."}, {"section_title": "2-13", "text": "Exhibit 2-4. General classroom teacher teacher-level questionnaire topics, by round of data collection in the ECLS-K:2011: School years 2010- 11 and 2011-12 Teacher-level questionnaire content "}, {"section_title": "Special Education Teacher Questionnaires", "text": "As in the kindergarten year, a set of special education teacher questionnaires was completed in the spring of the first-grade year for each participating child with an Individualized Education Program (IEP) or equivalent program on record with the school. The respondent to the questionnaire could have been a staff member identified as the child's special education teacher, a related service provider if the child was not taught by a special education teacher, or the child's general classroom teacher if that teacher provided all of the child's education and services required by an IEP. Similar to the model used for the general classroom teacher questionnaires, two self-administered hard-copy instruments were used: a teacher-level questionnaire that collected information on the special education teacher's background, education, teaching experience, teaching position, and caseload; and a child-level questionnaire that collected information on the individual study child's disabilities, placement, and services received."}, {"section_title": "2-15", "text": "The special education teacher-level questionnaire used in first grade was almost identical to the questionnaire used in the spring of kindergarten. The only difference is that while the kindergarten teacher-level questionnaire asked a general question about courses taken related to Response to Intervention (RtI), the first-grade teacher-level questionnaire contained a more detailed item on coursework, listing course topics central to RtI programs. The special education teacher child-level questionnaire addressed the following topics in both kindergarten and first grade: current services received through an IEP; child's disabilities (primary and all those for which the child has received services); IEP goals and meeting those goals; classroom placement; expectations regarding general education goals; and the special education teacher's communication with other teachers and the child's parents. Two new items were added for first grade: the child's grade placement and his or her participation in assessments. An item on prekindergarten services the child had received, which was included in the spring of kindergarten, was omitted from the first-grade child-level questionnaire. Exhibit 2-6 shows the topics addressed in the kindergarten and first-grade special education teacher-level and child-level questionnaires by data collection round. "}, {"section_title": "School Administrator Questionnaires", "text": "In first grade, there were two versions of the school administrator questionnaire (SAQ): (1) a version for schools that were new to the study or for which a completed school administrator questionnaire was not received in the kindergarten year and (2) a shorter version for schools for which a school administrator questionnaire was completed in the kindergarten year. To reduce respondent burden, the shorter version did not include questions for which the responses were not expected to change significantly from year to year, for example, grades offered by the school, type of school (public, private, magnet, charter), adequacy of facilities, and grade retention policies. The school administrator questionnaires were hard-copy paper questionnaires completed by the school principal/administrator and/or his or her designee during the spring data collection round of the first-grade year. This is similar to the procedures used in the kindergarten rounds, where the school 2-17 administrator questionnaire was only fielded in the spring of kindergarten. The school administrator questionnaires addressed the following topics: school characteristics, facilities, and resources; schoolfamily-community connections; school policies and practices; school programs for particular populations (language minority children and children with special needs); Federal programs; staffing and teacher characteristics; and school administrator characteristics and background. While the school administrator questionnaires were very similar in the kindergarten and first-grade years, the questionnaires for first grade included new items related to charter schools; the implementation of Response to Intervention (RtI) programs; numbers of students evaluated and found eligible for IEPs; the method of determining eligibility for an IEP; monetary incentives for teachers for improved student performance; and whether or not the administrator spoke a language other than English during school hours with students and their families. Some items were revised, including school-based programs for parents and families; neighborhood problems; school safety issues; and recent changes at the school such as changes in funding, enrollment, student mobility, and staffing. Items that related specifically to kindergarten were either reworded to refer to first grade or, if not relevant to first grade, omitted from the first-grade questionnaires; for example, whether the school had a half-day or full-day kindergarten program and kindergarten readiness/placement testing. Other items that were omitted in first grade were about the availability of computers and Internet access since the study also gathers this information from the general classroom teachers. Exhibit 2-7 shows the topics addressed in the kindergarten and first-grade school administrator questionnaires by data collection round.  (returning schools) School characteristics, facilities, and resources"}, {"section_title": "X X X", "text": "School-family-community connections X X X School policies and practices X X X Response to Intervention programs X X School programs for particular populations (language minority children and children with special needs) X X X Federal programs X X X Staffing and teacher characteristics X X X School administrator characteristics and background X X X study procedures for school recruitment, field staff training, school contact in the fall, data collection, tracing activities, and data collection quality control. More detailed information about data collection methods can be found in the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011, Kindergarten Methodology Report (Tourangeau et al. forthcoming)."}, {"section_title": "Differences in Data Collection Methods Between Kindergarten and First Grade", "text": "School recruitment: For first grade, data collection staff team leaders recruited only new transfer schools, meaning those schools to which study children moved between kindergarten and the fall of first grade or between the fall and spring of first grade. Recruitment was not repeated for schools that had participated in the kindergarten year. Field staff training: Training for team leaders, school recruiters, assessors, and parent interviewers for the fall first-grade collection was conducted in person. Team leader and assessor training for the spring first-grade collection was also held in person, but parent-interviewer training was conducted via Web-Ex 12 and telephone role plays. 13 Advance school contact in the fall: All schools, including those that were not part of the fall subsample, were contacted in the fall to arrange for the spring assessments and to confirm that children who had attended the school in kindergarten were still enrolled. If a child was not still enrolled in the school, the school was asked to provide any updated contact information, including the child's new school, if the school had such information. Data collection: Data collection procedures used in first grade were the same as those used during the kindergarten year. As described above, however, revisions were made to the instruments that had been used in the kindergarten rounds. Tracing activities: In addition to the tracing activities described in the base-year User's Manual, birthday cards were mailed to sampled children. This helped to maintain a positive relationship 12 WebEx is an Internet-based web conferencing tool for sharing presentations in any format with an audience in multiple remote locations. The CAPI application was shown to interviewers using this tool. 13 Telephone role plays were conducted by having trainees work one-on-one interviewing Westat Telephone Research Center (TRC) staff. Members of the TRC were first trained by home office staff on project-specific interviewing techniques and providing appropriate feedback to interviewers."}, {"section_title": "2-21", "text": "with the study children and their families. It also served as a way to obtain updated home addresses; the project staff asked the post office for a forwarding address if the children had moved, and the card also acted as a prompt for parents to let project staff know about any address changes. Quality control: Quality control and validation procedures remained the same as in the kindergarten round."}, {"section_title": "2-22", "text": ""}, {"section_title": "ECLS-K:2011 DIRECT AND INDIRECT ASSESSMENT DATA", "text": "This chapter provides information about the direct and indirect assessment data from the kindergarten and first-grade year of the ECLS-K:2011. Although this manual primarily focuses on the first-grade collections, information is provided about the kindergarten assessment data for two main reasons: (1) it is expected that many analysts will be interested in including both kindergarten and firstgrade assessment data in their analyses, and 2 "}, {"section_title": "Direct Cognitive Assessment: Reading, Mathematics, and Science", "text": "The kindergarten and first-grade direct cognitive assessments measured children's knowledge and skills in reading, mathematics, and science. This section presents information about the assessment scores available in the data file. More detailed information about the development of the scores, including a more complete discussion of item response theory (IRT) procedures, can be found in the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011, Kindergarten Psychometric Report (Najarian et al. forthcoming) and in the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011, First-Grade and Second-Grade Psychometric Report (Najarian et al. forthcoming). A description of the administration of the direct assessments is provided in chapter 2, section 2.1.1. It must be emphasized that the assessment scores described below are not directly comparable with those developed for the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K). Although the IRT procedures used in the analysis of data were similar in the ECLS-K and in the ECLS-K:2011, each study incorporated different items and the resulting scales are different. 3-1"}, {"section_title": "IRT-Based Scores Developed for the ECLS-K:2011", "text": "Broad-based scores using the full set of items administered in the kindergarten and firstgrade assessments in reading, mathematics, science, and Spanish early reading skills (SERS) were calculated using IRT procedures. IRT is a method for modeling assessment data that makes it possible to calculate an overall score for each domain measured for each child that can be compared to scores of other children regardless of which specific items a child is administered. This method was used to calculate scores for the ECLS-K:2011 because, as discussed in chapter 2, the study employed a two-stage assessment (in reading and mathematics in kindergarten and in reading, mathematics, and science in first grade) in which children were administered a set of items appropriate for their demonstrated ability level, rather than all the items in the assessment. Although this procedure resulted in children being administered different sets of items, there was a subset of items that all children received (the items in the routing tests, plus a set of items common across the different second-stage forms). These common items were used to calculate scores for all children on the same scale. Similarly, for the single-stage (spring kindergarten) science and SERS assessments, IRT was used to calculate scores for all children on the same scale. In the single-stage forms, the assortment of items a child received was not dependent upon routing to a second stage, but instead on omissions by the child or the discontinuation of the administration of the assessment. In those cases, IRT was used to estimate the probability that a child would have provided a correct response when no response was available. IRT uses the pattern of right and wrong responses to the items actually administered in an assessment and the difficulty, discriminating ability, 1 and \"guess-ability\" of each item to estimate each child's ability on the same continuous scale. IRT has several advantages over raw number-right scoring. By using the overall pattern of right and wrong responses and the characteristics of each item to estimate ability, IRT can adjust for the possibility of a low-ability child guessing several difficult items correctly. If answers on several easy items are wrong, the probability of a correct answer on a difficult item would be quite low. Omitted items are also less likely to cause distortion of scores, as long as enough items have been answered to establish a consistent pattern of right and wrong answers. Unlike raw number-right scoring, which treats omitted items as if they had been answered incorrectly, IRT procedures use the pattern of responses to estimate the probability of a child providing a correct response for each assessment question. Finally, IRT scoring makes possible longitudinal measurement of gain in achievement, even when the assessments that are administered to a child are not identical at each point (for example, when a child was administered a 1 The discriminating ability describes how well changes in ability level predict changes in the probability of answering the item correctly at a particular ability level."}, {"section_title": "3-2", "text": "different level of the second-stage form of a given domain in the spring data collection than in the fall data collection). Two methods were used to calculate the scores provided in the data file. For scores within a grade (e.g., the fall and spring of first grade), a concurrent calibration model was applied where, for each domain, fall and spring data were pooled and calibrated together. Then, a chain-linking approach was used to place ability estimates (theta) and item parameters for the within-grade scores on the same scale in order to link the scores across grades. As a result, the ability estimates and assessment scores within each domain are directly comparable at each measured time point. The first-grade reading assessment forms differed somewhat from those in kindergarten due to the inclusion of several reading passages and associated item sets. As a result of this design difference, the calibration of items in the reading assessment required a more specialized treatment because of the possibility of local item dependence (e.g., the probability of success on items associated with the same passage may not be independent). Items associated with passage sets were treated as a single, polytomous item in the IRT calibration. This change in methodology required a re-calibration and re-reporting of the kindergarten reading scores since the release of the base-year file. Therefore, the kindergarten reading theta scores included in the K-1 data file are calculated differently than the previously released kindergarten theta scores and replace the kindergarten reading theta scores included in the base-year data file. The modeling approach stayed the same for mathematics and science, so the recalculation of kindergarten mathematics and science theta scores was not needed."}, {"section_title": "Theta and the Standard Error of Measurement (SEM) of Theta", "text": "A theta score is provided in the ECLS-K:2011 data file for each child who participated in the direct cognitive assessment for each cognitive domain assessed and for each administration. The theta score 2 is an estimate of a child's ability in a particular domain (e.g., reading, mathematics, science, or SERS) based on his or her performance on the items he or she was actually administered. Theta scores for reading, mathematics, and SERS are provided in the data file for the fall and spring kindergarten data collection rounds. A science theta score is provided for only spring kindergarten because the science assessment was not administered in the fall. Scores for all domains (reading, mathematics, science, and SERS) are provided for both the fall and spring first-grade rounds. The theta scores are reported on a metric ranging from -6 to 6, with lower scores indicating lower ability and higher scores indicating higher ability. Theta scores tend to be normally distributed because they represent a child's latent ability and are not dependent on the difficulty of the items included within a specific test. The standard error of theta provides a measure of uncertainty of the theta score estimate for each child. Adding and subtracting twice the standard error from the theta score estimates provides an approximate 95 percent confidence interval or range of values that is likely to include the true theta score. Unlike classical item theory, in which the precision of the scores is consistent across all examinees, IRT allows the standard error to vary. Larger standard errors of measurement can be the result of estimations of thetas in the extremes of the distribution (very low or very high ability) or for children who responded to a limited number of items (i.e., children who responded to all items administered generally have lower standard errors of measurement than those children responding to fewer items because more information about their actual performance is available, thereby making estimates of their ability more precise.) Tables 3-1 and 3-2 list the names of the variables pertaining to the IRT theta scores and standard errors of measurement available in the data file, along with the variable descriptions, value ranges, weighted means, and standard deviations. 3 3 The name and description for each variable in the tables begin with an \"X,\" indicating that it is a derived/calculated variable, and a data collection round number (1 for the fall kindergarten round, 2 for the spring kindergarten round, 3 for the fall first-grade round, and 4 for the spring first-grade round). These variable naming conventions are used for all the variables mentioned in this chapter. More information about variable naming conventions can be found in chapter 7.  "}, {"section_title": "3-4", "text": ""}, {"section_title": "Scale Scores", "text": "The IRT-based overall scale score for each content domain is an estimate of the number of items a child would have answered correctly in each data collection round if he or she had been administered all of the questions for that domain that were included in the kindergarten and first-grade assessments (that is, all of the 100 unique questions in the router and the three second-stage reading forms administered in kindergarten and in first grade; all of the 96 unique questions in the router and the three second-stage mathematics forms administered in kindergarten and in first grade; all of the 47 unique items administered in the router and three second-stage science forms in first grade and the single-stage kindergarten science form; and all 31 items administered in the single-stage SERS form [the same SERS assessment was used in all four data collection rounds]). To calculate the IRT-based overall scale score for each domain, a child's theta is used to predict a probability for each assessment item that the child would have gotten that item correct. Then, the probabilities for all the items fielded as part of the domain in every round are summed to create the overall scale score. Because the computed scale scores are sums of probabilities, the scores are not integers. Gain scores in each domain may be obtained by subtracting the IRT scale scores at an earlier round from the IRT scale scores at a later round. For example, subtracting the fall kindergarten mathematics score from the spring kindergarten mathematics score would result in a gain score across the kindergarten year. Similarly, a gain score from kindergarten entry to the end of first grade would be obtained by subtracting the fall kindergarten mathematics score from the spring first-grade mathematics score. 4 Scores for different subject areas are not comparable to each other because they are based on different numbers of questions and content that are not necessarily equivalent in difficulty (for example, if a child's IRT scale score in reading is higher than in mathematics, it would not be appropriate to interpret that to mean the child is doing better in reading than in mathematics). Table 3-3 provides the names of the variables pertaining to the IRT scale scores available in the data file, along with the variable descriptions, value ranges, weighted means, and standard deviations. "}, {"section_title": "Raw Number-Right Scores for the ECLS-K:2011", "text": "Several raw number-right scores, which are counts of the number of items a child answered correctly, are provided in the data file. Raw number-right scores for the Simon Says and Art Show subtests of the preLAS (Duncan and De Avila 1998) provide information on children's basic English proficiency. They are derived from the 10 items administered in the Simon Says assessment and the 10 items administered in the Art Show assessment. The Simon Says and Art Show subtests of the preLAS were administered to all children in kindergarten, so all children have raw number-right scores for these two subtests in the fall and spring rounds. In first grade, however, the Simon Says and Art Show subtests of the preLAS were administered only to children who spoke a language other than English at home and 3-7 did not pass the preLAS in the prior round in which they were assessed. 5 Therefore, only a limited subsample of children have these scores in the fall and spring first-grade rounds. A raw number-right score is provided for children's performance on the set of 20 English basic reading skills (EBRS) items. The EBRS items target specific early reading skills, predominantly letter recognition and letter sounds, with a few phonemic awareness, vocabulary, and word reading items. These items were administered to all children as part of the reading assessment routing test in kindergarten, so kindergarten EBRS scores are available for all children. However, in the fall and spring first-grade administrations of the reading assessment, the EBRS items were only administered to children whose performance on the routing items of the reading assessment routed them to the low-or middledifficulty second-stage test. Children who were routed to the highest-difficulty second-stage test did not receive the EBRS items. Therefore, only a subsample of children have EBRS raw-number right scores in first grade. Additionally, number-right scores are provided for the 10 items common to the EBRS and SERS for those children who were administered both assessments. Only Spanish-speaking children who did not obtain a high enough score on the preLAS subtests to take all the assessments in English were administered the SERS items, so these number-right scores are only available for those children. A child who was administered the SERS has responses to these 10 items administered in English as part of the EBRS and to these 10 items administered in Spanish as part of the SERS. Each child administered both the EBRS and SERS will thus have two scores for the 10 common items: (1) number correct for the 10 EBRS items and (2) number correct for the 10 SERS items. Table 3-4 provides the names of the variables pertaining to the different raw number-right scores available in the data file, along with their descriptions, value ranges, weighted means, and standard deviations. 5 For information on administration procedures, see chapter 2, section 2.1.1. "}, {"section_title": "3-8", "text": ""}, {"section_title": "Variables Indicating Children's Pathway Through the Assessment", "text": "Several variables indicating how children were routed through the assessment are available in the data file. X3FLSCRN and X4FLSCRN can be used to determine routing based on the child's home language and performance on the English language screener used for the study. These variables are coded 0 for children who were eligible for the entire battery in English because they are native English speakers or they demonstrated sufficient basic English skills as determined by their score on the preLAS. Cases coded 1, Spanish speaker, routed through Spanish assessment, did not demonstrate sufficient basic English skills as determined by their score on the preLAS, and, because Spanish was their primary language, they were administered the SERS assessment, followed by the mathematics 3-9 and executive function assessments in Spanish, after completing the EBRS section of the reading assessment in English. For the comparable kindergarten variables (X1FLSCRN and X2FLSCRN), a code of 2, Other language speaker (not Spanish/English), was used for children who spoke a non-English language other than Spanish and did not demonstrate sufficient basic English skills, as determined by their score on the preLAS, to take the assessments in English. The cognitive assessment ended for these children after the EBRS section of the reading assessment. However, in the fall and spring of first grade, there were no children who spoke a non-English language other than Spanish who did not demonstrate sufficient English skills; therefore, no cases are coded 2 on X3FLSCRN and X4FLSCRN. X3EXDIS and X4EXDIS can be used to identify children who were excluded from the assessment because they needed an accommodation the study did not provide or because they had an Individualized Education Program (IEP) that indicated they could not take part in standardized assessments. These variables are coded 1, Excluded from assessment due to disability, for children who were excluded from the assessment for these reasons, and 0, for all other children."}, {"section_title": "Choosing the Appropriate Score for Analysis", "text": "When choosing scores to use in analysis, researchers should consider the nature of their research questions, the type of statistical analysis to be conducted, the population of interest, and the audience. The sections below discuss the general suitability of the different types of scores for different analyses."}, {"section_title": "\uf06e", "text": "The IRT-based theta scores are overall measures of ability. They are appropriate for both cross-sectional and longitudinal analyses. They are useful in examining differences in overall achievement among subgroups of children in a given data collection round or across rounds, as well as in analysis of correlations between achievement and child, family, and school characteristics. The fall kindergarten, spring kindergarten, fall first-grade, and spring first-grade theta scores are on the same metric. Therefore, an analyst looking at growth across the kindergarten year could subtract the fall kindergarten score from the spring kindergarten score to compute a gain score. Or when looking at growth from kindergarten entry to the end of first grade, an analyst could subtract the fall kindergarten score from the spring first-grade score to compute a gain score. The theta scores may be more desirable than the scale scores for use in a multivariate analysis because generally their distribution tends to be 3-10 more normal than the distribution of the scale scores. 6 However, for a broader audience of readers unfamiliar with IRT modeling techniques, the metric of the theta scores (from -6 to 6) may be less readily interpretable. Researchers should consider their analysis and the audience for their research when selecting between the theta and the scale score.\nThe IRT-based scale scores also are overall measures of achievement. They are appropriate for both cross-sectional and longitudinal analyses. They are useful in examining differences in overall achievement among subgroups of children in a given data collection round or in different rounds, as well as in analysis looking at correlations between achievement and child, family, and school characteristics. The fall kindergarten, spring kindergarten, fall first-grade, and spring first-grade scale scores are on the same metric. Therefore, an analyst looking at growth across the kindergarten year could subtract the fall kindergarten score from the spring kindergarten score to compute a gain score. Or when looking at growth from kindergarten entry to the end of first grade, an analyst could subtract the fall kindergarten score from the spring first-grade score to compute a gain score. Results expressed in terms of scale score points, scale score gains, or an average scale score may be more easily interpretable by a wider audience than results based on the theta scores. \uf06e preLAS subtest raw number-right scores provide information on children's basic English proficiency. These scores may be of interest to users conducting research on children with limited English proficiency. However, because of the limited number of items included in these subtests, these scores do not represent a comprehensive measure of proficiency or of reading skills and knowledge. The primary purpose of fielding these subtests in the ECLS-K:2011 was so they could be used as an English language proficiency screener. For the kindergarten assessments, when all children received the preLAS regardless of language background, the majority of children in the ECLS-K:2011 scored highly or near perfect on these subtests, which was expected given that the subtests came from a standardized assessment for preschoolers and the majority of ECLS-K:2011 children spoke English, even if it was not their primary home language. The preLAS scores are of limited value for children who were not English language learners. For the first-grade assessments, the preLAS was only administered to those children who spoke a language other than English at home and had failed the preLAS in the prior round in which they were assessed. Therefore, analysts should be aware that only a subset of cases have valid preLAS scores in the first-grade rounds. The IRT-based reading theta or scale scores, which are available for all children, should be used by analysts interested in performance on the reading assessment, regardless of a child's home language.\nIn the fall and spring of kindergarten, the EBRS raw number-right scores provide information on children's performance on the first 20 items administered to all children as part of the reading assessment routing test. In the first-grade rounds, only children who were routed into the low-or middle-difficulty second-stage test (based on their performance on the 30 items in the reading routing assessment) have an EBRS raw-number right score, because children routed into the high form did not receive the EBRS items (for more information on routing through the assessment, see chapter 2, section 2.1.1). These EBRS scores would be useful for someone with a specific analytic interest in the knowledge and skills covered in this particular item set, which are among the most basic knowledge and skills measured in the reading assessment. As with the preLAS subtest items, children who were not English language learners tended to do well on these items on the assessment, and so these scores may be of limited value for them. Also, since these are raw scores, the difficulty of the items children answered correctly is not reflected in the score. A child who answered only the first 10 items correctly would have the same score as a child who answered 5 easier and 5 more difficult items correctly. The IRT-based reading theta or scale scores, which are available for all children in both kindergarten and first grade, should be used by analysts interested in overall performance on the reading assessment, regardless of a child's home language. \uf06e EBRS/SERS common item raw number-right scores provide information on Spanishspeaking children's performance on 10 items that were administered in both English and Spanish. Researchers may find these scores useful in an analysis focusing on Spanish-speaking English language learners because the scores allow for a comparison of the number of correct responses in English with the number of correct responses in the child's primary home language. It is important to note that these items are direct translations from the existing English items to Spanish. They have not been scaled together, and the item difficulties may not be exactly comparable from one language to the other. Although this is the case, the items have very limited language load, and expert reviewers selected items that translated easily and that could be expected to be roughly equivalent in difficulty in either language. Also, analysts interested in looking at these scores across time should be aware that the number of children with these scores is lower in first grade than in kindergarten, because more Spanish-speakers were routed through the assessments in English in first grade.\nThe first digit of X4CLASS2 indicates the specific type of kindergarten class in which the child was enrolled (full-day, part-day, unknown, or child not identified as in kindergarten). It was derived from a combination of X4GRDLVL (from the spring TQCK, spring TQC1, and the FMS) and responses on the teacher-reported child-level questionnaire in fall 2011 or spring 2012 (spring TQCK, fall TQC; variables T4KGRADE and T3GRADE). If data on program type from the spring TQCK or fall TQC were missing, then data from the grade-level composite (X4GRDLVL) were used to set the grade level of kindergarten or other. There are four values for the first digit of X4CLASS: 1 (full-day class), 2 (part-day class), 3 (unknown kindergarten class), and 9 (child not identified as in kindergarten).\nThe second digit of X4CLASS2 indicates whether the teacher provided data on a fullday class (A4KFULDAY), a half-day a.m. class (A4KHALFAM), a half-day p.m. class (A4KHALFPM), or both full-day and half-day classes (A4KBOTHCL) in the teacher-level questionnaire (spring TQAK). There are five values for the second digit of X4CLASS, which points data users to the appropriate class-specific variables from the teacher-level questionnaire that should be used for each child, or indicates if no spring TQA data are available: 0 (missing teacher data), 1 (full-day teacher data), 2 (half-day a.m. teacher data), 3 (half-day p.m. teacher data), and 9 (teacher data reported in multiple columns). Users should use the first digit of the X4CLASS2 variable to determine the type of classroom in which a child was enrolled. Users interested in incorporating teacher and classroom characteristics from the teacher-level questionnaire into their analyses should use the second digit to identify which group of class-specific variables (half-day a.m., half-day p.m., or AD [all-day]) apply to each child. In instances of inconsistent teacher reporting, the first and second digits may not agree with one another. However, the second digit was assigned after a careful review of the data, so the associated variables should be used for each child. For example, if the child was in a full-day kindergarten class according to the spring TQCK and the second digit points to the half-day a.m. variables, the user should use the half-day a.m. data, because it was determined that the teacher reported information for that child's full-day class in the half-day a.m. column of the questionnaire. The meaning of each category in on X4CLASS2 means that the child was reported to be in a full-day class and the teacher provided data for a full-day class, whereas a value of 21 on X4CLASS2 means that the child was reported to be in a part-day class, but the teacher provided data for a full-day class and did not also provide data for a morning or afternoon part-day class. A value of 19 on X4CLASS2 means that the child was reported to be in a full-day class, and the teacher provided data on multiple types of classes (e.g., a teacher may have provided data on a half-day morning class and a half-day afternoon class, or a full-day class and a halfday morning class). For cases with a 9 as the second digit of X4CLASS2, the data user should examine the teacher-provided data to determine which class-specific data they prefer to link to the child. Although the teacher did not provide data consistently for one type of class in these cases, there may be some classspecific data that match the child's class type, and there may be data associated with another class type that a user would want to use.\nAn interviewer or CAPI program error was corrected in data editing.\nA partial roster was collected in an earlier round, but not included in a data file because it was not completed (i.e., a breakoff not meeting the rule for data delivery), and a person included in the partial roster was no longer in the household when the roster was completed in a later interview. Specific cases for which these circumstances occurred are described in the Appendix: Data Anomalies and Errata. Determining household membership in a given round. In round 1, respondents were not asked if persons were currently household members, because this was the first household enumeration for the study and all enumerated persons were household members at that time. For rounds 2 and 4, analysts can determine the current household membership at the time of the parent interview for the round by examining the variables P2CUR_# and P4CUR_#, respectively. Analysts should not look for the first \"empty\" position in the roster series to determine the last person with roster data in the household since, as noted above, all persons retain their household positions permanently; i.e., if person 3 leaves the household, then person 4 still remains in position 4.\nValues imputed by hot deck were not donated.\nFor case 10005679, person 4 is in the fall 2010, spring 2011, and fall 2011 data, but did not appear in the spring 2012 household roster and was not added by the respondent. Therefore, it is unclear whether person 4 is still in the household in spring 2012. \uf06e There is one case (CHILDID=10007937) that joined the study in fall 2010, but the fall 2010 parent interview data are not on the file because of an error. This case shows person 3 leaving the household in spring 2012, which is accurate, but there are no fall 2010 data showing this person in the household because those data are not in the file. Also, there is one case (CHILID=10005961), that did not complete the household roster in fall 2010 and thus the fall 2010 parent interview data are not in the file; however, there were some relationship data collected in the household roster for fall 2011 and those data were carried over to spring 2012. Person 4 was in the household in fall 2010, but left by spring 2012. Similarly, there is a case (CHILDID=10008803) that did not complete the household roster in spring 2011 and thus the spring 2011 parent interview data are not in the file; however, there were some relationship data collected in the household roster for spring 2011 and those data were carried over to spring 2012. \uf06e There are four cases that have values for a change in the relationship of the respondent to the focal child FSQ121 (P4CHGRESPREL) that should be noted. Specifically, there are two cases (CHILDID=10000099, 10009577) where the relationship of the respondent to the child appears to have changed (FSQ121 (P4CHGRESPREL)=1), but because of an error either by the interviewer or the respondent, the data do not indicate that the person's relationship changed. There is also one case (10010610) that shows a change in relationship for person 6 from \"other relative\" to biological father between spring 2011 and spring 2012; however, the relationship was biological father in both rounds. Although there was an interviewer comment in spring 2011 that person 6 was a biological father, because person 6 was coded as an \"other relative\" in spring 2011, the question about whether there had been a change in relationship FSQ121 (P4CHGRESPREL) was asked in spring 2012. In spring 2012, FSQ121 (P4CHGRESPREL)=1 because the relationship was corrected to biological father during data collection.\nThere are also some cases that have values for a change in the relationship of the respondent's spouse to the focal child that should be noted. In case 10008531, the data show a change in relationship for person 5 from FSQ150 (P4DAD_5)=4 (foster father or male guardian) to FSQ150 (P4DAD_5)=5 (other male parent or guardian) between spring 2011 and spring 2012; however, the relationship was \"other male parent or guardian\" in both rounds. The question about a change in relationship is coded as FSQ122 (P4CHGSPSPREL)=2 (no change). In case 10009589, the relationship change question was recorded as FSQ122 (P4CHGSPSPREL)=2 for person 4. There was an interviewer error that suggested a possible problem with this answer, but there is not enough information to update the variable value. There are also cases where P4CHGSPSPREL=1, but the data do not indicate that the person's relationship changed. Although the relationship did not change between rounds, due to respondent or interviewer error, P4CHGSPSPREL was coded 1 (yes, there was a change). These are cases with CHILDIDs 10001140, 10002377, 10002714, 10003224, 10003933, 10005670, 10008188, 10008727, 10009339, 10009577, 10009936, 10011166, 10013528, 10014205, 10014237, 10014288, 10015929, 10016212, 10016307, 10016701, 10017139, 10006734, 10009894, 10011392, 10012026, 10014194, 10016622, 10000123, 10004678, 10008906, 10009679, 10009935, 10013380, 10017423, 10002237, 10013980, and 10008185. \uf06e Some cases were purposely asked about the parents' country of origin FSQ212 (P4PARCT1 and P4PARCT2) in both spring 2011 and spring 2012 because they had missing information about the age that the parent(s) came to the United States. Although most cases reported the same country of origin at both time points, some cases reported a country that was different.\nThere are two cases (CHILDID=10002294, 10017993) where FSQ212 (P4PARCT2)=1 (United States), but FSQ213 (P4PAREM2) for the age that the person moved to the United States has a valid age. Both cases had answers that were coded as \"other\" in the parent interview, but indicated the United States. Because the answers were not coded as the United States during the interview, the question about the age moved to the United States was asked.\nOne case (CHILDID=10005679) was asked country of origin questions both in spring 2011 and spring 2012. This case identification number was confused with another case (10014103) in data collection in the base year of the study and this affected some preloaded information. The country of origin for the second parent figure FSQ212 (P4PARCT2) is the same in both spring 2011 and spring 2012, but differs for the first parent figure in FSQ212 (P4PARCT1). Case 10014103 did not have data collected for country of origin, but should have.\nThere are two cases (CHILDID=10003551, 10004836) who reported unusual combinations of bedtime hour HEQ560A (P4BEDTMH) and AM/PM designations in HEQ565 (P4BEDTMAP) (1:10 p.m., 2:08 p.m., respectively). These times were reported in the data.\nThere is one case (CHILDID=10006989) that has a -9 (not ascertained) for the amount of money paid for child care by a relative CCQ096 (P4RAMTCH), a -1 (not A-4 applicable) for the number of hours per week for child care by another relative CCQ110 (P4RHROTH), and a 2 (no) for currently has care from nonrelative CCQ115 (P4NRNOW). Household members were added after the interview (based on interviewer comments), so the path followed in the interview for this item was incorrect. \uf06e There are some cases that have a disability diagnosis for the focal child and have follow-up questions about that diagnosis recorded in variables other than those used for the child's specific diagnosis. In the parent interview, respondents were asked to provide the diagnosis of the child's disability, if applicable, in question CHQ125 (P4LRNDIS-P4OTHDIA). If a diagnosis did not fit one of the categories in the parent interview specifications, the diagnosis was entered as \"other.\" Follow-up questions about age at diagnosis and medication taken for a particular diagnosis (CHQ130-CHQ173) were asked about the diagnosis entered as \"other.\" Later, in coding conducted after the parent interview was completed, some answers in the \"other\" category were assigned existing codes that were available in the interview (e.g., generalized anxiety disorder, CHQ125 (P4GENANX)), but the follow-up questions about age at diagnosis and medication taken for a particular diagnosis (CHQ130-CHQ173) remain in the questions that go with the \"other\" category.\nCase 10014762 had interviewer errors in the household roster, and the correct questions were not asked in NRQ. Because of these problems, the question about contact with the biological father NRQ040 (P4BDCNTC)=-9 (not ascertained). Parent Interview: Spring 2012 Errors in the CAI Programming \uf06e One case, (CHILDID 10005750) should not be used for analysis. An interviewer error caused all spring 2012 data collected for one parent to be overwritten by another parent from a previous round in a different household. Therefore, the data collected and reported in spring 2012 for this case are not matched to the correct persons in the roster and all parent data are not accurate for this case.\nA problem with the CAI code caused 247 cases to not have the questions about the child's country of origin (INQ300 (P4BTHPLC); INQ310 (P4CNTRYB); INQ320 (P4YRCOME); and INQ330 (P4CITIZN)) asked. These have been set to -9 (not ascertained). \uf06e Some questions about communication issues (CHQ205 (P4PRBART); CHQ206A-H (P4TLKLD, P4TLKSFT, P4CHEW, P4SWALLO, P4STUTER, P4CLEFT, P4ABNRML, P4MALFRM); CHQ210 (P4EVALCO); and CHQ215 (P4CMDIAG)) were asked again in spring 2012 even though the specifications indicate that they could have been skipped because there were data from this section in spring 2011. For cases with data for these items in both spring 2011 and spring 2012, the spring 2012 data could be used for more updated information. \uf06e There were some cases that had education data collected in spring 2012 (PEQ020 (P4HIG_1_I); PEQ021 (P4HIS_1_I); PEQ020 (P4HIG_2_I); PEQ021 (P4HIS_2_I)) that also had education data collected in spring 2011. There was a programming issue that resulted in data being reversed between the parents in the household so that education data were collected again for some parents and not collected for those with missing data from spring 2011. The composites for parent education use the most A-5 recently obtained data from spring 2012. Missing data were imputed. In addition, case 10005679 was conducted under the wrong interview number in the base year of the study. Because of this, previously reported education data were not available during the spring 2012 interview, and the education questions were asked again. \uf06e There were 29 males and 8 females who had base-year data, but did not get asked about a change in employment from fall 2010 EMQ010 (P4EMPCHG_1_I or P4EMPCHG_2_I) because of a roster number comparison issue in CAPI when a person in a roster position above a key parent left the household in the base year.\nIn one case (CHILDID=10001666), the interviewer deleted the male parent figure (a grandparent in a household without parents, recorded as the second parent in the composite variable X4IDP2) in the parent interview and then added him back in with a different age. The data for this case are correct, but users should be aware that there was no real change in the household roster even though the value of X4IDP2 changed from 3 in fall 2010 and spring 2011 to 6 in spring 2012. Because the parent figure was added to the household roster as a new person, the case followed the path in the parent interview for a new parent figure rather than the path that it would have taken if the parent figure was previously in the household. Although some questions that would not have been asked about a parent figure previously in the household were asked about this parent figure (race, ethnicity, country of origin, education, and employment), the answers in spring 2012 match the answers in earlier rounds. For employment questions, there were some changes to job title and duties, so the current data in composite variables X4PAR2SCR_I and X4PAR2OCC_I are an update to previous information.\nThree administrators responded to questions about school type in the school administrator questionnaire by checking both public and private options. These cases were investigated and coded appropriately on the composite X4SCTYP.\nA step was missed in the creation of the composite X4RCETH, percent minority students in the school. Values from the school frame should have been assigned when the school did not return a school administrator questionnaire (SAQ). This step was omitted in error. As a result, X4RCETH has more values of -9 (not ascertained) than it should. This variable will be corrected in the kindergarten-to-second grade data file.\nThe composite variable for year-round school status (X4YRRND) draws upon both SAQ and school frame variables. Comparing the beginning month (composite X4SCHBMM) to the school ending month (composite X4SCHEMM), there are 61 child IDs associated with schools that have stated they are not year round when the beginning and end months are either 1 month apart or the same month. There are 163 child IDs associated with schools that have stated they are year round when the beginning and end months are 2 or more months apart. \uf06e Some children who were participants in round 3 were not participants in round 4 (198 children). Of this group, 132 have no round 4 teacher ID, and 103 have an unidentified school because the child moved to a nonsampled county or had an assessment result of \"not located.\" In the composite about whether children changed teachers from fall to spring (X34CHGTCH), it was assumed that these cases changed schools and, therefore, teachers. Other (Specify) Variables. In reviewing \"other (specify)\" responses to questions, there were times when a sufficient number of common responses were given to warrant the addition of a new category to the response options. The categories added after data collection ended, during review of the data, are listed in exhibit A-1. Users should keep in mind that had these new categories been offered as response options to all respondents during data collection, it is possible that more respondents would have chosen them. Why has the child fallen behind in school work? MARK ALL THAT APPLY. \uf0a7 ELL/Language issues \uf0a7 Easily distracted/lack of focus or attention (excludes \"ADD\" and \"ADHD\") 5 Kindergarten teacher questionnaire, (child level) spring 2012 Why has the child fallen behind in school work? MARK ALL THAT APPLY. \uf0a7 ELL/Language issues \uf0a7 Easily distracted/lack of focus or attention (excludes \"ADD\" and \"ADHD\") CHQ125 Parent interview, spring 2011 What was the diagnosis or were the diagnoses? Outliers (that is, unusually high or unusually low values) are top-or bottom-coded to prevent identification of unique schools, teachers, parents, and children without affecting overall data quality. The category value labels for variables that are top-and bottom-coded in the PUF are edited versions of the RUF category labels and reflect the new highest and lowest categories. \uf06e Some continuous variables are converted into categorical variables, and some categorical variables have their categories collapsed in the K-1 PUF. Category value labels are provided for continuous variables that are converted into categorical variables.\nVariables with too few cases and/or a sparse distribution are suppressed in the K-1 PUF. The values for these variables are set to -2 or -4 and labeled \"suppressed\" in the ECB. The value -2 means that the data for this variable are suppressed to protect the respondent's confidentiality. The value -4 means that the data for this variable are suppressed because of an error in the administration of the instrument; there are only 23 variables with a value -4, and they are all from the kindergarten parent interview.\nVariables that provide a particularly identifying characteristic, such as a specific disability, or information that could be matched against external data sources to obtain a specific identifying characteristic, such as exact date of marriage or divorce, are also suppressed. The values for these variables are set to -2. There is a comment field in the variable frequency distribution view screen of the ECB that displays a comment for each masked variable indicating whether the variable from the restricted-use file has been recoded or suppressed in the K-1 PUF. Exhibits 1 to 12 below present the lists of masked variables for the base year. The exhibits display the variable name, variable label, and a comment indicating whether the variable was recoded or suppressed. When applicable, the reason for suppression is also provided. Exhibits 13 to 21 present the lists of masked variables for first grade. Section 7.1 of the user's manuals explain the variable naming conventions. All variables from the special education teacher questionnaire part A (i.e., all variables with the prefix D2 or D4) and from the special education teacher questionnaire part B (i.e., all variables with the prefix E2 or E4) are suppressed on the K-1 PUF. In addition, all variables from the teacher-level questionnaire for children in kindergarten in the spring 2012 round of data collection are suppressed, with the exception of the variable indicating the year the questionnaire was completed. For brevity, these variables are not included in the exhibits."}, {"section_title": "Analytic Considerations for Measuring Gains in the ECLS-K:2011", "text": "An important issue to be considered when analyzing achievement scores and gains is assessment timing: children's age at assessment, the date of assessment, and the time interval between assessments. Most sampled children were born throughout the second half of 2004 and first half of 2005, but their birth dates were not related to testing dates. As a result, children were tested at different developmental and chronological ages. Assessment dates ranged from August to December for the fall data collections, and from March to June for the spring rounds. Children assessed in December may be expected to have an advantage over children assessed in the first days or weeks of school. Substantial differences in intervals between assessments may also affect analysis of gain scores. Children assessed in September and June in a given grade have more time to learn skills than children assessed in November and March. These differences in interval may or may not have a significant impact on analysis results. In designing an analysis plan, it is important to consider whether and how differences in ages, assessment dates, and intervals may affect the results, to look at relationships between these factors and other variables of interest, and to adjust for differences if necessary. When using the IRT scale scores as longitudinal measures of overall growth, analysts should keep in mind that gains made at different points on the scale have qualitatively different interpretations. Children who made gains toward the lower end of the scale, for example, in skills such as identifying letters and associating letters with sounds, are learning different skills than children who made gains at the higher end of the scale, for example, those who have gone from reading single words to reading sentences, although their gains in number of scale score points may be the same. Comparison of gains in scale score points is most meaningful for groups that started with similar initial status. One way to account for children's initial status is to include a prior round assessment score as a control variable in an analytic model. For example, the fall kindergarten scale score could be included in a model using the spring kindergarten scale score as the outcome."}, {"section_title": "Reliability of the ECLS-K:2011 Scores", "text": "Reliability statistics assess consistency of measurement, or the extent to which test items in a set are related to each other and to the score scale as a whole. For tests of equal length, reliability estimates can be expected to be higher for sets of items that are closely related to the underlying construct than for tests with more diversity of content. Conversely, for tests with similar levels of diversity in content, reliabilities tend to be higher for longer tests compared to shorter tests. In general, the domain with the most diverse content in the ECLS-K:2011 assessment, science, had lower reliability coefficients than reading and mathematics. 7 Reliabilities were highest for the scores derived from the largest number of test items, namely the IRT ability estimates, which are based on all items taken by each child. Reliabilities were lowest for the scores based on the fewest items, namely the raw number-right scores. Reliability statistics appropriate for each type of score were computed for each subject area for fall and spring kindergarten and fall and spring first grade. For the IRT-based scores, the reliability of the overall ability estimate, theta, is based on the variance of repeated estimates of theta for each individual child compared with total sample variance. These reliabilities, ranging from .75 to .99 for the reading, mathematics, science, and SERS assessments also apply to the scores derived from the theta estimate, namely, the IRT scale scores. Alpha coefficients for the preLAS Simon Says and Art Show, EBRS, and EBRS/SERS common-number correct scores ranged from .64 to .99. The coefficients for several of the scores based on 10 items are relatively low due to the low number of observations and items in the set. Tables 3-5 and 3-6 present the reliability statistics for all of the assessment scores in kindergarten and first grade.  "}, {"section_title": "Dimensional Change Card Sort", "text": "The Dimensional Change Card Sort is used to collect information on children's cognitive flexibility. In this task, children are asked to sort a series of 22 picture cards according to different rules. Each card has a picture of either a red rabbit or a blue boat. The children are asked to sort each card into one of two trays depending on the sorting rule they have been told to follow. One tray has a picture of a red boat and the other has a picture of a blue rabbit. For the first set of items, the Color Game (each set is referred to as a game), the rule is to sort the cards by color (i.e., red or blue). For example, a blue boat card would be sorted into the blue rabbit tray. In the second game, the Shape Game, the rule is to sort the cards by shape (i.e., rabbit or boat). For example, a red rabbit card would be sorted into the blue rabbit tray. If the child correctly sorts four of the six cards in the Shape Game, then he or she moves on to the third game: the Border Game. In the Border Game, the sorting rule (by color or by shape) depends on whether or not the card has a black border around the edges. If the card has a border, the child is to sort by color; if there is no border on the card, the child is to sort by shape. Item-level data for the Dimensional Change Card Sort for fall and spring kindergarten and fall and spring first grade are provided in the ECLS-K:2011 K-1 data file. There are six variables with item-level results for the color game, six variables with item-level results for the shape game, and six variables with item-level results for the Border Game. There were four practice items administered to children, but the item-level results from these practice items are not included in the data file. The itemlevel data for the color and shape games are scored \"correct\" (i.e., card sorted into the correct tray according to the sorting rule) or \"incorrect\" (i.e., card sorted into the incorrect tray). There is a third score provided for the Border Game, \"not administered\"; this code indicates that the child was not administered the item because he or she did not answer enough items correctly to advance to this item in the assessment. The \"not administered\" code is different than a system missing code in that only those children who were administered the Dimensional Change Card Sort could have a \"not administered\" code. If a child was not administered the Dimensional Change Card Sort at all, his or her data for these scores would be coded as missing. Variable names for the item-level data from the fall kindergarten assessments begin with \"C1,\" and the variable names for the item-level data from the spring kindergarten assessments begin with \"C2.\" Similarly, variable names for item-level data from the fall and spring firstgrade assessments begin with \"C3\" and \"C4,\" respectively. Using scoring rules provided by the developers, four scale scores were developed from the Dimensional Change Card Sort data for the fall and spring kindergarten and the fall and spring first-grade 3-16 rounds of data collection: the pre-switch score, the post-switch score, the Border Game score, and a total score. The pre-switch score is the number of cards the child correctly sorted by color (i.e., the first phase of the assessment). The post-switch score is the number of cards the child correctly sorted by shape (i.e., after switching from sorting by color to sorting by shape). The Border Game score is the number of cards the child correctly sorted when the sorting rule was determined by the presence (or absence) of a border around the card. 8 A final combined scale score reflects the totals for the three tasks (i.e., the Color, Shape, and Border Games). 8 All children initially attempted six Color Game trials, and then moved to the Shape Game. Children who did not correctly sort at least four of the six cards in the Shape Game were not administered the Border Game and do not have a Border Game score. As a result, the n with valid (i.e., nonmissing) data for the post-switch score is higher than the n with valid (i.e., nonmissing) data for the Border Game score. For more information on the administration procedures and the scores for the Dimensional Change Card Sort, see The Dimensional Change Card Sort (DCCS): A Method of Assessing Executive Function in Children (Zelazo 2006). "}, {"section_title": "Numbers Reversed", "text": "This measure assesses the child's working memory. It is a backward digit span task that requires the child to repeat an orally presented sequence of numbers in the reverse order in which the numbers are presented. For example, if presented with the sequence \"3\u20265,\" the child would be expected to say \"5\u20263.\" Children are given 5 two-number sequences. If the child gets three consecutive twonumber sequences incorrect, then the Numbers Reversed task ends. If the child does not get three and 4 eight-digit number items). Each item is scored \"correct\" (i.e., the child correctly repeated the number sequence in reversed order), \"incorrect\" (i.e., the child did not correctly repeat the number sequence in reversed order), or \"not administered\" (i.e., the child was not administered the item because he or she did not answer enough items correctly to advance to this item). The \"not administered\" code is different than a system missing code in that only those children who were administered the Numbers Reversed subtask could have a \"not administered\" code. If a child was not administered the Numbers Reversed subtask at all, his or her case would have a missing code for the Numbers Reversed scores. Variable names for the item-level data from the fall kindergarten assessments begin with \"C1,\" and variable names for the item-level data from the spring kindergarten assessments begin with \"C2.\" Similarly, variable names for item-level data from the fall and spring first-grade assessments begin with \"C3\" and \"C4,\" respectively. Variable descriptions for these items indicate the length of the digit sequence (e.g., C1 Numbers Reversed Two-digit sequence #1). Numbers Reversed was administered in Spanish for children routed through the Spanish assessment. Data from English and Spanish administrations are combined into the same item-level variables. Researchers who want to account for language of administration in their analyses can use the variables X1FLSCRN, X2FLSCRN, X3FLSCRN, and X4FLSCRN, which are also in the data file, to identify which children were administered Numbers Reversed in English and which children were administered Numbers Reversed in Spanish. In addition to the item-level data, three scores developed using guidelines from the publisher scoring materials are included in the data file for Numbers Reversed. Before analyzing the Numbers Reversed data, it is important that researchers understand the characteristics of these scores and how these characteristics may affect the analysis and interpretation of the Numbers Reversed data in the context of the ECLS-K:2011. The three scores developed using publisher guidelines are a W score, a standard score, and percentile rank. Depending on the research question and analysis being conducted, one of the scores may 3-19 be more preferable than another. For example, the W score may be best for a longitudinal analysis, whereas the percentile rank and standardized score may be better suited for an analysis focusing on one point in time. The descriptions below provide more information about which score may be better suited for a given analysis. 9 The W score, a type of standardized score, is a special transformation of the Rasch ability scale and provides a common scale of equal intervals that represents both a child's ability and the task difficulty. The W scale is particularly useful for the measurement of growth and can be considered a growth scale. Typically, the W scale has a mean of 500 and standard deviation of 100. Furthermore, the publisher of the Woodcock-Johnson III Tests of Cognitive Abilities (Woodcock-Johnson III) has set the mean to the average of performance for a child of 10 years, 0 months. This means that it would be expected that most children younger than 10 years, 0 months would obtain W scores lower than the mean of 500, and most older children would be expected to have scores above the mean of 500. Also, as a child develops with age, it would be expected that his or her W score would increase to reflect growth. For example, when a child's W-ability score increases from 420 to 440, this indicates growth, and this would be the same amount of growth in the measured ability as any other student who gained 20 W points elsewhere on the measurement scale. As mentioned above, the W score is an equal-interval scale, suited for analyses such as correlations and regressions. Higher W scores indicate that a child provided more correct responses and generally indicate that a child was able to correctly respond to at least some longer number sequences.  10 The standard score and the percentile rank also show a lower mean in the ECLS-K:2011, which may also be attributable to differences between the ECLS-K:2011 population and the norming sample. "}, {"section_title": "Numbers Reversed Data Flags", "text": "Two flags indicate the presence or absence of Numbers Reversed data. X3NRFLG and X4NRFLG indicate the presence of first-grade data for the fall and spring, respectively."}, {"section_title": "Indirect Cognitive Assessment, the Academic Rating Scale", "text": "The Academic Rating Scale was developed for the ECLS-K to obtain teachers' evaluations of children's academic achievement in three domains: language and literacy, science, and mathematical thinking. The ECLS-K:2011 fielded the Academic Rating Scale developed for the ECLS-K with some modifications to the item text. Teachers rated the child's skills, knowledge, and behaviors on a scale from \"not yet\" to \"proficient\" (table [3][4][5][6][7][8][9]. If a skill, knowledge, or behavior had not been introduced in the classroom yet, the teacher was instructed to mark that item as NA (not applicable or skill not yet taught). "}, {"section_title": "Teacher-Reported Social Skills", "text": "In the fall kindergarten, spring kindergarten, fall first-grade, and spring first-grade collections, teachers reported how often their ECLS-K:2011 children exhibited certain social skills and behaviors using a four-option frequency scale ranging from \"never\" to \"very often.\" Teachers also had the option of indicating that they had not had an opportunity to observe the described behavior for the child being asked about.  11 The Social Skills Rating System is a copyrighted instrument (1990 NCS Pearson) and has been adapted with permission.   "}, {"section_title": "3-26", "text": ""}, {"section_title": "Parent-Reported Social Skills", "text": "In the fall kindergarten, spring kindergarten, and spring first-grade parent interviews, parents were asked to report how often their child exhibited certain social skills and behaviors using the same frequency scale described above for the teacher-reported social skills items. These parent items also are based on items from the Social Skills Rating System. Chapter 2, section 2.1.2 has additional information on the parent interviews. Four social skill scales were developed based on parents' responses to these interview questions. The score on each scale is the mean rating on the items included in the scale. The four social skill parent scales are as follows: Self-Control (5 items), Social Interaction (3 items), Sad/Lonely (4 items), and Impulsive/Overactive Behaviors (2 items). A score was computed when the respondent provided a rating on at least a minimum number of the items that composed the scale. The minimum 3-28 numbers of items that were required to compute a score were as follows: self-control (4 out of 5 items), social interaction (2 out of 3 item), sad/lonely (3 out of 4 items), and impulsive/overactive (2 out of 2 items). Higher scores indicate that the child exhibited the behavior represented by the scale more often (e.g., higher self-control scores indicate that the child exhibited behaviors indicative of self-control more often; higher scores on the social interaction scale indicate that the child interacted with others in a positive way more often). The variable names, descriptions, value ranges, weighted means, and standard deviations for the parent scores are shown in table 3-12. Data for the individual items contributing to each scale are not included in the data file due to copyright restrictions.   "}, {"section_title": "Teacher-Reported Approaches to Learning Items and Scale", "text": "The fall kindergarten, spring kindergarten, fall first-grade, and spring first-grade child-level teacher questionnaire included seven items, referred to as \"Approaches to Learning\" items, that asked the teachers to report how often their ECLS-K:2011 children exhibited a selected set of learning behaviors (keeps belongings organized; shows eagerness to learn new things; works independently; easily adapts to changes in routine; persists in completing tasks; pays attention well; and follows classroom rules). 12 These items were presented in the same item set as the social skills items adapted from the Social Skills Rating System (described above in section 3.4), and teachers used the same frequency scale to report how often each child demonstrated the behaviors described. The Approaches to Learning scale score is the mean rating on the seven items included in the scale. A score was computed when the respondent provided a rating on at least four of the seven items that composed the scale.  "}, {"section_title": "Parent-Reported Approaches to Learning Items and Scale", "text": "The fall kindergarten, spring kindergarten, and spring first-grade parent interview included six items, referred to as \"Approaches to Learning\" items, that asked parents to report how often their child exhibited learning behaviors (keep working at something until finished; show interest in a variety of things; concentrate on a task and ignore distractions; help with chores; eager to learn new things; creative in work and play). 13 These items were asked within the same set of items as the social skills items adapted from the Social Skills Rating System (described above in section 3.5) in section SSQ (Social Skills, Problem Behaviors, and Approaches to Learning) of the parent interview, and parents used the same frequency scale to report how often their child demonstrated the behaviors described. The Approaches to Learning scale score is the mean rating on the six items included in the scale. A score was computed when the respondent provided a rating on at least four of the six items that composed the scale. Higher scale scores indicate that the child exhibited positive learning behaviors more often. The variable names, descriptions, value ranges, weighted means, and standard deviations for the parent Approaches to Learning scale scores are shown in table 3-15. The Approaches to Learning scale had a reliability estimate of .70 for the fall kindergarten data collection, .72 for the spring kindergarten data collection, and .74 for the spring first-grade data collection. Additionally, the item-level data for the parent-reported Approaches to Learning items are included in the data file along with the other parent interview data. and were asked to indicate how \"true\" or \"untrue\" those statements were about that child on a 7-point scale ranging from extremely untrue to extremely true, with a middle option of \"neither true nor untrue.\" If a statement or situation did not apply to that child, the teacher could indicate \"not applicable.\" The data file includes two scale scores derived from these items: (1) Attentional Focus and 2 "}, {"section_title": "Student-Teacher Relationship Scale", "text": "The Student-Teacher Relationship Scale (Pianta 2001) is a 15-item, teacher-reported measure of closeness and conflict between the teacher and child. As part of the spring kindergarten and spring first-grade child-level teacher questionnaire, the teacher was presented with 15 descriptive statements about his or her relationship with the ECLS-K:2011 child and asked to indicate the degree to which each statement applied to their relationship using a 5-point scale ranging from \"definitely does not apply\" to \"definitely applies.\" Two scales were developed based on guidelines from the author of the scale: Closeness and Conflict. The Closeness Scale score is the average rating on the seven items included in the scale, while the Conflict Scale score is the average rating on the eight items included in that scale. A score was  3-34"}, {"section_title": "SAMPLE DESIGN AND SAMPLING WEIGHTS", "text": "The Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011) will provide national data on children's characteristics as they progress from kindergarten through the 2015-16 school year, when most of the children will be in fifth grade. In the 2010-11 school year, the ECLS-K:2011 collected data from a nationally representative sample of 18,174 children enrolled in 968 schools. 1 All 18,174 children were eligible for the first-grade data collections. This chapter summarizes the process used to select the sample for the study in the base year (i.e., kindergarten), describes how the sample design changed for the first-grade year, and provides information necessary to properly analyze the data that were collected."}, {"section_title": "Sample Design for the Base Year", "text": "For the base year, the sample for the ECLS-K:2011 was selected using a three-stage process. In the first stage of sampling, the United States was divided into primary sampling units (PSUs), or geographic areas that are counties or groups of contiguous counties, and 90 PSUs were sampled for inclusion in the study. In the second stage, samples of public and private schools with kindergarten programs or that educated children of kindergarten age (i.e., 5-year-old children) in ungraded settings were selected within the sampled PSUs. Both PSUs and schools were selected with probability proportional to measures of size (defined as the population of 5-year-old children) that took into account a desired oversampling of Asians, Native Hawaiians, and other Pacific Islanders (APIs). 2 In the third stage of sampling, children enrolled in kindergarten and 5-year-old children in ungraded schools or classrooms were selected within each sampled school. For a detailed description of the three stages of sampling, see User's Manual."}, {"section_title": "ECLS-K:2011 School Sample for the Base Year", "text": "A total of 1,221 clusters of schools 3 were selected for the ECLS-K:2011, of which 1,003 were clusters of public schools and 218 were clusters of private schools. This resulted in 1,036 sampled public schools and 283 sampled private schools, for a total of 1,319 sampled schools. The sample frames used to select schools were the 2006-07 Common Core of Data (CCD) and the 2007-08 Private School Survey (PSS), which were the most recent CCD and PSS data available at the time of sampling. Because the 2006-07 CCD and the 2007-08 PSS school frames were several years old, additional schools were sampled from supplemental frames that included newly opened schools and existing schools that added a kindergarten program after the 2006-07 CCD and the 2007-08 PSS were collected. These additional schools were added to the original school sample. In total, 33 new schools were added, of which 16 were public, 4 were Catholic, and 13 were non-Catholic private schools. The total number of sampled schools after updating was 1,352 (1,052 public schools and 300 private schools). For a detailed discussion of the supplemental school sample, see section 4.1.2.7 of the base-year User's Manual. Early in the process of recruiting schools that had been sampled for the study, it was determined that the rate at which public schools were agreeing to participate was lower than expected, and it would be difficult to meet the target number of participating schools by the end of the recruitment period. The decision was made to select public schools not selected into the original ECLS-K:2011 sample that would replace those sampled public schools that had already refused to participate. For a detailed discussion of school substitution, see section 4.   1 School characteristics are taken from the original school frame. 2 States in each region: Northeast: Connecticut, Massachusetts, Maine, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Iowa, Illinois, Indiana, Kansas, Michigan, Minnesota, Missouri, North Dakota, Nebraska, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Maryland, Mississippi, Louisiana, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, New Mexico, Nevada, Oregon, Utah, Washington, and Wyoming. Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2010 and spring 2011."}, {"section_title": "4-3", "text": ""}, {"section_title": "The Base-Year Sample of Children", "text": "The goal of the sample design was to obtain an approximately self-weighting sample of children, with the exception of APIs who needed to be oversampled to meet sample size goals. Table 4-2 shows the distribution of the eligible children sampled for the ECLS-K:2011, by selected characteristics. Table 4-3 shows the distribution of the children who participated in the base year, by selected characteristics. As mentioned in the base-year User's Manual, operational problems prevented the study from conducting data collection activities in some areas of the country where Asian, Native Hawaiian/Other Pacific Islander, and American Indian/Alaska Native students sampled for the study resided. For this reason, base-year response rates for these groups of students were lower than response rates for students of other racial/ethnic backgrounds. More specifically, a relatively small number of ECLS-K:2011 sample children in the Native Hawaiian/Other Pacific Islander group resided in Hawaii at the time of sampling. Also, nonresponse on the child assessment, parent interview, or both leads to some of these sampled cases not being included in weighted analyses depending on the weight used. In addition to the above, none of the ECLS-K:2011 sample children in the American Indian/Alaska Native group resided in Alaska at the time of sampling. Users are encouraged to consider these sample characteristics when making statements about children in these two racial groups. As a reminder, however, the study was not designed to be representative at the state level or for subgroups within any specific racial or ethnic group. 910 784 126 \u2020 Not applicable. 1 School characteristics are taken from the original school frame. Race/ethnicity information was obtained from schools at the time of sampling. 2 States in each region: Northeast: Connecticut, Massachusetts, Maine, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Iowa, Illinois, Indiana, Kansas, Michigan, Minnesota, Missouri, North Dakota, Nebraska, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Maryland, Mississippi, Louisiana, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, New Mexico, Nevada, Oregon, Utah, Washington, and Wyoming. Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 3 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. 870 727 143 \u2020 Not applicable. 1 School characteristics are taken from the original school frame. Race/ethnicity information is from the base-year race/ethnicity composite X12RACETH. 2 States in each region: Northeast: Connecticut, Massachusetts, Maine, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Iowa, Illinois, Indiana, Kansas, Michigan, Minnesota, Missouri, North Dakota, Nebraska, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Maryland, Mississippi, Louisiana, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, New Mexico, Nevada, Oregon, Utah, Washington, and Wyoming. Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 3 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file."}, {"section_title": "4-4", "text": ""}, {"section_title": "Sample Design for the First-Grade Year", "text": ""}, {"section_title": "Fall First Grade", "text": "A subsample of students was selected for the fall first-grade data collection via a three-step procedure. In the first step, 30 PSUs were sampled from the 90 PSUs selected for the base year. These 90 PSUs consist of 10 self-representing PSUs due to their large population size, and 80 non-self-representing PSUs selected from 40 strata. The 10 self-representing PSUs were included in the fall first-grade sample with certainty. The remaining 20 PSUs were selected from the 80 non-self-representing PSUs. To select the 20 non-self-representing PSUs, 20 strata were sampled with equal probability from the 40 strata used to stratify the 80 non-self-representing PSUs in the full sample, and then one PSU was sampled within each stratum also with equal probability. This is equivalent to selection with probability proportional to size since the original PSU sample was selected with probability proportional to size. In the second step, all eligible schools within the sampled PSUs with students who were sampled in the base year were included in the fall first-grade sample. Table 4-4 shows the characteristics of the subsampled schools for the fall first-grade data collection in this second step. In the third step, students attending the subsampled schools who were respondents 4 in the base year and who had not moved outside of the United States were included as part of the fall first-grade sample. A subsample of eligible students who had moved to another school in the same PSU or another sampled PSU were assessed in their new school (or home, if the student's new school refused to participate in the study). An attempt was made to complete a parent interview, but not a child assessment, for students who moved to a PSU that was not part of the full ECLS-K:2011 sample of 90 PSUs.  1 Characteristics are taken from the original school frame. 2 States in each region: Northeast: Connecticut, Massachusetts, Maine, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Iowa, Illinois, Indiana, Kansas, Michigan, Minnesota, Missouri, North Dakota, Nebraska, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Maryland, Mississippi, Louisiana, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, New Mexico, Nevada, Oregon, Utah, Washington, and Wyoming. Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 3 Locale information was taken from the school sampling frame for most schools. For a very small number of schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2011. Table 4-5 shows the school characteristics for the subsampled schools with base-year respondents. Student sampling was conducted in the base year only. In the fall of first grade, an eligible student was one who was a base-year respondent and who had not moved outside the United States. All eligible students found still attending the subsampled fall first-grade schools were included in the fall data collections. Transfer schools (those schools that children moved into after fall kindergarten) are not included in this table. Table 4-6 shows the characteristics of base-year respondents in the fall first-grade sample.  1 Characteristics are taken from the original school frame. 2 States in each region: Northeast: Connecticut, Massachusetts, Maine, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Iowa, Illinois, Indiana, Kansas, Michigan, Minnesota, Missouri, North Dakota, Nebraska, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Maryland, Mississippi, Louisiana, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, New Mexico, Nevada, Oregon, Utah, Washington, and Wyoming. Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 3 Locale information was taken from the school sampling frame for most schools. For a very small number of schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2011. Other 4 259 209 \u2020 Not applicable. School characteristics are from the original school frame. Race/ethnicity is from the base year race/ethnicity composite; where it is missing the information comes from the schools' student lists. States in each region: Northeast: Connecticut, Massachusetts, Maine, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Iowa, Illinois, Indiana, Kansas, Michigan, Minnesota, Missouri, North Dakota, Nebraska, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Maryland, Mississippi, Louisiana, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, New Mexico, Nevada, Oregon, Utah, Washington, and Wyoming. Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. Locale information was taken from the school sampling frame for most schools. For a very small number of schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. This category includes children who are more than one race (non-Hispanic), and children whose race/ethnicity is unknown. NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment due to a disability) or parent interview data from at least one round of data collection in the base year. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2011."}, {"section_title": "4-8", "text": ""}, {"section_title": "4-9", "text": ""}, {"section_title": "4-10", "text": ""}, {"section_title": "4.2.2", "text": ""}, {"section_title": "Spring First Grade", "text": "All base-year respondents-those students in the base year who have assessment scores or parent data in at least one of the two rounds of data collection-were part of the spring first-grade sample. Students who were not assessed in kindergarten because of a disability and those who had only height and weight measurements are also considered base-year respondents. Table 4-7 shows the school characteristics for the schools with base-year respondents. Transfer schools (those schools that children moved into after the fall of kindergarten) are not included in this table.  1 Characteristics are taken from the original school frame. 2 States in each region: Northeast: Connecticut, Massachusetts, Maine, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Iowa, Illinois, Indiana, Kansas, Michigan, Minnesota, Missouri, North Dakota, Nebraska, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Maryland, Mississippi, Louisiana, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, New Mexico, Nevada, Oregon, Utah, Washington, and Wyoming. Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 3 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2012."}, {"section_title": "4-11", "text": "The characteristics of base-year respondents who were eligible for the spring first-grade data collection are those presented above in table 4-3; since there was no subsampling for the spring round of data collection, all base-year respondents were initially eligible for data collection."}, {"section_title": "Following Movers", "text": "Not all students who moved away from their original base-year schools between kindergarten and first grade (known as \"movers\") were followed into their new schools. While some movers were followed with certainty, some subsampling of other movers occurred, as described below. Homeschooled children, that is those who were enrolled in a school at the time of sampling in the base year but left school to become homeschooled, were followed with certainty; they were assessed in their home if there was parental consent to do so. Destination schools. When four or more students moved from an original sampled school into the same transfer school, all those movers were followed into the new school, which is referred to as a destination school. This type of movement occurred for children who attended sampled schools that ended at kindergarten, which are referred to as terminal schools. All base-year students in the terminal schools attended first grade in a school that was different from their base-year school. In some cases, a base-year school did not terminate in kindergarten, but for some reason four or more students from that school moved together to first grade into the same transfer school. For example, this would happen if the students' kindergarten school closed. More than one destination school may be identified for an original school if separate clusters of four or more students move into different transfer schools. Language minority (LM) students, students with an Individualized Education Program (IEP), and students who had an Individualized Family Service Plan (IFSP). Students who were identified as language minority (LM) based on parent report of home language in the base year, as well as students identified as currently having an Individualized Education Program (IEP), were followed at a rate of 100 percent. The IEP status of the child was obtained during the pre-assessment call when the team leader asked the school coordinator whether the child had an IEP or equivalent program on record with the school. The school records also may have indicated that a child had an Individualized Family Service Plan (IFSP) when he or she was younger, even if the child did not have an IEP at the time of data collection, which the team leader could have noted during the call. Additionally, information about whether a child had had an IFSP prior to kindergarten was collected in the base-year parent interview."}, {"section_title": "4-12", "text": "Approximately 92 percent of children who had an IFSP before starting kindergarten, according to parent report, were followed through the spring first-grade data collection. 5 All other movers. Fifty percent of students who did not meet one of the criteria described above (i.e., did not move to a destination school, were not LM, and did not have an IEP) were sampled with equal probability to be flagged as \"follow\" if they moved from their original sample school. If a student was flagged as \"do not follow,\" no data were collected for him or her. Students flagged as \"do not follow\" were not sought for participation in any further data collection. If a student was flagged as \"follow,\" and 1. the student moved into a school in a study PSU: the student was included in all aspects of data collection (child assessment, parent interview, school administrator questionnaire, and teacher questionnaires); 2. the student moved into a school outside a study PSU: only a parent interview was attempted; 3. the student moved into a school outside the country: the student was out of scope and considered ineligible for continuation in the study. Procedures for students in the fall subsample. In the fall of first grade, 50 percent of all students in the subsample had their follow flag set to \"follow\" after the base-year data collection. There are some differences between the group of IFSP children who were followed and those who were not. However, some of these differences appear to be related to the likelihood that a child had an IEP (and, therefore, whether the child became part of the protected group as a result of the IEP). For example, compared to those IFSP children who were not followed, a higher percentage of IFSP children who were followed attended public schools, which are required to provide disability services through an IEP. The subsampling process itself should not have introduced bias into the sample of IFSP children who were followed, because cases were randomly flagged to be followed. Additionally, the sampling weights developed for use with first-grade data account for this random subsampling. A comparison of key weighted estimates (such as school type, region of residence, school locale, percent of students in the school who were nonwhite, and student race/ethnicity, gender, and year of birth) between kindergarten and first grade generally suggests the loss of those children who were not followed has little impact on the overall estimates for children who had IFSPs before age 3. Where slight differences between the kindergarten and first-grade estimates were noticed (for example, on the percent of nonwhite students in a school), the pattern with the sample of IFSP children is reflective of differences seen in the full ECLS-K:2011 sample. Also, it should be kept in mind that identifying a child to be followed with certainty does not necessarily mean that the child would have participated in the round(s) in which he or she was followed. Due to general sample attrition, the IFSP students who were not flagged to be followed with certainty comprise only about half of all IFSP students who did not participate in first grade. It is unlikely that differences in weighted estimates for the entire group of IFSP children (about 680) are due solely to the absence of the approximately 60 IFSP cases that were not followed in first grade. Nonparticipation of IFSP children in later rounds of the study for any reason does reduce the IFSP sample available for analysis. As is the case for analysis of any small subgroup, users should consider the size of their analytic sample and whether there is enough power in the data to make generalizations about the groups being examined."}, {"section_title": "4-13", "text": "Children were sampled with equal probability to be flagged as \"follow,\" meaning that if they transferred to a new school they would be followed into that new school for the fall first-grade data collection. As explained in detail below, all students who are subsampled in the fall, regardless of their mover status are followed in the spring first grade data collection. Procedures for students in the spring main sample. In the spring of first grade, 50 percent of the schools in the main sample were subsampled with equal probability to have follow flags (i.e., all students in the 50 percent subsample of schools have flags set to \"follow\"). All fall first-grade schools in the 30 sampled PSUs were included in the \"mover follow\" sample for the spring of first grade. An additional sample of schools that were not part of the fall subsample was selected to arrive at 50 percent of the entire sample of schools being included in the \"mover follow\" subsample in the spring first-grade data collection. In this way, students who were originally sampled for fall first-grade data collection were included in the spring data collection with certainty. These fall subsample cases were followed for the spring data collection even if they were movers in the fall and had their fall mover flag set to \"not follow\" or they were nonrespondents in the fall. Also, this method allows fall first-grade movers to continue to be followed in each subsequent round of data collection, as well as more clustering of the movers to be followed, thus cutting down on field costs."}, {"section_title": "Calculation and Use of Sample Weights", "text": "The ECLS-K:2011 data should be weighted to compensate for differential probabilities of selection at each sampling stage and to adjust for the effect nonresponse can have on the estimates. For the base year, weights were provided at the child and school levels. Estimates produced using the baseyear child-level weights were representative of children who attended kindergarten or who attended an ungraded school or classroom and were of kindergarten age in the United States in the 2010-11 school year. Estimates produced using the base-year school-level weight were representative of schools with kindergarten programs or schools that educate children of kindergarten age in an ungraded setting. For the first-grade data collections, weights are provided only at the child level, to produce estimates for the kindergarten cohort during the 2011-12 school year. There are no school-level weights because the school sample is no longer nationally representative; it is not representative of schools with first grade or ungraded schools serving children of first-grade age. It is simply a set of schools attended by the children in the ECLS-K:2011 cohort during the 2011-12 school year."}, {"section_title": "4-14", "text": "The use of weights is essential to produce estimates that are representative of the cohort of children who were in kindergarten in 2010-11. Main sampling weights should be used to produce survey estimates. When testing hypotheses (e.g., conducting t tests, regression analyses, etc.) using weighted data from a study such as the ECLS-K:2011 that has a complex design, analysts also should use methods to adjust the standard errors. Two such methods are jackknife replication variance estimation and the Taylor series linearization method. Replicate weights are provided in the data file for use with the paired jackknife replication procedure, and PSU and stratum identifiers are provided for use with the Taylor series method."}, {"section_title": "Types of Sample Weights", "text": "Main sampling weights designed for use with data from a complex sample survey serve two primary purposes. When used in analyses, the main sampling weight weights the sample size up to the population total of interest. In the ECLS-K:2011, weighting produces national-level estimates. Also, the main sampling weight adjusts for differential nonresponse patterns that can lead to bias in the estimates. If people with certain characteristics are systematically less likely than others to respond to a survey, the collected data may not accurately reflect the characteristics and experiences of the nonrespondents, which can lead to bias. To adjust for this, respondents are assigned weights that, when applied, result in respondents representing their own characteristics and experiences as well as those of nonrespondents with similar attributes. A sample weight could be produced for use with data from every component of the study (e.g., data from the fall kindergarten parent interview, from the fall first-grade child assessment, or from the spring first-grade teacher questionnaire) and for every combination of components for the study (e.g., data from the spring first-grade child assessment with data from the spring first-grade school administrator questionnaire, or data from the spring kindergarten child assessment with data from the fall first-grade child assessment and the fall first-grade parent interview). However, creating all possible weights for a study with as many components as the ECLS-K:2011 would be impractical, especially as the study progresses and the number of possible weights increases. In order to determine which weights would be most useful for researchers analyzing data from first grade, completion rates for each fall firstgrade and spring first-grade component (e.g., response to the child assessment, the parent interview, various parts of the teacher questionnaire) were reviewed in combination with completion rates from the kindergarten year, and consideration was given to how analysts are likely to use the data."}, {"section_title": "4-15", "text": "The best approach to choosing a sample weight for a given analysis is to select one that maximizes the number of sources of data included in the analyses for which nonresponse adjustments are made, which in turn minimizes bias in estimates, while maintaining as large an unweighted sample size as  Exhibit 4-2, which presents the same information as exhibit 4-1 in matrix format, was developed to further assist researchers in deciding which weight to use for analyses. In exhibit 4-2, the components for which nonresponse adjustments are made for each weight are noted with a \"Yes.\" Researchers should choose a weight that has a \"Yes\" in the column(s) for the source(s) of data they are using in their analyses. The best weight would have a \"Yes\" for each and every source used. For example, if a researcher is conducting an analysis that includes fall first-grade child assessment data, fall first-grade parent interview data, and child-level data reported by the teachers in the fall of first grade, the weight 4-18 W3CF3P3T0 should be used since it adjusts for nonresponse on all three of those components (i.e., exhibit 4-2 shows a \"Yes\" in the fall first-grade child assessment, parent, and teacher columns). However, for many analyses, there will be no weight that adjusts for nonresponse to all the sources of data that are included and for only those sources. When no weight corresponds exactly to the combination of components included in the desired analysis, researchers might prefer to use a weight that includes nonresponse adjustments for more components than they are using in their analysis (i.e., a weight with \"Yes\" in columns corresponding to components that are not included in their analyses) if that weight also includes nonresponse adjustments for the components they are using. Although such a weight may result in a smaller analytic sample than would be available when using a weight that corresponds exactly to the components from which the analyst is using data, it will adjust for the potential differential nonresponse associated with the components. If researchers instead choose a weight with nonresponse adjustments for fewer components than they are using in their analysis, missing data should be examined for potential bias. "}, {"section_title": "4.3.2", "text": ""}, {"section_title": "Computation of Sample Weights", "text": "The computation of weights follows a general rule: (1) a base weight is computed to reflect the sample design, and 2 Main sampling weights (indicated by the suffix 0) and replicate weights (indicated by the suffixes 1 to 40 or 1 to 80) were computed and included in the data file. In the sections that follow, only the main sampling weight is discussed, but any adjustment done to the main sampling weight was done to the replicate weights as well."}, {"section_title": "Student Base Weights", "text": "Only base-year respondents were eligible to participate in the first-grade rounds of data collection. For the fall of first grade, when only a subsample of students was included in data collection, the first-grade student base weight is the product of the base-year student base weight adjusted for baseyear nonresponse and the inverse of the selection probabilities for the primary sampling units for the fall subsample. For the spring of first grade, when the full sample of students was included in data collection, the first-grade student base weight is the base-year student base weight adjusted for base-year nonresponse. The adjustment factor for base-year nonresponse is the sum of the base weights of the 6 This was part of the school nonresponse adjustment that was done in the base year."}, {"section_title": "4-21", "text": "eligible students in the base year divided by the sum of the base weights of the base-year respondents. 7 For a description of the computation of the base-year student base weights, see section 4.2.2.3.1 of the base-year User's Manual."}, {"section_title": "Student Weights Adjusted for Mover Subsampling", "text": "The student base weight described in section 4.3.2.1 was adjusted to reflect the subsampling of movers described in section 4.2.3. For every student who is a base-year respondent, a \"follow\" flag was assigned a value of 0 (do not follow if moved) or 1 (follow if moved). A mover-subsampling adjustment factor was set to 1 if the student was not a mover, 2 if the student was a mover and was followed into his or her new school, and 0 if the student was a mover and was not followed. The moversubsampling adjusted weight is the product of the base weight described in section 4.3.2.1 and this mover-subsampling adjustment factor. Note that child assessments were not conducted and school staff questionnaires were not fielded for students who moved into nonsampled PSUs even if their flag was set to \"follow\"; therefore, they are counted as nonrespondents in the adjustment for student nonresponse. An attempt was made to complete a parent interview for students who moved into nonsampled PSUs if their flag was set to \"follow\"; therefore, their parents would be counted as respondents in the adjustment for parent nonresponse if a parent interview was completed."}, {"section_title": "Student Nonresponse-Adjusted Weights", "text": "The mover-subsampling adjusted weight described in section 4.3.2.2 was adjusted for nonresponse to produce each of the student-level weights described in exhibit 4-1. For each weight, a response status was defined based on the presence of data for the particular component(s) and round(s) covered by the weight. For example, for the weight W3CF3P_30, an eligible respondent is a base-year respondent who satisfies both of these criteria: (1) the student has child assessment data 8 from the spring of kindergarten and fall of first grade, and (2) the student has parent interview data from either the fall or spring of kindergarten, as well as parent data from the fall of first grade. An ineligible student is one who moved out of the country or is deceased or moved to another school and was not assigned to be followed. A student of unknown eligibility is one who could not be located. The remaining students are eligible nonrespondents. Nonresponse adjustment was done in two steps: (1) adjustment for children whose eligibility was not determined (i.e., those who could not be located, or those who moved to another sampled PSU and who did not have parent interview data because the parent could not be contacted), and (2) adjustmentfor eligible nonrespondents. In the first step, a portion of cases with unknown eligibility was assumed to be ineligible. Nonresponse classes were created using school and child characteristics and used for both unknown eligibility and nonresponse adjustments. Note that the weights involving BASC data are not computed only for children with BASC data or who were eligible for the BASC component. They are computed for the entire sample and include additional adjustments for BASC unknown eligibility and BASC nonresponse. For example, weight W4C4P4TZ0 is nonzero for children with child assessment data from both kindergarten rounds and the spring of first grade, as well as parent data from fall kindergarten and the spring of first grade, and teacher data (teacher/classroom or child-level) from the spring of first grade. It includes adjustment for nonresponse associated with these sets of data but also adjustments for BASC unknown eligibility and nonresponse, and is, therefore, appropriate for analyses that include BASC data along with data from these other components."}, {"section_title": "Raking to Sample Control Totals", "text": "To reduce the variability due to the subsampling of movers and to ensure that the final weights continue to sum to the base-year population total, the student nonresponse-adjusted weights were raked to sample-based control totals using the first-grade student base weights. Raking is a calibration estimator that is closely related to poststratification. The poststratification adjustment procedure involves applying a ratio adjustment to the weights. Respondents are partitioned into groups, known as poststrata cells, and a single ratio adjustment factor is applied to the weights of all units in a given poststratification cell. The numerator of the ratio is a \"control total\" usually obtained from a secondary source; the denominator is a weighted total for the survey data. Therefore at the poststratum level, estimates obtained using the poststratified survey weights will correspond to the control totals used. If either the cell level 4-23 population counts are not available for all cells or the majority of the cell sample sizes are too small, raking is used to adjust the survey estimates to the known marginal totals of several categorical variables. Raking is essentially a multivariate poststratification. In the ECLS-K:2011, multiple background characteristics from schools, students, and parents were combined to create raking cells. The student records included in the file used for computing the control totals are records of base-year eligible children. The sum of the base weights from this file is the estimated number of children who were in kindergarten in 2010-11. Raking was done within raking cells (also known as raking dimensions). The raking dimensions were based on single characteristics (e.g., locale) or a combination of characteristics (e.g., age and race/ethnicity). Chi-Square Automatic Interaction Detector (CHAID) analysis was used to determine the best set of raking cells. The final weight is the product of the raking factor and the student nonresponseadjusted weight. The raking factor was computed as the ratio of the base-year sample control total for a raking cell over the sum of the nonresponse-adjusted first-grade weights in that raking cell."}, {"section_title": "Characteristics of Sample Weights", "text": "The statistical characteristics of the sample weights are presented in table 4-8. For each weight, the number of cases with a nonzero weight is presented along with the mean weight, the standard deviation, the coefficient of variation (i.e., the standard deviation as a percentage of the mean weight), the minimum weight, the maximum weight, the skewness, the kurtosis, and the sum of weights. The procedure for raking to control totals included respondents and ineligible cases. Afterwards, weights of ineligible cases were set to zero. Because a portion of children of unknown eligibility was assumed to be ineligible (as discussed in section 4.3.2.3) and this adjustment for unknown eligibility was done within adjustment cells, there are small differences in the sums of weights. "}, {"section_title": "4-24", "text": ""}, {"section_title": "Variance Estimation", "text": "The precision of the sample estimates derived from a survey can be evaluated by estimating the variances of these estimates. For a complex sample design such as the one employed in the ECLS-K:2011, replication and Taylor Series methods have been developed to correctly estimate variance. These methods take into account the clustered, multistage sampling design and the use of differential sampling rates to oversample targeted subpopulations. For the ECLS-K:2011, in which the first-stage self-representing sampling units (i.e., PSUs) were selected with certainty and the first-stage non-selfrepresenting sampling units were selected with two units per stratum, the paired jackknife replication method (JK2) is recommended. This section describes the JK2 and the Taylor series methods, which can be used to compute correct standard errors for any analysis."}, {"section_title": "4-25", "text": ""}, {"section_title": "4.3.4.1", "text": ""}, {"section_title": "Jackknife Method", "text": "The final main sampling and replicate weights can be used to compute estimates of variance for survey estimates using the jackknife method with two PSUs per stratum (JK2) using several software packages, including WesVar, AM, SUDAAN, SAS, Stata, and R. In the jackknife method, each survey estimate of interest is calculated for the full sample as well as for each of the g replicates, where g is 80 for the spring weights, and 40 for the fall weights. The variation of the replicate estimates around the fullsample estimate is used to estimate the variance for the full sample. The variance estimator is computed as the sum of squared deviations of the replicate estimates from the full sample estimate: where \u03b8 is the survey estimate of interest, \u03b8\u02c6 is the estimate of \u03b8 based on the full sample, G is the number of replicates, and \u03b8\u02c6 (g ) is the g th replicate estimate of \u03b8 based on the observations included in the g th replicate. Each main sampling weight that does not include adjustments for nonresponse to components from the fall first-grade data collection has 80 corresponding replicate weights for use with the JK2 method. The replicate weights begin with the same characters as the main sampling weight and end with the numbers 1 to 80. For example, the replicate weights corresponding to weight W4C4P_20 are W4C4P_21 through W4C4P_280. For weights that include nonresponse adjustments for components from the fall first-grade data collection, there are 40 replicate weights. For example, weight W3CF3P_30 has W3CF3P_31 through W3CF3P_340 as replicate weights."}, {"section_title": "Taylor Series Method", "text": "Variance stratum and variance unit (first-stage sample unit [i.e., PSU]) identifiers were also created to be used in statistical software that computes variance estimates based on the Taylor series method (for example, AM, SUDAAN, SAS, SPSS, and Stata). In this method, a linear approximation of a statistic is formed and then substituted into the formula for calculating the variance of a linear estimate appropriate for the sample design."}, {"section_title": "4-26", "text": "If is the corresponding vector of estimators based on a sample s of size n(s), is the population parameter of interest, and is an estimator of \u03b8 , then and The Taylor series method relies on a simplified procedure for estimating the variance for a linear statistic even with a complex sample design and is valid when analyzing data from large samples in which the first-stage units are sampled with replacement. 9 The stratum and first-stage unit identifiers needed to use the Taylor series method were assigned as follows: all independent sampling strata were numbered sequentially from 1 to h; within each sampling stratum, first-stage sampling units were numbered from 1 to nh. Care was taken to ensure that there were at least two responding units in each stratum. For instances in which a stratum did not have at least two responding units, the stratum was combined with an adjacent stratum. Stratum and first-stage unit identifiers are provided in the data file. Each main sampling weight has corresponding stratum and PSU identifiers for use with the Taylor series method. The stratum and PSU identifiers begin with the same characters as the main sampling weight and end with either STR or PSU. For example, the stratum and PSU identifiers corresponding to weight W4PF40 are W4PF4STR and W4PF4PSU, respectively."}, {"section_title": "Specifications for Computing Standard Errors", "text": "For the jackknife replication method, the main sampling weight, the replicate weights, and the method of replication must be specified. All analyses of the ECLS-K:2011 data using the replication method should be done using JK2. As an example, an analyst using the main sample weight W3CF3P_30 to compute child-level estimates of mean reading scores for the fall of first grade would need to specify W3CF3P_30 as the main sampling weight, W3CF3P_31 to W3CF3P_340 as the replicate weights, and 9 For the ECLS-K:2011, the sample of primary sampling units (PSUs) was selected using the Durbin method. In this method, two PSUs were selected per stratum without replacement with probability proportional to size and known joint probability of inclusion in such a way to allow variances to be estimated as if the units had been selected with replacement."}, {"section_title": "4-27", "text": "JK2 as the method of replication. Note that there are 40 replicate weights for each weight that involves the fall first-grade data collection, and 80 replicate weights for each weight not involving the fall firstgrade data collection. For the Taylor series method, the main sampling weight, the sample design, the nesting stratum, and PSU variables must be specified. As an example, an analyst using the main sample weight W3CF3P_30 to compute child-level estimates of mean reading scores for the fall of first grade must specify the main sampling weight (W3CF3P_30), the stratum variable (W3CF3P_3STR), and the PSU variable (W3CF3P_3PSU). The \"with replacement\" sample design option, WR, must also be specified if using SUDAAN."}, {"section_title": "Use of Design Effects", "text": "An important analytic device is to compare the statistical efficiency of survey estimates from a complex sample survey such as the ECLS-K:2011 with what would have been obtained in a hypothetical and usually impractical simple random sample (SRS) of the same size. In a stratified clustered design, stratification generally leads to a gain in efficiency over simple random sampling, but clustering has the opposite effect because of the positive intracluster correlation of the units in the cluster. The basic measure of the relative efficiency of the sample is the design effect (DEFF), defined as the ratio, for a given statistic, of the variance estimate under the actual sample design to the variance estimate that would be obtained with an SRS of the same sample size: The root design effect is the square root of the design effect: where SE is the standard error of the estimate. As discussed above, jackknife replication and Taylor Series can be used to compute more precise standard errors for data from complex surveys. If statistical analyses are conducted using software packages that assume the data were collected using simple random sampling (i.e., adjustments are not 4-28 made using jackknife replication or the Taylor series method), the standard errors will be calculated under this assumption and will be incorrect. They can be adjusted using the average root design effect (DEFT), although this method is less precise than JK or Taylor series. 10 The standard error of an estimate under the actual sample design can be approximated as the product of the DEFT and the standard error assuming simple random sampling. In the ECLS-K:2011, a large number of data items were collected from children, parents, teachers, school administrators, and before-and after-school care providers. Each item has its own design effect that can be estimated from the survey data. Standard errors and design effects are presented in the tables below for selected items from the study to allow analysts to see the range of standard errors and design effects for the study variables. They were computed using the paired jackknife replication method in WesVar. However, as discussed in section 4.3.4, not all statistical analysis software packages have procedures to compute the variance estimate or standard error using the replication method, and some analysts may not have access to software packages that do have such procedures. In such situations the correct variance estimate or standard error can be approximated using the design effect or the root design effect. As the first step in the approximation of a standard error, the analyst should normalize the overall sample weights for packages that use the weighted population size (N) in the calculation of standard errors (SPSS but not SAS). The normalized weight will sum to the sample size (n) and is calculated as where n is the sample size (i.e., the number of cases with a valid main sampling weight) and N is the sum of weights. See exhibit 4-2 for the type of weights to use and table 4-8 for the sample size n and the sum of weights N. As the second step in the approximation, the standard errors produced by the statistical software, the test statistics, or the sample weight used in analysis can be adjusted to reflect the actual 10 Common procedures in SAS, SPSS, and Stata assume simple random sampling. Data analysts should use the SURVEY procedure (SAS), the Complex Samples module (SPSS), or the SVY command (Stata) to account for complex samples."}, {"section_title": "4-29", "text": "complex design of the study. To adjust the standard error of an estimate, the analyst should multiply the standard error produced by the statistical software by the square root of the DEFF or the DEFT as follows: A standard statistical analysis package can be used to obtain VARSRS and SESRS. The DEFF and DEFT used to make adjustments can be calculated for specific estimates, can be the median DEFF and DEFT across a number of variables, or can be the median DEFF and DEFT for a specific subgroup in the population. Adjusted standard errors can then be used in hypothesis testing, for example, when calculating t and F statistics. A second option is to adjust the t and F statistics produced by statistical software packages using unadjusted (i.e., SRS) standard errors. To do this, first conduct the desired analysis weighted by the normalized weight and then divide a t statistic by the DEFT or divide an F statistic by the DEFF. A third alternative is to create a new analytic weight variable in the data file by dividing the normalized analytic weight by the DEFF and using the adjusted weight in the analyses. Table 4-9 shows estimates, standard errors, and design effects for 29 means and proportions selected from the fall data collection. Table 4-10 shows the median design effects for the same items but for subgroups. For each survey item, table 4-9 presents the number of cases for which data are nonmissing, the estimate, the standard error taking into account the actual sample design (Design SE), the standard error assuming SRS (SRS SE), the root design effect (DEFT), and the design effect (DEFF). Standard errors (Design SE) were produced in WesVar using JK2 based on the actual ECLS-K:2011 complex design. For each survey item, the variable name as it appears in the ECLS-K:2011 electronic codebook (ECB) is also provided in the table. Table 4-11 and table 4-12 show the same statistics but for 55 means and proportions selected from the spring data collection. In general, design effects for fall first-grade are larger than design effects for spring firstgrade for similar items. This is due to the larger variability in the weights as a result of subsampling. As was the case in the base year, design effects for the teacher-level data and the school-level data are quite large compared to the rest because the intraclass correlation is 100 percent for children in the same class with the same teacher, and children in the same school. Design effects are also large when the estimate applies only to a small sample of children.  1 Estimates for variables with names starting with X3 or P3 were computed using weight W3CF3P_30, except for those with names starting with X3T. 2 Estimates for variables with names starting with X3T or T3 were computed using weight W3CF3P3T0. NOTE: SE is the standard error based on the sample design. SEsrs is the standard error assuming simple random sampling. DEFT is the root design effect. DEFF is the design effect. Estimates produced with the restricted-use file. Due to top-and bottom-coding, the same estimates may not be obtained from the public-use file. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2011.    1 Estimates for assessment scores, age, height, weight and BMI were computed using weight W4CS4P_20. 2 Estimates for score variables from the parent interview were computed using weight W4CS4P_40. 3 Estimates for score variables from the teacher questionnaire were computed using weight W4CS4P_2T0. 4 Estimates for variables from the parent interview were computed using weight W4CS4P_40. 5 Estimates for variables from the teacher and school administrator questionnaires were computed using weight W4CS4P_2T0. NOTE: SE is the standard error based on the sample design. SEsrs is the standard error assuming simple random sampling. DEFT is the root design effect. DEFF is the design effect. Estimates produced with the restricted-use file. Due to top-and bottom-coding, the same estimates may not be obtained from the public-use file. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2012. "}, {"section_title": "4-30", "text": ""}, {"section_title": "4-31", "text": ""}, {"section_title": "4-33", "text": ""}, {"section_title": "4-35", "text": "This page intentionally left blank."}, {"section_title": "RESPONSE RATES", "text": "This chapter presents unit response rates and overall response rates for the different instruments included in the first-grade year of the ECLS-K:2011. A unit response rate is the ratio of the number of units with a completed interview, questionnaire, or assessment (for example, the units are students with a completed assessment) to the number of units sampled and eligible for the interview, questionnaire, or assessment. Unit response rates are used to describe the outcomes of data collection activities and to measure the quality of the study. The overall response rate indicates the percentage of eligible units with a completed interview, questionnaire, or assessment, taking all survey stages into account."}, {"section_title": "Study Instruments", "text": "For the ECLS-K:2011 first-grade data collections, there were several survey instruments, as shown in exhibit 5-1. Response rates are presented in section 5.2 for all of these instruments, separately for each round of data collection in which the instrument was included and, for selected instruments, for combinations of rounds of data collection. No Yes School administrator completed at least one item in the school administrator questionnaire 1 In the spring data collection, there were two versions of the teacher-level teacher questionnaire: (1) TQAK was filled out by a teacher who had only sampled students who were in kindergarten linked to him or her, and (2) TQA1 was filled out by a teacher who was linked to a group of sampled students that included at least one student in first grade or above, though the group could have also included students in kindergarten. 2 In the spring data collection, there were two versions of the student-level teacher questionnaire: (1) TQCK was filled out for sampled students who were in kindergarten, and (2) TQC1 was filled out for sampled students who were in first grade or above. 3 In the spring data collection, there were two versions of the school administrator questionnaire: (1) SAQA was given to administrators in schools for which there were no school administrator data from the spring of kindergarten, and (2) SAQB was given to administrators in schools for which there were school administrator data from the spring of kindergarten. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2011 and spring 2012."}, {"section_title": "Unit Response Rates and Overall Response Rates", "text": "All tables have weighted and unweighted response rates. The weight used in the computation of the student-level unit response rate is the first-grade student base weight. For a description of these weights, see chapter 4. While unweighted rates are useful for evaluating sample performance, only weighted rates are discussed in this section. The tables in this chapter present response rates for the different components of data collection shown above in exhibit 5-1 (the child assessment, parent interview, general classroom teacher questionnaires, school administrator questionnaire (SAQ), and special education teacher questionnaires) computed at the student level. Response rates for all students and response rates by selected school and student background characteristics are provided. In order to compute response rates by different characteristics, the selected characteristics must be known for both respondents and nonrespondents. For rates for the spring first-grade data collection, information on the school characteristics presented in the tables, such as school enrollment or 5-2 percent minority, comes from the first-grade SAQ for the original or transfer school that the child attended in spring first-grade. When data from the first-grade SAQ are not available, the information used in the tables comes from the base year, but again for the school the child attended in spring first-grade. For rates for the fall data collection where the SAQ was not administered, school characteristic information comes from the spring first-grade SAQ for the original or transfer school the child attended in fall first grade if that is the same as the spring first-grade school. When a school does not have SAQ data either from first grade or the base year, data are from the Common Core Data (CCD) or the Private School Survey (PSS). Information on the child characteristics presented in the tables comes from the firstgrade data collection. If first-grade data are not available, base-year data are used. As noted in chapter 4, the fall first-grade data collection was conducted with a subsample of students attending schools that had participated in the base year and were located within the subsample of 30 PSUs selected for the fall collection. While all students attending the subsample schools who had been originally sampled for the study are considered to be part of the fall subsample (7,019 children in 346 schools), only those students who were base-year respondents 1 were followed for participation in the fall first-grade data collection. Of those 6,109 base-year respondents, about 20 were ineligible for fall firstgrade because they had moved out of the country, and about 300 were not included in the fall data collection because they were movers who were subsampled out of the study. Students who were excluded from the assessment due to lack of accommodations are not included in the calculation of response rates for the child assessment. The denominator used to calculate the unweighted fall child assessment response rate is 5,765. The denominator used to calculate the unweighted fall parent interview response rate is 5,792. For the teacher response rates, the denominator is 5,481. This denominator is lower because it excludes homeschooled children 2 as well as those children who do not have either a child assessment score or parent interview from the current round, fall first-grade. 3 The parent and teacher response rates are computed at the student level, meaning they indicate the percentages of students for whom a parent interview was completed and for whom a teacher questionnaire was received, respectively. Table 5-1 presents weighted and unweighted student-level response rates for the child assessment and parent interview in the fall first-grade data collection, by selected school characteristics. For the fall child assessment, the weighted student-level response rate was 88.7 percent. With the exception of the \"Unknown\" categories for each school characteristic, almost all of the response rates by the selected school characteristics exceed 90 percent. The highest response rates were in the South census 1 A base-year respondent has child data (scoreable assessment data or height or weight measurements or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. 2 Homeschooled children were enrolled in a school at the time of sampling in the base year but left school to become homeschooled. 3 A fall first-grade respondent has child data (scoreable assessment data or height or weight measurements or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from the fall first-grade round of data collection."}, {"section_title": "5-3", "text": "region (97.1 percent), in towns and rural areas (96.9 percent and 97.0 percent respectively), and for students in schools with 150 to 299 students enrolled (98.2 percent). The lowest response rates were found for students in other private schools (92.9 percent) and in the schools with smallest enrollment (89.4 percent). For the fall parent interview, the weighted response rate was 86.7 percent, which was lower than most of the response rates when looking at rates by specific school characteristics in the table. The average response rate is brought down by the very low response rates for students for which the characteristics of their schools are unknown (i.e., those in the \"Unknown\" categories). The highest response rates were for students in Catholic schools (92.6 percent) and schools in the lowest percent minority group (92.5 percent), and the lowest response rate was for students in schools with in smallest enrollment size category (85.5 percent).  1 Student had scoreable reading or mathematics or science data, or at least one executive function score, or a height or weight measurement. 2 Parent answered all applicable items in the time use section of the questionnaire (TUQ). 3 Because the School Administrator Questionnaire (SAQ) was not administered in fall first grade, school characteristics (school type, region, locale, percent minority in the school) were calculated using the SAQ responses for round 3 participants who were also round 4 participants and attending the same school in both rounds, where available. When round 4 SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). Due to differences between the way prior-round SAQ/CCD/PSS data were used to generate estimates in this table and the way those data were used to calculate the round 4 composite variables (especially percent minority enrolled), estimates in this table cannot be replicated using the data file. 4 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total.   . Black students also had a lower response rate for the parent interview (81.2 percent). 1 Student had scoreable reading or mathematics or science data, or at least one executive function score, or a height or weight measurement. 2 Parent answered all applicable items in the time use section of the questionnaire (TUQ). 3 Race/ethnicity information comes from the composite variable X_RACETH_R. Information collected from schools at the of sampling was used to code race/ethnicity for a small number of cases with missing data on X_RACETH_R. 4 Sample sizes have been rounded to the nearest 10. Therefore, detail may not sum to total. NOTE: The fall first-grade data collection included only 30 percent of the PSUs. The weighted response rates were calculated using the fall firstgrade student base weight. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2011. Aside from the \"Unknown\" categories, which had very low response rates, the lowest response rates were for students in other private schools (92.5 percent) and students in schools with the highest percentage of minority enrollment (92.3 percent). 1 Because the School Administrator Questionnaire (SAQ) was not administered in fall first grade, school characteristics (school type, region, locale, percent minority in the school) were calculated using the SAQ responses for round 3 participants who were also round 4 participants and attending the same school in both rounds, where available. When round 4 SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). Due to differences between the way prior-round SAQ/CCD/PSS data were used to generate estimates in this table and the way those data were used to calculate the round 4 composite variables (especially percent minority enrolled), estimates in this table cannot be replicated using the data file. 2 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total. 3 "}, {"section_title": "5-4", "text": ""}, {"section_title": "5-6", "text": ""}, {"section_title": "5-7", "text": ""}, {"section_title": "5-9", "text": "The overall response rate indicates the percentage of possible interviews, questionnaires, or assessments completed, taking all survey stages into account. In the base-year data collection, children were identified for assessment in a two-stage process. The first stage involved the recruitment of sampled schools to participate in the study. Assessments were then conducted for the sampled children whose parents consented. In fall first-grade, children in the subsampled schools were eligible for follow-up unless they became ineligible because they moved out of the country or they were movers who were not sampled for follow-up. Under this design, the response rate for the school is the percentage of original sampled schools in the subsample that had base-year responding children who were allowed to be followed up in fall first-grade. The response rate for the child assessment is the percentage of sampled and eligible children who completed the assessment. The overall response rate is the product of the base-year before-substitution school response rate and the child assessment response rate. The overall weighted and unweighted response rates for the child assessment, the parent interview, and the student-level teacher questionnaire in the fall first-grade data collection are presented in tables 5-5 and 5-6. All schools in the fall subsample either responded to the fall data collection (they have fall first-grade students) or became ineligible (the base-year respondents who were in these schools moved to other schools). Because children were sampled in the base year and school participation after the base year was not required for the children to stay in the study, the school response rates used to calculate the student-level response rates in these tables are those from the base year (the base-year response rates are presented in The final overall response rate for the fall child assessment (the product of the base-year school response rate and the fall child assessment rate) was 55.6 percent. Looking at child assessment response rates by school characteristics, the highest response rates were for students attending schools in the Midwest region (70.8 percent) and students attending schools in which the percentage of enrolled students who were racial/ethnic minorities was 86 percent or higher (67.4 percent). The subgroups with the lowest response rates were the Northeast (54.6 percent) and West (55.9 percent) regions, and students in schools with an enrollment size of between 300 to 499 students (55.8 percent). The overall response rate for the fall parent interview was 54.4 percent. Looking at parent interview response rates by school characteristics, the patterns of response by subgroup are similar to what was observed for the child assessment. 5-11 1 Student had scoreable reading or mathematics or science data, or at least one executive function score, or a height or weight measurement. 2 Parent answered all applicable items in the time use section of the questionnaire (TUQ). 3 Because the School Administrator Questionnaire (SAQ) was not administered in fall first grade, school characteristics (school type, region, locale, percent minority in the school) were calculated using the SAQ responses for round 3 participants who were also round 4 participants and attending the same school in both rounds, where available. When round 4 SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). Due to differences between the way prior-round SAQ/CCD/PSS data were used to generate estimates in this table and the way those data were used to calculate the round 4 composite variables (especially percent minority), estimates in this table cannot be replicated using the data file. 4 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total.   Because the School Administrator Questionnaire (SAQ) was not administered in fall first grade, school characteristics (school type, region, locale, percent minority in the school) were calculated using the SAQ responses for round 3 participants who were also round 4 participants and attending the same school in both rounds, where available. When round 4 SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). Due to differences between the way prior-round SAQ/CCD/PSS data were used to generate estimates in this table and the way those data were used to calculate the round 4 composite variables (especially percent minority enrolled), estimates in this table cannot be replicated using the data file. 2 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total. 3 States in each region: Northeast: Connecticut, Massachusetts, Maine, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Iowa, Illinois, Indiana, Kansas, Michigan, Minnesota, Missouri, North Dakota, Nebraska, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Maryland, Mississippi, Louisiana, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, New Mexico, Nevada, Oregon, Utah, Washington, and Wyoming. NOTE: The fall first-grade data collection included only 30 percent of the PSUs. A respondent is defined as a child for whom a teacher questionnaire was returned and the questionnaire had at least one response. The weighted overall response rate was calculated using the school base weight for the school response rate component and the student base weight for the student response rate component. The counts of students by subgroups do not sum to the total because homeschooled students and students with unknown school characteristics are not included in this "}, {"section_title": "5-10", "text": ""}, {"section_title": "5-13", "text": "In the spring first-grade data collection, the 18,174 base-year respondents were part of the sample. Of these, about 70 were ineligible because they moved out of the country, and about 1,370 were not included in the spring data collection because they were movers who were subsampled out of the study. Students who were excluded from the assessment due to lack of accommodations are not included in the calculation of response rates for the child assessment. The denominator used to calculate the unweighted child assessment response rate is 16,661. The denominator used to calculate the unweighted parent response rate is 16,733. Students who were homeschooled and those who were not spring firstgrade respondents 4 were not eligible for the teacher questionnaires. The denominator used to calculate the teacher and the school administrator response rates is 15,623. As with the fall response rates, the parent and teacher rates are computed at the student level, meaning they indicate the percentages of students for whom a parent interview was completed or for whom a teacher questionnaire was received. Above it was noted that there were two versions of each type of teacher questionnaire, one pertaining to kindergarten and one pertaining to first grade. The response rates are calculated as the percentage of all students whose teacher completed a questionnaire, regardless of the version completed. That is, separate response rates are not calculated for each version. The school administrator rate is also computed at the student level and indicates the percentage of students whose school administrator completed a questionnaire. As with the teacher questionnaires, there were two versions of the administrator questionnaire, and response rates are not calculated separately for each version. Table 5-7 presents weighted and unweighted response rates for the child assessment and the parent interview in the spring first-grade data collection, by selected school characteristics. The weighted response rate for the spring child assessment was 88.0 percent, however most subgroups have response rates greater than 95 percent. The average response rate is brought down by the very low response rates for students for which the characteristics of their schools are unknown (i.e., those in the \"Unknown\" categories). Aside from the students who fall in the \"Unknown\" categories, the lowest response rates were for students in other private schools (89.3 percent) and in schools with the smallest enrollment size (92.5 percent).  1 Student had scoreable reading or mathematics or science data, or at least one executive function score, or a height or weight measurement. 2 Parent answered all applicable items in the family structure section of the questionnaire (FSQ). 3 School characteristics (school type, region, locale, percent minority in the school) were calculated using the School Administrator Questionnaire (SAQ) responses for round 4 participants where available. When round 4 SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). Due to differences between the way prior-round SAQ/CCD/PSS data were used to generate estimates in this table and the way those data were used to calculate the composite variables (especially percent minority enrolled), estimates in this table cannot be replicated using the data file. 4 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total. 5   Indians/Alaskan Natives (also 62.7 percent).  Table 5-9 presents weighted and unweighted response rates for the general classroom teacher questionnaires in the spring first-grade data collection, by selected school characteristics. The weighted response rate for the teacher-level questionnaire was 87.7 percent, which is lower than the response rates for most subgroups due to the very low response rates for the \"Unknown\" categories. The highest rates were observed for students in schools with 15 percent or less minority enrollment (96.8 percent) and in Catholic schools (96.7 percent). Among categories other than \"unknown,\" the lowest rates were found in schools with at least 86 percent minority enrollment (84.4 percent), in the West (87.1 percent), and in the cities (87.0 percent). For the student-level teacher questionnaires, the weighted response rate was 87.9 percent. The response rates by subgroup are very close to the rates observed for the teacher-level questionnaire.   1 School characteristics (school type, region, locale, percent minority in the school) were calculated using the School Administrator Questionnaire (SAQ) responses for round 4 participants where available. When round 4 SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). Due to differences between the way prior-round SAQ/CCD/PSS data were used to generate estimates in this table and the way those data were used to calculate the composite variables (especially percent minority enrolled), estimates in this table cannot be replicated using the data file. 2 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total. 3 States in each region: Northeast: Connecticut, Massachusetts, Maine, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Iowa, Illinois, Indiana, Kansas, Michigan, Minnesota, Missouri, North Dakota, Nebraska, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Maryland, Mississippi, Louisiana, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, New Mexico, Nevada, Oregon, Utah, Washington, and Wyoming. NOTE: A respondent is defined as a child for whom a teacher questionnaire was returned and the questionnaire had at least one response. The weighted response rates were calculated using the spring first-grade student base weight. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2012. Table 5-10 presents weighted and unweighted response rates for the teacher questionnaires in the spring first-grade data collection, by selected student characteristics. The highest subgroup rates were observed for White students (90.2 percent), American Indians/Alaskan Natives (89.7 percent), and students born in 2004 (89.7 percent). The subgroups with the lowest rates were Asian students [5][6][7][8][9][10][11][12][13][14][15][16][17][18][19] (81.9 percent) and students born in 2006 (74.1 percent), though the latter group had very few respondents. Response rates by subgroup for the student-level teacher questionnaire show similar patterns as those for the teacher-level questionnaire. Race/ethnicity information comes from the composite variable X_RACETH_R. Information collected from schools at the of sampling was used to code race/ethnicity for a small number of cases with missing data on X_RACETH_R. 2 Sample sizes have been rounded to the nearest 10. Therefore, detail may not sum to total. NOTE: A respondent is defined as a child for whom a teacher questionnaire was returned and the questionnaire had at least one response. The weighted response rates were calculated using the spring first-grade student base weight. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2012. Table 5-11 presents weighted and unweighted overall response rates for the child assessment and the parent interview in the spring first-grade data collection, by selected school characteristics. As for fall first-grade, the overall response rate is the percentage of possible assessments, interviews, or questionnaires completed, taking into account all survey stages. All schools with base-year respondents either responded to the spring data collection or became ineligible because they no longer had eligible students (base-year respondents in these schools moved to other schools). As for fall first-grade, the school response rates used in the overall rates are from the base year because children were sampled in the base year and are eligible to stay in the study regardless of school participation after the base year,"}, {"section_title": "5-20", "text": "The overall response rates are calculated as the product of the school response rate from the spring kindergarten data collection (see table 5-2 of the base-year User's Manual for those response rates) and the child assessment and parent interview response rates from the spring of first grade The overall response rate for the spring child assessment was 55.2 percent. The highest response rates were found in the Midwest (71.9 percent) and in schools in which the percentage of enrolled students of racial/ethnic minorities was 86 percent or more (67.3 percent). The lowest rates were found in the Northeast (54.5 percent) and for students in other private schools (54.4 percent). For the parent interview, the overall weighted response rate for the spring data collection was 47.8 percent. The highest response rate was in the Midwest (58.3 percent), while the lowest rates were found in the Northeast (45.6 percent) and West (46.1 percent).  1 Student had scoreable reading and/or mathematics and/or science data, or executive function scores, or student had height and/or weight measurement. 2 Parent answered all applicable items in the family structure section of the questionnaire (FSQ). 3 School characteristics (school type, region, locale, percent minority in the school) were calculated using the School Administrator Questionnaire (SAQ) responses for round 4 participants where available. When round 4 SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). Due to differences between the way prior-round SAQ/CCD/PSS data were used to generate estimates in this table and the way those data were used to calculate the composite variables (especially percent minority enrolled), estimates in this table cannot be replicated using the data file. 4 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total. 5 States in each region: Northeast: Connecticut, Massachusetts, Maine, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Iowa, Illinois, Indiana, Kansas, Michigan, Minnesota, Missouri, North Dakota, Nebraska, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Maryland, Mississippi, Louisiana, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, New Mexico, Nevada, Oregon, Utah, Washington, and Wyoming. NOTE: The weighted overall response rates were calculated using the school base weight for the school response rate component and the student base weight for the student response rate component. The counts of students by subgroups do not sum to the total because homeschooled students and students with unknown school characteristics are not included in this  teacher questionnaires in the spring first-grade data collection, by selected school characteristics. The overall response rate for the teacher-level questionnaire was 55.0 percent. Response rates were highest in the Midwest (68.7 percent) and for students in schools with enrollment of between 1 and 149 students (65.9 percent). The lowest response rates were found in the Northeast (52.4 percent) and West (51.0 percent) and"}, {"section_title": "5-21", "text": ""}, {"section_title": "5-22", "text": "for students in schools with an enrollment of at least 750 students (52.3 percent). The overall response rate for the student-level teacher questionnaire was 55.1 percent. The response rates by subgroup follow a similar pattern as those for the teacher-level questionnaire.  1 School characteristics (school type, region, locale, percent minority in the school) were calculated using the School Administrator Questionnaire (SAQ) responses for round 4 participants where available. When round 4 SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). Due to differences between the way prior-round SAQ/CCD/PSS data were used to generate estimates in this table and the way those data were used to calculate the composite variables (especially percent minority enrolled), estimates in this table cannot be replicated using the data file. 2 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total. 3 States in each region: Northeast: Connecticut, Massachusetts, Maine, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Iowa, Illinois, Indiana, Kansas, Michigan, Minnesota, Missouri, North Dakota, Nebraska, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Maryland, Mississippi, Louisiana, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, New Mexico, Nevada, Oregon, Utah, Washington, and Wyoming. NOTE: A respondent is defined as a child for whom a teacher questionnaire was returned and the questionnaire had at least one response. The weighted overall response rates were calculated using the school base weight for the school response rate component and the spring first-grade student base weight for the student response rate component. The counts of students by subgroups do not sum to the total because homeschooled students and students with unknown school characteristics are not included in this Tables 5-13 through 5-15 present response rates that reflect response across the fall and spring first-grade collections combined. These rates are referred to as longitudinal response rates."}, {"section_title": "5-23", "text": "Response rates are for cases with a response for a given component in both the fall and the spring. The denominators for the unweighted response rates in these tables include students who were part of the fall first-grade subsample who remained eligible in the spring of first grade. The weight used to compute estimates for tables 5-13 through 5-15 showing longitudinal response rates is the fall first-grade student base weight that includes the 30 percent subsampling and the mover subsampling adjustments but does not include adjustments for unknown eligibility or nonresponse. Information on the school and child characteristics comes from the first-grade data collection. If first-grade data are not available, base-year data are used. Table 5-13 presents the weighted and unweighted response rates for students who completed a child assessment in both the fall and spring first-grade data collections, and for students who had a complete parent interview in both the fall and spring first-grade data collections, by selected school 5-24 characteristics. The denominator used to calculate the unweighted child assessment longitudinal response rate is 5,748. The denominator used to calculate the unweighted parent interview longitudinal response rate is 5,776. The weighted response rate for students with assessments in both fall and spring is 85.5 percent. The highest response rates were for students in rural locales (94.6 percent), in Catholic schools (94.0 percent), in schools with enrollment of between 500 and 749 students (94.0 percent), and in the South (93.9 percent). With the exception of the \"unknown\" categories, the lowest rate was found among students in other private schools (72.1 percent). The weighted response rate for parent interviews in both fall and spring was 74.3 percent. The highest rates were found for students in Catholic schools (83.8 percent) and schools with zero to 15 percent of their students in a racial/ethnic minority group (84.9 percent). The lowest rates were for students in schools in the West (72.1 percent) and in schools with at least 86 percent of students in a racial/ethnic minority group (also 72.1 percent).  1 Student had scoreable reading or mathematics or science data, or at least one executive function score, or a height or weight measurement, in both fall and spring first grade. 2 Parent answered all applicable items in the time use section of the questionnaire (TUQ) in fall and the family structure questions (FSQ) in spring. 3 Because the School Administrator Questionnaire (SAQ) was not administered in fall first grade, school characteristics (school type, region, locale, percent minority in the school) were calculated using the SAQ responses for round 3 participants who were also round 4 participants and attending the same school in both rounds, where available. When round 4 SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). Due to differences between the way prior-round SAQ/CCD/PSS data were used to generate estimates in this table and the way those data were used to calculate the round 4 composite variables (especially percent minority enrolled), estimates in this table cannot be replicated using the data file. 4 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total. 5 States in each region: Northeast: Connecticut, Massachusetts, Maine, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Iowa, Illinois, Indiana, Kansas, Michigan, Minnesota, Missouri, North Dakota, Nebraska, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Maryland, Mississippi, Louisiana, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, New Mexico, Nevada, Oregon, Utah, Washington, and Wyoming. NOTE: The fall first-grade data collection included only 30 percent of the PSUs. The weighted response rates were calculated using the fall firstgrade student base weight. The school characteristics are the same as for the fall first-grade tables. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2011 and spring 2012. percent respectively). Among subgroups with larger numbers of sampled students, the highest response rate was for White students (79.4 percent), while the lowest response rate was for Black students (65.2 percent). 2 Parent answered all applicable items in the time use section of the questionnaire (TUQ) in fall and the family structure questions (FSQ) in spring. 3 Race/ethnicity information comes from the composite variable X_RACETH_R. Information collected from schools at the of sampling was used to code race/ethnicity for a small number of cases with missing data on X_RACETH_R. 4 Sample sizes have been rounded to the nearest 10. Therefore, detail may not sum to total. NOTE: The fall first-grade data collection included only 30 percent of the PSUs. The weighted response rates were calculated using the fall firstgrade student base weight. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2011 and spring 2012. Table 5-15 presents overall weighted and unweighted response rates for students who completed a child assessment in both the fall and spring first-grade data collections, and for students who have a complete parent interview in both the fall and spring first-grade data collections, by selected 5-28 school characteristics. The overall weighted response rate for students with assessments in both fall and spring was 53.6 percent. The highest response rates were in the Midwest (68.4 percent) and in schools with at least 86 percent of students who were racial/ethnic minorities (also 65.7 percent), while the lowest rate was for students in other private schools (43.9 percent). The overall weighted response rate for students with a complete parent interview in both fall and spring was 46.6 percent. The highest rates were in the Midwest (55.8 percent) and in Catholic schools (54.0 percent), while the lowest rates were in the West (42.3 percent), in towns (44.7 percent), and for students in schools with enrollment between 300 and 499 students (44.3 percent).  1 Student had scoreable reading or mathematics or science data, or at least one executive function score, or a height or weight measurement, in both fall and spring first grade. 2 Parent answered all applicable items in the time use section of the questionnaire (TUQ) in fall and the family structure questions (FSQ) in spring. 3 Because the School Administrator Questionnaire (SAQ) was not administered in fall first grade, school characteristics (school type, region, locale, percent minority in the school) were calculated using the SAQ responses for round 3 participants who were also round 4 participants and attending the same school in both rounds, where available. When round 4 SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). Due to differences between the way prior-round SAQ/CCD/PSS data were used to generate estimates in this table and the way those data were used to calculate the round 4 composite variables (especially percent minority enrolled), estimates in this table cannot be replicated using the data file. 4 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total. 5 States in each region: Northeast: Connecticut, Massachusetts, Maine, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Iowa, Illinois, Indiana, Kansas, Michigan, Minnesota, Missouri, North Dakota, Nebraska, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Maryland, Mississippi, Louisiana, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, New Mexico, Nevada, Oregon, Utah, Washington, and Wyoming. NOTE: The fall first-grade data collection included only 30 percent of the PSUs. The weighted overall response rate was calculated using the school base weight for the school response rate component and the student base weight for the student response rate component. The counts of students by subgroups do not sum to the total because homeschooled students and students with unknown school characteristics are not included in this  Response rates are not broken down by subgroup for the special education teacher questionnaires because of the relatively small number of students eligible for this component. The denominator for the special education teacher rates is 979. The two special education teacher questionnaires had similar response rates.  Tables 5-17 and 5-18 present response rates for the school administrator questionnaire (SAQ) included in the spring first-grade data collection. In the base year, the school sample was representative of schools educating kindergartners and kindergarten-aged children, so the base-year"}, {"section_title": "5-25", "text": ""}, {"section_title": "5-26", "text": ""}, {"section_title": "5-27", "text": ""}, {"section_title": "5-29", "text": ""}, {"section_title": "5-30", "text": "User's Manual presented response rates at the school level. After the base year, the school sample is the set of schools attended by children in the ECLS-K:2011 and is no longer nationally representative sample of schools. For this reason, response rates for the SAQ are presented only at the student level. Table 5-17 presents the weighted and unweighted response rates for the school administrator questionnaire, by selected school characteristics. They are rates for students who were not homeschooled and who are spring first-grade respondents. 5 The weighted response rate for the school administrator questionnaire was 87.9 percent. The highest response rates for this questionnaire were for students in schools with school enrollment of fewer than 150 (98.6 percent), in Catholic schools (97.7 percent), in towns (95.4 percent), and in schools with zero to 15 percent of students who were racial/ethnic minorities (97.1 percent). Aside from students in the \"Unknown\" categories, for which response rates were very low, the lowest response rates were for students in the largest schools (85.4 percent) and students in schools with at least 86 percent of students who were racial/ethnic minorities (84.6 percent).  1 School characteristics (school type, region, locale, percent minority in the school) were calculated using the School Administrator Questionnaire (SAQ) responses for round 4 participants where available. When round 4 SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). Due to differences between the way prior-round SAQ/CCD/PSS data were used to generate estimates in this table and the way those data were used to calculate the composite variables (especially percent minority enrolled), estimates in this table cannot be replicated using the data file. 2 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total. 3 States in each region: Northeast: Connecticut, Massachusetts, Maine, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Iowa, Illinois, Indiana, Kansas, Michigan, Minnesota, Missouri, North Dakota, Nebraska, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Maryland, Mississippi, Louisiana, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, New Mexico, Nevada, Oregon, Utah, Washington, and Wyoming. NOTE: A respondent is defined as an eligible student for whom the school was eligible for the school administrator questionnaire, the questionnaire was returned, and there was at least one response. The weighted response rates were calculated using the spring first-grade student base weight. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2012. Table 5-18 presents the weighted and unweighted response rates for the school administrator questionnaire, by selected student characteristics. The highest weighted response rate for the school administrator questionnaire was for White students (90.8 percent). Excluding subgroups with small numbers of sampled students, the lowest response rates were for Black (85.0 percent), Hispanic (85.3 percent), and Asian students (84.9 percent).  1 Race/ethnicity information comes from the composite variable X_RACETH_R. Information collected from schools at the of sampling was used to code race/ethnicity for a small number of cases with missing data on X_RACETH_R. 2 Sample sizes have been rounded to the nearest 10. Therefore, detail may not sum to total. NOTE: A respondent is defined as an eligible student for whom the school was eligible for the school administrator questionnaire, the questionnaire was returned and there was at least one response. The weighted response rates were calculated using the spring first-grade student base weight. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2012. Table 5-19 shows the overall response rates for the school administrator questionnaire. The overall weighted response rate was 55.1 percent. The highest response rates were for students in the Midwest (68.6 percent) and in schools with enrollment between 1 and 149 students (67.6 percent), while the lowest rates were found in the Northeast (52.6 percent), in the West (51.3 percent), and for students in schools with the largest student enrollment (51.1 percent). 1 School characteristics (school type, region, locale, percent minority in the school) were calculated using the School Administrator Questionnaire (SAQ) responses for round 4 participants where available. When round 4 SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). Due to differences between the way prior-round SAQ/CCD/PSS data were used to generate estimates in this table and the way those data were used to calculate the composite variables (especially percent minority enrolled), estimates in this table cannot be replicated using the data file. 2 To maintain confidentiality, the number of respondents is reported to the nearest 10 for census region and, therefore, may not sum to the total. 3 States in each region: Northeast: Connecticut, Massachusetts, Maine, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Iowa, Illinois, Indiana, Kansas, Michigan, Minnesota, Missouri, North Dakota, Nebraska, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Maryland, Mississippi, Louisiana, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, New Mexico, Nevada, Oregon, Utah, Washington, and Wyoming. NOTE: A respondent is defined as an eligible student for whom the school was eligible for the school administrator questionnaire, the questionnaire was returned and there was at least one response. The weighted overall response rates were calculated using the school base weight for the school response rate component and the spring first-grade student base weight for the student response rate component.. The counts of students by subgroups do not sum to the total because homeschooled students and students with unknown school characteristics are not included in this produced from the parent interview."}, {"section_title": "5-32", "text": ""}, {"section_title": "5-33", "text": ""}, {"section_title": "5-34", "text": ""}, {"section_title": "Effect of Nonresponse on Parent Interview Data", "text": "Estimates weighted by the nonresponse-adjusted weights are compared with estimates weighted by the base weight (which are referred to as unadjusted estimates). The base weight only takes into account the selection probabilities of the sampling units and the subsampling of movers to be followed. The weights with nonresponse adjustments are the standard weights used to analyze ECLS-K:2011 data. The adjusted weight used in this analysis is W4CS4P_40, which is adjusted for nonresponse to the spring parent interview. For a discussion of how the weights were constructed, see chapter 4. Large differences between the adjusted and unadjusted weights indicate the potential for bias in the estimates. If the differences are small, then the chance for substantial nonresponse bias is reduced. Larger differences could be indicative of substantial nonresponse bias. However, if characteristics associated with the differences are used in the nonresponse adjustment process, the likelihood that the weighted estimates are biased as a result of nonresponse would be lower. This method of examining nonresponse bias provides a look at the need for the nonresponse adjustment and its effectiveness. Table 5-20 shows estimates of selected items from the parent interview. The differences between the unadjusted and adjusted estimates are very small, and thus, the potential for substantial nonresponse bias seems unlikely.   Table 5-21 shows the differences between unweighted and weighted estimates, and between estimates produced using base weights (unadjusted estimates) and estimates produced using adjusted weights. The differences are shown in absolute value and as a percent. For example, for the differences between unweighted and unadjusted estimates, the difference is the absolute value of unweighted estimate minus the unadjusted estimate, and the percent is the difference divided by the unweighted estimate. In general, the percent differences between unweighted and unadjusted estimates, and between unadjusted and adjusted estimates, are very small for the mean estimates (less than 1 percent). For the proportion estimates, the differences are larger (average is 10 percent), but this is mostly due to variables with a small proportion of cases with uncommon characteristics (for example, students who went to school in a town, compared with those who went to school in a city or suburb). This shows that there is some potential for nonresponse bias in the unweighted parent data, but the weights used to produce estimates were adjusted for nonresponse and, thus, reduce that potential bias. "}, {"section_title": "5-36", "text": ""}, {"section_title": "5-37", "text": ""}, {"section_title": "5.3.2", "text": ""}, {"section_title": "Effect of Nonresponse on Characteristics from the Base Year", "text": "In this section, the effect of nonresponse is explored by comparing estimates of selected base-year characteristics between base-year respondents and spring first-grade respondents. 6 The estimates are unadjusted estimates (i.e., they are weighted by the base weights). Base-year estimates are weighted by the base-year base weight that takes into account only the selection probabilities of the sampling units. Spring first-grade estimates are weighted by the spring-first grade base weight that takes into account the selection probabilities and the subsampling of movers to be followed. an average of 1.61 percent. There are two characteristics with differences greater than 5 percent. These are estimates for the percent of the sample that is Asian and the percent of the sample that is Black. As shown in table 5-8, response rates for these two groups of children, particularly for the parent interview, were relatively lower in the spring of first grade than they were for children in other racial/ethnic groups. Since race/ethnicity is one of the characteristics used to construct nonresponse cells for nonresponse adjustments, any potential bias in would be reduced in estimates produced using weights adjusted for nonresponse. 6 A base-year respondent has child data (scoreable assessment data or height or weight measurements or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. A spring first-grade respondent has child data (scoreable assessment data or height or weight measurements or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from the spring first-grade round of data collection 5-41 "}, {"section_title": "DATA PREPARATION", "text": "In the first-grade rounds, two types of data collection instruments were again used for the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011): computer-assisted interviews and assessments (CAI) and self-administered paper forms (hard copy). As in the base year, once data were collected, they were reviewed and prepared for release to analysts. The approaches used to prepare the data differed with the mode of data collection. The direct child assessments and parent interviews were conducted using CAI. Editing specifications were built into the CAI programs used by assessors or interviewers to collect these data. The teacher and school administrator hard-copy questionnaires were self-administered. When these hard-copy questionnaires were returned to the data collector's home office, staff recorded the receipt of these forms into a project-specific form tracking system. Data from the hard-copy forms were then captured by scanning the completed forms. Before scanning, coders reviewed the questionnaires to ensure that responses were legible and had been written in appropriate response fields for transfer into an electronic format. Coding of open-ended 1 \"other, specify\" text responses into existing or new categories was conducted after the data were scanned and reviewed for range and logical consistency. The following sections briefly describe the data preparation activities for both modes of data collection, focusing on the first-grade activities. More detailed information on all of these data preparation activities can be found in the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), User's Manual for the ECLS-K:2011 Kindergarten Data File andElectronic Codebook, Public Version (NCES 2015-074) (Tourangeau et al. 2015)."}, {"section_title": "Coding Text Responses", "text": "Additional coding was required for some of the items asked in the CAI parent interview once the data had been collected. These items included \"other, specify\" text responses and responses to questions asking about parent or guardian occupation, which interviewers had entered into the CAI system verbatim. Review of \"other, specify\" items. As in the base (i.e., kindergarten) year, trained data preparation staff reviewed respondents' verbatim \"other, specify\" text responses. There was a small number of items in the parent interview for which additional categories were added to categorize \"other, specify\" text responses that occurred with sufficient frequency. For example, a sufficient number of parents provided an \"other, specify\" response to the question about diagnoses children received as a result of having their hearing evaluated by a professional, reporting that no problem was found. A new response category was added to classify these responses. Text responses that did not fit into any preexisting category and were not common enough to be coded into new categories were left coded as \"other\" in the data. New categories added as a result of this review of \"other, specify\" responses are noted as such in exhibit A-1. There were no \"other, specify\" items in the child assessments. Text responses that could not be coded using the autocoding system were coded manually using a customized computer program designed for coding occupations. The customized coding computer program provided a text string with occupation information to coders, who then determined and assigned the most appropriate occupation code by reviewing occupation text descriptions in the coding manuals. In addition to the occupation text strings, the coders used other information collected from respondents such as main duties at work, highest level of education, and name of the employer to ensure that the occupation code assigned to each case was appropriate. Over half the occupations (52.6 percent) were manually coded. Every manually coded occupation text response was coded at least twice. Two coders assigned codes independently, without knowledge of each other's codes (i.e., using a double-blind coding process). A coding supervisor adjudicated all reported occupations for which the codes assigned independently by each coder differed. Of all the occupations that were assigned a code, 28.6 percent (2,571) required adjudication, either because the autocode and manually assigned code differed (for the autocoded occupations) or because the two manually assigned codes differed (for the manually coded occupations). Of the 4,269 reported occupations that were autocoded, 616 occupations (14.4 percent) required adjudication because the coder disagreed with the autocoding. Of the 4,734 reported occupations that were manually coded, 1,955 (41.3 percent) required adjudication because the two human coders disagreed. Following the adjudication process, the coding supervisor conducted a review of all occupation codes that were assigned manually. There were an additional 148 manually coded occupations (1.6 percent of all codes) for which the two coders assigned the same code, but the supervisor disagreed with the original manually assigned code and assigned a new occupation code. Adjudication rates were somewhat higher in first grade than in the base year because more coding staff were assigned to the occupation coding activity; some of the staff, though trained on the coding scheme and rules, were new to the task. The occupation coding supervisor for first grade participated in base year occupation coding as well and was familiar and experienced with the NCES coding scheme. When the supervisor disagreed with the \"same code\" assigned by the two coders, the case 6-3 was subject to additional examination, and together the supervisor and coders considered the merits of the proposed codes before a final code was assigned."}, {"section_title": "Household Roster Review", "text": "The fall first-grade parent interview was much shorter than the parent interview included in other data collection rounds and did not include a household roster in which information on household composition was collected. Therefore no household roster review was required for that round of data collection. The spring first-grade parent interview did include a household roster. Following protocols established during the base year, three general types of checks were run on the spring household roster information to identify missing or inaccurate information that would require editing. \uf06e First, the relationship of an individual living in the household to the study child was compared to the individual's listed age and sex. Inconsistencies such as a male mother or a biological mother over age 65 were examined further and corrected when the interview contained sufficient information to support a change fixing the inconsistency. \uf06e Second, while it is possible to have more than one mother or more than one father in a household, households with more than one mother or more than one father were reviewed to ensure they were not cases of data entry error. Corrections were made whenever clear errors were identified and a clear resolution existed. \uf06e Third, the relationship of an individual in the household to both the study child and the respondent was examined, as there were cases in which the relationship of an individual to the study child conflicted with his or her status as the spouse/partner of the respondent. For example, in a household containing a child's grandparents but not his or her parents, the grandmother may be designated the \"mother\" figure, and the grandfather thus becomes the \"father\" figure for the purposes of some questions in the interview by virtue of his marriage to the grandmother. In this example, these cases would have been examined but left unchanged. Both the original-and correct (grandfather)-relationship data and the new \"parent-figure\" designation (father) that had been constructed were retained. In other situations discrepancies in the parent figure relationships to the child indicated an error, and the data were edited. For example, in a household containing two mothers, if a review of the audio recording from the interview indicated the relationship of the second mother was documented incorrectly by the interviewer-that the second mother was not a mother to the focal child-in this example, the relationship of the second mother would have been edited (corrected). A flag on the data file (X4EDIT) identifies cases that were reviewed or edited for any of the reasons described above; the flag is set to 1 if the case was identified for review for any of these 6-4 household roster checks. Note that a code of 1 does not necessarily indicate that the data were changed; if the data were reviewed and found to be as reported by the respondent or there was no clear error to be fixed, the reviewed data were left as is. There were just under 1,500 cases (12 percent) identified for household roster review in spring first grade."}, {"section_title": "Partially Complete Parent Interviews", "text": "Parents did not have to complete an entire interview for the data collected from them to be included on the data file. However, parent interviews did have to be completed through a specified section of the interview for those data to be included. For the abbreviated parent interview in the fall first-grade round, the respondent had to answer questions in the section on time use (TUQ) for the parent interview data to be included on the data file. There were eight partially completed fall parent interviews for which the respondent answered at least some questions in the TUQ section but did not finish the entire interview. For the spring first-grade round, the respondent had to answer questions at least through the section on family structure (FSQ). There were 655 partially completed spring parent interviews for which the respondent answered at least some questions through the FSQ section but did not complete the entire interview. 2 All data derived from questions asked after the interview termination point for these partially completed interviews are set to -9 for \"not ascertained.\""}, {"section_title": "Receipt, Coding, and Editing of Hard-Copy Questionnaires", "text": ""}, {"section_title": "Receipt Control", "text": "Receipt control was managed in the same manner for first grade as in the base year. Refer to the base-year User's Manual for details."}, {"section_title": "6.2.2", "text": ""}, {"section_title": "Scanning of Hard-Copy Questionnaires", "text": "Scanning of hard-copy questionnaires was managed in the same manner for first grade as in the base year. Refer to the base-year User's Manual for details."}, {"section_title": "Coding for Hard-Copy Questionnaires", "text": "Similar to the process described for the parent interview and identical to base-year practices, \"other, specify\" text responses were reviewed by the data editing staff. There was a small number of items in the hard-copy questionnaires for which additional categories were added to categorize \"other, specify\" text responses that occurred with sufficient frequency. For example, a sufficient number of teachers provided an \"other, specify\" response to the question about why a child had fallen behind in school work, reporting that the child was easily distracted or lacked focus or attention. A new response category was added to classify these responses. Text responses that did not fit into any preexisting category and were not common enough to be coded into new categories were left coded as \"other\" in the data."}, {"section_title": "Data Editing", "text": "The data editing process for hard-copy questionnaires was managed in the same manner for first grade as in the base year. Refer to the base-year User's Manual for details. This chapter focuses primarily on the composite variables that were created from information obtained during the first-grade data collections. Most of the variables have been computed in the same way as those that were created using information collected in the base year. However, a small number of them differs slightly either because the same exact information available in the base year was not available in first grade or because it was determined there was a better way to compute the composite after release of the base-year data file. These differences are noted in the descriptions of the variables. To the extent feasible, the composite variables have also been computed in the same way as those created for the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K). This results in consistency between the two studies and facilitates comparisons between the two cohorts. However, some composites were created differently in the ECLS-K:2011 than in the ECLS-K. Documentation for both studies should be consulted before conducting cross-cohort analyses using composites. As discussed in Appendix B, the public-use file is derived from the restricted-use file and is identical in format. However, masking techniques such as re-categorization and top-and bottom-coding have been applied to some data to make them suitable for public release. As a result of masking, some variables in the public-use file may not contain the exact same categories and values described in this chapter. Please see Appendix B for information on which variables are modified in the public-use file and see the public-use codebook for the exact categories and values provided in the public data. The chapter is divided into several sections. Sections 7.1 through 7.4 focus on the naming conventions of the study and describe identification variables, missing values, and data flags. Section 7.5 provides details about the creation of composite variables, and section 7.6 focuses on the methodological variables. Section 7.7 discusses variables used to identify children who changed teachers between the fall and spring data collections. Finally, section 7.8 discusses variables about summer school and vacation."}, {"section_title": "Variable Naming Conventions", "text": "Variables are named according to the data source (e.g., parent interview, teacher questionnaire) and the data collection round to which they pertain. With the exception of the identification variables described in section 7. Some variable names end with a suffix denoting a particular feature of the variable of which users should be aware. The suffix \"_R\" indicates that the variable has been updated or revised since its release in the base-year data file. The suffix of \"2\" is used for composites that are based on new questions or have new categories. The suffix \"_I\" indicates that missing data for the variable have been imputed, or a composite variable is based on imputed source variables. Imputation is discussed in section 7.5.2.9."}, {"section_title": "Identification Variables", "text": "The kindergarten-first-grade data file contains a child identification (ID) variable (CHILDID) that uniquely identifies each record. For children who have a twin who also participated in the study, TWIN_ID is the child identification number of the focal child's twin. The file also contains an ID for the parent (PARENTID). The parent ID number (PARENTID) is the same number as the child ID. Unlike in the ECLS-K, CHILDID is randomly generated, so it cannot be used to group "}, {"section_title": "7-4", "text": "Children's general classroom teachers are identified in the restricted-use file with the ID variables T3_ID, the fall 2011 teacher identification number, and T4_ID, the spring 2012 teacher identification number. In first grade, children were expected to have a single general classroom teacher for all subjects, so each child was linked to only one classroom teacher at each round. If a teacher had more than one study child in his or her classroom, each child was assigned the same classroom teacher ID. For children in the fall 2011 subsample who had the same teacher for the entire school year, T3_ID and T4_ID are identical. For children who had an Individualized Education Program (IEP) on record with the school that was identified as part of the process for determining accommodations for the child assessment, D4T_ID provides the identification number for their special education teacher or related service provider. For some students, the general classroom teacher was also the student's special education teacher. However, D4T_ID does not match T4_ID for these students. The ID variables S3_ID and S4_ID indicate the school the child attended at the time of the fall 2011 and spring 2012 data collections, respectively. As with the general classroom teacher ID variables, if a school had more than one study child in it, each child was assigned the same school ID, and for children in the fall 2011 subsample who attended the same school for the entire school year, S3_ID and S4_ID are identical. Each child has a school identification number for each round; however, not all identification numbers represent specific schools. Instead, certain identification numbers have been designated to identify children who were homeschoolers (9100), moved to a nonsampled county (9997), were unlocatable (9995), moved outside the United States (9993), were movers who were not subsampled to be followed into their new schools (9998), were deceased (9994), or whose parents asked for them to be removed from the data (9999). If a child does not have an IEP on record with the school that was identified as part of the process for determining accommodations for the child assessment, there is no special education teacher associated with that child, and D4T_ID is missing. Also, in most cases, if a child does have an IEP identified as part of the process for determining accommodations for the child assessment and, therefore, a special education teacher associated with him or her, there is an ID provided in D4T_ID whether or not the special education teacher responded to the spring 2012 special education teacher questionnaires. There could be missing special education data for the child's teacher-level or child-level questionnaire (for example, if the special education teacher replied to only one of the two questionnaires or did not fully complete the questionnaires). If a special education teacher did not complete a teacher-level questionnaire, completed a child-level questionnaire for one child, and did not complete another childlevel questionnaire for a child to whom the teacher was also linked, both children would have the same 7-5 D4T_ID. However, only the child for whom the teacher completed the child-level questionnaire would have data for those variables. It is left to users to determine how they would like to set \"not applicable\" versus \"not ascertained\" codes when data for D4T_ID are missing."}, {"section_title": "Missing Values", "text": "Variables on the ECLS-K:2011 data file use a standard scheme for identifying missing data. The -1 (not applicable) code is used to indicate that a respondent did not answer a question due to skip instructions within the instrument. In the parent interview, \"not applicable\" is coded for questions that were not asked of the respondent because a previous answer made the question inapplicable to the particular respondent. For example, a question about a child's sibling's age is not asked when the respondent has indicated that the child has no siblings. For the teacher and school administrator selfadministered instruments, \"not applicable\" is coded for questions that the respondent left blank because the written directions instructed him or her to skip the question due to a certain response on a previous question that made the question inapplicable to the particular respondent. One example of the use of \"not applicable\" is found in the spring 2012 school administrator questionnaire version A (SAQ-A) question In order to protect the confidentiality of study participants, some data are suppressed in the public-use data file. The code -2 indicates the suppression of data for confidentiality. The suppression code -4 is used in rare instances in which there was a problem in the administration of an item that led to a high proportion of cases having missing data on the affected item, and the data that were collected were not useful. Although the administration error typically did not affect all cases, the -4 missing data code is assigned to all cases, whether or not the specific case had a response or was missing data due to the error. Information about a number of school characteristics that was collected in the SAQ-A (the school administrator questionnaire given to schools that were new to the study or had not previously completed an SAQ) was not collected in the SAQ-B (the school administrator questionnaire given to schools that had previously completed an SAQ). This data collection approach reduced respondent burden by eliminating questions about school characteristics that were unlikely to change in 1 year, such as public/private control and the grade levels taught at the school. The code -5 is a special \"not applicable\" code indicating that a child does not have a value for the given school characteristic variable because it was not included in the abbreviated SAQ-B. The -7 (refused) code indicates that the respondent specifically told the interviewer that he or she would not answer the question. This, along with the -8 (don't know) code and the -9 (not ascertained) code, indicate item nonresponse. The -7 (refused) code is not used in the school or teacher data. The -8 (don't know) code indicates that the respondent specifically told the interviewer that he or she did not know the answer to the question. The -8 (don't know) code is not used in the school or 7-7 teacher data. For questions where \"don't know\" is one of the options explicitly provided, a -8 is not coded for those who choose this option; instead the \"don't know\" response is coded as indicated in the value label information for the variable associated with that question. For the school and teacher self-administered questionnaires, -9 (not ascertained) indicates item nonresponse. For data that are not collected using the self-administered questionnaires (e.g., direct assessment scores), a -9 means that a value was not ascertained or could not be calculated due to nonresponse. The -9 (not ascertained) code is also used in the parent interview data when the interview ended before it was finished. In these cases, the code of -9 is used for all variables associated with interview questions that came after the point at which the parent ended the interview. One exception to this coding scheme is the pointer variables. 3 The -9 code was also used in the parent interview for questions that were edited 4 or inadvertently skipped in computer-assisted interviewing (CAI) programming. After editing, for complete interviews, the data for all questions that should have been asked but were not are coded as -9 (not ascertained), while the data for other skipped questions are coded as -1 (not applicable); codes -7 and -8 are used only when respondents state a response of \"don't know\" or \"refused,\" and not as a result of editing or inadvertently skipping a question as a result of CAI programming. Missing values (-1, -7, -8, or -9) in questions that allow for more than one response are coded the same for all coding categories used for the question. For example, in the spring 2012 parent interview, if the question about languages spoken in the home (PLQ040) has the answer of -8 (don't know), then all the languages in the same question (e.g., Arabic, French, Korean), in addition to any categories added for coding responses that were not in the computer-assisted personal interview (CAPI) questionnaire (e.g., sign language), are also coded as -8 (don't know). The \"system missing\" code appears as a blank when viewing codebook frequencies and in the ASCII data file. System missing codes (blanks) in the base-year data file indicate that data for an entire instrument or assessment are missing due to unit nonresponse. For example, when a child's parent does not participate in the parent interview, all of the data associated with questions from the parent interview are coded \"system missing\" (blank) for that child. These blanks may be converted to another value when the data are extracted into specific processing packages. For instance, SAS converts these blanks into periods (\".\") for numeric variables. Codes used to identify missing values (-1, -7, -8, -9,  Composite variables may be derived using data from one or more instrument(s) in one round of data collection, from instrument data across multiple rounds, and from both instrument data and data from administrative records in one or more rounds. If a particular composite is inapplicable for a certain case, as school composite variables are for children who are homeschooled, the variable is given a value of -1 (not applicable) for that case. In instances where a variable is applicable but complete information 7-9 required to construct the composite is not available, the composite is given a value of -9 (not ascertained). The -7 (refused) code is not used for any of the composites except for the height and weight composites. The -8 (don't know) code is not used for any of the composites. There is variation in the use of system missing for composite variables. Some child demographic variables (date of birth, sex, and race/ethnicity) are considered applicable to all 18,174 children who participated in the base year and are not assigned a value of system missing for any case. For composite variables using data from both a survey instrument and other administrative or school data sources, only nonparticipants in a given round of data collection are assigned values of system missing. For composite variables using data from only one instrument, (e.g., X4LANGST, primary household language, is derived from the spring 2012 parent interview), a value of system missing is assigned if the instrument on which they are based was not completed; if the instrument was completed and an item used in the composite derivation was missing, the composite is assigned a value of -9 as described above. Flags (X3RDGFLG, X4RDGFLG, X3MTHFLG, X4MTHFLG,   X3SCIFLG, X4SCIFLG, X3NRFLG, X4NRFLG, X3DCCSFLG, X4DCCSFLG,   X3HGTFLG, X4HGTFLG, X3WGTFLG, X4WGTFLG, X3FLSCRN, X4FLSCRN,"}, {"section_title": "Data Flags", "text": ""}, {"section_title": "Child Assessment", "text": ""}, {"section_title": "X3ASMTST, X4ASMTST, X3EXDIS, X4EXDIS)", "text": "Fourteen flags indicate the presence or absence of child assessment data. X3RDGFLG and X4RDGFLG denote whether a child had scoreable English or Spanish reading assessment data in fall 2011 and spring 2012, respectively; X3MTHFLG and X4MTHFLG denote whether a child had scoreable English or Spanish mathematics assessment data in fall 2011 and spring 2012, respectively; X3SCIFLG and X4SCIFLG denote whether a child had scoreable science assessment data in fall 2011 and spring 2012, respectively. X3NRFLG and X4NRFLG indicate the presence of numbers reversed scores. If a child answered fewer than 10 questions in any direct cognitive assessment domain (reading, mathematics, or science), the assessment was not considered scoreable. Only items actually 7-10 attempted by the child counted toward the scoreability threshold. 5 A flag value of 1 indicates that the child responded to 10 or more questions in the assessment for that domain, and thus has the associated scores. A flag value of 0 indicates the child had fewer than 10 responses and does not have a score. For the Numbers Reversed and DCCS assessments, a child could receive a score as long as the child started the assessment task and answered at least one test question following the practice items in order to have a W-ability score (for Numbers Reversed) or at least one shape game item in order to have a post-switch score (for DCCS). Flags for each of the scores are coded 1 if the child has a W-ability score (for Numbers Reversed) or post-switch score (for DCCS), coded 0 if the child participated in the child assessment but does not have a score, and set to system missing if the child did not participate in the child assessment. The child's assessment status for the fall of 2011 and spring of 2012 is indicated by the composites X3ASMTST and X4ASMTST, respectively. The valid values include 1 for children who have assessment data in the data file, 6 2 for those children who were excluded due to disability (and, therefore, do not have assessment data in the data file), and 3 for children who do not have assessment data in the data file and were not excluded due to disability. Note that those excluded due to disability (code 2) are considered to be participants in the data collection round. In addition, two composite variables use FMS data to indicate whether the child was excluded from the assessment due to a disability: X3EXDIS and X4EXDIS. Study team leaders obtained information from school staff in the fall of 2011 and spring of 2012 about whether a child had an IEP on file and if any information in a child's IEP indicated that he or she would need Braille, large print, or sign language. It was also determined whether the IEP specifically prohibited the child from participating in standardized assessments such as those conducted in the ECLS-K:2011. If so, the child was not assessed, and XnEXDIS was coded 1 (child was excluded from the assessment due to a disability). Otherwise, XnEXDIS was coded 0 (child was not excluded from the assessment due to a disability). Students could have been excluded from taking the assessment for other reasons (e.g., lack of parental consent); these children are also coded 0 on XnEXDIS. The number of cases with system missing values varies across the four occurrences of XnEXDIS. This reflects the sample for each round. The cases that are system missing on X1EXDIS are cases that were added to the sample in the spring of the base year and thus were not members of the sample in round 1. The cases that are system missing on X3EXDIS are those that were not selected for the fall subsample for round 3. There are no cases coded system missing on these variables in rounds 2 and 4."}, {"section_title": "Parent Data Flags (X3PARDAT, X4PARDAT, X4EDIT, X3BRKFNL, X4BRKFNL)", "text": "There are two flags that describe the presence of parent interview data. X3PARDAT is coded as 1 if there was a fully completed or partially completed interview in fall 2011. A partially completed parent interview in fall 2011 was one that ended before all applicable questions were answered, but that had answers to questions through section TUQ (time use). 7 X4PARDAT is coded as 1 if there was a fully completed or partially completed interview in spring 2012. A partially completed interview in spring 2012 was one that ended before all applicable questions were answered, but that had answers to questions through section FSQ (family structure). 8 In addition, the flag X4EDIT indicates whether, for a given case, household matrix data were reviewed or edited. It is coded as 1 if a parent interview household matrix was edited (e.g., if an age of a household member was reported incorrectly and had to be updated, or a person who was added to the household in error needed to be deleted from the household) or reviewed for editing even if no data were changed (e.g., if there were data that suggested a possible problem, but after examining the case the data were left as they were reported). This flag is 7 A case that did not complete the entire parent interview had to complete section TUQ to be counted as a partial complete in fall 2011. The TUQ section would be considered complete with one question answered if TUQ040 was not greater than or equal to 1 (the child was not away from home for a least a week during the summer). If TUQ040 was greater than or equal to 1, TUQ060 also had to be answered. If TUQ060=91 (some other place), TUQ060OS had to be answered. 8 A case that did not complete the entire parent interview had to complete all of section FSQ that was applicable to it to be counted as a partial complete in spring 2012. The FSQ section was considered complete if the country of origin questions FSQ212, FSQ212OS, and FSQ213 (used to create country of origin variables P4PARCT1, P4PARCT2, P4PAREM1, and P4PAREM2 for parent 1 and parent 2) that were applicable were answered in spring 2012 or, if nonmissing data were present for country of origin in spring 2010 (and thus did not need to be asked again), FSQ200 (P4CURMAR) was answered in spring 2012. If a case had missing data for country of origin, missing data for the age when the person moved to the United States, or if the question about age was not asked in spring 2011 because the person lived in a U. S. territory, the country of origin questions were asked in both spring 2011 and spring 2012."}, {"section_title": "7-12", "text": "included to make users aware that data cleaning or review of household matrix data was necessary for a particular case. If something about the household composition or characteristics of the household members seems unusual (e.g., the child is identified as having a 34-year-old brother in the household), and this flag is set to 1, this is an indication that the unusual data were reviewed and left as is or edited to appear as they do in the data file. The composite variables X3BRKFNL and X4BRKFNL indicate a final breakoff from the round 3 and round 4 parent interviews, respectively. These composites identify the variable associated with the last question answered by a parent who decided to terminate an interview. The breakoff point is provided only for those parent interviews with a status of partially complete. Cases for which a parent completed the interview have a value of -1, indicating that the case was not a breakoff."}, {"section_title": "Teacher Flags (X3TQCDAT, X4TQC1DAT, X4TQCKDAT, X4TQT1DAT, X4TQTKDAT, X4SETQA, X4SETQC)", "text": "Two types of data were collected from teachers using two different questionnaires, a teacherclassroom-level questionnaire and a child-level questionnaire. The first type of data, teacher and classroom data, were collected in the spring 2012 teacher-level questionnaire and include information about the teacher's background and topics such as instructional level and time spent teaching different subjects, classroom characteristics, instructional materials used in the classroom, homework assignments, and criteria used to evaluate children's progress. One teacher-level questionnaire was completed by each teacher linked to at least one ECLS-K:2011 child, and the data from that questionnaire have been linked to every ECLS-K:2011 child in his or her class. The second type of data, which pertain to an individual study child, were collected from the teacher in the child-level questionnaire. Teachers were asked to complete one child-level questionnaire for each sampled child in his or her class in fall 2011 and spring"}, {"section_title": "2012.", "text": "The data file contains flag variables that can be used to determine whether data were obtained from a teacher. 9 There are separate flag variables corresponding to each of the teacher questionnaires (teacher-level and child-level) given to the specific teacher in the fall and spring data collections (X4TQT1DAT and X4TQTKDAT for the teacher-level questionnaire; X3TQCDAT, X4TQC1DAT, and X4TQCKDAT for the child-level questionnaire). By the second year of the study, most children were in the first grade. For children in the first grade, teachers were given questionnaires specific to first grade, and the flags indicating the presence or absence of data from these questionnaires are X4TQT1DAT and X4TQC1DAT. For children who were still in kindergarten in the second year of the study, their teachers were given questionnaires specific to kindergarten, and the flags indicating the presence or absence of data from these questionnaires are X4TQTKDAT and X4TQCKDAT. The childlevel questionnaire in the fall of 2011 was the same for all children, regardless of their grade; therefore, only one child-level teacher questionnaire data flag was created for the fall (X3TQCDAT). There are six children who were enrolled in kindergarten in spring 2012, but were in a mixed-grade classroom with first-graders; for these children the teachers completed the teacher-level questionnaire for first grade and the child-level questionnaire for kindergarten. Two flags indicate the presence of data from each of the two special education teacher questionnaires for spring 2012 (X4SETQA for the teacher-level questionnaire; X4SETQC for the childlevel questionnaire). Cases linked to a special education teacher who did not complete a questionnaire and cases that were not linked to a special education teacher have a value of 0 on these flags. Users interested in information about whether special education teacher questionnaires were requested, regardless of whether special education questionnaires were completed in the spring of 2012, can use the composite variable X4SPECS, which is based on information from the FMS rather than the special education questionnaires. X4SPECS is described further below in section 7.5.1.12."}, {"section_title": "School Administrator Data Flag (X4INSAQ)", "text": "There is a flag for the school administrator questionnaire (X4INSAQ) that is coded 1 if there are data from the spring 2012 school administrator questionnaire (SAQ) and 0 if there are no data from the SAQ."}, {"section_title": "Other Child Status Flags (X3DEST, X4DEST, X3FALLSMP)", "text": "Three additional child status flags are included in the data file. The variable X3DEST is nonmissing for respondents in the fall round and indicates whether the child was in a destination school in 7-14 the fall of 2011. Destination schools are schools for which it was determined that at least four ECLS-K:2011 children moved into them; this typically happened when children attended a school that ended with a particular grade (e.g., a school that only provided education through kindergarten) or a school closed. This variable is 1 if the school a child attended was identified as a destination school; otherwise, it is 0. It is set to system missing if the child was in the fall 2011 subsample, but was not a fall 2011 participant, or if the child was not in the fall 2011 subsample. X4DEST is nonmissing for respondents in the spring round and is 1 if the child attended a destination school in the spring of 2012, and 0 otherwise. The identification variable X3FALLSMP indicates whether a child was selected to participate in the round 3 fall subsample. A value of 1 indicates the child was selected and either participated in the fall 2011 child assessment or had a parent complete the fall parent interview, while 2 indicates the child was selected but does not have a complete child assessment or parent interview. A value of 3 indicates the child was not selected for the fall subsample."}, {"section_title": "Composite Variables", "text": "To facilitate analysis of the survey data, composite variables were derived and included in the data file. This section identifies the source variables and provides other details for the composite variables. Most composite variables were created using two or more variables that are also available in the data file, each of which is named in the text that explains the composite variable. Other composites, for example, X_CHSEX_R, were created using data from the Field Management System (FMS) and the sampling frame, which are not available in the data file. Note that some of these variables have been updated or revised since their release on the base-year data file. Such variables have an \"_R\" suffix in their name."}, {"section_title": "Child Composite Variables", "text": "There are many child-level composite variables on the child catalog. The nonassessment variables are described in further detail here. The child-level composites for the direct and indirect child assessment are described in chapter 3. was within the field period. If the assessment date fell outside the field period, the modal assessment date for the child's school was used to set the composite and was retained for the data file. 11 The kindergarten-first grade data file also includes age at assessment variables for the baseyear (X1KAGE_R, X2KAGE_R). These are revised versions of the age at assessment variables that were in the kindergarten file (X1KAGE, X2KAGE). The X1KAGE and X2KAGE variables in the kindergarten file were intended to be an approximate age at assessment and were calculated by dividing the total 10 X_DOBDD and X_DOBDD_R indicate the child's exact day of birth. These are administrative variables that are not included in the K-1 longitudinal data file for issues related to confidentiality. 11 Some assessments that were partially but not entirely completed during the field period were assigned a final status after the end of the data collection round. Thus, assessment dates after the end of the field period reflected the timing of the assignment of the final disposition, not the actual date of assessment. These cases were adjusted so that the assessment date reflects the modal date for the school."}, {"section_title": "7-16", "text": "number of days (between the child's birth date and the assessment date) by 30 to calculate the child's age at assessment in months. The revised variables for age at assessment in kindergarten are based on the number of days in each month and are adjusted for leap years."}, {"section_title": "Child's Sex (X_CHSEX_R)", "text": "Information about child's sex was collected from schools at the time of sampling and stored in the FMS, collected from parents in the fall kindergarten parent interview, confirmed by parents in the spring kindergarten parent interview, and asked again in the fall 2011 or spring 2012 interviews if parent interview items about the child's sex were missing due to unit or item nonresponse. The composite variable indicating the child's sex was derived using data from (P4CHSEX, P3CHSEX, or X_CHSEX, the composite variable from the base year which includes data from the base-year parent interviews and FMS) with an order of preference for which source should be used. Spring 2012 data for the child's sex were given priority for creating the composite, followed by the fall 2011 data. In creating the composite, the spring 2012 data were given priority over other values because they were collected in the most recent interview and any values that were missing from the parent interview due to unit or item nonresponse had the potential to be updated in spring 2012. If there had not been a parent interview prior to fall 2011 or spring 2012, the value from the most recent interview in which the child's sex was collected was used. If the data about the child's sex were not collected in those rounds because information about child sex was collected in the base year, then the composite from the base year was used. This information was updated for a small number of children based on information collected from parents in the fall 2011 or spring 2012 parent interviews. responses about the child's race were missing. Parent interview responses about the races of the child's biological parents were not used in the creation of child race composite variables. Race/ethnicity information was updated in these composite variables for a small number of children based on information collected from parents in the spring 2012 parent interviews. Parents were asked about the child's ethnicity in the spring of 2012 if ethnicity in the parent interview items for the child were missing due to unit or item nonresponse. Specifically, parents were asked whether or not their child was Hispanic. Parents were also asked about the child's race in spring 2012 only if parent interview race data for the child were missing. Parents were asked to indicate in which of five race categories (White, Black or African American, Asian, Native Hawaiian or other Pacific Islander, American Indian or Alaska Native) their child belonged, and they were allowed to indicate more than one. From these responses, a series of five dichotomous race variables were created that indicate separately whether the child belonged to each of the five specified race groups. In addition, one additional dichotomous variable was created to identify those who had indicated that their child belonged to more than one race category. 12 The seven dichotomous ethnicity and race variables (X_HISP_R, X_AMINAN_R, X_ASIAN_R, X_HAWPI_R, X_BLACK_R, X_WHITE_R, X_MULTR_R) were created using parent data from spring 2012, or if those data were not asked in spring 2012 because they were asked in a previous round of the study, the dichotomous composites were set to the values of the dichotomous race composites that used parent data from the base year (X12HISP, X12AMINAN, X12ASIAN, X12HAWPI, X12BLACK, X12WHITE, X12MULTR). Otherwise, the dichotomous ethnicity and race composites were set to -9 (not ascertained). Using the six dichotomous race variables and the Hispanic ethnicity variable, the race/ethnicity composite variables for the child (X_RACETHP_R, X_RACETH_R) were created.  12 Unlike the ECLS-K, in the ECLS-K:2011 there was not a field to enter \"other\" race in the race question."}, {"section_title": "7-18", "text": "Native, non-Hispanic; More than one Race, non-Hispanic) are coded according to the child's reported race. If the report about whether the child was Hispanic was -7 (refused) or -8 (don't know), or if the child is not Hispanic and parent reported race is missing, X_RACETHP_R is coded -9 (not ascertained); if the report about whether the child was Hispanic is also missing from the FMS, or if the child is not Hispanic and race is also missing from the FMS, X_RACETH_R is coded -9 (not ascertained). The difference between X_RACETHP_R and X_RACETH_R is that if race or ethnicity data are missing from the spring 2012 parent interview, X_RACETH_R is set to the value for the base-year composite, X12RACETH, which uses both parent data and FMS data, while only parent report data were used for the variable X_RACETHP_R. Thus, there are more missing data for X_RACETHP_R than for X_RACETH_R. The categories for X_RACETHP_R and X_RACETH_R are mutually exclusive, meaning that a child is coded as just one race/ethnicity. Users interested in the specific races of children who are identified as multiracial, or who are interested in identifying the race(s) of children who are identified as Hispanic, should use the dichotomous race variables discussed above."}, {"section_title": "Child's Height (X3HEIGHT, X4HEIGHT)", "text": "To obtain accurate measurements, each child's height was measured twice in each data collection round. The height measurements were entered into the computer program used for the assessment, with a lower limit set at 35 inches and an upper limit set at 60 inches. For the height composites, if the two height measurements obtained within a round (i.e., C3HGT1 and C3HGT2 for fall 2011 and C4HGT1 and C4HGT2 for spring 2012) were less than 2 inches apart, the average of the two height values was computed and used as the composite value. If the two measurements were 2 inches or more apart, for X3HEIGHT (the child's height in fall 2011), the measurement that was closest to 47.01 inches for boys and 46.63 inches for girls was used as the composite value. This is the 50th percentile height for children who were 6 and a half years old (79.21 months for boys; 78.59 months for girls: the average age at assessment in fall 2011 using the composite X3AGE). If the two spring measurements were 2 inches or more apart, for X4HEIGHT (the child's height in spring 2012), the measurement that was closest to 48.25 inches for boys and 48.15 inches for girls was used as the composite value. This is the 50th percentile height for children who were 7 years old (85.66 months for boys; 85.04 months for girls: the average age at assessment in spring 2012 using the composite X4AGE). The height averages come from the 2000 Centers for Disease Control and Prevention (CDC) Growth Charts (www.cdc.gov/growthcharts/html_charts/wtage.htm). 13 The two height measurements were 2 or more inches apart in 30 cases for X3HEIGHT and 42 cases for X4HEIGHT. If one value for height was missing, the other value was used for the composite. If both the first and second measurements of height were coded as -8 (don't know), then the height composite was coded as -9 (not ascertained). If both the first and second measurements of height were coded as -7 (refused), then the height composite was coded as -7 (refused). If both the first and second measurements of height were coded as -9 (not ascertained) because height data were missing as the result of a breakoff in the child assessment or the measurements had different missing values (e.g., one was -8 and the other was -9), then the height composite was coded as -9 (not ascertained). In 118 cases, the child's height in the spring of 2012 (X4HEIGHT) was shorter than in the fall of 2011 (X3HEIGHT). A difference of 1 inch or less (48 children) could be a function of things such as slouching versus standing upright or differences in shoes, hairstyle, thickness of socks, or a combination of these factors. However, 70 children were recorded as being more than 1 inch shorter in the spring than in the fall, and 40 of those were recorded as being more than 2 inches shorter. In addition, 151 children were recorded as having a shorter height in the spring of 2012 than in the spring of 2011. Of these children, 71 were recorded as having a height difference of 1 inch or less; 80 were recorded as having a height difference of more than 1 inch; and 47 were recorded as having a height difference of more than 2 inches. These discrepancies may result from measurement error or recording error. Analysts should use their own judgment in how to use these cases in their analysis."}, {"section_title": "Child's Weight (X3WEIGHT, X4WEIGHT)", "text": "To obtain accurate measurements, each child's weight was measured twice in each data collection round. The weight measurements were entered into the computer program used for the assessment, with a lower limit set at 20 pounds and an upper limit set at 120 pounds. Values outside the range that were documented in assessor comments were included in the data file. For the weight composites, if the two weight measurements obtained within a round (i.e., C4WGT1 and C4WGT2 for spring 2012 and C3WGT1 and C3WGT2 for fall 2011) were less than 5 pounds apart, the average of the two weight values was computed and used as the composite value. If the two measurements were 5 or more pounds apart, for X3WEIGHT the measurement that was closest to 48.84 pounds for boys or 47.55 pounds for girls was used as the composite value. These are the median weights for children who were 6 and a half years old (79.21 months for boys; 78.59 months for girls: the average age at assessment in fall 2011 using the composite X3AGE). If the two measurements were 5 or more pounds apart, for X4WEIGHT the measurement that was closest to 51.53 pounds for boys or 50.91 pounds for girls was used as the composite value. These are the median weights for children who were 7 years old (85.66 months for boys; 85.04 months for girls: the average ages at assessment in spring 2012 using the composite X4AGE). The weight averages come from the 2000 CDC Growth Charts (see www.cdc.gov/growthcharts/html_charts/wtage.htm). 14 The two weight measurements were 5 or more pounds apart in 28 cases for X3WEIGHT and 61 cases for X4WEIGHT. If one value for weight was missing, the other value was used for the composite. If both the first and second measurements of weight were coded as -8 (don't know), the weight composite was coded as -9 (not ascertained). If both the first and second measurement of weight in the child assessment were coded as -7 (refused), then the weight composite was coded as -7 (refused). If both the first and second measurements of weight in the child assessment were coded as -9 because weight data were missing as the result of a breakoff in the child assessment or the measurements had different missing values (e.g., one was -8 and the other was -9), then the weight composite was coded as -9 (not ascertained). There are 16 children whose round 4 weights are more than 10 pounds lower than their round 3 weights; of these, four changes are in the range of 24.25 pounds to 36 pounds. There are 32 children whose round 4 weights are more than 15 pounds higher than their round 3 weights; of these, five changes are in the range of 25.3 to 50.6. It is possible that some of these changes result from measurement error. Analysts may wish to review such cases and determine how to account for these weight changes in their analysis."}, {"section_title": "7.5.1.7", "text": "Child's Body Mass Index (X3BMI, X4BMI) Composite body mass index (BMI) was calculated by multiplying the composite weight in pounds by 703.0696261393 and dividing by the square of the child's composite height in inches (Keys et al. 1972;Mei et al. 2002). Unrounded values of height and weight were used in the calculation of BMI. If either the height or weight composite was coded as -9 (not ascertained), -7 (refused), or -8 (don't know), the BMI composite was coded as not ascertained (-9)."}, {"section_title": "Child's Disability Status (X2DISABL2, X4DISABL2, X4DISABL)", "text": "Two composite variables based on information obtained in the parent interview were created to indicate whether a child had a disability diagnosed by a professional. The variables differ in how missing data were treated during their creation. Questions in the spring 2012 parent interview asked about the child's ability to be independent and take care of himself or herself, ability to pay attention and learn, overall activity level, overall behavior and ability to relate to adults and children, emotional or psychological difficulties, ability to communicate, difficulty in hearing and understanding speech, and eyesight. If parents indicated that their child had any issues or difficulties in response to these questions, follow-up questions asked whether the child had been evaluated by a professional for that particular issue and whether a diagnosis of a problem was obtained by a professional (CHQ120, CHQ125, CHQ215, CHQ245, CHQ246, CHQ300, CHQ301). Questions were also asked about current and past receipt of therapy services or participation in a program for children with disabilities (CHQ340, CHQ341). The composite variable X4DISABL is coded 1 (yes) if the parent answered \"yes\" to at least one of the questions about diagnosis (indicating a diagnosis of a problem was obtained) or therapy services (indicating the child received services) (CHQ120, CHQ215, CHQ245, CHQ300, CHQ340, CHQ341) and the questions about the specific diagnoses (CHQ125, CHQ246, CHQ301) were not coded -7 (refused,) -8 (don't know), or -9 (not ascertained); or in the case of the vision diagnosis (CHQ301), the question was not coded as only nearsightedness (myopia), farsightedness (hyperopia), color blindness or deficiency, or astigmatism; or in the case of a hearing diagnosis (CHQ246), the question was not coded as only external ear canal ear wax."}, {"section_title": "7-22", "text": "Using these criteria to calculate X4DISABL, a child could be coded as having a disability even if data for some of the questions about diagnoses or therapy services (CHQ120, CHQ215, CHQ245, CHQ300, CHQ340, CHQ341) were missing. This is because a child is coded as not having a disability if there are data for at least one of the questions about diagnoses or therapy services (CHQ120, CHQ215, CHQ245, CHQ300, CHQ340, CHQ341), and the response was either 2 (no) or the item was -1 (inapplicable) (because the child did not have issues that indicated a question should be asked), even if data for some of these questions were missing. In addition to having \"no\" answers or \"inapplicable\" codes for the diagnoses or therapy services questions, if the child had a diagnosis, but the specific diagnosis was not reported (was refused, don't know, or not ascertained), X4DISABL was also coded 2 (no) because there was no reported disability. The composite was coded as missing only if all of the data for the questions about diagnoses or therapy services (CHQ120, CHQ215, CHQ245, CHQ300, CHQ340, CHQ341) were -7 (refused), -8 (don't know), or -9 (not ascertained), or if the items that skipped to these items were -7 (refused), -8 (don't know), or -9 (not ascertained). A more conservative approach when coding cases that had incomplete data for the diagnoses and services variables was used to derive the variable X4DISABL2. Whereas X4DISABL codes cases with missing data as \"no\" as long as all the information that was collected indicates the child does not have a diagnosed disability or receive services for a diagnosed disability, X4DISABL2 is coded -9 (not ascertained) when any of the questions about diagnoses or therapy services (CHQ120, CHQ215, CHQ245, CHQ300, CHQ340, CHQ341) are -7 (refused), -8 (don't know), or -9 (not ascertained), or the items that skipped to these items are -7 (refused), -8 (don't know), or -9 (not ascertained). For X4DISABL2, if there are no \"yes\" answers for a disability, but any of the evaluation (CHQ115, CHQ210, CHQ235, CHQ290), diagnoses (CHQ120, CHQ215, CHQ245, CHQ300), or therapy questions (CHQ340, CHQ341) are -7 (refused), -8 (don't know), or -9 (not ascertained), 15 or if any of the evaluation, diagnosis, or therapy questions were not asked (were -1 for inapplicable) because of missing data for questions that skipped to those questions (and thus it is not known if they should have been asked), X4DISABL2 is coded -9 (not ascertained). In addition, if the parents indicated that a diagnosis had been obtained, but the specific diagnosis was coded as refused, don't know, or not ascertained, X4DISABL2 is coded as -9 (not ascertained). This approach is more conservative because it does not assume that the response for unanswered questions was \"no.\" Due to these differences in coding, the number of cases identified as having a diagnosed disability is higher for X4DISABL than it is for X4DISABL2. The disability variable included on the base-year data file, X2DISABL, was derived in the same way as X4DISABL. Their derivation was based on the methods used to create the disability composites for the ECLS-K. The variable X2DISABL2 is an additional composite created for the K-1 file to provide a variable for the kindergarten-year data that is derived in the same way as X4DISABL2. X2DISABL2 is calculated like X4DISABL2, but is based on spring 2011 data."}, {"section_title": "Primary Language in the Child's Home (X4LANGST)", "text": "A composite variable was created to indicate whether English was a primary language spoken in the home or whether a non-English language was the primary language spoken. Parents were asked if any language other than English was regularly spoken in their home (P4ANYLNG). If a language other than English was not spoken in the home, or if a language other than English was spoken in the home but the primary language of the household (P4PRMLNG) was English, the composite is coded as 2 (English language). If both English and another language were spoken in the home, and the respondent reported that two or more languages were spoken equally or they could not choose a primary language, the composite is coded 3 (cannot choose primary language or two languages equally). Otherwise, if a language other than English was spoken (P4ANYLNG), either solely (P4ENGTOO) or primarily in the home (P4PRMLNG), the composite is coded as 1 (non-English language)."}, {"section_title": "Student Grade Level (X3GRDLVL, X4GRDLVL)", "text": "The X3GRDLVL composite indicates the child's fall grade level as reported by the teacher. It is constructed using F3CLASS2 16 (child's class, e.g., all-day kindergarten or first grade) and T3GRADE (child's grade level from the fall TQC). The values include 1 for kindergarten (either full or part day), 2 for first grade, 3 for second grade, and 4 when the child is in an ungraded setting. In all other cases the value is set to -9 for not ascertained. The X4GRDLVL composite indicates the child's spring grade level as reported by the teacher. It is constructed using F4CLASS2 (child's class, e.g., all-day kindergarten or first grade), T4GRADE (child's grade level from the spring TQC1), and T4KGRADE (child's kindergarten program type from the spring TQCK). The valid reported values include 1 for kindergarten, 2 for first grade, 3 for second grade, 4 for third grade, and 5 when the child is in an ungraded setting. In all other cases the value is set to -9 for not ascertained."}, {"section_title": "Student Kindergarten Class Type and Teacher Class Data Reporting (X4CLASS2)", "text": "Although most children in the study were in first grade in the spring of 2012, some were still in kindergarten. X4CLASS2 was created as a two-digit variable in order to provide information about the type of kindergarten class in which a child was enrolled (a half-day a.m. class, a half-day p.m. class, or a full-day class) and what teacher/classroom variables should be used for each child for those children who were still in kindergarten. Children who were in first grade or higher are included in categories indicating they were not in kindergarten. Information about kindergarten class type and grade level were taken from the following places: 1 The structure of the spring TQAK was such that the teacher was asked to report information separately (in different columns) for each type of class that he or she taught. In the data file, information about half-day a.m., half-day p.m., and full-day kindergarten classes is stored in different variables associated with each classroom type. Because of inconsistencies in reporting by teachers, it is not always clear which variables should be used for the specific class in which the child is enrolled. Some teachers did not always report data in the column associated with the type of class he or she indicated teaching (for example, in TQAK the teacher reported teaching a full-day kindergarten class but reported data in the half-day a.m. kindergarten column), some teachers did not report teaching the same type of kindergarten class in which he or she indicated the child was enrolled (for example, in TQAK the teacher reported teaching only a half-day p.m. kindergarten class but reported in TQCK that the child was in a half-day a.m. kindergarten class), and some teachers reported teaching another class in addition to the type of class in which the child was enrolled (for example, in TQAK the teacher reported teaching both half-day a.m. and half-day p.m. kindergarten classes and reported in TQCK that the child was in a half-day a.m. kindergarten class). X4CLASS2 is an indicator of agreement in child-level information (in X4GRDLVL, spring TQCK, and fall TQC), and class type information in TQAK and tells users which set of variables (half-day a.m., half-day p.m., or full-day) describe the particular kindergarten classroom in which the child was enrolled."}, {"section_title": "Child Linked to a Special Education Teacher (X4SPECS)", "text": "The composite variable X4SPECS indicates whether or not children were linked to a special education teacher and special education questionnaires were requested from teachers in the spring of 2012, based on the presence or absence of a link to a special education teacher or related service provider in the FMS. The value is 1 if special education questionnaires were requested and 2 if special education questionnaires were not requested. Study team leaders asked school staff what accommodations were required for the child to be assessed. During the discussion about accommodations, team leaders were also supposed to record whether the child had an Individualized Education Program (IEP) on file with the school but did not require any accommodations for the study assessments. The link to a special education teacher was established automatically when this information was entered in the FMS by study team leaders. Information about receipt of special education services was first obtained in the fall of 2011 and then updated, if necessary, in the spring of 2012. If a child had an IEP, the team leader was required to indicate a link to both a classroom teacher and a special education teacher. The links were verified by team leaders by looking at FMS reports that indicated required teacher links for each child. There are a few cases of a mismatch between X4SPECS and special education teacher reports about an IEP. In about 20 cases, there were FMS data indicating the child had an IEP on record at the school (and thus a special education teacher questionnaire was requested from the teacher and 7-29 X4SPECS = 1), but the special education teacher indicated in the child-level questionnaire that the child did not have an IEP (E4RECSPE=2). During the parent interview, data on age, sex, and relationship were collected for all new household members. Data about a change in relationship to the child (since the previous interview with relationship data) were collected in spring 2012 for those with specific relationships described in the parent interview specifications. Data about race and ethnicity were collected for specific household members who were new to the household and for specific previous household members with missing race or ethnicity data. Other data were also collected about parents in spring 2012 (e.g., country of origin, education level) depending on the characteristic and whether previous data had been collected for that parent. References to \"parents\" in this chapter include both parents and guardians."}, {"section_title": "Family and Household Composite Variables", "text": "The composite variables for parents (e.g., parent age, parent education) are for the parents Two composite variables take the ages of the household members into account to indicate the total numbers of (1) adults and 2 For each household member in each round, roster variables include the following, where * is the round number (1, 2, or 4) and # is the household roster number (1 through If there is no parent interview completed in a given round, then the items for that round are assigned a value of system missing. Beginning in round 4, if a person has left the household (e.g., P4CUR_# = 2, not a current household member), the roster variables for that position are assigned a value of -1 for that round and subsequent rounds in which a parent interview is completed. In rare cases, there are roster positions for which all values are system missing or -1 across all rounds but P4CUR_# = 2 (not a current household member). This may occur in the following circumstances: \uf06e A new household member was the respondent for round 3, when there was no roster confirmation and completion in the parent interview, but had left the household by the time the round 4 parent interview was completed. 19"}, {"section_title": "Food Security Status", "text": "The food security status of the children's household was determined by responses to the 18 food security questions (P4WORRFD through P4NOMONY) asked in section FDQ of the spring 2012 parent interview. 20 The questions measured the households' experiences related to food insecurity and reduced food intake in the last 12 months. Questions were asked about adults' experiences separately from the experiences of the children in the household. They were combined into scales using statistical methods based on the Rasch measurement model. The food security questions were developed by academic researchers using ethnographic and case-study methods with low-income women and families to identify natural language used to describe their situations and behaviors when they had difficulty obtaining enough food. The scales derived from the food security questions were validated using statistical methods based on item response theory and by comparing measured food security with other indicators of food adequacy. Composites were created that indicate the food security status of the child's household generally (based on all 18 adult and child items), as well as the food security status of the adults (based on 10 household-and adult-referenced items) and of the children (based on 8 childreferenced items) in the household separately. When interpreting food security statistics, users should keep in mind that food security status is a household-level characteristic. In most households classified as having very low food security, the children in the household were not food insecure at that level of severity. Young children in U.S. households are generally protected from disrupted diets and reduced food intake to a greater extent than are older children or adults in the same households (Nord and Hopwood 2007) The household food security raw score, X4FSRAW2, is a count of affirmative responses to the 18 food security items, and an ordinal-level measure of food insecurity. It can be used in analyses as an ordinal measure of food insecurity or to identify more severe or less severe categories of food insecurity than those identified in the categorical food security variables described in section 7.5.2.3.3. The raw score is ordinal, not interval, so it should not be used when a linear measure is required, such as for calculation of a mean. Responses to items skipped because of screening are assumed to be negative for the purpose of creating the score. For cases that have some missing data but at least some valid responses, missing responses were considered to be negatives. Cases with no valid responses to any of the 18 food security items are coded as missing -9 (not ascertained). X4FSRAW2 ranges from 0 to 18. X4FSADRA2 is the adult food security raw score, which is a simple count of the number of household-and adultreferenced food security items affirmed by the parent, and ranges from 0 to 10. X4FSCHRA is the children's food security raw score, which is a simple count of the number of child-referenced food security items affirmed by the parent. It ranges from 0 to 8."}, {"section_title": "Food Security Status: Continuous Measures (X4FSSCAL2, X4FSADSC2, and X4FSCHSC)", "text": "X4FSSCAL2 is the scale score presentation of the household food security items. It is a continuous, interval-level measure of food insecurity and is appropriate for linear models, such as correlation, regression, or analysis of variance. This scale score is a Rasch transformation of the raw score (X4FSRAW2). Valid values range from 1.4 to 13, with higher values indicating more severe food deprivation. Under Rasch-model assumptions, the scale score for households that affirm no items (raw score = 0) is undefined. It is less than the lowest measured value (1.4), but its precise value is unknown and may vary substantially among households. For such cases, X4FSSCAL2 is assigned a value of -6. These households are food secure, but the appropriate size of the interval between their score and the score of households that affirmed one item is not known and varies from household to household. If these cases (a substantial majority of all cases) are included in linear models, appropriate methods must be used. For example, if the food security scale score is a dependent variable, a selection model such as Tobit may be appropriate. If the food security scale score is a predictor variable, a value of 0 may be assigned to cases with a raw score of 0 and a dummy variable added to identify households with a raw score of 0."}, {"section_title": "7-36", "text": "X4FSADSC2 is the adult food security scale score. This is a measure of the severity of food insecurity experienced by adults in the household in the previous 12 months. It is a continuous, intervallevel measure based on the Rasch measurement model and is appropriate for linear models, such as correlation, regression, or analysis of variance. It is on the standard (logistic-unit) metric described in  Children's Food Security in U.S. Households, 1995-99 (Nord andBickel 2002). Valid values range from 4.1 to 12.2, with higher values indicating more severe food deprivation. The scale score is undefined for households that affirmed no child-referenced items and is coded -6 (see discussion of X4FSSCAL2 above)."}, {"section_title": "Food Security Status: Categorical Measures (X4FSSTAT2, X4FSADST2, and", "text": ""}, {"section_title": "X4FSCHST)", "text": "X4FSSTAT2 is a categorical measure of household food security status based on the household's food security raw score, X4FSRAW2. X4FSSTAT2 assigns households into one of three ordered categories: food secure (raw scores 0-2), having low food security (raw scores 3-7), and having very low food security (raw scores of 8 or more). The two categories \"low food security\" and \"very low food security\" together make up the more general category, food insecurity. X4FSSTAT2 is appropriate for comparing percentages of households with food insecurity or very low food security across subpopulations and can be used as a categorical variable in associative models. X4FSADST2 is a categorical measure of adults' food security status based on the household's adult food security raw score, X4FSADRA2. X4FSADST2 identifies households as food secure (raw scores 0-2), having low food security among adults (raw scores 3-5), or having very low food security among adults (raw scores of 6 or more). This variable is appropriate for comparing percentages 7-37 of households with food insecurity among adults and very low food security among adults across subpopulations. X4FSCHST is a categorical measure of children's food security status based on the children's food security raw score, X4FSCHRA. X4FSCHST identifies households as having only food secure children (raw scores 0-1), having low food security among children (raw scores 2-4), or having very low food security among children (raw scores [5][6][7][8]. The two categories \"low food security among children\" and \"very low food security among children\" together make up the more general category, food insecurity among children (alternatively described as, \"households with food insecure children\"). determined from household roster variables whether there was a mother (biological, adoptive, step-, or foster) and/or a father (biological, adoptive, step-, or foster) in the household. Using this information, the following method was used to create X4IDP1 and X4IDP2 for the spring."}, {"section_title": "1.", "text": "If there was only one mother (of any type, including unknown type) and only one father (of any type, including unknown type) in the household, the mother was identified as parent 1 (X4IDP1) and the father was identified as parent 2 (X4IDP2)."}, {"section_title": "2.", "text": "If there was only one mother (of any type, including unknown type) in the household and no other parent figure (of any type), the mother was identified as parent 1 and parent 2 is coded -1 (not applicable). If there was a mother and she had a male spouse/partner in the household who was not identified as a father (of any type, including unknown type), the spouse/partner was identified as parent 2.\nSelect the Settings menu and then the Control Panel folder icon.\nSelect the Control Panel tab."}, {"section_title": "3.", "text": "If there was only one father (of any type, including unknown type) in the household and no other parent figure (of any type), the father was identified as parent 1 and parent 2 is coded -1 (not applicable). If there was a father and he had a female spouse/partner in the household who was not identified as a mother (of any type), the spouse/partner was identified as parent 1 and the father was identified as parent 2.\nIn the Control Panel window, click the Display icon.\nIn the Control Panel window, click the Display icon."}, {"section_title": "4.", "text": "If there were two mothers (or a mother and female spouse/partner) in the household, an order of preference was used to identify one mother to be parent 1, with the order specified as biological, adoptive, step-, foster mother or female guardian, then other female parent or guardian. 22 The other mother was identified as parent 2. If there were two mothers of the same type (e.g., two adoptive mothers) or there were two mothers and the type for both was -7 (refused) or -8 (don't know), the mother with the lowest household roster number was identified as parent 1 and the other mother was identified as parent 2.\nSelect the Settings tab.\nSelect the Change display settings tab."}, {"section_title": "5.", "text": "If there were two fathers in the household (or a father and male spouse/partner), an order of preference was used to identify one father to be parent 1, with the order specified as biological, adoptive, step-, foster father or male guardian, then other male parent or guardian. The other father was identified as parent 2. If there were two fathers of the same type (e.g., two adoptive fathers) or there were two fathers and the type for both was -7 (refused) or -8 (don't know), the father with the lowest household roster number was identified as parent 1 and the other father was identified as parent 2.\nSet the Desktop Area to 1024 x 768 pixels with the Desktop Area slidebar. If you have a Windows Vista or Windows 7 \u00ae operating system, you can check or set your desktop area as follows: 1. Click the Windows Start Button.\nSet the Desktop Area to 1024 x 768 pixels with the Desktop Area slidebar. As noted above, the ECB requires approximately 20 megabytes of available disk space on your hard drive. If 20 megabytes of space is not available, you may wish to delete unnecessary files from the drive to make space for the ECB."}, {"section_title": "6.", "text": "If there was no one in the household identified as a mother or father, then a female respondent or the female spouse or partner of a male respondent was identified as parent 1. If the female parent figure had a male spouse or partner, the spouse/partner was identified as parent 2. If the respondent was male and had a female spouse or partner, she was designed as parent 1 and he was designated as parent 2. For example, if a child lived with his grandmother (the respondent) and grandfather, and neither his mother nor father lived in the household, then the grandmother was identified as parent 1 and the grandfather was identified as parent 2. If the grandfather lived in the household, but no grandmother or parents lived there, the grandfather respondent would be parent 1 and parent 2 would be coded -1. Demographic information such as age, race, and education was collected for these \"parent figures.\" Once parents/parent figures were identified, X4HPAR1 and X4HPAR2 were created to identify the specific relationship of parent 1 and parent 2 to the study child. 23 It should be noted, however, that for households in which the child lived with parent figures other than his or her mother and/or father, the parent figures identified in X4IDP1 and X4IDP2 were not defined as parents (meaning biological, step-, adoptive, or foster) for the construction of X4HPAR1 and X4HPAR2. For example, if there are a grandmother and grandfather and there are no parents listed in the household, X4HPAR1 and X4HPAR2 would be coded as category 15 (no resident parent). 22 There were new categories in the ECLS-K:2011 parent interview for \"Other female parent or guardian\" in FSQ140 and \"Other male parent or guardian\" in FSQ150 that were not included in the ECLS-K. 23 These variables are a combination of P4HMOM and P4HDAD from the ECLS-K."}, {"section_title": "7-39", "text": "X4HPARNT indicates the type(s) of parents living in the household with the study child. The values for the X4HPARNT composite are as follows: \uf06e 1 = two biological/adoptive parents; \uf06e 2 = one biological/adoptive parent and one other parent/partner; \uf06e 3 = one biological/adoptive parent only; and \uf06e 4 = one or more related or unrelated guardian(s). When study children are living with parent figures (e.g., grandmother and grandfather), rather than biological, adoptive, step-, or foster parents, X4HPARNT is coded 4. In addition to two questions asking where parent 1 and parent 2 were born (P4PARCT1, P4PARCT2) and when, if applicable, they moved to the United States (P4PAREM1, P4PAREM2), there are three sections in the parent interview that asked questions about the residential parent(s) or parent Each of these sections was completed during the parent interview for up to two parents or parent figures. To indicate which household member or members were the subject of each section, \"pointer\" variables that hold the household roster number of the person were used. The pointer variables P4EMPP1, P4PEQHH1, and P4PLQHH1 are always equal to X4IDP1, where applicable, and the pointer variables P4EMPP2, P4PEQHH2, and P4PLQHH2 are always equal to X4IDP2, where applicable. That is, there is no difference between the pointer variables and the composite variables that identify the parents, other than when a pointer is not applicable. The PLQ parent pointers are based on P4ANYLNG and the parent identifiers X4IDP1 and X4IDP2. If P4ANYLNG = 2, -7, or -8 (no, refused, don't know), section PLQ is not applicable and the pointers are set to -1 (not applicable). Thus, if P4ANYLNG = 2, -7, or -8 (no, refused, don't know), P4PLQHH1 could be -1, even though there is a person for X4IDP1. If P4ANYLNG = -9 (not ascertained), the PLQ parent pointers are set to -9 (not ascertained). If P4ANYLNG = 1, then P4PLQHH1 will have a value that matches X4IDP1, and P4PLQHH2 will have a value that matches X4IDP2."}, {"section_title": "7-40", "text": "To illustrate how the pointer variables work, suppose there is a household with both a mother and a father who were listed as the third and fourth individuals in the household roster. According to the rules outlined above, household member #3, the mother, becomes parent 1 and X4IDP1 equals 3. All applicable pointer variables for parent 1 take on the value 3. Similarly, household member #4, the father, becomes parent 2 and X4IDP2 equals 4. All applicable pointer variables for parent 2 take on the value 4. Table 7-1 identifies the pointer variables included in the data file. The pointer variables are necessary to determine which parent should be assigned the answers to items about language use, education and human capital, and employment. Returning to the above example, the answers to the education questions (e.g., P4HIG_1_I, P4ENR_1, P4FPT_1, etc.) for household member #3, the mother, are stored in variables that end with the suffix \"_1\" to correspond with the fact that the mother's household roster number was assigned to X4IDP1. That is, the suffix \"_1\" indicates that the data are for parent 1. The answers to the education questions (e.g., P4HIG_2_I, P4ENR_2, P4FPT_2, etc.) for household member #4, the father, are stored in variables that end with the suffix \"_2\" to correspond with the fact that the father's household roster number was assigned to X4IDP2. That is, the suffix \"_2\" indicates that the data are for parent 2.  X4PAR1AGE is a composite variable for the age of parent 1 from the household roster and X4PAR2AGE is the composite variable for the age of parent 2 from the household roster. 24 The ages of all household members (other than the child) who had their ages collected in fall 2010 or spring 2011"}, {"section_title": "7-41", "text": "were incremented by one year in the spring 2012 parent interview program. Other household members who were not in the study in fall 2010 or spring 2011 had their ages collected in spring 2012. For information about how the first and second parents were selected for these and other parent variables, see section 7.5.2.4 above. The composite variables for race/ethnicity for the parent/guardians were derived in the same way as those for the child, except that there are no variables that supplement parent-reported race/ethnicity with FMS data as was done for children. All data on parent race/ethnicity come from the parent interview. Spring 2012 race/ethnicity information for parents is provided in the data file in categorical race/ethnicity composites (X4PAR1RAC for parent 1 in the household and X4PAR2RAC for parent 2). 25 Race and ethnicity information was collected only once for each parent/guardian. If race and ethnicity information was collected in the fall of 2010 or spring of 2011, it was not collected again in the spring of 2012. The questions about race and ethnicity were only asked in the spring 2012 parent interview to collect this information for new parents/guardians in the household or when this information was missing for parents/guardians who lived in the household at the time of the spring 2011 interview. Respondents were allowed to indicate that they, and the other parent figure when applicable, were Hispanic or Latino, and whether they belonged to one or more of the five race categories (White, Black or African American, American Indian or Alaska Native, Asian, Native Hawaiian or other Pacific Islander). 26 From these responses, a person's race/ethnicity was classified into eight mutually exclusive categories. A person's race/ethnicity was classified as \"more than one race, not Hispanic\" if more than one race was specified and the answer to the question about being Hispanic or Latino was 2 (no). A person's race/ethnicity was classified as \"Hispanic, race specified\" if the answer to the question about being Hispanic or Latino was 1 (yes) and at least one race was indicated in the question about race. If a person was Hispanic or Latino, but a race was not indicated, that person's race/ethnicity was classified as \"Hispanic, no race specified.\" The remaining race/ethnicity categories (White, non-Hispanic; Black or 24 These variables are a combination of P4HDAGE and P4HMAGE in the ECLS-K. 25 These variables are a combination of P4HDRACE and P4HMRACE in the ECLS-K. 26 In the ECLS-K, there was an \"other\" category for race. In the ECLS-K:2011, the \"other\" category was not included as a response option."}, {"section_title": "7-44", "text": "African-American, non-Hispanic; Asian, non-Hispanic; Native Hawaiian or Other Pacific Islander; non-Hispanic; and American Indian or Alaska Native, non-Hispanic) were coded according to the person's reported race when the person was not Hispanic or Latino. If the answer to the question about being Hispanic or Latino was -7 or -8 (refused or don't know, respectively), or if the person was not Hispanic and the answer to the question about race was -7 or -8 (refused or don't know, respectively), race/ethnicity was coded -9 (not ascertained). Parent race/ethnicity was obtained for all parents/guardians and spouses of respondent parents/guardians but may or may not have been collected for a parent's boyfriend or girlfriend. For example, in a household with a birth mother and stepfather, the race/ethnicity of both parents was obtained. However, in a household with a birth mother and her boyfriend, the race/ethnicity of the mother was obtained but that of the boyfriend was not unless he was the respondent. 27"}, {"section_title": "Parent Education Variables (X4PAR1ED_I, X4PAR2ED_I)", "text": "There are two parent education composite variables on the file: X4PAR1ED_I (parent 1's highest level of education) and X4PAR2ED_I (parent 2's highest level of education). This composite variable describes the education level of parents who were in the household at the time of the spring 2012 interview. If a parent figure in the spring of 2012 was also a household member in the kindergarten year, and educational data about the highest education level were collected for that person in the kindergarten year, then questions about education level were not asked in spring 2012; in these cases that parent's highest education level from the base-year composite X12PAR1ED_I or X12PAR2ED_I was carried forward to X4PAR1ED_I or X4PAR2ED_I, depending on whether the person was identified as parent 1 or parent 2, respectively. 28 The composite variables are based on reports of the parent's highest education level (P4HIG_1_I, P4HIG_2_I) and whether the parent had a high school degree or its equivalent, such as a GED (P4HIS_1_I, P4HIS_2_I). 29 If the highest education level reported for a parent was in grades 0 27 In spring 2012, there are races and ethnicities for persons who did not qualify to have race and ethnicity asked in spring 2012, but did qualify to have race and ethnicity collected in an earlier round of the study. Persons who have race and ethnicity on the file for spring 2012 include the focal child; those with a relationship of mother/female guardian or father/male guardian in any round (P*REL_* = 1 or 2 or P*UNR = 3 or 4); those who were a respondent in any round (P*PER_* = 1); and persons who were spouse/partners of respondent parents in any round. This is different from how race and ethnicity were included on the file for spring 2011. In spring 2011, with some exceptions noted in the base-year user's manual, the races and ethnicities on the file were for persons who qualified to have race and ethnicity in that round. 28 The skip in the spring 2012 parent education section was based on highest education level from fall 2010 or spring 2011 (P1HIG_1,P1HIG_2, P2HIG_1, P2HIG_2) rather than high school degree/GED (P1HIS_1, P1HIS_2, P2HIS_1, P2HIS_2). Cases that had missing data for high school degree/GED (P1HIS_1, P1HIS_2, P2HIS_1, P2HIS_2) were treated on the X12PAR1ED_I and X12PAR2ED_I variables as if there was no high school degree/GED. In spring 2012, the high school degree/GED (P1HIS_1, P1HIS_2, P2HIS_1, P2HIS_2) data are imputed if they are missing, and this value is incorporated into X4PAR1ED and X4PAR2ED. 29 For some cases, education, data were collected in spring 2011 and inadvertently collected again for the same parents in spring 2012. There was a programming issue that resulted in data being reversed between the parents in the household so that education data were collected again for 7-45 through 12 (e.g., P4HIG_1_I= 11) and the parent had a high school degree or its equivalent (e.g., P4HIS_1_I=1 or 2), or if the highest education level was 13 (high school equivalent/GED) or 14 (high school diploma), then the composite variable is coded as 3 (high school diploma or equivalent). Otherwise, the education composite is coded according to the value of the highest education level even if the data for whether the parent had a high school degree or its equivalent were missing. Some codes on the highest education question were grouped together in the composite variable categories. The categories \"vocational/technical after high school, but no vocational/technical diploma\" and \"vocational technical program after high school diploma\" (e.g., P4HIG_1_I= 15 or 16) were coded as 4 (vocational/technical program). The categories \"some college, but no degree\" and \"associate's degree\" (P4HIG_1_I=17 or 18) were coded as 5 (some college). The categories \"doctorate degree\" and \"professional degrees after a bachelor's degree\" (e.g., P4HIG_1_I=22 or 23) were coded as 9 (doctorate or professional degree). The variables reflect the education level of the household member(s) corresponding to X4IDP1 and X4IDP2. For example, if X4IDP1 and X4IDP2 pointed to a child's grandmother and grandfather, then the highest level of education would be collected about these nonparent guardians. See section 7.5.2.4 for more detailed discussion of how X4IDP1 and X4IDP2 were determined. As described in section 7.5.2.9, education data are imputed if they are missing. In the base year of the study, the composite variable for parent education itself was imputed; however, for the spring 2012 parent interview data, the variables used to create the composite education variable (highest education and whether the parent had a high school degree or equivalent) were first imputed, and these imputed variables were used to compute the composite variable."}, {"section_title": "Parent Occupation Variables (X4PAR1EMP_I, X4PAR2EMP_I, X4PAR1OCC_I, X4PAR2OCC_I, X4PAR1SCR_I, X4PAR2SCR_I)", "text": "Several composites can be used to describe parents' employment status, their occupations, and the prestige of their occupations. The pointer variables for employment data, P4EMPP1 and P4EMPP2, are set to the same value as X4IDP1 and X4IDP2, and can be used to identify the household roster number of the individual(s) to which the data refer. some parents who had previous education data and not collected for those with missing data from spring 2011. For persons who had education data collected in both spring 2011 and spring 2012, the composites for parent education use the most recently obtained data from spring 2012. Missing data were imputed. Imputation was performed on the variables (e.g., P4HRS_1_I, P4DO1_I) that were used to create the X4PAR1EMP_I and X4PAR2EMP_I composite variables. Each variable has a separate imputation flag (e.g., IFP4PAY_1 is the imputation flag for P4PAY_1_I, the variable for whether parent 1 had paid job last week) indicating whether data were imputed for each case in the data file. Imputation is described in section 7.5.2.9. If a parent figure in the spring 2012 parent interview was also a parent figure in the fall 2010 parent interview, and occupational data were collected for that parent in that earlier round, a question was asked about whether the parent had changed his or her employment situation (P4EMPCHG_1_I and P4EMPCHG_2_I) since the date of the fall 2010 interview. If no change in employment was reported, 30 Because some persons were not looking for work according to the five categories described above, even though it was reported that a parent was looking for work (P4LOK_1_I = 1), the parent is coded as not in the labor force (X4PAR1EMP_I = 4) rather than as looking for work (X4PAR1EMP_I = 3). If a parent was reported as looking for work (P4LOK_1_I =1), the questions about the parent's last occupation were asked. There are 207 cases with occupation data that are categorized as X4PAR1EMP = 4 (not in the labor force) because they indicated that all they were doing to look for work was looking at/reading want ads or some \"other\" activity that did not qualify them to be classified as looking for work; there are 65 cases with occupation data for where X4PAR2EMP = 4. Among these cases, in one case for X4PAR1OCC_I and three cases for X4PAR2OCC_I, a parent who is not working, on vacation, or looking for work has an occupation code. In these cases, the parent was initially reported as looking for work and the occupation information was collected. However, in \"other, specify\" response upcoding, the parents' status on P4LOK_* (looking for work) was changed to not looking for work because the information provided in the other, specify text field did not indicate an activity that qualified them to be categorized as actively looking for work. The collected occupation information was retained for these cases."}, {"section_title": "7-47", "text": "then information about the hours of work per week and the number of jobs the parent had was collected and used in creating the employment composite variable, but other questions about employment and occupation were not asked. In these cases, the fall 2010 parent interview data were brought forward and used in the most recent occupation composite variable. The composite variables for parent occupation, X4PAR1OCC_I and X4PAR2OCC_I, are Exhibit 7-5. Industry and occupation codes used in the ECLS-K:2011"}, {"section_title": "Executive, Administrative, and Managerial Occupations", "text": "This category includes senior-level and middle management occupations and occupations that directly support management. Senior-level managers are persons concerned with policymaking, planning, staffing, directing, and/or controlling activities. Middle managers include persons who plan, organize, or direct and/or control activities at the operational level. Workers in this category are not directly concerned with the fabrication of products or with the provision of services. Other officials and administrators include consultants, library directors, custom house builders, and location managers. Legislators are also included in this category."}, {"section_title": "Engineers, Surveyors, and Architects", "text": "This category includes occupations concerned with applying principles of architecture and engineering in the design and construction of buildings, equipment and processing systems, highways and roads, and land utilization."}, {"section_title": "Natural Scientists and Mathematicians", "text": "This category includes those engaged primarily in the application of scientific principles to research and development. Natural scientists are those in the physical sciences (e.g., chemistry, physics) and the life sciences (e.g., biology, agriculture, medicine). In addition, this category includes those in computer science, mathematics (including statistics), and operations research."}, {"section_title": "Social Scientists, Social Workers, Religious Workers, and Lawyers", "text": "This category includes occupations concerned with the social needs of people and with basic and applied research in the social sciences."}, {"section_title": "Teachers: College, University, and Other Postsecondary Institution; Counselors, Librarians, and Archivists", "text": "This category includes those who teach at higher education institutions and at other postsecondary (after high school) institutions, such as vocational institutes. In addition, vocational and educational counselors, librarians, and archivists are included here."}, {"section_title": "Teachers, Except Postsecondary Institution", "text": "This category includes prekindergarten and kindergarten teachers, elementary and secondary teachers, special education teachers, instructional coordinators, and adult education teachers (outside postsecondary education)."}, {"section_title": "Physicians, Dentists, and Veterinarians", "text": "This category includes health care professionals who diagnose and treat patients. In addition to physicians, dentists, and veterinarians, this category includes optometrists, podiatrists, and other diagnosing and treating professionals, such as chiropractors, hypnotherapists, and acupuncturists."}, {"section_title": "7-49", "text": "Exhibit 7-5. Industry and occupation codes used in the ECLS-K:2011-Continued"}, {"section_title": "Registered Nurses, Pharmacists, Dieticians, Therapists, and Physician's Assistants", "text": "This category includes occupations concerned with the maintenance of health, the prevention of illness and the care of the ill through the provision and supervision of nursing care; compounding drugs, planning food service or nutritional programs; providing assistance to physicians; and the provision of therapy and treatment as directed by physicians."}, {"section_title": "Writers, Artists, Entertainers, and Athletes", "text": "This category includes occupations concerned with creating and executing artistic works in a personally interpreted manner by painting, sculpturing, drawing, engraving, etching, and other methods; creating designs for products and interior decorations; designing and illustrating books, magazines, and other publications; writing; still, motion picture, and television photography/filming; producing, directing, staging, acting, dancing, singing in entertainment; and participating in sports and athletics as a competitor or player and administering and directing athletic programs."}, {"section_title": "Health Technologists and Technicians", "text": "This category includes occupations concerned with providing technical assistance in the provision of health care. For example, clinical laboratory technologists and technicians, dental hygienists, radiologic technicians, licensed practical nurses (LPNs), and other health technologists are included here."}, {"section_title": "Technologists and Technicians, Except Health", "text": "This category includes those providing technical assistance in engineering and scientific research, development, testing, and related activities, as well as operating and programming technical equipment and systems."}, {"section_title": "Marketing and Sales Occupations", "text": "This category includes occupations involving selling goods or services, purchasing commodities and property for resale, and conducting wholesale or retail business."}, {"section_title": "Administrative Support Occupations, Including Clerks", "text": "This category includes occupations involving preparing, transcribing, transferring, systematizing, and preserving written communications and records; collecting accounts; gathering and distributing information; operating office machines and data processing equipment; operating switchboards; distributing mail and messages; and other support and clerical duties such as bank teller, data entry keyer, etc."}, {"section_title": "Service Occupations", "text": "This category includes occupations providing personal and protective services to individuals, and current maintenance and cleaning for building and residences. Some examples include food service, health service (e.g., aides or assistants), cleaning services other than household, and personal services. Once occupations were classified in X4PAR1OCC_I and X4PAR2OCC_I, they were assigned the average of the 1989 General Social Survey (GSS) prestige scores, which are reported in variables X4PAR1SCR_I and X4PAR2SCR_I. If the parent's occupation was 22 (Unemployed, Retired, Unclassifiable), the prestige score was set to -9 (not ascertained). If the parent's occupation was -1 (No Occupation) on X4PAR1OCC_I or X4PAR2OCC_I, the prestige score was also coded -1. Although the GSS prestige scores are from 1989, they are still being used by the current GSS survey and matched to 1980 census codes. 31 Because these prestige scores were also used for the ECLS-K 1998-99 cohort, they allow for comparisons to the ECLS-K. Table 7-2 provides the prestige score values for each occupation category. was on leave from a job or unemployed and actively looking for work, he or she was asked the occupation questions. Category 22 was used only if a respondent reported an occupation that could not be classified in the coding scheme, \"unemployed,\" or \"retired.\") Because these occupations could not be classified, the prestige score is coded -9 (not ascertained) When occupation is -1, the prestige score is also -1. As described in section 7.5.2.9, occupations were imputed if such information was not collected in the parent interview. The imputation flag variables IFX4PAR1OCC and IFX4PAR1SCR indicate whether the occupation (X4PAR1OCC_I) and occupational prestige score (X4PAR1SCR_I) for parent 1 were imputed. These flags match in value because the prestige score (e.g., X4PAR1SCR_I) is coded directly from occupation (e.g., X4PAR1OCC_I). Similarly, the flags IFX4PAR2OCC and IFX4PAR2SCR indicate whether the occupation (X4PAR2OCC_I) and occupational prestige score (X4PAR2SCR_I) for parent 2 were imputed."}, {"section_title": "Household Income and Poverty (X4INCCAT_I, X4POVTY_I)", "text": "Household income data were collected in the spring 2012 parent interview. Parents who participated in the spring 2011 parent interview were told what detailed income range (from PAQ110 in spring 2011) was reported in that interview and asked if their household income was still in that range. Parents who said their income changed and those who had missing income information from spring 2011 because of item or unit nonresponse were asked to report income by broad range ($25,000 or less or more than $25,000) and by detailed range (table 7-3). 32 The composite X4INCCAT_I was created using the detailed income range information. If the respondent reported that the range in which household income fell was the same as the range reported in the spring of 2011 (P4INCSAM_I = 1), then the value of X2INCCAT_I (the composite from spring 2011) was used for the value of X4INCCAT_I. Otherwise, X4INCCAT_I was set to the value of P4INCLOW_I (detailed income range for those who reported the broad income range in P4HILOW_I as $25,000 or less) or P4INCHIG (detailed income range for those who reported the broad income range in P4HILOW_I as more than $25,000). When data for the broad range variable (P4HILOW_I) or one of the detailed range variables (P4INCLOW_I, P4INCHIG_I) were missing (i.e., coded -7 (refused), -8 (don't know), or -9 (not ascertained)), income information was imputed. Section 7.5.3.8 has a description of the imputation of missing data for the components used in the calculation of X4INCCAT_I. 32 Starting at category 9 of the detailed income range, the categories for the income variable in the ECLS-K:2011 are different from those used in the ECLS-K. More narrow ranges of income were used at higher income levels in the ECLS-K:2011 in order to determine whether household income was near 200 percent of the federal poverty threshold given household size. If so, follow-up question about exact income were asked. Reported income was used to determine household poverty status in the spring of 2012, which is provided in variable X4POVTY_I. For some households, more detailed information about household income than the ranges described above was collected. Specifically, when parent respondents reported a detailed household income range suggesting the household income was close to or lower than 200 percent of the U.S. Census Bureau poverty threshold for a household of its size, the respondents were asked to report household income to the nearest $1,000 (referred to as exact income) in order to determine household poverty status more accurately. Table 7-4 shows the reported detailed income categories for households of a given size for which respondents were asked the exact income question. For example, a respondent in a household with two people would have been asked to provide an exact income if the respondent had indicated that their household income was in the range of less than or equal to $30,000. Table 7-4 also shows how the income categories compare to the value that is 200 percent of the weighted average 2011 poverty threshold. 33 33 The CAPI program used to conduct the parent interview was programmed to only ask for exact income when parent respondents reported a detailed household income range suggesting the household income was close to or lower than 200 percent of the U.S. Census Bureau poverty threshold for a household of its size. Although the parent interview in which this information was collected was conducted in the spring of 2012, the 2010 poverty thresholds were used for instrument programming because they were the most recent thresholds available when programming was done. The question about exact income was asked for the following conditions: "}, {"section_title": "7-54", "text": "When information about exact household income was available (P4TINCTH_I), it was used in conjunction with household size (X4HTOTAL) to calculate the poverty composite. When exact income was not available because the exact income question was not asked, the midpoint of the detailed income category (X4INCCAT_I) was used in conjunction with household size (X4HTOTAL). 34 Years Old, retrieved 9/3/2013 from http://www.census.gov/hhes/www/poverty/data/threshld/index.html. 2 The 2011 weighted poverty thresholds were used for the poverty composite because respondents in the spring of 2012 were asked about household income in the past year. At the time that the spring 2012 parent interview was finalized, the most updated poverty thresholds available were the weighted 2010 poverty thresholds. Poverty thresholds for 2011 were similar to the poverty thresholds for 2010. However, because of differences in four categories, exact income should have been asked for some narrow ranges of incomes according to the 2011 thresholds, but it was not asked because the 2010 thresholds were used. Using the 2011 poverty thresholds rather than the 2010 poverty thresholds, any cases with the following incomes were not asked exact income when they should have been: a household of three with an income between $35,001 and $35,832, a household of four with an income of $45,001 to $46,042; a household of six with an income between $60,001 and $61,694, and a household of seven with an income between $70,001 and $70,170. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2012. Household poverty status in the spring of 2012 was determined by comparing total household income reported in the parent interview to the weighted 2011 poverty thresholds from the U.S. Census Bureau (shown in , which vary by household size. Although the parent interview was conducted in the spring of 2012, the 2011 weighted poverty thresholds were used in the derivation of the poverty composite because respondents were asked about household income in the past year. Exact income (P4TINCTH_I) was asked in the parent interview or imputed for all persons in categories 1 and 2 of the poverty composite. Imputation of exact income was conducted according to thresholds in the parent interview. Households with an exact income that fell below the appropriate threshold were classified as category 1, \"below the poverty threshold,\" in the composite variable. Households with an exact income that was at or above the poverty threshold but below 200 percent of the poverty threshold were classified as category 2, \"at or above the poverty threshold, but below 200 percent of the poverty threshold,\" in the composite variable. Households with a total income (either exact or the income representing the midpoint of the detailed range reported by the composite) that was at or above 200 percent of the poverty threshold were classified as category 3, \"at or above 200 percent of the poverty threshold,\" in the composite variable. 35 For example, if a household contained two members and the household income was lower than $14,657, the household was considered to be below the poverty threshold and would have a value of 1 for the composite. If a household with two members had an income of $14,657 or more, but less than $29,314 (200 percent of the poverty threshold for a household of two), the composite would have a value of 2. If a household with two members had an income of $29,314 or more, the composite would have a value of 3.   In order to provide SES data for as many children who had an otherwise complete parent interview as possible, missing values were imputed for each of the individual items used to compute the composite variables that factor into the derivation of SES, namely parent education, employment, occupational prestige, and household income. For example, missing values for highest grade completed (P4HIG_n_I) and diploma status (P4HIS_n_I) were imputed for cases for which these items were asked in the spring of 2012 but the data were missing (-7 (refused), -8 (don't know), or -9 (not ascertained)) and those imputed data were used to compute the parent education composite variables. Missing data for individual items related to parent employment (whether employment had changed since the spring of 2011, whether the parent had worked for pay in the last week or was on leave or vacation, hours worked in a typical week, whether the parent was looking for work and if, so, what the parent was doing to find work) were imputed, and then those imputed data were used to compute the occupation composite variables if necessary (i.e., cases missing employment status that were imputed to be working or on leave from a job also had their occupation imputed and a prestige score assigned to the imputed occupation; cases missing data for the variables about looking for work and that were imputed to be actively looking for work (defined by EMQ070 answers 1-5) also had occupation imputed). The different income category variables and the question asking whether there was a change in household income from the kindergarten year were also imputed. This was a change in procedure from the imputation method used in the base year where the education, occupation, and household income composites were imputed. Imputing the individual items, rather than the composites, allows any valid data that exist for any items related to the components to be used to compute SES. Two methods were used to impute missing data: longitudinal imputation and hot deck imputation. Longitudinal imputation (carrying forward a base-year value) was sometimes used when base-year data were available for the items for which data were missing in the spring of 2012. For example, in some cases a parent interview broke off before the questions in the employment section were asked, but employment and occupation data for the parent(s) in the household were available from the base-year data file. Similarly, for some cases data for the income variables were reported in the base year When longitudinal imputation was not possible (either because there was a change in parent figures or base-year data were not available), hot deck imputation was used. In hot deck imputation, the value reported by a respondent for a particular component variable (e.g., highest grade completed or occupation) is assigned or \"donated\" to a \"similar\" person who failed to respond to that question. Auxiliary demographic information known for both donors and nonrespondents is used to form imputation cells that include donors and nonrespondents with similar values for the characteristics that define the cells. The specific demographic characteristics used to define imputation cells varied by the component being imputed, as noted below. The imputed value for a case with a missing value is taken from a randomly selected donor among the respondents within the cell. For each imputed variable, imputation cells were created using demographic characteristics that were the best predictors of the variable. Characteristics such as census region, school type (public/Catholic/non-Catholic religious/private nonsectarian), school locale (city/suburb/town/ rural), household type (female single parent/male single parent/two parents), parents' race/ethnicity, and parents' age range were used to form the cells. Chi-square automatic interaction detector (CHAID) analyses were used to determine these predictors. In some cases, data for an item may have been missing for both the spring of 2012 and the base year, but the base-year data were imputed at the composite level rather than at the item level. Where appropriate, imputed base-year composite variables or base-year component variables were used as sort variables in spring 2012 imputation. For example, if highest grade completed is missing in the fall of 2010 (P1HIG_n) and in the spring of 2012 (P4HIG_n_I) for a given parent, then the imputed value of X12PAR*ED_I (the base-year parent education composite variable) was used as a sort variable in the spring 2012 imputation process for that parent. The order of imputation is parent 1's education variables; parent 2's education variables; parent 1's labor force status variables; parent 1's occupation; parent 2's labor force status variables; parent 2's occupation; whether the household income had changed from the kindergarten year; detailed income range when the broad income range is known; detailed income range when the broad income range is not known; and exact income where applicable based on household income and detailed income range. Imputation cells for each component imputed were created using the other components, when possible. The hot deck imputation was implemented as follows: \uf06e For households with both parents present, parent 1's and parent 2's variables were imputed separately. \uf06e Imputed as well as reported values were used to create imputation cells. For any given component, the imputation cells were created using (1) collected and imputed data for those variables that were imputed before the given component, and (2) collected data only for those variables that were imputed after the given component."}, {"section_title": "7-59", "text": "After imputation was completed, the occupational prestige variables (X4PAR1SCR_I and X4PAR2SCR_I) were created by assigning the average of the 1989 General Social Survey (GSS) prestige score associated with parent occupation, as described above in section 7.5.2.7. Upon completion of imputation, the composite variables that are used in the computation of SES were created. These are parent education (X4PAR1ED_I and X4PAR2ED_I), parent occupational prestige scores (X4PAR1SCR_I and X4PAR2SCR_I), and household income (X4INCCAT_I). Although imputation was conducted on the item-level variables used to compute these composites, the names of the composites themselves also carry the _I designation to indicate that they contain imputed data. These composite variables do not have their own imputation flags. The imputation flags associated with the variables used to compute the composites can be reviewed to identify cases for which the composite is based on imputed data. 37 The values of each SES component were then normalized so that the component had a mean of 0 and a standard deviation of 1. In this normalization step, -1 (not applicable) values are treated as missing. This is also known as the z-score. For the h-th SES component, a z-score z hi for the i-th household was computed as where hi is the value of the h-th SES component for the i-th household; \u0305 w is the weighted mean 38 of hi ; and sd( \u0305 w ) is the standard deviation of \u0305 w . Note that where h is household income, hi is the natural log of the midpoint of the detailed income range. The weight used to compute the z-score is the spring first-grade child base weight. The SES variable for the i-th household was then computed as 37 The questionnaire items about occupation (job title, job activities, employer, industry) are not included in the data file; the imputation flags for occupation are associated with the occupation composite variables. 38 The first-grade base weight (i.e., sample weight) adjusted for base-year nonresponse and mover subsampling was used."}, {"section_title": "7-60", "text": "where m is the number of components. Note that for households with only one parent present and for parents who were retired or not currently in the labor force, not all the components were defined. In these cases, the SES is the average of the z-scores of the available components. X4RESREL2 is coded as -9 (not ascertained). Because the interviewer initially asked to speak with the previous round respondent at the beginning of the spring 2012 parent interview, the respondent for the fall 2010 interview (X1RESID), the spring 2011 interview (X2RESID), and the spring 2012 interview (X4RESID) was the same person for many cases."}, {"section_title": "Teacher Composite Variables", "text": "In addition to the teacher data flags discussed in section 7.4.3 above, there are several composite variables on the file that use data from teachers. There is a composite variable (X34CHGTCH) discussed below in section 7.7 indicating whether the child changed teachers between the fall and spring data collections. There are also composite variables about the child's closeness and conflict with the teacher (X4CLSNSS, X4KCLSNSS, X4CNFLCT, X4KCNFLCT). These are described in chapter 3, along with other variables derived from teacher reports of children's social skills. Other variables that use teacher data are about the child's classroom experiences (e.g., X4CLASS) and are discussed above in section 7.5.1 about the child composites."}, {"section_title": "School and Class Composite Variables", "text": "Variables describing children's school and class characteristics were constructed using data from the teacher, the school administrator, and the sample frame. Details on how these variables were created are provided below."}, {"section_title": "School Type (X4SCTYP)", "text": "In the spring of 2012, the school administrator questionnaire (SAQ) given to administrators in schools that did not have base-year data (SAQ-A) contained a question on school type that was used in the creation of the spring school type composite (X4SCTYP). Base-year data from the round 2 composite, X2SCTYP, were used for the composite X4SCTYP when such data were available. X4SCTYP was created as follows when SAQ-A was given to school administrators: If question A6 in the SAQ (\"Which of the following characterizes your school?\") was answered as \"a regular public school (not including magnet school or school of choice)\" (S4REGPSK); \"a public magnet school\" (S4MAGSKL); or \"a charter school\" (S4CHRSKL), the school was coded as \"public.\" If the question was answered as \"a Catholic school\" of any type (S4CATHOL, S4DIOCSK, S4PARSKL, or S4PRVORS), the school was coded as \"Catholic.\" If the question was answered as \"other private school, religious affiliation\" (S4OTHREL), the school was coded as \"other religious.\" Otherwise, if the question was answered as \"private school, no religious affiliation\" (S4OTNAIS, S4OTHRNO), then the school was coded as \"other private.\" If there were data from the base year for X2SCTYP, X4SCTYP was set to the value for X2SCTYP. If data from the school administrator questionnaire were missing, information about school type from the school master file (which included FMS and frame data) were used. Homeschooled children have a code of -1 (not applicable). 40 Children who changed schools and were not followed and children who were not located in the spring of 2012 have missing values (-9) for X4SCTYP. The variable X4SCTYP is set to system missing for children who were not participants in the spring 2012 round. In addition, these children have a value of 990000000 on the variable F4CCDLEA."}, {"section_title": "Public or Private School (X3PUBPRI, X4PUBPRI)", "text": "X3PUBPRI and X4PUBPRI are broad indicators of school type (with only two categoriespublic and private) and are derived from the more detailed school type variables from the school frame for fall 2011 and X4SCTYP described above. In both fall 2011 and spring 2012, these composites were created as follows: If school type indicated in fall 2011 or X4SCTYP is 4 (public), then X3PUBPRI and X4PUBPRI, respectively, are coded \"public\" (1). If school type indicated in fall 2011 or X4SCTYP is 1, 2, or 3 (Catholic, other religious, or other private), then X3PUBPRI and X4PUBPRI, respectively, are coded \"private\" (2). If school type is coded as -1 (not applicable) in fall 2011 or in X4SCTYP because the child was homeschooled, then X3PUBPRI and X4PUBPRI are coded -1 (schooled at home). X3PUBPRI and X4PUBPRI are coded -9 (not ascertained) if data on school type are not available in fall 2011 and X4SCTYP, respectively. X3PUBPRI is set to system missing for children who did not participate in round 3; similarly, X4PUBPRI is set to system missing for those who did not participate in round 4."}, {"section_title": "School Enrollment (X4ENRLS)", "text": "There is a composite variable in the data file (X4ENRLS) that indicates total school enrollment on October 1, 2011 (or the date nearest to that date for which the school administrator had data available). Total school enrollment was created using the school enrollment variable from the school administrator questionnaire (S4ANUMCH). If school administrator data on total school enrollment were missing, enrollment data were obtained from the 2009-10 Private School Universe Survey (PSS) for private schools and from the 2010-11 Common Core of Data (CCD) public school universe data for public schools. If enrollment data were also missing on the PSS or CCD, but spring 2011 enrollment data 40 These children were enrolled in a school at the time of sampling in the base year, but were homeschooled during the spring of 2012."}, {"section_title": "7-63", "text": "were available for the school, the value of X4ENRLS was set to the value of X2KENRLS. In all other cases the variable is coded -9 (not ascertained)."}, {"section_title": "Percent Non-White Students in the School (X4RCETH)", "text": "The composite variable X4RCETH indicates the percentage of the student population that was non-White in the spring of 2012. 41 The composite is derived from a question in the school administrator questionnaire (question A9 in SAQ-A, and question A6 in SAQ-B) that asked the number or percentage of students in the school who were the following race/ethnicities: Hispanic/Latino of any School administrators were allowed to report their answers to the student racial/ethnic composition questions as either numbers or percentages. All answers provided as numbers were converted to percentages using the total enrollment variable S4TOTENR as the denominator before computing the composite variable. 42 The sum of the calculated percentages for each race/ethnicity category was allowed to be within +/-5 percent of 100 percent to allow for minor reporting errors of numbers that did not add to the reported total or percentages that did not add to 100 percent. In a few cases, this procedure resulted in a total sum of percentages that was slightly over 100 percent. Totals greater than 100 percent are topcoded to 100 percent. A flag for each individual race/ethnicity variable indicating whether the school administrator reported the information as a number or a percent is included in the data file. 43 Because the composite is calculated as a percent, these flags will not be needed by users unless they are interested in examining how answers were reported. If the flag (S4ASIAFL S4HISPFL, S4BLACFL, S4WHITFL, S4AIANFL, S4HAWPFL, and S4MULTFL) for each of the race/ethnicity variables (S4ASIAPT, S4HISPPT, S4BLACPT, S4WHITPT, S4AIANPT, S4HAWPPT, and S4MULTPT) is equal to 1, that indicates the information was reported by the school administrator as a percentage. If the flag (S4ASIAFL S4HISPFL, S4BLACFL, S4WHITFL, S4AIANFL, S4HAWPFL, and S4MULTFL) for each of the race/ethnicity variables (S4ASIAPT, S4HISPPT, S4BLACPT, S4WHITPT, S4AIANPT, S4HAWPPT, and S4MULTPT) is equal to 2, that indicates the information was reported by the school administrator as a number. In some cases, the composite could not be derived from the survey data because at least some data used to compute it were missing or the data collected from administrators appeared to be in error. If the composite could not be derived from the SAQ response, the percentage of non-White students in the school was obtained from the 2010-11 CCD (for public schools) or the 2009-10 PSS (for private schools). If these data were also missing on the CCD or PSS, the composite was coded based on the spring kindergarten composite X2RCETH if the child attended the same school. If those data were also missing, X4RCETH is coded -9 (not ascertained). If the study child was homeschooled in the spring of 2012, the composite is coded -1 (not applicable)."}, {"section_title": "Highest and Lowest Grade at the School (X4LOWGRD, X4HIGGRD)", "text": "Two composite variables indicate the lowest grade taught at the school (X4LOWGRD) and the highest grade taught at the school (X4HIGGRD). They are derived from information collected from the school administrator during the spring data collection (for administrators in schools for which baseyear data were not available, who received questionnaire SAQ-A) or during the base year (for 43 There were also other questions in the school administrator questionnaire that allowed for answers to be recorded as either a number or percent. The flags for these variables are S4ADAFLG (average daily attendance reported as number or percent); S4ASIAF2 (question about Asian or Pacific Islander teachers, not Hispanic or Latino, reported as number or percent); S4HISPF2 (question about Hispanic teachers reported as number or percent); S4BLACF2 (question about Black teachers, not Hispanic or Latino, reported as number or percent); S4WHITF2 (question about White teachers, not Hispanic or Latino, reported as number or percent); S4AIANF2 (question about American Indian or Alaska Native teachers, not Hispanic or Latino, reported as number or percent); S4HAWPF2 (question about Native Hawaiian or Pacific Islander teachers, not Hispanic or Latino, reported as number or percent); and S4MULTF2 (question about teachers of two or more races, not Hispanic or Latino, reported as number or percent). In all cases, the variables related to these flags provide information as numbers or percentages, with the flags indicating how the answers were originally reported by school administrators."}, {"section_title": "7-65", "text": "administrators in schools for which base-year data were available who received questionnaire SAQ-B). For administrators who received questionnaire SAQ-A, both variables are created by first coding answers of \"ungraded\" in question A5 of the SAQ-A (\"Mark all grade levels included in your school\") or \"ungraded\" in the data from the frame as category 15 (ungraded) and then coding the lowest grade in the school and the highest grade in the school, respectively. The grade level for children in transitional kindergarten, kindergarten, or pre-first grade is coded as category 2 (kindergarten). For administrators who received questionnaire SAQ-B because they had data about the highest and lowest grade at the school collected during the base-year of the study, the composites X4HIGGRD and X4LOWGRD were set to the base-year composite values for X2HIGGRD and X2LOWGRD, respectively. Data from the school frame were used if information about the highest and lowest grade at the school was not collected from the school administrator."}, {"section_title": "Students Approved for Free or Reduced-Price School Meals (X4FMEAL_I, X4RMEAL_I)", "text": "Composites indicating the percent of students in the school who were approved for free school meals and the percent of students in the school who were approved for reduced-price school meals were derived from information collected from the school administrator during the spring 2012 data collection. 44 School administrators were asked to report the total enrollment in the school (S4ANUMCH_I), the number of children in the school who were approved for free school meals (S4NMFRM_I), and the number of children who were approved for reduced-price school meals (S4NMRDM_I). The percentage of children approved for free school meals is computed as the ratio of S4NMFRM_I to S4ANUMCH. Likewise, the percent of children approved for reduced-price school meals is the ratio of S4NMRDM_I to S4ANUMCH_I. 45 Children who were homeschooled have these free and reduced-price meal composites set to -1. Some school administrators did not complete the school administrator questionnaire, and among those who did, not all responded to all three questions needed to compute these composites related to free or reduced-price meals. Table 7-7 shows the level of missing data for the school meal composite 44 Both public schools and nonprofit private schools are eligible for the National School Lunch Program. 45 X4FMEAL_I and X4RMEAL_I were top-coded to 100 percent, if necessary."}, {"section_title": "7-66", "text": "variables among the schools that had at least one child or parent respondent in the spring 2012 data collection. Missing data for the school meal composite variables were imputed for all cases that are considered participants in the spring 2012 round and attended a public or private school that reported participating in the USDA school breakfast or lunch program. 46 Values of zero were imputed for cases for which the school administrator indicated the school did not participate in the USDA meal program and did not report the number of approved students. Values were imputed in several ways. If a school administrator questionnaire was completed, but data for one or more of the variables contributing to the school meal composites (S4ANUMCH_I, S4NMFRM_I, S4NMRDM_I) were missing in the spring of 2012, the missing spring 2012 values were imputed as the base-year values from the composites for reduced-priced (X2RLCH2_I) and free school meals (X2FLCH2_I). If the base-year data were not available, data from the 2010-11 CCD were used to impute for these missing values for public schools. 47 Imputation using data from the PSS could be done for private schools if total school enrollment (S4ANUMCH_I) was the only piece of information missing because the PSS does not have information on receipt of free or reduced-price meals. If any of the variables contributing to the school meal composites could not be imputed using the procedures just described, they were imputed using the hot-deck method described above in section 7.5.2.9. In hot-deck imputation, a school with a non-missing value for a component has this value assigned or \"donated\" to a similar school with a missing value for the component. Schools are similar if they belong in the same imputation cell. Imputation cells were created using district poverty category (created from the district poverty variable X4DISTPOV described in section 7.5.7), census region, school type, and whether the school received Title I funding. The hot deck method was applied as follows. First, any missing values for total enrollment were imputed. Then the proportions of students approved for free and reduced-price lunch were imputed from similar donors. The imputed proportions were then multiplied by the total enrollment to give the imputed values of S4NMFRM_I and S4NMRDM_I. This approach was used to ensure that imputation resulted in plausible combinations of S4NUMCH_I, S4NMFRM_I, and S4NMRDM_I. When no school administrator questionnaire was completed, the composite variables X4FMEAL_I and X4RMEAL_I were imputed directly, without imputing the individual components first. Imputation in these cases was first attempted by using frame information, then by carrying forward the composite value from spring 2011 where available. Finally, for a few cases, hot-deck imputation was used to impute the composites, when no frame or base-year data were available. In some cases, the children's schools are unknown because the child was unlocatable or the child moved to a nonsampled county and was not followed into his/her school, but a parent interview was completed. In such cases, data were not imputed for these composites because no information about the school was available (e.g., public or private control, school size, or even if the child was enrolled in a school). X4FMEAL_I and X4RMEAL_I are coded as -9 for these cases."}, {"section_title": "School Year Start and End Dates (X4SCHBDD, X4SCHBMM, X4SCHBYY, X4SCHEDD, X4SCHEMM, X4SCHEYY)", "text": "The composite variables indicating school year start and end dates, which are listed below, were derived from question A2 in the school administrator questionnaires (S4SYRSMM, S4SYRSDD, S4SYRSYY, S4SYREMM, S4SYREDD, S4SYREYY). If the school administrator did not answer that question, data for these variables come from information contained in the FMS.  X3REGION and X4REGION are coded -9 (not ascertained) for children who were unlocatable or moved out of a sampled county and were not followed in the fall of 2011 or spring of 2012, respectively, but for whom there are parent interview data. Children who were homeschooled in the fall of 2011 have a code of -1 on X3REGION, and those homeschooled in the spring of 2012 have a code of -1 on X4REGION. X3REGION and X4REGION are set to system missing for those who did not participate in rounds 3 or 4, respectively. For the fall 2011 and spring 2012 school locality variables, X3LOCALE and X4LOCALE, the categories correspond to the 2006 NCES system for coding locale (http://nces.ed.gov/surveys/ruraled/definitions.asp). If data are not available for the child's school from the PSS or CCD, and locale data were available from the base year, the composites were set to the value of X2LOCALE or X1LOCALE. Otherwise, the composites are coded -9 (not ascertained). Some -9 (not ascertained) values for X3LOCALE and X4LOCALE are associated with cases in which children who moved were unlocatable or moved out of a sampled county and were not followed in fall 2011 or spring 2012, respectively, but for whom there is parent interview data. In fall 2011, children who were homeschooled are coded as -1 on X3LOCALE and those homeschooled in spring 2012 are coded as -1 on X4LOCALE. X3LOCALE and X4LOCALE are set to system missing for those who did not participate in rounds 3 or 4, respectively. These locale categories are the following: 11 -City, Large: Territory inside an urbanized area and inside a principal city with population of 250,000 or more; 43 -Rural, Remote: Census-defined rural territory that is more than 25 miles from an urbanized area and is also more than 10 miles from an urban cluster. Some schools have different values for X*LOCALE between the base year and rounds 3 and 4. The differences in values reflect changes in the PSS or CCD source data. The classification of locale has undergone some changes since the ECLS-K study conducted with children in the kindergarten class of 1998-99. Information on these changes is available at the NCES website at http://nces.ed.gov/ccd/rural_locales.asp."}, {"section_title": "Field Management System (FMS) Composite Variables", "text": "Several composite variables were created from data stored in the FMS, which were obtained from frame data as well as by field staff during visits to the schools and discussions with school staff."}, {"section_title": "Year-Round Schools (X4YRRND)", "text": "The year-round school composite variable is based on information obtained from the school staff member who helps coordinate the data collection activities in the school (referred to as the school coordinator) about whether a school is a year-round school. It is not based on information from the school administrator collected in the SAQ. The values for this composite variable are 1 (year-round school) and 0 (not year-round school). If the child was homeschooled in the spring of 2012, the composite is coded as -1 (not applicable). If these data were not obtained in the spring of 2012 but the data about being a yearround school was collected in the base year, the composite was set to the value of the base-year composite, X12YRRND. 7-71 . There are 60 ECLS-K:2011 public schools with a missing value for X4DISTPOV because the values were missing in the SAIPE source data."}, {"section_title": "Methodological Variables", "text": "To facilitate methodological research, 11 variables pertaining to aspects of the data collection work are included in the data file. These include identifiers for parent interview work area (F3PWKARE, F4PWKARE), parent interviewer identification number (F3PINTVR, F4PINTVR), child assessment work area (F3CWKARE, F4CWKARE), and child assessor identification number (F3CASSOR, F4CASSOR) and were extracted from the FMS. A \"work area\" is the group of schools that each team leader was assigned. Team leaders managed a group of 2 to 4 other individuals who worked as child assessors and parent interviewers for the sampled cases in the work area."}, {"section_title": "Children Who Changed Teachers Between Rounds (X34CHGTCH)", "text": "Teacher identification numbers (T3_ID, T4_ID) and school identification numbers (S3_ID, S4_ID) were used to determine whether children changed teachers between the fall of 2011 and the spring of 2012. This variable is only valid for cases that participated in the fall data collection. Otherwise, if the fall and spring teacher identification numbers are not missing and are equal to each other, then X34CHGTCH is coded 0 (no change). If a teacher identification number is missing in either the fall of 2011 or the spring of 2012, and the school identification numbers are not missing and do not match, then X34CHGTCH is coded as 1 (changed teachers). If both teacher identification numbers are not missing and they are not equal to each other, then X34CHGTCH is also coded as 1 (changed teachers). Otherwise, if the child could not be located or one or both teacher identification numbers are missing and the child is 7-72 in the same school, X34CHGTCH is coded as -9 (not ascertained). Children who were homeschooled in the spring of 2012 have a code-1, \"schooled at home,\" for X34CHGTCH. 48"}, {"section_title": "Summer School and Vacation (X3SUMSH, X3SUMVD)", "text": "One composite variable provides the number of hours a child spent in summer school during the summer of 2011 (X3SUMSH) and another indicates the length of a child's summer vacation (X3SUMVD). X3SUMSH is derived from parent interview questions on whether the child attended summer school, the length of the summer school session itself (days, weeks, or months), and the amount of time, in days and hours per day, of attendance (P3SUMSCH, P3SMSCNUM, P3SMSCUNT, P3NDYPRM, and P3NHRPRM). If the child did not attend summer school then X3SUMSH is set to 0. If the variables indicating (1) that the child attended summer school or (2) the amount of time in summer school were -7 (refused), -8 (don't know), or -9 (not ascertained), then X3SUMSH is set to -9 (not ascertained). X3SUMVD indicates the length of a child's summer vacation in days. It is calculated as the length of time between the last day of school in the kindergarten year and the first day of school in the 2011-12 school year. The ending date of the child's spring kindergarten school (X2SCHEMM, X2SCHEDD) is subtracted from the round 4 composites for the beginning date of the child's round 3 school (X4SCHBMM, X4SCHBDD) or from beginning dates in the school master file if the composite school beginning dates are not available because the child was not a round 4 participant. If the child is homeschooled in fall 2011, X3SUMVD is set to -1 (inapplicable). When data for any of the components needed to derive this composite are missing, the composite is set to -9 (not ascertained). This includes instances where the child switched schools between rounds 3 and 4 thus making the child's round 4 school data no longer a suitable replacement for round 3 data."}, {"section_title": "ELECTRONIC CODEBOOK", "text": ""}, {"section_title": "Introduction", "text": "This chapter provides specific instructions for installing the ECLS-K:2011 Electronic Codebook (ECB). The functionality of the ECB, which is the same throughout the three ECLS studies, is fully described in the Help File for the ECLS-K:2011 longitudinal kindergarten-first grade (K-1) ECB on CD-ROM. The information in the ECB's Help File provides a comprehensive tour through the ECB and addresses all of the functions and capabilities of the program. These functions allow users to access the accompanying data catalog and view the data in various ways by performing customized searches and extractions. Using the ECB, the data user can create SAS, SPSS for Windows, and Stata syntax programs that can be run to generate an extract data file from the text (ASCII) data file on the CD-ROM. Additionally, the ECLS-K:2011 K-1 CD-ROM contains Portable Document Format (PDF) files of the associated questionnaires and parent interviews in appendix A; the record layout for the data file in appendix B; this User's Manual in appendix C; base weights in appendix D; and a description of the data file in appendix E."}, {"section_title": "Hardware and Software Requirements", "text": "The ECB program is designed to run under Windows 95 \u00ae , Windows 98 \u00ae , Windows 2000 \u00ae , Windows XP \u00ae , or Windows NT \u00ae 4.0 on a Pentium-class or higher personal computer (PC). The ECB has been successfully tested using current versions of Windows Vista and Windows 7. The ECB is not designed for use on Apple Macintosh systems, but Mac users can create a data file using the record layout provided in appendix B on the CD-ROM. The PC should have a minimum of 20 megabytes of available disk space. The program will fit best visually on screens set to a desktop area of 1024 x 768 pixels. It will still work on other screen settings, but it may not make the best use of the available screen space. If you have a Windows NT \u00ae or earlier operating system, you can check or set your desktop area as follows: 1. Click the Windows Start button."}, {"section_title": "Installing and Starting the ECB", "text": "The ECB is provided on the ECLS-K:2011 K-1 CD-ROM and is intended to be installed and run from within the Windows 95 \u00ae , Windows 98 \u00ae , Windows 2000 \u00ae , Windows XP \u00ae , Windows NT \u00ae 4.0, Windows Vista, or Windows 7 \u00ae environment. The sections in this chapter provide you with step-by-step instructions for installing the program on your PC and starting the program."}, {"section_title": "Installing the ECB Program on Your Personal Computer", "text": "Program installation is initiated by running the Setup.exe file found within the CD-ROM's root directory. There are other cases that are not errors that have FSQ010 (P4CUR_*) variables set to 2 (not a current household member) for some persons even though they were not in the household in fall or spring kindergarten. These cases had new respondents in fall 2011 that were not previously in the household in the base year of the study and left the household by spring 2012. \uf06e For one case (CHILDID=10008937), persons 6 and 7 were in the household in spring 2011, but were not listed as household members on the file. These persons are listed as household members in spring 2012."}, {"section_title": "Hard-Copy Questionnaires", "text": "The hard-copy data were examined for inconsistent reporting across items and unusual values (e.g., total school enrollment (SAQ question A3A (S4ANUMCH_I)) of less than 100 or more than 900 students). Although there were some inconsistent answers and data that were outside expected ranges, all answers were confirmed as representing the values reported by respondents."}, {"section_title": "School Administrator Questionnaire (SAQ): Spring 2012 \uf06e", "text": "In spring 2012, the SAQ-A version of the school administrator questionnaire was given to schools that were new to the study or had not previously completed an SAQ in the spring 2011 kindergarten round. The SAQ-B version of the school administrator questionnaire was given to schools that had completed an SAQ in the kindergarten round. In spring 2012, an SAQ-B was fielded in error to 33 schools; the SAQ-A should have been fielded to these schools because the spring 2011 SAQ had been refused. In 21 cases, the SAQ-B was completed and receipted-school identification numbers impacted are listed below.* The remaining 12 schools either refused the SAQ-B or never returned it. There are many inconsistencies between variables A3A (S4ANUMCH_I), A3B (S4BNUMCH), A3C (S4CNUMCH), and A9H (S4TOTENR). Many respondents reported inconsistent or anomalous values where the intention of the respondent is unclear. For example, there are instances where school turnover is very high (large numbers of students leaving and entering the school during the school year). In other instances, student enrollment totals do not match between A3A (S4ANUMCH_I) and A9H (S4TOTENR) or (A3A (S4ANUMCH_I) + A3B (S4BNUMCH) -A3C (S4CNUMCH)) and A9H (S4TOTENR). While these data have been heavily scrutinized during collection, data users should carefully review these data to attempt to determine anomalies during analysis."}, {"section_title": "A-6", "text": "\uf06e Data users should be aware that there is no variable available to indicate whether a principal or other administrator completed sections A-G of the SAQ-A or sections A-F of the SAQ-B. Therefore, an assumption that S4RYYEMP and S4RMMEMP apply to non-principals if valid data are reported and S4RYYEMP and S4RMMEMP apply to principals if they equal -9 (not ascertained) cannot be made with confidence. Users should take this into consideration when analyzing these data."}, {"section_title": "Field Management System Variables", "text": "In the base year, F1CLASS and F2CLASS included an indication of whether a kindergarten class was a morning, afternoon, or full-day class. F3CLASS and F4CLASS indicate whether a kindergarten class is part-day or full-day, but do not give the morning or afternoon information."}, {"section_title": "Composite Variable Anomalies, Errata, and Considerations", "text": "Chapter 7 of this manual provides detailed information about the composite variables that were created and included in the data file. In this section, several data considerations related to the composite variables are described. Analysts are encouraged to carefully review the descriptions of the composite measures of interest to them in chapter 7."}]