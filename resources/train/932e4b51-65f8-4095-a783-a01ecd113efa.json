[{"section_title": "Abstract", "text": "In the fields of neuroimaging and genetics, a key goal is testing the association of a single outcome with a very high-dimensional imaging or genetic variable. Often, summary measures of the high-dimensional variable are created to sequentially test and localize the association with the outcome. In some cases, the results for summary measures are significant, but subsequent tests used to localize differences are underpowered and do not identify regions associated with the outcome. Here, we propose a generalization of Rao's score test based on projecting the score statistic onto a linear subspace of a high-dimensional parameter space. In addition, we provide methods to localize signal in the high-dimensional space by projecting the scores to the subspace where the score test was performed. This allows for inference in the high-dimensional space to be performed on the same degrees of freedom as the score test, effectively reducing the number of comparisons. Simulation results demonstrate the test has competitive power relative to others commonly used. We illustrate the method by analyzing a subset of the Alzheimer's Disease Neuroimaging Initiative dataset. Results suggest cortical thinning of the frontal and temporal lobes may be a useful biological marker of Alzheimers risk."}, {"section_title": "Introduction", "text": "In scientific fields where high-dimensional data are prominent, significant interest lies in testing the association of a single continuous or categorical outcome with a large number of predictors. A common approach used in neuroimaging is to perform sequential tests to reduce the number of hypothesis tests. For example, it is common to first perform a test for the association of a phenotype with an imaging variable averaged across the entire brain. If the test rejects the null hypothesis of no association between brain and phenotype, then subsequent tests are conducted on regional averages of the data or on every voxel in the image using multiplicity correction to address the number of tests performed. Often, location-specific results yield few or no significant findings due to reduced signal and the necessary adjustment for the large number of tests, even though the whole brain average data show a significant association.\nIn this paper, we propose a unified approach to test the association of an imaging or other high-dimensional predictor with an outcome and perform post hoc inference to localize signal. The framework is a modification of Rao's score test for models with a high-dimensional or infinite-dimensional parameter. The theory developed assumes the parameter being tested is defined on a compact space such as the brain. Though the approach is designed for hypothesis testing in neuroimaging, it is applicable to a wide range of scientific domains.\nThe standard framework assumes a model where Y i are iid observations from density f (y; \u03b8) and that the parameter \u03b8 = (\u03b1, \u03b2) \u2208 \u0398 \u2282 R m+p where \u03b1 \u2208 R m is a nuisance parameter and \u03b2 \u2208 R p is the parameter of interest. We seek to test the hypothesis H 0 : \u03b2 = \u03b2 0 . Define the score function U = U(\u03b8) = n \u22121 n i=1 \u2202 log f (Y i |\u03b8) \u2202\u03b2 (\u03b8). Let \u03b8 0 = (\u03b1, \u03b2 0 ) be the null value of the parameter, where \u03b1 is the true value of the parameter. Let S = U(\u03b1, \u03b2 0 ) be the score function evaluated at the maximum likelihood estimate of \u03b1 under the null hypothesis H 0 . Under the null and the conditions described in Appendix A.2, the covariance of S can be obtained from the Fisher information evaluated at the null parameter value,\nThe sum of scores (Sum) test originally discussed by Rao [1948] has been used in genetics and neuroimaging [Pan, 2009 , Madsen and Browning, 2009 . The Sum test is based on the statistic\nwhere \u03b6 \u2208 R p is a given vector of weights. The denominator is an estimate of the variance of S T \u03b6, so that the statistic is asymptotically \u03c7 2 1 [Rao, 1948] . The Sum test is locally most powerful [Rao, 1948, Cox and Hinkley, 1979] , however, the test has low power when there is a large number of variables that are not associated with the outcome . This is due to the fact that the variance of the statistic in the numerator of (1) increases by adding variables unassociated with the outcome without increasing the expected value of the numerator.\nIn the case of unknown weights, when p < n, Rao [1948] proposed maximizing the Sum test statistic with respect to the weights,\nThis statistic is distributed as \u03c7 2 p under the null. When n > p, (2) is the usual score statistic, however this statistic cannot be used for high dimensional data due to the estimate\u03a9 being noninvertible when p > n.\nFor finite dimensional parameters, our proposed test can be thought of as a generalization of Rao's test in the case where the estimate of the information matrix is noninvertible. When p > n, the test maximizes the statistic (1) with respect to the vector \u03b6 over a subspace, L, of R p . Maximization of the Sum test in the subspace L is equivalent to projecting the scores for the original model to a lower dimensional space where the information matrix is invertible. For this reason, we call the test a projected score test (PST). The procedure does not assume sparsity, but attempts to conserve power by reducing the dimension of the data and performing inference in the lower dimensional space.\nIn many cases, if a score test rejects H 0 , then it is of primary interest to perform post hoc inference to identify nonzero parameters. In neuroimaging, this amounts to a high-dimensional testing problem where the association is tested at each location in the image. The standard approach is to perform a hypothesis test at each parameter location and use a multiplicity correction procedure. Such methods this in neuroimaging that control the family-wise error rate (FWER) have relied on Gaussian random field theory [Friston et al., 1994] , but have recently been shown to have type 1 error rates far from the nominal level in real data due to assumption violations [Eklund et al., 2016 , Silver et al., 2011 . Recently, considerable research activity has focused on leveraging the dependence of the tests to control the false discovery rate (FDR) in high-dimensional settings [Efron, 2007] . Sun et al. [2015] develop a procedure to control the FDR for spatial data as well as an approach for controlling the expected proportion of false clusters. Fan et al. [2012] discuss estimation of the false discovery proportion (FDP) under dependence for normally distributed test statistics based on a factor approximation. In contrast, the PST post hoc inference procedure is performed by projecting the scores onto L, and controlling the FWER of the projected scores.\nSeveral recent studies have considered hypothesis tests for functional data, which is conceptually similar to our approach for an infinite-dimensional parameter. Reiss and Ogden [2010] propose inverting simultaneous confidence bands for the parameter of a functional predictor to test which locations of the image are associated with the outcome. Smith and Fahrmeir [2007] use a binary Markov random field model to compute the joint probability that the marginal parameter estimates are equal to zero. Our post hoc inference is most similar to Smith and Fahrmeir [2007] as the interpretation of the contribution of the scores retains a marginal interpretation.\nHere, we derive the asymptotic null distribution for the PST statistic under some standard regularity conditions. For data that are measured on a compact space, such as brain images, we discuss sufficient theoretical assumptions for characterizing test behaviors as both n and p approach infinity. For a normal linear model, we show how the finite sample distribution of the statistic can be calculated exactly for fixed n and p.\nTo demonstrate how the test can be used in neuroimaging, we investigate the association of cortical thickness with mild cognitive impairment (MCI) in the Alzheimer's Disease Neuroimaging Initiative (ADNI) study, a data set where p =18,715 and n = 628. The outer surface of the brain (cortex) represents a highly folded sheet in 3-dimensional space. The thickness of the cortex is known to be affected in individuals with psychopathology and neurological illness. MCI is a subtle pre-Alzheimer's disease decline in cognitive functioning. There is significant clinical interest in finding biological markers of MCI in order to identify those at risk for developing Alzheimer's disease, as prevention strategies and therapeutics for early disease are increasingly common. In this data set, we seek to localize regions of the brain where cortical thinning provides additional information with regard to the diagnosis of MCI beyond what can be ascertained by neurocognitive scales alone.\nFor the remainder of the manuscript, we denote matrices by uppercase italic letters (X), vectors by lowercase (x), and random vectors by uppercase roman letters (X). Hilbert spaces are denoted with black-board letters (X) and Greek letters denote model parameters. For the singular value decomposition (SVD) of any matrix we will assume that the smallest dimensions of the matrices obtained are equal to the rank of the matrix X unless otherwise noted.\nL \u2212 \u2192 denotes convergence in law and P \u2212 \u2192 denotes convergence in probability."}, {"section_title": "The Projected Score Test", "text": "In Section 2.1 and Appendix A.1 we define the PST statistic, give its asymptotic distribution, and lay out the theoretical framework. In Section 2.2, we detail conditions sufficient for studying asymptotics in p. We discuss maximization of the sum test for normal linear models in Section 2.3."}, {"section_title": "PST for finite-dimensional parameters", "text": "We assume the observed data are finite-dimensional representations that are generated from an underlying stochastic process. In Appendix A.1 we describe how to define the finite-dimensional likelihood from the infinitedimensional likelihood. Here, we informally define the finite-dimensional likelihood, the interested reader can refer to Appendix A.1 for further details.\nLet V be a nonempty compact subset of R 3 and B(V) be the space of square integrable functions from V to R. V represents the space on which data can be observed; in neuroimaging this space is the volume of the brain. The underlying stochastic processes are assumed to take values in B(V), but the observed finite-dimensional data are p-dimensional discretizations of the stochastic processes defined on V. Thus, the observed data can be described as iid observations\nObservations of Y i are a vector of k variables that are nonimaging covariates and the outcome variable, together with the observed finite-dimensional neuroimaging data. We denote the collection of data by Y = (Y 1 , . . . , Y n ). We define a parameter space \u0398 = R m \u00d7 R p that includes a finite-dimensional nuisance parameter \u03b1 \u2208 R m and the p-dimensional discretized parameter of interest \u03b2 p \u2208 R p . Together these parameters describe the joint distribution of the imaging and nonimaging data.\nDenote the finite dimensional likelihood by (\u03b8 p ; Y), where \u03b8 p = (\u03b1, \u03b2 p ) and Y are the discretized parameters and data, respectively. Define the score function U np = \u2202 \u2202\u03b2p {\u03b8 p ; Y} and let\ndenote the score function evaluated at the maximum likelihood estimate (MLE) under the null hypothesis H 0 : \u03b2 p = \u03b2 p0 .\nLet the Fisher information for the full model be\nwhere \u03b8 p0 = (\u03b1, \u03b2 p0 ). Then the asymptotic variance for\nWith the finite parameter scores defined, we can define the PST.\nDefinition 2.1. Let P L be the orthogonal projection matrix onto a linear space L \u2282 R p with r = dim(L) < n \u2212 m. Let S np be as defined in (3) and\u03a9 be the plug-in estimator of the covariance (5) obtained from\nwhere\u03b8 p0 = (\u03b1, \u03b2 p0 ) denotes the maximum likelihood estimate of the parameter vector under the null hypothesis H 0 : \u03b2 = \u03b2 p0 . Then the PST statistic with respect to L is defined as\nThe following theorem states that the asymptotic distribution (with respect to n) of the PST statistic can be found for any finite dimensional likelihood based on independent observations provided the same regularity conditions required for the convergence of the scores to a multivariate normal random variable. Theorem 2.2. Assume all objects are as described in Definition 2.1. Let P L = QQ T where the columns of the r \u00d7 p matrix Q are any orthonormal basis for L. Define\nand assume the estimateV = Q T\u03a9 (\u03b8 p0 )Q is invertible, and that the conditions given in Appendix A.2 are satisfied.\nThen, under the null, the rotated scores, denoted S Q np , are\nthe PST statistic is\nand\nTheorem 2.2 requires thatV is nonsingular, however, in practice it is possible to ensure that Q is in the column space of\u03a9(\u03b8 p0 ), so thatV \u22121 exists. The proof of Theorem 2.2 is given in Appendix A.3. We also demonstrate there that the result of Theorem 2.2 does not depend on the choice of Q. We show how L can be chosen for GLMs in Sections 3.1 and 3.2 and for imaging data in the analysis of the ADNI dataset in Section 5."}, {"section_title": "The PST as p \u2192 \u221e", "text": "We will show that as p \u2192 \u221e the PST statistic converges to an integral over a stochastic process. The rate that p approaches infinity does not depend on the sample size. Here, we assume the data can take values on the Hilbert space Y = R k \u00d7B(V), where V is a nonempty compact subset of R 3 and B(V) is the space of square integrable functions from V to R.\ni a stochastic process taking values in B(V). Realizations of Y i are a vector of k variables that are nonimaging covariates and the outcome variable, together with a function on V. We assume the parameter \u03b2 \u2208 B(V). The infinite-dimensional score function is defined in Appendix A.1 as the Fr\u00e9chet derivative of the likelihood with respect to the parameter \u03b2,\nAnd the score is defined as the function\nThroughout we assume that the infinite-dimensional scores converge in law, i.e.\nwhere S is a mean zero Gaussian process. Theorem A.2 in Appendix A.4 gives conditions under which this convergence holds [Van der Vaart, 2000] . The following definition of the PST statistic for infinite-dimensional parameters is motivated by formula (8).\nDefinition 2.3. Let (q 1 (v), . . . , q r (v)) be an orthonormal basis for the linear subspace L \u2282 B(V) with respect to the L 2 (\u03bd) inner product where \u03bd is the Lebesgue measure, and r = dim(L) < n \u2212 m. Assume q j are such that \u03bd({v : q j is discontinuous at v}) = 0 (11) for all j = 1, . . . , r.\nDefine the column vector S Q n \u2208 R r , with jth element\nand letV n be the r \u00d7 r covariance matrix with (j, k)th element\nwhere\nAssume thatV n is invertible. Then the PST statistic with respect to L is defined as\nWhile we have given a definition of the PST statistic in infinite dimensions, in practice this statistic is not estimable because it depends on functions which are only observed on finite a grid. The following theorem states that as the resolution of the grid is increased then the finite parameter PST statistic converges to the PST statistic for the infinite-dimensional parameter. Moreover, as the sample size increases the statistic converges to a statistic based on the Gaussian process S (10). The rate that p increases does not depend on n.\nTheorem 2.4. Let S np be as defined in (3). For objects as defined in Definition 2.3, let\nT , where v jp and V jp are as defined in Appendix A.1. Let Q p be the p \u00d7 r matrix with jth column q jp . Denote S Q np = Q T p S n . Define the jth element of the vector S Q \u2208 R r as\nAssume the conditions for Theorems 2.2 and A.2, and that S n and\nThen as n, p \u2192 \u221e,\nThe proof is given in Appendix A.5."}, {"section_title": "The PST in Normal Linear Models", "text": "The finite-sample distribution for the PST statistic can be found exactly for a normal linear model. Define X = [x 1 , . . . , x n ] T to be an n\u00d7m full rank matrix of covariates for each observation,G = [g 1 , . . . , g n ]\nT to be an n \u00d7 p full rank matrix of predictor variables of interest with p > n, and\u1ef8 = [\u1ef8 1 , . . . ,\u1ef8 n ] T to be n \u00d7 1 normal random vector with independent elements conditional on X and G. The Sum test with normal error is based on the model\nwhere all variables are as previously defined and E i \u223c N (0, \u03c3 2 ) are independent. If we let AA T = (I \u2212 H) be the SVD of the projection (I \u2212 H), where [Rao, 1948, Lin and Tang, 2011] \nwhere \u03b6 \u2208 R p is a known vector of weights, S np = n \u22121 G T Y is the score vector evaluated at the MLEs under the null (E(\u1ef8 i ) = \u03b1 T x i ), and\u03a9 = n \u22121\u03c32 G T G is an estimate of the variance of S np which corresponds to using the estimator (6).\nThe PST statistic for this model with respect to a linear subspace L of R p by Definition 2.1 is\nThe following theorem gives a closed-form expression for R L and its null distribution.\nTheorem 2.5. Define W be the projection matrix onto the column space of GP L . Then\nand under the null,\nwhere F (n\u2212m\u2212r),r is F-distributed with (n \u2212 m \u2212 r) and r degrees of freedom.\nThe proof can be found in Appendix A.6. The form of equation (16) shows that for a normal linear model, the test statistic is a ratio of quadratic forms. Due to the rotation invariance of Y under the null, the finite-sample distribution of R L depends only on the sample size and the dimension of the basis, but not on the particular choice of L.\n3 Specifying the linear subspace L"}, {"section_title": "Specifying L in generalized linear models", "text": "Here, we discuss choices for the selection of L in the context of GLMs with the canonical link function. We restrict attention to finite dimensional parameters and forgo the subscripts on the finite sample score vector S. Define\nT , and G = [g 1 , . . . , g n ] T where objects are as defined in\nT is from an exponential family where the expectation can be written\nwhere h is the canonical link function. For the GLM with canonical link, the scores are [McCullagh and Nelder, 1989 ]\n) is the ith fitted value under the null. Let \u0393 be the n \u00d7 n diagonal matrix with ith diagonal element\n2 . Then the estimate of the covariance (5) obtained using (6) i\u015d\nThe score statistic is obtained from the scores and the estimated information as in expression (2). In this setup, the basis for L can be constructed from the principal component analysis (PCA) of G. We write the PCA of G in terms of the SVD G = T * DQ T , where the principal scores are T = T * D = GQ. With this basis, the PST is equivalent to performing Rao's score test in a principal components regression model. To see this, first note that principal component regression is defined by\nThe scores for \u03b2 T are\nwhich are the same as the rotated scores in (7). The information estimate is also equivalent. Thus the score test statistic, nS\nAnother useful basis for L may be constructed from vectors that are indicators of variables that are expected to have a similar relationship with the outcome. The anatomical basis used in Section 5 is an example. To define the basis vectors q j , j = 1, . . . , r, we let Q j \u2282 {1, . . . , p} such that Q j \u2229 Q j = \u2205 for j = j , and then set the kth element of the jth basis vector to be q kj = 1(k \u2208 Q j ). These define orthogonal basis vectors since the sets Q j are disjoint. This basis is equivalent to averaging r subsets of the p predictor variables and performing a hypothesis test of the regression onto the r averaged variables.\nThe choice of the basis is a critical decision as it affects the power and interpretation of the post hoc inference. To clarify, under the alternative the scores have nonzero mean\nIf the projection is orthogonal to \u00b5 then the test will have power equal to the type 1 error rate. The PCA basis assumes that \u00b5 has a spatial pattern similar to the covariance structure of the predictor variables. The anatomical basis assumes that all locations within a region have the same parameter value. We discuss the effect of the basis on the interpretation of the post hoc inference in Section 4."}, {"section_title": "Choosing a dimension for the PCA basis", "text": "In order to choose a dimension for the PCA basis, we propose an adaptive procedure that sequentially tests bases of increasing dimension while controlling the type 1 error rate. To do this we first condition on the parameter estimate\u03b1 for the reduced model and perform the SVD (\u0393 \u2212 \u0393X(\nWe use subsets of columns Q as the basis L. For two columns q j and q k of Q\nThus each projected score n 1/2 q T j S is asymptotically independent because n 1/2 Q T S is asymptotically normal and can be tested by a separate chisquared test at level \u03b1 * . If this is done sequentially for r = 1, . . . , n \u2212 m, then, due to their asymptotic independence, the probability of a type 1 error under the global null is\nwhere \u03c7 2 1 (\u03b1 * ) denotes the 1 \u2212 \u03b1 * quantile of a chi-squared distribution. The approximate equality is due to the asymptotic approximation and the final inequality is the geometric series solution. In order to control the type 1 error at level \u03b1, we choose \u03b1 * = 1 \u2212 (1 + \u03b1) \u22121 . Then we sequentially test r = 1, . . . , (n \u2212 m) until we fail to reject a test at level \u03b1 * . Note that the power depends critically on the first test in the sequence; subsequent tests serve only to increase the dimension of the basis. If the first component is orthogonal \u00b5 in (19), the probability of reaching other components that do is less than \u03b1 * . A potentially more robust procedure is to test chunks of PCs by varying r = {r 1 = 0, r 2 , r 3 , . . . , r k = n\u2212m} and for the jth test perform a chi-squared test of all PCs (r j + 1), . . . , r j+1 on r j+1 \u2212 r j degrees of freedom. So long as the tests are independent, which they are under the global null, the rejection threshold \u03b1 * will control the type 1 error rate at level \u03b1. This adaptive PCA (aPCA) procedure is implemented below by testing the first 5 components together and sequentially testing components 6, . . . , n\u2212m one-at-a-time. We demonstrate the procedure in the ADNI data analysis below and type 1 error rates are assessed in Section 6."}, {"section_title": "Post hoc Inference for Localizing Signal", "text": "After performing the test of association using the PST statistic, it is of primary interest to investigate the contribution of the scores to the statistic in order to identify which locations in the image are associated with the outcome and the direction of the effect. This can be done by projecting the scores onto L and performing inference that controls the FWER for the projected scores. Because the projected scores are distributed in a subspace of R p , inference is much less conservative compared to performing inference on the original score vector.\nOur aim is to construct a rejection region for each element of the projected score vector (P L S) j , for j = 1, . . . p. Under the null,\nThe diagonal elements of P L \u2126P T L are not equal, so defining a single rejection threshold for all elements favors rejection for elements with larger variances. To resolve this issue we scale by the inverse of the standard deviation of the projected scores. Let \u2206 be the diagonal matrix with jth diagonal element\nThen the rejection threshold that controls the FWER for the standardized projected scores is defined by c that satisfies\nThus, the distribution of the infinity norm of \u2206P L S can be used to compute a rejection threshold for the standardized projected scores that controls the FWER. The rejection threshold that controls the FWER is c \u2208 R + such that\nThis set defines the region where the probability any element of the standardized projected score vector is greater than c is equal to \u03b1 under the global null H 0 : \u03b2 = \u03b2 0 . We reject the null hypothesis at location j if the observed projected score |(\u2206P L s) j | > c. This threshold corresponds to a single-step \"maxT\" joint multiple testing procedure [Dudoit and van der Laan, 2008] and satisfies the assumption of subset pivotality, so it controls the FWER at the nominal level in the case that some projected scores have nonzero mean [Westfall and Young, 1993] \nwhere Z \u223c N r (0, I). Thus we can approximate the region in (20) by finding c so that\nwhere \u03c6 r denotes the PDF of Z. In practice we approximate this interval by plugging in estimates for \u2206 and V 1/2 . This integral is difficult to calculate due to the large dimensions of Q, but can be approximated quickly and easily using Monte Carlo simulations. B simulations are used to estimate the CDF of the infinity norm,F B (\u00b7), which we use to obtain p-values for each observed standardized projected score, (\u2206P s) j , by evaluating\nor a rejection threshold can be obtained by using\nThe p-value for a given element of the standardized projected score vector is the probability of observing a projected score as large as (\u2206P L s) j under the global null H 0 : \u03b2 = \u03b2 0 . The standard deviation of the Monte Carlo estimate (21) decreases at a \u221a B rate and depends only on the volume of the space being integrated, so the procedure will perform well for computing adjusted p-values with a small error [Press et al., 2007] . For example, because the volume of the space being integrated is 1, with 10,000 simulations the standard deviation is on the order of B \u22121/2 = 0.01. Rejection of the null hypothesis H 0 : \u03b2 = \u03b2 0 is not strictly necessary to proceed with the post hoc inference procedure; the post hoc procedure can be used separately from the PST. In addition, it is important to note that the post hoc inference can have improved power by interpreting the projected scores. When the alternative hypothesis is true, the rejection regions for the projected scores do not necessarily control the type 1 error for the unprojected scores. This is demonstrated in the imaging simulations in Section 6.\nAs mentioned above, the basis affects the interpretation of the inference on the projected scores. For the PCA basis the interpretation is as follows: over repeated experiments if the data are projected onto L, then the probability of falsely rejecting one or more scores j with (\u2206P L \u00b5) j = 0 is at most \u03b1, where \u00b5 is as defined in (19)."}, {"section_title": "ADNI Neuroimaging Data Analysis", "text": "We obtained data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). The ADNI was launched in 2003 as a public-private partnership, led by Principal Investigator Michael W. Weiner, MD. The ADNI is a longitudinal observational study designed to investigate the early biomarkers of Alzheimer's disease; detailed MRI methods are given by Jack et al. [2008] . Mild cognitive impairment (MCI) represents a subtle pre-Alzheimer's Disease decline in cognitive performance. The goal of our analysis is to identify whether a subset of the neuroimaging data from the ADNI can provide more information regarding diagnosis of MCI than the standardized memory tests obtained as part of the study. Moreover, we are interested in localizing areas of the cortex that differ between healthy controls (HC) and individuals with MCI. Three-dimensional T1-weighted structural images for 229 healthy controls and 399 subjects with MCI were obtained as part of the ADNI. This sample consists of subjects who had images and a composite memory score available at baseline.\nWe perform the analysis in two ways: First, we proceed with standard analysis methods currently available for neuroimaging data in open access software [Fischl, 2012] . Second, we use the PST statistic and the highdimensional inference procedure described above.\nCortical thickness was estimated using Freesurfer Dale, 2000, Dale et al., 1999] . Subjects' thickness data were registered to a standard template for analysis and smoothed at 10mm FWHM to reduce noise related to preprocessing and registration. The template contains 18,715 vertex locations where cortical thickness is measured for each subject. Our goal is to identify whether the 18,715 cortical thickness measurements provide any additional information regarding the diagnosis of the individuals. For all analyses we include age, sex, and the composite memory score as covariates [Crane et al., 2012] ."}, {"section_title": "Standard Neuroimaging Analysis Procedure: Average and Vertex-wise Testing", "text": "Because neuroimaging studies typically collect many types of images with many covariates and possible outcomes, it is common to obtain a summary measure of a high-dimensional variable, and then proceed with further analysis if the summary measure appears to be associated with an endpoint of interest. In this analysis we first take the average of all the cortical thickness measurements across the cortical surface for each subject and perform a regression with diagnosis as the outcome using logistic regression. Specifically let C i denote the average cortical thickness measurement for subject i, and X i denote a vector with an intercept term, age, an indicator for sex, and the composite memory score for subject i. Then we fit the model\nIf there is a significant relationship with the average cortical thickness measurements, i.e. if we reject H 0 : \u03b2 C = 0, then we will proceed by performing mass-univariate vertex-wise analyses by running a separate model at each point on the cortical surface. The analysis using the average cortical thickness variable suggests a highly significant association of cortical thickness with diagnosis, indicating that subjects with thinner cortices are more likely to have MCI ( For the vertex-wise analyses, we use the software package Freesurfer to perform Benjamini-Hochberg (BH) correction separately across each hemisphere (Figure 1 A) . The spatial extent of the FDR-corrected results is more limited than what we might expect given the very strong association between diagnosis and average cortical thickness. Uncorrected exploratory analyses were conducted to further identify regions related to the whole-brain results (Figure 1 B) . The most significant results occur in left and right frontal lobes. These analyses suggest that thinning in larger portions of the frontal and temporal lobes is associated with increased risk of MCI; however, these results are not found using a method that guarantees control of the FWER or FDR."}, {"section_title": "PST and High-Dimensional Inference Procedures", "text": "To use the PST procedure we perform the following steps:\n1. Select a subspace L."}, {"section_title": "Perform the PST for the association between the image and diagnosis.", "text": "3. If the test in step 2 rejects, then perform post hoc inference as in Section 4.\nWe select a basis for L in the two ways described in Section 3.1. For this analysis we use the aPCA procedure described in Section 3.2 to choose the best PCA basis by testing the first 5 components together and sequentially testing components 6, . . . , n \u2212 m. We also present results for the PCA basis fixed at several other dimensions (r = 10, 20, 50) to demonstrate how the basis affects the results of the analysis. In addition we consider a basis constructed from the r = 148 regions (74 per hemisphere) of the anatomical atlas of Destrieux et al. [2010] . If we were unwilling to condition on the covariance structure of the scores or the anatomical atlas, a basis could be constructed that approximates a predetermined covariance structure (e.g. a spatial AR(1)), or a covariance structure estimated from an independent sample to be used to construct the PCA basis. In addition to the PST we perform the sequence kernel association test (SKAT) [Wu et al., 2011] , the sum of powered scores (SPU) test using the infinity norm, which corresponds to testing the max across the scores , and the adaptive sum of powered scores test (aSPU), which has competitive power to many other score tests . The SKAT is known to be more powerful if there is a distributed signal, and the SPU infinity norm will be most powerful for a sparse signal. The aSPU test combines multiple tests based on the norms S \u03b3 \u03b3 for \u03b3 varying over a finite subset of N by choosing one with the smallest p-value. Permutation testing is used to assess the significance of these statistics.\nThe aPCA basis selected r = 7 by testing for r = 5 and then sequentially testing the next two PCs. With this basis we reject the null hypothesis using the PST ( Table 2 ), indicating that there is an association between the image and diagnosis conditioning on the effects of age, sex, and composite memory score. The test rejects at the \u03b1 = 0.05 threshold irrespective of which basis is used. The SKAT, SPU, and aSPU tests also reject the null.\nGiven the results of the PST we are then interested in investigating how the scores contribute to the significant test statistic. To investigate the contributions of the scores to the PST statistic we perform post hoc inference on the projected scores. We use 10,000 simulations to obtain rejection regions for each of the basis dimensions. The simulations ran for all bases in less than 2 minutes.\nResults suggest that thinner cortex in bilateral temporal and frontal lobes and right precuneus is associated with an increased risk of MCI ( Figure  1 C & D) . Results are given as \u2212 log 10 (p) where p is obtained using the simulated distribution (21). These locations are known to be thinner in AD versus HC as well as in AD versus MCI [Singh et al., 2006] and the results here demonstrate that there are significant differences between MCI and HC in the same region. The results indicate that the degree of frontal Table 2 : The \u03c7 2 PST statistic and associated p-values for various basis dimensions; Adaptive, 10, 20, and 50. \"Anatomical\" is a basis constructed from an anatomical atlas of dimension 148. The last column denotes the 5% familywise error rejection thresholds for the projected scores, i.e. the probability any projected score is above those values under the null is 5%. The thresholds are obtained using 10,000 simulations. and temporal lobe thinning is correlated with diagnostic severity, and suggest that measurements of cortical thickness may provide useful information over neurospsychological scales in identifying people at risk for AD. Differences in these regions between MCI and HC were previously shown by Wang et al. [2009] ; however the authors did not control for multiple comparisons or adjust for covariates.\nTo reiterate, the blue areas in Figure (21) and indicate the probability of observing a projected score statistic as extreme under the global null H 0 : \u03b2 = 0. Though interpretation is restricted to the projected scores, the results align strongly with previous reports [Singh et al., 2006 , Wang et al., 2009 .\nTo demonstrate the impact of the choice of r, we performed post hoc inference on the scores for 4 different PCA bases (Figure 2) . It is clear from Figure 2 that increasing the dimension of the basis increases the spatial specificity of the results. However, the larger bases also come with the cost of reduced power due to the larger degrees of freedom of the basis. This is also illustrated in Table 2 , where the larger bases have a higher rejection r=7 r=10 r=50 r=20 3.0 1.3\nComparison of Bases for post hoc Inference -log 10 (p) Figure 2 : PST inference for PCA bases of various rank; Adaptive (7), 10, 20, 50. Increasing the dimensionality of the basis increases spatial specificity, but comes with the cost of more conservative inference (see e.g. Table 2 ).\nthreshold."}, {"section_title": "Neuroimaging-based Simulation Study", "text": "As a simulation study, we perform analyses using data generated for the right hemisphere of the cortical thickness data from the ADNI dataset measured at p =9,361 locations called vertices. We simulate an artificial outcome of interest that is categorical, as in the ADNI analyses presented above. We select two anatomical regions (superior temporal sulcus and superior frontal sulcus) of 669 vertices total to have a negative association with the outcome and one region (anterior part of the cingulate gyrus and sulcus) of 191 vertices to have a positive association. The first two of these regions were selected because of their association in the ADNI data set. The third region was selected to compare the performance of the tests when there are different locations with positive and negative associations with the outcome. To create a mean and covariance structure similar to real data within the regions of association, we create the mean vectors and covariance matrices for the simulations from the full sample of subjects used in the ADNI Freesurfer analysis above, yielding two full rank covariance matrices, \u03a3 \u2212 and \u03a3 + and mean vectors \u00b5 + and \u00b5 \u2212 .\nFor each simulation, we select a random subset of size n without replacement from the subset of control subjects used in the ADNI neuroimaging analysis. Data within the negatively and positively associated regions are generated as independent multivariate normal distributions for each subject, with covariance structures G i,\u2212 \u223c N (\u00b5 \u2212 , \u03a3 \u2212 ) and G i,+ \u223c N (\u00b5 + , \u03a3 + ), respectively. We centered the imaging data prior to analysis.\nIn each simulation the outcome is generated under a logistic model\nwhere \u03b1 0 is set to the log ratio of MCI to controls in the neuroimaging analyses section. 1, is a vector of ones, and \u03b2 is an unknown parameter that we vary from 0 to 0.005. We multiply the values in the positive region by 2 to increase signal because it is a spatially smaller cluster than the two negative regions. In addition to simulations where the coefficients are constant across each region, in the Supplement we perform simulations generating the parameters from a uniform distribution. We construct the subspace L in three ways. The first is to use the adaptive procedure (Section 3.2) in each sample conditioning on the estimate\u03b1 0 . The second basis type is constructed in each sample from the first r = 10, 20, 50 principal components from a PCA of G(I \u2212 H), where H is the projection onto the intercept. The third basis is constructed from regions of anatomical atlas of Destrieux et al. [2010] , by randomly grouping the 74 regions into r groups and using normed indicator vectors for each group as the basis.\nWe assess power for indices with nonzero mean and type 1 error for indices with zero mean. If we denote the set of indices with a nonzero association with the outcome by J, then the expectation of the score \u00b5 j is nonzero only for j \u2208 J, where \u00b5 is as defined in (19). So, for indices with j / \u2208 J we report type 1 error and for indices with j \u2208 J we report power.\nSimilarly, the mean of the standardized projected scores, \u2206P \u00b5, determines type 1 error and power for the projected scores \u2206P S. The FWER and FDR of the projected scores are reported for the basis constructed from the anatomical atlas and the PCA bases. In general, no element of the standardized projected mean is exactly zero, so type 1 error is assessed by thresholding the standardized projected parameter vector at the 0.2 quantile and reporting the rejection rate for vertices with projected parameter values below that threshold.\nWe perform 1000 simulations for sample sizes of n = 100, 200 and compare the PST for the adaptive procedure and fixed bases with dimensions of r = 10, 20, 50. In addition, we compare the PST to the sequence kernel association test (SKAT) [Wu et al., 2011] , the sum of powered scores (SPU) test using the infinity norm, and the adaptive sum of powered scores test (aSPU) . We assess pointwise power and type 1 error of the PST inference with uncorrected, Bonferroni-corrected, and BH-corrected results. We also compare FWER and FDR between methods. For these comparisons we assess the type 1 error for the unprojected scores using inference designed for the projected scores.\nThe PST with fixed bases demonstrates superior power to the other tests (Figure 3 ), due to its ability to remove the influence of unassociated scores from the test by maximizing over the basis and by leveraging the spatial information in the data. If these features of the data were not informative then the PST would not perform well. The aPCA is has better power than the other PCA bases because a low rank basis is all that is required to capture the signal in the data. aSPU is adaptive to the sparsity of the signal, so it performs better than the SKAT, but does not use the information in the covariance of the scores to leverage power.\nAs expected, the post hoc inference procedure controls the FWER of the projected scores for all basis dimensions (Table 3 ). In general, the post hoc inference procedure does not control the FWER or FDR of the unprojected scores (Table 4) as the inference is intended for the projected scores. However, for larger PCA bases our procedure does control the FDR (bold rows in Table  4 ). This is likely because the projection captures most of the variation in \u00b5, so that the projection \u2206P \u00b5 is close to \u00b5. Future investigation of whether inference for the projected scores will control any error rate for unprojected score vector is warranted.\nThe vertexwise error rate describes how effective a procedure is at controlling the error rate for the unprojected scores at each location. The vertexwise Table 3 : Error rates for the projected scores for the adaptive PCA bases and anatomical bases for n = 200 and \u03b2 = 0.002.\nerror rate of the PST inference procedure for the unprojected scores using the PCA 10 basis is low while maintaining better vertexwise power than BH (Figure 4 ; PCA 10). This is because in any given sample there may be a high false positive rate, but the errors across samples do not appear in the same locations. The BH and Bonferroni corrections both work well at controlling the vertexwise type 1 error rate but have lower power compared to the PCA-based PST procedure (Figure 4) . The bases constructed from the anatomical atlas tend to have large regions of vertexwise type 1 error for the unprojected scores. At the largest basis dimension the atlas allows for enough specificity to reduce the vertexwise error. All methods have lower power to detect the positive cluster than the two negative clusters. This is possibly due to the characteristics of the covariance structure in the positive cluster which overlaps gyral and sulcal regions."}, {"section_title": "Discussion", "text": "We have proposed the PST, which maximizes the weights for the Sum statistic in a subspace of the parameter space. The procedure offers a novel post hoc inference on the projected scores by performing inference in the subspace where the test statistic was estimated. Because the posthoc inference is based on the same model and degrees of freedom as the PST statistic, the interpretation of high-dimensional results agree closely with the results from the PST. Instead of choosing a specific value for the weight vector, \u03b6 as in the sum test, our methodology allows the investigator to select a space to consider for \u03b6. The ability to choose a space makes the procedure very flexible. For example, in imaging the basis for the space can be chosen based on anatomical or functional labels, or from data acquired in another imaging modality. Particular hypotheses can be targeted by selecting a basis that includes indicators of certain regions or weights particular locations to target specific spatial patterns. If orthogonal indicator vectors are used as the basis, then the approach can be seen as testing averages of subregions of the data as in Section 5. In this case, the PST procedure can be seen as a maxT multiple testing procedure that accounts for the correlation structure of the tests. There are several limitations of the proposed procedure. First, the success of the procedure depends critically on the projection chosen. If a projection is chosen that is orthogonal to the mean vector then the PST will fail to capture any signal in the data. This is a limitation of any dimension-reducing procedure. Further research could investigate whether maximization of the score test with regularization can yield a test statistic whose distribution is tractable. Regularization may remove the subjectivity of selecting a basis and make the procedure more robust. Second, while the dimension reduc-tion procedure preserves power and the results align closely with those from previous research, the inference does not guarantee control of the FWER or FDR of the original score vector. Future research will investigate how inference of the original score vector can be made by thresholding the projected score vector. This is similar in concept to the dependence-adjusted procedure discussed by Fan et al. [2012] for controlling the FDP and may offer increased power by leveraging the covariance of the test statistics. These limitations notwithstanding, our procedure generalizes Rao's score test to the high-and infinite-dimensional settings and introduces a new inference approach based on projecting the test statistics to a lower-dimensional space where inference can be made on fewer degrees of freedom."}, {"section_title": "A. APPENDIX A A.1. Theoretical framework", "text": "We assume the observed data are finite-dimensional representations that are generated from an underlying stochastic process. To be more specific, define the Hilbert space Y = R k \u00d7 B(V), where V is a nonempty compact subset of R 3 and B(V) is the space of square integrable functions from V to R. V represents the space on which data can be observed; in neuroimaging this space is the volume of the brain. Let (Y, Y, P) be a probability space where Y is the Borel \u03c3-algebra on Y. i are unobservable and we only observe discretized data at a finite number of locations that are voxels in the image.\nWe define a parameter space \u0398 = A\u00d7B that includes a finite-dimensional parameter \u03b1 \u2208 A \u2282 R m and an infinite-dimensional parameter on \u03b2 \u2208 B. Together these parameters describe the joint distribution of the imaging and nonimaging data. Throughout, we assume B = B(V), so that the infinitedimensional parameter and infinite-dimensional data are defined on the same space, but this assumption is not required. The distribution of the observed data will be defined by a p-dimensional discretization of the infinite-dimensional parameter. We will prove that under a few assumptions, as n, p \u2192 \u221e the test statistic for the discretized data approaches the statistic for the infinite-dimensional parameter.\nTo relate the unobserved data Y to the parameters, we further assume that the measure P is in a set of probability models, {F \u03b8 : \u03b8 \u2208 \u0398}, indexed by the parameter \u03b8 = (\u03b1, \u03b2).\nThat is, there exists a regular point \u03b8 0 \u2208 \u0398 such that for all sets A \u2208 Y\nWe define the density function f \u03b8 as the Radon-Nikodym derivative of F \u03b8 with respect to the Lebesgue measure \u00b5,\nbe the log-likelihood function for \u03b8. In order to define the finite-dimensional data and parameter space we must partition V into finitely many sets and define the observable random variables as a realization from the partitioned space. For any integer p, the space V can be partitioned into p nonempty sets. Denote the partition V p = {V 1p , . . . , V pp }. Note that, by the definition of a partition,\nk+p whose distribution is determined by the finite parameter \u03b8 p = (\u03b1, \u03b2(v 1 ), . . . , \u03b2(v p )) \u2208 R m+p . In order to define a finite-dimensional likelihood from the likelihood for the infinite-dimensional parameters we define the function \u03b2 p \u2208 B(V) by\nand the stochastic processes\nAs V p is a partition, each v is in only one V jp . This allows us to define the log-likelihood from the function\nwhere\np ); (\u03b1, \u03b2 p )). Finally, assuming is Fr\u00e9chet differentiable with respect to \u03b2, we can define the scores\nwhere \u03b2 0 denotes the value of the parameter under the null H 0 : \u03b2 = \u03b2 0 and \u03b1 is the maximum likelihood estimator for \u03b1 under the null."}, {"section_title": "A.2. Conditions for Theorem 2.2", "text": "The conclusion of Theorem 2.2 requires the asymptotic normality of the scores, which holds under the following conditions:\n1. The ability to interchange integration and expectation of the likelihood so that\n2. The score (5) A.3. Proof of Theorem 2.2\nwhere the last line follows from a standard maximization lemma [Johnson and Wichern, 2007, p. 80] . Equation (7) holds by the multivariate central limit theorem and the variance estimate of n 1/2 Q T S np i\u015d\nwhich converges to V (\u03b8 0 ) by the continuous mapping theorem becaus\u00ea\nr by the continuous mapping theorem.\nRemark. The conclusion of Theorem 2.2 implies that expression (8) does not depend on the choice of Q. This fact can also be shown directly, as follows. Consider another matrix Q * with orthonormal columns such that P = Q * Q T * , and accordingly defineV\nT * is of rank r, M is of rank r and hence invertible, so\nand thus formula (8) for the PST statistic is unchanged by substituting Q * ,V * for Q,V .\nA.4. Details for Section 2.2 \nThe following theorem [Van der Vaart, 2000, Thm 18 .14] gives conditions under which n 1/2 S n \u2192 L S, where S is a mean zero Gaussian process.\nTheorem A.2. The sequence of elements \u221a nS n converges in law to a mean zero Gaussian process S if and only if a. The sequence n 1/2 (S n (v 1 ), . . . , S n (v p )) converges in distribution in R p for every finite set of points v 1 , . . . , v p \u2208 V.\nb. For every , \u03b7 > 0 there exists a partition of V into finitely many sets\nThe second equality follows because the numerator on the right-hand side is a Riemann integral and our assumptions (11) and that S n (\u00b7; Y) is continuous for almost all Y in Theorem 2.4 guarantee that q j (v)S n (v) is integrable. The final equality follows from the continuous mapping theorem since S n \u2192 L S. For the limit\nwhere the first line applies the continuous mapping theorem to the finite dimensional vector S np and the continuity of S implies the integral exists. For both directions the limit on p requires (13), so that the volume of all voxels goes to zero. Theorems 2.2 and A.2 are needed to ensure that S np \u2192 S p and S n \u2192 S. The proof for the convergence ofV np \u2192 P V is a similar argument and relies on the assumption that the sample paths of \u2202/\u2202\u03b8 log f (Y i ; \u03b8(v)) are continuous almost everywhere, so that the Riemann integral converges.\nA.6. Proof of Theorem 2.5\nWe will ignore the term 1 \u03c3 2 in the maximization as it is constant with respect to \u03b6. Define M = GL to be the column space of W . From the definition of R\nwhere \u03b3 = GP \u03b6. Because \u03b3 must be in the subspace M since it is the column space of W and GP . The solution to the Rayleigh quotient (A6) is the solution to the largest generalized eigenvalue problem,\nwhere \u03bb max \u2208 R, and \u03b3 = 1. By letting \u03c6 = W \u03b3 the solution is equivalent to the largest eigenvalue problem\nThus, we have\nTo derive (17), note\nThe numerator and denominator of the random term on the left hand side are independent since P (I\u2212P ) = 0. So (A7) is distributed as 1+ By true null we mean that the expectation of the projected score is zero. This assumption allows us to construct rejection regions assuming all scores are true nulls, but still have strong control of the FWER, i.e. in the case that \u2206P \u00b5 = 0, where \u00b5 is defined in (19). Subset pivotality is satisfied for normally distributed statistics because the covariance of the statistics is not affected by changing the mean structure. Thus, the maximum over a subset of true nulls is not affected by the value of the mean for the other statistics. So long as the covariance estimates for the true nulls are consistent we will maintain asymptotic control of the FWER. In our post hoc inference procedure the variance estimates are consistent because (\u2206P\u03a9P \u2206) jj \u2192 P Var{(\u2206P S) j } + (\u2206P \u00b5) j (\u2206P\u03a9P \u2206) jk \u2192 P E{(\u2206P S) j (\u2206P S) k }.\nBecause (P \u00b5) j = 0 for all true nulls the variance and covariance estimates are consistent for all of true nulls."}, {"section_title": "B. Supplementary Material B.1. Simulation analyses", "text": "In addition to the simulations performed in Section 6 we performed similar simulations in which the data were generated from the model logit(EY i ) = \u03b1 0 \u2212 \u03b2\u03c9\nwhere \u03c9 1j \u223c Unif(0.5, 1.5) and \u03c9 2j \u223c Unif(1, 3). Table A1 gives FWER and FDR for the simulation analyses with uniform coefficients. Figure A1 gives power results for the additional simulations. Table A2 gives FWER and FDR for the simulations where there is no association between image and outcome. Table A2 : Supplementary simulation analysis error rates for the projected scores for the PCA and anatomical bases for n = 200 and \u03b2 = 0."}]