[{"section_title": "Abstract", "text": "We consider high-dimensional regression over subgroups of observations. Our work is motivated by biomedical problems, where disease subtypes, for example, may differ with respect to underlying regression models, but sample sizes at the subgroup-level may be limited. We focus on the case in which subgroup-specific models may be expected to be similar but not necessarily identical. Our approach is to treat subgroups as related problem instances and jointly estimate subgroup-specific regression coefficients. This is done in a penalized framework, combining an 1 term with an additional term that penalizes differences between subgroup-specific coefficients. This gives solutions that are globally sparse but that allow information-sharing between the subgroups. We present algorithms for estimation and empirical results on simulated data and using Alzheimer's disease, amyotrophic lateral sclerosis and cancer datasets. These examples demonstrate the gains our approach can offer in terms of prediction and the ability to estimate subgroup-specific sparsity patterns."}, {"section_title": "Introduction", "text": "High-dimensional regression has been well studied in the case where all samples can reasonably be expected to follow the same model. However, in several current and emerging applications, observations span multiple subgroups that may not be identical with respect to the underlying regression models. Examples abound, from disease subtypes in biomedicine to customer subsets in business applications. We are specifically motivated by biomedical problems, where sets of samples, such as disease subtypes, although related, may differ with respect to underlying biology and therefore have different relationships between covariates and a response of interest.\nThus, we focus on high-dimensional regression in group-structured settings. In particular, we consider linear regression in a commonly-encountered scenario in which the same set of p covariates or predictors is available in each of K subgroups. That is, we consider subgroup-specific linear regression problems indexed by k, each with subgroup-specific sample size n k , a response vector y k of length n k , a n k \u00d7 p feature matrix X k and a p-vector \u03b2 k of regression coefficients. The problem we address is estimating the regression coefficients \u03b2 1 . . . \u03b2 K .\nWe propose an approach to jointly estimate the regression coefficients that induces global sparsity and encourages similarity between subgroup-specific coefficients. We consider the following penalized formulation and its variant\u015d B = arg min\nwhere, B = [\u03b2 1 . . . \u03b2 K ] is a p \u00d7 K matrix that collects together all the regression coefficients, \u00b7 q denotes the q norm of its argument and \u03bb, \u03b3, \u03c4 are tuning parameters. The last term is a fusion-type penalty between subgroups; note that the difference is taken between entire vectors of subgroup-specific coefficients. An 2 fusion penalty is shown above, although other penalties may be used; in this manuscript, we also consider an 1 variant. The parameters \u03c4 k,k allow for the possibility of controlling the extent to which similarity is encouraged for specific pairs of subgroups.\nOur proposal shares similarities with both the group lasso [Yuan and Lin, 2006] and the fused lasso [Tibshirani et al., 2005] but differs from both in important ways. In contrast to the group lasso, we consider subgroups of samples or observations rather than groups of coefficients and in contrast to the fused lasso, we consider fusion of entire (subgroup-specific) coefficient vectors, rather than successive coefficients under a pre-defined ordering. Obozinski et al. [2010] showed how the group lasso could be used in subgroup-structured settings, essentially by considering the global problem and defining groups (in the group lasso sense) corresponding to the same covariate across all subgroups. This means that each covariate tends either to be included in all subgroup-specific models or none. In contrast, our approach allows subgroups to have different sparsity patterns, whilst pulling subgroup-specific coefficients together and inducing global sparsity. Our work is also similar in spirit to recent work concerning joint estimation of graphical models over multiple problem instances [Danaher et al., 2014 , Oates et al., 2014 .\nWe are motivated by emerging problems in biomedical research and specifically in personalized medicine. High-dimensional regression problems are now becoming common in this area, with several high-dimensional data types already in mainstream use. In the personalized medicine setting, samples usually correspond to individuals and the subgroups k to e.g. diseases or disease subtypes. It is increasingly clear that many disease subtypes differ in their biology [see e.g. Weinstein et al., 2013 , Akbani et al., 2014 , suggesting that relationships between covariates and responses of interest may differ between them. However, sample sizes tend to be limited, especially at the subgroup level, posing problems for the subgroup-wise strategy of solving each problem separately. On the other hand, pooling all the data together into a single regression problem may lead to severe mis-specification if the underlying subgroup-specific models do indeed differ.\nThese issues may lead to losses in terms of predictive ability and perhaps just as important in the ability to efficiently estimate subgroup-specific influences that may themselves be of interest. In contrast to simple pooling, our approach allows subgroups to have different sparsity patterns and regression coefficients, but in contrast to the subgroup-wise approach it takes advantage of similarities between subgroups.\nWe show empirical results in the context of two neurodegenerative diseases -Alzheimer's disease and amyotrophic lateral sclerosis (ALS) -and cancer (see below for full details of the applications and data). The responses concern disease progression in Alzheimer's and ALS and therapeutic response in cancer cell lines. In the Alzheimer's and ALS examples, subgroups are based on clinical factors, while in the cancer data they are based on the tissue type of the cell lines.\nAcross the three examples, data types include genetic, clinical and transcriptomic variables. We find that our approach can improve performance relative to pooling or subgroup-wise analysis. Importantly, in cases where pooling or subgroup-wise analyses do well (perhaps reflecting a lack of subgroup structure or insufficient similarity respectively) our approach remains competitive. This gives assurance that penalization is indeed able to share information appropriately in real-world examples. We emphasize that the goal of the empirical analyses we present is not to give the best predictions possible in these applications, but rather to explore joint estimation in group-structured biomedical problems."}, {"section_title": "Methods", "text": ""}, {"section_title": "Notation", "text": "Each subgroup k \u2208 {1 . . . K} has the same set of p covariates, but subgroupspecific sample size n k . Total sample size is n = K k=1 n k . For subgroup k, X k is the n k \u00d7 p feature matrix and y k the corresponding n k \u00d7 1 vector of observed responses. Subgroup-specific regression coefficients are \u03b2 k \u2208 R p .\nWhere convenient we collect all regression coefficients together in a p\u00d7K matrix B = [\u03b2 1 . . . \u03b2 K ] and accordingly we use \u03b2 j,k to denote the coefficient for covariate j in subgroup k."}, {"section_title": "Model Formulation", "text": "We seek to jointly estimate the regression coefficients B = [\u03b2 1 . . . \u03b2 K ] whilst ensuring global sparsity and encouraging agreement between subgroup-specific coefficients. We propose the criterion\n(1) and a variant with an 1 norm in the last term\nHere, \u03bb, \u03b3, \u03c4 are tuning parameters. The role of the last term is to encourage similarity between subgroup-specific regression coefficients. The special case K = 1 recovers the classical lasso (applied to all data pooled together). The tuning parameters \u03c4 k,k give the possibility of controlling the extent of fusion between specific subgroups. By default all \u03c4 's are set to unity (\"unweighted fusion\"), but they can also be set to specific values as discussed below (\"weighted fusion\"). In the above formulation, we assume that y k and X k have been standardized (at the subgroup level) so that no intercept terms are required. Note that the regularization parameters \u03bb, \u03b3 are the same across subgroups. The difference between the two variants is that the first, 2 fusion encourages similarity between subgroup-specific coefficients, while the second 1 version allows for exact equality. The 2 formulation has the computational advantage that the fusion part of the objective function becomes continuously differentiable, and the estimate of the objective function at each step can be obtained by soft-thresholding, analogously to coordinate descent for regular lasso problems. In the 1 formulation on the other hand, the fusion constraint is only piece-wise continuously differentiable, leading to a more difficult optimisation problem (see below)."}, {"section_title": "Comparison with group and fused lasso", "text": "Our formulation resembles the group lasso and fused lasso, but differs from both in important ways. The original group lasso [Yuan and Lin, 2006] was designed to consider groups of covariates within a single regression problem. Let X be the feature matrix and y the vector of responses in a standard regression problem. Letting l \u2208 {1 . . . L} index groups of covariates, the group lasso criterion i\u015d\nwhere X (l) is the submatrix of X corresponding to the covariates in group l, \u03b2 (l) the corresponding regression coefficients and \u03bb l 's tuning parameters. The penalty tends to include or exclude all members of a group from the model, i.e. all coefficients in a group may be set to zero giving groupwise sparsity.\nIn our setting, the subgroups are subsets of samples rather than covariates. Nevertheless, as shown in Obozinski et al. [2010] , one could use a group lassolike criterion for estimation in the multiple subgroup setting by forming groups l each comprising all the coefficients for a single covariate j \u2208 {1 . . . p} across all K regression problems. This encourages covariates to either be included in all the subgroup-specific models or none.\nThe fused lasso [Tibshirani et al., 2005] is also aimed at a single regression problem, but assumes that the covariates can be ordered in such a way that successive coefficients may be expected to be similar. This leads to the following criterion\u03b2 = arg min\nwhere \u03bb, \u03b3 are tuning parameters and we have assumed that the covariates are in a suitable order. The final term encourages similarity between successive coefficients. Efficient solutions for various classes of this problem exist [e.g. Hoefling, 2010 , Liu et al., 2010 , Ye and Xie, 2011 . Our approach shares the use of a fusion-type penalty, but focuses on a different problem, namely that of jointly estimating regression coefficients across multiple, potentially non-identical, problems. Accordingly, our penalty encourages agreement between entire coefficient vectors from different subgroups and does not require any ordering of covariates."}, {"section_title": "Setting the tuning parameters \u03c4", "text": "For weighted fusion, the parameters \u03c4 k,k could be set by cross-validation but this may be onerous in practice. As an alternative we consider setting \u03c4 k,k using a distance function d(k, k ) based on the covariates. The idea is to allow more fusion between subgroups that are similar with respect to d, while allowing the \u03c4 k,k to be set in advance of estimation proper. However, this assumes that similarity in the covariates reasonably reflects similarity between the underlying regression coefficients, which may or may not be the case in specific applications.\nWe consider two variants. The first sets d(k, k ) = \u00b5 k \u2212 \u00b5 k 2 where \u00b5 k , \u00b5 k are the sample means of the covariates in the subgroups k, k respectively (we assume the data have been standardized). The second approach additionally takes the covariance structure into account by using the symmetrised Kullback-\n, wher\u00ea p k ,p k are estimated marginal distributions over the covariates in the subgroups k, k respectively and KL(p q) is the KL-divergence between distributions p and q. In practice, this requires simplifying distributional assumptions. Below we use multivariate Normal models for this purpose, with the graphical lasso [Friedman et al., 2008] used to estimate the \u03a3 k 's. For both approaches, we set\nwith d max the largest distance between any pair of groups k,k (this scales \u03c4 to the unit interval)."}, {"section_title": "Optimisation", "text": "We describe a coordinate descent approach for optimising equation (1). While it is possible to derive a block coordinate descent approach for equation (2) [e.g. following Friedman et al., 2007] , this is generally inefficient for the highdimensional problems that we consider. Instead, we will describe an optimization procedure based on a proximal gradient approximation derived in Chen et al. [2010] ."}, {"section_title": "Coordinate Descent for 2 Fusion", "text": "The 2 fusion penalty is continuously differentiable and we can obtain the optimal value for\u03b2 j,k in equation (1) at each step by first calculating optimal values without the lasso penalty:\nThen\u03b2 j,k can be obtained by soft-thresholding on\u03b2 * k,j . The procedure is summarized in Algorithm 1."}, {"section_title": "Algorithm 1 Block Coordinate Descent", "text": "while not converged AND i < n iter do"}, {"section_title": "5:", "text": "for all j in 1:P do 6: \u03b2 temp j,1:K \u2190 DescentUpdateL2(X, Y , \u03b2, j, \u03b3, \u03c4 ) using eq. (5) 7:\nWhile Algorithm 1 is easy to understand and implement, a naive implementation in most programming languages will be still be slow due to the need for an inner for-loop over p, where p can be in the tens of thousands for the kinds of problems we will consider. In order to efficiently optimize B, we reformulate (1) as a classical lasso problem and apply the glmnet software [Friedman et al., 2010] . We transform the sum in first part of the objective into matrix form y f lat \u2212 X diag b f lat by defining X diag as a block-diagonal n \u00d7 pK matrix with X k along the diagonals. The vector b f lat is a flattened version of B with stacked \u03b2 k vectors, and similarly for y f lat . So we have:\nNow we move the 2 fusion penalty into the first squared term by defining the augmented matrix X aug diag , and augmented vector y aug f lat , such that\nwhere\nwith \u0393 a pK(K \u22121)/2\u00d7pk matrix encoding the pair-wise fusion constraints, and 0 a pK\n, k < k of p rows of \u0393 corresponds to the fusion constraint between two coefficient vectors \u03b2 k and \u03b2 k , with:\nWe can see that (6) is a classical lasso problem, to which glmnet can be directly applied.\nCompute \u2206f L1 (W i ) according to (11).\n6:\ni \u2190 i + 1"}, {"section_title": "Proximal-Gradient Approach for Fused L1 Penalty", "text": "Optimising equation (2) by block gradient descent, while possible, is highly inefficient due to having to deal with the discontinuities in the objective function space. In Chen et al. [2010] , the authors describe a proximal relaxation of this problem that introduces additional smoothing to turn the objective function f L1 (B) into a continuously differentiable function f \u00b5 L2 (B). Chen et al. deal with the multi-task regression setting (with common X for each task); it is straightforward to adapt their procedure for the subgroup regression setting with different X k per subgroup.\nIt is notationally convenient to first introduce a graph formulation of the fusion penalties. We will think of the fusion constraints in terms of an undirected graph G = (V, E) with vertex set V = {1 . . . K} corresponding to the subgroups and edges between all vertices. Then the 1 penalised objective function can be written as:\nwhere the last term includes both sparsity and fusion penalties, via the matrix C = (\u03bbI K , \u03b3H), with I K the identity matrix of size K, C a K \u00d7 |E| matrix (|E| = K 2 in this case) and\nif e = (m, l) and k = l. Note that unlike in Chen et al. [2010] , we require the explicit sum over k in the objective to account for different sample sizes N k in different groups 1 . The graph formulation allows for zero edges by setting \u03c4 k,k to zero. We have implicitly assumed in the formulation of (1) and (2) that the relationship between subgroups is represented by an undirected graph. However, (8) is completely general, and it would be straightforward to incorporate a directed graph in our model. We have not pursued this avenue here, as there is no reason to suspect directionality in the subgroup relationships for the applications we consider below, and including directionality would double the number of tuning parameters \u03c4 k,k that need to be considered.\nFollowing Chen et al. [2010] , we can introduce an auxiliary matrix A \u2208 Q = {A | A \u221e \u2264 1, A \u2208 R p\u00d7(K+|E|) }. Because of duality between 1 and \u221e , we can write BC 1 = max A \u221e\u22641 A, BC . A smooth approximation of BC 1 is then obtained by writing:\nwhere \u00b5 is a positive smoothness parameter, and d(A) \u2261 1 2 A 2 F , with \u00b7 F the Frobenius norm. They show that for a desired accuracy , we need to set \u00b5 = p(K+|E|) . Theorem 1 in Chen et al. [2010] gives the gradient of f \u00b5 (B) as \u2206f \u00b5 (B) = A * C T , where A * is the optimal solution of (9). Replacing BC 1 by f \u00b5 (B) in equation (8), we obtai\u00f1\nwhich is now continuously differentiable with gradient\nChen et al. further show that A * = S(BC/\u00b5) where function S truncates each entry of A * to the range [-1,1] to ensure that A * \u2208 Q. An upper bound L U of the Lipschitz constant L can be derived as:\nwhere \u03bb max (M ) is the largest eigenvalue of M and\n. With the derivation of the gradient in (11) and the Lipschitz bound in (12), we can now apply Nesterov's method [Nesterov, 2005] for optimizing (10). The procedure is summarized in Algorithm 2. For more details on the proximal approach see Chen et al. [2010] ."}, {"section_title": "Algorithm 2 Proximal Gradient Optimization", "text": "while not converged AND i < n iter do"}, {"section_title": "Simulation Study", "text": "To test the performance of the proposed approach, we simulated data from a model based on characteristics of a recent cancer dataset, the Cancer Cell Line Encyclopedia [CCLE; Barretina et al., 2012] . We treat cancer types as subgroups. To simulate data, we first estimated means and covariance matrices \u00b5 k , \u03a3 k for each of K = 9 subgroups (the eight cancer types with the latest sample sizes in CCLE plus a ninth for all other cancer types; covariances were estimated using the graphical lasso). For each group k, we then sampled covariates from the multivariate normal N (\u00b5 k , \u03a3 k ). For a given total sample size n, subgroup sizes were consistent with those in the original data. We used a random subset of 200 gene expression levels (i.e. the dimensionality was fixed at p = 200). This parametric approach allowed us to vary sample sizes freely, including the case of total n larger than in the original dataset. The set-up is intended to roughly reflect the correlation structure of the covariates, but we do not expect it to capture all aspects of the real data. We are interested in the situation in which it may be useful to share information between subgroups. But we are also interested in investigating performance in settings that do not agree with our model formulation (the extreme cases being where subgroups are either entirely dissimilar or identical). Let V = {1 . . . K} be the set of subgroup indices (here, K = 9). We set regression coefficients to be identical in a subset V 0 \u2286 V of the subgroups, such that the size K 0 = |V 0 | of the subset governs the extent to which fusion could be useful. Specifically, if K 0 = K, all subgroups have the same regression coefficients (i.e. favoring a pooled analysis using a single regression model) and at the other extreme if K 0 = 1 all groups have differently drawn coefficients. Intermediate values of K 0 give differing levels of similarity.\nFor a given value K 0 , we defined membership of V 0 by considering the differences between the subgroup-specific models for the covariates. Specifically, we choose the K 0 groups that minimized the sum of symmetrised KL divergences between subgroup-specific models. A coefficient vector was then drawn separately for each subgroup k / \u2208 V 0 and one, shared coefficient vector drawn for all k \u2208 V 0 . Each draw was done as follows. We first sampled a binary vector b of length p from a Bernoulli, i.e. b i \u223c Bernoulli(0.1). Then we drew \u03b2 i \u223c N trunc (0, 1) if b i = 1 and set \u03b2 i = 0 otherwise, where N trunc (0, 1) denotes a standard Normal with the interval (\u22120.1, 0.1) excluded (this is to ensure non-zero coefficients are not very small in magnitude). Note that in the case of K 0 = 1, all groups have separately drawn coefficients and the between-subgroup KL divergence plays no role.\nWe compare our approaches with pooled and subgroup-wise analyses. These are performed using classical lasso (we use the glmnet implementation) on respectively the whole dataset or each subgroup separately. Figure 1 shows performance when varying the number K 0 of subgroups with shared coefficients, with the total number of samples fixed at n = 250. Here, a smaller value of K 0 corresponds to less similarity between subgroup-specific coefficients in the underlying models. At intermediate values of K 0 the fusion approaches offer gains over pooled and subgroup-wise analyses. This is because the pooled analyses are mis-specified due to the inhomogeneity of the data, while the subgroup-wise analyses, although correctly specified, must confront limited sample sizes since they analyze each subgroup entirely separately. In contrast, the fusion approaches are able to pool information across subgroups, but also allow for subgroup-specific coefficients. Importantly, even at the extremes of K 0 = 1 (separately drawn coefficients for each subgroup) and K 0 = 9 (all subgroups have exactly the same coefficients), the fusion approaches perform well. This demonstrates their flexibility in adapting the degree of fusion. Figure 2 shows performance as a function of total sample size n. Here, the number of subgroups with identical coefficients is fixed at K 0 = 4. This gives a relatively weak opportunity for information sharing, since 5/9 groups have separately drawn coefficients. Since the true \u03b2 k 's are not identical, the pooled analysis is mis-specified and accordingly even at large sample sizes, it does not catch up with the other approaches. As expected, subgroup-wise analyses perform increasingly well at larger sample sizes. However, at smaller sample sizes the fusion approaches show some gains.\nThe 1 and 2 fusion approaches seem similar in performance. Our 2 implementation leverages the glmnet package and is more computationally efficient than the 1 approach. For computational convenience, in examples below we show results from the 2 approach only."}, {"section_title": "Alzheimers disease: prediction of cognitive scores", "text": "Here, we use data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) [Mueller et al., 2005] to explore the ability of fusion approaches to estimate regression models linking clinical and genetic covariates to disease progression, as captured by cognitive test scores.\nIn 2014, ADNI made a subset of its data available for a DREAM challenge [Allen et al., 2016] and we use these data here. The dataset consists of a total of n = 767 individuals who were followed up over at least 24 months. Cognitive function was evaluated using the mini-mental state examination (MMSE). At baseline, individuals were classified as either cognitively normal (CN), early mild cognitive impairment (EMCI), late mild cognitive impairment (LMCI) or diagnosed with Alzheimer's disease (AD). These form clinically-defined subgroups for our analysis. For the present analysis, we use only genetic data (single nucleotide polymorphisms or SNPs) and clinical profile as covariates and disregard the neuroimaging data. The task is to predict the slope of MMSE scores over a 24-month period. The total number of SNPs available is \u223c 10 7 . Filtering by linkage disequilibrium reduces this to \u223c 2 \u00d7 10 6 . For computational ease, we pre-selected 20,000 of this latter group that gave the smallest residuals when regressed with the clinical variables against responses in the training set. We note that this biases our analyses, but we emphasize that our goal in this section is not biomarker discovery but comparison between approaches all using the same (pre-selected) covariates. Figure 3 shows root mean squared error (RMSE) separately for each of the four subgroups. The fusion approaches offer substantial gains compared with pooled and subgroup-wise analyses (the latter performed very badly and are not shown in the figure) . The biggest gain is for the AD subgroup. For the weighted fusion analysis the tuning parameters \u03c4 k,k were set using the distance between the means of each subgroup (in the space of genetic and clinical variables). Weighting did not appear to improve performance. Figure 4 shows scatter plots of predicted MMSE slopes versus the true slopes. The predictions shown were obtained in a held-out fashion via 10-fold crossvalidation (CV), as were the RMSE and Pearson correlations shown. Figure 5 shows a comparison of the estimated regression coefficients themselves. The subgroup-wise approach is much sparser than the other methods, likely due to the fact that it must operate entirely separately on each (relatively small-sample) subgroup. The pooled approach finds more influential variables but obviously there is no subgroup-specificity. The fused approach selects more variables than the subgroup-wise analysis, but there are many instances of subgroup-specificity in the estimates. to improve readability."}, {"section_title": "Prediction of therapeutic response in cancer cell lines", "text": "The Cancer Cell Line Encyclopedia [CCLE, Barretina et al., 2012 ] is a panel of 947 cancer cell lines with associated molecular measurements and responses to 24 anti-cancer agents. Here, we use these data to explore group-structured regression. We treat the area above the dose-response curve as the response and use expression levels of \u223c 20,000 human genes as covariates. We treat the cancer types as subgroups k. After discarding cell lines with missing values, we arrive at n \u223c 500 samples. shows results broken down by subgroup for two examples (responses PD-0332991 and PLX4720). In the former case, the fusion approaches largely outperform pooled and subgroup-wise analyses. In the second, pooled is the best performer, although the fusion approaches are similar in most subgroups."}, {"section_title": "ALS: prediction of disease progression", "text": "Amyotrophic lateral sclerosis (ALS) is an incurable neurodegenerative disease that can lead to death within three to four years of onset. However, about ten percent of patients survive more than 10 years. Prediction of disease progression remains an open question. We use data from the PROACT database, specifically data that were used in the 2015 DREAM ALS Stratification Prize4Life Challenge (data were retrieved from the PROACT database on 22/06/2015). Our aim is not to optimize predictive performance per se but rather to provide a case study exploring the use of fusion approaches in a moderate-dimensional, clinical data setting. In contrast to the Alzheimers example above, here the data are less high-dimensional and the subgrouping less clear cut (see below).\nThe data consist of observations from n = 2, 393 patients. Each patient was enrolled in a clinical trial and followed up for a minimum of 12 months after the start of the trial. Disease progression is captured by a clinical scale called the ALS Functional Rating Scale (ALSFRS). The task is to predict the slope of the ALSFRS score from 3 to 12 months (after the start of the trial). For each patient, available covariates include ALSFRS scores for the 0-3 month period, demographic information and longitudinal measurements of clinical variables. We follow the featurization and imputation procedures devised by Mackay [see K\u00fcffner et al., 2015] and obtain a total of p = 615 covariates.\nSubgroups were defined as follows. The first subgroup consists of patients with disease onset before the start of the trial. The second subgroup consists of patients for whom onset was after the start of the trial and who have negative ALSFRS slope. The third subgroup of patients also had onset after the start of the trial but positive ALSFRS slope. Thus, the subgroups reflect severity of onset. As we believe that the pre-trial onset group are likely to differ most from the others, we manually set the distance between groups 1 and the other two groups to 1 (the maximum), and set the distance between groups 2 and 3 to 0.1. Figure 8 shows (held-out) RMSEs by subgroup; we see that the largest improvement in prediction performance is in subgroup 1. The fusion approach leads to a modest improvement. The difference between weighted and unweighted fusion is negligible 2 . "}, {"section_title": "Conclusions", "text": "Many biomedical datasets are heterogenous, spanning multiple disease types (or other biological contexts) that are related but also expected to have specific underlying biology. This means that large datasets are often usefully thought of as comprising several smaller datasets, that have similarities but that cannot be assumed to be identically distributed. Statistically efficient regression in these group-structured settings requires ways to pool information where useful to do so, whilst retaining the possibility of subgroup-specific parameters and sparsity patterns. We proposed a penalized likelihood approach for high-dimensional regression in the group-structured setting that provides group-specific estimates with global sparsity and that allows for information sharing between groups. In any given application, even when there are good scientific reasons to suspect differences in regression models between subgroups, it is hard to know in advance whether the nature of any differences is such that a specific kind of joint estimation would be beneficial. For example, if sample sizes are small and groups only slightly different, pooling may be more effective, or if the groups are entirely different, fusion of the kind we consider may not be useful. This means that in practice, either simple pooling or subgroup-wise analysis may be more effective than fusion. In our approach, the tuning parameter \u03b3 (set by crossvalidation) determines the extent of fusion in a data-adaptive manner, and we saw in several examples that this appears successful in giving results that are at worst close to the best of pooling and subgroup-wise analyses. For settings with widely divergent n k 's, it may be important to allow tuning parameters to depend on n k (we did not do so) and to consider alternative formulations that allow for asymmetric fusion.\nAn appealing feature of our approach is that it allows for subgroup-specific sparsity patterns and parameter estimates that may themselves be of scientific interest. We discussed point estimation, but did not discuss uncertainty quantification for these subgroup-specific estimates. A number of recent papers have discussed significance testing for lasso-type models [see e.g. Wasserman and Roeder, 2009 , Lockhart et al., 2014 , St\u00e4dler and Mukherjee, 2016 and we think some of these ideas could be used with the models proposed here."}, {"section_title": "Software Availability", "text": "The R code used for the experiments in this paper has been made available as R package fuser on GitHub: https://github.com/FrankD/fuser. Scripts for reproducing the results in this paper can be obtained at: http://fhm-chicas-code. lancs.ac.uk/dondelin/SubgroupFusionPrediction. "}]