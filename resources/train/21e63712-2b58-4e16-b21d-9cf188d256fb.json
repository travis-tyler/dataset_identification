[{"section_title": "i", "text": ""}, {"section_title": "EXECUTIVE SUMMARY", "text": "Starting in 2009, national nonresponse propensity score models were used for the Quarterly Agricultural Survey (QAS) to identify likely nonrespondents. Initially, these scores were given to Field Offices (FO) for use at their own discretion. Over time, FOs were given guidelines on how to use the scores in their data collection efforts. In 2012, the national-level model was replaced with state-specific models to give more accurate nonresponse propensity scores."}, {"section_title": "Two questions drove this research:", "text": "\uf0b7 Can the models identify potential nonrespondents in the QAS? \uf0b7 Can standardized data collection procedures be developed that use the nonresponse scores to increase response rates for the QAS? Structured data collection protocols were developed and tested in September and December 2012 for the QAS. The data collection protocols used the nonresponse propensity scores in conjunction with strata to assign data collection method codes. Nine treatment states used the new data collection protocols in September and six states in December to test whether the protocols increased response rates. Non-participating states served as the comparison states. Using the overall response rates from the survey, we determined that the models do identify potential nonrespondents for the QAS, identifying both refusals and inaccessibles well. Results for increasing response rates were mixed. Predicted response rates were compared to actual response rates for the treatment and comparison states. The results showed the treatment states were more effective at targeting refusal operations than the comparison states, but the converse was true for inaccessible operations. Interpreting the differences in response rates can be misleading, however, since non-treatment states could also have used the same data collection methods as prescribed by the treatment, and treatment states did not always use the prescribed methods, making it difficult to evaluate specific treatments. Note: The name of the Quarterly Agricultural Survey changed to the Crops Acreage and Production Survey (APS) after this research was completed, but for the purposes of this report, the original name will be used."}, {"section_title": "RECOMMENDATIONS", "text": "1. Continue to use nonresponse propensity scores for the Quarterly Agricultural Survey (QAS). 2. Continue to develop and evaluate data collection method procedures to increase response rates for both refusals and inaccessibles/noncontacts. The procedures tested in this study were inconclusive, so more work on effective ways to increase response rates, while containing costs, is needed. 3. Expand the use of nonresponse propensity scores to other surveys. Other surveys may find they are good predictors of nonresponse, and be able to develop tailored data collection strategies to specific subgroups.  ................................................................................................... 7 3.1.1 September Results .................................................................................................. 8 3.1.2 December Results ................................................................................................ 10 "}, {"section_title": "3.2", "text": "Can we develop standardized data collection procedures that use the nonresponse scores to increase response rates for our surveys by targeting potential nonrespondents? . 14 "}, {"section_title": "..C-1 INTRODUCTION", "text": "Survey nonresponse has been a growing concern for many years. Survey response rates are declining in both public and private sectors. Approaches to reduce survey nonresponse include using face-to-face interviews, sending a personalized letter, or using incentives (Dillman, 1978;Groves and Couper, 1998). Calibration weighting is one of the ways to adjust for survey nonresponse (Earp et al, 2008;Kott, 2005). In 2009, NASS started developing a technique to proactively identify likely nonrespondents by using classification trees. Previous research by McCarthy and Jacob (2009) and McCarthy, Jacob, and McCracken (2010) modeled survey nonresponse for the Quarterly Agricultural Survey (QAS) with classification trees. Earp and McCarthy (2011) used classification trees to identify likely nonrespondents for the Agricultural Resource Management Survey Phase III (ARMS III). Identifying likely nonrespondents before data collection allows the use of more effective data collection strategies to maximize response, reduce nonresponse bias, and contain costs. This paper describes an evaluation of a set of field offices using standardized procedures compared to a set of field offices using a variety of data collection protocols. This project involved the development and field testing of a set of standard procedures used by selected states, but because this was done as was part of the operational data collection, the other states continued using the procedures they had in place. This evaluation was not set up as a controlled experiment, but we can compare the actual response rates to the predicted response rates for our selected states and for all the other states. We did not examine or evaluate how targeting these operations could impact or bias estimates from the QAS. Our research questions were: \u2022 Can models identify potential nonrespondents in the QAS? \u2022 Can standardized data collection procedures be developed that use the nonresponse scores to increase response rates for the QAS? Data mining is a machine-learning technique that is able to handle large datasets like the one used for this study. Within data mining, classification (or decision) trees are used to predict the outcome of a binary variable, such as survey response/nonresponse, from auxiliary data. The primary objective of classification trees is classification of groups (in our case respondent/nonrespondent operations). Classification trees are non-theoretical and completely data driven; no hypothesis is proven or disproven, the only concern is how well they can classify into groups (survey response/nonresponse in this case). Logistic regression is another common technique used to predict the outcome of a binary variable. However, classification trees are preferred to logistic regression in this situation for multiple reasons. First, in logistic regression, operations with missing data are usually removed from the analysis, whereas classification trees retain such operations. Especially in cases where we are interested in survey response/nonresponse, missing values can be indicative of what we are predicting. Second, a limited number of predictors can be used in a regression analysis. Our data have over 60 predictor variables. It is not ideal to use all of these variables (and possible interactions) in a regression model but all of these variables can be used in a classification tree analysis. By using classification trees, we do not have to specify the variables in the model beforehand; it automatically detects significant relationships and interaction effects without prespecification which reduces the risk of selecting the wrong variables or other model specification errors (Schouten, 2007). Logistic regression requires pre-specification of the variables to include in the model which leaves the user at risk for selection bias, inclusion of the wrong variables, and other issues. Also, the more variables we include in our model, the harder it is to avoid issues of multicollinearity in logistic regression. This is less of a problem for classification trees (Phipps & Toth, 2012). Variables that do not predict survey response/nonresponse will simply be dropped from the classification tree model. Once developed, the models identified likely respondents and nonrespondents. This allowed the possibility for targeted data collection methodology to use limited funds effectively. For example, for likely respondents, cheaper data collection methods, such as mail or telephone, may be used. This allows saving more costly personal interviews for likely refusals and inaccessibles (i.e., noncontacts). In addition, knowing the likelihood of specific operations to respond could help during data collection by allowing changes to data collection strategies part way through the data collection period. For example, near the end of data collection, changing the mode or interviewer for the remaining records that are more likely to respond may positively impact response rates and data collection costs. Widely used data collection methods that effectively solicit response include the use of incentives, personal enumeration, and multiple calling attempts on different days and times. These methods can be tailored to specific operations based on their relative likelihood of responding. Managing these data collection methods can help contain monetary costs while attempting to increase (or maintain) response rates."}, {"section_title": "METHODS", "text": "McCarthy and Jacob (2009) and McCarthy, Jacob, and McCracken (2010) developed national nonresponse models for NASS surveys using classification trees. These trees have been used since their creation to identify and flag likely nonrespondents for the QAS. Records are assigned a rank order \"refusal\" score ranging from one to four and an \"inaccessible\" score ranging from one to five. Records with refusal/inaccessible scores of one are the most likely to be a refusal/inaccessible, and those with a score of four or five are the least likely to be a refusal/inaccessible."}, {"section_title": "Targeted Data Collection", "text": "As mentioned earlier, targeting data collection for records (i.e., operations) based on their propensity to respond can possibly help increase response rates and/or contain data collection costs. These gains can be maximized if the operations' impact on the survey results are also taken into account. In addition to possible gains in response rates and decreased costs, managing data collection based on an operation's impact on the survey results could help reduce nonresponse bias. The distribution of agricultural production across operations is often highly skewed, with some operations contributing a large percentage of a particular commodity. With this in mind, we wanted to identify the impact an operation has on survey estimates so we could use our resources to maximize response rates for those operations that cover the most production. The QAS uses a multivariate probability proportional to size (MPPS) sample design. However, sampled operations are also assigned to strata, which are used in post data collection for itemlevel imputation and nonresponse weighting adjustments. Strata definitions vary by state, but generally indicate either gross measures of the operation's size or presence of significant (or rare) commodities within the state. The strata value was used to determine the impact of an operation on the QAS estimates, which has advantages and disadvantages. Strata are readily available on the NASS sampling frame for all operations. Because strata are clearly defined within each state, no development time was needed to use them. However, the strata differ by state, making cross-state comparisons difficult. Also, specialty strata that target operations with control data for commodities important to state level estimates such as potatoes, cherries, or pineapple differ by state, so they cannot uniformly help identify operations that have a high impact on a particular commodity across states. In addition, the QAS strata only take into account the value of sales or the acreage for a commodity, but no other variables. Given the pros and cons, as well as time and resource constraints, strata were chosen as the best indicator of impact for this study. The largest operations are assigned strata values of 90 and above, while specialty operations are assigned values of 70-79 in the states that use them. NASS assigns each operation a data collection method code (DCM) to control data collection. The DCM codes used for the September and December 2012 QASs were: (1) Mail only, (2) Mail with data collection center (DCC) or National Operations Center (NOC) phone followup, (3) Field Office (FO) handling, with cases sent to field enumerators to call first and then attempt in-person, (4) Mail by print mail center (PMC) with FO follow-up, (5) Office hold, and (6) Coordinated surveys. For the September 2012 QAS, nine treatment states (KS, NE, ND, MD, NH, FL, IA, MI, SC) followed our instructions using the strata (our proxy for the impact of the operation), the propensity scores, and other variables to assign DCM codes. For the December 2012 QAS, six of the nine original states (KS, NE, ND, MD, NH, FL) continued to use the specific instructions. The remaining states were the comparison states. For both the September and December 2012 QASs, high impact operations were defined as operations in strata 90 or above (typically very large producers in terms of value of sales) or in specialty commodity strata. Although only the treatment states were required to use our method code assignment criteria, nothing prohibited the comparison states from following a similar protocol. The DCM assignment criteria are shown in Tables 1 and 2 for September and December. Based on state office feedback about their experience with the procedures in September, changes were made to the instructions for December, as reflected in Table 2. During the September study, treatment states gave feedback on the data collection protocol resulting in slight changes to the instructions. The modifications for December included: specialty commodities were assigned to personal visit (PV); EO strata and highly likely to refuse/be inaccessible can be mailed first; disconnected phone numbers were considered; and records with an existing appointment were held in the DCC during call-out. These changes are shown in Table 2 in italics. See Appendix A for the memoranda and specifications sent to the field offices each quarter. Measures of impact and the likelihood of responding were used to guide data collection methods in an effort to maximize response and coverage while minimizing costs. Hence the farms most likely to be nonrespondents and the high impact farms were targeted for field enumeration by the field offices (DCM codes = 3 and 4). This mode of data collection has the highest response rates, but also the highest cost. Cases that were sent for field enumeration were completed by field enumerators, either on the phone or in person. These cases were not sent to the DCC or the NOC, but were called or visited in person by field enumerators. Often, enumerators are familiar with the local farms, particularly large ones, because those are interviewed for multiple surveys. Therefore, field enumerators may be better equipped to perform the data collection. In general, operations that were not as important to the estimates and operations that were highly likely to respond were mailed questionnaires and had phone follow-up. This method of data collection is much cheaper than field enumeration, so it was a good choice for 'easier' (likely to respond) operations and those that had less potential impact on the estimates. Towards the end of data collection, state offices asked the NOC or DCC to send some of the cases that were not yet complete back to the state office for one last attempt at getting responses. NASS refers to this process as \"callout.\" During callout for the September and December 2012 QASs, treatment states took back operations that were highly likely to respond and had the most anticipated impact on the estimates. These were operations in strata 90 or above and specialty strata, if applicable (70-79), with a nonresponse propensity score of 4 for refusals or 5 for inaccessibles. There cases were returned to the state offices for two reasons. First, they were fairly likely to respond. The notion was that a state office may have information that could help obtain a response from these impact operations. Second, these cases were in the defined impact group, so they were most likely to impact at least one estimate for the state. The offices may also have field enumerators who were particularly good at obtaining response in-person or who knew more information about the operation."}, {"section_title": "Results", "text": ""}, {"section_title": "Can models identify potential nonrespondents in the Quarterly Agricultural Survey (QAS)?", "text": "To address this question, response rates were compared across the nonresponse propensity groups for the September and December 2012 QASs. The rates for the treatment and comparison states were similar, so only the overall response rates are shown in this section. The separate response rates for the treatment states and the comparison states are shown in Appendix B for informational purposes."}, {"section_title": "September Results", "text": "Tables 3 and 4 and Figure 1 show the overall response rates for records flagged as highly likely nonrespondents, both refusals and inaccessibles, within the nonresponse propensity groups for the September 2012 survey. Again, separate response rates for the treatment states and the comparison states are shown in Appendix B.   Tables 3 and 4 and Figure 1 show that the models worked fairly well at predicting nonresponse for the 2012 September Crop/Stocks survey because, in general, response rates for the records that were most likely to be refusals or inaccessible were indeed much lower than for the records that were predicted less likely to be nonrespondents. Overall response rates for operations that were predicted most likely to be nonrespondents are very low -21.06 percent for refusals and 38.94 percent for inaccessibles. The response rate increases as predicted for refusals for all propensity groups. The response rate for inaccessibles generally increases with propensity group, but decreases between operations in propensity groups 2 and 3. It is unclear why this dip occurs in response rates for these two groups. The decrease could be somewhat artificial, as the operations with a noncontact propensity score equaling 1 were the operations states focused on in the study. It is possible that this additional attention to the operations most likely to be inaccessible increased the response rate for that group. This could explain why operations in that group have a similar and higher response rate than those with scores equaling 2 and 3. High impact operations for this study were those in strata greater than or equal to 90 (the largest operations) and those in the specialty strata, i.e., strata 70-79, if applicable. Response rates were calculated for the high impact operations to see if the scores accurately identified potential nonrespondents in that group. Again, response rates for the treatment states and comparison states were done separately, but showed no difference in the response trend, so those response rates are broken out in tables in Appendix B. Tables 5 and 6 below show the response rates for the high impact operations across all states for September 2012.   The same pattern shown in Tables 3 and 4 emerge in Tables 5 and 6. In general, response rates for the high impact records that are mostly likely to be refusals and inaccessibles are indeed lower than for the high impact records that were predicted to be less likely to be nonrespondents."}, {"section_title": "December results", "text": "Tables 7 and 8 show the overall response rates for the 2012 December Crop/Stocks survey by propensity group. Appendix B shows the breakdown by the treatment and comparison states.  As in September, Tables 7 and 8 for December show the models are fairly good at predicting nonresponse. In general, response rates for the records that are mostly likely to be refusals and inaccessibles are indeed quite a bit lower than for the records that were predicted to be less likely to be nonrespondents. This is shown graphically for the December survey in Figure 2. As in September, high impact operations in December were those with strata values greater than or equal to 90 and the specialty strata (70-79). Overall, 12.28 percent of the high impact operations flagged as highly likely refusals responded to the survey and 29.26 percent of the high impact operations flagged as highly likely inaccessible responded to the survey. As seen in September, in December, the response rate for inaccessibles generally increases with propensity group, but decreases between operations in propensity groups 2 and 3. It is unclear why this dip occurs in response rates for these two groups. Tables 9 and 10 show the response rates for the high impact operations by propensity score. Appendix B shows the breakdown for the treatment and comparison states.   The same patterns shown in Tables 7 and 8 emerge in Tables 9 and 10. In general, response rates for the high impact records that were most likely to be refusals and inaccessibles are indeed lower than for the records predicted to be less likely to be nonrespondents. Tables 3-10 show that the nonresponse propensity scores predict nonresponse fairly accurately for the Quarterly Agricultural Surveys for both the entire sample and the high impact operations."}, {"section_title": "Can standardized data collection procedures that use nonresponse scores increase response rates for the Quarterly Agricultural Surveys?", "text": "As stated earlier, although the treatment states agreed to follow the data collection procedures provided, other states may also have used the same procedures (or used the same procedures for some subset of their records), but there was no systematic way to determine how similar the procedures used by other states were to the treatment procedures. Also, based on the debriefing questions with the treatment states after September 2012, the treatment procedures were very similar to those normally used during data collection to set data collection method codes. These two issues could cause effects in the treatment group to be small, and make it difficult to interpret the results comparing the treatment and comparison states."}, {"section_title": "Following procedures", "text": "Method Codes used for the September QAS were summarized for four treatment states (IA, MD, MI, and NE) for which we had Method Code data. Treatment assignments were given for Method 2 (mail with phone follow up) and Method 3 (FO handling, to be sent to field enumerator to call first, then do personal visit). Table 11 shows that the treatment protocol was often not followed.   Table 11, we can see that IA, MD, and NE assigned at least half of the operations that the procedures specified to be assigned Method Code 2 to the treatment method. All of these states used Method 3 (field enumerators) to some extent when they should have used Method 2 (mail with phone follow up). There were much fewer operations that should have been assigned to Method Code 3 compared to Method Code 2. With the exception of IA, at least 50 percent of the operations that were supposed to be assigned Method Code 3 were assigned according to the treatment procedures. Generally, data collection by mail with phone follow up does not get as high of a response as personal visit, but given the relatively high propensity to respond and relatively low impact of the identified operations, the tradeoff of response rates for costs was deemed appropriate for the treatment protocol. However, apparently field offices often disagreed and were apprehensive about using mail with phone follow up for several operations. Iowa, Maryland, and Nebraska also used data collection efforts defined by other Method Codes (not 2 or 3) to operations that should have used Method Codes 2 and 3. Operations were also identified that should have been targeted for the callout (if necessary), but since there was no Method Code for follow up procedures, we could not determine which received follow-up."}, {"section_title": "Predicted vs. Actual Response Rates", "text": "Because states have different baseline response rates, it may be misleading to directly compare any given state's response rate to another. Therefore, each state's actual response rate was compared with that state's predicted response rate (from the state level models) to determine if that state's response rate was higher or lower than predicted by the model. We used the national model to flag these records. However, in order to compare state-level response rates, we applied the model rules to each state's samples to obtain a state specific response rate predicted by the model. State level models varied in the number of levels they have (i.e., more terminal nodes). Therefore, all states have a group of highest likelihood nonrespondents (NRP=1), but the number of rank order groups after that varied by state models. Therefore, we focused this comparison on the highest likelihood group. For the operations that were the most highly likely to be refusals and the operations that were most highly likely to be inaccessible, we calculated the difference between the actual response rates and the predicted response rates for each state and averaged them together to get one number for the treatment states and one number for the comparison states. Using this calculation, the higher the positive value, the better the increase in response rate. Negative values indicate decreases in response, with larger negative numbers showing larger decreases. Table 12 shows the average difference for the treatment and comparison states. Appendix Tables C1 and C2 provide the actual and predicted response rates for each of the treatment and comparison states, respectively, along with some discussion. As shown in those appendix tables, several states in both the treatment and comparison states had better than predicted response rates, but several had lower than predicted response rates.  Table 12 shows that during the September 2012 QAS, the treatment states had better response for predicted refusals, but worse response for likely inaccessibles when compared to the comparison states. As mentioned earlier, however, the treatment effect should be small because all states could use the treatment procedures, and those treatment procedures closely matched what states already do. As in September, we also compared the actual response rates and predicted response rates for the most highly likely nonrespondents for December because of possible different baseline response rates among states. Again, although we used the national model to flag these records, we applied the model rules to each state's samples to obtain a state specific response rate predicted by the model. Also similar to September, we only compared the records in the NRP=1 (most highly likely nonrespondents) group because it was the only group that was the same across all state models. Using this calculation, the higher the positive value, the better the increase in response rate. Negative values indicate decreases in response, with larger negative numbers showing larger decreases. Appendix Tables C3 and C4 provide the actual and predicted response rates for each of the treatment and comparison states, respectively. As shown in those appendix tables, and consistent with September results, several states in both the treatment and comparison states had better than predicted response rates, but several had lower than predicted response rates.  Table 13 shows that the difference between the actual and predicted response rates were better for the treatment state refusals, but worse for inaccessibles. This is similar to September results."}, {"section_title": "CONCLUSION AND RECOMMENDATIONS", "text": "In general, the prediction models accurately identified refusals and inaccessibles for the Quarterly Agricultural Survey. Operations in categories with higher predictions of nonresponse had consistently lower response rates. Inaccessibles were a bit mixed, but generally, the same trend held. The research question regarding whether targeting the nonrespondents increases response rates is more complicated. As stated earlier, this was an evaluation of a set of field offices using standardized procedures compared to a set of field offices using a variety of data collection protocols, but was not set up as a controlled experiment. Data collection methods were developed with input from the participating states, so the treatment protocol was similar to what many states already do. The treatment data collection methods may have also been used by comparison states. Finally, the treatment protocol was not always followed. For these reasons, it is difficult to compare the results between the treatment and comparison states and to generalize the findings. Results from the September and December experiments were mixed when comparing the differences between predicted and actual response rates. For both the September and December surveys, when compared to the comparison states, the difference between the actual and predicted response rates was higher for the treatment states' predicted refusals, but lower for the predicted inaccessibles. However, improvements for refusals were small and, as discussed, the treatment protocol was not always followed, so we do not know if the treatment states were more effective at targeting the refusal operations than the comparison states. Due to the caveats already mentioned, we cannot determine whether substantial gains in response rates were attained using the standardized data collection methods developed for this research. However, the nonresponse models were shown to accurately predict nonrespondents. It is possible that different procedures for setting data collection method codes might be effective at increasing response rates. Also, it may be advantageous to create different procedures for operations that are highly likely to refuse versus those that are highly likely to be inaccessible. The procedures and methods used for these groups to increase response are likely different. Although almost all records were sent to the field, the response rate for the high impact records (those in the largest and specialty commodity strata) that were flagged as the most likely to be nonrespondents was only 10-17 percent. NASS should consider spending this money on other groups that might show better increases such as the highly likely to be inaccessible group or the groups that have refusal or inaccessible propensity scores of 2 or 3. For example, it may be better to target operations with propensity to refuse scores of 2 or 3 (i.e., medium likely to refuse) with Field Office special handling since it may be easier to gain their cooperation. One area that still needs additional work is defining high impact operations. For this study, high impact only involved operations in strata 90 or above and specialty commodities, if applicable. However, other criteria may better identify high impact operations. For example, certain control data may more accurately indicate an operation's impact on a particular estimate. This could be taken into account up front to classify operations into impact groups. NASS should look into what variables would be the best impact indicators based on the variables of interest. NASS should continue to develop standardized data collection procedures that target nonrespondents by exploring alternatives to the procedures used here. Ideas include analyzing the relationship between response rates, mode, and nonresponse score; targeting the operations that are somewhat likely to be nonrespondents; and considering special data collection procedures for the largest farms. We could not make clean comparisons of the treatment states and comparison states for several reasons. The main reason is that the data collection procedures across state offices or regional offices are not standardized. In most cases, we did not know what procedures state offices used, but only what we asked them to do. It is difficult to evaluate new or different procedures if it is unclear exactly what is being compared. Also, as is often the case during research projects that use production samples and data, we could not completely separate the treatment and comparison state's procedures or enforce the procedures in the treatment states and regions. If we want to evaluate changes to procedures, we need to move as an agency to standardized procedures that can be modified for specific states or regions for testing. NASS is currently undergoing that process. While this study was ongoing, Research and Development Division developed state-level models and implemented them starting in the June 2013 QAS data collection period. The state models were built using the same variables as the national models, which include demographic information, list frame variables, response history indicators, joint burden indicators, as well as QAS and Census of Agriculture variables. These models vary in size and complexity; some states have small, condensed trees while other states have much larger, expansive trees. This means that some state trees have only 1 level (only one important splitting variable) while other state trees have 5 or 6 levels. Although the models vary in size and complexity across states, the only types of variables in all the models are response history variables and joint burden indicators. Although the trees vary in size and complexity, a rank score of 1 still means that an operation is a highly likely refusal or inaccessible. The national model works well broadly, but it is possible that specific models for particular states will perform better than the national model at predicting likely refusals and inaccessibles. No evaluation has been done on the state-level models at this time. In addition, no evaluation has been done to examine the impact on estimates or nonresponse bias of using these targeted data collection techniques."}, {"section_title": "Recommendations", "text": "Based on the research done, the following are recommended: 1. Continue to use nonresponse propensity scores for the Quarterly Agricultural Survey (QAS). 2. Continue to develop and evaluate data collection method procedures to increase response rates for both refusals and inaccessibles/noncontacts. The procedures tested in this study were inconclusive, so more work on effective ways to increase response rates, while containing costs, is needed. 3. Expand the use of nonresponse propensity scores to other surveys. Other surveys may find they are good predictors of nonresponse, and be able to develop tailored data collection strategies to specific subgroups. 4. Investigate a more formal way to identify impact operations to further target operations. In five out of the nine treatment states (56 percent), actual response rates were higher than the predicted nonresponse rates for refusals, suggesting that these states were able to increase response for their likeliest refusals. Typically, the difference between the predicted and actual response rates for these states was at or below ten percent. The exception is Michigan with a difference of about +22. However, four of the states (44 percent) had a decrease in response rates, all about 10 percent or less, with the average difference for refusals of 2.78. In seven of the nine treatment states (78 percent), the actual response rates were higher than the predicted nonresponse rates for inaccessible cases, while two states (22 percent) had a decrease in predicted response rates for the inaccessibles. The average magnitude of the difference for inaccessible cases is 9.36. Appendix C State level predicted vs. actual response rates C-2 "}]