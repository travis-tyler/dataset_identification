[{"section_title": "Abstract", "text": "Abstract-Automated and accurate classification of MR brain images is extremely important for medical analysis and interpretation. Over the last decade numerous methods have already been proposed. In this paper, we presented a novel method to classify a given MR brain image as normal or abnormal. The proposed method first employed wavelet transform to extract features from images, followed by applying principle component analysis (PCA) to reduce the dimensions of features. The reduced features were submitted to a kernel support vector machine (KSVM). The strategy of Kfold stratified cross validation was used to enhance generalization of KSVM. We chose seven common brain diseases (glioma, meningioma, Alzheimer's disease, Alzheimer's disease plus visual agnosia, Pick's disease, sarcoma, and Huntington's disease) as abnormal brains, and collected 160 MR brain images (20 normal and 140 abnormal) from Harvard Medical School website. We performed our proposed methods with four different kernels, and found that the GRB kernel achieves the highest classification accuracy as 99.38%. The LIN, HPOL, and IPOL kernel achieves 95%, 96.88%, and 98.12%, respectively. We also compared our method to those from literatures in the last decade, and the results showed our DWT+PCA+KSVM with GRB kernel still achieved the best accurate classification results. The averaged processing time for a 256 \u00d7 256 size image on a laptop of P4 IBM with 3 GHz processor and 2 GB RAM is 0.0448 s. From the experimental data, our method was effective and rapid. It could be applied to the field of MR brain image classification and can assist the doctors to diagnose where a patient is normal or abnormal to certain degrees."}, {"section_title": "INTRODUCTION", "text": "Magnetic resonance imaging (MRI) is an imaging technique that produces high quality images of the anatomical structures of the human body, especially in the brain, and provides rich information for clinical diagnosis and biomedical research [1] [2] [3] [4] [5] . The diagnostic values of MRI are greatly magnified by the automated and accurate classification of the MRI images [6] [7] [8] .\nWavelet transform is an effective tool for feature extraction from MR brain images, because it allows analysis of images at various levels of resolution due to its multi-resolution analytic property. However, this technique requires large storage and is computationally expensive [9] . In order to reduce the feature vector dimensions and increase the discriminative power, the principal component analysis (PCA) was used [10] . PCA is appealing since it effectively reduces the dimensionality of the data and therefore reduces the computational cost of analyzing new data [11] . Then, the problem of how to classify on the input data arises.\nIn recent years, researchers have proposed a lot of approaches for this goal, which fall into two categories. One category is supervised classification, including support vector machine (SVM) [12] and knearest neighbors (k-NN) [13] . The other category is unsupervised classification [14] , including self-organization feature map (SOFM) [12] and fuzzy c-means [15] . While all these methods achieved good results, and yet the supervised classifier performs better than unsupervised classifier in terms of classification accuracy (success classification rate). However, the classification accuracies of most existing methods were lower than 95%, so the goal of this paper is to find a more accurate method.\nAmong supervised classification methods, the SVMs are state-ofthe-art classification methods based on machine learning theory [16] [17] [18] . Compared with other methods such as artificial neural network, decision tree, and Bayesian network, SVMs have significant advantages of high accuracy, elegant mathematical tractability, and direct geometric interpretation. Besides, it does not need a large number of training samples to avoid overfitting [19] .\nOriginal SVMs are linear classifiers. In this paper, we introduced the kernel SVMs (KSVMs), which extends original linear SVMs to nonlinear SVM classifiers by applying the kernel function to replace the dot product form in the original SVMs [20] . The KSVMs allow us to fit the maximum-margin hyperplane in a transformed feature space. The transformation may be nonlinear and the transformed space high dimensional; thus though the classifier is a hyperplane in the high-dimensional feature space, it may be nonlinear in the original input space [21] .\nThe structure of the rest of this paper is organized as follows. Next Section 2 gives the detailed procedures of preprocessing, including the discrete wavelet transform (DWT) and principle component analysis (PCA). Section 3 first introduces the motivation and principles of linear SVM, and then turns to the kernel SVM. Section 4 introduces the K-fold cross validation, protecting the classifier from overfitting. Experiments in Section 5 use totally 160 images as the dataset, showing the results of feature extraction and reduction. Afterwards, we compare our method with different kernels to the latest methods in the decade. Final Section 6 is devoted to conclusions and discussions."}, {"section_title": "PREPROCESSING", "text": "In total, our method consists of three stages:\nStep 1. Preprocessing (including feature extraction and feature reduction);\nStep 2. Training the kernel SVM;\nStep 3. Submit new MRI brains to the trained kernel SVM, and output the prediction.\nAs shown in Fig. 1 , this flowchart is a canonical and standard classification method which has already been proven as the best classification method [22] . We will explain the detailed procedures of the preprocessing in the following subsections. Figure 1 . Methodology of our proposed algorithm."}, {"section_title": "Feature Extraction", "text": "The most conventional tool of signal analysis is Fourier transform (FT), which breaks down a time domain signal into constituent sinusoids of different frequencies, thus, transforming the signal from time domain to frequency domain. However, FT has a serious drawback as discarding the time information of the signal. For example, analyst can not tell when a particular event took place from a Fourier spectrum. Thus, the quality of the classification decreases as time information is lost.\nGabor adapted the FT to analyze only a small section of the signal at a time. The technique is called windowing or short time Fourier transform (STFT) [23] . It adds a window of particular shape to the signal. STFT can be regarded as a compromise between the time information and frequency information. It provides some information about both time and frequency domain. However, the precision of the information is limited by the size of the window.\nWavelet transform (WT) represents the next logical step: a windowing technique with variable size. Thus, it preserves both time and frequency information of the signal. The development of signal analysis is shown in Fig. 2 .\nAnother advantage of WT is that it adopts \"scale\" instead of traditional \"frequency\", namely, it does not produce a time-frequency view but a time-scale view of the signal. The time-scale view is a different way to view data, but it is a more natural and powerful way, because compared to \"frequency\", \"scale\" is commonly used in daily life. Meanwhile, \"in large/small scale\" is easily understood than \"in high/low frequency\".\nThe three levels of wavelet decomposition greatly reduce the input image size as shown in Fig. 9 . The top left corner of the wavelet coefficients image denotes the approximation coefficients of level-3, whose size is only 32 \u00d7 32 = 1024."}, {"section_title": "Discrete Wavelet Transform", "text": "The discrete wavelet transform (DWT) is a powerful implementation of the WT using the dyadic scales and positions [24] . The fundamentals of DWT are introduced as follows. Suppose x(t) is a square-integrable function, then the continuous WT of x(t) relative to a given wavelet \u03c8(t) is defined as\nwhere\nHere, the wavelet \u03c8 a,b (t) is calculated from the mother wavelet \u03c8(t) by translation and dilation: a is the dilation factor and b the translation parameter (both real positive numbers). There are several different kinds of wavelets which have gained popularity throughout the development of wavelet analysis. The most important wavelet is the Harr wavelet, which is the simplest one and often the preferred wavelet in a lot of applications [25] [26] [27] . Equation (1) can be discretized by restraining a and b to a discrete lattice (a = 2 b & a > 0) to give the DWT, which can be expressed as follows.\nHere ca j,k and cd j,k refer to the coefficients of the approximation components and the detail components, respectively. g(n) and h(n) denote for the low-pass filter and high-pass filter, respectively. j and k represent the wavelet scale and translation factors, respectively. DS operator means the downsampling. Equation (3) is the fundamental of wavelet decomposes. It decomposes signal x(n) into two signals, the approximation coefficients ca(n) and the detail components cd (n). This procedure is called one-level decompose. The above decomposition process can be iterated with successive approximations being decomposed in turn, so that one signal is broken down into various levels of resolution. The whole process is called wavelet decomposition tree, shown in Fig. 3 ."}, {"section_title": "2D DWT", "text": "In case of 2D images, the DWT is applied to each dimension separately. The LL subband can be regarded as the approximation component of the image, while the LH, HL, and HH subbands can be regarded as the detailed components of the image. As the level of decomposition increased, compacter but coarser approximation component was obtained. Thus, wavelets provide a simple hierarchical framework for interpreting the image information. In our algorithm, level-3 decomposition via Harr wavelet was utilized to extract features.\nThe border distortion is a technique issue related to digital filter which is commonly used in the DWT. As we filter the image, the mask will extend beyond the image at the edges, so the solution is to pad the pixels outside the images. In our algorithm, symmetric padding method [28] was utilized to calculate the boundary value."}, {"section_title": "Feature Reduction", "text": "Excessive features increase computation times and storage memory. Furthermore, they sometimes make classification more complicated, which is called the curse of dimensionality. It is required to reduce the number of features.\nPCA is an efficient tool to reduce the dimension of a data set consisting of a large number of interrelated variables while retaining most of the variations. It is achieved by transforming the data set to a new set of ordered variables according to their variances or importance. This technique has three effects: it orthogonalizes the components of the input vectors so that uncorrelated with each other, it orders the resulting orthogonal components so that those with the largest variation come first, and eliminates those components contributing the least to the variation in the data set.\nIt should be noted that the input vectors be normalized to have zero mean and unity variance before performing PCA. The normalization is a standard procedure. Details about PCA could be seen in Ref. [10] .\nAs stated above, the number of extracted features was reduced from 65536 to 1024. However, it is still too large for calculation. Thus, PCA is used to further reduce the dimensions of features to a higher degree. The curve of cumulative sum of variance versus the number of principle components is shown in Fig. 10 . The variances versus the number of principle components from 1 to 20 are listed in Table 3 . It shows that only 19 principle components (bold font in table), which are only 1.86% of the original features, could preserve 95.4% of total variance. Table 4 . Confusion matrix of our DWT+PCA+KSVM method (Kernel chose LIN, HPOL, IPOL, and.\n(O denotes for output, T denotes for Target)"}, {"section_title": "KERNEL SVM", "text": "The introduction of support vector machine (SVM) is a landmark in the field of machine learning. The advantages of SVMs include high accuracy, elegant mathematical tractability, and direct geometric interpretation [29] . Recently, multiple improved SVMs have grown rapidly, among which the kernel SVMs are the most popular and effective. Kernel SVMs have the following advantages [30] : (1) work very well in practice and have been remarkably successful in such diverse fields as natural language categorization, bioinformatics and computer vision; (2) have few tunable parameters; and (3) training often involves convex quadratic optimization [31] . Hence, solutions are global and usually unique, thus avoiding the convergence to local minima exhibited by other statistical learning systems, such as neural networks."}, {"section_title": "Motivation", "text": "Suppose some prescribed data points each belong to one of two classes, and the goal is to classify which class a new data point will be located in. Here a data point is viewed as a p-dimensional vector, and our task is to create a (p \u2212 1)-dimensional hyperplane. There are many possible hyperplanes that might classify the data successfully. One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two classes, since we could expect better behavior in response to unseen data during training, i.e., better generalization performance. Therefore, we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized [32] . Fig. 5 shows the geometric interpolation of linear SVMs, here H1, H2, H3 are three hyperplanes which can classify the two classes successfully, however, H2 and H3 does not have the largest margin, so they will not perform well to new test data. The H1 has the maximum margin to the support vectors (S 11 , S 12 , S 13 , S 21 , S 22 , and S 23 ), so it is chosen as the best classification hyperplane [33] ."}, {"section_title": "Principles of Linear SVMs", "text": "Given a p-dimensional N -size training dataset of the form where y n is either \u22121 or 1 corresponds to the class 1 or 2. Each x n is a p-dimensional vector. The maximum-margin hyperplane which divides class 1 from class 2 is the support vector machine we want. Considering that any hyperplane can be written in the form of\nwhere \u00b7 denotes the dot product and W the normal vector to the hyperplane. We want to choose the W and b to maximize the margin between the two parallel (as shown in Fig. 6 ) hyperplanes as large as possible while still separating the data. So we define the two parallel hyperplanes by the equations as\nTherefore, the task can be transformed to an optimization problem, i.e., we want to maximize the distance between the two parallel hyperplanes, subject to prevent data falling into the margin. Using simple mathematical knowledge, the problem can be formulated as min\nIn practical situations the w is usually be replace by Homogeneous Polynomial (HPOL)\nThe reason leans upon the fact that w is involved in a square root calculation. After it is superseded with formula (8), the solution will not change, but the problem is altered into a quadratic programming optimization that is easy to solve by using Lagrange multipliers [34] and standard quadratic programming techniques and programs [35, 36] ."}, {"section_title": "Kernel SVMs", "text": "Traditional SMVs constructed a hyperplane to classify data, so they cannot deal with classification problem of which the different types of data located at different sides of a hypersurface, the kernel strategy is applied to SVMs [37] . The resulting algorithm is formally similar, except that every dot product is replaced by a nonlinear kernel function. The kernel is related to the transform \u03d5(x i ) by the equation\nThe value w is also in the transformed space, with w = i \u03b1 i \u03b3 i \u03d5(x i ). Dot products with w for classification can be computed by\n. In another point of view, the KSVMs allow to fit the maximum-margin hyperplane in a transformed feature space. The transformation may be nonlinear and the transformed space higher dimensional; thus though the classifier is a hyperplane in the higher-dimensional feature space, it may be nonlinear in the original input space. Three common kernels [38] are listed in Table 1 . For each kernel, there should be at least one adjusting parameter so as to make the kernel flexible and tailor itself to practical data."}, {"section_title": "K-FOLD STRATIFIED CROSS VALIDATION", "text": "Since the classifier is trained by a given dataset, so it may achieve high classification accuracy only for this training dataset not yet other independent datasets. To avoid this overfitting, we need to integrate cross validation into our method. Cross validation will not increase the final classification accuracy, but it will make the classifier reliable and can be generalized to other independent datasets. Cross validation methods consist of three types: Random subsampling, K-fold cross validation, and leave-one-out validation. The K-fold cross validation is applied due to its properties as simple, easy, and using all data for training and validation. The mechanism is to create a K-fold partition of the whole dataset, repeat K times to use K \u2212 1 folds for training and a left fold for validation, and finally average the error rates of K experiments. The schematic diagram of 5-fold cross validation is shown in Fig. 7 .\nThe K folds can be purely randomly partitioned, however, some folds may have a quite different distributions from other folds. Therefore, stratified K -fold cross validation was employed, where every fold has nearly the same class distributions [39] . Another challenge is to determine the number of folds. If K is set too large, the bias of the true error rate estimator will be small, but the variance of the estimator will be large and the computation will be time-consuming. Alternatively, if K is set too small, the computation time will decrease, the variance of the estimator will be small, but the bias of the estimator will be large [40] . In this study, we empirically determined K as 5 through the trial-and-error method, which means that we suppose parameter K varing from 3 to 10 with increasing step as 1, and then we train the SVM by each value. Finally we select the optimal K value corresponding to the highest classification accuracy."}, {"section_title": "EXPERIMENTS AND DISCUSSIONS", "text": "The experiments were carried out on the platform of P4 IBM with 3 GHz processor and 2 GB RAM, running under Windows XP operating system. The algorithm was in-house developed via the wavelet toolbox, the biostatistical toolbox of Matlab 2011b (The Mathworks c ). We downloaded the open SVM toolbox, extended it to Kernel SVM, and applied it to the MR brain images classification. The programs can be run or tested on any computer platforms where Matlab is available."}, {"section_title": "Database", "text": "The datasets consists of T2-weighted MR brain images in axial plane and 256 \u00d7 256 in-plane resolution, which were downloaded from the website of Harvard Medical School (URL: http://med.harvard.edu/AANLIB/), OASIS dataset (URL: http:// www.oasis-brains.org/), and ADNI dataset (URL: http://adni.loni.ucla.edu/). We choose T2 model since T2 images are of higher-contrast and clearer vision compared to T1 and PET modalities.\nThe abnormal brain MR images of the dataset consist of the following diseases: glioma, meningioma, Alzheimer's disease, Alzheimer's disease plus visual agnosia, Pick's disease, sarcoma, and Huntington's disease. The samples of each disease are illustrated in Fig. 8 .\nWe randomly selected 20 images for each type of brain. Since there are one type of normal brain and seven types of abnormal brain in the dataset, 160 images are selected consisting of 20 normal and 140 (= 7 types of diseases \u00d720 images/diseases) abnormal brain images. The setting of the training images and validation images is shown in Table 2 since 5-fold cross validation was used. "}, {"section_title": "Classification Accuracy", "text": "We tested four SVMs with different kernels (LIN, HPOL, IPOL, and GRB). In the case of using linear kernel, the KSVM degrades to original linear SVM. We computed hundreds of simulations in order to estimate the optimal parameters of the kernel functions, such as the order d in HPOL and IPOL kernel, and the scaling factor \u03b3 in GRB kernel. The confusion matrices of our methods are listed in Table 4 . The element of ith row and jth column represents the classification accuracy belonging to class i are assigned to class j after the supervised classification.\nThe results showed that the proposed DWT+PCA+KSVM method obtains quite excellent results on both training and validation images. For LIN kernel, the whole classification accuracy was (17 + Table 5 . Classification accuracy comparison of 10 different algorithms for the same MRI dataset and same number of images."}, {"section_title": "Approach from literatures", "text": "Classification Accuracy (%) DWT+SOM [12] 94 DWT+SVM with linear kernel [12] 96 DWT+SVM with RBF based kernel [12] 98 DWT+PCA+ANN [41] 97 DWT+PCA+kNN [41] 98 DWT+PCA+ACPSO+FNN [25] 98 Moreover, we compared our method with six popular methods (DWT+SOM [12] , DWT+SVM with linear kernel [12] , DWT+SVM with RBF based kernel [12] , DWT+PCA+ANN [41] , DWT+PCA+k NN [41] , and DWT+PCA+ACPSO+FNN [25] ) described in the recent literature using the same MRI datasets and same number of images. The comparison results were shown in Table 5 . It indicates that our proposed method DWT+PCA+KSVM with GRB kernel performed best among the 10 methods, achieving the best classification accuracy as 99.38%. The next is DWT+PCA+ACPSO+FNN method [25] with 98.75% classification accuracy. The third is our proposed DWT+PCA+KSVM with IPOL kernel with 98.12% classification accuracy."}, {"section_title": "Time Analysis", "text": "Computation time is another important factor to evaluate the classifier. The time for SVM training was not considered, since the parameters of the SVM keep unchanged after training. We sent all the 160 images into the classifier, recorded corresponding computation time, computed the average value, depicted consumed time of different stages shown in Fig. 11 .\nFor each 256 \u00d7 256 image, the averaged computation time on The total computation time for each 256 \u00d7 256 size image is about 0.0448 s, which is rapid enough for a real time diagnosis."}, {"section_title": "CONCLUSIONS AND DISCUSSIONS", "text": "In this study, we have developed a novel DWT+PCA+KSVM method to distinguish between normal and abnormal MRIs of the brain. We picked up four different kernels as LIN, HPOL, IPOL and GRB. The experiments demonstrate that the GRB kernel SVM obtained 99.38% classification accuracy on the 160 MR images, higher than HPOL, IPOL and GRB kernels, and other popular methods in recent literatures.\nFuture work should focus on the following four aspects: First, the proposed SVM based method could be employed for MR images with other contrast mechanisms such as T1-weighted, Proton Density weighted, and diffusion weighted images. Second, the computation time could be accelerated by using advanced wavelet transforms such as the lift-up wavelet. Third, Multi-classification, which focuses on specific disorders studied using brain MRI, can also be explored. Forth, novel kernels will be tested to increase the classification accuracy.\nThe DWT can efficiently extract the information from original MR images with little loss. The advantage of DWT over Fourier Transforms is the spatial resolution, viz., DWT captures both frequency and location information. In this study we choose the Harr wavelet, although there are other outstanding wavelets such as Daubechies series. We will compare the performance of different families of wavelet in future work. Another research direction lies in the stationary wavelet transform and the wavelet packet transform.\nThe importance of PCA is demonstrated in the discussion section. If we omitted the PCA procedures, we meet a huge search space (as shown in Fig. 10 and Table 3 , PCA reduced the 1024 dimensional search space to 19 dimensional search space) which will cause heavy computation burden and worsened classification accuracy. There are some other excellent feature transformation methods such as ICA, manifold learning. In the future, we will focus on investigating the performance of these algorithms.\nThe proposed DWT+PCA+KSVM with GRB kernel method shows superiority to the LIN, HPOL, and IPOL kernels SVMs. The reason is the GRB kernel takes the form of exponential function, which can enlarge the distance between samples to the extent that HPOL can't reach. Therefore, we will apply the GRB kernel to other industrial fields.\nThere are two different schools of classification. One is while-box classification, such as the decision-trees or rule-based models. The readers can extract reasonable rules from this kind of classifiers. For example, a typical decision tree can be interpreted as \"If age is less than 15, turn to left node, and then if gender is male, then turn to right node, and . . . \". Therefore, the white-box classifiers make sense to patients.\nThe other school is black-box classification, which means that the classifier is intuitionistic, so the reader cannot extract reasonable rules even the kind of classifiers works better and gets higher classification accuracy than the white-box classifiers. From another point of view, this kind of classifiers is really designed by \"artificial intelligence\" or \"computer intelligence\". The computer constructed the classifier using its own intelligence not the human sense.\nOur method belongs to the latter one. Our goal is to construct a universal classifier not regarding to the age, gender, brain structure, focus of disease, and the like [42] , but merely centering on the classification accuracy and highly robustness. This kind of classifier may need further improvements since the patients may need convincing and irrefutable proof to accept the diagnosis of their diseases.\nThere are literatures describing wavelet transforms, PCA, and kernel SVMs. The most important contribution of this paper is to propose a method which combines them as a powerful tool for identifying normal MR brain from abnormal MR brain. Meanwhile, we tested four kernels, and find GRB kernel as the most successful one. This technique of brain MRI classification based on PCA and KSVM is a potentially valuable tool to be used in computer assisted clinical diagnosis."}]