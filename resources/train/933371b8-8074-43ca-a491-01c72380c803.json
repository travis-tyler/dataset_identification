[{"section_title": "Abstract", "text": "The analysis and diagnosis of Alzheimer's disease (AD) can be based on genetic variations, e.g., single nucleotide polymorphisms (SNPs) and phenotypic traits, e.g., Magnetic Resonance Imaging (MRI) features. We consider two important and related tasks: i) to select genetic and phenotypical markers for AD diagnosis and ii) to identify associations between genetic and phenotypical data. While previous studies treat these two tasks separately, they are tightly coupled because underlying associations between genetic variations and phenotypical features contain the biological basis for a disease. Here we present a new sparse Bayesian approach for joint association study and disease diagnosis. In this approach, common latent features are extracted from different data sources based on sparse projection matrices and used to predict multiple disease severity levels; in return, the disease status can guide the discovery of relationships between data sources. The sparse projection matrices not only reveal interactions between data sources but also select groups of biomarkers related to the disease. Moreover, to take advantage of the linkage disequilibrium (LD) measuring the non-random association of alleles, we incorporate a graph Laplacian type of prior in the model. To learn the model from data, we develop an efficient variational inference algorithm. Analysis on an imaging genetics dataset for the study of Alzheimer's Disease (AD) indicates that our model identifies biologically meaningful associations between genetic variations and MRI features, and achieves significantly higher accuracy for predicting ordinal AD stages than the competing methods."}, {"section_title": "Introduction", "text": "Alzheimer's disease (AD) is the most common neurodegenerative disorder (Khachaturian, 1985) . In order to predict the onset and progression of AD, NIH funded the Alzheimer's Disease Neuroimaging Initiative (ADNI) to facilitate the evaluation of genetic variations, e.g., Single Nucleotide Polymorphisms (SNPs) and phenotypical traits, e.g., Magnetic Reso-nance Imaging (MRI). In addition to progression study, it is becoming important in medical studies to identify the relevant pathological genotypes and phenotypic traits, and to discover their associations. Although found in many bioinformatics applications (Consoli, Lefevre, Zivy, de Vienne, & Damerval, 2002; Hunter, 2012; Gandhi & Wood, 2010; Liu, Pearlson, Windemuth, Ruano, Perrone-Bizzozero, & Calhoun, 2009) , association studies are scarce and especially in need in the AD study.\nMany statistical approaches have been developed to discover associations or select features (or variables) for prediction in a high dimensional problem. For association studies, representative approaches are canonical correlation analysis (CCA) and its extensions (Harold, 1936; Bach & Jordan, 2005) . These approaches have been widely used in expression quantitative trait locus (eQTL) analysis (Parkhomenko, Tritchler, & Beyene, 2007; Daniela & Tibshirani, 2009; Chen, Liu, & Carbonell, 2012) . For disease diagnosis based on high dimensional biomarkers, popular approaches include lasso (Tibshirani, 1994) , elastic net (Zou & Hastie, 2005) , and group lasso (Yuan & Lin, 2007) , and Bayesian automatic relevance determination (MacKay, 1991; Neal, 1996) . Despite their wide success in many applications, these approaches are limited by the following reasons:\n\u2022 Most association studies neglect the supervision from the disease status. Because many diseases, such as AD, are a direct result of genetic variations and often highly correlated to clinical traits, the disease status provides useful yet currently unutilized information for finding relationships between genetic variations and clinical traits.\n\u2022 For disease diagnosis, most sparse approaches use classification models and do not consider the order of disease severity. For subjects in AD studies, there is a natural severity order from being normal to mild cognitive impairment (MCI) and then from MCI to AD. Classification models cannot capture the order in AD's severity levels.\n\u2022 Most previous methods are not designed to handle heterogeneous data types. The SNPs values are discrete (and ordinal based on an additive genetic model), while the imaging features are continuous. Popular CCA or lasso-type methods simply treat both of them as continuous data and overlook the heterogeneous nature of the data.\n\u2022 Most previous methods ignore or cannot utilize the valuable prior knowledge. For example, the occurrence of some combinations of alleles or genetic markers in a population are more often or less often than that would be expected from a random formation of haplotypes from alleles based on their frequencies, which is known as Linkage Disequilibrium (LD) (Falconer & Mackay, 1996) . To our knowledge, this structure has not been utilized in association discovery.\nTo address these problems, we propose a new Bayesian approach that unifies multiview learning with sparse ordinal regression for joint association study and disease diagnosis. It can also conduct nonlinear classification over latent variables (Zhe, Xu, Qi, & Yu, 2014) and find associations by incorporating the LD information as an additional prior for the SNPs data (Zhe, Xu, Qi, & Yu, 2015) . In more detail, genetic variations and phenotypical traits are generated from common latent features based on separate sparse projection matrices and suitable link functions, and the common latent features are used to predict the disease status (See Section 2). To enforce sparsity in projection matrices, we assign spike and slab priors (George & McCulloch, 1997 ) over them; these priors have been shown to be more effective than l 1 penalty to learn sparse projection matrices (Goodfellow, Couville, & Bengio, 2012; Mohamed et al., 2012) . In order to take advantage of the linkage disequilibrium, which describes the non-random association of alleles at different loci, we employ an additional graph Laplacian type of prior for the SNPs view. The sparse projection matrices not only reveal critical interactions between the different data sources but also identify biomarkers in data relevant to disease status. Meanwhile, via its direct connection to the latent features, the disease status influences the estimation of the projection matrices so that it can guide the discovery of associations between heterogeneous data sources relevant to the disease.\nTo learn the model from data, we develop a variational inference approach (See Section 3). It iteratively minimizes the Kullback-Leibler divergence between a tractable approximation and exact Bayesian posterior distributions. We extend the proposed sparse multiview learning model by incorporating the linkage disequilibrium information about SNPs in Section 4. We then employ our model to the real study of AD in Section 5. The results show that our model achieves the highest prediction accuracy among all the competing methods. Furthermore, our model finds biologically meaningful predictive relationships between SNPs, MRI features, and AD status."}, {"section_title": "Sparse Heterogeneous Multiview Learning Models", "text": "In this section, we first present the notations and assumptions, and then present the sparse heterogeneous multiview learning model."}, {"section_title": "Notations and Assumptions", "text": "First, let us describe the data. We assume there are two heterogeneous data sources: one contains continuous data -for example, MRI features -and the other contains discrete ordinal data -for instance, SNPs. Note that we can easily generalize our model below to handle more views and other data types by adopting suitable link functions (e.g., a Poisson model for count data). Given data from n subjects, p continuous features and q discrete features, we denote the continuous data by a p \u00d7 n matrix X = [x 1 , . . . , x n ], the discrete ordinal data by a q \u00d7 n matrix Z = [z 1 , . . . , z n ] and the labels (i.e., the disease status) by a n \u00d7 1 vector y = [y 1 , . . . , y n ] . For the AD study, we let y i = 0, 1, and 2 if the i-th subject is in the normal, MCI or AD condition, respectively."}, {"section_title": "Spare Heterogeneous Multiview Learning Model", "text": "To link the two data sources X and Z together, we introduce common latent features U = [u 1 , . . . , u n ] and assume X and Z are generated from U by sparse projection. The common latent feature assumption is sensible for association studies because both SNPs and MRI features are biological measurements of the same subjects. Note that u i is the latent feature for the i-th subject with dimension k. We denote the proposed Spare Heterogeneous Multiview Learning Model by SHML. In a Bayesian framework, we assign a Gaussian prior over U, p(U) = i N (u i |0, I), and specify the rest of the model (see Figure 1 "}, {"section_title": "Continuous Data Distribution", "text": "Given U, X is generated from\nis an identity matrix, and \u03b7 \u22121 I is the precision matrix of the Gaussian distribution. For the precision parameter \u03b7, we assign a conjugate prior Gamma prior, p(\u03b7|r 1 , r 2 ) = Gamma(\u03b7|r 1 , r 2 ) where r 1 and r 2 are the hyperparameters and set to be 10 \u22123 in our experiments."}, {"section_title": "Ordinal Data Distribution", "text": "For an ordinal variable z \u2208 {0, 1, . . . , R\u22121}, its value is decided by which region an auxiliary variable c falls in\nIf c falls in [b r , b r+1 ), z is set to be r. For the AD study, the SNPs Z take values in {0, 1, 2} and therefore R = 3. Given a q \u00d7 k projection matrix H = [h 1 , h 2 , ...h q ] , the auxiliary variables C = {c ij } and the ordinal data Z are generated from\nHere \u03b4(a) = 1 if a is true and \u03b4(a) = 0 otherwise."}, {"section_title": "Label Distribution", "text": "The disease status labels y are ordinal variables too. To generate y, we use the ordinal regression model based the latent representation U,\nwhere f is the latent continuous values corresponding to y, w is the weight vector for the latent features and\nNote that the labels y are linked to the data X and Z via the latent features U and the projection matrices H and G. Due to the sparsity in H and G, only a few groups of variables in X and Z are selected to predict y."}, {"section_title": "Sparse Priors for Projection Matrices and Weights Vector", "text": "Because we want to identify a few critical interactions between different data sources, we use spike and slab prior (George & McCulloch, 1997) to sparsify the projection matrices G and H. The spike and slab priors are continuous bimodal priors to model hypervariance parameters, which controls both the selection of the variable and the effective scale of choosing this variable. We apply the spike and slab prior over the weight vector w. Specifically, we use a p \u00d7 k matrix S g to represent the selection of elements in G: if s g ij = 1, g ij is selected and follows a Gaussian prior distribution with variance \u03c3 2 1 ; if s g ij = 0, g ij is not selected and forced to almost zero (i.e., sampled from a Gaussian with a very small variance \u03c3 2 2 ). We have the following prior over G:\nwhere \u03c0 ij g in \u03a0 g is the probability of s ij g = 1, and \u03c3 2 1 \u03c3 2 2 (in our experiment, we set \u03c3 2 1 = 1 and \u03c3 2 2 = 1o \u22126 ). To reflect our uncertainty about \u03a0 g , we assign a Beta hyperprior distribution:\nwhere l 1 and l 2 are hyperparameters. We set a diffuse and non-informative hyperprior, i.e., l 1 = l 2 = 1 in our experiments. Similarly, H is sampled from\nh . S h are binary selection variables and \u03c0 ij h in \u03a0 h is the probability of s ij h = 1. We assign Beta hyperpriors for \u03a0 h :\nwhere d 1 and d 2 are hyperparameters. We set d 1 = d 2 = 1 in our experiments since we have found that they are not sensitive to the final performance. Similarly for weights vector w,\nwhere p(w j |s\nw . s w are binary selection variables and \u03c0 j w in \u03c0 w is the probability of s j w = 1. We assign Beta hyperpriors for \u03c0 w :\nwhere e 1 and e 2 are hyperparameters. We similarly set e 1 = e 2 = 1 in our experiments."}, {"section_title": "Joint Distribution", "text": "Based on all these specifications, the joint distribution of our model is\nDifferent from Figure 1 , we put the conjugate prior for S w and S g into the joint distribution. Then the next step is to estimate the distributions of the latent variables and their hyperparemeters."}, {"section_title": "Model Inference", "text": "Given the model specified in the previous section, now we present an efficient method to estimate the latent features U, the projection matrices H and G, the selection indicators S g and S h , the selection probabilities \u03a0 g and \u03a0 h , the variance \u03b7, the auxiliary variables C for generating ordinal data Z, the auxiliary variables f for generating the labels y, the weights vector w for generating f and the corresponding selection indicators and probabilities s w and \u03c0 w . In a Bayesian framework, this estimation task amounts to computing their posterior distributions. However, computing the exact posteriors turns out to be infeasible since we cannot calculate the normalization constant of the posteriors based on Equation (1) . Thus, we resort to a mean-field variational approach. Specifically, we approximate the posterior distributions of U, H, G, S g , S h , \u03a0 g , \u03a0 h , \u03b7, w, C and f by a factorized distribution\nwhere \u03b8 denotes all the latent variables.\nVariational inference minimizes the Kullback-Leibler (KL) divergence between the approximate and the exact posteriors\nMore specifically, using a coordinate descent algorithm, the variational approach updates one approximate distribution, e.g, q(H), in Equation (2) at a time while having all the others fixed. The detailed updates are given in the following paragraphs."}, {"section_title": "Updating Variational Distributions for Continuous Data", "text": "For the continuous data X, the approximate distributions of the projection matrix G, the noise variance \u03b7, the selection indicators S g and the selection probabilities \u03a0 g are\nThe mean and covariance of g i are calculated as follows:\nwhere \u00b7 means expectation over a distribution,x i and s i g are the transpose of the i-th rows of X and S g , s i g = [\u03b2 i1 , . . . , \u03b2 ik ] , and g 2 ij is the j-th diagonal element in \u2126 i . The computation of parameters \u03b2 ij and Q(\u03c0 ij g ) can be found in Appendices A."}, {"section_title": "Updating Variational Distributions for Ordinal Data", "text": "For the ordinal data Z, we update the approximate distributions of the projection matrix H, the auxiliary variables C, the sparse selection indicators S h and the selection probabilities \u03a0 h . To make the variational distributions tractable, we update Q(H) in a column-wise way and re-denote\nThe computation of parameters in the distributions of S h and \u03a0 h is given in Appendices B."}, {"section_title": "Updating Variational Distributions for Labels", "text": "For the ordinal labels y, we update the approximation distributions of the auxiliary variables f , the weights vector w, the sparse selection indicators s w and the selection probabilities \u03c0 w . The variational distributions of f and w are\nThe computation of parameters in the variational distributions of s w and \u03c0 w can be found in Appendices C."}, {"section_title": "Updating Variational Distributions for Latent Representation U", "text": "The variational distribution for U is given by\nwhere\nThe required moments are given in Appendices D."}, {"section_title": "Label Prediction", "text": "Let us denote the training data as D train = {X train , Z train , y train } and the test data as \nwhere y i test is the prediction for i-th test sample."}, {"section_title": "Sparse Heterogeneous Multiview Learning Model with Linkage Disequilibrium Priors", "text": "In population genetics, lLinkage Disequilibrium (LD) refers to the non-random association of alleles at different loci, i.e., the presence of statistical associations between alleles at different loci that are different from what would be expected if alleles were independently, randomly sampled based on their individual allele frequencies (Slatkin, 2008) . If there is no linkage disequilibrium between alleles at different loci they are said to be in linkage equilibrium.\nLinkage Disequilibrium also appears in the SNPs, which is a measure between pairs of SNPs and can be regarded as a natural indicator for the correlation between SNPs. This information can be publicly retrieved from www.ncbi.nlm.nih.gov/books/NBK44495/. To incorporate such correlation as a prior in our model, we first introduce a latent q \u00d7 k matrix H, which is tightly linked to H as explained later. Each columnh j ofH is regularized by the graph Laplacian of the LD structure, i.e.,\nwhere L is the graph Laplacian matrix of the LD structure. As shown above, the prior p(H|L) has the same form as p(0|H, L), which can be viewed as a generative model -in other words, the observation 0 is sampled fromH. This view enables us to combine the generative model for graph Laplacian regularization with the sparse projection model via a principled hybrid Bayesian framework (Lasserre et al., 2006) . To link the two models together, we introduce a prior overH:\nwhere the variance \u03bb controls how similarH and H are in our model. For simplicity, we set \u03bb = 0 so that p(H|H) = Dirac(H \u2212 H) where Dirac(a) = 1 if a = 1 and Dirac(a) = 0 if a = 0. Adopting this additional information, the new graphical model is designed as shown in Fig. 2 .\nThe graphical representation of our model, where X is the continuous view, Z is the ordinal view, y are the labels and L is the graph laplacian generated by the LD structure.\nBased on all these specifications, the joint distribution of our model is\nThe inference is almost the same with the original model described in Section 2, except the updating of the sparse projection matrix H. Given the ordinal data Z and the updates of other variables, we update the approximate distributions of the projection matrix H, the auxiliary variables C, the sparse selection indicators S h and the selection probabilities \u03a0 h . The variational distributions of C and H are\nThe updating of other variables remains the same."}, {"section_title": "Experimental Results and Discussion", "text": "In order to examine the performance of the proposed method , we design a simulation study and a realworld study for Alzheimer's Disease."}, {"section_title": "Simulation Study", "text": "We first design a simulation study to examine the basic model, i.e., our model, in terms of (i) estimation accuracy on finding associations between the two views and (ii) prediction accuracy on the ordinal labels. Note that a similar study can be conducted on the model with LD priors."}, {"section_title": "Simulation Data", "text": "To generate the ground truth, we set n = 200 (200 instances), p = q = 40, and k = 5. We designed G, the 40 \u00d7 5 projection matrix for the continuous data X, to be a block diagonal matrix; each column of G had 8 elements being ones and the rest of them were zeros, ensuring each row with only one nonzero element. We designed H, the 40 \u00d7 5 projection matrix for the ordinal data Z, to be a block diagonal matrix; each of the first four columns of H had 10 elements being ones and the rest of them were zeros, and the fifth column contained only zeros. We randomly generated the latent representations U \u2208 R k\u00d7n with each column u i \u223c N (0, I). To generate Z, we first sampled the auxiliary variables C with each column c i \u223c N (Hu i , 1), and then decided the value of each element z ij by the region c ij fell in-in other words, z ij = 2 r=0 r\u03b4(b r \u2264 c ij < b r+1 ). Similarly, to generate y, we sampled the auxiliary variables f from N (0, U U + I) and then each y i was generated by"}, {"section_title": "Comparative Methods", "text": "We compared our model with several state-of-the-art methods including (1) CCA (Bach & Jordan, 2005) , which finds the projection direction that maximizes the correlation between two views, (2) sparse CCA (Sun, Ji, & Ye, 2011; Daniela & Tibshirani, 2009) , where sparse priors are put on the CCA directions, and (3) multiple-response regression with lasso (MRLasso) (Kim, Sohn, & Xing, 2009) where each column of the second view (Z) is regarded as the output of the first view (X). We did not include results from the sparse probabilistic projection approach (Archambeau & Bach, 2009 ) because it performed unstably in our experiments. Regarding the software implementation, we used the built-in Matlab routine for CCA and the code by (Sun et al., 2011) for sparse CCA. We implemented MRLasso based on the Glmnet package (cran.r-project.org/web/packages/glmnet/index.html).\nTo test prediction accuracy, we compared the proposed SHML model based on the Gaussian process prior with the following ordinal or multinomial regression methods: (1) lasso for multinomial regression (Tibshirani, 1994) , (2) elastic net for multinomial regression (Zou & Hastie, 2005) , (3) sparse ordinal regression with the spike and slab prior, (4) CCA + lasso, for which we first ran CCA to obtain the latent features H and then applied lasso to predict y, (5) CCA + elastic net, for which we first ran CCA to obtain the projection matrices and then applied elastic net on the projected data, (6) Gaussian Process Ordinal Regression (GPOR) (Chu & Ghahramani, 2005) , and (7) Laplacian Support Vector Machine (LapSVM) (Melacci & Mikhail, 2011) , a semi-supervised SVM classification method. We used the published code for lasso, elastic net, GPOR and LapSVM. For all the methods, we used 10-fold cross validation on the training data for each run to choose the kernel form (Gaussian or linear or Polynomials) and its parameters (the kernel width or polynomial orders) for our model, GPOR, and LapSVM.\nBecause alternative methods cannot learn the dimension automatically for simple comparison, we provided the dimension of the latent representation to all the methods we tested in our simulations. We partitioned the data into 10 subsets and used 9 of them for training and 1 subset for testing; we repeated the procedure 10 times to generate the averaged test results."}, {"section_title": "Results", "text": "To estimate linkage (i.e., interactions) between X and Z, we calculated the cross covariance matrix GH . We then computed the precision and the recall based on the ground truth. The precision-recall curves are shown in Figure 3 . Clearly, our method successfully recov- ered almost all the links and significantly outperformed all the competing methods. This improvement may come from i) the use of the spike and slab priors, which not only remove irrelevant elements in the projection matrices but also avoid over-penalizing the active association structures (the Laplace prior used in sparse CCA does over penalize the relevant ones) and ii) more importantly, the supervision from the labels y, which is probably the biggest difference between ours and the other methods for the association study. The failing of CCA and sparse CCA may be due to the insufficient representation of all sources of data caused by using only one projection direction. The prediction accuracies on unknown y and their standard errors are shown in Figure 4a and the AUC and their standard errors are shown in Figure 4b . Our proposed SHML model achieves significant improvement over all the other methods. It reduces the prediction error of elastic net (which ranks the second best) by 25%, and reduces the error of LapSVM by 48%. Association, 2012). We conducted association analysis and diagnosis of AD based on a dataset from Alzheimer's Disease Neuroimaging Initiative(ADNI) 1 . The ADNI study is a longitudinal multisite observational study of elderly individuals with normal cognition, mild cognitive impairment, or AD. We applied the proposed method to study the associations between genotypes and brain atrophy measured by MRI and to predict the subject status (normal vs MCI vs AD). Note that the statuses are ordinal since they represent increasing severity levels."}, {"section_title": "Real-World Study on Alzheimer's Disease", "text": "After removing missing values, the data set consists of 625 subjects including183 normal, 308 MCI and 134 AD cases, and each subject contains 924 SNPs and 328 MRI features. The selected SNPs are those top SNPs separating normal subjects from AD in ADNI. The MRI features measure the brain atrophies in different brain regions based on cortical thickness, surface areas or volumes, which are obtained from FreeSurfer software 2 . To test the diagnosis accuracy, we compared our method with the previously mentioned ordinal or multinomial regression methods. We employ the extended model with linkage disequilibrium priors, denoted as SHML-LD, to discover the associations.\nWe compare both SHML and SHML-LD with the state-of-the-art classification methods. And we used the 10-fold cross validation for each run to tune free parameters on the training data. To determine the dimension k for the latent features U in our method, we computed the variational lower bounds as an approximation to the model marginal likelihood (i.e., evidence), with various k values {10, 20, 40, 60}. We chose the value with the largest approximate evidence, which led to k = 20 (see Figure 5) . Our experiments confirmed that with k = 20, our model achieved the highest prediction accuracy, demonstrating the benefit of evidence maximization.\nAs shown in Figure 6 , our method achieved the highest prediction accuracy, higher than that of the second best method, GP ordinal Regression, by 10% and than that of the worst method, CCA+lasso, by 22%. The two-sample t test shows our model outperforms the alternative methods significantly (p < 0.05). We also examined the strongest associations discovered by our model. Firstly, the ranking of MRI features in terms of prediction power for the three different disease populations (normal, MCI and AD) demonstrate that most of the top ranked features are based on the cortical thickness measurement. On the other hand, the features based on volume and surface area estimation are less predictive. Particularly, thickness measurements of middle temporal lobe, precuneus, and fusiform were found to be most predictive compared with other brain regions. These findings are consistent with the memory-related function in these regions and findings in the literature for their prediction power of AD. We also found that measurements of the same structure on the left and right sides have similar weights, indicating that the algorithm can automatically select correlated features in groups, since no asymmetrical relationship has been found for the brain regions involved in AD. Secondly, the analysis of associating genotype to AD prediction also generated interesting results. Similar to the MRI features, SNPs that are in the vicinity of each other are often selected together, indicating the group selection characteristics of the algorithm. For example, the top ranked SNPs are associated with a few genes including CAPZB (F-actin-capping protein subunit beta), NCOA2 (The nuclear receptor coactivator 2) and BCAR3(Breast cancer anti-estrogen resistance protein 3).\nAt last, biclustering of the gene-MRI associations, as shown in Figure 7 , reveals interesting patterns in terms of the relationship between genetic variations and brain atrophy measured by structural MRI. For example, the top ranked SNPs are associated with a few genes including BCAR3 (Breast cancer anti-estrogen resistance protein 3) and NCOA2, and MAP3K1 (mitogen-activated protein kinase kinase kinase 1) which have been studied more carefully in cancer research. The set of SNPs are associated with cingulate in negative directions, which is part of the limbic system and involves in emotion formation and processing. Compared with other structures such as temporal lobe, it plays a more important role in the formation of long-term memory. For example, the association between MAP3K1 and the caudate anterior cingulate cortex has been identified. Literature has shown that MAP3K1 is associated with biological processes such as apoptosis, cell cycle, chromatin binding and DNA binding 3 , and cingulate cortex has been shown to be severely affected by AD (Jones et al., 2006) . The strong association discovered in this work might indicate potential genetic effects in the atrophy pattern observed in this cingulate subregion."}, {"section_title": "Related Work", "text": "The proposed our model model is related to a broad family of probabilistic latent variable models, including probabilistic principle component analysis (Tipping & Bishop, 1999) , probabilistic canonical correlation analysis (Bach & Jordan, 2005) and their extensions (Yu, Yu, Tresp, Kriegel, & Wu, 2006; Archambeau & Bach, 2009; Guan & Dy, 2009; Virtanen, Klami, & Kaski, 2011) . They all learn a latent representation whose projection leads to the observed data. Recent studies on probabilistic factor analysis methods put more focus on the sparsity-inducing priors to the projection matrix. Among them, Guan and Dy (2009) used the Laplace prior, the Jeffrey's prior, and the inverse-Gaussian prior; Archambeau and Bach (2009) employed the inverse-Gamma prior; and Virtanen et al. (2011) used the Automatic Relevance Determination(ARD) prior. Despite their success, these sparsity-inducing priors have their own disadvantages -they confound the degree of sparsity with the degree of regularization on both relevant and irrelevant variables, while in practical settings there is little reason that these two types of complexity control should be so tightly bounded together. Although the inverse-Gaussian prior and the inverse-Gamma prior provide more flexibility of controlling the sparsity, they suffer from being highly sensitive to the controlling parameters and thus lead to unstable solutions. In contrast, our model adopts the spike and slab prior, which has been recently used in multi-task multiple kernel learning (Titsias & L\u00e1zaro-Gredilla, 2011) , sparse coding (Goodfellow et al., 2012) , and latent factor analysis (Carvalho, Chang, Lucas, Nevins, Wang, & West, 2008) . Note that while our Beta priors over the selection indicators lead to simple yet effective variational updates, the hierarchical prior in the work of Carvalho et al.(2008) can better handle the selection uncertainty. Regardless what priors are assigned to the spike and slab models, they generally avoid the confounding issue by separately controlling the projection sparsity and the regularization effect over selected elements.\nSHML is also connected with many methods on learning from multiple sources or views (Hardoon, Leen, Kaski, & Shawe-Taylor, 2008) . Multiview learning methods are often used to learn a better classifier for multi-label classification -usually in text mining and image classification domains -based on correlation structures among the training data and the labels (Yu et al., 2006; Virtanen et al., 2011; Rish, Grabarnik, Cecchi, Pereira, & Gordon, 2008) . However, in medical analysis and diagnosis, we meet two separate tasksthe association discovery between genetic variations and clinical traits, and the diagnosis on patients. Our proposed SHML conducts these two tasks simultaneously: it employs the diagnosis labels to guide association discovery, while leveraging the association structures to improve the diagnosis. In particular, the diagnosis procedure in SHML leads to an ordinal regression model based on latent Gaussian process models. The latent Gaussian process treatment differentiates ours from multiview CCA models (Rupnik & Shawe-Taylor, 2010) . Moreover, most multiview learning methods do not model the heterogeneous data types from different views, and simply treat them as continuous data. This simplification can degrate the predictive performance. Instead, based on a probabilistic framework , SHML uses suitable link functions to fit different types of data."}, {"section_title": "Conclusions", "text": "We have presented a new Bayesian multiview learning framework to simultaneously find key associations between data sources (i.e., genetic variations and phenotypic traits) and to predict unknown ordinal labels. We have shown that the model can also employ background information, e.g., the Linkage Disequilibrium information, via an additional graph Laplacian type of prior. Our proposed approach follows a generative model: it extracts a common latent representation which encodes the structural information within all the data views, and then generates data via sparse projections. The encoding of knowledge from multiple views via the latent representation makes it possible to effectively detect the associations with high sensitivity and specificity.\nExperimental results on the ADNI data indicate that our model found biologically meaningful associations between SNPs and MRI features and led to significant improvement on predicting the ordinal AD stages over the alternative classification and ordinal regression methods. Despite the drawbacks of the proposed framework in slow training speed and requirement of careful tuning parameters, it has strong modeling power due to the Bayesian nature. Although we have focused on the AD study, we expect that our model, as a powerful extension of CCA, can be applied to a wide range of applications in biomedical research -for example, eQTL analysis supervised by additional labeling information. \nwhere \u03c8(x) = "}, {"section_title": "Appendix B. Parameter Update for Ordinal Data", "text": "The variational distributions of S h and \u03a0 h introduced in Section 3.2 are given by where \u03a6(\u00b7) is the cumulative distribution function of a standard Gaussian distribution. Note that in Equation (26), Q(c ij ) is a truncated Gaussian and the truncation is controlled by the observed ordinal data z ij ."}]