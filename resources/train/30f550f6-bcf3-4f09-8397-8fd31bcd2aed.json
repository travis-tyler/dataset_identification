[{"section_title": "Abstract", "text": "We develop a conditional generative model for longitudinal image datasets based on sequential invertible neural networks. Longitudinal image acquisitions are common in various scientific and biomedical studies where often each image sequence sample may also come together with various secondary (fixed or temporally dependent) measurements. The key goal is not only to estimate the parameters of a deep generative model for the given longitudinal data, but also to enable evaluation of how the temporal course of the generated longitudinal samples are influenced as a function of induced changes in the (secondary) temporal measurements (or events). Our proposed formulation incorporates recurrent subnetworks and temporal context gating, which provide a smooth transition in a temporal sequence of generated data that can be easily informed or modulated by secondary temporal conditioning variables. We show that the formulation works well despite the smaller sample sizes common in these applications. Our model is validated on two video datasets and a longitudinal Alzheimer's disease (AD) dataset for both quantitative and qualitative evaluations of the generated samples. Further, using our generated longitudinal image samples, we show that we can capture the pathological progressions in the brain that turn out to be consistent with the existing literature, and could facilitate various types of downstream statistical analysis."}, {"section_title": "Introduction", "text": "Consider a dataset of longitudinal or temporal sequences of data samples\nwhere each sample x i comes with sequential covariates {y t } N i=1 , one for each time point t. In other words, we assume that for each sequential sample i, x i \u2192 x 2 i \u2192 x 3 i corresponding to the given cognition progression (i.e., brain regions with high (red) and low (blue) disease pathology). The Generated Sequence follows the trend of the Real Data Sequence (i.e., similar (\u2248) to the real brain image progression) from the subjects with similarly decreasing cognition scores.\nwith each time point may be an assessment of disease severity or some other clinical measurement.\nOur high level goal is to design conditional generative models for such sequential image data. In particular, we want a model which provides us a type of flexibility that is highly desirable in this setting. For instance, for a sample drawn from the distribution after the generative model has been estimated, we should be able to \"adjust\" the sequential covariates, say at a time point t, dynamically to influence the expected future predictions after t for that sample. It makes sense that for a heart rate sequence, the appropriate subsequence should be influenced by when the \"violence\" stimulus was introduced as well as the default heart rate pattern of the specific sample (participant) [2] . Notice that when t = 1, this construction is similar to conditional generative models where the \"covariate\" or condition y may simply denote an attribute that we may want to adjust for a sample: for example, increase the smile or age attribute for a face image sampled from the distribution as in [26] .\nWe want our formulation to provide a modified set of x t s adaptively, if we adjust sequential covariates y t s for that sample. If we know some important clinical information at some point during the study (say, at t = 5), this information should influence the future generation x t>5 conditioned both on this sequential covariate or event y 5 as well as the past sequence of this sample x t<5 . This will require conditioning on the corresponding sequential covariates at each time point t by accurately capturing the posterior distribution p(x t |y t ). Such conditional sequence generation needs a generative model for sequential data which can dynamically incorporate time-specific sequential covariates y t of interest to adaptively modify sequences.\nThe setup above models a number of applications in medical imaging and computer vision that require generation of frame sequences conditioned on frame-level covariates. In neuroimaging, many longitudinal studies focus on identifying disease trajectories [3, 5, 28, 18] : for example, at what point in the future will specific regions in the brain exceed a threshold for brain atrophy? The future trend is invariably a function of clinical measures that a participant provides at each visit as well as the past trend of the subject. From a methodological standpoint, constructing a sequential generative model may appear feasible by appropriately augmenting the generation process using existing generative models. For example, one could simply concatenate the sequential measurements {x t } as a single input for existing non-sequential conditional generative models such as conditional GANs [31, 19] and conditional variational autoencoders [38, 1] . We will see why this is not ideal shortly.\nWe find that for our application, an attractive alternative to discriminator-generator based GANs, is a family of neural networks called normalizing flow [36, 35, 10, 9] which involve invertible networks (i.e., reconstruct input from its output). What is particularly relevant is that such formulations work well for conditionally generating diverse samples with controllable degrees of freedom [4] -with an explicit mechanism to adjust the conditioning variable. But the reader will notice that while these models, in principle, can be used to approximate the posterior probability given an input of any dimension, concatenating a series of sequential inputs quickly blows up the size for these highly expressive models and renders them impractical to run, even on high-end GPU clusters. Even if we optimistically assume computational feasibility, variable length sequences cannot easily be adapted to these innately non-sequential generative models, especially for those that extend beyond the training sequence length. Also, data generated in this manner involve simply \"concatenated\" sequential data and do not consider the innate temporal relationships among the sequences, which is fundamental in recurrent models. For these reasons, adapting existing generative models, will involve setting up a generative model which is recursive for variable length inputs.\nGiven various potential downstream applications and the issues identified above with conditional sequential generation problem, we seek a model which (i) efficiently generates high dimensional sequence samples of variable lengths (ii) with dynamic time-specific conditions reflecting upstream observations (iii) with fast posterior probability estimation. We tackle the foregoing issues by introducing an invertible recurrent neural network, CRow, that includes recurrent subnetwork and temporal context gating. These modifications are critical in the following sense. Invertibility lets us precisely estimate the distribution of p(x t |y t ) in latent space. Introducing recurrent subnetworks and temporal context gating enables obtaining cues from previous time points x <t to generate temporally sensible subsequent time points x \u2265t . Specifically, our contributions are: (A) Our model generates conditional sequential samples {x t } given sequential covariates {y t } for t = 1, . . . , T time points where T can be arbitrarily long. Specifically, we allow this by posing the task as a conditional sequence inverse problem based on a conditional invertible neural network [4] . (B) Assessing the quality of the generated samples may not be trivial for certain modalities (e.g., nonvisual features). With the specialized capability of the normalizing flow construction, our model estimates the posterior probabilities p(x t |y t ) of the generated sequences at each time point for potential downstream analyses involving uncertainty. (C) We demonstrate an interesting practical application of our model in a longitudinal neuroimaging dataset. We show that the generated longitudinal brain pathology trajectories (an illustration in Fig. 1 ) can lead to identifying specific regions in the brain which are statistically associated with Alzheimer's disease (AD)."}, {"section_title": "Preliminary: Invertible Neural Networks", "text": "We first describe an invertible neural network (INN) which inverts an output back to its input for solving inverse problems (i.e., z = f (x) \u21d4 x = f \u22121 (z)). This becomes the building block of our method; thus, before we present our main model, let us briefly describe a specific type of invertible structure which was originally specialized for density estimation with neural network models."}, {"section_title": "Normalizing Flow", "text": "Estimating the density p X (x) of sample x is a classical statistical problem in various fields including computer vision and machine learning in, e.g., uncertainty estimation [14, 15] . For tractable computation throughout the network, Bayesian adaptations are popular [34, 12, 33, 27, 23, 18] , but these methods make assumptions on the prior distributions (e.g., exponential families).\nA normalizing flow [36, 35] first learns a function f (\u00b7) which maps a sample x to a latent variable z = f (x) where z is from a standard normal distribution Z. Then, with a change of variables formula, we estimate where |J X | is a Jacobian determinant. Thus, f (\u00b7) must be invertible, i.e., x = f \u22121 (z), and to use a neural network as f (\u00b7), a coupling layer structure was introduced in Real-NVP [9, 10] for an easy inversion and efficient |J X | computation as we describe next.\nForward map (Fig. 2a) . Without loss of generality, in the context of network structures, we use an input u \u2208 R\n. Then, we forward map u 1 and u 2 to v 1 and v 2 respectively:\nwhere s and r are independent functions (i.e., subnetworks), and \u2297 and + are element-wise product and addition respectively. Then, v 1 and\nInverse map (Fig. 2b) . A straightforward arithmetic allows an exact inverse from v to u (i.e., v \u2192 u):\nwhere the subnetworks s and r are identical to those used in the forward map in Eq. (2), and and \u2212 are elementwise division and subtraction respectively. Note that the subnetworks are not explicitly inverted, thus any arbitrarily complex network can be utilized. Also, the Jacobian matrix J v = \u2202v/\u2202u is triangular so its determinant |J v | is just the product of diagonal entries (i.e., i exp(s(u 1 )) i ) which is extremely easy to compute (we will discuss this further in Sec. 3.2.1).\nTo transform the \"bypassed\" split u 1 (since u 1 = v 1 ), a coupling block consisting of two complementary coupling layers is constructed to transform both u 1 and u 2 :\nand its inverse\nSuch a series of transformations allow a more complex mapping which still comes with a chain of efficient Jacobian determinant computations, i.e., det(AB) = det(A) det(B) where A and B are the Jacobian matrices of two coupling layers. More details are included in the supplement. Note that we have used (and will be using) u and v as generic input and output of an INN. Thus, specifically in the context of normalizing flow, by simply considering u and v to be x and z respectively, we can use a coupling layer based INN as a powerful invertible function f (\u00b7) to perform the normalizing flow described in Eq. (1)."}, {"section_title": "Model Setup: Conditional Recurrent Flow", "text": "In this section, we describe our conditional sequence generation method called Conditional Recurrent Flow (CRow). We first describe a conditional invertible neural network (cINN) [4] which is one component of our model. Then, we explain how to incorporate temporal context gating and discuss the settings where CRow can be useful."}, {"section_title": "Conditional Sample Generation", "text": "Naturally, an inverse problem can be posed as a sample generation procedure by sampling a latent variable z and inverse mapping it to x = f \u22121 (z), thus generating a new sample x. The concern is that we cannot specifically 'choose' to generate an x of interest since a latent variable z does not provide any interpretable associations with x.\nIn other words, estimating the conditional probability p(x|y) is desirable since it represents an underlying phenomenon of the input x \u2208 R d and covariate y \u2208 R k (e.g., the probability of a specific brain imaging measure x of interest given a diagnosis y). In fact, when we cast this problem into a normalizing flow, the goal becomes constructing an invertible network f (\u00b7) which maps a given input x \u2208 R d to its corresponding covariate/label y \u2208 R k and its latent variable z \u2208 R m such that [y, z] = f (x). The mapping must have an inverse for x = f \u22121 ([y, z]) to be recovered. Specifically, when a flow-based model jointly encodes label and latent information (i.e., [y, z] = v = f (x) via Eq. (4)) while ensuring that p(y) and p(z) are independent, then the network becomes conditionally invertible (i.e.,\n) conditioned on given y). Such a network can be theoretically constructed through a bidirectionaltype training [4] , and this allows a conditional sampling\n) and the posterior estimation p(x|y). Bidirectional training. This training process involves three losses: (1) L Z (p(y, z), p(y)p(z)) enforces p(y) and p(z) to be independent by making the network output p(y, z) to follow p(y)p(z) which is true if and only if p(y) and p(z) are independent. (2) L Y (y, y gt ) is the supervised label loss between our prediction y and the ground truth y gt . (3) L X (p(x), p X ) improves the likelihood of the input x with respect to the prior p X . L Z and L X are based on a kernel-based moment matching measure Maximum Mean Discrepancy (MMD) [11, 44] , also see appendix.\nIn practice, x and [y, z] may not be of the same dimen- sions. To construct a square triangular Jacobian matrix, zero-padding both x and [y, z] can alleviate this issue while also increasing the intermediate subnetwork dimensions for higher expressive power. Also, the forward mapping is essentially a prediction task that we encounter often in computer vision and machine learning, i.e., predicting y = f (x) or maximizing the likelihood p(y|x) without explicitly utilizing the latent z. On the other hand, the inverse process of deriving x = f \u22121 (y), allows a more scientifically based analysis of the underlying phenomena, e.g., the interaction between brain (x) and observed cognitive function (y)."}, {"section_title": "Conditional Recurrent Flow (CRow)", "text": "The existing normalizing flow type networks cannot explicitly incorporate sequential data which are now increasingly becoming important in various applications. Successful recurrent models such as gated recurrent unit (GRU) [6, 40] and long short-term memory (LSTM) [16, 37] explicitly focus on encoding the \"memory\" from the past and output proper state information for accurate sequential predictions given the past. Similarly, generated sample sequences must also follow sequentially sensible patterns or trajectories resembling likely sequences by encoding appropriate temporal information for the subsequent time points.\nTo overcome these issues, we introduce Conditional Recurrent Flow (CRow) model for conditional sequence generation. Given a sequence of input/output pairs {u t , v t } for t = 1, . . . , T time points, modeling the relationship between the variables across time needs to also account for the temporal characteristic of the sequence. Variants of recurrent neural networks (RNN) such as GRU and LSTM have been showing success in sequential problems, but they only enable forward mapping. We are specifically interested in an invertible network which is also recurrent such that given a sequence of inputs {u t } (i.e., features {x t }) and their sequence of outputs {v t } (i.e., covariates/labels and latent information {y t , z t }), we can model the invertible relationship between those sequences for posterior estimation and conditional sequence generation as illustrated in Fig. 1 .\nWithout loss of generality, we can describe our model in terms of generic {u t } and {v t }. We follow the coupling block described in Eq. (4) \nand the inverse is\nNote that the hidden states h t 1 and h t 2 generated from the recurrent network of the subnetworks are implicitly used within the subnetwork architecture (i.e., inputs to additional fully connected layers) and also passed to their corresponding recurrent network in the next time point as in Fig. 3. "}, {"section_title": "Temporal Context Gating (TCG)", "text": "A standard (single) coupling layer transforms only a part of the input (i.e., u 1 in Eq. (2)) by design which results in the determinant of a triangular Jacobian matrix J v :\nthus\n. This is a result from Eq. (2):\n(1) the element-wise operations on u 2 for the diagonal submatrix of partial derivatives \u2202v 2 /\u2202u 2 = diag(exp s(u 1 )), (2) the bypassing of u 1 = v 1 for \u2202v 1 /\u2202u 1 = I, and (3) \u2202v 1 /\u2202u 2 = 0. Ideally, transforming u 1 would be beneficial. However, this is explicitly avoided in the coupling layer design since this should not involve u 1 or u 2 directly; otherwise, J v will not be triangular. Using h t in CRow. In the case of CRow, it incorporates a hidden state h t\u22121 from the previous time point which is neither u nor v. This is our temporal information which adjusts the mapping function f (\u00b7) to allow more accurate mapping depending on the previous time points of the sequence which is crucial for sequential modeling.\nSpecifically, we incorporate a temporal context gating f TCG (\u03b1 t , h t\u22121 ) using the temporal information h t\u22121 on a given input \u03b1 t at t as follows:\nwhere cgate(h t\u22121 ) can be any learnable function/network with a sigmoid function at the end. This is analogous to the context gating [30] in video analysis which scales the input \u03b1 t (since cgate(h t\u22121 ) \u2208 (0, 1)) based on useful context, which in our setup is the temporal information h t\u22121 . Preserving the Jacobian structure. In the context of |J v | computation in Eq. (8), we perform f TCG (u 1 , h t\u22121 ) = u 1 \u2297 cgate(h t\u22121 ) (w.l.o.g., we omit t for u and v). Importantly, we observe that this 'auxiliary' variable h t\u22121 could safely be used to transform u 1 without altering the triangular structure of the Jacobian matrix for the following two reasons: (1) we still perform an element-wise operation u 1 \u2297 cgate(h t\u22121 ) resulting a diagonal submatrix for \u2202v 1 /\u2202u 1 , and (2) \u2202v 1 /\u2202u 2 is still 0 since u 2 is not involved in f TCG (u 1 , h t\u22121 ). Thus, we now have\nwhere\nAs seen in Fig. 3 , we place f TCG to transform the \"bypassing\" split (non-transforming partition) of each layer of a block (i.e., the \"bypassing\" partition u t 2 gets transformed by f TCG2 ). We specifically chose a gating mechanism for conservative adjustments so that the original information is preserved to a large degree through simple but learnable 'weighting'. The full forward and inverse steps involving f TCG can easily be formulated by following Eq. (6) and Eq. (7) while respecting the order of operations seen in Fig. 3 . See appendix for details."}, {"section_title": "How do we use CRow?", "text": "In essence, CRow aims to model an invertible mapping [{y t }, {z t }] = f ({x t }) between sequential/longitudinal measures {x t } and their corresponding observations {y t } with {z t } encoding the latent information across t = 1, . . . , T time points. Once we train f (\u00b7), we can perform the following exemplary tasks:\n(1) Conditional sequence generation: Given a series of observations of interest {y t }, we can sample {z t } (each independently from a standard normal distribution) to generate {x\n. The advantage comes from how {y t } can be flexibly constructed (either seen or unseen from the data) such as an arbitrary disease progression over time (see Fig. 1 ). Then, we randomly generate corresponding measures {x t } to observe the corresponding longitudinal measures for both quantitative and qualitative analyses. Since the model is recurrent, the sequence length can be extended beyond the training data to model future trajectory.\nA potential direction would be to use the generated sequences to directly enable common data analysis procedures (i.e., statistical analysis on synthetic data) and help evaluate scientific hypotheses.\n(2) Sequential density estimation: Conversely, given {x t }, we can predict {y t }, and more importantly, estimate the density p X ({x t }) at each t. When {x t } is generated from {y t }, the estimated density can indicate the 'integrity' of the generated sample (i.e., low p X implies that the sequence is perhaps less common with respect to {y t })."}, {"section_title": "Experiments", "text": "We validate our framework in both a qualitative and quantitative manner with two sets of experiments: (1) two image sequence datasets and (2) a neuroimaging study."}, {"section_title": "Conditional Moving MNIST Generation", "text": "Moving Digit MNIST: We first test our model on a controlled Moving Digit MNIST dataset [39] of image sequences showing a hand-written digit from 0 to 9 moving in a path and bouncing off the boundary (see supplement for animations). This experiment qualitatively shows that the images in a generated sequence with specific conditions (i.e., image labels) are consistent across the sequence. Here, we specifically chose two digits (e.g., 0 and 1) to construct \u223c13K controlled sequences of frame length T = 6 where each frame of a sequence is an image of size 20 by 20 (vectorized as x t \u2208 R 400 ) and has a one-hot vector y t \u2208 R 2 of digit label at t indicating one of the two possible digits. We found this intuitive and interpretable assessment before experimenting with arguably less interpretable datasets (i.e., neuroimaging data we show later).\nTraining. Our model consists of three coupling blocks, each block shown in Fig. 3 , where each subnetwork q contains one GRU cell and three layers of residual fully connected networks with ReLU activation. For each TCG (f TCG in Fig. 3 , Eq. (9)), the network cgate(\u00b7) is a single fully connected network with sigmoid activation. Each input frame u t = x t is split into two halves u 1 and u 2 . Models were trained on T = 6 time points, but further time points data can be generated since our model is recurrent. Each training sequence has a digit label sequence {y t } for t = 1, . . . , 6 where all y t are \"identical\" in each sequence since the the same digit is shown throughout the sequence.\nGeneration. Now, we want to generate sequences showOurs:\ncINN: ing digits gradually transform (e.g., changing from 1 to 0). We first specified sequential conditions (i.e., digit label) that change midway through the sequence (e.g., {y t } sequence indicating digit labels 1\u21921\u21920\u21920\u21920\u21920). Then, we generated the corresponding sequences {x t } and visually check if the changes across the frames look natural. Note that we trained only the image sequences with consistent digit labels. One demonstrative result is shown in Fig. 4 where we compare the generated image sequences with condition (i.e., digit label) changing from 1 to 0. Our result at the top of Fig. 4 shows gradual transition while cINN result does not show such temporally smooth and consistent behavior.\nDensity estimation. Our model quantifies its output confidence in the form of density (i.e., likelihood) shown at the top of each generated images in Fig. 4 . Not only our model adjusts generation based on inputs, but it also outputs lower density at the frame showing the most drastic transformation as such patterns were not observed during the training, i.e., the likelihood decreases when then condition changes and then increase as the sequence goes. This means that our model simultaneously shows the conditional generation ability and estimates outputs' relative density given the training data seen. Different from other generative models, it allows conditional generation on sequential data while maintaining exact and efficient density estimation. More examples are shown in Fig. 5 (and appendix) .\nMoving Fashion MNIST: We also tested our model on a more challenging dataset called Moving Fashion MNIST [43] of moving apparel image sequences. The image sizes. frames lengths, and moving paths are identical to those of Moving Digit MNIST. An important difference is that they are real images of 10 types of apparels (i.e., T-shirt, Bag, etc. see supplement for the full list) instead of hand-written digits. The same models and training setups were used to generate the transforming sequences in a similar manner. In Fig. 6 , we show the examples of various apparels successfully transforming to other types while moving. Compared to Moving Digit MNIST, capturing the smooth transformations of these apparel images are more challenging as the apparel shapes vary more in terms of shapes and sizes."}, {"section_title": "Longitudinal Neuroimaging Analysis", "text": "In this neuroimaging experiment, we evaluate if our conditionally generated samples actually exhibit statistically robust and clinically sound characteristics when trained with a longitudinal Alzheimer's disease (AD) brain imaging dataset. We generated a sufficient number of longitudinal T-shirt [0] \u2192 Bag [8] Ankle boot [9] \u2192 Sneaker [7] T-shirt [0] \u2192 Long sleeve [4] brain imaging measures (i.e., {x t }) conditioned on various covariates (i.e., labels {y t }) associated with AD progression (e.g., memory). Thus, the generated brain imaging sequences should show the pathology progression consistent with the covariate progression (see Fig. 1 and Fig. 7 for illustrations). We then performed a statistical group analysis (i.e., healthy vs. disease progressions) to detect disease related features from the imaging measures. In the end, we expected that the brain regions of interests (ROIs) identified by the statistical group analysis are consistent with other AD literature with statistically stronger signal (i.e., lower p-value) than the results using the original training data.\nDataset. The Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu) is one of the largest and still growing neuroimaging databases. Originated from ADNI, we use a longitudinal neuroimaging dataset called The Alzheimer's Disease Prediction of Longitudinal Evolution (TADPOLE) [29] . We used data from N =276 participants with T = 3 time points.\nInput. For the longitudinal brain imaging sequence {x t }, we chose Florbetapir (AV45) Positron Emission Tomography (PET) scan measuring the level of amyloid-beta deposited in brain which has been a known type of pathology associated with Alzheimer's disease [42, 22] . The AV45 images were registered to a common brain template (MNI152) to derive the gray matter regions of interests (82 Desikan atlas ROIs [8] , see appendix). Thus, each of the 82 ROIs (x t \u2208 R 82 ) holds an average Standard Uptake Value Ratio (SUVR) measure of AV45 where high AV45 implies more amyloid pathology in that region.\nCondition. For the corresponding labels {y t } for longitudinal conditions, we chose five covariates known to be tied to AD progression (normal to impaired range in square brackets): of samples. See supplement and [29] for details.\nAnalysis. We performed a statistical group analysis on each condition {y t } independently with the following pipeline: (1) Training: First, we trained our model (the same subnetwork as Sec. 4.1) using the sequences of SUVR in 82 ROIs for {x t } and the covariate ('label') sequences for {y t }. (2) Conditional longitudinal sample generation: Then, we generated longitudinal samples {x t } conditioned on two distinct longitudinal conditions: Control (healthy covariate sequence) versus Progression (worsening covariate sequence). Specifically, for each condition (e.g., Diagnosis), we generate N 1 samples of Control (e.g., {x\nconditioned on {y t } = CN\u2192MCI\u2192AD). Then, we perform a two sample t-test at t = 3 for each of 82 ROIs between {x\ngroups, and derive p-values to tell whether the pathology levels between the groups significantly differ in those ROIs.\nResult 1: Control vs. Progression (Table 1 , Top row block). We set the longitudinal conditions for each covariate based on its associated to healthy progression (e.g., low ADAS13 throughout) and disease progression (e.g., high ADAS13 related to eventual AD onset). We generated N 1 = 100 and N 2 = 100 samples for each group respectively. Then, we performed the above statistical group difference analysis under 4 setups: (1) Raw training data, (2) cINN [4] , (3) Our model, and (4) Our model + TCG. With the raw data, the sample sizes of the desirable longitudinal conditions were extremely small for all setups, so no statistical significance was found after type-I error control. With cINN, we occasionally found few significant ROIs, but the non-sequential samples with only t = 3 could not generate realistic samples. With CRow we consistently found significant ROIs and detected the most number of ROIs (the ROIs for Diagnosis shown in Fig. 8 ) including many AD-specific regions reported in the aging literature such as hippocampus and amygdala [20, 22] (see appendix for the full list). Table 1 . The colors denote the -log p-value. AD-related ROIs such as hippocampus, putamen, caudate, and amygdala are included.\nAD when early detection leads to effective prevention. With N 1 = 100 and N 2 = 100 samples, no significant ROIs were found in all models. To improve the sensitivity, we generated N 1 = 150 and N 2 = 150 samples in all models and found several significant ROIs only with CRow related to an early AD progression such as hippocampus [13, 21, 24, 17] (full list in the appendix). Statistical advantages. By generating realistic samples with CRow, we achieve the following advantages: (1) Increasing sample size makes the hypothesis test more sensitive and robust -rejecting the null when it is indeed false -leading to a lower type-II error. (2) Also, we do not simply detect spurious significant ROIs because (i) we control for type-I error via the most conservative Bonferroni multiple testing correction, and (ii) we additionally improve the statistical power of detecting the true effects (i.e., significant ROIs) that at least need to be detected with the raw data only. In Table 2 , we show that the significant ROIs identified with the real data only are also detected through our framework with improved p-values from the Control vs. Progression experiment. These results on the generated data suggest that one can utilize CRow in a statistically meaningful manner without neglecting the true signals from important AD-specific ROIs [13, 32] . Note that the scientific validity of our findings requires further investigation on additional real data. These preliminary results, however, point to the promise of using such models to partly mitigate problems related to recruiting large number of participants for statistically identifying weak disease effects.\nGeneration assessments. In Fig. 7 , we see the generated samples (Left) through CN\u2192MCI\u2192AD in three views of the ROIs and compare them to the real training samples (Right). We observe that the generated samples have similar AV45 loads through the ROIs, and more importantly, the progression pattern across the ROIs (i.e., ROIs turning more red indicating amyloid accumulation) follows that of the real sequence as well. We also quantified the similarities between the generated and real data sequences by comput- Table 2 : p-values in ROIs improve (get lower) with the sequences generated by CRow with increased sample size over using real sequence data.\ning effect size (Cohen's d [7] ) which measures the difference between the two distributions (Table 3) showing that CRow generates the most realistic sequences. Scientific remarks. Throughout our analyses, the significant ROIs found such as amygdala, putamen, temporal regions, hippocampus (e.g., shown in Fig. 8 ) and many others reported as AD-specific regions in the aging literature [13, 20, 21, 32, 41, 25] . This implies that the generated longitudinal sequences could resemble the underlying distribution of the real data which we may not be available with large enough sample sizes. The appendix includes additional details on the scientific interpretation of the results."}, {"section_title": "Conclusion", "text": "We design generative models for longitudinal datasets that can be modulated by secondary conditional variables. Our architecture is based on an invertible neural network that incorporates recurrent subnetworks and temporal context gating to pass information within a sequence generation, the network seeks to \"learn\" the conditional distribution of training data in a latent space and generate a sequence of samples whose longitudinal behavior can be modulated based on given conditions. We demonstrate experimental results using three datasets (2 moving videos, 1 neuroimaging) to evaluate longitudinal progression in sequentially generated samples. In neuroimaging problems which suffer from small sample sizes, our model can generate realistic samples which is promising. Cohen Table 3 : Difference between the generated sequences and the real sequences at t = 3. Lower the effect size (Cohen's d), smaller the difference between the comparing distributions. In all settings, CRow with TCG generates the most realistic sequences with the smallest effect sizes."}]