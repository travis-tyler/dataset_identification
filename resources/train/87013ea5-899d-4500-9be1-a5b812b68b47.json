[{"section_title": "", "text": ""}, {"section_title": "INTRODUCTION", "text": "This document, Guidance for Benthic Habitat Mapping: An Aerial Photographic Approach, describes proven technology that can be applied in an operational manner by state-level scientists and resource managers. This information is based on the experience gained by NOAA Coastal Services Center staff and state-level cooperators in the production of a series of benthic habitat data sets in Delaware, Florida, Maine, Massachusetts, New York, Rhode Island, the Virgin Islands, and Washington, as well as during Center-sponsored workshops on coral remote sensing and seagrass and aquatic habitat assessment. The original benthic habitat document, NOAA Coastal Change Analysis Program (C-CAP): Guidance for Regional Implementation (Dobson et al.), was published by the Department of Commerce in 1995. That document summarized procedures that were to be used by scientists throughout the United States to develop consistent and reliable coastal land cover and benthic habitat information. Advances in technology and new methodologies for generating these data created the need for this updated report, which builds upon the foundation of its predecessor."}, {"section_title": "Objectives of This Document", "text": "The goal of this document is to provide technical guidance to data developers working to produce digital spatial data on benthic habitat. Using these methods, developers will be able to produce consistent benthic data suitable for regional comparison and application to various coastal management issues. All mapping efforts are designed to answer specific questions about the environment and meet objectives specific to a given project. The techniques used in generating a map determine its utility for meeting those objectives. The methods described in this document are designed to meet the following general objectives: \u2022 Produce digital baseline data on the spatial extent and characteristics of benthic habitats."}, {"section_title": "\u2022", "text": "Produce synoptic data over estuary-sized study areas.\nProvide data that optimize the efficiency of further in-situ sampling.\nProvide data at a resolution that can contribute to environmental permitting processes (such as Clean Water Act Section 404 fill determinations).\nProduce data that support change detection over extensive areas. The technical recommendations are designed to allow some flexibility in the choices of classification scheme, remote sensor data source, analysis procedures, and other key elements that vary regionally; however, all have been applied in various regions of the country and should be usable with minor modifications in the majority of geographic settings. The primary audience of this document is the spatial data analyst tasked with developing baseline benthic habitat data. The methods that follow rely strongly on aerial photointerpretation and photogrammetry. Effective implementation of these technologies requires a specialized set of skills and experience. Project analysts ideally should have a background in remote sensing and photogrammetry. A familiarity with the physical and biological components of the study area is also very important and a working knowledge of geographic information system (GIS) technology is essential to producing the digital data and conducting further spatial analysis of the results. A secondary audience is the coastal resource manager. Managers can use the major components of this document as guidance for preparing technical statements for grants or contracts, and for project planning. One element that is usually of particular interest to managers is the expected cost of a mapping project. The actual cost of a project is determined by many project variables and objectives. Therefore, specific information on costs is not provided in this document. Cost information is best obtained on a project-by-project basis in consultation with commercial data and service providers and other professionals working in the field.\nIt is readily obtainable throughout the nation through either federal, state or private sources. A strong commercial aerial survey industry exists that can collect new imagery, and several national archives contain historical imagery that is available to the public.\nIt is easily integrated into the coastal management process. Photography is already commonly used to address other coastal management issues such as shoreline and coastal development. This data source is familiar to the coastal management community and has been accepted into the legal process. \u2022 It can be obtained when environmental conditions are optimal. Sufficient aircraft/camera services are available so that a mission can be held on standby until conditions are right.\nIt provides sufficient spectral and superior spatial resolution for detecting subtle submerged features. Depending on scale, this imagery produces an image that the human eye can easily interpret and resolves features smaller than 1 meter. The image acquisition process consists of two components: 1) setting project acquisition specifications, and 2) addressing environmental considerations. Discussions of both follow.\nAccess to large parts of the study area is limited and might require boat travel or overland hiking. \u2022 Suitable GPS receivers are not available. In these instances it is worthwhile to obtain ancillary sources of control. A variety of sources exist that provide this type of information. These sources vary in quality and have their own advantages and disadvantages. In addition, some are more universally available than others. Ancillary control source data, like any other spatial data fall into either vector or raster format. Examples of a vector source would be a precision shoreline coverage with physiographic features that could be used to rectify a photo. A common raster source is a digital orthophotoquad. Orthophotoquads were used successfully as ancillary control sources in three Center benthic mapping projects (Coastal Massachusetts; Florida Bay, Florida; and Apalachicola Bay, Florida). These data are available in many coastal areas through the U.S. Geological Survey or state governments. They are most often compiled from National Aerial Photography Program (NAPP) aerial photographs acquired at a scale of 1:40,000 and typically contain sufficient detail to allow features common to the orthophotoquad and the mission photography to serve as GCPs. The spatial resolution of these products is usually less than 2 meters, and they are often stored in a compressed image format (MrSid) that greatly reduces their size. A potential weakness of these images as source material is the lack of any elevational value for control features. However, in flat coastal areas this has not resulted in excessive loss of horizontal accuracy. Besides digital raster and vector data as a source of features for rectifying other images, it is sometimes possible to obtain the original GCPs used to generate such data. Great caution should be exercised when using these points to rectify project imagery. In most cases these points will have been collected for purposes other than benthic mapping. Therefore, their distribution across the landscape may not be optimal for producing a solution over water areas. Significant data documentation (metadata) should also accompany points to be used from other sources to allow the user to determine whether the data logging thresholds are compatible with the current project and to assist in finding the points on the image reliably. Figure 9 shows a typical orthophotoquad. The detail in this imagery provides numerous features suitable for control points.\nNo discontinuities such as land areas, or dredged or natural channels partition the distribution into spatial units less than 0.03 hectares.\nSmall micro-habitat areas within a polygon are not large relative to the minimum mapping unit. In any effort to map benthic habitats using remote sensing, it is likely that some habitat areas will be missed due to small size, sparse cover, or poor conditions (turbidity plumes, depth, etc.). Due to the constraint of the minimum mapping unit and the possibility of suboptimal photography, delineations of benthic habitat will tend to be conservative. The degree of underestimation depends upon the atmospheric and hydrographic conditions at the time of photography, the experience of the photointerpreter, and the nature of the subject area. Optimizing conditions for photography will minimize habitat underestimation and other errors, particularly in areas that are intrinsically more difficult to interpret. Where habitat edges are clearly distinct in superior quality photography, they may also be detected in inferior quality photography (e.g., high biomass of SAV along a clear water channel with a steep bank of light-colored sediment). In other cases where the edges are not clearly distinct in superior quality photography they are likely to remain undetected in inferior photography (e.g., low biomass of SAV growing on a shallow depth gradient of deep, turbid water over dark-colored sediment). The deep-water edge of habitat may be difficult to delineate. This edge may also be at high risk for loss due to degradation in water quality that limits the illumination of the bottom with photosynthetically active radiation. Bottom types with unrecognized signatures due to poor photographic conditions cannot be mapped as habitat unless the area is rephotographed or additional sources of data are incorporated into the database. When photointerpretation is difficult or not possible, the preferred option is to rephotograph the area under better conditions. Although desirable, this may not be possible. Even under the best photographic conditions, delineation of all or part of some habitat polygons may require additional effort in the form of surface level verification or direct inclusion of surface level data. Polygon borders derived from surface-level data must be so designated in the project metadata to meet truth in labeling requirements. Suitable surface level positioning techniques include GPS or more traditional survey positioning techniques that can be demonstrated to provide high levels of the positional accuracy. In the case of an SAV polygon, the extent of coverage of the bottom by shoots of SAV and the pattern of distribution of the shoots or bed form (e.g. circular, doughnut-shaped, irregular patches and/or continuous cover of SAV), reflects the interaction of biotic, physical, and anthropogenic factors. An example of an index of coverage is an adaptation of the crown density scale originally developed to categorize percent cover by the crowns of trees in forests (Orth et al. 1991). The Center and USGS are currently discriminating between continuous beds of SAV and patchy beds. These two habitats have different functional values and the disintegration of continuous SAV cover into a patchy bed structure can be a sign of habitat stress. Patchy beds may also be a sign of new colonization. For these reasons identification of patchy habitat areas is important to habitat monitoring and trend or change analysis. The Center is applying a percent cover system that has been employed successfully for several years by the St. Johns River and South Florida Water Management Districts for mapping SAV. This system defines continuous SAV polygons as areas where 51-100 percent of the bottom is covered by that . Areas where coverage is 50 percent or less are defined as patchy SAV as long as none of the patches exceed the MMU. It should be noted that percent cover and bed form identifications are affected by factors such as water depth, species composition, and brightness of bottom sediments. The degree of contrast between shoots and exposed sediment, and clarity of the photographic image determine the minimum detection unit of features within SAV. Analysis of change over time at a given location therefore requires that different photographic conditions be considered. Field verification will be especially important in evaluating habitat change. Changes in coverage or bed form over time in a given location may indicate changing conditions in that habitat polygon or disturbances such as scarring by boat propellers. Figure x shows a sample delineation between continuous seagrass beds and patchy beds. Consideration of the minimum mapping unit and individual patch size was essential to this determination Some data including species, biomass, productivity, functional status, and health of various habitats may not be interpretable from the aerial photographs. Species identification is not possible from aerial photography in temperate areas such as North Carolina, many portions of the Gulf of Mexico, and the Chesapeake Bay. However, in some tropical areas photographic signatures may be sufficiently distinct to discriminate between genus, such as high-biomass turtle grass and lower bio-mass shoal grass beds.\nOuter boundaries of beds are more important than density categorizations within beds.\nOne of the most difficult decisions to make is whether areas with patches of SAV are one polygon of patchy SAV or individual SAV polygons. In this case, the minimum mapping unit of 0.03 hectares should be used to make the determination. \u2022 Erring on the side of lumping is preferred except in areas where small patches are the only vegetation. In deciding whether to exclude or include an area with only a few patches (all less than 0.03 hectares), include the polygon of patchy SAV if the total area is greater than 0.03 hectares. Err on the side of including these areas rather than excluding them. \u2022 A cutoff should be approximately 10 percent cover. Areas with less than 10 percent cover are unlikely to be reliably delineated and may better be classified as whatever the remaining 90 percent cover type is. In summary, the habitat delineation process should be carried out to preserve the maximum detail obtainable from the photography. Because a focus of these methods is on supporting habitat change detection, small movements of an SAV edge due to increased turbidity or other factors are important to capture. Generalized habitat delineations will be unlikely to capture these small changes.\nWhen the number of photographs in the project is very large and storage of scanned digital photographs might be problematic.\nWhen the environmental conditions are challenging and interpretation off of the original or first generation film media is critical. \u2022 When existing base maps for analog interpretation and zoom transfer are clearly outdated or inaccurate.\nThis method has the advantage of allowing multiple analysts to work simultaneously on a project in a networked environment.\nWorking with a digital image allows the user to employ several image enhancement processes. Some of the most useful are histogram stretching and spatial filtering. These can assist in mapping certain selected areas; however, they must be used with great caution since they may highlight image artifacts that do not represent actual habitats.\nThe loss of resolution associated with the scanning process does potentially hinder the interpretation process. The scanning process produces a spectral record of the raw image that is based on a systematic x, y array of pixels of a given size. Narrow image features that are not oriented along the same axis as the scan array can be captured poorly by the scanning process. In addition, the native resolution of most film emulsions is on the order of multiple line-pairs per millimeter. Most scan resolutions are unable to capture images at this resolution.\nLarger projects can raise issues associated with file storage and data retrieval. A typical color 23 centimeter x 23 centimeter (9 inch x 9 inch) frame at a scale of 1:24,000, scanned to produce a 1 meter pixel (acceptable for interpretation), can result in a raw file size of 100 megabytes.\nWhen project areas are relatively small with extensive land areas for reference points. All of these methods, if used by experienced analysts, will produce acceptable benthic habitat data. The choice of which method or which suite of methods to use is determined by the project partner. The Center typically works in partnership with statelevel cooperators and private-sector vendors to accomplish a project and can provide technical guidance and in-kind assistance to partners/firms in completing a project according to any of these methods, as well as in selecting an approach for developing benthic habitat data.\nPrepare the line work. This involves manually indicating start/stop nodes on polygonal boundaries, assigning easily identifiable names to the measured tics and clarifying any line-work uncertainties prior to digitizing.\nAfter manual or semiautomatic generation of line work from a screen display (analog method). A vector is a digital element with a defined structure that represents physical forms (elements) such as points, lines, and polygons. These terms are defined below as used in referring to ESRI format data. \u2022 Points represent the locations of a specific geographic feature or object that has no area, such as a mountain peak, or navigation buoy.\nLines consist of unclosed segments and represent linear geographic features, such as rivers, roads, or utility lines.\nPolygons are closed sets of line segments defining homogeneous areas, such as a particular soil type, land use, or water body. The label point links each polygon to its attributes This creates a vector layer that consists of both vector features and the attribute information. Vector data are expressed as a series of vertex coordinates. The vertices that define each element are referenced with x, y values (longitude, latitude or easting, northing). Although it is possible to have points, lines and polygons in a single layer, a layer typically consists of one type of feature.\nVideography produces a permanent record of observations that are objective and not as vulnerable to interpretation as diver notes. \u2022 Videography allows detailed observation and characterization of the habitat. This view allows an analyst to identify individual plant and animal species. It also facilitates discrimination between a pure seagrass meadow and one in which algae are growing or have been blown into the canopy. This may not be detectable through aerial imaging. This close view can also allow an analyst to distinguish live coral in a reef from dead rubble that is still standing in place. The level of encrusting algae present in the environment can be determined from videography, which may give an indirect measure of the amount of nutrient loading.\nVideography is a powerful tool when used to confirm habitat presence or clarify causes of uncertain signatures in the imagery. \u2022 Videography allows supplemental delineations in areas that are not interpretable in the imagery. While videography is a valuable tool in characterizing benthic habitats, there are a number of issues that need to be considered when incorporating this type of data into a comprehensive mapping strategy. Most pressing of these is the difficulty of relating micro-scale features visible in a video camera to macro-scale features commonly observed on aerial imagery. For example, the video record will often reveal small open areas within polygons that are mapped as continuous SAV beds. A continual consideration of minimum mapping unit and the different scales of information provided by the two data sets is needed to incorporate video information properly. In addition, detailed interpretation methods for visually categorizing habitat on video images are necessary to ensure consistency.\nWhere there are optical limitations to delineating habitat, such as those presented by deep water or turbidity. Videography is a powerful tool for making quick identifications for thematic accuracy in spot drops and for creating images of representative habitat types. Any of the above units is suitable for this. In order to relate the video to other imagery, the Center recommends that a GPS track be established for any transect work or waypoints be marked for point observations. If possible, the GPS coordinate data stream should be encoded directly onto the video tape so that the data are self referencing. There are a number of units that will accomplish this. Videography is typically used in both the signature development stage and the final accuracy assessment stage of a benthic habitat mapping project. During signature development, the video is used to assist the analyst in mapping habitat from the aerial imagery. The GPS track of the video should be overlaid onto the imagery, and the habitat delineation guided by changes along the track in the video. Videography is also used as part of the accuracy assessment process. Habitat edges detected in the video should be compared to line work in the draft data for spatial coincidence and accuracy.\nThe orientation of the track lines. Nonsystematic track line layouts tend to bias the results along certain axes.\nThe density of the survey grid. Lines that are closer together better support interpolation across the entire study area. \u2022 It is useful to produce several grids when evaluating the results of a survey. These grids are based on the raw echo returns, the bottom classification, and the bathymetry. This allows a visual check of the logic of the interpolated bottom classification. The Center currently employs the RoxAnn sensor to characterize areas not easily discernable from the aerial imagery or in sensitive or hazardous environments such as in inlets, near rocky outcrops, and in areas with rocky high-relief bottoms. This type of single-beam sensor has been most useful in the shallow water estuaries where Center projects most commonly occur. However, in deeper water areas other acoustic sensors can be just as useful.  Figure 19 shows a filled 3D output from the RoxAnn system. The track line data are interpolated to produce a comprehensive colorcoded wire mesh diagram of the bottom. This figure was generated from deepwater seagrass surveys off the west Florida continental shelf. A bathymetric spike on the left side of the figure may be due to a fish strike or a missed return. Figure 20 shows another output processed through ESRI's ArcView software. In this example, a 200-meter cell size grid has been created for the study area. The grids have been assigned a class based on the majority class of points within the 200-meter grid. The validity of the interpolation between measured transect lines will depend on the spatial variability of the bottom and the required accuracy of the interpolated data set. The survey grid in this instance was more systematic to take advantage of additional benthic sampling, which assisted in the acoustic calibration and also supported the final map generation.\nEvaluate new technologies and facilitate their adoption by Center partners through methods development.\nEmphasize comprehensive, raster format data as the preferred source data structure for habitat mapping.\nProvide data that facilitate trend analysis and capture ecologically significant changes in the distribution and spatial arrangement of benthic habitats.\nProvide data that integrate seamlessly with other spatial data for comprehensive environmental analysis. Users of this document are encouraged to consult directly with Center staff and to visit the Center's benthic habitat mapping project Web site for updates on methods, Center activities, and technical resources. It is also recommended that users contact other Center partners for recommendations in their region. Through this continuing dialogue, interested scientists will fill the existing data gaps in the nation's nearshore habitat database. Several technologies are currently in an operational phase of their development and have great promise for producing data that meet the objectives for which the following methods were designed. They include both optical and acoustic sensors."}, {"section_title": "What Can Be Mapped Using These Methods", "text": "The methods in the sections that follow rely strongly on aerial photography, in either analog or scanned digital format, as the primary data source. Aerial photography is a powerful tool for identifying habitats within the photic zone. In the nearshore estuarine and marine environment this zone can range from as shallow as 2 meters to as deep as 30 meters. These methods are well suited to deriving certain types of information about benthic habitats and are less suited to others. Some of the characteristics that can be determined using this approach are spatial extent and distribution, habitat fragmentation (expressed as a percent bottom-cover value), and qualitative measures of biomass (in the case of submerged aquatic vegetation). Characteristics that are more difficult to assess using aerial photography are condition or health, species composition, and sediment texture. In order to capture this more detailed information, as well as map in deep or turbid areas where the photography is ineffective, the Center routinely employs submersed videography and single-beam acoustics as supplemental technologies. These systems capture more specific information about benthic habitats and also detect certain habitats that cannot be effectively mapped from the air. However, these systems are limited by the scale at which they collect data and usually require some level of statistical interpolation to produce a comprehensive map. An explicit habitat classification system is not included with this document. Currently the Center is developing a comprehensive benthic classification system based strongly on the best available systems in use by the National Ocean Service (FMRI 1998), the U.S. Fish and Wildlife Service (Cowardin et al. 1979), and various state natural resource agencies. Nevertheless, some discussion about what habitats can be mapped using these methods is needed. The following habitats can be reliably mapped using aerial photography: An example of a digital benthic habitat map of the Buck Island area in the U.S. Virgin Islands produced using the methods described in this document follows in figure 1. This map illustrates a high-resolution multiclass map derived from conventional color analog aerial photography and compiled using analytical photogrammetry. In this situation no existing source of ground control was available and points had to be measured in the field. Depths in the map area range from 0 to -20 meters. The mapping was supported by two field verification visits to the study area. The classification incorporates some geomorphological or structural components that give context to the biologically oriented habitat classes. Some unique features in this area also were captured in this map, specifically the haystack coral structures. "}, {"section_title": "Document Structure", "text": "This document provides specific practical advice and guidance without prescribing a particular instrument or software. The structure of this report is loosely intended to follow typical project data development processes, specifically: A sample project plan is included as Appendix 1 that also outlines some of the major topics that should be considered during the project planning step. The Center recognizes that there are a variety of technologies and approaches to benthic habitat mapping. Several of these have great promise for meeting the mapping objectives previously listed. A brief discussion of some of these technologies is presented in Appendix 2. Additional appendices include tables that can assist with flight planning, scanning and ground control point documentation. Just as the 1995 implementation document required updates due to changes in technology, these methods will be supplemented and revised based on technological advances, emerging environmental issues, and the needs and capabilities of the coastal management community. Users of this document are strongly encouraged to consult with the Center's benthic habitat mapping project Web page (www.csc.noaa.gov/crs/bhm) for the latest updates and technical guidance."}, {"section_title": "IMAGE ACQUISITION", "text": "No element of a benthic mapping effort is more critical to project success than collecting source photography at the proper specifications and under the optimal environmental conditions. Following these procedures and carefully addressing the environmental variables will have a profound effect on the quality of any mapping. Failure to consider them can make mapping impossible. The following sections have been drawn from the 1995 report (Dobson et al. 1995) and were developed based on change detection pilot work by Dr. Randolph Ferguson and Lisa Wood of the NOAA Center for Coastal Fisheries and Habitat Research at Beaufort North Carolina. These guidelines have proven themselves in every subsequent Center project. They remain essential for successful benthic mapping. As discussed in the 1995 report, benthic habitat mapping is primarily a photogrammetric task relying on aerial photography as the primary data source. Aerial photography has been successfully used for benthic mapping of diverse habitats (Sheppard et al. 1995;Hopley 1978) and it has the following advantages for mapping aquatic habitat: \u2022 It provides wide area synoptic coverage amenable to mapping. Depending on the scale, a typical aerial photograph provides a planimetric (map-based) view of several square kilometers of the ground at consistent resolution."}, {"section_title": "Mission Specifications", "text": ""}, {"section_title": "Film", "text": "The recommended film has been Aerocolor 2445 color negative film. However, Kodak is now producing Aerocolor film 2444. This film has been applied in a Center benthic mapping project in coastal Massachusetts and initial results indicate a slight improvement over 2445. Another good choice is Aerochrome 2448 color reversal film. This film has a slower effective aerial film speed that reduces its ability to image darker submerged features but since it is a reversal film it provides a first generation product for interpretation. Aerographic 2405 black and white negative film has a sufficient spectral range to be useful for benthic mapping as well, although the loss of color hampers the ability to discriminate between certain bottom types and between dark bottoms and deeper water. Anti-vignetting filters are recommended on all these films. Haze filters (Wratten HF-3 or 2B) are also strongly encouraged to minimize the degrading effect of haze on photographic images. These filters can be used with 2445 and 2448 films. Aerocolor 2444 has an integral ultraviolet filter and normally does not require additional filtering. Polarizing filters subdue reflections from the water surface. However, their effectiveness is influenced by sun angle, and the use of polarizing filters also affects color saturation of the film. It should be noted that comparable medium-to high-speed aerial films are available from other sources as well. Infrared film is generally not recommended for delineating benthic habitats. In a North Carolina pilot project, with tandem cameras, Aerochrome 2443 false color infrared film was much less effective than color film at recording benthic features in shallow, moderately turbid water. Near-infrared light is absorbed in only a few decimeters of the water surface, and red wavelengths only penetrate to depths of a few meters (Lillesand and Keifer 2000). Conventional color film gives more information than either black-andwhite or infrared film, and can be critical for initial mapping attempts in new or unfamiliar areas. This film may permit identification of species in some tropical, clear water areas. Color negative film also appears to be better than color reversal or black-and-white film for identification of habitat under moderately turbid or hazy conditions. Color transparencies (diapositives) are dimensionally stable and are most amenable to illumination of dark areas of the photograph for viewing under magnification. Paper prints are not as dimensionally stable as transparencies (i.e., they are subject to stretching and shrinking) and typically have lower spatial resolution than diapositives due to a lower density of silver halide grains. However, paper prints don't require backlighting, which makes them more useful in the field. Flight Line Orientation and Overlap/Sidelap Flight lines can be planned using U.S. Geological Survey (USGS) 7.5-minute quads, which at 1:24,000 provide good detail for developing a flight map. NOAA nautical charts should also be consulted in setting up a flight plan since they depict bathymetry and can indicate areas where submerged aquatic vegetation (SAV) or other important benthic habitats may be located. Occasionally, airspace is restricted for military or other use. These restrictions are usually indicated on aeronautical charts, which will likely be used by the flight crew during the actual mission. Reconnaissance flights can provide valuable perspective on habitat distribution if timed to optimize visualization of shallow bottoms. Ideally, each photograph in a flight line records sufficient cultural and shoreline features to register the image to a base map, or support a digital rectification process, about one-third of the exposure. This permits correction of photographic scale and orientation to the external reference system. At a scale of 1:24,000 (1 inch = 2,000 feet), a standard 9 inch by 9 inch aerial photograph has a coverage of 18,000 feet by 18,000 feet. Large areas of open water require parallel flight lines. These should be oriented such that they support bridging of control points if aerotriangulation is planned, or to maximize the amount of land features that will assist in image rectification. Overlap of photographs includes endlap of adjacent photographs along a flight line and sidelap of photographs along parallel flight lines. Sixty percent endlap allows stereoscopic interpretation, facilitates interpretation from the most central region of the photographs, and usually compensates for loss of coverage due to sun glint in the photographs. In certain instances where sun glint is expected to be a problem endlap can be increased to 80 percent to ensure that glint does not prevent interpretation of certain areas. Sidelap of 30 percent ensures contiguous coverage of adjacent flight lines and produces a block of aerial photographs that may be subjected to photogrammetric bundle adjustment if necessary. It also supports the use of pass points for those using photogrammetric data development methods."}, {"section_title": "Scale", "text": "Photography should be obtained at a scale appropriate to the areal extent of habitat, local water conditions, type of habitat being studied, and resolution requirements for the resultant data. Determining a mission scale requires a compromise between resolution sufficient to detect small features, coverage of habitat with land features sufficient for horizontal control, and cost. Photographic scale for benthic mapping normally ranges from 1:12,000 to 1:48,000. For most estuaries, a scale of 1:24,000 represents a good balance between area coverage and small-feature detection. This scale also matches the USGS 7.5-minute topographic quad maps. For chronically turbid estuarine or brackish water areas, 1:12,000 or larger scale photographs obtained at times of minimal turbidity may be required for acceptable visualization of submerged features. Small-scale (higher altitude) photography may be necessary to bridge habitat delineated in larger-scale (low altitude) photographs to local horizontal control points on adjacent land feature that are not included in the larger-scale photos. Global Positioning System (GPS) instruments on board the airplane to position photographic centers during exposure may be used to reduce this limitation of larger-scale photography. For extensive areas of relatively clear water such as the Florida Keys, a scale of 1:48,000 may be sufficient and cost-effective."}, {"section_title": "Metric Cameras", "text": "The Center recommends the use of USGS-calibrated metric aerial framing cameras as a primary sensor for benthic mapping. These cameras have the highest quality optic lenses and are specially designed to strict tolerances to produce finely detailed photographs. The exact camera geometry is also recorded in the camera calibration report. These reports are available from the U.S. Geological Survey and provide a measure of the distortion and specifications for each camera/lens system. This information is critical to all photogrammetric applications, where the objective is to remove radial distortion inherent in the image, topographic displacement, and the effects of aircraft tilt from the image before compiling a map. These effects are accounted for in the photogrammetric orientation process and are recorded directly when the aircraft employs a GPS and an Inertial Motion Unit. Normal aerial survey mission specifications require that tilt be less than 4 degrees, crab be less than 3 degrees, and both together not exceed 5 degrees (American Society of Photogrammetry and Remote Sensing -Draft Standards for Aerial Photography, 1995). Distortions greater than these can cause offsets in the apparent horizontal locations of features both above and below the water surface."}, {"section_title": "Environmental Considerations", "text": "Knowledge of the study area is important to a successful project. This information includes the following: Primary and secondary seasonal windows and the day and time of conducting photography should be selected to optimize the visibility of habitat in the photography. Water bodies in different locations and at different times of the year will be more or less sensitive to surface waves, turbidity from local runoff, plankton blooms, and local resuspension of sediment. Tidal currents and temperature gradients can also result in material being suspended in the water column. Seasonal and daily trends for haze, cloud cover, and wind direction, duration and velocity should be included in planning for photography. The decisions of when to have the aircraft arrive at the study area (within the seasonal window) and when to collect photography should be based on NOAA National Ocean Service tide tables, local knowledge of factors affecting water clarity and depth, observation of recent weather patterns (precipitation and wind direction and speed), and water clarity. The decision to photograph should be based on ground observations of conditions on the day of the mission and finally on observations by the flight crew while in the air. It is critical to a project's success that specific individuals be charged with making these determinations (one ground crew and one flight crew), and that these individuals remain in close contact. In most instances the ground-crew member will be the mapping project leader. The flight-crew member will often be part of an aerial survey company. Both individuals must have a common appreciation of the importance of proper environmental conditions to mission success. Project plans should allow two or three months for primary and secondary photographic windows to assure the occurrence of optimal conditions for photography. For singleday missions it may be possible to have the plane and flight crew fly to the study area on the day of photography. In the North Carolina pilot project, staging of the plane and flight crews to the study area several times for periods of several days was required to complete missions involving more than one day of actual photography. In tropical clear-water areas these windows can probably be greatly reduced, while in the Pacific northwest they may have to be doubled. The decision to fly should be driven primarily by atmospheric conditions (clouds), water clarity, and tidal stage. Other conditions should be optimized as much as possible once these variables have been addressed."}, {"section_title": "Phenology", "text": "All other conditions being acceptable, the best time of year to acquire photography is during the season of maximum biomass or flowering of dominant species. Considering the phenologic overlap, that is the season when most plants or communities are at their peak, for the entire community. This peak is June for the SAV of the Pacific Northwest and Atlantic Northeast, April and May for the southeastern U.S., and September for most of the other species of SAV in the eastern U.S. While biomass may be high during the summer months, in many areas the strong potential for runoff from convective storms and haze during this season limits the ability to acquire adequate photography. Figure 2, on the following page, illustrates the apparent differences in habitat that can result from changes due to phenological stage of seagrass. The frame on the left was acquired in the spring while manatee grass was in full foliage. The frame to the right was acquired later in the season when above-ground biomass had acquiesced (died back). There is no real loss of seagrass between the two dates. \nField observations are best made as close as possible to the date of photo acquisition to facilitate comparison between data recorded in the field and signatures observed on the imagery. If this is not possible, the field team may find it advantageous to conduct fieldwork during the same month that the photography was acquired to reduce the error from observations made during the growing and nongrowing season of SAV. Thus, if the photography was acquired in June, when SAV has the highest biomass, and the fieldwork were to be conducted in January when biomass is at its lowest, the differences in observation would likely be due to the differences in the phenology."}, {"section_title": "Turbidity", "text": "Aerial photography should be conducted when turbidity is low. Care should be exercised in areas adjacent to sources of suspended sediment and nutrients. Data collection should be avoided during seasonal phytoplankton blooms. These blooms may also occur several days after rain events. Data acquisition also should be avoided immediately following heavy rains or persistent strong winds, which can deliver sediment loads into nearshore waters or resuspend sediment on the bottom. Potential days for photography are those during the photographic window for which high-water clarity is expected, based on local experience, recent weather patterns, and surfacelevel observation. The ground and flight crews should confirm water clarity on the day of photography. The subtle effects of turbidity on image interpretability can be seen in figure 3. In the left frame (1988) the edge of a mixed SAV bed in the lower left is clearly defined, while an algal bloom has obscured this same edge in the frame at right (1996). \nMany observations can be made by swimming, snorkeling, scuba diving and with the assistance of a towed video camera. The field team may find it advantageous to conduct verification activities when phytoplankton blooms are not in season, or after periods of heavy wind or rain, in order to decrease the amount of time required to adequately characterize the site."}, {"section_title": "Tidal Stage", "text": "Generally, aerial photography should be collected within approximately 2 hours of the lowest tide as predicted by the NOAA National Ocean Service tide tables, although factors affecting water depth and water clarity should be considered simultaneously. In general, extreme low tide, which may be -0.5 to -1.0 or more meters lower than mean low water around the coast of the U.S., is preferred, if other constraints have been met. In some estuaries there is a significant lag in tidal stage, which should be considered for data acquisition. There are instances, however, where low or falling tide may not be optimal. This can occur in estuaries where tannic or turbid water is draining out through an inlet during falling tides. A rising tide in these cases can cause an inflow of clear marine water that improves the ability to see bottom features. In certain areas, strong tidal flow can resuspend sediments and degrade water clarity. Missions should be timed to avoid these events in these locations."}, {"section_title": "Wind and Surface Waves", "text": "For aerial photography, the absence of wind and waves is the ideal condition. Winds of 0 -5 miles per hour (mph) are generally not a problem and winds from 5 -10 mph may be acceptable. The direction, persistence, fetch (the distance that wind can blow unobstructed over water), and recent wind events should be taken into account. Winds blowing offshore typically pose less of a problem than winds coming in from open water. Breaking waves and associated turbidity, whitecaps, wrack lines, and/or floating debris should not be visible from the air or in the photographs. For some areas, ocean swell can be an important consideration and should not exceed 3 feet.  "}, {"section_title": "Sun Angle", "text": "Sun angle affects the illumination of benthic features, shadows from tall shoreline features, and the amount of glint in aerial imagery. Sun glint can preclude visualization of benthic features where it occurs in the photograph. As sun angle increases, glint also increases and moves from the edge toward the center of the photograph. Using a typical mapping camera equipped with a six-inch focal length lens mounted in a standard vertical camera mount, the maximum angular field of view is approximately 94 degrees, or 47 degrees off-nadir (into the corners of the photograph). Sun elevations of 43 degrees would just begin to create glint under these conditions. As sun angle increases, sun glint also increases and moves from the edge toward the center of the photograph. Sun angles ranging between 30 degrees and 45 degrees are recommended. Sun angles above 30 degrees illuminate the bottom sufficiently for photographic purposes while angles above 45 degrees tend to produce glint. Sun glint can be partially compensated for by acquiring imagery with endlap of 60 percent. This endlap has the additional benefit of facilitating stereo analysis of the imagery. Increasing the endlap to 80 percent will improve coverage when higher sun angles cannot be avoided. Sun glint can also be minimized when the sun and land are on the same side of the plane because sun glint is not as detrimental to the imagery when it occurs on land. Shadows from tall objects on shore such as trees, however, can preclude visualization of benthic features and may be a factor when the land and sun are on the same side of the plane.  Clouds and Haze It is best to have no clouds or haze in benthic photography. Haze will reduce the contrast in any aerial photography and hinder the ability to distinguish benthic features and habitats. Clouds pose a problem not only when they occur over water, but also when they occur over surrounding shoreline areas, thus reducing the potential source of features for use as control or pass points to support image rectification and photogrammetry. Cloud shadows have almost as serious an impact on water features as clouds. The maximum amount of cloud cover recommended is 5 percent. Figure 6 shows how cloud shadows on water can be confused with SAV beds or make interpretation impossible. In many regions of the country, cumulus clouds tend to form during warm, summer months in the early afternoon. Photography acquired in the morning generally has a better chance of avoiding interference from these convective clouds; however, in certain regions, convective clouds can even be a problem during this time of day."}, {"section_title": "COLLECTION OF GROUND CONTROL POINTS FOR IMAGE RECTIFICATION", "text": "Rectification of aerial imagery over extensive water bodies poses the greatest challenge to benthic mapping. Even in areas where habitat can be successfully imaged and mapped, the lack of usable ground control data to bring the data into a spatial reference plane can severely hamper the mapping effort. Typically, project leaders will address this issue by either obtaining spatial location information (x, y, z coordinates) from another image or map source or by measuring image features on the ground with surveying instruments or GPS receivers. With the improvement in the coastal Continuously Operating Reference Stations (CORS) and radio beacons operated by the U.S. Coast Guard and other agencies, Global Positioning System (GPS) technology has been established as a powerful way of making direct ground control point measurements. The following sections outline guidance for this type of measurement as well as recommendations for using other sources of control."}, {"section_title": "Ground Control Point Selection and Measurement with GPS", "text": "In cases where no acceptable raster or existing point source of ground control points (GCPs) exists for photorectification, it will be necessary for the project to collect its own. If the resources permit, it is often worthwhile to collect at least some independent GCPs as a cross-reference to a raster control source that may provide the bulk of GCPs. It is recommended that GCPs be selected after the imagery has been received and screened. In this way, points can be selected with the best possible distribution throughout the study area and for the individual photo frames. Collecting GCPs is a field-intensive process, so all effort should be made to be efficient in selecting the points."}, {"section_title": "Point Selection Criteria", "text": "Choosing the right features for GCPs will make the measurement process easier and also facilitate the later image rectification. Good GCPs should meet the following criteria: \u2022 They should be fixed cultural features that are unlikely to shift position with time. \u2022 They should be easily visible on the photographs. This will help when the time comes to measure the point during the rectification process. \u2022 They should be easily accessible, preferably by vehicle. If a point can be measured without walking a long distance then more points can be measured in a given time. \u2022 They should be on public land or rights-of-way to minimize access restriction by fences and trespass laws. \u2022 They should be located away from tall buildings, large metal objects, and power lines that may cause multipath effects in the signal. \u2022 They should be located in open areas away from heavy tree cover to minimize the potential loss of satellite signals. Examples of good GCPs are sidewalk corners and intersections, the corners of paved parking lots, railroad crossings (in cases where it is safe), and piers and pier abutments. Less desirable GCPs are trees or bushes, curves or banks of streams, corners of buildings, and areas of heavy vegetation cover or sheer terrain."}, {"section_title": "Point Distribution", "text": "GCPs should be selected after a review of the photography and should be well distributed throughout the photo. Photogrammetric orientation solutions require a minimum of three points to establish a model. However, it is advisable to collect at least several additional points (five or six) to allow rejection of points that don't contribute to an acceptable solution. In cases where multiple flight lines converge, more points should be collected to ensure that polygons delineated on adjacent frames are not out of position with each other. Another case where additional points are recommended occurs when a flight line begins to extend into remote areas where no control is available, as in an area where triangulation or bridging is required. In this case, errors occurring along the triangulated strip are minimized to the greatest extent possible by a robust rectification at the beginning. Linear arrangements of points or points clustered in small portions of the photographs will typically not produce the most accurate rectifications. If full aerotriangulation is anticipated for the mapping project, a greatly reduced number of points is required. In these instances GCPs should be collected primarily at the beginning and ends of flight lines and around the edges of parallel lines in a block. Point Measurement and Documentation As GCPs are chosen during a review of the photography, it is recommended that the points be annotated in some way on a set of working prints. This assists in later measurement of the points during the rectification process. In addition to documenting the points on the photos, a data sheet describing each point should be completed. The most important component of this sheet is a detailed sketch of the actual feature being mapped. In many cases the scale of the working print will not allow this to be adequately annotated. The detailed drawing on the data sheet will be valuable later during the rectification process when the photos can be observed under high magnification. The sheets also form a cross record for ensuring that all points were measured. A sample sheet is included at the end of this document in Appendix 5. The actual measurement or recording of Global Positioning System signals while on site should normally follow common GPS practices. The goal of this process is to produce a positional record for the feature that is accurate to less than 1 meter. Using either real-time differential correction from a radio beacon, or post-processing, the field data should produce data that meet this requirement, provided the GPS receiver is of sufficient quality. GPS receivers that are capable of tracking 12 satellites (12-channel) are more than adequate for this task and are commonly available on the commercial market. Typical settings for GPS data collection are:  "}, {"section_title": "Sources of Differential Correction Radio Signals", "text": "With the recent deactivation of Selective Availability (SA) by the U.S. Department of Defense, the performance of uncorrected GPS receivers was greatly enhanced. Previously, uncorrected signals could produce errors on the scale of hundreds of meters. Without SA, expected positional errors are on the scale of tens of meters. While this has improved general navigation, this accuracy is still usually too low for use in GCP measurement. In order to produce data of acceptable spatial accuracy, GCPs should be measured using differential GPS. Both real-time corrections and correction through post-processing are acceptable. "}, {"section_title": "Alternative Sources of Control", "text": "Although taking measurements at GCPs specifically selected for a project ensures that the quality and distribution of measurements are sufficient for project needs, there are several reasons why it would be worth exploring alternate sources of control. The reasons for this might be as follows: \u2022 The study area is remote and would require extensive travel to visit."}, {"section_title": "PHOTOINTERPRETATION AND IMAGE ANALYSIS", "text": "The following section draws on Chapter 4 of Dobson et al. 1995; however, since 1995 mapping efforts using these methods have been applied to many other habitats besides SAV. Other habitats that have been successfully mapped include algae, corals, and hardbottom habitats. Habitats such as sand flats, mud areas, and rubble bottoms have also occasionally been mapped. All of these habitats can be identified in aerial photos within the photic zone. Specific guidance on actual polygon delineation is provided in the following discussion based on experiences in Indian River Lagoon, Florida, and other project areas around the country. Benthic habitats can be interpreted from metric-quality aerial photographs acquired as recommended in the previous sections. The accurate identification of these habitats in aerial photographs requires visual evaluation of the fundamental elements of image interpretation (tone, color, contrast, texture, shadow, etc.). This type of mapping also requires experience at ground level in the study area since the photographic images of habitat features vary in ways that cannot readily be modeled, described, or communicated. Training for a habitat change analysis effort includes literature research, discussions with local ecologists and biologists, site visits, overflights in small aircraft, and examination of historical aerial photographs of the area (Maragos and Cook 1995). Training of photointerpreters is active throughout the life of the project. Photointerpretation of shallow-water benthic habitats is best accomplished using stereo pairs of photographs and high quality stereoscopic instruments (e.g. Wild, AVIOPRET, APT2, stereoscopes). Polygons can then be annotated on overlays fixed to each photograph or digitally delineated through a screen display or stereoplotter view. To be delineated as a particular habitat, recognizable and verified signatures associated with that habitat must be present in the photographs (Hopley 1978). These habitat signatures will often vary depending upon species composition, bottom sediment, depth, season, haze, clouds, water clarity and surface disturbances, and sun angle at the time of photography. The analyst will need to account for this variability in habitat signature and aggregate them into one habitat class where appropriate or discriminate between habitat classes during the mapping interpretation process. The designation of a given area as a particular habitat is a function of minimum detection unit, minimum mapping unit (mmu), the resolving power/resolution of the image data, and the area's proximity to other similar habitats. The minimum detection unit is the smallest feature that can be discerned in the imagery. Assuming a photographic scale of 1:24,000, high-quality optics, high-resolution film, and ideal conditions, it is usually possible to have a minimum detection unit of approximately 1 meter. In pilot work at Buck Island, Virgin Islands, with clear marine water, features of this size could be seen at depths of over 25 meters (80 feet). All detected habitat types that appear to be in a continuum with the same adjacent habitat types in an area that exceeds 0.03 hectares will be mapped as a single polygon. The minimum mapping unit is the smallest area to be mapped as habitat. At a map scale of 1:24,000, the minimum mapping unit is 0.03 hectares (0.25 acres) for benthic habitats (i.e., a diameter of about 0.8 millimeters on the map represents a diameter of about 20 meters or an area of about 0.03 hectares on the ground). Therefore, isolated groups of shoots with a diameter of less than 20 meters may be detected but not mapped as habitat. The presence of a representative bottom signature in the photograph defines habitat if the following conditions are met: \u2022 The total area exceeds 0.03 hectares."}, {"section_title": "Recommended Interpretation Decision Rules", "text": "The following guidelines/strategies for interpretation are presented as general guidelines to help standardize interpretation. These recommendations are based on technical guidance used by the St. Johns River and South Florida Water Management Districts for their ongoing SAV monitoring/mapping project. These recommendations Figure 10. This example shows a sample delineation derived using the interpretation decision rules listed above. Small patches of SAV in this area have been aggregated where the patches are closer than the minimum mapping unit (mmu) and the aggregated area of patchiness is larger than the mmu. are based on the need to conduct change analysis over large areas with as much consistency as possible. Local needs and project scope may require some modification of these decision rules. \u2022 Outer boundaries of beds are more important than internal structure (patchiness, shapes or sand patches within) of beds."}, {"section_title": "DATA DEVELOPMENT METHODS", "text": "The Center recommends three methods for creating digital habitat data from aerial photography and other supplemental data streams. These methods are designed to maximize the capture of thematically and spatially accurate data on benthic habitat. Three methods are presented to provide guidance to agencies with varying levels of technical capacity. They are also intended to be applicable by state and private-sector scientists and environmental technicians working in a variety of environmental settings. All the methods assume a certain level of expertise in remote sensing and photointerpretation. The objective in each case is to identify all living benthic habitat despite the interference presented by the water column. An additional objective is to produce spatially precise baseline data that minimizes positional offsets that can detract from the accuracy of subsequent change detection efforts. Three methods listed in order of preference are recommended for producing benthic data:"}, {"section_title": "Analytical Photogrammetry", "text": "This method is the preferred technique for generating digital benthic data. The method has been applied successfully by several of the water management districts in the state of Florida. This method employs a stereoplotter to delineate and simultaneously rectify and digitize habitat polygons. The three-dimensional stereo model of the aerial photographs is leveled and scaled in the analytical plotter (AP), and the interpreter views a three-dimensional landscape during photointerpretation. All polygonal interpretations are automatically stored in digital x, y, and z coordinates in their proper planimetric position during photointerpretation (Welch et al. 1992), avoiding any error which might arise during information transfer using a zoom transfer scope or hand digitization where base map inaccuracies may be a factor. The analytical photogrammetric process employs either original or duplicate film diapositives. This is the optimal interpretation medium for photomapping. Analytical stereoplotters typically have system resolutions on the order of 1 to 2 microns. Therefore, to reduce the effects of film shrinkage and distortion that could affect the orientation process, it is recommended that film be processed to a 7mil estar base for dimensional stability. In addition to stable diapositives, this method requires ground control points with elevational values to correct for terrain displacement, radial distortion, and aircraft positional aberrations (roll, pitch and yaw). These ground control points can be obtained from a variety of sources, as well as created through GPS as part of the project activities as previously discussed. Detailed information on the camera specifications is also needed for this method. This is available through the camera calibration report. The USGS maintains a calibration report archive for many of the metric cameras used by government and aerial survey firms in the U.S. This can be obtained through formal request or by accessing the camera calibration database maintained by the USGS Optical Science Lab. A typical calibration report contains highly accurate measurements of a camera's lens distortion, film plane geometry, and focal length. The analytical method consists of three orientation processes (interior, relative, and absolute). An aerotriangulation step is then required to adjust a block or strip of photo frames. Once these corrections are complete, the interpretation/digitization of habitat polygons is accomplished in one step with great spatial precision."}, {"section_title": "Orientation Process", "text": "The interior orientation consists of specifying camera parameters that will set up the image on the stage of the instrument. It also will guide the setup of all subsequent photos in a project. The interior orientation process is accomplished by measuring all camera fiducials. The Center recommends very strict tolerances for this process since all error in the orientation process will propagate throughout the rest of the compilation process. Typical thresholds for this orientation are less than 5 microns. The relative orientation process follows and involves setting up a stereo model that allows three-dimensional viewing through two overlapping photos. This is accomplished by selecting scheme points common to the two photos and measuring them in three dimensions. The Center recommends a set of 12 paired scheme points that are well distributed through the overlap area between the two photos, although a minimum of 6 can be used to produce a solution. The large open-water areas typical to benthic mapping make this difficult, since bottom features may be required to serve as scheme points. It is important that scheme point features be clearly visible on both photos and not obscured by glint, turbidity, or poor exposure on one frame. In areas with large water expanses the Center recommends that additional scheme points on land areas be added to the normal 12. Once the stereo model has been set up through the relative orientation process, an absolute orientation is needed to introduce the image coordinate system into a geographic reference plane. It is necessary that the reference plane be cartesian; therefore the Center recommends that a Universal Transverse Mercator or State Plane coordinate system be used. During the absolute orientation, ground control point features are measured in the photography. The x, y, and z coordinate values for each point are input into a table/database prior to the measuring process, either manually or by importing existing tabular data. In cases where the aircraft has GPS/IMU data available, these data can be imported into the orientation process, thus greatly reducing the need for measured Ground Control Points (GCPs). At the end of this process, each image coordinate will have a corresponding real-world reference coordinate and compilation (delineation of habitat boundaries) can begin. The interpretation rules described in the following sections serve as a guideline during this process."}, {"section_title": "Aerotriangulation", "text": "In cases where multiple strips of photography are required for the project, or in areas where GCPs are not available, an aerotriangulation process will be required. In fact, it is recommended that this procedure be used whenever possible, as it reduces the number of GCPs that have to be measured. This process will rely on tie points common to exterior oriented photos to rectify photos without any actual GCPs within them. If GPS/IMU information on photo centers is available, these data will also greatly facilitate the aerotriangulation process, especially over open-water areas. A triangulation algorithm will typically employ an iterative least-squares approach to adjusting the block or strip of photography. The convergence value at which this process ceases should be in ground units. It is especially important in the aerotriangulation process to measure tie points accurately and have a minimal convergence value. At the conclusion of the triangulation and compilation process, a set of spatially precise positionally registered habitat polygons will be exported from the photogrammetric environment. A subsequent effort to build polygon topology, perform Quality Assurance/Quality Control (QA/QC), and attribute the polygon data will still be needed."}, {"section_title": "Recommendations", "text": "Analytical stereoplotters are very expensive and require special training. Some additional expense to locate x, y, and z control points may be necessary to successfully level the block of aerial photography. This technology may be outside the capability of many Center partners. For this reason it is recommended that this technique be employed in the following cases: \u2022 When there is a need for a spatially precise baseline to support future change detection projects. These often require high levels of positional accuracy to reduce change caused by positional shifts between data sets. \u2022 When the data are likely to be critical to local permitting activities.\nBased on the characteristics of soft-copy photogrammetry, the Center recommends its use in the following situations: \nThis method is the simplest and easiest to implement by the greatest number of Center cooperators. It requires the least capital investment of any of the methods. In the hands of experienced and diligent photointerpreters, it can produce benthic habitat data suitable for detailed analysis and for change detection. While this document prefers photogrammetric methods over analog interpretation, it is useful under the following conditions: \u2022 When the experience and skill level of the photointerpreters is high but the technical resources available to support the work are low."}, {"section_title": "Soft-Copy Photogrammetry", "text": "Recent advances in soft-copy photogrammetry allow analytical stereoplotter functions to be accomplished using computer workstations and image processing software (e.g. ERDAS OrthoBase, Socet Set, Softplotter, and Intergraph). Therefore, this alternative will become more affordable and attractive in the future. Fundamental to the soft-copy approach is the fact that the analysis relies on a native digital or scanned analog aerial photograph. In order to preserve image geometry and the spectral balance in the native imagery, photogrammetric scanners are recommended to produce the scans. The scan resolution (pixels per inch) will vary based on the scale of the original imagery, but should be sufficient to capture small features that allow identification of different habitats. Since texture plays such a primary role in this habitat determination, preserving a high spatial resolution is important to this process. A minimum resolution that has proven satisfactory for soft-copy interpretation is 1 meter, although higher resolutions are desirable if file size and computational capacity do not become limiting factors. In cases where the original film diapositives remain available to the analyst, lower resolutions can be used. In areas where making an interpretation from the screen is difficult, the analyst can then fall back on the film data."}, {"section_title": "Orientation Process and Aerotriangulation", "text": "The orientation steps required to build stereo models and register them to a ground reference plane are the same in soft-copy photogrammetry as in analytical. One advantage of some soft-copy systems is the ability to automatically generate large numbers of pass points based on similar image signatures between stereopairs. The analyst can then choose the best of these points when conducting the relative and absolute orientations. These points are also then useful in the aerotriangulation process. Because the images are stored in digital format, large numbers of images can be called up quickly when doing the block adjustment, unlike analytical instruments, which require the analyst to reload film diapositives and redo the interior orientation to accomplish the same step. If a digital elevation model (DEM) exists for the region, the scanned photography can be orthorectified to this digital data set. As in analytical photogrammetry, aircraft kinematic GPS and IMU data provide detailed information on aircraft position and camera nadir location. These data are stored digitally and assist in rectification of imagery over open water with no GCPs."}, {"section_title": "Image Manipulation", "text": "In cases where the analyst is interpreting directly from a scanned photograph, there are a variety of image processing techniques that can assist in delineating habitat. These include histogram manipulation and stretches to highlight dark areas and increase contrast, and also spatial filters to emphasize certain spatial patterns in the data. Two useful techniques are histogram matching and low-pass filtering (Hale and Frazer 1997). These tools can assist an analyst during the mapping process but should be used carefully to avoid misinterpretation of image artifacts and noise. Spectral clustering of aerial photography has been employed in certain studies to produce a quick initial organization of the photo data. These routines can highlight certain features, especially sand bottoms, but they are extremely vulnerable to misclassification from spectrally similar features on land or from image artifacts such as glint or sea state. This approach is not recommended as a mapping method. The level of effort needed to extract meaningful habitat information from clustered aerial photography is usually significantly greater than that required of a skilled analyst using the spectral and spatial data inherent in the photo to perform a manual interpretation. However, in certain localized areas of a study area image manipulation may be useful as an ancillary interpretation tool. When directly interpreting/compiling from scanned rectified aerial photos it is recommended that, in areas of overlapping coverage, interpretation be done in the center portion of the photo. Flight plans that incorporate at least 30 percent sidelap and 60 percent endlap will facilitate this approach. The soft-copy photogrammetric method can produce spatially precise habitat polygonal data of comparable quality to that produced in an analytical environment. However, there are some advantages and disadvantages to this method that should be considered when there is a choice of whether to use either the analytical or soft-copy approach. \u2022 Skill requirements and computer hardware necessary to conduct this type of mapping are not as high as for analytical photogrammetry but are substantial nevertheless."}, {"section_title": "Analog Interpretation", "text": "Analog interpretation is the traditional method for generating data from aerial photography. This method involves visually interpreting original or duplicate film and delineating habitat polygons on transparent or semi-transparent overlays attached to the photos. The Center recommends that film diapositives be viewed on a light table to accomplish the interpretation. Original film materials (reversal films) are preferable due to the loss of detail that occurs during the duplication or printing process. Transparent polyfilm is the recommended overlay material since it is structurally stable and doesn't hinder the interpretation process. Each overlay should be registered to the photo by annotating the fiducial marks and some portion of the film titling (usually the frame, roll, date, and mission identification) to allow the overlay to be removed and reattached correctly. Stereopairs with overlays attached (adjacent photographs with overlap) are best interpreted using high quality stereoscopic instruments (e.g. Leica SD3000, Wild AVIOPRET, APT2, B&L stereoscopes, etc.). If overlap is 60 percent or greater, only every other frame need be annotated. The alternate frame can then be devoted to fieldwork. High-quality drafting pens are recommended for the annotation process. Pen sizes of 4x0 or 3x0 produce a fine continuous line that can be easily digitized at a scale of 1:24,000; these pens produce lines of 4.3 and 6.0 meters width on the ground. Once the film has been interpreted and overlays completed, it is then necessary to bring the interpreted habitat polygons into a corrected reference frame and capture the information digitally. The reference process usually involves using a zoom transfer scope to transfer the photo overlay annotations to a hard-copy planimetric base map of some type. USGS 7.5-minute quadrangles are the usual choice for this process, although printed orthoquads and NOAA nautical charts are also commonly used. A second acetate or mylar overlay is often attached to the base map. The zoom transfer scope allows rubber sheeting of the photo overlay and superimposes the view on a simultaneous view of the base map using multiple optical lenses. In this manner the distortion and displacement inherent in all aerial photography is removed. Habitat delineations from the aerial photo are then traced onto the base map overlay using technical drafting pens."}, {"section_title": "Table Digitization", "text": "Following transfer of all polygonal annotations to the base map mylars, the linework is digitized through manual table digitization; alternatively, vector polygon data can be generated after scanning the overlay and running an automated digitization routine (ArcTrace, R2V). Although an analyst is still required to assist in the automated decision process, these vectorization routines have the potential to greatly reduce the labor associated with the digitization/vectorization process. For these routines to work smoothly, the linework should be heavy enough and contain enough contrast from the background mylar to be easily captured during the scanning process and thus recognized by the vectorization algorithm. For projects employing the analog method and a zoom transfer rectification process, it will be necessary to bring line work that has been transferred from the photography to a hard-copy base map into a digital environment. In table digitizing, a copy of the base map is attached to a digitizing table and registered on the table using a GIS software such as ArcInfo. Digitization normally is accomplished by first measuring a set of graticule tic marks on the map and, once the results show an acceptable spatial error (expressed as root mean square error or RMSE), then tracing the line work with the digitizing puck. Because the process relies on hard-copy maps, it is recommended that stable media such as mylar be used for the base maps whenever possible to minimize the spatial error introduced by shrinkage of the map media. Some simple steps to successful table digitizing include the following: \u2022 Use the highest quality maps. This includes the best material, the most appropriate scale, and the most recent publication dates. \u2022 Establish a procedure for the digitizing. This typically involves digitizing one feature at a time, using a fixed naming convention, etc."}, {"section_title": "Screen Digitization of Manually Delineated Habitat Polygons", "text": "An alternative approach to zoom transfer and table digitization is the screen digitization approach. In this method the aerial photo, with interpreted overlay attached, will be scanned and rectified using control derived from GCPs or other rectified images. In the Center's Florida Bay project, interpreted source diapositives at a scale of 1:48,000 were scanned at 600 ppi (pixels per inch). This resulted in a digital image with a resolution of 2 meters. This image provided enough detail to conduct an image-to-image rectification with digital orthophotoquads provided by the state and adequately captured the line work on the overlay. Once the photos/overlays have been scanned and rectified the line work will then be digitized to create the digital polygonal data set. There are several software packages that allow digitization and attribution of the line work. The analyst will manually trace the scanned line work or will employ a vectorizing software that will automatically create a preliminary vector line coverage. When manually digitizing the scanned line work, it is important that the analyst trace the line consistently. Most linework on acetate is annotated using high-quality drafting pens. Depending on the gauge of pen used, the delineated lines can be fairly thick. The analyst should track either the center of the line so that small manual errors during the digitization process do not alter the delineated polygon boundaries. Automated digitizing software (R2V, ArcTrace, etc.) exists that can greatly facilitate the process. Some of these routines capture a digital line coverage in one pass that then requires an analyst to edit to close polygons, connect arcs, or remove artifacts. Others require iterative decision making by the analyst to assist the routine in capturing the appropriate lines during breaks or intersections. At the conclusion of the initial line work generation process, additional processes will be required to produce a polygon vector coverage with all appropriate habitat attributes. These include editing, building, cleaning, and labeling the polygonal line work. Once these preparatory steps are completed, the line work is ready for digitization. A general rule is to avoid working near the edge of the table where inaccuracies are sometimes introduced. The process should start with tic measurements to check RMSE values. RMSE values at this stage of the process are expressed in terms of digitizer inches. General values of 0.003 to 0.004 are good for paper maps at 1:24,000. The actual RMSE values for a project should reflect the spatial accuracy requirements of the data in ground units. During the digitizing process it is recommended that only one feature at a time be digitized, such as shoreline, then one class of polygons, etc. It is also recommended that line work should be traced to overshoot intersection points. These can be removed later in the editing process. At the conclusion of either the screen process (digitizing or interpretation/digitizing) or the table process, the topology building and editing process will follow."}, {"section_title": "EDITING AND ATTRIBUTING POLYGONAL HABITAT DATA", "text": "A final component of benthic mapping from aerial photography is bringing delineated habitat polygons into a digital format with polygon topology and attributes. Due to its widespread availability and relative ease of manipulation, preferred formats for this digital line work are Environmental Systems Research Institute (ESRI) polygon coverages or shapefiles. The editing and attribution process will be needed after the following two data development processes: \u2022 After exporting photogrammetrically derived line work into either Arc generate, auto-cad (.dxf), or design (.dgn) files (analytical or soft-copy methods)."}, {"section_title": "Building and Cleaning Polygon Topology", "text": "Once line work has been digitized, it is necessary to establish the delineated areas as discrete polygon units recognizable in a GIS software environment. Two ESRI Arc commands accomplish this, Build and Clean. The Clean command generates a coverage with correct polygon or arc-node topology. Clean edits and corrects geometric coordinate errors, assembles arcs into polygons and creates feature attribute information for each polygon or arc; that is, it creates a polygon attribute table (PAT) or arc attribute table (AAT). Clean can be used to process line coverages in which intersecting arcs must be split, or to re-create arc-node topology after editing. This tool will assist in the prevention of dangles and unclosed polygons. It is critical that attention be given to setting the tolerances for the clean process. Applying the clean process with inappropriate settings can severely damage a coverage. The choice of a fuzzy tolerance is especially important. No two coordinates (vertices along an arc) in the output coverage will be within this distance of each other. In this respect the fuzzy tolerance determines the detail of the output coverage and strongly influences its resolution. A fuzzy tolerance that is too small may prevent the clean process from resolving congruent arc segments, that is, parallel arc segments that are within the fuzzy tolerance of one another. Existing slivers or undershoots may not be successfully cleaned. Conversely, a fuzzy tolerance that is too large may collapse polygons or merge arcs that should not merge. Specific parameters that must be set for both the Clean and digitizing process include setting arc and node snap tolerances, weeding and grain tolerance.  Arcsnap. When the arcsnap option is enabled, new and edited lines that do not terminate at existing nodes will be automatically snapped to the nearest line if there is a line within the set snap distance. A node will be created at this new intersection. Figure 12 illustrates how arcsnap affects dangling lines and nodes. Weed. If Weed is enabled, vertices must be at least the weed distance apart to be retained. Grain tolerance. Specify the distance between adjacent vertices in lines. This parameter is used when lines are splined or densified. When using Clean with the Poly option, polygon label points are recommended but not required. Do not run Clean on a geographic coverage. Geographic coverages have units in decimal degrees, decimal seconds, radians, etc. These units are designed to measure angles. They do not measure distances. They represent a spherical coordinate system and should not be confused with a two-dimensional rectilinear coordinate system. Users must first project a geographic coverage to a suitable projection to convert angles of latitude/longitude to a Cartesian coordinate system using distance units such as feet, meters, or kilometers. As a general practice, when editing ESRI vector coverages it is advisable to follow a sequential naming convention that increases with each editing session. This ensures that data can be reconstructed if a mistake is made and ensures that the Info file associated with the coverage is always up-to-date. Once a clean polygon coverage has been constructed, the polygons must be attributed according to their habitat class by assigning polygon labels. This can be a meticulous process, and it is often helpful to have the source photography and sometimes a hardcopy printout of the digital line work to assist in this process."}, {"section_title": "Reviewing Edited Polygon Coverages", "text": "The Quality Assurance and Quality Control process for reviewing vector data in detailed habitat maps is especially important. There are two processes involved in reviewing the line work digitized from the rectified images. Both rely on visual inspection by a technician. The first process utilizes a digital display of the scanned and rectified imagery. The polygonal vector data are overlaid onto the imagery and reviewed to assure all lines have been successfully captured from the overlay. This is readily accomplished by examining each frame of photography with the line work assigned an easily viewable color such as yellow, magenta or red. All line work shared by adjacent photos must be edge matched (this also serves as a check of the relative quality of the image rectification). Once the analyst is assured that all the line work has been captured and all polygons closed, especially where line work extends over adjacent photos, the next step is to check for attribute accuracy and completeness. All polygons should have label points and polygonal topology (see discussion of cleaning and building topology). Each habitat class should be expressed as an ASCII string and also as a numeric value. This allows a user to examine the table for habitat type, and the numeric value allows statistical examination and manipulation of the habitat data by class. This becomes important when generating random samples for accuracy assessment. Each class should also be assigned a unique color, and the coverage should be checked for obvious errors, i.e., inclusion polygons that are most likely non-SAV and adjacent polygons of the same class. Another check should be made with only the centroids, assigned a unique habitat class color or habitat attribute, displayed. This allows the line work and any identification code on the imagery to be viewed through the polygonal data. It is often helpful to conduct both these reviews with hard-copy printouts. Test plots of the line work should be printed out at the same scale as the photography and compared to the photos on a light table. Both line work and attributes can then be assessed and any unacceptable data flagged for further editing in the digital environment. In complicated coverages over larger areas, it is helpful to work with the assistance of a grid laid over the printed linework to reduce the chances of missing an item or rechecking areas that have already been reviewed. Queries of the polygon attribute table are also recommended to check for naming consistency and other issues."}, {"section_title": "FIELD SURVEYS", "text": "Field surveys are critical to any successful remote sensing project. Due to the difficulties of mapping submerged habitat imposed by the intervening water column, field surveys are even more critical to creating accurate benthic data. The Center has traditionally employed extensive field surveys in all of its benthic mapping projects, and this component will continue to be an important element of future project work. Field surveys provide critical opportunities for educating image analysts, verifying the accuracy of data, deploying ancillary technologies to assist in the mapping, and documenting more detailed habitat character and conditions. However, field surveys are often one of the more costly components of a project. For this reason, efficiency in conducting field operations is especially important. Field activities typically fall into one of two general categories, signature development and accuracy assessment, the former occurring primarily at the beginning or during a project and the latter at the end. Regardless of whether field verification is for signature development or final accuracy assessment, many of the methods for recording site information and the logistics associated with fieldwork will be the same. Because benthic field verification can occasionally involve operation of electronic equipment from small craft, planning for environmental, and other conditions is especially important. The following sections present some of the issues to be considered for field visit planning, logistics, and equipment."}, {"section_title": "Planning", "text": "Successful fieldwork is contingent upon good planning. Project leaders need to consider many factors before leading a team onto the water. During the initial planning of a field trip, it is important to identify the goals and desired results of the fieldwork. The goal will determine the types of data that need to be collected and the amount of preparation time. Other questions must be considered as well: \u2022 Is this fieldwork for signature development or accuracy assessment? \u2022 Considering the goal, what are the minimum amounts of sites that need to be visited? For most fieldwork, a boat will be necessary to access sites. It is important to consider the size of the team and the amount of equipment that can fit comfortably on the boat. The boat must be suitable for the type of fieldwork to be conducted; a dingy would not be suitable for offshore fieldwork, while a large deep-draft boat would not be suitable for getting into shallow marshes. Once the goals are clearly identified, a determination of what equipment is necessary to conduct the fieldwork must be made. Two very important considerations must be taken prior to beginning the fieldwork: \u2022 Most computer and GPS equipment is not waterproof, particularly against saltwater, unless explicitly stated. The equipment will be vulnerable to humidity, spray and rain. In addition, equipment will be sensitive to direct sunlight and heat. \u2022 What will be the source of power? Batteries may be short-lived and take up a lot of space on a boat. A generator produces a lot of noise, fumes, and heat and needs to be protected from salt water and direct sunlight. Connecting to the boat's battery may be a feasible option; however, the equipment may possibly drain the boat's battery. The best option for a field team is to use a variety of power sources rather than rely upon one option. Weather Check the local weather forecast and conditions the day of fieldwork. A boat with \"housing\" can extend the amount of fieldwork and the life of the equipment, as the housing provides protection from harsh weather, direct sunlight, or humidity. In addition, it is important to determine the desired navigational track for the fieldwork. The conditions offshore may be extremely different from conditions within a protected bay. Unfortunately, the team can only plan to do fieldwork during times when the weather is traditionally calm. It is extremely important to recognize that conditions may change daily and that fieldwork may be canceled or postponed."}, {"section_title": "Tides", "text": "Using bathymetry maps or local expertise, determine if areas to be visited are only accessible during high or low tide. This will reduce the number of boat strandings and decrease the amount of time at each study site. A nautical chart and/or sound local knowledge is highly recommended."}, {"section_title": "Field Equipment", "text": "The Center has developed field methods that rely on various hardware and software components to conduct the signature development and accuracy assessment field verifications. The following section describes the equipment commonly used by project technicians. This information is included to provide guidance to other researchers in conducting their own field work. Below are lists of minimal and additional equipment for field verification. The primary concern in obtaining field information, regardless of the equipment used, is to record observations with locational certainty. Viewing tube/bucket for surface observations \u2022 Source of electrical power to support the equipment. The Center has had much success with small portable generators (1000 Watts). In certain cases, much equipment can be run from the boat battery. Care must be taken, however, to avoid drawing down batteries required for starting the boat engines."}, {"section_title": "Signature Development", "text": "Signature development is the process of visiting areas visible on the imagery and determining the actual habitat present in the field. This process serves primarily as a training tool to support the subsequent mapping effort. Signature development begins as the aerial imagery is being screened for quality and acceptability. During this review, certain portions of the study area will be easily interpreted, while others will be more difficult. This could be due to isolated problems with turbidity or depth, or unique habitats either not previously encountered by the analyst or unique to a particular study area. A signature development visit should be designed around these initial areas of confusion. It is recommended that signature development field surveys address the following themes. Areas of Confusion/Uncertainty These sites will be the higher priority sites during any field visit. Typical confusion sites are caused by factors such as turbidity, depth, glint, sediment color that is confused with living habitat, unfamiliar habitats, and subtle gradients between identifiable habitats. There are often a number of these confusion sites within the study area. When prioritizing confusion sites for field examination, the emphasis should be placed on clearly discernable signatures that cannot easily be assigned to a category. Areas of confusion caused by deep or turbid water should be the next priority. As many of these sites should be visited as possible."}, {"section_title": "Areas of Initial Confidence", "text": "In addition to visiting the priority confusion sites, it is also important to visit and verify areas that initially seemed easily categorized or mapped. This is important to test the assumptions about the habitat as observed in the preliminary photointerpretation. Occasionally, this visit reveals that the initial assumptions were not correct and that what had been a simple site may now be a confusion site. These types of adjustments often happen during signature development trips, and some amount of flexibility is necessary."}, {"section_title": "DATA VALIDATION METHODS", "text": "For every completed data set it is necessary to test the quality and correctness of the data prior to use and distribution. This final stage is the accuracy assessment. Accuracy assessment falls into two categories, spatial and thematic. Spatial accuracy is the evaluation of the positional correctness of the data, while thematic accuracy is a measure of whether a habitat or resource is correctly labeled in the final data. Both are of critical importance since errors in baseline data can be propagated through the change detection process resulting in false estimates of habitat gain or loss. Portions of the following discussions on spatial and thematic accuracy are drawn from Chapter 5 of the 1995 report. Recommendations on how to assess the spatial and thematic accuracy of benthic data are also included. In addition, a short discussion of the timeliness of the assessment and possible approaches to address this issue are presented in a section on temporal accuracy."}, {"section_title": "Spatial Accuracy", "text": "Spatial or positional accuracy is a measure of the accuracy of the geometric placement of points, lines, and polygon boundaries. Positional accuracy of photographic delineations of submersed habitat is increasingly of concern to resource managers; however, it has not often been a subject of independent verification. This is compounded by the fact that positional errors may be difficult to detect even when verifying a specific polygon in the field. For a single time period, positional errors may not greatly affect the aggregate area of each cover type. For change detection, however, positional accuracy is a crucial concern (Ferguson et al. 1992(Ferguson et al. , 1993. Change data, especially data produced by post-classification comparison, will conspicuously record positional errors in excess of about 10 meters between data sets. This compounds the problem of recognizing real changes in the extent of habitat, which also tend to occur at polygon edges and class boundaries. Registration of benthic habitat polygon edges is a function of a combination of factors, including the metric quality of photographs, the methods used to transfer the information to a planimetric map base, the spatial accuracy of that base map, the photorectification process (including the quality of the source control points), and the quality of the digitization performed. In every case the spatial accuracy of the final digital data set is only as high as the least accurate source of control. The spatial accuracy of delineations derived using table methods will only be as accurate as the base map. In many cases, the most commonly available base maps are USGS 7.5minute quads. These have spatial accuracies that meet National Map Accuracy Standards, which at a scale of 1:24,000 amounts to 13.3 meters on the ground. This is the minimum spatial accuracy for Center data. With the increased availability of higher sources of control, such as differential GPS units and digital orthoquads, and as image rectification software becomes more common and easy to apply, the need to depend on base maps at National Map Accuracy Standards is diminishing. Soft-copy and analytical photogrammetric methods should produce data with spatial accuracies on the order of 1 or 2 meters. While the recent deactivation of Selective Availability by the U.S. Department of Defense has greatly increased the spatial accuracy of GPS units without a real-time differential correction beacon, the accuracies of uncorrected GPS readings remain on the order of tens of meters and is still not recommended for determining spatial accuracy of Center digital data."}, {"section_title": "Recommended Approach", "text": "The Center's benthic habitat mapping project has implemented two tests of spatial accuracy based on project work in Indian River Lagoon, Florida, and coastal Massachusetts. In the first test, the benthic habitat polygons are spatially buffered to produce a zone following the delineated lines. The width of the zone should be determined by the expected accuracy of the weakest source of control. In the Center's experience, using differential GPS as the primary source of control, a buffer of 5 meters on either side of the line is recommended. This buffer allows for real-time differential GPS readings that produce values less than 1 meter, but that occasionally produce readings greater than 1 meter. It also allows for boat motion and addresses the difficulty of precisely positioning a vessel on the water. This buffer should be determined by the scale of the imagery and by the sources of control that were used to bring the data into a reference plane. The maximum buffer width should be 13 meters on either side of the polygon boundary to be consistent with National Map Accuracy Standards. This buffered vector data set then becomes a source of randomly selected spatial accuracy points that are verified in the field by video observation or diving. In multidensity habitat classes only the boundaries of high-density (continuous cover) polygons should be examined to reduce potential subjectivity associated with patchy polygon boundaries (Bruce et al.1998). At each sample point, a video or diver transect can be run across the buffer zone, and if the edge of a habitat is detected within the zone, then the polygon boundary can be considered to have met the spatial accuracy requirements. This test is most needed in large open-water areas and is also recommended in more enclosed environmental settings to check the accuracy of the benthic polygon data. When performing this test, consideration should be given to any seasonal differences between the date of the imagery and the date of the field verification. Changes in phenology may affect the apparent edge of certain habitats such as seagrass meadows and algal beds. A second test of spatial accuracy is also recommended to further address the challenges of precisely positioning a small vessel on the water for more than a few moments, and the occasional difficulty of determining a habitat edge during low visibility diving or in a patchy environment. In this test, a fixed terrestrial linear feature, such as a road or reinforced shoreline is delineated periodically during the mapping process. The closer this feature is to the center of the image and the water the better. A differential GPS field measurement is then made on this linear feature (on land) where it is possible to be more certain of the measurement. In cases where Aids to Navigation (ATONs) are present and visible in the imagery, they can also be used to assess spatial accuracy. The NOAA Office of Coast Survey has coordinate information for many of these ATONs that can be compared to imagederived coordinates. If no existing coordinate data are available for ATONs in a particular area, they can be measured using GPS. Day marks on fixed pilings are preferred for this type of measurement, while anchored buoys are less useful because their position may shift depending on tidal currents, wind, etc. Great caution should be exercised if ATONs are to be measured. They are typically located along channels, and the field crew must avoid posing a hazard to navigation during the measurement process. Both tests should be done at several locations throughout the data set to determine overall spatial accuracy. If possible, fixed spatial accuracy points should be measured on at least every flight line. In cases of long flight lines with many frames, a measurement from every tenth frame may be needed. Habitat polygon boundary points may be measured in conjunction with the thematic accuracy assessment. At the conclusion of the spatial accuracy assessment, the results should be reported as either correctly located within the buffer or located outside the buffer. For those samples falling outside the buffered line work, the distance and azimuth of the boundary should be reported. This information can then be used to improve the rectification or to edit the line work appropriately. Figure 13 shows a graphic representation of the buffer approach and how samples are drawn from within the buffered area. During field verification, the points are visited and any habitat boundaries within the buffer indicate that the data are within the tolerances for spatial accuracy.\nCurrently the approach for thematic accuracy assessment of submersed habitat is similar to that for emergent and upland habitat, but it should be noted that data for submersed habitat are intrinsically vector, not raster. Since Center benthic habitat data are typically in a vector data structure, the actual units of the data are the polygons themselves, which constitute the thematic accuracy assessment sample units (Elliot and Bruce 1998). Habitat polygons (samples) should be selected by random sample stratified by class. Other stratification criteria that are useful are region (water body) and bathymetry. Vector water body and bathymetric contours can be merged with the habitat data through the Arc Union process to produce a stratified source for sample units. Additional sample locations from potential habitat sites (for example, areas of suitable depth but devoid of mapped habitat) should also be selected. By converting the vector habitat data to a raster format such as an Arc grid, automated sample generating routines within commercial image processing softwares such as ERDAS Imagine can be used. The resulting sample set can be stored in a database format that is compatible with pen-based field softwares or ESRI's ArcView. An ongoing area of research within the remote sensing community is the appropriate sample size for a classified remotely derived data set. This sample size should be based on the expected variance of the cover classification. To date, little research has been done in the area of benthic habitat. Congalton and Green (1999) recommend as a rule of thumb a sample size of 50 samples (polygons) per class. This was also the sample size for land cover classification accuracy assessment recommended by Hay (1979). In every thematic accuracy assessment effort a balance must be reached between the need to collect a statistically valid sample size and the challenges of visiting a large number of sites in small boats over project areas on the scale of major estuaries. While the 50 points per class number was derived based on experience with terrestrial land cover data, this approach was successfully implemented in the Willapa Bay, Washington benthic mapping project. Over a period of several days it was possible to sample this many points from a 4-class map with a reasonable effort. When visiting the sample points (polygons) it is important to examine the polygon for both internal attribute integrity as well as the accuracy of its delineated edge. In large polygons this can be an intensive process involving multiple video tows or dives. Observations can be made starting with circular tows or dives in the interior of polygons. The National Ocean Service uses a 7-meter radius for this type of observation in coral reef mapping. This should be the minimum area surveyed to assess the internal attributes of a polygon. Video or dive tracks should then be made across the polygon to determine how well it has been delineated. In many cases at least two tracks normal to each other are needed to fully assess the polygon. During field verification, the following minimum number of items should be recorded: When making field observations for comparison to aerial synoptic data, the issue of scale becomes an important factor. Diver or video observation typically takes place on a scale of meters, while remote observations are made at the kilometer scale. Individuals making the field verification should bear in mind that they will see small habitat changes within an area likely to have been given a single habitat attribute by the mapper. Small openings and bare areas are often seen within a continuous SAV bed and do not necessarily indicate an inaccuracy. Likewise, individual plants or coral heads do not necessarily constitute SAV or coral habitats. A limited study comparing video transects to aerial photography in Core Sound, North Carolina, highlighted significant challenges to integrating these two data streams due to scale issues (Fonseca et al., in publication). At the conclusion of the field visit, the accuracy assessment database containing the field observations should be used to construct an error matrix. This matrix should be used to calculate overall and category-specific accuracies as well as a kappa coefficient and conditional kappa for each class. The kappa coefficient is a statistical measure of the actual agreement minus chance agreement and measures how well the categorization sample reflects the actual data. A kappa value of 0.0 is obtained when agreement between the reference data and a categorization result is the same as the agreement that would occur from chance alone. The upper limit of kappa is 1.0, which occurs only when there is perfect agreement (Rosenfeld and Fitzpatrick-Lins 1986). Kappa values below 0.5 may suggest that the results of the accuracy assessment do not actually reflect the validity of the data. The results of the error analysis should meet an overall accuracy of 85 percent."}, {"section_title": "Thematic Accuracy", "text": "Thematic or attribute accuracy is a measure of the probability that the cover type for any given polygon is properly identified according to the classification scheme. The remote sensing literature contains a variety of procedures for measuring attribute accuracy (Congalton 1991). Generally, these procedures serve well for single time periods and for relatively small study areas. Historical data, however, cannot be field verified and conventional procedures also are difficult to apply to large (hundreds of square kilometers) open-water areas, especially when operating out of small boats. Thematic accuracy of remotely derived spatial data is influenced by a number of factors. The most critical is the quality of the source imagery. Others are the complexity of the benthic environment, the experience of the analyst, the amount of signature development that has been done to support the mapping, temporal differences between image date/season and verification data/season, the spatial precision of the field verification data, and the logical integrity of the classification system (Congalton et al. 1983;Congalton 1988;and Felix and Binney 1989). The reported thematic accuracy of a digital map may also be negatively influenced by errors in making an accurate field determination of habitat type. This can occur when poor visibility hinders direct observation of habitat in a site or when the boundary between habitat classes occurs across a transition zone of patchy or fragmented habitat."}, {"section_title": "Temporal Accuracy", "text": "An important issue to be considered when planning the accuracy assessment of a benthic habitat data set is timeliness. Many benthic habitats, especially seagrass meadows, are dynamic resources that colonize new areas and die back from existing areas from season to season. Many are also subject to significant impacts from catastrophic events such as hurricanes. Unfortunately, it is not unusual for the accuracy assessment stage of a mapping project to occur months or years after the original imagery and source data were collected. The changes that can occur in a habitat over that time can hinder the ability to assess both the spatial and thematic accuracy of a data set. To the extent feasible, the Center recommends that during the initial signature development stage, a selected set of field observations be collected that will later serve as accuracy assessment samples more concurrent with the imagery. Collecting this initial field information as close as possible to the date of the aerial mission increases the utility of these points for a true accuracy assessment. These samples should then be separated from the signature development data and not examined until the final accuracy assessment stage. If a previous data set is available, the polygons in this data may be a source of initial sample points that can be visited during the overflight, before the analyst has imagery in hand. These points can then serve as a set of concurrent accuracy assessment samples."}, {"section_title": "SUPPLEMENTAL MAPPING TECHNOLOGIES", "text": "Although aerial imagery such as photography can provide a wealth of information on submerged habitat, inevitably there will be areas that are difficult to characterize from an aerial platform. These areas might be localized turbidity plumes, glint or sea-state obscured areas, or deeper water locations. In these instances, technologies that are in more direct contact with the habitat can assist in the mapping effort and also provide a verification tool for the photographic mapping. Two technologies, underwater videography and single-beam acoustic sensing, are commonly employed for habitats found in shallow nearshore estuarine and marine areas. The Center recommends that these technologies be employed as supplements to the aerial photography rather than as the primary data source for benthic mapping unless conditions preclude aerial imaging."}, {"section_title": "Videography", "text": "Submersed videography is a powerful tool for characterizing the benthic environment and complements. Videography has been used successfully in a variety of environmental settings, including both clear and turbid water estuaries (Aronson et al. 1995;Norris et al.1997) This technology has the following advantages for benthic characterization: \u2022 Videography reduces the need for direct diver observation. This is especially important in colder water, in hazardous areas, and in deeper environments. The ability to reduce diving for field verification also lengthens the season in which these operations can be conducted in northern project areas. Deploying certain video systems is also faster than using direct dive observation, increasing efficiency and cost effectiveness."}, {"section_title": "System Types", "text": "The Center has employed videography in routine project work since 1995, and this work has been successful in a variety of geographic settings. Several types of videographic sensors are commonly used in shallow benthic habitat mapping. These are handheld, towed, and fixed. Handheld video systems are carried by a diver. These systems are best used to provide reference images of different habitat types, or to film sampling or analysis activities underwater. Because they are hand held, there is inherently a lack of spatial reference to the resulting data; therefore, it is best deployed in situations where spatial location is not as critical or spatial reference can be determined by documenting vessel position information and measuring the distance and direction between the diver and the vessel. Handheld units have the advantage of requiring minimal power, being portable, and easy to use. They do, however, require a person in the water to collect imagery, which can increase data collection time and cost. They are also not well suited to hazardous conditions. Towed video systems are deployed from both small boats and larger vessels. These units involve pulling a camera on either a controlled umbilical or freehand. The platform can either be a sled that rides the bottom or a more free-flying unit. While there is some positional offset due to the length of the cable, a GPS track recorded on the survey boat will represent the trail of any tows. In fixed tow arrangements, the offset and depth can later be calculated to determine the actual position of the unit at a given time.  Figure 15 shows the Fishers TOV-1 towed video camera unit. This unit features a low-lux pan camera in a rugged hydrodynamic housing. It is designed to skip over the bottom on weighted runners. Fixed units are usually deployed on a davit or pole attached to the survey boat. Because these units are mounted on the boat, a GPS record of the track is directly correlated to the location of the video view. The disadvantages of these systems, however, are that they require fixed structures on the survey boat and are not easily portable. They are less useful in deeper waters where the pole may not be long enough to ensure that the bottom is in view or in waters with varied bathymetry where sharp rises in the bottom may cause the mount to ground or the bottom to fade out of view. All of these units have their utility and can be included in the development of benthic habitat data sets. The choice is up to the cooperator and determined by the local study area character and logistical capabilities. Both towed and fixed video systems can be set up to allow real-time viewing by analysts in the boat. This allows a field party to stop the video survey should something unusual appear and then deploy divers for direct observation. By recording the video data onto magnetic tape the data can be reviewed later. This will assist with the photointerpretation process and will form a permanent record of conditions at the site. VHS video is also a widely available medium for sharing data.  Figure 16 shows a typical analog video frame captured through digital frame-grabbing software. Blades of manatee grass are clearly visible in this image. In addition, information on substrate type, leaf health, epiphytization, and macrofauna can be obtained from this imagery. The field of view in this image is less than one square meter. When comparing this type of imagery to aerial photography, the vast scale differences between the two must be considered."}, {"section_title": "Recommendations on Deployment", "text": "The Center employs videography as a supplement and verification tool to aerial photography. Videography is deployed in selected areas to confirm assumptions and resolve confusion in the imagery. It also serves as a record for long-term transect monitoring and more detailed characterization of the benthic environment. The Center recommends a towed unit for collecting video imagery over more extensive areas. These units allow sampling in deeper waters as well as very shallow environments. They also are more responsive to changes in underwater topography and are simpler to deploy on small boats. The Center recommends that the video unit be towed across habitat gradients to identify breakpoints. Two cases where this is most important follow: \u2022 Where gradual changes are occurring between habitats, such as between continuous and patchy SAV or between seagrass and algae, etc.\nAs with videography, the Center generally recommends single-beam acoustics as a supplement to synoptic data derived from raster remote sensing. Experience in Rehoboth Bay, Delaware; Apalachicola Bay, Florida; and the West Florida Continental Shelf suggests that this technology is best applied in medium scale (tens of square kilometers) projects. Similar work by the U.S. Army Corps of Engineers Waterways Experiment Station supports this (Sabol et al. 1996). Certain single-beam sensors are vulnerable to noise produced by rough seas, so missions should be scheduled to avoid this problem. Transect spacing is a variable that should ideally address the variability of the bottom and the logistics of the field effort. In Sabol's work in St. Andrew's Bay, Florida, a spacing of 50 meters was adequate. A possible test of the adequacy of a proposed transect spacing is to sub-survey a small area at a higher transect density and compare raw and interpolated results."}, {"section_title": "Single-Beam Acoustics", "text": "Single-beam acoustic sensors are increasingly being applied to seabed characterizations. These sensors are excellent tools for deriving detailed information about habitat, providing a textural component to imagery, and acquiring bathymetry in shallow estuaries. Single-beam sensors range in complexity from simple echosounders with strip paper output or LCD display to more sophisticated signal processors that extract additional information from specific components of the return waveform or from secondary echoes."}, {"section_title": "Echo-Sounders", "text": "Echo-sounders produce a visual display of bottom texture by showing rough areas of vegetation or rocks. They also have demonstrated utility for detecting SAV (Maceina and Shireman 1990;Miner 1993). A qualitative measure of bottom hardness can sometimes be obtained by examining subsurface echoes. The wavelengths employed by these echo-sounders vary depending on the model. In general, shorter wavelengths (higher frequencies) produce more detailed bottom information, but these signals are more vulnerable to attenuation by the water column. Longer wavelength (lower frequencies) produce less detail but penetrate to greater depths and thus have a larger footprint. Echo-sounders have the following advantages and disadvantages as tools for benthic habitat mapping: Acoustic Signal Processors Signal processing instruments build on the basic technology of the echo-sounder. In these more sophisticated instruments, more technology is devoted to producing a precisely formed wave front on the initial pulse. This is because the altered shape of the returning first pulse is analyzed as part of the processing. The altered shape of the return wave produces a record of the bottom roughness component. Signal processing devices use additional analyses to extract further bottom information from the initial acoustic pulse. Two units that are commercially available are the QTC View sensor manufactured by Questar-Tangent and the RoxAnn sensor manufactured by Marine Microsystems Ltd. The QTC unit examines shape of specific regions of the returning wave front and builds an acoustic signature based on the signal response at these regions (Prager et al. 1995). This approach supports a three-dimensional analysis of acoustic signals that corresponds to different bottom types. Software for the system allows statistical analysis of the acoustic returns similar to spectral clustering algorithms used in optical remote sensing. The RoxAnn system expands on the traditional use of the first backscatter return for measuring water depth and limited seafloor sediment information by collecting the second backscatter return (Burns et al. 1989). The first backscatter returns, E1 values in the Roxann vernacular, provide a measure of the roughness of the bottom by isolating the oblique back reflection. The isolation is necessary in order to eliminate the element of hardness, sub-bottom reverberation, that is already calculated within the second echo calculation. The second backscatter return, E2, indicates hardness. This is inferred from signals that have reflected from the seabed and back to the sea surface before reaching the transducer head. The reflections of these signals provide a measure of the acoustic impedance of the seabed relative to the seawater above. This is derived from a combination of both amplitude and length of the second echo (Rukavina et al. 1997). Combining the information from both returns allows the operator to discriminate between bottom types. Because these signal processing units focus on the subtle characteristics of returning waves, there is little subsurface information captured by the software. Sediment penetration for the RoxAnn at 220 kilohertz can be 5 to10 centimeters while penetration at 30 kilohertz may reach 1 to 2 meters depending on the sediment type. All signal processing acoustic sensor units incorporate a live GPS signal to provide spatial locational data. Usually the total system consists of the signal processing device, an echo-sounder, a GPS unit, and a field PC running a data logging/analysis software. These systems tend to be portable and can be deployed with minimal power requirements. "}, {"section_title": "Data Acquisition", "text": "When planning an acoustic survey, areas of known habitat type should be visited to collect signature data. There are two approaches to conducting an acoustic survey of an area. If possible, a predetermined survey grid pattern can be set up and loaded into a navigation software package. Then, using live GPS data, the tracks can be followed by the survey boat. In the experience of Center staff on small open boats, it is difficult to maintain this type of predetermined grid. Changes in tidal depth can render certain parts of the study area inaccessible during parts of the day. In addition, wind and currents can make following the grid tracks almost impossible. In these situations the Center recommends a strategy that collects data on tracks that increase in proximity as the survey progresses. The initial set of tracks follows the perimeter of the study area (usually the shallowest portions of an enclosed water body). Then a set of tracks crossing the area from west to east are alternated with tracks running north to south (or along length/width axes). In each subsequent pass, the track bisects the remaining unsurveyed area. This approach thus starts with an initial grid that increases in spatial resolution as the survey continues. Should weather curtail operations, at least a gross survey of the entire area can be accomplished. The survey pattern shown in figure 17 is an example of this latter strategy in an enclosed water body. Assuming the end product will be a grid interpolated from the point data, the desired resolution of the final data and the spatial variability of the bottom habitats should determine the spacing of the lines. Data acquisition in both QTC View and RoxAnn sensors is limited by two factors. The first factor is depth. There must be at least 1.4 meters of water between the transducer head and bottom in the RoxAnn unit and 7.0 meters for the QTC View. Depths shallower than this cause the first echo to come back too soon to be captured by the transducer. Both units typically contain the LCD display of the main echo-sounder. As depths approach the minimum, the analyst should cross-reference the depth displays recorded in the data logging software with the echosounder display. Water that is too shallow can cause the softwares to lock up and repeat the last good depth, bottom class, and echo values, or it can cause no response at all. The second limiting factor for data acquisition is speed. In practice, using a portable over-the-side transducer mount for the RoxAnn unit, speeds of approximately 5 knots were optimal for data collection and efficient boat operation. Regardless of the speed, a consistent speed is recommended to avoid changes in the density of the sample points and to optimize continual capture of the second echo (Schlagenweit 1993). In addition, attention must be paid to the location of the transducer. The transducer should be below the hull, as vertical as possible, and situated away from the propellor wash to minimize signal distortion. The footprint of the high-frequency (220 kilohertz) RoxAnn on the bottom is one-tenth of the water depth. Calibration/Classification An initial calibration of the acoustic sensor may be required for each unique study location. This may be an automated process or may require gathering returns from the hardest/softest and roughest/smoothest bottom to set the parameters of expected return values. In normal practice, data are classified while the surveying is under way. During the survey, the analyst assigns classification values to the acoustic data and thus subsequent data are assigned to these classes. This process of \"training\" the system with field observations is called calibration. As new signals are received, a camera or diver can be deployed to determine the bottom type. There are two approaches to organizing the initial acoustic data. The first is to rely on previous classifications or the factory calibration. This approach approximates a supervised training method where existing values help determine the assignment of new data. A second approach is to begin with no previous calibration (a blank \"box\" in the case of RoxAnn) and observe the signal. When a signal repeatedly clusters around certain echo values, then a camera, grab sample, or diver is deployed to identify that particular bottom type. The Center recommends this approach or a combination of the two for initial surveying. Starting without a calibration reduces the initial bias in a classification. In starting with the blank box, it is necessary to collect signals from the softest/smoothest and hardest/roughest bottoms in the study. This will allow an intuitive arrangement of subsequent bottom types by the analyst. Local experts should be consulted to locate these initial target bottom types. If necessary, notations on nautical charts may be used. In figure 17, clearly defined signatures were collected for continuous seagrass and sand. The other categories, sparse grass, fine sand and mud were developed partly by their relationship to the initial classes.  "}, {"section_title": "Data Post Processing", "text": "The acoustic data can be exported from most single-beam sensors as a delimited ASCII text file with a numeric attribute equivalent to the classification, x and y coordinates, depth, first and second echo values, time, and date fields. An initial filtering of the data to clean out repetitive records (the RoxMap acquisition software defaults to the last known good value if speed is too great or depth is too shallow) results in a point data set ready for importation into a GIS such as ArcView or Imagine. The data can then be interpolated or otherwise analyzed depending on the desired outcome. The Center has been using an Inverse Distance Weighting interpolation method with a nearest neighbor classifier, which can be accomplished in an ArcView Spatial Analyst environment. This has produced satisfactory results although the output grid cell size is a variable that is often specific to each project. Several factors influence the success of the interpolation: \u2022 Diversity of bottom types in an area. Areas of very diverse bottom classes with high spatial variability should be interpolated with caution to avoid overgeneralization."}, {"section_title": "DATA QUALITY REPORTING AND DOCUMENTATION", "text": "As described in the 1995 document, Quality Assurance and Quality Control from data acquisition through final database compilation are the responsibility of each project team. Acceptance of the final data is contingent upon demonstration that the project meets the following standards. The Center's standards of data quality are based on authoritative references (Goodchild et al. 1990;Chrisman 1991;Lunetta et al. 1991;."}, {"section_title": "Spatial Accuracy Requirements", "text": "Change data between two vector data sets require a high level of spatial accuracy in order to reduce the amount of change produced by simple offsets. Although early data sets were restricted in their accuracy by the quality of their base maps, the recent increases in spatial accuracy provided by differential GPS and photogrammetric methods for data compilation have made it possible to achieve spatial accuracy on the order of a few meters. In order to be most useful for subsequent change analysis, it is expected that spatial accuracy be correct to within 13 meters. This is based on analog data development methods that rely on USGS topographic maps as a source of control."}, {"section_title": "Thematic Accuracy Requirements", "text": "Reference data for accuracy assessment must have a resolution and reliability that meet or exceed those of the remotely sensed data. Occasionally in satellite remote sensing efforts aerial photographs are used as reference data. However, since aerial photography forms the primary source data for these benthic mapping methods, only field observations will suffice to test the thematic accuracy of benthic habitat data. These surface-level observations must be evaluated in accordance with the imagery's minimum detection unit and minimum mapping unit for the remote data and with the classification system used to categorize the habitat. The simple presence of an individual species or natural feature may not, in itself, establish an area as a particular type of habitat. A number of questions need to be answered to draw a conclusion about the appropriate cover category to assign based on the reference data: does a characteristic species or feature meet the minimum detection unit of the remote sensor? What other characteristic species or features also are present within the minimum mapping unit? Due to the subjectivity associated with any photointerpretation process, it is critical that the thematic accuracy of delineated habitat be as high as possible. The Center emphasizes the importance of field observation and verification in any remote sensing/mapping study. Using positionally registered field observation methods, thematic accuracies over 85 percent should be obtainable by regional scientists. In order to be accepted into the NOAA archive, benthic habitat data should meet or exceed this overall accuracy."}, {"section_title": "Other Data Parameters", "text": "In order to facilitate fieldwork, integration with ancillary data, and incorporation with other project data, the Center requires that benthic habitat data be projected into a Universal Transverse Mercator projection with a North American Datum of 1983 as a horizontal datum. Units of measure should be meters and the GRS 1980 spheroid should be used. This is a Cartesian system that is amenable to photogrammetry and that also integrates well with other Center coastal spatial data."}, {"section_title": "Metadata", "text": "As part of a federal agency dealing with spatial data, the Center is required to document its data in a format that is compliant with the Federal Geographic Data Committee (FGDC) guidelines. Many state agencies are required to produce metadata in formats unique to their states. These formats may be more or less compliant with the FGDC guidelines. The Center recommends that producers of data include the following information at a minimum: \u2022 Lineage: A record of the type of data sources and the operations involved in the creation of a database. This includes not only the primary data source (usually aerial photography) but also supplemental sources of information such as videography, existing maps, in-situ measurements, etc. Temporal Accuracy and Precision: The time over which source materials were acquired and observations made. \u2022 Fitness for Use: The degree to which the data quality characteristics of each database and its components collectively suit an intended application. Any prohibitions or warnings against inappropriate or unsanctioned use should be documented. \u2022 Minimum Mapping Unit: This is the smallest unit discretely identified in the data set. Objects smaller than this will have been aggregated into larger units. \u2022 Projection Information: Documentation of the spatial reference plane into which the data have been projected, including projection, datum, spheroid, zones, units, etc. Information on these characteristics of a data set are sufficient to allow a user to intelligently analyze it, incorporate it with other geospatial information, and apply it to environmental decision making in an appropriate manner. However, as part of the goal of producing a nationally standard data set for coastal benthic habitat, the Center will require the production of the FGDC metadata record. The Center encourages cooperators to produce this record themselves as they are best suited to document their own processes. The Center has produced several tools for generating FGDC metadata from ESRI vector coverages in an ArcView environment (www.csc.noaa.gov/metadata/text/download.html). These tools greatly reduce the effort needed to produce the FGDC record. NOAA Coastal Services Center staff are also available to assist with the production of this record."}, {"section_title": "CONCLUSION", "text": "This document is intended to provide technical guidance to benthic habitat data developers working with the NOAA Coastal Services Center and for state and national level mapping efforts. An effort has been made to provide general guidance that is not limited by specific hardware or software whenever possible. These methods reflect technologies that have been proven efficient and effective for generating benthic habitat data in nearshore marine and tidal estuary environment. However, as existing technologies improve and as new technologies emerge, new technical guidance will be required. The following principles will be used to update this guidance document: \u2022 Seek a balance between the latest technologies and proven methods applicable by state-level natural resource scientists on a broad scale."}, {"section_title": "Digital Aerial Photography", "text": "This type of imagery has the mission flexibility advantage of other airborne sensors. In addition, in certain systems aircraft positional information is collected concurrent with the imagery through Inertial Motion Units (IMUs) and Global Positioning System (GPS) receivers in the aircraft. This information can greatly facilitate rectification of the imagery over extensive open-water areas or in remote locations. The imagery can be collected to produce whatever pixel size is needed to meet project requirements. Most digital cameras have a limited number of spectral bands (3-5) that adequately represent most benthic habitats without collecting numerous additional bands that contribute only limited information to a mapping effort. Digital aerial photography is also smoothly integrated into most soft-copy photogrammetric analysis environments. A prime advantage of this type of imagery is the lack of chemical processing needed to produce a working image. This can save time and allow reflights on short notice if conditions warrant. A potential disadvantage of these systems is that some systems do not employ metric cameras. This can slightly hinder the rectification process. The NOAA Coastal Services Center is currently exploring the utility of this technology for operational benthic habitat mapping and expects to produce technical guidance on the collection and analysis of this data stream."}, {"section_title": "Side-Scan Sonar", "text": "This swath acoustic sensing technology has been operational for many years for mapping and surveying deepwater environments. They are active sensors that pulse the bottom at an oblique angle and record the returning acoustic echoes. Many units record this information directly onto analog paper, although digital systems allow the data to be analyzed at a later date. Different wavelengths are available depending on the unit, and the wavelength determines the size of the feature (spatial resolution) that will be imaged. Multi-Beam Sonar Multi-beam sonars usually integrated with side-scan sonar to produce an image of the bottom that also has bottom topography combined with the acoustic backscatter response. Multi-beam systems usually incorporate Inertial Motion Unit (IMU) data to assist with registering the bathymetric and backscatter data (rectification). These units allow vessel position (heave, roll, pitch, and yaw) to be compensated for to reduce distorting the final bathymetric bottom profile. Both side-scan and multi-beam sonar units have most often been deployed in deeper water environments (well beyond the photic zone). However, new shallow-water units are becoming available that can be deployed from small boats. These systems have great promise as an additional benthic mapping tool, not only for deep water areas, but also for shallow turbid estuaries. Both side-scan and multi-beam sonars have demonstrated utility for identifying certain sediments and consolidated bottoms. The Center is investigating this technology for mapping other habitat areas, such as seagrass meadows and algal flats. All of the above methods are forms of remote sensing applicable over extensive geographic areas. There are also point sampling methods for identifying and characterizing benthic habitats. These methods can be used to produce a map through statistical interpolation methods and have the primary advantage of producing highly detailed information on bottom type, condition, and often biological and chemical processes that are occurring there. Benthic Grab and Core Sampling These methods have traditionally been used to develop information on bottom types. They rely on direct collection of a portion of the bottom and are deployed from both small and large vessels. They have the advantage of allowing direct observation and quantitative measurement of such characteristics as sediment type, organic content, and in-faunal presence. They are limited by their inability to capture samples in hard substrate areas, and they require an understanding of the spatial diversity of the bottom for accurate interpolation over distances. This type of sampling is often made more efficient and useful when used in conjunction with other spatial data such as that derived from remote sensing."}, {"section_title": "Sediment Profiling Imagery", "text": "A powerful tool for characterizing the benthic environment is the sediment profiling camera. These instruments penetrate the substrate and photograph the sediment profile. They can be deployed from small boats but often require fixed davits for effective deployment. This type of data provides a context for grab and core sampling and also captures structural information (layering, worm tubes, etc.) that more invasive grabbing and coring techniques might not preserve. Some of the advantages of this technology are the permanent record provided by the photography, and the information about the water-sediment interface. Some potential disadvantages are the size and complexity of the units, and the narrow field of view. Both this technology and the grab/core techniques are more powerful when applied in the context of other spatial habitat data. FLIGHT PLANNING CHART FOR 153mm (6\") LENS AND 23cm (9\") FILM Common U.S. Government Scales: 1:20,000 USDA ACSC panchromatic 1:48,000 USDOC NOS conventional color 1:40,000 USGS NAPP panchromatic and CIR 1:58,000 USGS NHAP panchromatic and CIR"}]