[{"section_title": "Abstract", "text": "ABSTRACT. The Gifi system of analyzing categorical data through nonlinear varieties of classical multivariate analysis techniques is reviewed. The system is characterized by the optimal scaling of categorical variables which is implemented through alternating least squares algorithms. The main technique of homogeneity analysis is presented, along with its extensions and generalizations leading to nonmetric principal components analysis and canonical correlation analysis. A brief account of stability issues and areas of applications of the techniques is also given."}, {"section_title": "A Geometric Introduction to Homogeneity Analysis", "text": "Homogeneity analysis, also known as multiple correspondence analysis, can be introduced in many different ways, which is probably the reason why it was reinvented many times over the years (for details see Section 2) . In this paper we motivate homogeneity analysis in graphical language, since complicated multivariate data can be made more accessible by displaying their main regularities in pictures (e.g. scatterplots).\nConsider the following, fairly typical, situation that arises in practice in various fields in the physical, social and life sciences. Data on J categorical variables have been collected for N objects or individuals, where variable j 2 J = f1; 2; : : : ; J g has`j categories (possible values). The use of categorical variables is not particularly restrictive, since in every data application a continuous numerical variable can be thought of as a categorical variable with a very large number of categories. Given such a data matrix, one can represent all the available information by a bipartite graph, where the first set of N vertices corresponds to the objects and the second set of P j2J`j vertices to the categories of the J variables. Each object is connected to the categories of the variables it belongs to; thus, the set of N P j2J`j edges provides information about which categories an object belongs to, or alternatively which objects belong to a specific category. Thus, the N vertices corresponding to the objects all have degree J, while the P j2J`j vertices corresponding to the categories have varying degrees, equal to the number of objects in the categories. We can then draw this graph and attempt to find interesting and useful patterns in the data. In Figure 1 .1 the bipartite graph of a toy example corresponding to a 43 contingency table with 7 objects is given.\nHowever, except for very small data sets (both in terms of objects and variables) such a representation is not very helpful. A better approach would be to try to find a low dimensional space in"}, {"section_title": "Categories of first variable", "text": "Categories of second variable Objects FIGURE 1.1. A Toy Example which objects and categories are positioned in such a way that as much information as possible is retained from the original data. Hence, the goal becomes to construct a low-dimensional joint map of objects and categories in Euclidean space (R p ). The choice of low dimensionality is because the map can be plotted and the choice of Euclidean space stems from its nice properties (projections, triangle inequality) and our familiarity with Euclidean geometry. The problem of drawing graphs in a manner that they are easy to understand and present has attracted a lot of attention in the computer science literature [25] . There are different approaches of drawing such maps and different ways of finding them; a particular set of criteria defines the former [23, 24] and the specific algorithm employed determines the latter.\nLet X be the Np matrix containing the coordinates of the object vertices in R p , and Y j ; j 2 J the`j p matrix containing the coordinates of the`j category vertices of variable j. We call X the object scores matrix and Y j 's the category quantifications matrices. If we assign random values to X and the Y j 's and plot the vertices and the corresponding edges we will typically get a picture similar to the one shown in Figure 1 .2 for the mammals dentition data set that is analyzed later on in subsection 1.1. It can be seen that very little has been gained by this 2-dimensional representation. The picture has too much 'ink' and no interesting patterns have emerged. A more informative picture would emerge if the edges are short, or in other words if objects are close to the categories they fall in, and categories are close to the objects belonging in them [24] . Hence, our goal becomes of making a graph plot that minimizes the total squared length of the edges. This criterion is chosen because it leads to an eigenvalue problem, and thus is nicely rlated to many classical multivariate analytic techniques.\nThe data are coded by using indicator matrices G j , with entries G j i; t = 1 ; i = 1 ; : : : ; N ; t = 1; : : : ; j 1 if object i belongs to category t, and G j i; t = 0 if it belongs to some other category.\nX , G j Y j ; (1.1) where SSQ H denotes the sum of squares of the elements of the matrix H. We want to minimize (1.1) simultaneously over X and the Y j 's. The loss function (1.1) is at the heart of the Gifi system [32] , and the entire system is mainly about different versions of the above minimization problem.\nBy imposing various restrictions on the category quantifications Y j and in some cases on the coding of the data, different types of analysis can be derived.\nIn order to avoid the trivial solution corresponding to X = 0 , and Y j = 0 for every j 2 J, we require in addition X 0 X = N I p ; (1.2) u 0 X = 0 :\nThe second normalization restriction basically requires the graph plot to be centered around the origin. The first restriction standardizes the squared length of the object scores (to be equal to N), and in two or higher dimensions also requires the columns of X to be in addition orthogonal.\nAlthough this is computationally convenient, in many respects is not completely satisfactory, a fact already noted by Guttman [49] .\nLet us examine the solution to our minimization problem (1.1) subject to the normalization constraints (1.2) and (1.3). An Alternating Least Squares (ALS) algorithm is employed. In the first step, (1.1) is minimized with respect to Y j for fixed X. The set of normal equations is given by D j Y j = G 0 j X; j 2 J; (1.4) where D j = G 0 j G j is the`j `j diagonal matrix containing the univariate marginals of variable j. Geometrically, the discrimination measures give the average squared distance (weighted by the marginal frequencies) of the category quantifications to the origin of the p-dimensional space. It can be shown that (assuming there are no missing data) the discrimination measures are equal to the squared correlation between an optimally quantified variable G j\u0176j :; s in dimension s, and the corresponding column of object scoresX:; s (see chapter 3 in [32] ). Hence, the loss function can also be expressed as We summarize next some basic properties of the Homals solution."}, {"section_title": "GEORGE MICHAILIDIS AND JAN DE LEEUW", "text": "Category quantifications and object scores are represented in a joint space (see Figure 1 .4 for the mammals dentition data set).\nA category point is the centroid of objects belonging to that category, a direct consequence of (1.5) (see Figure 1 .3 for two variables from the mammals dentition data set).\nObjects with the same response pattern (identical profiles) receive identical object scores (follows from (1.7)) (see Figure 1 .7). In general, the distance between two object points is related to the 'similarity' between their profiles.\nA variable discriminates better to the extent that its category points are further apart (follows from (1.9)).\nIf a category applies uniquely to only a single object, then the object point and that category point will coincide.\nCategory points with low marginal frequencies will be located further away from the origin of the joint space, whereas categories with high marginal frequencies will be located closer to the origin (follows from (1.5)).\nObjects with a 'unique' profile will be located further away from the origin of the joint space, whereas objects with a profile similar to the 'average' one will be located closer to the origin (direct consequence of the previous property).\nThe category quantifications of each variable j 2 J have a weighted sum over categories equal to zero. This follows from the employed normalization of the object scores, since\nThe Homals solutions are nested. This means that if one requires a p 1 -dimensional Homals solution and then a second p 2 p 1 dimensional solution, then the first p 1 dimensions of the latter solution are identical to the p 1 -dimensional solution.\nThe solutions for subsequent dimensions are ordered. This means that the first dimension has the absolute maximum eigenvalue. The second dimension has the also the maximum eigenvalue subject to the constraint that X:; 2 is uncorrelated to X:; 1, and so forth.\nThe solutions for the object scores are uncorrelated (follows from (1.3)). However, the solutions for the quantifications need not necessarily be uncorrelated; in fact, their correlation patterns might be rather unpredictable.\nThe solution is invariant under rotations of the object scores and of the category quantifications. To see this, suppose we select a different basis for the column space of the object scores X; that is, let X = X R, where R is a rotation matrix satisfying R 0 R = RR 0 = I p . We then get from (1.5) that Y j = D ,1 j G 0 j X =\u0176 j R. Thus, the axes of the joint space can not be uniquely identified.\n1.1. An Illustration: Mammals Dentition Example. In this section we discuss the results we obtained by applying the Homals algorithm to the mammals dentition data set. The complete data together with the coding of the variables are given in Appendix A. The main question is whether the technique managed to produce a fairly clean picture and uncover some interesting features of this data set. A two dimensional analysis gives an adequate fit, with eigenvalues .73 and .38 for the two dimensions respectively. The graph plot of this solution is given in Figure 1 .4. We immediately notice that the objects and the categories have been arranged in such a way so that the amount of ink on the graph is minimized. Moreover, several patterns have emerged. In order to study those patterns more closely we turn our attention to the arrangement of only the category vertices on the map (see Figure 1 .5). It can be seen that they form 4 groups. In the upper right corner we can find the categories BM1, TM1, TC2, BC2, TI4, BI3, BI4, TP5 and BP5. Thus, objects located in this area of the map are associated with these categories. In the upper left corner we find  the categories BP1, BP2, TP1, TP2, TP3, BI2, and TI2, while in the center the categories TC1,  BC1, TM2, BM2, BI1, TI3, BP3, BP4 and TP4 . However, the latter group can be split further into subgroups. For example, we see that the categories TP4 and BP4 are close together, thus suggesting that objects with 3 top premolars, usually have 3 bottom premolars as well; similarly, for the subgroup TC1, BC1, TM1 and BM1, we have that animals with 0 top and bottom canines have more than 3 top and bottom molars. Finally, in the lower and slightly to the left area of the map we find a group of objects mainly characterized by the categories TI1 and BI5. At this point, it would be interesting to include in the picture the objects themselves (see Figure 1 .6) along with their respective frequencies (see Figure 1.7) . We see that the objects are located in the periphery of the map, which is a consequence of the first centroid principle. Moreover, the majority of the objects form three separate clusters located in the upper right, upper left and lower left areas of the picture. For example in the lower left area we find the following animals: elk, deer, moose, reindeer, antelope, bison, mountain goat, muskox, and mountain sheep. Similarly, in the upper right area we find among others various types of squirrels (ground, gray, fox), and rats (kangaroo, pack, field mouse, black, etc), and in the upper right animals such as jaguar, cougar, lynx, but also various types of seals (fur, sea lion, grey, elephant, etc). Finally, in the middle upper part of 8 GEORGE MICHAILIDIS AND JAN DE LEEUW the picture we find a group of various types of bytes. Hartigan [51] using a tree type clustering algorithm found many similar groupings (e.g. beaver and squirrels, weasel and otters, deer and elk, various types of bats). However, it is worth noting that the clustering algorithm classified the hairy tail mole together with the opposum, instead of the bats, and the pack rat with the armadillo instead of the squirrels. This is due to the nature of the particular algorithm, which is quite sensitive to the order of the presentation of the objects and to the selected variables used at different levels of the tree. On the other hand, the Homals algorithm positions the objects by taking into consideration the similarity of the entire tooth profile. As mentioned above, a variable discriminates better to the extent that its category points are further apart in the derived map. The discrimination measures, shown in Figure 1 .8, indicate that the variables TC, BC, TM and BM discriminate exclusively along the first dimension, while the remaining variables discriminate equally well on both dimensions. Reexamining Figure 1 .5, we see that categories TC1, BC1, TM2 and BM2 are to the left of the origin, while categories TC2, BC2, TM1 and BM1 are to the right of the origin. This implies that this set of variables does a primary splitting of the objects into two groups, while the other set of variables does further splittings especially along the second dimension. It is also interesting to examine the plots of the original versus the transformed scales given in Figure 1 .9. Obviously, such plots are totally uninteresting for binary variables, and therefore are ommitted. However, for the remaining variables they reveal nonlinear patterns in both dimensions. In some cases, the patterns are monotone (e.g. variables TI in both dimensions, and BP in the first dimension) suggesting an implicit ordering in the original scale, while in others the pattern is not very clear. "}, {"section_title": "Other Aspects of Homogeneity Analysis", "text": "In this section we study various aspects of homogeneity analysis. More specifically, we provide some alternative ways of introducing the technique and study its connection to an eigenvalue problem. We briefly review how to handle missing data in this framework and more elaborate coding schemes of the data. Finally, we discuss how homogeneity analysis is related to other techniques proposed in the literature that deal with categorical variables.\n2.1. Some Alternative Introductions of Homogeneity Analysis. Homogeneity analysis has been motivated and introduced in the previous section in pure graphical language. The basic premise was that complicated multivariate data can be made more accessible by displaying their main regularities and patterns in plots. What the technique accomplished was to scale the N objects (map them into a low dimensional Euclidean space) in such a way that objects with similar profiles were close together, while objects with different profiles were relatively apart. However, the technique can be introduced from a different starting point.\nAnother possibility to introduce homogeneity analysis is through linearization of the regressions. Consider a column of the object scores X:; s as N data values on the x-axis. Let the category quantifications in the same s th dimension of these N data points correspond to the y-axis values. The regression line of x on y has slope equal to 1. The reason is that the category quantifications y are averages of x-values within that category (follows from (1.5)). Remarkably enough, the regression of y on x is also perfectly linear with slope given by the eigenvalue s . This is because the object scores x are proportional to the averages of the category quantifications applying to an object (follows from (1.7)). Therefore, the Homals solution could be defined on the basis of BP5 TP5  TM1  BI4  BM1  BC2 TI4  BP3  BI3  TC2  TP2  TP3  TI2  TI3  BP2 BI2  TP1  BM2  BP1  TC1  TM2  BC1  BI1  TP4  BP4 BI5 TI1 A third possible interpretation of the Homals solution is in terms of a principal components analysis of the quantified data matrix. It can be shown [17] that the sum of squared correlations between the optimal quantified variables G j Y :; s and the vector X:; s is maximized."}, {"section_title": "Homogeneity Analysis as an Eigenvalue and a Singular Value Decomposition Problem.", "text": "One of the reasons why squared edge length is appealing is that it makes the minimization problem an eigenvalue problem. To see this substitute the optimal\u0176 j = D ,1 j G 0 j X for given X in the loss function (1.1), to get , X; ?\nwhere the asterisk has replaced the argument over which the loss function is minimized. Let [32] ) by the singular value decomposition of J , 1=2 LGD ,1=2 = UV; (2.4) where the left-hand side is the superindicator matrix in deviations from column means and corrected for marginal frequencies. The optimal X corresponds to the first p columns of the matrix U (the first p left-singular vectors). Notice that the complete eigenvalue and singular value solutions have q = P J j=1`j , J dimensions. The advantage of employing the ALS algorithm is that it only computes the first p q dimensions of the solution, thus increasing the computational efficiency and decreasing the computer memory requirements. single category, where the indicator matrix is completed with a single additional column for each variable with missing data, and (ii) missing data multiple categories, where each missing observation is treated as a new category. The missing data passive option essentially ignores the missing observations, while the other two options make specific strong assumptions regarding the pattern of the missing data."}, {"section_title": "Alternative Coding Schemes.", "text": "The coding scheme considered so far is the so called crisp coding of the indicator matrix. The main advantages it presents are: it is simple and computationally efficient (due to the sparseness of the indicator matrix), it allows for nonlinear transformation of the variables, it is very robust even when coding noisy data, and the number of parameters (categories) per variable is generally small. Its disadvantages are: in many data analytic situations the determination of the categories is arbitrary, when coding interval data there is uncertainty about the allocation of values near the category boundaries, and for some data subsequent intervals are functionally related. Many alternatives have been suggested in the literature (for a thorough account see [99] ), but the most commonly used alternative coding scheme is called fuzzy coding, a generalization of the strict logical coding of the indicator matrix. Instead of having a single 1 indicating a specific category, with zeros everywhere else, a whole set of nonnegative values adding up to 1 can be assigned to each object. In some cases these values can even be considered probabilities that the object lies in the respective categories. The main advantage of this scheme is that when a value lies near the boundary between categories, it may be allocated to both categories in appropriate amounts. The main disadvantage of any more general coding scheme is the computational burden it introduces to the ALS procedure. Finally, various ways of handling missing data have been proposed (see [47] )."}, {"section_title": "Comparison to Other Techniques.", "text": "In this section we attempt to relate homogeneity analysis to other widely used multivariate techniques, such as correspondence analysis, multidimensional scaling etc."}, {"section_title": "Relationship with Correspondence Analysis.", "text": "A special case of homogeneity analysis is the analysis of a crosstable that represents the association of two categorical variables. In this case the rows and columns of the table correspond to the categories of the two variables. Fisher's [30] eye color and hair color data set represents a prototypical example. Fisher described his objective as finding systems of row scores and column scores that maximize the correlation coefficient of the two variables, and also provided other interpretations of these scores in terms of analysis of variance, discriminant analysis and canonical correlation analysis. However, if one switches from a one-dimensional solution to a higher dimensional solution, it is then possible to regard the systems of row and column scores as coordinates in a certain space and an elegant geometric interpretation can be given. The French approach to correspondence analysis is mainly characterized by the emphasis on geometry [3, 10, 70, 47] . In the French literature the analysis of a crosstable is called correspondence analysis (\"analyse des correspondences\") and the analysis of a collection of indicator matrices, which is equivalent to homogeneity analysis, multiple correspondence analysis (\"analyse des correspondences multiple\").\nLet F be an I J contingency table, whose entries f ij give the frequencies with which row category i occurs together with column category j. Let r = F u denote the vector of row marginals, c = F 0 u the vector of column marginals and n = u 0 c = u 0 r the total number of observations. Finally, let D r = diag r be the diagonal matrix containing the elements of vector r and D c = diag c the diagonal matrix containing the elements of vector c. proportional rows obtain zero distances. In addition, remaining squared differences between entries are weighted heavily if the corresponding column marginals are small, while these differences do not contribute much to the 2 -distances if the column marginals are large. Finally, due to the role of the column marginals, the distances between the rows change when new observations are added to the crosstable. In a similar manner, 2 -distances can be defined between columns of the crosstable.\nThe objective of correspondence analysis is to approximate the 2 -distances by Euclidean distances in some low dimensional space. In order to derive the coordinates X of the row categories of table F in the new Euclidean space, we consider the singular value decomposition of the matrix of the observed frequencies minus the expected frequencies corrected for row and column marginals\nwhere E = rc 0 =n. The optimal scores X are then given (after normalization) by X = n 1=2 D ,1=2 r U; LGD ,1=2 = UV; (2.9) which is identical to (2.4). This result shows that homogeneity analysis could also be viewed as approximating the 2 -distances between the rows of the superindicator matrix. This special situation is due to the fact that the row marginals of the superindicator matrix are all equal to J. Subsequently the characteristic row weights in correspondence analysis are eliminated, and hence we deal with an unweighted origin (u 0 X = 0) and unweighted principal axes (X 0 X = N I p ). Obviously this does not hold in the presence of missing values that are coded as zeros, thus rendering unequal row marginals. Finally, it is worth noting that the special relationship between homogeneity and correspondence analysis holds only in case rows are analyzed, despite the fact that correspondence analysis is a symmetric technique regarding the treatment of rows and columns. The problem arises, when considering column differences of the superindicator matrix, from the fact that the unequal column marginals enter into the picture."}, {"section_title": "Relationship with Multidimensional Scaling.", "text": "In multidimensional scaling (MDS) the objective is to approximate given measures of association = f ij g, often called dissimilarities or proximities, between a set of objects by distances DX = f ij Xg between a set of points in some low-dimensional space. In multivariate analysis the object of the analysis is a multivariate data matrix Z and the distance approach chooses the association between the rows of the data matrix as the prime target of the analysis. This implies that each row of Z is regarded as a profile and the dissimilarities Z are derived among the profiles. It is easy then to see that any multivariate analytic technique can be regarded as a MDS method by correctly specifying the kind of dissimilarity measure involved in it. In MDS, since the dissimilarities are approximated in a low-dimensional space, a loss function is used to measure the difference between and the low-dimensional DX. In practice, squared dissimilarities 2 are used, because of the additivity implied by Pythagoras' theorem. A typical loss function in MDS is given by\nwhich shows that squared dissimilarities are approximated by squared distances. A moment of reflection shows that if we consider as squared dissimilarity measures the 2 -distances of the rows of the superindicator matrix G (see previous subsection), then homogeneity analysis can be regarded as a MDS technique. The primary difference between homogeneity analysis and a general MDS technique is that the homogeneity analysis solution is obtained at the expense of stronger normalization conditions and a metric interpretation of the data."}, {"section_title": "Relationship with Cluster Analysis.", "text": "As we have seen in the previous subsection homogeneity analysis provides with an approximation of the 2 -distances of the rows of the superindicator matrix G by low dimensional Euclidean distances. The 2 -distances are a measure of the dissimilarities between objects, based on the classification given in the data. The data indicate which categories are shared by many objects, and also how many objects belong to each category.\nThese two pieces of information are contained in G and in D (the matrix of univariate marginals for all variables). In homogeneity analysis classification follows from interpreting the configuration of object points in the p-dimensional space. Or to put differently, we are looking to identify clouds (clusters) of object scores and characterize them. In that sense, homogeneity analysis resembles a cluster technique.\nvan Buuren and Heiser [91] have developed a technique called Groupals, that simultaneously allocates the object points to only one of K groups and optimally scales the variables. Hence, the clustering and transformation problems are treated simultaneously. An alternating least squares algorithm is proposed to solve this problem. Groupals can be regarded as a forced classification method with optimal scaling features. A problem that often arises in practice is that the algorithm converges to local minima, a property inherited from the combinatorial nature of the problem."}, {"section_title": "Relationship with Discriminant Analysis and Analysis of Variance.", "text": "Homogeneity analysis can be stated in discriminant analysis and analysis of variance terms. Suppose for the time being that the matrix of object scores X is known. Each categorical variable j 2 J defines a partitioning of these object scores. This means that we can decompose the total variance T of X in a between B and a within (group) W component. We now wish to scale the objects (find the optimal X) in such a way that W will be as small as possible, while keeping T equal to a constant (the identity matrix for example).\nThis leads to a trivial solution: all objects in the first category of the variable get the same score, all objects in the second category get another score, and so on. The location of the points X is arbitrary but they satisfy W = 0 and B = T = I. However, in the presence of more than one variables, a trivial solution for one is not a trivial solution for another variable. Hence, we have to seek a compromise solution to the problem. For given X let us define T , B and W , which are averages over all J variables. Clearly for all variables the total variance of X is the same. The objective becomes to find a configuration X so that W becomes minimum, subject to the constraint T = T = I. This is another way of defining homogeneity analysis.\nIn homogeneity analysis terminology we have the total variance given by T X 0 X = N I p , the variance between categories of X given by X 0 P j X for variable j (with P j = G j D ,1 j G 0 j ), and the variance within categories of X given by X 0 I N , P j X for variable j. Thus, homogeneity analysis maximizes the average between categories variance, while keeping the total variance fixed. Consequently, the main difference between discriminant analysis and homogeneity analysis is that in the former we have a single categorical variable and X must be of the form U V , with U known 18 GEORGE MICHAILIDIS AND JAN DE LEEUW and weights V unknown. In homogeneity analysis the number of variables J is greater than one and X is completely unknown (or U = I).\n2.6. Other Approaches to Analyzing Categorical Data. As we have seen homogeneity analysis is primarily a data descriptive technique of (primarily) categorical data and its origins can be traced back to the work of Hirschfeld [56] , Fisher [30] and especially Guttman [49] , although some ideas go further back to Pearson (see the discussion in de Leeuw [18] ). The main objective is to scale (assign real values to) the categories so that a particular criterion is optimized (e.g. the edge length loss function (1.1)). The technique has been rediscovered many times and is also known as (multiple) correspondence analysis [3] , dual scaling [78, 79] , quantification theory [54] and also simultaneous linear regression, centroid scaling, optimal scoring, biplot, each name emphasizing some particular aspect of the technique. For example the French group around Benz ecri paid particular attention to contingency tables and emphasized the geometrical aspects of the technique, while Nishisato's derivation stems from analysis of variance considerations (see subsection 2.5.3), and Guttman was trying to apply principal component analysis to categorical data. However, in spite of the fact that the various approaches have a common starting point, most of them have passed the stage of basic formulation and moved towards their own unique advancement. Hence, we have Nishisato's efforts to apply dual scaling techniques to a wider variety of data such as multiway data matrices, paired comparison, rank order, successive categories, and sorting [80] . On the other hand, a lot of work has been done by the French group on extending and generalizing correspondence analysis beyond simply examining the interaction of row and column variables, by assuming stronger underlying mechanisms that generated the data [27, 28, 29, 98] . The Gifi group by considering generalizations of the loss function (1.1) and by placing restrictions on the category quantifications attempts to incorporate other popular multivariate techniques in the system, while retaining the focus on the graphical representations of the data and the exploratory nature of the techniques (for more details see sections 3-4). Thus, we see the various groups/approaches branching out, diverging from their starting point and exploring new directions. However, a common point that all of them retain is that the methods/techniques are usually not introduced by way of an estimation problem based on a model involving parameters and error terms. Rather, one directly poses an optimization problem for some type of loss function, while statistical inference takes a back seat [9] . Nevertheless, there have been many attempts to transform correspondence analysis of contingency tables into a model based approach appropriate for formal inference. In this line of research we have association models and correlation models [40, 41, 44, 33, 34, 42, 43, 15] and their extensions to handle ordinal data [85, 35, 82] . On another line we have the development of latent structure models for analyzing a single or a set of multidimensional contingency tables [68, 39, 50, 13, 14] . Finally, it is worth mentioning that the ideas of optimal scaling of the variables can be found in the ACE methodology [8] , in the ALSOS system [103] and in recent developments in discriminant analysis [52, 53] ."}, {"section_title": "Nonlinear Principal Component Analysis", "text": "In the Gifi system nonlinear PCA is derived as homogeneity analysis with restrictions [17] . The starting point for this derivation is the loss function given in (1.1). However, rank-one restrictions of the form Y j = q j 0 j ; j 2 J; (3.1) are imposed on the multiple category quantifications, with q j being a`j-column vector of single category quantifications for variable j, and j a p-column vector of weights (component loadings). Thus, each quantification matrix Y j is restricted to be of rank-one, which implies that the quantifications in p dimensional space become proportional to each other. The introduction of the rank-one restrictions allows the existence of multidimensional solutions for object scores with a single quantification (optimal scaling) for the categories of the variables, and also makes it possible to incorporate the measurement level of the variables (ordinal, numerical) into the analysis. This is impossible in the multiple quantification framework (homogeneity analysis) presented in section 1. First, consider a multiple treatment of numerical variables. In this case, the quantification of the categories must be the same as the standardized a priori quantification. This implies that multiple numerical quantifications contain incompatible requirements. Second, consider a multiple treatment of ordinal variables. This requirement is not contradictory in itself; however, the different quantifications must have the same order as the prior quantifications, thus resulting in being highly intercorrelated. It follows that such an option does not have much to offer.\nTo minimize (1.1) under the restriction (3.1), we start by computing the\u0176 j 's as in (1.5). We then partition the Gifi loss function as follows: At this point we need to take into consideration the restrictions imposed by the measurement level of the variables. This means that we have to project the estimated vectorq j on some cone C j . In case of ordinal data the relevant cone C j is the cone of monotone transformations given by C j = fq j jq j 1 q j 2 : : : q j l j g. The projection to this cone is solved by a weighted monotone regression in the metric D j (the weights) (see de Leeuw [16] and references therein). In case of numerical data the corresponding cone is a ray given by C j = fq j jq j = j + j s j g, where s j is a given vector; for example, the original variable quantifications. The projection to this cone amounts to a linear regression problem. However, it can be seen that there is no freedom for choosing q j different than s j , and soq j becomes irrelevant. Finally, in case of nominal data the cone is the R l j space and the projection is done by simply setting q j =q j . We then set\u0176 j =q j^ 0 j and proceed to compute the object scores. This solution that takes into consideration the measurement level of the variables is referred in the literature ( [32] , [17] [32] ). Hence, we have a technique that is invariant under all nonlinear transformations of the variables, and in the special case in which we allow for linear transformations only we get back to ordinary principal components analysis. The Princals model allows the data analyst to treat each variable differently; some may be treated as multiple nominal and some others as single nominal, ordinal or numerical. Moreover, with some additional effort (for details see [75] ) one can also incorporate in the analysis categorical variables of mixed measurement level, that is variables with some categories measured on an ordinal scale (e.g. Likert scale) and some on a nominal scale (e.g. categories in survey questionnaires corresponding to the answer \"not applicable/don't know\"). In that sense, Princals generalizes the Homals model. Therefore, the complete Princals algorithm is given by the following steps:\nStep 0: Initialize X, so that u 0 X = 0 and X 0 X = N I p .\nStep 1: Estimate the multiple category quantifications by\u0176 j = D ,1 j G 0 j X; j 2 J.\nStep 2: Estimate the component loadings by^ j = Y 0 j D j q j =q 0 j D j q 0 j ; j 2 J.\nStep 3: Estimate the single category quantifications byq j =\u0176 j j = 0 j j ; j 2 J.\nStep 4: Account for the measurement level of the j th variable by performing a monotone or linear regression.\nStep 5: Update the multiple category quantifications by setting\u0176 j =q j^ 0 j ; j 2 J.\nStep 6: Estimate the object scores byX = J ,1 P J j=1 G j Y j .\nStep 7: Column center and orthonormalize the matrix of the object scores.\nStep 8: Check the convergence criterion.\nIn principle, to obtain the minimum over all monotone or linear transformations of the q j 's, steps 2-5 should be repeated until convergence is reached for the criterion given by (3.3). However, since the value of the loss function will be smaller after a single iteration of the inner ALS loop, inner iteration upon convergence is not necessary in practice. The above algorithm is implemented in the Fortran program from the University of Leiden called Princals and also in the corresponding program in SPSS [88] . From equation (3.6) it can be seen that if a variable is treated as multiple nominal it does not contribute anything to the single loss component. Furthermore, two components are incorporated in the single loss part: first the rank-one restriction, that is the fact that single category quantifications must lie on a straight line in the joint space, and second the measurement level restriction, that is the fact that single quantifications may have to be rearranged to be either in the right order (ordinal variables) or equally spaced (numerical variables). For the mammals dentition data set the latter would imply that the plots containing the transformed scales (see Figure 1 .9) would only show straight lines with the categories arranged in an increasing or decreasing order for ordinal variables and additionally equally spaced for numerical variables. Of course, one can immediately see that for binary variables these distinctions are of no essence."}, {"section_title": "Remark 3.2. Nonlinear Principal Components Analysis and Eigenvalue Problems.", "text": "In section 2.2 we showed that homogeneity analysis under the loss function (1.1) corresponds to an eigenvalue problem, and an ALS algorithm was primarily used for computational efficiency purposes. For the problem at hand an ALS procedure becomes a necessity, because except from the special case where all the variables are treated as single numerical, the problem does not admit an eigenvalue (or a singular value) decomposition. The latter fact also implies that in some cases the ALS algorithm might converge to a local minimum (see chapter 4 in [32] ). This shows that missing data do not affect the inner ALS iteration loop where the single category quantifications and the component loadings are estimated."}, {"section_title": "An Example: Crime Rates of U.S. Cities.", "text": "The data in this example give crime rates per 100,000 people in seven areas -murder, rape, robbery, assault, burglary, larceny, motor vehicle theft-for 1994 for each of the largest 72 cities in the United States. The data and their categorical coding is given in Appendix B. In principal, we could have used homogeneity analysis to analyze and summarize the patterns in this data. However, we would like to incorporate into the analysis the underlying monotone structure in the data (higher crime rates are worse for a city) and thus treated all the variables as ordinal in a nonlinear principal components analysis. In Figure 3 .1 the component loadings of the seven variables of a two dimensional solution are shown. In case the loadings are of (almost) unit length, then the angle between any two of them reflects the value of the correlation coefficient between the two corresponding quantified variables. It can be seen that the first dimension (component) is a measure of overall crime rate, since all variables exhibit high loadings on it. On the other hand, the second component has high positive loadings on rape and larceny and negative ones on murder, robbery and auto theft. Moreover, it can be seen that murder, robbery and auto theft are highly correlated, as are larceny and rape. The assault variable is also correlated, although to a lesser degree, with the first set of three variables and also with burglary.\nIn Figure 3 .2 some of the variable transformations are shown. It can be seen that some variables such as murder and robbery (not shown here) received linear transformations, while some others (e.g. assault, larceny) distinctly nonlinear ones. Finally, in Figure 3 .3 the variable quantifications along with the object scores are depicted. Notice that all the quantifications lie on straight lines passing through the origin, a result of the imposed rank-one restriction (3.1). On the right of the graph we find the cities with high crime rates on all seven areas (Tampa, Atlanta, Saint Louis, Miami), and on the left cities with few crime incidents (Virginia Beach, Honolulu, San Jose, El Paso, Raleigh, Mesa, Anaheim). In the lower part of the graph and somewhat to the left there is a cluster of cities that have few rapes and larcenies, but are somewhere in the middle with respect to the other crime areas (New York, Philadelphia, Los Angeles, Long Beach, Houston, San Francisco, Jersey City) and in the lower right cities cities with many murder, robbery and auto theft incidents (Detroit, Newark, Washington DC, Oakland, New Orleans, Chicago, Fresno). On the other hand, cities in the upper right part of the graph are characterized by large numbers of rapes, larcenies and burglaries (Oklahoma, Minneapolis, Baton Rouge, Kansas City, Burmingham), while cities in the center are somwhere in the middle with respect to crime (e.g. Denver, Dallas, Las Vegas, Phoenix, Boston, Cleveland, Seattle to name a few). Finally, in the upper left we find a cluster of cities that have higher numbers of larceny and rape incidents (Colorado Springs, Lexington, Anchorage, San Antonio, Akron, Aurora). It should be mentioned that the nature of the original data (numerical variables) makes it possible to run an ordinary principal components analysis (equivalent to treating all the variables as single numerical in the present framework), and many of the patterns discussed above would be present. The use of nonlinear transformations sharpened some of the findings, and reduced the effect of some outlier observations."}, {"section_title": "Extension to Multiple Sets of Variables", "text": "Hotelling's [60] prime goal was to generalize multiple regression to a procedure in which the criterion set contained more than one variable. He proposed a replacement of a set of criterion variables by a new composite criterion that could be predicted optimally by the set of predictor variables. His objective, formulated asymmetrically, was to maximize the proportion of variance in the composite criterion that was attributable to the predictor set. In a subsequent paper Hotelling [61] retained the idea of partitioning the variables into sets, but formulated a more symmetric account of the technique. More specifically, he wanted to study the relationship between two sets of variables after having removed linear dependencies of the variables within each of these two sets. Hence, any variable may contribute to the analysis in as much as it provides independent information with respect to the other variables within its own set and to the extent that it is linearly dependent with the variables in the other set. The relationship between the two sets was channeled through a maximum correlation, labeled the canonical correlation, between a linear combination (called canonical variables) of the variables in the first set and a linear combination of the variables in the second one. If the data analyst is interested in more than a single solution, a second pair of canonical variables orthogonal to the first one is to be found, exhibiting the second largest correlation, and the procedure is repeated until a p dimensional solution is determined. Hotelling's procedure is known in the literature as canonical correlation analysis [36] .\nStarting with Steel [89] various attempts have been made and approaches suggested to generalize Hotelling's procedure to K sets of variables. In the two set problem, the canonical correlation serves as the single criterion. In a K set problem there exist 1=2KK , 1 canonical correlations between the optimal set of canonical variables that can be collected in a K K correlation matrix R. The generalizations deal with different criteria that can be formulated as functions of the matrix R. The most common ones (discussed in the book by Gittins [36] that deals exclusively with this subject) are: 1. Minimize the determinant of R, or equivalently minimize the product of the eigenvalues of R, proposed by Steel [89] . 2. Maximize the sum of the correlations in R, proposed by Horst [58, 59] . 3. Maximize the largest eigenvalue of R, also proposed by Horst [58, 59] . 4 . Maximize the sum of squares of the eigenvalues of R, proposed by Kettenring [65] . 5. Minimize the smallest eigenvalue of R, also proposed by Kettenring [65] . 6. Maximize the largest eigenvalue of R, proposed by Carroll [12] .\nThe last criterion is equivalent to maximizing the sum of correlations between each canonical variable and an unknown coordinate vector x [32] . The introduction of the comparison vector x, brings this criterion close to formulations of homogeneity analysis presented above. This criterion is also discussed in the works of Saporta [84] , de Leeuw [19] , and van der Burg et al. [96] .\nIn the Gifi system the last criterion is considered; therefore, a generalization of the familiar loss function (1.1) Step 1: For given X the optimal Y j is given b\u0177\nwhere V kj = P j2Jk G j Y j , G j Y j ; k = 1 ; : : : ; K ; j 2 J.\nStep 2: For given Y j 's, the optimal X is given b\u0177\nStep 3: The object scores are column centered and orthonormalized in order to satisfy the normalization constraints.\nEquations (4.2) and (4.3) illustrate the centroid principle, which is at the heart of the Gifi system. Category quantifications are centroids of the object scores corrected for the influence of the other variables in the set, and object scores are averages of quantified variables. In the presence of rank- where s ; s = 1; : : : ; p are the eigenvalues of the matrix LP ? L, with P ? = K ,1 P K k=1 P k and P k = P j2Jk G j D ,1 j G 0 j . Therefore, the minimum loss is a function of the p largest eigenvalues of the average projector of the K subspaces spanned by the columns of the matrices P j2Jk G j ."}, {"section_title": "4.", "text": "1. An Example: School Climate. The data for this example come from the National Education Longitudinal Study of 1988 (NELS:88). Recently, there has been a lot of interest among researchers and policy makers on the importance of the school learning environment and the influence of individual and peer behaviors on student performance. For example goal six of the National Education Goals Panel [77] states that by the year 2000 \"every school in America will be free of drugs and violence and will offer a disciplined environment conducive to learning.\" Because in many situations learning is constrained in anatmosphere of fear and disorderliness, student behavior influences school atmosphere and the climate for learning (whether it takes the form of violence and risk taking activities such as bringing weapons to school or using alcohol and drugs) or a low commitment to academic effort (such as poor attendance, lack of discipline or study habits) [11] . These student behaviors also play a key role in determining student success in school and beyond (see [63] and references therein), as well as the way students, teachers and administrators act, relate to one another and form their expectations and to a certain extent beliefs and values [1, 81] . Thus, this particular set of variables from NELS:88 addresses issues directly related to the school culture and climate, as seen from the students' point of view.\nThe variables were divided into three sets J1 = A , C; J 2 = D , I; J 3 = J; K .\nThe first set characterizeds attendance patterns of students, the second set deals with issues that affect the overall school environment, and the third with attitudes of students towards their teachers. The generalized canonical correlations for the two dimensional solution are .65 and .36 respectively, and the mulitple correlations of the three sets with each canonical variable (dimension) are 28 GEORGE MICHAILIDIS AND JAN DE LEEUW .67, .86, .75 for the first dimension and .45, .69 and .58 for the second one. The multiple correlations are a measure of the amount of variance of the canonical variable explained by the particular set. It can be seen that the second set does the best job, followed by the third and the first set in both dimensions.\nThe component loadings of the variables are shown in Figure 4 .1. The loadings of all variables are pretty high on the first dimension (canonical variable), with the exception of variable B (student absenteeism). On the other hand, variables E (robbery), H (drugs), I (weapons), and J (physical abuse of teachers) load positively on the second component, while variables A (tardiness), D (physical conflicts), F (vandalism), G (alcohol), and K (verbal abuse of teachers) load negatively on the second canonical variable. Hence, the first canonical variable can be interpreted as an overall measure of school climate, while the second one distinguishes between students that experience a rough and potentially dangerous school environment (upper half), with those that experience a simply rough environment.\nBy examining the category points plot (see Figure 4 .1) we expect to find the students attending schools where the overall climate is perceived to be poor in the right part of the picture and in particular in the lower right, those that believe the climate at their school is good at the lower left, those going to schools with some sort of problems somewhere in the middle, and those going to schools with problems described by variables A, D, F, G and K in the upper middle part of the picture. In Figure 4 .1 the object scores are ploted, together with their frequencies and the category points (lie on the red lines). The object point with the large spike on the left end corresponds to the profile consisting of only 4's; that is these students (approximately 11 of the sample) indicated that none of the areas covered by the 11 variables is a problem in their school. On the other hand, the big spike on the right end of the picture corresponds to the other extrem profile of only 1's; hence, close to 2 of the students attend pretty rough schools. It can also be seen that approximately 40 of the object scores are concentrated on the lower left part, indicating that these areas are at most a minor problem in the respective schools."}, {"section_title": "Stability Issues", "text": "The techniques presented so far aim at the uncovering and representation of the structure of categorical multivariate data. However, there has been no reference to any probabilistic mechanism that generated the data under consideration. The focus of these techniques is in providing a low dimensional representation of the original high dimensional space (where presumably the dependencies and interdependencies in the data are much easier to describe and characterize). As Kendall points out \"many of the practical situations which confront us are not probabilistic in the ordinary sense ... It is a mistake to try and force the treatment of such data into a classical statistical mould, even though some subjective judgment in treatment and interpretation may be involved in the analysis\" (see [64] , p. 4). Nevertheless, one may always pose the question of whether the patterns in the various plots are real or mere \"chance\" effects. Thus, the goal in this section is to give a brief overview of the question of stability of the representation. The concept of stability is central in the Gifi system and is used in the following sense: data analysis results are stable when small and/or unimportant changes of input lead to small and unimportant changes in the results (output) (see [32] p. 36). By reversing this definition, we get that in the Gifi system a result can be characterized as unstable in those instances where small and unimportant changes in the input result in significant changes in the output. In our analysis, we consider as input the data set at hand -objects and variables-, the coding of the variables, the dimensionality of the solution, the measurement level of the variables, the type of technique employed -Homals, Princals-, and as output category quantifications, object scores, discrimination measures, eigenvalues, component loadings etc. It should be noted that while in other types of data analysis the output usually consists of a small number of point estimates and their standard errors (e.g. regression analysis), for the techniques under consideration there exists a whole series of output.\nThe most important forms of stability are: In this section, we would focus primarily on stability under data selection. However, we are also going to look briefly into analytic and algebraic stability. It should be noted that issues of numerical stability have been addressed during the presentation of the various models (e.g. normalization issues etc).\nThe distinction between internal and external stability may provide a better understanding of the concept of stability [47] as used in the Gifi system. External stability refers to the conventional notions of statistical significance and confidence. In the conventional statistical framework, the aim of the analysis is to get a picture of the empirical world and the question is to what extent the results do indeed reflect the real population values. In other words, the results of any of our models are externally stable in case any other sample from the same population produces roughly the same results (output). Consequently, the confidence regions of the output parameters are relatively small. Internal stability deals with the specific data set at hand. The models produce a simple representation of the data and reveal associations between the variables. An internally stable solution implies that the derived results give a good summary of that specific data set. We are not interested in population values, because we might not know either the population from which the data set was drawn or the sampling mechanism; in the latter case, we might be dealing with a sample of convenience. Possible sources of instability in a particular data set are outlying objects or categories that have a large influence on the results. Internal stability can be thought of as a form of robustness.\nBoth external and internal stability play a role in the practice of data analysis. It is often the case that a data analyst wants to get insight in the structure of the population, particularly whenever the data set has been collected by a well defined sampling mechanism. In such cases, external stability of the results allows the practitioner to draw firmer conclusions about the structure of the underlying population. On the other hand, when a data analyst is only interested in the structure of the specific data set, internal stability ensures the invariance of the sample solution. When dealing with external stability a 'new' sample should be drawn and the technique applied to it. The word 'new' may mean (i) a truly new sample from the population, (ii) a fictitious 'new' sample, common in classical statistical analysis of stability [22] , (iii) a 'new' perturbed sample created by resampling with replacement from the sample at hand. In case of a fictitious 'new' sample, the data analyst attempts to assess stability of the technique by examining what would have happened if a truly 'new' sample was drawn from the underlying population. When dealing with internal stability, only the third possibility is available.\nThe notions of external and internal stability are directly linked with the notions of descriptive and inferential use of the models introduced in the previous sections. The main distinction between these two notions is whether the models are (i) exclusively used to reduce the complexity of the data and uncover their basic structure, or (ii) used to draw conclusions and generalize them from the sample to the population (see the debate between de Leeuw [22] and Molenaar [76] and references therein, see also Leamer [69] for some provocative thoughts on the subject from the Bayesian viewpoint). The former approach is closer to the exploratory data analytic school, while the latter closer to the dictates of classical statistics. Moreover, it is easy to see that the inferential use of the models agrees with the concept of external stability, while the descriptive use with that of internal stability. We believe that both approaches are useful and informative whenever used in the appropriate context. For example, when dealing with a data set from a well designed study -correctly identified population, proper sampling mechanism, proper collection of the data set, so that another sample can be drawn in the future-it is reasonable to be interested not only in examining the structure of that particular data set, but also in generalizing the findings to the population. Stability of the results is crucial in this exercise, because if the solution is not stable -small changes in the input lead to large and/or important changes in the output (results)-then the second part of the exercise -generalization from the sample to the population-becomes irrelevant. On the other hand, sometimes it is not possible to specify with great accuracy the population from which the sample was drawn, or the sampling mechanism used to collect the sample, or we might be interested just in the structure of that particular data set. In this frame of reference, the descriptive approach is the appropriate or relevant one, and stability is associated with the concept of internal stability. The data set regarding multivariate analysis books in chapter 1 in the Gifi book [32] , and samples of convenience are typical examples of this type.\nIn the remainder of this section we will address stability issues related to merging merging categories, omitting a variable from the analysis and the bootstrap. The first two topics have been briefly addressed in [32] , while the third one has been examined in [32, 73, 20, 95, 72] . 5.1. Merging Categories. Merging categories can be formalized algebraically by introducing indicator matrices G C j ; j 2 J of dimension`j k j , with k j `j, to replace G j by G j G C j . In case k j =`j, we get that G C j I`j and nothing changes. The orthogonal projector on the subspace spanned by the columns of nation measure is eliminated from the analysis the overall fit of the solution (eigenvalue) will not be affected much. Results for eigenvectors (object scores) are less complete, and it seems that a general pattern is hard to establish."}, {"section_title": "Permutation Methods.", "text": "Although we emphasized the exploratory nature of the techniques described in this paper, nevertheless we would like to determine whether the structure observed in the data is \"too pronounced to be easily explained away as some kind of fluke\", to paraphrase Freedman and Lane [31] . Permutation tests can help to study the concept of \"no structure at all.\" The idea behind using such tests is that they represent a nice way of formalizing the notion of no structure. The random variation is introduced conditionally on the observed data, which implies that we do not have to assume a particular model that generated the data, thus making them useful in nonstochastic settings as well [31] . Each new data set is generated by permuting the values the objects are assigned for each variable, resulting in destroying the original profiles of the objects. Then, the technique of interest is applied to the newly generated data and the eigenvalues of the solution computed. For small data sets in terms of both objects and variables (e.g. J=2) it is possible to derive the permutation distribution of the eigenvalues by complete enumeration of the possible cases. However, for all other cases one has to resort to Monte Carlo methods [21] .\nWe present next the results of such a test for the mammals dentition example. The two panels of Figure 5 .1 give the freqency distribution of the first and second eigenvalues of the homogeneity analysis solution over 1000 replications. It can immediately be seen that the observed eigenvalues of .73 and .38 in the original data are way to the right, thus suggesting that the informal null hypothesis of absence of structure is false and hence the patterns in the data (e.g. various groupings of the mammals) are real. We develop the method in a general context (for a comprehensive account see also [95] ).\nSuppose we have J categorical variables. Each variable takes values in a set S j (the range of the variable [32] ) of cardinality`j (number of categories of variable j). Define S = S 1 : : : S J to be the profile space, that has cardinality`= Q J j=1`j . That is the space S = fs 1 ; : : : ; s j ; s j 2 S j ; j 2 Jg contains the J-tuples of profiles. Let S be a` P J j=1`j binary matrix, whose elements Sh; t are equal to 1 if the h th profile contains category t, and 0 otherwise; that is S maps the space of profiles S to its individual components. Let also G S be a N `indicator matrix with elements G S t; h = 1 if the t th object (individual etc) has the h th profile in S, and G S t; h = 0 otherwise. The superindicator matrix G = G 1 j : : : jG J can now be written as G = G S S. Hence, there is a one-to-one correspondence between the ordinary indicator matrices G j and the space of profiles S.\nLet P be a probability distribution on S. Since the space S is finite, P corresponds to a vector of proportions p = fp h g with Ph =1 p h = 1. In the present framework, it is not difficult to see that each observed superindicator matrix G corresponds to a realization of the random variable that has a multinomial distribution with parameters N;p. The output (category quantifications, discrimination measures, object scores, component loadings etc) of the techniques introduced in the previous sections can be thought of as functions ."}, {"section_title": "THE GIFI SYSTEM OF DESCRIPTIVE MULTIVARIATE ANALYSIS 35", "text": "From a specific data set of size N we can draw N N sets also of size N, with replacement. In our case, each subset corresponds to a matrix G S . The basic idea behind bootstrapping techniques is that we might as well have observed any matrix G S of dimension N `consisting of the same rows, but in different frequencies, than the one we observed in our original sample. So, we could have observed a superindicator matrix G m , associated with a vector of proportions p m , which is a perturbed version of . The output of our techniques would then naturally be a function p m . Suppose that we have a sequence of p m 's and thus of functions p m . Then, under some regularity conditions on the : (Hadamard or Fr echet differentiability [87] ) it can be shown that p m is a consistent estimator of and that P p m zjp m is a consistent estimator of P p zjp [71, 86] , where P denotes the conditional probability given p m . The previous discussion indicates that the appropriate way to bootstrap in homogeneity analysis is to sample objects with replacement, or in other words, sample rows of the data matrix."}, {"section_title": "Remark 5.1. Bias Correction and Construction of Cnfidence Regions", "text": "Two of the main issues in the theory of bootstrap are: (i) how to produce unbiased bootstrap estimates and (ii) how to construct confidence regions with the correct coverage probability [26] . The main problem in the present context is that by construction the parameters of the techniques (eigenvalues, category quantifications, etc) are multidimensional, and moreover the dimensions are correlated with each other. Regarding bias correction two possible solutions proposed by Markus [72] b corresponds to the b th bootstrap point,^ to the sample estimate, and^ to the mean of the B bootstrap points. The first one defines bias as a shift of the estimate with respect to the population value, while the second as a reflection with respect to the original sample value. Regarding the problem of constructing confidence regions, several approaches have been suggested in the literature. Weinberg et al. [101] constructed ellipses based on the bootstrap variance-covariance matrix. They assumed that the sampling distribution is normal and the construction of confidence regions is based on F values. A similar approach can be found in Takane and Shibayama [90] . Heiser and Meulman [55] suggested to construct ellipses by performing a singular value decomposition of the matrix of bootstrap points that are in deviations from their means. This procedure results in a spherical representation that determines the circle covering the 1 , 100 points with the shortest distance to the centroid. Subsequently, the circle is transformed into an ellipse. This construction avoids any link to the normal distribution. Markus [72] uses the convex hull of the scatter of the bootstrap points to construct the confidence regions. She then discards the 100 of the outer vertices of the hull, and the resulting hull is considered to be the desired confidence region (this algorithm is discussed in [46] ; see also [45] ). This method resembles the percentile method for estimating bootstrap confidence intervals [26] ."}, {"section_title": "Results of Previous Studies.", "text": "There have been several studies that have used bootstrap methods to assess the stability of nonlinear multivariate techniques -homogeneity analysis, correspondence analysis, canonical correlation analysis- [32, 95, 73] . The most comprehensive one is the monograph by Markus [72] . In this section we will attempt to briefly summarize the results of these studies.\n(1) The most important finding from a computational point of view is that to obtain valid results a large number of bootstrap replications is required (over 1,000). (2) The bootstrap confidence regions give on average the right coverage probabilities. However, for categories with low marginal frequencies the coverage probabilities might be under-or overestimated. (3) Bias correction is beneficial to the coverage probabilities of eigenvalues but rather harmful to that of category quantifications, discrimination measures and component loadings. It seems that the translation method is the most appropriate for bias correction. (4) Marginal frequencies of about 8 seems to be the absolute minimum to ensure valid confidence regions. In light of this finding, merging categories appears to be not only overall beneficial, but necessary in many situations. (5) Both ellipses and peeled convex hulls produce valid confidence regions. However, this result heavily depends on a number of parameters, such as sample size, number of bootstrap replications, category marginal frequencies. In case of small sample sizes, the behavior of confidence regions becomes erratic. (6) There are no results regarding the stability of patterns for ordinal and/or numerical variables (ordering of categories), and also in the presence of missing data.\nIn Figure 5 .2 we present the bias corrected bootstrap means of some of the optimal transformations, along with 2 standard error bands of the category quantifications of the mammals dentition data set, based on 1,000 bootstrap replications. The bootstrapped means and standard errors (in parentheses) of the eigenvalues in dimensions 1 and 2 are .738 (.035) and .386 (.027), respectively.\nIt can be seen that the fit of the solution in both dimensions is particularly stable, thus indicating that the patterns observed in the data set are real. Regarding the category quantifications we see that the first dimension exhibits a far more stable behavior than the second one. However, for the variables that discriminate along the second dimension (TI, BI) the results are satisfactory . Moreover, we see that categories with low marginal frequencies (e.g. BI1) exhibit more variation than categories with larger frequencies, thus confirming the results of previous studies."}, {"section_title": "Concluding Remarks", "text": "In this paper a brief account of some varieties of multivariate analysis techniques, known as the Gifi system, is given. The central themes of the system are the notion of optimal scaling of categorical data and its implementation through alternating least squares algorithms. The starting point of the system is homogeneity analysis, a particular form of optimal scaling. The use of various types of restrictions allows homogeneity analysis to be molded into various other types of nonlinear multivariate techniques. These techniques have been extensively used in data analytic situations. In the Gifi book [32] the entire chapter 13 is devoted to applications covering the fields of education, sociology and psychology. Also, in their books Greenacre [47] and Benz ecri [4] give [57] , zoology [94] , environmental studies [93] , medicine [95] and food science [97] . However, the Gifi system has evolved beyond homogeneity analysis and its generalizations; hence, new techniques have been developed for path models [32] , time series models [92] , linear dynamical systems [5] etc. In closing, it should be mentioned that the Gifi system is part of a still quite active research program."}, {"section_title": "Appendix A -Dentition of Mammals", "text": "The data for this example are taken from Hartigan's book [51] . Mammals' teeth are divided into four groups: incisors, canines, premolars and molars. A description of the variables with their respective coding is given next. "}, {"section_title": "TI:", "text": ""}]