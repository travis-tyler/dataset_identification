[{"section_title": "Abstract", "text": "One of the most challenging aspects of medical image analysis is the lack of a high quantity of annotated data. This makes it difficult for deep learning algorithms to perform well due to a lack of variations in the input space. While generative adversarial networks have shown promise in the field of synthetic data generation, but without a carefully designed prior the generation procedure can not be performed well. In the proposed approach we have demonstrated the use of automatically generated segmentation masks as learnable class-specific priors to guide a conditional GAN for the generation of patho-realistic samples for cytology image. We have observed that augmentation of data using the proposed pipeline called \"SynCGAN\" improves the performance of state of the art classifiers such as ResNet-152, DenseNet-161, Inception-V3 significantly."}, {"section_title": "Introduction", "text": "The modern machine learning algorithms such as deep learning, have been greatly dependent on the availability of a large amount of high-quality data. But for various niche domains such as medical imaging large quantities of data are generally unavailable due to various constraints, such as lack of patients, infrastructural inadequacy, noisy environments, lack of experts for annotations and so on. However, with the advent of generative adversarial networks(GANs) [3] , an avenue for high quality data generation has opened. In its base form, GANs are capable of generating samples from a randomly sampled prior which demonstrates likeliness to a predefined data distribution. However, without proper guidance, the generation process can result in eccentric outputs. However, conditional GANs (CGANs) [8] , on the other hand, use a semantically sensible prior for guiding the data generation process to generate more accurate and meaningful samples. In the proposed work, we explore the ability of CGANs to work with learnable priors for efficient data generation to improve classifier performance on cytology images. In most practical cases the number of available data samples is too limited for deep learning approaches to thrive. Thus data augmentation serves as a primary tool for improving learning ability. Though annotating pixel specific masks for cytology images is a difficult and expensive job, however, with adequate expertise and a decent amount of labor it is possible to annotate at least a small batch of samples for a better semantic representation. The proposed approach makes use of such semantic masks to serve as a prior for CGANs. While generating fully detailed cytology images without priors is much difficult, the generation of segmentation masks from scratch is a much simpler task given that the output distribution is binomial. Our proposed approach makes use of this factor to create learnable segmentation masks that can guide CGANs for synthetic data generation. Some relevant studies are discussed in the next section. The proposed methodology is provided in the subsequent section followed by experimental setups, results, and discussions in section 4 and future scopes are discussed in the conclusion thereafter."}, {"section_title": "Related Works", "text": "Most common methods for data augmentation involved affine transformations[?] such as translation, rotation, scaling, shear, flipping and so on. Also, it has been noticed that training with added noise results in a much more robust classifier. The introduction of generative adversarial networks (GANs) has brought a shift in the paradigm of generative processes in computer vision. Several approaches have made use of GANs for data augmentation. Adar et al. [2] proposed a GAN based liver lesion data augmentation technique where after the extraction of ROI for classification was done using CNN. Dataset was augmented in two ways: i) the ROIs were augmented by affine transformations ii) the synthetic data was generated from ROIs using DCGAN(Deep Convolution Generative adversarial Network) and ACGAN(Auxiliary Classifier GAN). DCGAN showed greater performance compared to ACGAN. Shin, et al. [9] proposed a GAN based model to segment tumor of brain MRI images of two traditional datasets: ADNI and BRATs. Normal brain MRI images were segmented using an image to image translation model using CGAN [6] . Synthetic abnormal brain MRI scans were obtained from labels(tumors) by incorporating some changes in the label(e.g. increasing the size, changing the position of the label, or placing the tumor in a healthy brain MRI segmentation map). The synthetic images were used in data augmentation for training the model. Improved performance of tumor segmentation was observed by adding the synthetic data to the real data but without using normal data augmentation methods. Tom et al. [11] simulated patho-realistic ultrasound images of the IVUS dataset using deep generative models. Tissue echogenicity maps were generated from the ground truth of the dataset. From these maps simulation of ultrasound images was produced using a physics-based simulator. Two-staged GAN was used to generate patho-realistic ultrasound images and stability of training state. In the first stage, images from the simulator were taken as input to GAN from which low-resolution images were generated. In the second stage, these low-resolution images from the first stage of GAN were transformed into high-resolution images. Bissoto et al. [1] suggested a GAN based model to generate high-resolution images of skin lesion of the ISIC challenge dataset. Classifiers were trained on real data as well as on synthetic data."}, {"section_title": "Proposed Methodology", "text": "The goal of the current work is to generate realistic cytology images similar to images collected during FNAC(Fine Needle Aspiration Cytology) test. The cytology images were collected from Theism Medical Diagnostics Centre, Dumdum, Kolkata. These cytological data were mainly collected by FNAC test, and were captured using an Olympus microscope at 40X magnification in the presence of the professional practitioners. Around 156 cytology images were collected among which 77 were benign samples and 79 were malignant samples. "}, {"section_title": "The data augmentation pipeline", "text": "The proposed method of synthetic data generation consists of 3 phases. Firstly, segmentation masks are collected from real images using an unsupervised method. Secondly, a CGAN is trained on these pairs of images and auto-generated segmentation masks. Two sets of synthetic segmentation masks are generated using a GAN for each of the two classes. Finally, these synthetic segmentation masks are used to guide the previously trained CGANs to generate patho-realistic samples for data augmentation."}, {"section_title": "Mask generation", "text": "The proposed methodology requires a set of pixel-level annotations to guide a CGAN for data generation. Due to the lack of hand-annotated samples, an unsupervised approach was used for nuclei segmentation. There have been many developments in the field of image segmentation lately [?] . For our work, first, the contrast of the RGB cytology image is increased by the histogram stretching method. The image is then converted to a grayscale image. To eliminate the irrelevant portions, adaptive thresholding [7] based segmentation algorithm is adopted and the RGB image is converted to a binary segmented mask. But the red blood cell, cytoplasm which had similar high local contrast is distinguished using the Gaussian Mixture clustering algorithm. Finally, the refined binary segmented mask is extracted. The presence of an unsupervised mask generation technique alleviates the necessity of large amount of training data. Generating RGB images from scratch using a traditional GAN is difficult as the generation procedure can be represented as a prediction of 256-dimensional multinomial distribution across three channels for each pixel. However, the segmentation mask is simply a pixel specific binomial distribution which is much easier to predict when starting without a predefined prior. Thus a CGAN [8] must be trained which takes the segmentation masks generated in the previous step as a prior and a generator loss is reduced against the corresponding RGB image. We derive inspiration from the pix2pix network [6] . For the generator, we use a modified UNet like architecture. Normally the UNet architecture used transposed convolution for upscaling the feature maps. However, that results in checkerboard artifacts due to overlap of kernels during the fractional stride.\nInstead, bilinear interpolation opted for upscaling the feature map followed by a 3 \u00d7 3 convolution layer for refinement. The discriminator network has been directly implemented using the PatchGAN discriminator as demonstrated in [6] .\nThe discriminator attempts to detect real and fake samples from the dataset and the generator respectively. The objective function V can be written as:\nHere G and D refers to the generator and discriminator. x represents the RGB sample, y represents the corresponding auxiliary representation which serves as a prior for the generator. The x and y samples are drawn from the input data distribution p data (x, y) that consists of RGB images and their auxiliary representations or segmentation masks in the current scenario. In our case, the auxiliary representations are the automatically generated segmentation masks as described in section 3.2.\nAt every iteration, the discriminator and the generator are trained alternatively as was performed previously. During the training of the discriminator, the segmentation mask and its corresponding RGB image are concatenated on its channel dimension. It is then passed through the discriminator (Patch GAN) [6] and discriminator loss of the real image is calculated as below\nwhere x represent samples from the input database and y refers to segmentation masks of those samples. A binary cross-entropy loss function is used to calculate the adversarial loss. While training the generator, the segmentation masks are passed through the generator network and the loss is calculated. The loss has two components denoting the adversarial loss exhibited by the discriminator and the mean square error between the generated sample and the actual RGB image from the dataset that corresponds to the mask y. G loss = \u2212 log(D(G(y))) + \u03bb MSE(G(y), x|y)\nwhere \u03bb is the weight of Mean Squared Error (MSE) loss. The weight of the mean squared error loss is set to 100 based on empirical analysis on a small validation set. x and y represent samples from the RGB image and the segmentation mask dataset."}, {"section_title": "Training the GAN to generate segmentation masks", "text": "A conditional GAN(CGAN) usually generates synthetic samples conditioned by some predefined priors. In the current scenario, the CGAN has been trained to generate samples from segmentation masks highlighting the spatial distribution of nuclei across the cytoplasm. To generate patho-realistic synthetic samples during the evaluation phase a class-specific prior distribution is necessary. For that purpose, we train a GAN model [3] to generate binary segmentation masks based on a randomly drawn seed from a gaussian distribution. While models like CycleGAN [?] can be used for image translation, it is not suitable for synthetic data generation. The most straight forward method to generate synthetic samples would have been to train an end-to-end GAN. However, it has been noticed in ablation studies that without a prior the quality of outputs is very poor. The primary reason being the complexity of predicting the intensity value of a pixel. Given the output image has three channels, each pixel exists within a search space with 256 3 . However, a binary segmentation mask is a much easier output to predict given that each pixel belongs to a binomial distribution. On the other hand, the shape information encoded within these segmentation masks is quite informative about the class of the samples, namely, benign or malignant. We train two separate GANs trained on segmentation masks belonging to each of the predefined classes.\nThe objective of the GAN network is simply defined as\nHere x refers to samples drawn from the input data distribution p data (x). z refers to randomly sampled priors from a Gaussian distribution p(z). G and D refers to the generator and discriminator network. The architecture of the GAN used in the current work is very similar to the one described in the previous section. It consists of a generator inspired from UNet whos transposed convolutions have been replaced with bilinear interpolation for upscaling and a convolution layer for refinement. The discriminator is derived from the PatchGAN discriminator as demonstrated in [6] . During the training phase, the discriminator loss is given by, D Loss = \u2212(log(D(x)) + log(1 \u2212 D(G(z)))) (5) and the generator loss is given by:\nHowever, due to very low number of samples, the discriminator was too overpowering and saturates at a very early stage. To deal with this issue some additional measures were taken as described below [?] .\n-Label smoothing: The labels for real and fake samples are set as 1 and 0 by default. To enforce some fuzziness in the system, a random number between 0.9 and 1 was taken for real samples and a random number between 0.1 and 0 was taken for fake samples while training. However, this is unnecessary while training the generator as we want to bottleneck the learning curve of the discriminator and not the generator. -Randomly flipping labels: To even further confuse the discriminator at some random iterations real samples are labeled as fake and vice versa. This confusion results provide some breathing space for the generator so that it can learn the requisite features.\nOther models such as Wasserstein GAN[?] can further improve results."}, {"section_title": "Evaluating on the trained model to generate synthetic images", "text": "The final phase generates class specific patho-realistic synthetic samples. According to the pipeline discussed in section 3.1, at first the class-specific GANs are used to generated segmentation masks (refer section 3.4). Then the generated segmentation masks are fed into the trained CGAN model (refer section3.3) to obtain the RGB synthetic images. The synthetic data distribution is modeled as,\nwhere, G CGAN and G GAN refers to the generators of the CGAN and GAN model described in section 3.3 and 3.4 respectively. While a simple CGAN trained on a prior denoting the class of samples could also be viable, having a richer representation like a mask provides more flexibility in terms of variations in the synthetic dataset."}, {"section_title": "Experiments and Results", "text": "The main objective of the work is to generate class-specific synthetic data similar to microscopic cytology images that can boost the performance of standard image classification algorithms. For our experiments we have used three classifiers namely, ResNet-152 [4] , Inception-V3 [10] and DenseNet-161 [5] . Each of these networks has a proven track record in tough image classifications tasks such as the ILSVRC. In the original dataset, there were 156 images in total. Out of which 77 were benign samples and 79 were malignant samples. For the benefit of a cleaner calculation, a total of 150 images were selected with 75 images from each class. The dataset was randomly divided in the ratio of 3:1:1 into training, validation, and testing set with an equal number of samples in each class. The ratio of synthetic data to original training data was maintained at 2:1. Thus 90 synthetic training images were generated per class. "}, {"section_title": "Experimental Setup", "text": "The performance metric defining the goodness of the synthetic data generation pipeline, referred to as SynCGAN in the current work, is given by its impact on the test accuracy obtained using the three previously mentioned network, namely, ResNet-152, Inception-V3, and DenseNet-161. The experiment was conducted to analyze the impact of synthetic data augmentation on several grounds.\n1. Impact of augmentation using data generated by SynCGAN, 2. Performance of SynCGAN generated data augmentation against traditional data augmentation, 3. Performance of SynCGAN generated data augmentation against GAN generated data augmentation.\nThe CGAN model trained for a maximum of 200 epochs and the best model was saved based on minimum generator loss on the validation dataset. While the GAN model was trained for almost until the generator loss saturated(1600 epochs). For both the cases adam optimizer was used. All the experiments were conducted on Nvidia GTX 1060 GPU."}, {"section_title": "Observations and Analysis", "text": "The first observation as shown in table 2, shows that augmentation of data generated with the proposed SynCGAN improves the performance of classifiers. When compared with traditional augmentation techniques like random horizontal and vertical flipping, random rotation and addition of Gaussian noise, the proposed method of augmentation has a higher impact. When traditional data augmentation was combined with SynCGAN based augmentation, the performance was either at par or lower than exclusive SynCGAN based augmentation.\nSecondly, as a control to our proposed model, we implemented a purely GAN based pipeline for data augmentation, which performed far below the proposed model as shown in table 3. This GAN based architecture also had a generator and discriminator network similar to our proposed model for a fair comparison. "}, {"section_title": "Conclusion", "text": "In the present work, a CGAN based data augmentation technique has been proposed using class specific priors that improves the performance of various state of the art CNNs such as ResNet-152, DenseNet-161, and Inception-V3 on cytology images corresponding to FNAC tests. Unlike a normal GAN, we have used learnable segmentation masks as class-specific priors to guide a conditional GAN for more robust synthetic data generation. It is to be noted that the method is quite dependent on the mask generation algorithm and hence extensive studies may be performed using other available nuclei segmentation techniques for further analysis. Furthermore, the method may be further generalized to adapt to other types of cytological data."}]