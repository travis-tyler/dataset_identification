[{"section_title": "Abstract", "text": "ABSTRACT It becomes increasingly important in using genome-wide association studies (GWAS) to select important genetic information associated with qualitative or quantitative traits. Currently, the discovery of biological association among SNPs motivates various strategies to construct SNP-sets along the genome and to incorporate such set information into selection procedure for a higher selection power, while facilitating more biologically meaningful results. The aim of this paper is to propose a novel Bayesian framework for hierarchical variable selection at both SNP-set (group) level and SNP (within group) level. We overcome a key limitation of existing posterior updating scheme in most Bayesian variable selection methods by proposing a novel sampling scheme to explicitly accommodate the ultrahigh-dimensionality of genetic data. Specifically, by constructing an auxiliary variable selection model under SNP-set level, the new procedure utilizes the posterior samples of the auxiliary model to subsequently guide the posterior inference for the targeted hierarchical selection model. We apply the proposed method to a variety of simulation studies and show that our method is computationally efficient and achieves substantially better performance than competing approaches in both SNP-set and SNP selection. Applying the method to the Alzheimers Disease Neuroimaging Initiative (ADNI) data, we identify biologically meaningful genetic factors under several neuroimaging volumetric phenotypes. Our method is general and readily to be applied to a wide range of biomedical studies.\nKEYWORDS imaging genetics; genome-wide association studies; SNP-set; Bayesian variable selection; Markov chain Monte Carlo I N modern genetics, genome-wide association studies (GWAS) has become a popular tool to study complex human diseases (Walsh et al. 2014; Wang et al. 2014; Hibar et al. 2015) . The goal of GWAS is to identify single nucleotide polymorphisms (SNPs) associated with complex traits. Over the last few years, improvement in genotyping technology has enriched the measurements of SNPs to .1 million (Altshuler et al. 2008) . Due to the ultrahigh-dimensionality of SNPs, most GWAS approaches analyze one SNP at a time to test marginal association of the SNP with phenotype. However, in many scenarios, large differences can exist between marginal effects of SNPs and their joint effects (He and Lin 2011). Thus, it is imperative to carry out whole-genome GWAS that considers all SNPs together.\nThere are at least two challenges associated with wholegenome GWAS. The first one is the \"large p small n\" problem. Corresponding author: Department of Healthcare Policy and Research, Cornell University Weill Cornell, 402 East 67th St., New York, NY 10065. E-mail: yiz2013@ med.cornell.edu 2 Data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.usc. edu/wp-content/themes/freshnews-dev-v2/documents/policy/ADNI_Acknowledgement_"}, {"section_title": "I", "text": "N modern genetics, genome-wide association studies (GWAS) has become a popular tool to study complex human diseases (Walsh et al. 2014; Wang et al. 2014; Hibar et al. 2015) . The goal of GWAS is to identify single nucleotide polymorphisms (SNPs) associated with complex traits. Over the last few years, improvement in genotyping technology has enriched the measurements of SNPs to .1 million (Altshuler et al. 2008) . Due to the ultrahigh-dimensionality of SNPs, most GWAS approaches analyze one SNP at a time to test marginal association of the SNP with phenotype. However, in many scenarios, large differences can exist between marginal effects of SNPs and their joint effects (He and Lin 2011) . Thus, it is imperative to carry out whole-genome GWAS that considers all SNPs together.\nThere are at least two challenges associated with wholegenome GWAS. The first one is the \"large p small n\" problem. To address this issue, various regularization or screening methods (Tibshirani 1996; Fan and Li 2001; Efron et al. 2004; Zou and Hastie 2005; Zou 2006; Fan and Lv 2008) were proposed and recently extended to the context of GWAS (Hoggart et al. 2008; Wu et al. 2009; Cho et al. 2010; He and Lin 2011; Sampson et al. 2013; Jiang et al. 2016; Bao and Wang 2017; Huang et al. 2017) . As an alternative, Bayesian methods also play a prominent role in solving variable selection problem. O'Hara and Sillanp\u00e4\u00e4 (2009) provided an overview of several commonly used Bayesian variable selection methods and posterior simulation algorithms, such as the Gibbs variable selection (GVS) (Dellaportas et al. 2002) and the stochastic search variable selection (SSVS) (George and McCulloch 1993) . Compared with regularization methods, Bayesian models have the natural advantage to quantify uncertainty and combine prior information. Their recent application on GWAS also showed a higher detection power by simultaneously fitting multiple marker effects and implicitly correcting biological structures (Sahana et al. 2010; Dashab et al. 2012; K\u00e4rkk\u00e4inen and Sillanp\u00e4\u00e4 2012) . One major limitation to apply Bayesian variable selection models on GWAS is the intensive computation. Therefore, existing approaches made attempts to explore different inference mechanisms to reduce the computational cost or improve the mixing of Markov chain. For instance, besides traditional Markov chain Monte Carlo (MCMC) algorithm (Guan et al. 2011) , variational Bayes (Carbonetto et al. 2012) , evolutionary stochastic search (Bottolo et al. 2013 ) and other variations of stochastic searching algorithms (Briollais et al. 2016; Yang et al. 2017) were developed under Bayesian sparse models for multi-SNP analysis. Alternatively, Bayesian lasso (Li et al. 2010; Jiang et al. 2016 ) and Bayesian mixed model (Zhou et al. 2013; Zhou 2014) were also considered to improve the scalability in the presence of ultrahigh-dimensional SNP data.\nAnother challenge is caused by biological architecture. Multiple causal SNPs may be located in a single region, each with a small effect. In order to increase power to map them, it is desirable to consider them simultaneously and perform SNP-set analysis. A variety of methods were proposed for SNP-set analysis, including, but not limited to, Kwee et al. (2008) , Wang and Abbott (2008) , Wu et al. (2010) and a more recent Bayesian latent sparse model by Lu et al. (2015) . Tzeng et al. (2011) provided an overview of different marker-set approaches for gene-trait association and appealing features of set-based methods compared with those using individual SNPs. For the marker-set methods, grouping strategies to define SNP-sets play a vital role in practice. As suggested by Wu et al. (2011) , the ones incorporating biological information, for example grouping SNPs by genes, pathways or haplotype/linkage disequilibrium (LD) blocks, tend to gain more power. Based on the defined SNP-sets, analysis could be carried out through weighted sum of genotypes (Wang and Elston 2007; Price et al. 2010) , U-statistics (Tzeng et al. 2003; Wei et al. 2008) , or variance-component methods (Tzeng and Zhang 2007; Wu et al. 2010) . Meanwhile, the use of SNP-sets can also reduce the number of predictors as well as alleviate the collinearity issue since the correlations are much smaller among SNPsets than SNPs.\nThe appealing features of SNP-set analysis motivate us to incorporate set-wise information into the selection procedure. In this paper, we develop a Bayesian hierarchical variable selection model to carry out whole-genome association analysis and achieve SNP/SNP-set trait association mapping. Our variable selection approach is inherently hierarchical, and involves selection at both SNP-set level and individual SNP level. Although there is a broad literature on Bayesian variable selection under high or ultrahigh-dimensional feature space (Bottolo et al. 2010; Johnson and Rossell 2012; Johnson 2013) , few efficient hierarchical variable selection methods have been developed. Stingo et al. (2011) considered gene expression data, and adopted Bayesian spike-andslab priors to simultaneously select genes and pathways. Rockova et al. (2014) proposed a two-step procedure to carry out hierarchical variable selection under Bayesian group Lasso. Combining spike-and-slab priors with shrinkage priors, Duan and Thomas (2013) , Zhang et al. (2014a,b) , Liquet et al. (2017) proposed similar modeling frameworks, and achieved group level selection and withingroup shrinkage. Though the existing approaches are promising, they suffered with two limitations. First, the methods employ traditional MCMC algorithm, which are computational intensive and difficult to scale up. Second, the group level selection does not utilize any structural information, which could lead to poor performance. Recently, Tang et al. (2017) proposed a Bayesian hierarchical generalized linear models incorporating group information, and developed an EM algorithm for parameter estimation. However, the method does not impose group-level sparsity and may be less powerful under sparse signal like GWAS application.\nIn this paper, we develop a Bayesian hierarchical selection model, named Sparse Group Hierarchical Sampling (SGHS), with an efficient posterior inference algorithm. The key idea of SGHS is to reallocate posterior computation spending on the potential signal and noise parts via a \"smart\" proposal distribution. The sampling scheme for the proposal distribution is specified by an auxiliary model constructed under set-wise variable selection using factor regression. Such a modeling scheme allows the MCMC algorithm to explore the entire sample space more efficiently and dramatically mitigate the computational burden of updating large-scale unknown parameters. Simultaneously, grouping and structural information are integrated within the posterior inference, leading to the improvement on selection accuracy and results interpretability.\nThe remainder of this article is organized as follows. In Model specification, we present our basic model for variable selection, prior specifications, and a standard posterior computation algorithm. In Sparse group hierarchical sampling, we propose our SGHS for hierarchical variable selection. We conduct simulation studies in Simulation studies to assess the performance of our proposed approach, and in The Alzheimer's Disease neuroimaging initiative, we apply the method to the ADNI data to identify disease related genetic information. Finally, we conclude with a Discussion."}, {"section_title": "Materials and Methods", "text": ""}, {"section_title": "Model specification", "text": "We introduce the variable selection model in a Bayesian framework. Our description is based on GWAS, but the proposed method is general and readily extended to other applications. Assume there are n subjects in the data. For subject i \u00bc 1; . . . ; n, y i is a trait of interest, such as the brain volume of a region of interest (ROI), and s i is a p 3 1-vector of clinical variables, including an intercept term. Suppose that the whole genome contains J SNPs, based on which, K SNP-sets are defined with J k SNPs included in SNP-set k. We assume the SNPsets are mutually exclusive, then the total number of SNPs J \u00bc P K k\u00bc1 J k . We let x ijk denote the genotype of SNP j within SNP-set k representing by the minor alleles, and we also conduct a subsequent normalization for each SNP.\nWe consider a standard regression model for hierarchical variable selection given by\nwith the residual error e i $ N\u00f00; h\u00de. Here c k 2 f0; 1g is the setwise selection indicator describing the selection status for SNP-set k, and g jk 2 f0; 1g is the individual one for SNP j\nOur goal is to identify risk SNP-sets along with specific SNPs by using the selection indicators \u00f0c k ; g jk \u00de j\u00bc1;...J;k\u00bc1;...;K . Particularly, x ijk is included in model (1) if both c j and g jk are nonzero. We can further write model (1) in a more compact form\nwhere y \u00bc \u00f0y 1 ; . . . ; y n \u00de \u22a4 ; e \u00bc \u00f0e 1 ; . . . ; e n \u00de \u22a4 ; x jk \u00bc \u00f0x 1jk ; . . . ; x njk \u00de \u22a4 ;\nHere \"M\" is the individual-to-set mapping matrix with \"\u2218\" representing the Hadamard product (Styan 1973) . As is the general case in practice, we assume that \"signals\" are sparse with the cardinality of the active set d\nIt is worth noting that, in practice, SNP-sets can be overlapped based on certain biologically grouping strategy, e.g., genes, pathways. In such cases, we can randomly assign a SNP to one of its overlapped SNP-sets.\nWe introduce priors for all the parameters in model (1). Specifically, we assign conjugate Gaussian priors to the regression coefficients as\nand conjugate hyper-priors for the variance parameters s point mass at zero. Compared with mixture prior, the above specification enables a more efficient updated scheme for coefficients, which we will explain later. In terms of the selection indicators c and g, we assume separate independent Bernoulli priors\nwhere r \u00bc \u00f0r 1 ; . . . ; r K \u00de and f \u00bc \u00f0f 11 ; f 12 ; . . . ; f JK \u00de; with r k controlling the proportion of SNP-sets in the model, and f jk determining the proportion of significant SNPs. We finally assign a prior for the variance of residual error h $ Inv 2 Gamma\u00f0a 0 ; b 0 \u00de. Parameters included in the posterior inference are a, b, c, g, s \n\u00c1 p\u00f0yjS; X; a; b; c; g; h\u00de:\nStandard MCMC algorithm can be implemented to update each parameter from its full conditional distribution. The details of updating scheme for all the parameters are provided in Appendix A1. As noted, to update b, we resort to a block updating scheme by dividing the whole long vector into two blocks, b 1 and b 0 ; corresponding to the selected (g \u00bc 1) and unselected (g \u00bc 0) predictors. Thus, the conditional distributions of b 1 and b 0 are given by\nwhere\ng X g with X g corresponding to the active set. Such vector-wise updating scheme can dramatically reduce computational cost and lead to better mixing than the single-site Gibbs sampling."}, {"section_title": "Sparse group hierarchical sampling", "text": "Standard MCMC algorithm becomes inefficient in the presence of high-dimensional data, and even computationally infeasible under GWAS with tens of thousands of predictors. Therefore, in this section, we propose the SGHS scheme to overcome the computational complexity in the posterior computation. The key idea of SGHS is to construct an auxiliary model based on set-wise variable selection, which subsequently realizes a reweight of posterior computational effort on potential signals and noises in the target model to efficiently search among model spaces.\nSpecifically, we first introduce an auxiliary set-wise indicatorc \u00bc \u00f0c 1 ; . . . ;c K \u00de; and define each of their elements\nThrough this definition, the auxiliary set-wise indicatorc is completely determined by the SNP-wise selection indicator, g; by the selection consistency between two levels, and the structure ofc stays the same as that of the set-wise selection indicator c. Accordingly, the posterior distribution for the parameters becomes \nwhere p\u00f0c g\u00de \u00bc 1 only if (7), and zero otherwise. In the right-hand side of (8), the first term consistent with Equation 5 is the main probability part, while the second term captures the connection of the selection information between SNP-set level and SNP level, inducing the incorporation of group membership. A comparison between (5) and (8) shows that the posterior probabilities and sampling schemes for parameters a, b, h, and hyper-parameters s \nwhere the subscripts \"c\" and \"*\" denote the current value and the proposed value, and \"\u2022\" represents all the other parameters. Here P\u00f0\u00c1j\u00c1\u00de specifies the sampling scheme for the auxiliary indicatorc * , and we design it to depend only on the data as shown in Equation 9. Such specification removes the interference of other parameters, which allows function P\u00f0\u00c1j\u00c1\u00de to be achieved by a separate model with variable selection only at SNP-set level. We refer to this model as an auxiliary model (distinguished from the target model) with the goal to help the posterior simulation of the target model. Given the sampled valuec * , the sampling scheme for \u00f0c * ; g * \u00de is specified by function H\u00f0\u00c1j\u00c1\u00de; which induces the incorporation of information from SNP-set level selection. Under such a proposal distribution, function P\u00f0\u00c1j\u00c1\u00de is a receiver with the set-wise selection information \"copied\" from the auxiliary model, and function H\u00f0\u00c1j\u00c1\u00de is a transmitter to pass such information from P\u00f0\u00c1j\u00c1\u00de to the target model. In the following sections, we will discuss the choice of P\u00f0\u00c1j\u00c1\u00de and H\u00f0\u00c1j\u00c1\u00de that could lead to a more efficient posterior sample procedure.\nReceiver function P\u00f0\u00c1j\u00c1\u00de: As discussed before, the sampling procedure of receiver function P\u00f0\u00c1j\u00c1\u00de is induced by an auxiliary SNP-set level selection model. In other words, realization of P\u00f0\u00c1j\u00c1\u00de can be directly simulated from the posterior distribution of the auxiliary model. As a natural choice of an auxiliary model, following model (1), we have\nwhere e a i $ N\u00f00; h a \u00de. We use a superscript \"a\" in (10) to distinguish the corresponding parameters in the auxiliary model from the target model.\nModel (10) turns out to be less attractive in general cases where not all the SNPs within a SNP-set are predictive for phenotype of interest. Since the assumption of nonzero coefficients in the risk SNP-sets is conflicted with the within-group level sparsity, it is difficult for model (10) to approximate the truth. To address this issue, we resort to an alternative set-wise selection model based on an empirical factor (principal component) regression under each SNP-set. Specifically, we adopt a reduced rank singular-value decomposition (SVD) on each X k , namely\nwhere U k is a diagonal matrix formed by positive singular values of X k , and A k is the L k 3 J k loadings matrix subject to A k \u00f0A k \u00de \u22a4 \u00bc I. By replacing each X k with the reduced SVD representation, the new auxiliary model becomes\nwhere u kl is the coefficient for factor l within SNP-set k. In GWAS data where there are strong correlations among predictors, to avoid overfitting, we could also allow certain truncation by specifying a cutoff on the number of factors included with\nselected SNP sets, the overall nonzero assumption of vector u k also fits many scenarios in real applications. Thus, we use model (11) as the auxiliary model in our applications. Again, the choice of an auxiliary model is not uniquely determined since it serves only as a platform to influence posterior sampling of the main model, but model (11), for example, has been shown to perform well in a wide range of scenarios. Following the prior specification in (3) for the target model, we assign similar conjugate priors for the coefficients\nand for the variance parameters\nand s 2\u00f0a\u00de u $ Inv 2 Gamma\u00f0a 3 ; b 3 \u00de. We assign independent Bernoulli priors for the auxiliary indicators\nwhere c a \u00bc \u00f0c a 1 ; . . . ; c a K \u00de and l \u00bc \u00f0l 1 ; . . . ; l K \u00de with l k controls the proportion of selection. We further introduce a hyperprior for each proportion parameter as follows: l k jt $ td\u00f00\u00de \u00fe \u00f01 2 t\u00deBeta\u00f0f ; g\u00de; (14) with f ; g the shape parameters. The hyper-prior (14) distinguishes the posterior probabilities between risk and nonrisk SNP-sets via a mixture of a zero point mass and a spread Beta distribution. By combining priors (13) and (14), we can integrate out l k ; which results in independent Bernoulli priors for indicator c a k with proportion parameter j \u00bc f \u00f01 2 t\u00de f \u00feg . In this case, we use (13) and (14) to ensure sparsity in the group level auxiliary model, which subsequently brings impact to the target model.\nTransmitter function H\u00f0\u00c1j\u00c1\u00de: To efficiently sample from the large feature space, we propose H\u00f0c * ; g * g c ; c c ;c * ;c c \u00de as follows:\nwith f \u00f0xja; b; c\u00de \u00bc \u00f01 2 a\u00ded 0 \u00f0x\u00de \u00fe a\nwhere u, n 1 ; and n 2 2 \u00f00; 1\u00de are tuning parameters to make sure detailed balance in the M-H step is satisfied, as well as to allow enough information to be borrowed from proposals. Specifically, u determines the amount of difference between SNP-set selection and the proposal, and if a SNP-set is currently unselected, n 1 controls the sparsity for the selection of its SNPs; otherwise, n 2 influences the agreement of the SNPlevel selection between proposed and current status. In practice, we specify u \u00bc 0:95, n 1 \u00bc 0:5; and n 2 \u00bc 0:9 to allow an efficient transmission of the selection information. To implement this M-H step, we first drawc k; * $ P\u00f0c k; * S; X; y\u00de with P\u00f0c k; * S; X; y\u00de simulated by the posterior distribution of c a k in the auxiliary model. Then, we draw \u00f0c k; * ; g k; * \u00de $ H\u00f0c k; * ; g k; * g k;c ; c k;c ;c k; * ;c k;c \u00de. Finally, we\nand set \u00f0c k; * ;c k; * ; g k; * \u00de \u00bc \u00f0c k;c ;c k;c ; g k;c \u00de when r , R with r $ U\u00bd0; 1. To further improve the mixing of Markov chains, in addition to the M-H step, we also conduct a further moving step for g k underc k \u00bc 1 from its full conditional (32). Since the true signal is sparse, such a moving step does not require a heavy computation. Under the specification of this proposal distribution, in our SGHS scheme, each SNP has a positive probability to be selected or unselected. However, instead of updating all the SNPs at each iteration of the posterior simulation, a large amount of nonrisk variables (noises) have been directly \"labeled\" iteration by iteration (without updating), which allows us to spend most of the computation updating potential signal part. As a result, the computational efficiency is dramatically improved compared with the existing MCMC algorithm. A detailed MCMC algorithm for the SGHS scheme is provided in Appendix A2."}, {"section_title": "Data availability", "text": "We used ADNI1 genetics and MRI image data that are available through application (http://adni.loni.usc.edu/about/ adni1/). All the ADNI data are shared without embargo through the LONI Image and Data Archive (IDA)."}, {"section_title": "Results", "text": ""}, {"section_title": "Simulation studies", "text": "We conduct simulation studies to evaluate the finite-sample performance of SGHS. Our goal is to select genetic markers that are highly associated with outcome of interest. We focus on the comparison between the proposed SGHS and existing methods include lasso (Lasso) (Tibshirani 1996) , smoothly slipped absolute deviation (SCAD) (Fan and Li 2001) , sparsegroup Lasso (SGL) (Friedman et al. 2010) , functional genome-wide association analysis (FGWAS) (Huang et al. 2017) , Bayesian variable selection with posterior inference via model averaging and subset selection (piMASS) (Guan et al. 2011) , and genome-wide efficient mixed model association (GEMMA) based on Bayesian sparse linear mixed model (Zhou 2014) . To better assess each method under GWAS, all the simulation scenarios are designed to mimic real genetic data. We generate n \u00bc 1000 subjects with the genetic information simulated from the Hapmap projects 2009-02 phaseIII data (International HapMap 3 Consortium 2010). Specifically, for each subject, we randomly combine two haplotypes from the CEPH population to form its genotypes. We consider both a low dimensional scenario by randomly selecting 5000 SNPs, and a high-dimensional one with 100,000 SNPs. Under each scenario, we determine SNP-sets (LD blocks) by starting from an initial SNP m with a putative block of SNPs Table 2 Simulation results: feature selection performance with J \u00bc 5000 Sens, the average sensitivity; Spec, the average specificity; J Stat, the average Youden's J statistic; and AUC, the average area under the receiver operating characteristic curve. Sens, the average sensitivity; Spec, the average specificity; J Stat, the average Youden's J statistic; and AUC, the average area under the receiver operating characteristic curve.\nfm; m \u00fe 1; . . . ; m \u00fe 100g and considering sub-blocks fm; m \u00fe 1; . . . ; m \u00fe kg with k \u00bc 100; 99; . . . ; until .50% of elements in the corresponding k 3 k matrix of r 2 value surpass the threshold k. To further assess the impact of LD structure, we set different thresholds with k \u00bc 0:01; 0:05; and 0.1 to construct the grouping information, based on which, we consider the following two cases of signal patterns to evaluate the robustness of variable selection.\nCase 1: We randomly select 10 risk SNP-sets. Within each of them, we randomly set 10 SNPs as risk ones. Case 2: We randomly select 10 risk SNP-sets. Within SNPset k with k \u00bc 1; . . . ; 5, we randomly set k SNPs as risk ones; and within SNP-sets k9 with k9 \u00bc 6; . . . ; 10, all the SNPs are risk ones. We denote d k as the number of risk SNPs in the SNP-set k.\nBased on the two signal patterns, we consider a variety of settings for the value of nonzero regression coefficients in model (1), i.e., the regression coefficients of risk SNPs denoted by b 1 , as shown in Table 1 . Specifically, in Case 1, where sparse signals exist, we consider six different settings. In Settings 1 and 2, as the starting point, we assign a unified genetic effect under different magnitudes. In Settings 3 and 4, the associations between the risk SNPs and the phenotype become more general but the majority of them are in the same sign. Finally, in Settings 5 and 6, we allow both positive and negative genetic effects within each SNP set to purposely create barriers for the internal SNP-set level selection. In Case 2, we further dilute the signals in the later half of risk SNPsets while keep sparsity in the former half in Settings I and II, and our design guarantees the average strength of signals in each risk SNP-set is comparable. For each setting, we generate 100 Monte Carlo (MC) datasets to assess feature selection performance among all the methods.\nTo implement SGHS, we conduct posterior inference with random initials for 10,000 iterations with 5000 burn-in for both the auxiliary and target models. The average computational time per dataset is 3.4 min for low dimensional scenarios and 26.5 min for high dimensional ones (Matlab implementation, 3.4 GHz CPU, 8 GB Memory, Windows System) to finish the whole posterior inference and the convergence is checked by GR method (Gelman and Rubin 1992) as well as trace plots. To allow for a noninformative hyper-prior, we assign a relative large value for a i and b i \u00f0i \u00bc 1; 2; 3\u00de as 10. We also set j \u00bc 0:1 to accommodate sparse signals in the auxiliary model, and less informative r k \u00bc 0:5 and f jk \u00bc 0:5 in the target model. Finally, we truncate the number of factors in each subset by looking for the minimum number of singular vectors that can explain $70% of the total variance. For the competing methods, we use R packages glmnet, ncvreg, and SGL to implement Lasso, SCAD, and SGL, and public released pipelines for FGWAS, GEMMA, and piMASS. For the Bayesian algorithms GEMMA and piMASS, we set all the tuning parameters as recommended by the manuals, and the average computational time is 10.9/ 48.6 min for GEMMA and 2.1/16.6 min for piMASS under low/high dimensional scenarios. Finally, the feature selection performance is assessed by sensitivity (Sens); specificity (Spec); Youden's J statistic (J Stat), which equals sensitivity \u00fe specificity21; and area under the receiver operating characteristic curve (AUC) for all the methods.\nThe simulation results are summarized in Table 2 and  Table 3 under low and high dimensions. To determine selection status for the Bayesian methods, i.e., GEMMA, piMASS, and SGHS, we use 0.5 as cutoff on the marginal posterior probability of selection indicator (Barbieri and Berger 2004) . We first compare our method to the competing approaches. For all the settings under different LD thresholds k and dimensions, our proposed method outperforms all competing methods in selection accuracy with a higher sensitivity, J Stat and AUC. Specifically, Settings 1 and 2 are constructed with a unified genetic effect among risk SNPs, and SGHS achieves the highest AUC and a much higher J Stat than its competitors under different dimensions and k. In Settings 3 and 4, with a more general genetic effect, SGHS maintains its satisfactory performance in feature selection. In Settings 5 and 6, where positively and negatively associated SNPs within a causal SNP-set are almost equally distributed, while we notice decreases on AUC and J Stat for almost all the methods, SGHS still achieves a superior selection performance compared to other methods along with a satisfactory AUC. This reflects, to some extent, the robustness of SGHS to the extreme settings. Finally, the diluted signal patterns in Settings I and II further deteriorates the performance for LASSO, SCAD, SGL, piMASS, and GEMMA, but brings little impact on FGWAS and SGHS, with the latter remaining the best performer. Overall, Table 2 and Table 3 show a general pattern that SGHS works considerably better on detecting true risk features, as indicated by much higher sensitivities. Even though false positives have also been brought in as a consequence, we still obtain a remarkable feature selection accuracy.\nWhen comparing among the competing methods, we find the results are somewhat mixed across different settings. Specifically, SGL is the one we originally expected to outperform the rest of the competing methods as it also works under a hierarchical selection framework. Although SGL obtains higher sensitivities than the other competing methods in most of the scenarios, a lower specificity due to large number of false positives deteriorates the overall performance, especially in extreme situations. The two Bayesian methods piMASS and GEMMA achieve similar feature selection performance. Different from SGL, they lack power to detect true signals with very low sensitivities but high specificities. The performances of LASSO, SCAD, and FGWAS fluctuate in between, and none of them generally outperform the others.\nIn terms of comparison among different settings, dimensions, and LD structures, as illustrated previously, methods generally work better to detect unified or clustered signal patterns but may encounter difficulty when both positive and negative effects exist. We further calculate the average marginal posterior probabilities for the risk SNPs in causal SNPsets obtained from SGHS under both cases with k \u00bc 0:05. Figure 1 shows the boxplots representing these posterior probabilities for Setting 1 of Case 1 and Setting I of Case 2 under different dimensions. As shown in Figure 1 , under Case 1, the possibilities to identify risk SNPs in each SNPset are comparable. Under Case 2, where relatively sparse LA  3  41021263  41272081  31  0  9  121993508  122159267  37  8  4  20956632  20827274  44  0  11  50057854  55275456  99  0  8  55302231  55441025  32  0  12  21970019  22242951  86  21  8  84373550  84826056  59  0  20  12888105  12982672  29  0  RA  2  81058605  81607450  51  0  8  53851670  54119394  50  2  6  89741617  89985379  72  4  9  20625875  21101230  99  0  7  104491613  105158228  57  5  14  73223110  73425315  47  0  7  150167583  150491084  85  11  15  58109235  349838  97  6  8  13013625  13253219  99  7  17  18879649  19697976  75  3  8  32221412  32465554  66  0  LH  2  38140126  38328300  44  0  6  118538069  119102035  99 1  173854659  175094025  99  0  11  46198841  47293457  80  0  5  151739218  152417867  93  0  16  10019899  10292060  99  42  6  32304085  32395036  99  35  GM  2  105454590  105798292  55  7  6  81868051  82061044  47  21  6  169471078  169589925  40  0  WM  2  46537604  46763587  70  0  8  98930457  99109800  48  19  6  31434111  31518354  99  0  12  106625131  106950695  51  0  8  4766370  4893353  52  0  20  35487159  35925296  33  0  WB  6  167287772  167537594  50  17  9  4767677  4904969  39 signals exist in the first five SNP-sets, we notice a higher probability in identifying signals within these SNP-sets. However, when it comes to SNP-sets 6-10 with diluted signals, the majority of genetic effect is lost during the selection. Finally, SGHS achieves an equally good, or even better, performance under high-dimensional scenarios compared with lower dimensions, and the impact of k is not strong in our simulation settings."}, {"section_title": "The Alzheimer's Disease neuroimaging initiative", "text": "There has been substantial interest in investigating neurodegenerative diseases such as Alzheimer's disease (AD) based on neuroimaging and genetic markers. Data used in the preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni. loni.usc.edu). The ADNI was launched in 2003 by the National Institute on Aging (NIA), the National Institute of Biomedical Imaging and Bioengineering (NIBIB), the Food and Drug Administration (FDA), private pharmaceutical companies, and nonprofit organizations, as a $60 million, 5-year public-private partnership. The primary goal of ADNI has been to test whether serial magnetic resonance imaging (MRI), positron emission tomography (PET), other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment (MCI) and early AD. Determination of sensitive and specific markers of very early AD progression is intended to aid researchers and clinicians to develop new treatments and monitor their effectiveness, as well as lessen the time and cost of clinical trials. The Principal Investigator of this initiative is Michael W. Weiner, MD, VA Medical Center and University of California, San Francisco. ADNI is the result of efforts of many coinvestigators from a broad range of academic institutions and private corporations, and subjects have been recruited from over 50 sites across the US and Canada. The initial goal of ADNI was to recruit 800 subjects but ADNI has been followed by ADNI-GO and ADNI-2. To date, these three protocols have recruited over 1500 adults, aged 55-90 years, to participate in the research, consisting of cognitively normal older individuals, people with early or late MCI, and people with early AD. The follow-up duration of each group is specified in the protocols of ADNI-1, ADNI-2, and ADNI-GO. Subjects originally recruited by ADNI-1 and ADNI-GO had the option to be followed in ADNI-2. For up-todate information, see www.adni-info.org. We conduct GWAS analysis on imaging phenotypes related to AD, and our goal is to identify genetic markers that are associated with imaging traits, and to further assess their predictive power. The advantage of using imaging phenotypes in GWAS is that imaging measurements tend to benefit the identification of pathogenic genes due to their close relationship with the biological etiology of multiple neurodegenerative and neuropsychiatric diseases, e.g., AD (Cannon and Keller 2006; Turner et al. 2006; Peper et al. 2007; Paus 2010; Scharinger et al. 2010; Chiang et al. 2011a,b) . For imaging traits, the raw MRI data were collected through 1.5 Tesla MRI scanners with protocols individualized for each scanner, including standard T1-weighted images obtained using volumetric three-dimensional (3D) sagittal MPRAGE or equivalent protocols with varying resolutions. The T1-weighted MRI images were preprocessed by standard steps including anterior commissure and posterior commissure correction, skull-stripping, cerebellum removing, intensity inhomogeneity correction, segmentation, and registration (Shen and Davatzikos 2004) . Subsequently, 93 ROIs were labeled automatically by labeling the template and transferring the labels following the deformable registration of subject images (Wang et al. 2011) . After calculating the volume of each ROI for each subject, we consider nine of them as phenotypes: six subcortical regions, including left and right hippocampal volumes, left and right lateral ventricular volumes, and left and right amygdala volumes; and three global volumetric measures, including whole gray matter volume, whole white matter volume, and whole brain volume.\nA total of 818 subjects were genotyped using the Human 610-Quad BeadChip (Illumina, San Diego, CA). For data quality control, we focused on the 760 Caucasian subjects and removed ones identified as (i) sex check failure, (ii) .10% missing SNP, and (iii) outliers in the phenotypes/genotypes stratification, resulting in 745 subjects. The original SNPs data were generated from the Human Genome reference sequence build hg18, which were lifted over to hg19 in the current analysis. As a typical step in GWAS, we removed SNPs with (i) .5% missing values, (ii) minor allele frequency smaller than 5%, and (iii) Hardy-Weinberg equilibrium Pvalue , 1e\n26 . We also calculated the LD blocks to form the SNP-sets, and removed SNP-sets with single SNP. Eventually, 421,823 SNPs are left in our analysis, grouped into 16,084 SNP-sets with the number of SNPs varying from 2 to 100. We also include gender, age, and the first five principle component calculated by EIGENSOFT (Price et al. 2006) into the analysis. We adopt the SGHS approach to investigate the joint association of SNPs with each of the nine MRI phenotypes in light of the autosomal LD blocks information. All the posterior simulation and hyper-parameters settings follow a similar line as the simulation studies.\nWe first list all the selected SNP-sets associated with the nine imaging phenotypes along with the numbers of total SNPs and selected SNPs belonging to each set in Table 4 . Based on the number of selected SNPs (columns 6 and 11), we observe the phenomenon that none of SNPs is selected within certain risk SNP-sets (the 0s in columns 6 and 11). This demonstrates the fact that some genetic information is diluted as it is widely studied in GWAS. We further consider the final SNP-level selection. The Manhattan plots in Figure 2 provide the SNP-wise inclusion probability with respective to each imaging phenotype. Under a 0.5 cutoff, for each phenotype, we map the selected SNPs to their associated genes, and summarize these risk genes along with the number of total/selected SNPs in Table  5 . A further comparison between the number of selected SNPs and the total number of SNPs belonging to the risk gene (columns 3 and 4) demonstrates that our method is capable of identifying both sparse and diluted genetic information. Among the selected genes, a number of them have been reported previously in the literature. Such genes include ASAH2B (Avramopoulos et al. 2007 ), SGMS1 (Hsiao et al. 2013) , GRIN2A (Leuba et al. 2014) , Tmem176b (Melchior et al. 2010) . In addition, several other genes have been shown to be related to the brain dysfunction or implicitly associated with Alzheimer's disease. For instance, BRINP1 has been shown to highly express in various brain regions, and a lack of BRINP1 may lead to human psychiatric disorders (Kobayashi et al. 2014) . CCR6 has been implicated as an important biomarker associated with the inflammatory process of AD-like diseases (Subramanian et al. 2010) . RNA-SET2 deficiency interferes with brain development and myelination (Henneke et al. 2009) . Genes like CADM2, DLC1, and ABCC9 are related to Autism spectrum disorder (ASD) or Parkinson's disease (Casey et al. 2012; Jones et al. 2013; Lin et al. 2014) , which may also serve as potential biomarkers for AD. Based on the selected genes, we also conduct a gene annotation analysis based on the enrichment for Kyoto Encyclopedia of Genes and Genomes (KEGG) (Kanehisa and Goto 2000) . The associated pathways are calcium signaling pathway, which is a key component to regulate the neuronal excitability and processes related to the development neural diseases such as AD (Berridge 2013) , and neuroactive ligandreceptor interaction, which is a well-known biomarker for cognitive ability (Antonell et al. 2013; Kong et al. 2015) . As a comparison, we also perform GWAS based on single SNP analysis via PLINK (Purcell et al. 2007 ) by performing quantitative trait association, and provide the Manhattan plots for 2log10\u00f0p\u00de value under each imaging phenotype in Figure 3 . As a result, there are considerably less risk SNPs [associated with the human collagen alpha 1 (XIII) chain gene COL13A1] identified under both the well accepted 5 3 10 28 threshold and the 10 27 threshold suggested by Li et al. (2012) compared with the result obtained by SGHS.\nFinally, we assess the capacity of the selected genetic markers to predict imaging phenotypes using polygenic score (The International Schizophrenia Consortium 2009). Besides single SNP analysis and SGHS, we also apply SGL as another competing method. We use twofold cross validation by randomly splitting the dataset into equally sized ones, and perform the analyses on each as the training one. Under the corresponding testing sets, the Nagelkerke pseudo R 2 is calculated first for single SNP analysis based on the selected SNPs under different thresholds of P-values, and the grid of selected SNP numbers is further used as the thresholds for SGL and SGHS to obtain their scores. The final R 2 is averaged over two testing sets and we repeat the procedure five times to remove splitting bias. We present the results for all autosomes in Figure 4 , which clearly shows a dramatic improvement of prediction by using SGHS compared with single SNP analysis and SGL in almost all the autosomes and thresholds. The predictive power for the selected risk profiles varies across different imaging phenotypes, and we also see a general pattern of an increase number of selected SNPs leading to a higher polygenic score."}, {"section_title": "Discussion", "text": "In this paper, we develop a unified Bayesian framework to realize hierarchical variable selection, while inducing grouping effect among predictors. Motivated by GWAS, our proposed method incorporates SNP-set information into the variable selection procedure, and facilitates selection at both SNP-set level and SNP level. Furthermore, by introducing a novel sampling scheme based on an auxiliary model for group-level selection, our approach is computationally efficient under high-dimensional feature space. We show in the simulation studies that the proposed method achieves considerably better performance than a number of competing methods under a wide range of settings. By applying the proposed method to the ADNI data set, we identify important genetic information that is highly associated with the volumes of ROIs in the brain.\nWhile our method is applied to an imaging-genetics study with a quantitative trait as phenotype, it is directly applicable to a dichotomous variable (e.g., case or control). As discussed in Albert and Chib (1993) , one could use a probit regression model for the binary outcome, which leads to few modifications on the current posterior sampling scheme. In addition, we can also consider an incorporation of more biological information in the selection procedure. For instance, it is interesting to conduct Bayesian variable selection by incorporating the information on pathways and gene networks in microarray data (Li et al. 2010; Stingo et al. 2011) or functional connectivity for neuroimaging studies Goldsmith et al. 2014) . Similarly, we may introduce Ising or binary Markov random field (MRF) priors to the two levels of selection indicators in order to incorporate hierarchical biological information.\nAfter a realization of whole genome-wide association analysis, one extension of our work is to move forward to the whole-brain and whole GWAS. In this case, we need to use a multiple multivariate regression model to further capture the association among phenotypes (Zhu et al. 2014) . Besides the potential low detection power, the prohibitive computational cost will be the biggest issue of such analysis. A different direction is to extend the current model to the longitudinal data, which will increase the power to detect genetic association with neuroimaging phenotypes (Xu et al. 2014) . In this case, we need to modify our method to model the temporal association between responses and predictors while accounting for complex temporal correction structure. \nDraw u 1 (the coefficients corresponding to the selected predictors) and u 0 (the coefficients corresponding to the unselected predictors) separately from "}, {"section_title": "M-H", "text": "Step For k \u00bc 1; . . . ; K, \u2022 Drawc k; * $ P\u00f0\u00c1 S; X; y\u00de;\n\u2022 Draw \u00f0c k; * ; g k; * \u00de $ H\u00f0\u00c1 g k;c ; c k;c ;c k; * ;c k;c \u00de;\n\u2022 Draw r $ U\u00bd0; 1. Set \u00f0c k; * ;c k; * ; g k; * \u00de \u00bc \u00f0c k;c ;c k;c ; g k;c \u00de when r , R with R defined by Equation 17."}, {"section_title": "Moving", "text": "Step For g jk , j \u00bc 1; . . . ; J k withc k \u00bc 1, k \u00bc 1; . . . ; K, draw g jk $ p\u00f0g jk b; y; a; c; g \u00bd2j;2k ; h; S; X\u00de with the full conditional distribution defined by (32)."}]