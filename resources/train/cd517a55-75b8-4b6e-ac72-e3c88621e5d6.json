[{"section_title": "Abstract", "text": "Abstract. Multi-atlas based morphometric pattern analysis has been recently proposed for the automatic diagnosis of Alzheimer's disease (AD) and its early stage, i.e., mild cognitive impairment (MCI), where multi-view feature representations for subjects are generated by using multiple atlases. However, existing multi-atlas based methods usually assume that each class is represented by a specific type of data distribution (i.e., a single cluster), while the underlying distribution of data is actually a prior unknown. In this paper, we propose an inherent structure-guided multi-view leaning (ISML) method for AD/MCI classification. Specifically, we first extract multi-view features for subjects using multiple selected atlases, and then cluster subjects in the original classes into several sub-classes (i.e., clusters) in each atlas space. Then, we encode each subject with a new label vector, by considering both the original class labels and the coding vectors for those sub-classes, followed by a multi-task feature selection model in each of multi-atlas spaces. Finally, we learn multiple SVM classifiers based on the selected features, and fuse them together by an ensemble classification method. Experimental results on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database demonstrate that our method achieves better performance than several state-of-the-art methods in AD/MCI classification."}, {"section_title": "Introduction", "text": "Multi-atlas based morphometric pattern analysis using magnetic resonance imaging (MRI) data are recently proposed for automatic diagnosis of Alzheimer's disease (AD) and its early stage, i.e., mild cognitive impairment (MCI) [1, 2, 3, 4] . Generally, multi-atlas based methods mainly focus on the direct morphometric measurement of spatial brain atrophy of subjects, by non-linearly registering a brain image onto multiple atlases. Thus, multi-view feature representations can be generated from those multi-atlas spaces for each subject, where each atlas is regarded as a specific view. Compared with single-atlas based methods, multiatlas based methods can reduce registration errors by using multiple atlases, which is helpful in improving subsequent learning performance [1, 2, 5] .\nIn the literature, most of existing multi-atlas based methods simply assume that each class is represented by a specific type of data distribution (i.e., a single cluster). Although such assumption may simplify the problem at hand, it will definitely degrade the learning performance because the underlying distribution structure of data is actually a prior unknown. In practice, the potentially complicated distribution structure of neuroimaging data within a specific class could result from several facts [6] , e.g., 1) different sub-types of a specific disease, and 2) an inaccurate clinical diagnosis. Intuitively, modeling such inherent structure of data distribution can bring more prior information to the learning process. However, no previous methods employ such information in their learning models.\nIn this paper, we propose an inherent structure-guided multi-view learning (ISML) method for AD/MCI classification. Specifically, we first non-linearly register each brain image onto multiple selected atlases, through which multi-view feature representations for each subject can be obtained from different atlases. To uncover the inherent distribution structure of data, we partition subjects in original classes into several sub-classes (i.e., clusters) by using a clustering algorithm. Then, we encode each of sub-classes with a unique coding vector, and regard these coding vectors as new class labels for corresponding subjects. Next, we adopt a multi-task feature selection method to select the most informative features in each atlas space. Based on these selected features, we then learn multiple SVM classifiers, with each SVM corresponding to a specific atlas space. Finally, we fuse these classifiers by an ensemble classification method. Experiments on the ADNI database demonstrate that our method outperforms several state-of-the-art methods in AD/MCI classification. Figure 1 illustrates the overview of our inherent structure-guided multi-atlas learning (ISML) method, which includes three main steps, i.e., 1) feature extraction, 2) inherent structure-guided sparse feature selection and 3) ensemble classification. Specifically, we first non-linearly register the brain images of those subjects onto multiple selected atlases, and then extract volumetric features from the gray matter (GM) tissue density map within each of multi-atlas spaces. Afterwards, we perform feature selection using the proposed inherent structureguided sparse feature selection method, where we cluster the original classes into several sub-classes and perform sparse feature selection using a multi-task feature selection method. With the selected features, we then learn a support vector machine (SVM) classifier in each of multi-atlas spaces, followed by an ensemble classification approach to combine those SVMs for making a final decision. In what follows, we will introduce each step in detail."}, {"section_title": "Proposed Method", "text": ""}, {"section_title": "Feature Extraction", "text": "In this study, we apply a standard image pre-processing procedure to the T1-weighted MR brain images for all studied subjects. Specifically, we first perform a non-parametric non-uniform bias correction [7] on the MR images to correct intensity in-homogeneity. Then, we perform a skull stripping [8] procedure, followed by a manual review or correction to ensure that the skull and the dura have been removed cleanly. Next, we remove the cerebellum by warping a labeled atlas to each skull-stripping image. Afterwards, we use the FAST method proposed in [9] to segment each brain image into three tissues, i.e., gray matter (GM), white matter (WM) and cerebrospinal fluid (CSF). Finally, all brain images are affine-aligned by the FLIRT method [10] .\nTo select appropriate atlases, we adopt a clustering method to select multiple atlases from all studied subjects. Specifically, we first partition the whole population of AD and NC brain images into K (K = 10 in this study) nonoverlapping clusters by using the Affinity Propagation (AP) clustering algorithm [11] , through which an exemplar image can be automatically determined for each cluster. We then regard these exemplar images as selected atlases (i.e., A 1 , \u00b7 \u00b7 \u00b7 , A K ), which is shown in Fig. 1 . By performing feature extraction as described in [12] in each atlas space, we can obtain D-dimensional (D = 1500 in this study) features from the GM tissue density map for each subject. Given K atlases, we now have K sets of feature representations for each subject."}, {"section_title": "Inherent Structure-Guided Sparse Feature Selection", "text": "To utilize the structure information of data, we first divide subjects in original classes into several sub-classes, and then encode these sub-classes with a popular one-versus-all (OVA) encoding strategy [13] , followed by a multi-task feature selection model to select the most informative features in each atlas space.\nSub-class Clustering and Encoding. For a specific class, we exploit the affinity propagation (AP) algorithm [11] to automatically partition the subjects within this class into several sub-classes (i.e., clusters) in each atlas space, where the cluster number can be determined by cross validation on training data. As shown in Fig. 2 , subjects belong to two original classes, i.e., Class 1 and Class 2.\nUsing the AP algorithm, we partition the subjects in Class 1 into two sub-classes, and divide subjects in Class 2 into three sub-classes (see Fig. 2 ). Then, we label all subjects with new class labels by encoding the original classes and those sub-classes using the OVA encoding strategy. For the original classes (see Fig. 2 ), each class is represented by a unique OVA coding vector, i.e., [ . Afterwards, we regard these 7-bit coding vectors as the new class labels for corresponding subjects. In this way, the original binary classification problem is transformed into a multi-task (e.g., 7 tasks in Fig. 2) learning problem, through which the structure information of the original classes can be incorporated into the learning process.\nMulti-task Feature Selection. As high-dimensional features could be redundant or noisy that has attracted research attention in many fields (e.g.,image retrieval [14] and classification [15] ), feature selection remains a popular research topic for dealing with high-dimensional features. Here, we exploit a multi-task feature selection model to perform feature selection in each atlas space. We denote matrices as boldface uppercase letters, vectors as boldface lowercase letters, and scalars as normal italic letters, respectively. Denote a i and a j as the i th row and the j th column of a matrix A, respectively. We further denote the Frobenius norm and the 2,1 norm of A as A F = i a i 2 and A 2,1 = i a i , respectively. Denote X \u2208 R N \u00d7D as the data matrix with N subjects of Ddimensional features. Let Y \u2208 R N \u00d7C denote the new class label matrix for N subjects, where each subject is labeled with a C-bit row vector. Here, C is the sum of the number of original classes and the number of sub-classes of all original classes. Denote\nD\u00d7C as the weight matrix for C learning tasks, where w c is the column weight vector for the c th learning task. To jointly select common features among different tasks, the multi-task feature selection (MTFS) model is defined as follows:\nwhere W 2,1 is a group sparsity regularization, and \u03bb is a parameter to trade off the balance between the empirical loss on the training data and the regularization term. Due to the group sparsity nature of 2,1 norm, the estimated optimal coefficient matrix W will have some zero-value row vectors, implying that the corresponding features are not useful in predicting the new class labels of training data. With the MTFS model defined in (1), we can select the most informative features in each atlas space, where the distribution structure of the original classes are used to guide the feature selection process."}, {"section_title": "Ensemble Classification", "text": "Then, we further propose an ensemble classification approach. To be specific, after feature selection in each atlas space, we can obtain K feature subsets using K atlases. Based on those selected features, we then learn K SVM classifiers, with each SVM corresponding to a specific atlas space. Finally, we combine these classifiers together by a majority voting strategy that is a simple and effective classifier fusion method. Given a new test sample, its class label can be determined by majority voting for the outputs of those SVMs."}, {"section_title": "Experiments", "text": ""}, {"section_title": "Data and Experimental Settings", "text": "To demonstrate the efficacy of our proposed method, we perform experiments on part of subjects in the ADNI database with T1-weighted MR images. There are totally 459 subjects randomly selected from those scanned with 1.5T scanners, including 97 AD, 117 progressive MCI (pMCI), 117 stable MCI (sMCI) and 128 normal controls (NC). In this study, we perform four groups of experiments, including 1) AD vs. NC classification, 2) pMCI vs. sMCI classification, 3) pMCI vs. NC classification, and 4) sMCI vs. NC classification. We employ a 10-fold cross-validation strategy to evaluate the performance of different methods. Specifically, all samples are partitioned into 10 subsets (each subset with a roughly equal size), and each time samples in one sub-set are successively selected as the test data, while those in the other nine subsets are used as the training data to perform feature selection and classifier construction. Finally, we report the average values of classification results among those 10 folds. We demonstrate the advantages of our proposed ISML method from two aspects. First, we compare ISML with three feature selection algorithms using the proposed ensemble classification strategy, including 1) Pearson Correlation (PC), 2) COMPARE proposed in [12] , and 3) LASSO. Second, we compare our ensemble classification method with conventional feature concatenation methods (i.e., PC con, COMPARE con, LASSO con, and ISML con). That is, we first concatenate those multi-view feature representations as a long vector, and then use a specific feature selection method (e.g., PC, COMPARE, LASSO, and ISML) to select the most informative features, followed by a SVM classifier. For fair comparison, all compared methods share the same multi-view feature representations generated from K atlases. The regularization parameter (i.e., \u03bb) in equation 1 and that in LASSO are chosen from the range {2 \u221210 , 2 \u22129 , \u00b7 \u00b7 \u00b7 , 2 0 } through inner cross-validation on the training data. There are K (K = 10 in this study) atlases selected from AD and NC subjects. Using these selected atlases, we extract D-dimensional (D = 1500 in this study) features from each of K atlas spaces. Here, we use the linear SVM with default parameters as the benchmark classifier. In addition, we evaluate the performance of different methods via three criteria, including classification accuracy (ACC), sensitivity (SEN), and specificity (SPE)."}, {"section_title": "Results and Discussion", "text": "We first record the averaged number of sub-classes (i.e., clusters) identified by our ISML method in the cross-validation process, i.e., 2.1/2.9 for AD/NC, 3.2/3.8 for pMCI/sMCI, 3.1/2.5 for pMCI/NC, and 3.2/2.3 for sMCI/NC. These results validate our intuition that there exists heterogeneity within a specific class.\nThen, we report the results achieved by different methods in four classification tasks in Table 1 , where the best results are shown in boldface. We further plot the ROC curves achieved by four ensemble-based methods in Fig. 3 . From Table 1 and Fig. 3 , one can observe two main points. First, in four classification tasks, our proposed ISML method generally outperforms the compared methods in terms of accuracy, sensitivity and specificity. For example, the accuracy achieved by ISML in AD vs. NC classification is 93.83%, which is much higher than the second best accuracy achieved by ISML con (i.e., 88.06%). Although the COMPARE method obtains the best specificity in sMCI vs. NC classification, its accuracy and sensitivity are much lower than those achieved by ISML. Second, methods using the ensemble classification strategy (i.e., PC, COM-PARE, LASSO, and ISML) generally outperform their conventional counterparts that adopt the feature concatenation strategy (i.e., PC con, COPARE con, LASSO con, and ISML con). Furthermore, we compare our ISML method with several state-of-the-art methods using MRI data of ADNI, with results given in Table 2 . Since very few works reports the results of sMCI vs. NC classification, we only show the results of three classification tasks (i.e., AD vs. NC, pMCI vs. sMCI, and pMCI vs. NC classification) in Table 2 . It is worth noting that the method proposed in [16] employs single-atlas feature representation extracted from one single atlas, while the others use multi-atlas based feature representations. It can be seen from Table 2 that, in three classification tasks, our ISML method yields the best classification results in terms of both accuracy and specificity, and comparable sensitivity with the work in [5] ."}, {"section_title": "Conclusion", "text": "In this paper, we propose an inherent structure-guided multi-view learning (ISML) method for AD/MCI classification. Specifically, we first extract multiview features for subjects, and then cluster subjects in original classes into several sub-classes, followed by a multi-task feature selection algorithm. Finally, we develop an ensemble classification method by fusing multiple classifiers constructed in multi-atlas spaces. Experimental results on the ADNI database demonstrate the efficacy of our ISML method."}]