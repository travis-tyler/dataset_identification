[{"section_title": "Abstract", "text": "Diffusion tensor imaging (DTI) is a popular magnetic resonance imaging technique used to characterize microstructural changes in the brain. DTI studies quantify the diffusion of water molecules in a voxel using an estimated 3 \u00d7 3 symmetric positive definite diffusion tensor matrix. Statistical analysis of DTI data is challenging because the data are positive definite matrices. Matrix-variate information is often summarized by a univariate quantity, such as the fractional anisotropy (FA), leading to a loss of information. Furthermore, DTI analyses often ignore the spatial association of neighboring voxels, which can lead to imprecise estimates. Although the spatial modeling literature is abundant, modeling spatially dependent positive definite matrices is challenging. To mitigate these issues, we propose a matrix-variate Bayesian semiparametric mixture model, where the positive definite matrices are distributed as a mixture of inverse Wishart distributions with the spatial dependence captured by a Markov model for the mixture component labels. Conjugacy and the double Metropolis-Hastings algorithm result in fast and elegant Bayesian computing. Our simulation study shows that the proposed method is more powerful than non-spatial methods. We also apply the proposed method to investigate the effect of cocaine use on brain structure. The contribution of our work is to provide a novel statistical inference tool for DTI analysis by extending spatial statistics to matrix-variate data."}, {"section_title": "Introduction", "text": "Measurement of signal attenuation from water diffusion, often considered one of the most important magnetic resonance contrast mechanisms (Alexander et al., 2007) , is usually achieved via diffusion tensor imaging (DTI) that maps and characterizes the 3-D diffusion of water molecules as a function of the spatial location (Basser et al., 1994) . The diffusion process in the brain reflects interactions with many obstacles, such as fibers, thereby revealing microscopic details about the underlying tissue architecture. Unlike ordinary images where scalars are summarized for each voxel, a distinguishing feature of DTI is each voxel is associated with a 3 \u00d7 3 symmetric positive definite matrices which can be interpreted as the covariance matrix of a 3-D Gaussian distribution modeling the local Brownian motion of the water molecules (Schwartzman et al., 2008) . These positive definite matrices are also called the diffusion tensors (DTs). One important clinical application of the DTI is to detect regions of local differences in the brain between two groups (i.e., normal versus disease), revealing anatomical structural differences (Lo et al., 2010) . For example, the motivating data for this paper comes from a clinical DTI study (Ma et al., 2017) , where the scientific objective is to detect regions of differences between cocaine users and non-cocaine users.\nStatistical analysis of DTI data is challenging due to the difficulty of modeling matrix-variate responses. One option is to project the DTs into fractional anisotropy (FA), a scalar describing the degrees of anisotropy of a diffusion process. However, some information is lost because different positive definite matrices may produce the same FA (Ennis and Kindlmann, 2006) . Matrix-variate methods potentially avoid information loss. There are relatively few matrix-variate methods available to analyze DTI data, and they can be broadly classified into the (inverse) Wishart matrix methods (Dryden et al., 2009 ) and the random ellipsoid models (Schwartzman et al., 2008; Lee and Schwartzman, 2017) . However, these voxel-level models ignore information from neighboring voxels that may have similar neuronal activity (see Figure 1 ), despite recommendations of incorporating this non-negligible spatial association to achieve efficient and valid inference (Spence et al., 2007) as well as studies revealing that the disease status at proximally-located/neighboring voxels can be similar (see Wu et al., 2013; Xue et al., 2018) . This motivates us to develop an improved spatial statistical model which (a) utilizes full matrix information, (b) captures spatial dependence, and (c) can be implemented via fast and elegant computing. The spatial neuroimaging toolbox for univariate responses is considerably rich: Woolrich et al. (2004b) proposed a fully Bayesian model for spatiotemporal imaging data; Kang et al. (2011) implemented spatial point processes for meta-analysis of imaging data; To select essential biological features, Musgrove et al. (2016) introduced spatial Bayesian variable selection for neuroimaging data; Recently, proposed spectral methods for ADNI data to provide computational benefits. All of these studies demonstrated an improvement in the precision of estimates by properly accounting for spatial dependence.\nIn this vein, the Potts model, a generalization of the Ising model in statistical mechanics, has also been successfully applied to imaging (Johnson et al., 2013; Li et al., 2018) . A desirable property of the Potts model is that it avoids smoothing over abrupt changes in the image intensity (Johnson et al., 2013) , and this makes it more attractive than available Gaussian kernel methods.\nTo this end, we assume the positive definite DTs follow a mixture of inverse Wishart distributions, with the mixture component labels modeled via a (spatial) Potts model, representing a discrete Markov random field. This semiparametric mixture specification refers to a class of flexible mixture distributions with a finite number of components (Lindsay and Lesperance, 1995) .\nBesides spatial modeling, another important topic in neuroimaging is detecting regions of differences between two groups. Previous attempts at identifying regions of differences between two groups were formulated through voxel-wise hypothesis testing (Schwartzman et al., 2008; Lee and Schwartzman, 2017 ). An alternative option is to construct multilevel hierarchical modeling accounting both subjective-level and group-level variation and use the group-level parameters for voxel-wise hypothesis testing (Woolrich et al., 2004a; Liu et al., 2014) . In this paper, we use the latter approach via extending the latent classic Potts model into a hierarchical two-way framework, allowing hypothesis testing via group-level parameters and inter-subject variability simultaneously.\nOur proposal is implemented using the Bayesian approach, accounting for the uncertainty of model parameters in all levels of the hierarchy. However, the Bayesian approach is often problematic in neuroimaging because of its heavy computational burden (Cohen et al., 2017) .\nAlthough the associated Markov chain Monte Carlo (MCMC) algorithm is mostly composed of computationally tractable Gibbs steps that can be paralleled, a major drawback of the Potts model is the intractable normalizing constant, creating a bottleneck for hyperparameter updates. In this paper, it is resolved via the double Metropolis-Hastings algorithm (Liang, 2010) recommended by Park and Haran (2018) .\nTo the best of our knowledge, this is the first work on exploring spatial associations in modeling positive definite matrix-variate data under a Bayesian semiparametric framework, with applications to DTI. In the rest of the paper, we first introduce the single-subject and multi-subject model, and the group hypothesis testing framework in Section 2. Relevant MCMC computational details appear in Section 3. To demonstrate the improvement in performance compared to plausible alternatives, we perform simulation studies in Section 4. In Section 5, we present the application to the motivating cocaine data set. Finally, Section 6 concludes with a discussion."}, {"section_title": "Model", "text": "In this section, we introduce the spatial Bayesian semiparametric mixture model for positive definite matrices. We introduce the single-subject model first in Section 2.1 and then extend to the multi-subject model in Section 2.2."}, {"section_title": "Single-Subject Model", "text": "Let A v be the p \u00d7 p DT at voxel v \u2208 {1, 2, .., n}. To ensure A v is symmetric and positive definite, it is usually parameterized as a (inverse) Wishart matrix (Dryden et al., 2009) or a Gaussian symmetric matrix-variate distribution (Schwartzman et al., 2008) . In this paper, we assume that A v follows an inverse Wishart distribution as\nwhere \nparameterized (Appendix A) to have mean \u03a3 and degrees of freedom \u03bd > p.\nSpatial dependence of the DTs is achieved through the dependence of the mean matrices M v .\nWe induce spatial dependence via the latent cluster labels that follow a weighted Potts model, specified via the full conditional distributions:\nwhere g \u2212v is the full set g = {g 1 , g 2 , ..., g n } excluding g v , N v is a set of indices of the neighboring voxels of v, and I[A] = 1 if event A is true and I[A] = 0 otherwise. Given g \u2212v but marginal over g v , the distribution of A v is the mixture of K inverse Wishart distributions:\nwhere IW p (A|V , m) is the inverse Wishart density function of A with the mean matrix V and the degrees of freedom m. Therefore, this semiparametric mixture model spans a rich class of density functions.\nVia the Potts model, an image can be considered as a network whose nodes are the voxels. In this network, every voxel is connected to its neighboring voxels. The full conditional distribution of g v depends only on the voxels in the neighboring set N v and therefore the process is Markovian.\nSince the spatial parameter \u03b2 is the coefficient of the neighboring term u\u2208Nv I(g u = k), the spatial parameter \u03b2 controls the dependence on the neighboring voxels.\nUnlike the classic Potts model (Wu, 1982) , the terms \u03b7 k are added as offset terms controlling the overall mass on each cluster. We set \u03b7 k = \u2212k \u03be so the parameter \u03be > 0 is the concentration parameter controlling the homogeneity of the latent cluster labels. It is problematic to pre-specify the number of components K in a mixture model (McCullagh et al., 2008) but the offset terms provide more weight on the key components and fewer weights on the trivial components. We fit the model by setting K to be an upper bound on the number of active clusters and allow the data to determine the number of active clusters via estimation of \u03be: if \u03be \u2192 0, there are several active clusters; if \u03be is large, there are a few active clusters. As a result, the model is less sensitive to the number of components K when the offset term \u03b7 k is included. This claim is verified in the simulation studies (Section 4) and the real data application (Section 5), where we find similar results for different K.\nQuantifying spatial dependence is a vital issue in spatial statistics and neuroimaging. Since this model is for matrix-variate data, we use the expected squared Frobenius norm to measure dependence. The dependence between matrices A and B can be summarized as\nThe norm increases as dependence decreases. If A and B are 1 \u00d7 1, the expected squared Frobenius norm is the classic variogram (Cressie, 1992) of spatial statistics.\nIn this regard, the expected squared Frobenius norm can be treated as the variogram for matrixvariate data and useful in measuring spatial dependence. In the rest of the paper, we simply call the expected squared Frobenius norm the variogram. For the Potts model described above, the variogram is\nwhere P (u, v|\u03b2, \u03be) is the marginal (over all other cluster labels g) probability of g u = g v , and\n\u03b3(m, \u03bd, \u03a3) is a measure of the variability in A v |M v and variability of V k across K. Therefore, the multivariate spatial dependence structure is separable (Cressie, 1992) in that the dependence is the product of a non-spatial term \u03b3(m, \u03bd, \u03a3) that controls cross dependence and a spatial term P (u, v|\u03b2, \u03be) that controls spatial dependence.\nWe give the expression of \u03b3(m, \u03bd, \u03a3) in Appendix B. When p = 3 and \u03a3 = I, the non-spatial term has an expression that is\nwhere m > 6 and \u03bd > 3. Therefore, in this special case, the cross dependence decreases if m or \u03bd is larger (See Figure 2 ). The spatial term P (u, v|\u03b2, \u03be) is intractable and so we use a Monte Carlo approximation to study the function. In Figure 3a , the function is computed under the scenario that the image is a 1-D grid with K = 100 and \u03be = 0. The spatial term P (u, v|\u03b2, \u03be) increases with distance and larger spatial parameter \u03b2 leads to the stronger spatial dependence. We also compute the function value with different K in Figure 3b . We have lim |u\u2212v|\u2192\u221e P (u, v|\u03b2, \u03be) = 1 \u2212 1 K\n, where relevant result can be found in studies of extreme value analysis (see Reich and Shaby, 2018) . Increasing K leads to smaller spatial dependence. Hence, we fix K to be large to eliminate long-range dependence (i.e., P (u, v|\u03b2, \u03be) < 1 for large |u \u2212 v|) and estimate \u03b2 to capture local dependence.\nFor a more intuitive understanding of this model, we also simulate the DTs and visualize the DTs as ellipsoids in a 40 \u00d7 40 grid. In these simulations, we use \u03be = 0, m = 4, and \u03bd = 30. In Figure 4a , the DTs within the same latent cluster label are similar to each other, indicating that spatial dependence of the DTs can be achieved by the latent cluster labels following the Potts model. In Figure 4b , larger spatial parameter \u03b2 leads to a realization with more dependence on their neighbors. "}, {"section_title": "Muti-Subject Model", "text": "Motivated by the cocaine users data set (Ma et al., 2017 ) that includes 11 cocaine users and 11\nnon-cocaine users, we extend the single-subject model to the multi-subject setting.\nThe clinical objective is to analyze the brain's physical structure for differences between the two groups. The objective can be statistically formulated as finding regions in the brain where the distribution of the DTs across the subjects is different between cocaine users and non-cocaine users.\nLet A iv be the DT and g iv be the cluster label for voxel v \u2208 {1, 2, ..., n} and subject i \u2208 {1, 2, ..., N }. By extending g v to g iv , the subject-level cluster labels not only model intra-subject spatial dependence but also allow inter-subject variability. As in the single-subject model, the DTs are conditionally independent given the random matrices M iv following a finite mixture model:\nHowever, the latent Potts model is generalized to account for multiple subjects. We define x i as the binary group indicator of subject i. In the motivating data, cocaine users have x i = 1 and non-cocaine users have x i = 0. To model intra-subject spatial dependence within a group, we extend the latent cluster process by introducing the group-level cluster labels h xv for group\nx \u2208 {0, 1} and voxel v \u2208 {1, 2, ..., n}. Both h xv and g iv are also spatially dependent with full conditional distributions\nwhere g \u2212(iv) is the set on g i = {g i1 , ..., g in } excluding g iv , h \u2212(xv) is the set h x = {h x1 , ..., h xn } excluding h xv , g is the set on {g 1 , ..., g N }, and h is the set on {h 0 , h 1 }. The joint probability mass function (PMF) of {g 1 , g 2 , ..., g N } \u222a {h 0 , h 1 } is given in Appendix C. Since the conditional densities in (4) satisfy the conditions of the Hammersley-Clifford Theorem (Clifford, 1990) , the\nA graphical representation of this latent Potts model is provided in Figure 5 . Cluster labels g i and h x can be understood as the spatial pattern of subject i and the general spatial pattern of subjects in group x, respectively. In comparison to the single-subject model, the group-clustering parameter \u03b1 is introduced for modeling multiple subjects. If \u03b1 = 0, A iv is independently distributed over subjects; otherwise, the subject-level cluster label g iv depends on the group-level cluster label h x i v , leading to the smaller inter-subject variability of spatial dependence pattern within one group. are h x = {h x1 , ..., h xn } and the subject-level cluster labels are g i = {g i1 , ..., g in }. Cluster labels h x and g i are mutually dependent. The subject-level cluster labels g i have inter-subject variability.\nThe group-level cluster labels h x are a summary of the spatial dependence of all subjects.\nTo further understand the role of \u03b1 and h xv , we inspect the density of A iv conditioned on h xv and marginal over all other labels (Appendix D). The conditional density of A iv given x i = x and h xv is the mixture of inverse Wishart distributions proportional to\nwhere the term exp \u2212k \u03be + \u03b1I(h xv = k) is the proportional weight of cluster k. Therefore, h xv = k elevates the mass on mixture component k at voxel v for all subjects with x i = x. Assuming \u03b1 > 0, the conditional density (5) depends on x i if and only if h 0v = h 1v .\nThe clinical objective is to find regions of differences between two groups, which can be formulated into finding voxels for which the distribution of A iv is different for x i = 0 or x i = 1. As shown in the conditional density (5), the test can be simplified to the test that\nBayesian inference provides estimates of the posterior probabilities of the hypotheses, which is further discussed in Section 3.\nWe investigate spatial dependence as in the single-subject model. To measure spatial dependence within and across subjects, we propose the variogram\nwith i = j for individual variogram and i = j for inter-subject variogram, respectively. For intersubject variogram, we also compare the within-group variogram for subjects with x i = x j and between-group variogram for subjects with x i = x j . Both individual variogram and inter-subject variogram are also separable (Cressie, 1992) . \u03b3(m, \u03bd, \u03a3) is the non-spatial term which has been discussed in Section 2.1. The spatial term P ij (u, v|\u03b1, \u03b2, \u03be) is the marginal probability of g iu = g jv .\nIn Figure 6a , the function P ii (u, v|\u03b1, \u03b2, \u03be) is computed using a Monte Carlo approximation under the scenario that the image is a 1-D grid with N = 5 and K = 100. The spatial parameter \u03b2 largely controls the within-subject dependence. In Figure 6b plotting P ij (u, v|\u03b1, \u03b2, \u03be), larger \u03b1 leads to more dependence in the within-group variogram. Therefore, \u03b1 controls the dependence of subjects within one group. Since g iu and g ju are assumed to be independent, the spatial term (Figure 6c ). "}, {"section_title": "Computation", "text": "We use MCMC to fit the model described in Section 2 (Appendix E). The codes are written in hybrid R and C++ codes. The final model is\nwhich is referred to as the Potts Model in the rest of the paper. Using the moment method (Robert, 2007) The MCMC algorithm is a combination of Gibbs and Metropolis-Hastings steps. The latent mean matrices and cluster labels are updated via Gibbs steps. Their full conditional distributions\nwhere n k = i,v I(g iv = k). In addition, P (g iv = k|.) and P (h xv = k|.) can be updated in parallel over i and x, respectively. Since the uniform prior is not conjugate, we have to sample [m|.] and [\u03bd|.] via Metropolis-Hastings sampling with log-normal random walk as proposal distribution.\nTo select regions of differences via Bayesian hypothesis testing, we reject the null hypothesis in (6) if P (h 0v = h 1v |.) < P (h 0v = h 1v |.). The posterior probabilities can be estimated through MCMC samples that h 0v = h 1v or h 0v = h 1v .\nUpdating the Potts hyperparameters \u03b1, \u03b2, and \u03be is problematic because the normalizing constant in the joint distribution function of the cluster labels is intractable (see the joint PMF in \u03b8 drawn from q(\u03b8| \u03b8) where \u03b8 is the current value and q(\u03b8| \u03b8) is a log-normal random walk transitional probability centered at \u03b8. Given the candidate \u03b8 , we draw labels g i = {g iv , ..., g in } and h x = {h xv , ..., h xv } using Gibbs sampling for each i and x, respectively. The candidate \u03b8 is accepted with the probability min(1, r) where r =\nwhere P(g, h|\u03b8) is the likelihood of {g 1 , g 2 , ..., g N } \u222a {h 0 , h 1 } conditioned on \u03b8. g i and h x are current values. Due to the concern that the double Metropolis-Hastings algorithm is not an exact sampling (Liang, 2010; Park and Haran, 2018) , the MCMC convergence of \u03b8 in the simulation studies (Section 4) and the real data analysis (Section 5) are monitored by Heidelberger and Welch's convergence diagnostic (Heidelberger and Welch, 1981) ."}, {"section_title": "Simulation", "text": "In this section, we illustrate the performance of our method using two simulation studies under different scenarios for synthetic data. We compare our method to the non-spatial DTI inference method the Gaussian symmetric matrix model (Schwartzman et al., 2008) referred to as the Random Ellipsoid Model. The Random Ellipsoid Model is a non-spatial matrix-variate method and assumes that A iv follows a Gaussian symmetric random matrix distribution with the probability density function (PDF) as\n, where \u03a3 xv is the DT's population mean at voxel v of group x and \u03c3 2 is the nuisance parameter.\nThe Random Ellipsoid Model selects regions of differences via testing \u03a3 0v = \u03a3 1v , where the test statistics are constructed by maximum likelihood estimations. the Potts Model has 8, 000 MCMC samples with 3, 000 discarded as burn-in. Methods are evaluated in terms of true positive rate (TPR), false positive rate (FPR), false discovery rate (FDR), and typical computation time.\nWe first investigate the performance of our method when the data are generated from a mixture model. We use a 40 \u00d7 40 grid with spacing 1 between adjacent grid points as an image. Each simulated data set consists of 5 subjects in the control group (x i = 0) and 5 subjects in the treatment group (x i = 1). For the control group (x i = 0), we equally partition the graph into 4 parts by rectangular regions so that g iv \u2208 {1, 2, 3, 4}, ordered by right-to-left. Thus each region is a 40 \u00d7 10 region. The treatment group has the same partition as the control group, except a 10 \u00d7 10 region at the middle of the second region where g iv = 5. This simulates the brain with a small region of difference between the two groups. For each simulation, \u03a3 k is generated based on the model\nGiven the simulated \u03a3 k , the data are generated based on the model A iv |\u03a3 g iv \u223c IW 3 (\u03a3 g iv , 5).\nOur model with K = 10, 50, 100 is compared to the Random Ellipsoid Model. The results averaged over 50 data sets are summarized in Table 1a . Our model has significantly improved performance in terms of the TPR, FPR, FDR in comparison to the Random Ellipsoid Model.\nThe small number of subjects might be one of the causes that the alternative produces low TPR. Schwartzman et al. (2008) discusses that the accuracy of maximum likelihood estimates of the Random Ellipsoid Model is dependent on the number of subjects. Since the choice of K does not affect selection accuracy, the simulation results also support the claim that the model can be less sensitive to the number of clusters if K is larger than the true clusters.\nTo determine robustness to model misspecification, we also simulate data from the spatial Cholesky process described as follows: The DT matrix for subject i at voxel v is determined by six independent spatial Gaussian processes U ivk (k \u2208 {1, 2, ..., 6}). These spatial Gaussian processes are arranged in the lower triangular matrix L iv with\n, thereby introducing spatial dependence and guaranteeing positive definite of responses. We again use a 40 \u00d7 40 grid with spacing 1 between adjacent grid points as an image. There are 10 subjects in the control group and 10 subjects in the treatment group. The six spatial Gaussian processes are simulated with variance \u03c4 2 = 0.1 and exponential correlation function with range parameter \u03c1 = 2. The mean of the six Gaussian processes are all 0 except for treatment subjects' 10 \u00d7 10 region in the center of the image where U ivk has mean 0.5 for k \u2264 3 and 0.25 for k > 3. This simulates the brain with a small region of difference between the two groups. We compare our model with K = 10, 50, 100 to the Random Ellipsoid Model. The results averaged over 50 simulations are summarized in Table 1b . The results demonstrate that our model maintains good performance, indicating the Potts Model is robust to this form of misspecification. In addition, under the spatial dependence assumption, the spatial models produce an overall better performance than the non-spatial model.\nA problematic issue to the use of Bayesian methods in neuroimaging data is their heavy computational burden. In both simulations, the Potts Model has a computational speed within a few hours. The Random Ellipsoid Model avoids the expensive MCMC, but since the performance of the Random Ellipsoid Model is too conservative, the Potts Model is a reasonable trade-off. "}, {"section_title": "Real Data Application", "text": "We apply this model to the data set of cocaine users (Ma et al., 2017) described in Section 2.\nThe data are provided by the Institute for Drug and Alcohol Studies of Virginia Commonwealth University (VCU). The study recruited 11 cocaine users and 11 controls to test for microstructural changes of the brain through DTI. In the data analysis, we focus on the corpus callosum containing 15,273 voxels because this region plays important roles such as transferring motor, sensory, and cognitive information between the brain hemispheres (Ma et al., 2009) . Conventionally, studies on cocaine use focus on this region (i.e., Ma et al., 2009 Ma et al., , 2017 Lane et al., 2010) .\nBefore model fitting, we examine the fit of the proposed model to the cocaine users data via empirical estimates of variograms. We denoteV\nF as the empirical variogram value of subjects i and j at distance d, where N d is the number of pairs with |u \u2212 v|= d. We plot these empirical variograms of the motivating data in Figure 7 . The DTs have a strong within-subject spatial dependence (Figure 9a ). The empirical within-group variogram (Figure 9b ) also increases with distance, indicating inter-subject dependence within a group, however, the empirical variogram is almost flat in the between-group variogram ( Figure   9c ), which suggests that the subjects are independent if they are in different groups. Since these empirical variograms perfectly match the theoretical variograms in Figure 6 , the the hierarchical Potts model assumptions about spatial dependence are reasonable. The empirical variograms of the cocaine users data (Ma et al., 2017) . The lines are the empirical variograms for each subject/pair.\nWe sample 11, 000 MCMC samples with 3, 000 discarded as burn-in and it typically takes 5 hours using a CPU with 3.4 GHz Intel Core i5. To study the sensitivity to K, we fit the model with K as 100, 200, 300, 400 and 500 and use the Rand index (Rand, 1971) for measuring the similarities of regions of differences detection with different K. The Rand index measures clustering similarity: if the two clusterings are almost identical, the index is close 1; otherwise, the index is close 0. In Table 2 , the Rand indices of any two K are close to 1, hence the selection is not sensitive to K. For a concise illustration, we use the result of K = 100 in the rest of this section. We first use the R package brainR (Muschelli et al., 2014) for 3-D visualizing the regions of differences. To investigate if the performance is improved by introducing spatial dependence,\nwe also compare it to the Random Ellipsoid Model. We give the confidence level 0.9 for the Random Ellipsoid Model. The selected regions of differences are displayed in Figure 8 . The Rand index of the two clusterings is 0.86 and so the results of the two analyses are similar but the Potts Model finds more spatial contiguous regions. Our study shows that a region of difference is detected in the splenium, which is consistent with previous clinical studies on cocaine use (see Lane et al., 2010) . The splenium is a component located on the posterior end of the corpus callosum with an essential role on cognition. Since many studies revealed that the disease status at proximally-located/neighboring voxels can be similar (see Wu et al., 2013; Xue et al., 2018) , our method may be more clinically meaningful than other studies because the detected regions are spatially contiguous. Furthermore, our test is able to find more regions of differences compared to alternative clinical studies on investigating the effect of cocaine use. For example, Ma et al.\n(2017) did not find such regions of differences by using the same data set. identical to the information obtained from the empirical variograms ( Figure 7) . Thus similar to the usage of the classic variogram, the generalized empirical variograms may also be a tool for obtaining the plug-in values of hyperparameters as an alternative (see Reich and Shaby, 2018) ."}, {"section_title": "Discussion", "text": "Although the spatial statistics literature on models and tools for matrix-variate data is sparse, the usage of positive definite matrix-variate is broad, which includes multiple-input and multipleoutput (MIMO) systems (Smith and Garth, 2007) and computer vision (Cherian et al., 2016) .\nOur major contribution is to present a spatial statistics formulation to model matrix-variate data via a Bayesian semiparametric mixture model. Our formulation retains the original data structure and accounts for spatial dependence by a computationally elegant model. In simulation studies, our model produces significantly improved performance compared to the non-spatial alternatives.\nThe application to the DTI data set of cocaine users demonstrates the novelty of this model for detecting clinically meaningful regions of differences. = \u03b3(m, \u03bd, \u03a3)P (g u = g v |\u03b2)\nObviously, E||A iu \u2212A iv || 2 F can be derived in the same way. Next, we give the the explicit expression of \u03b3(m, \u03bd, \u03a3). We first have\nwhere \u03c3 i (.) returns the i-th eigenvalue of the input function.\nThe term T r(EA u A u ) is complex. Gupta and Nagar (1999) Proof. See Gupta and Nagar (1999) [Section 3.3 .6] Then we first can obtain\nNext, we have Then we can obtain T r(EA u A u ) = T r(E\u03bb * ) = {(c 1 + c 2 )(\u03bd + 1)\u03bd\nIn summary, \u03b3(m, \u03bd, \u03a3) = 2(\u03c3 \u2212 \u03bb).\nAppendix C: The Joint Probability Mass Density (PMF) of {g 1 , g 2 , ..., g N } \u222a {h 0 , h 1 }\nIn this section, we show the expression of the PMF and validate that the PMF is valid:\n\u03b1I(g iv = h x i v ) + \nwhere u \u223c v means u and v are connected. It is obvious that the probability is positive and satisfies the pairwise Markov property stated in the Hammersley and Clifford Theorem. The normalizing constant Z(\u03b1, \u03b2, \u03be) = g,h exp(U (g, h, \u03b8)) is intractable. Since the summation is over finite and discrete indices, we have that 0 < Z(\u03b1, \u03b2, \u03be) < \u221e, revealing that P (h, g) is proper."}, {"section_title": "Appendix D: The Statistical Role of h xv", "text": "Step 1 \nStep 2 "}]