[{"section_title": "Executive Summary", "text": "Introduction The 1996 Beginning Postsecondary Students Longitudinal Study (BPS:96), sponsored by the National Center for Education Statistics (NCES) in the U.S. Department of Education, follows a cohort of students who started their postsecondary education during the 1995-96 academic year. These students were first interviewed during 1996 as part of the 1995-96 National Postsecondary Student Aid Study (NPSAS:96). In 1998, 2 academic years after the cohort's entry into postsecondary education, the first follow-up interview (BPS:96/98) was conducted. BPS:1996BPS: /2001 is the second and final follow-up interview with the BPS:96 cohort. This interview, which took place in 2001, focused on persistence and attainment among students enrolled in 4-year institutions and employment among students no longer enrolled. This report describes the procedures and results of the full-scale implementation of BPS:1996BPS: /2001 "}, {"section_title": "Sample Design", "text": "The respondent universe for the BPS:96/98 and BPS:1996/2001 interviews consisted of all students who began their postsecondary education for the first time during the 1995-96 academic year at any postsecondary institution in the United States or Puerto Rico. The students sampled were first-time beginning postsecondary students who attended postsecondary institutions eligible for inclusion in NPSAS:96 and who were themselves eligible for NPSAS:96. All BPS:1996/2001 sample members had completed either the NPSAS:96 interview, the BPS:96/98 interview, or both interviews. At the beginning of BPS:96/98, over 12,400 students had been identified as potentially both eligible for NPSAS:96 and first-time beginners (i.e., eligible for the BPS interviews). Of those students, about 10,350 were located and completed a BPS:96/98 interview, with almost 10,300 of them determined to be both NPSAS and BPS eligible. The majority of the BPS:1996/2001 sample consisted of these BPS:96/98 respondents. However, the BPS:96/98 respondents were supplemented by a subsample of about 100 BPS:96/98 nonrespondents. The BPS:1996/2001 sample was representative of the students who first began postsecondary education in 1995-96. last interview. The first section of the instrument collected information on postsecondary enrollment and degree attainment. A second section collected information on undergraduate education experiences. A third section, on postbaccalaureate education experiences, was included for those sample members who had completed a bachelor's degree since the last interview. A fourth section collected extensive employment information for the current job if no degree had been earned since the last interview. For those who had earned a degree, employment information was collected for the current job and for the first job held after degree completion, if different. The final section updated the sample members' family, financial, and disability status and their civic participation since the last interview."}, {"section_title": "Data Collection Design and Outcomes", "text": "Interviews were conducted using computer-assisted telephone interviewing (CATI). Cases for sample members for whom no locating information was available were sent directly to a specialized tracing unit for intensive tracing. The tracing unit was also used for intensive tracing once all contact information for sample members was exhausted during attempts to conduct the telephone interview. In addition to telephone interviewing and intensive tracing, field locating and interviewing were available for certain cases that fell into any one of 30 geographic clusters developed according to the zip code of the last known address for the sample member. Potential field cases were those in which CATI and intensive tracing failed to locate sample members or in which sample members initially refused to participate in the interview. Computer-assisted personal interviewing (CAPI) software was available on laptop computers for field interviewing."}, {"section_title": "Training", "text": "Training programs on successful locating and interviewing were developed for telephone and field staff. Topics covered administrative procedures required for case management; quality control; locating; interactions with sample members, parents, and other contacts; the nature of the data to be collected; and the organization and operation of the CATI and CAPI programs used for data collection. Tracing specialists received an abbreviated training specific to the needs of BPS:1996BPS: /2001 Interviewing CATI locating and interviewing began at the end of February 2001. Contact information for the BPS:96/98 respondents was loaded into CATI initially, followed by contact information for the BPS:96/98 nonrespondents several weeks after the start of CATI. Field interviewing began about 12 weeks following the start of telephone interviewing. Of the original starting sample, 21 sample members were found to be deceased since the last interview. The unweighted contact rate among the remaining BPS:1996/2001 sample members was 92 percent. Of those contacted, 96 percent were interviewed for an overall unweighted response rate of 88 percent. iv"}, {"section_title": "Refusal Conversion", "text": "Important to successful interviewing was the ability of the interviewers to gain the cooperation of sample members, thereby avoiding a refusal. The telephone interviewers included refusal conversion specialists with special training in attempting to convert (interview) sample members who have refused to complete the interview. From the point when a sample member refused, the case was handled only by these conversion specialists. In BPS:1996BPS: /2001 sample members refused at least once to participate in the interview. Of those, 74 percent were converted and interviewed.\nRefusal conversion procedures were used to gain cooperation from individuals who, over the course of data collection, refused to participate when contacted by telephone interviewers. Eighteen percent refused to be interviewed at some point during data collection and 74 percent of these refusals were successfully converted into completed interviews. The refusal rate and success of converting refusals varied according to the sample member's response status on the previous interviews and type of school, as shown in tables 3.6 and 3.7, respectively. As expected, initial refusal rates were lower (t = 4.7; p <.001) and refusal conversion rates higher (t = 4.7; p <.001) for those who had participated in both the NPSAS:96 and BPS:96/98 interviews.  Study: 1996(BPS:1996.  : 1996: (BPS:1996. 3."}, {"section_title": "Field Interviewing", "text": "Field interviewers were assigned a total of 1,213 cases, covering 30 geographic clusters. Cases were identified for the field for a number of reasons, including inability to locate in CATI, Puerto Rico residence, refusal in CATI, and exhaustion of locating leads. Only cases located in reasonable geographic proximity to a field interviewer were assigned to the field. Of the 1,213 cases fielded, 80 percent were contacted, and 90 percent of those were interviewed, for an unweighted response rate of 72 percent.\nField interviewing activities began upon completion of interviewer training and assignment of field cases, approximately 12 weeks after the start of CATI interviewing. CAPI procedures included attempts to locate, gain cooperation from, and interview study sample members either by telephone or in person. The goal of the field interviewing effort was to increase the response rate by locating hard to reach sample members and by persuading reluctant sample members to complete the interview. Field interviewers were often successful in gaining cooperation where CATI failed to do so for a number of reasons: (1) a sample member using Caller ID to screen out calls from our CATI call center may have been more inclined to answer the phone when the field interviewer's local telephone number was displayed, (2) many of the field interviewers were more experienced in refusal conversion, and (3) sample members were less likely to refuse in person. All sample members who were finalized in CATI and by TOPS as \"unlocatable\" were eligible for assignment to the field for CAPI interviewing. Sample members who had not completed the BPS:1996/2001 interview at the time field interviewing began and who resided in an identified geographic cluster were immediately assigned to a field interviewer. Field interviewers were provided with a detailed case history documenting all prior activity taken for the case. Nonrespondent cases not in a geographic cluster were sent for additional intensive tracing with RTI's TOPS unit. An additional mailing was sent to the best address identified for the sample member, and the case incentivized as \"hard to reach.\" Upon successfully locating sample members, field interviewers attempted to complete the interview using the same instrument used by telephone survey personnel. The field staff were supported by a computerized control system that tracked field assignments and assigned interview status codes. Daily reports tracked the field effort.\nCases were selected for field interviewing if they could not be located in CATI or had been extensively worked in CATI but the subject could not be reached (e.g., calls always reached an answering machine). Only cases located in close geographic proximity to one of the 30 geographic field clusters selected for BPS:1996/2001 were eligible for field interviewing. A total of 11.7 percent of cases were assigned to field interviewers. As shown in table 3.8, 80 percent of cases sent to the field was contacted, either in CATI or in the field, and 90 percent of those contacted was interviewed. Field interviewing rates by sector of the NPSAS:96 institution are presented in table 3.8."}, {"section_title": "Nonresponse Incentive", "text": "Incentives were offered as necessary to targeted sample members in order to encourage participation among sample members who would otherwise not have participated in the interview. Those offered incentives included the BPS:96/98 nonrespondents, a subset of refusal cases, and those who were hard to reach or could not be located. By the end of data collection, 4,106 sample members had been offered incentives and, of those, 72 percent were converted.\nAs discussed in chapter 2, incentives were offered to targeted sample members in order to encourage participation and help to compensate them for the time required to complete the interview, thereby reducing the number of nonrespondents. Response rates by nonrespondent type are provided in table 3.9 and by sector of the NPSAS:96 institution in table 3.10. The first group of potential nonrespondents to BPS:1996/2001 included refusals that had not been converted, hard-to-reach sample members, and unlocatable sample members. All nonrespondents from BPS:96/98 were offered an incentive because they already had not responded (either refused or could not be located) in the prior interview. Interviews were completed with 72 percent of the incentivized cases.  : 1996: (BPS:1996.  : 1996: (BPS:1996. C."}, {"section_title": "Indeterminate Responses", "text": "Efforts were made to encourage response to all items in the BPS:1996/2001 interview and to convert indeterminate responses (i.e., \"don't know\" and \"refusal\" responses), especially for those items that historically have had high nonresponse (e.g., income). As a result, item nonresponse was quite low throughout the interview. Only 9 of the 445 CATI items had indeterminate response rates in excess of 10 percent.\nEvery item in the CATI/CAPI interview accommodated indeterminate responsesthat is, \"don't know\" and \"refusal\" responsesfrom sample members, recorded using the computer function keys F3 and F4. In general, refusal responses to interview questions tend to be common for items considered sensitive by the respondent, such as income and credit card debt in the BPS:1996/2001 interview, while \"don't know\" responses may be provided for a number of reasons, the most obvious reason being that the answer is truly unknown or in some way inappropriate for the respondent. Don't know responses may also be evoked when (1) question wording is not understood by the respondent, (2) there is hesitancy on the part of the respondent to provide \"best guess\" responses, and (3) the respondent implicitly refuses to answer a question. Refusal and don't know responses introduce indeterminacies in the data set. While the preference is to avoid indeterminate responses entirely, they must be resolved by imputation or other means during analysis following data collection. Overall item nonresponse rates were low, with only 9 of the 445 items in the interview containing over 10 percent missing data. These items are shown in table 4.1. Item nonresponse rates are calculated based on the number of sample members for whom the item was applicable and asked.  : 1996: (BPS:1996."}, {"section_title": "Interview Timing", "text": "The average administration time for the BPS:1996/2001 interview was 17.8 minutes, over 2 minutes shorter than the first follow-up interview (BPS:96/98). In the 2001 interview, BPS:96/98 nonrespondents took an average of 3.6 minutes longer than BPS:96/98 respondents. This is because the 2001 interview updated enrollment and employment information since the last interview (in 1996 for BPS:96/98 nonrespondents and in 1998 for BPS:96/98 respondents). Particular thanks are extended as well to the study Technical Review Panel members who provided considerable insight and guidance in the development of the design and instrumentation of this study. We also extend our thanks to the project staff members of the two contractors, Research Triangle Institute (RTI) and MPR Associates. A number of staff from these organizationsincluding statisticians, analysts, survey managers, programmers, data collectors, and interviewerstoo numerous to name here, worked long hours on this study. At RTI, we are especially indebted to Lynne Kline, who produced the drafts and final versions of this report. Most of all, we are greatly indebted to the many postsecondary education institutions, students, former students, and their parents, relatives, and friends, who unselfishly gave of their time to provide study data and/or locating information. ix    Tables   Table 1.1 Operational schedule for BPS:1996BPS: /2001 Tables (continued)   Table 3.12 Average elapsed minutes to complete the BPS:1996/2001 interview, by enrollment since previous interview and by section Table 3.13 Average elapsed minutes to complete the BPS:1996/2001 interview, by section and by NPSAS:96 institutional sector Table 3.14 Number of calls made to sample members, by response status Table 3.15 Number of calls made to sample members, by NPSAS:96 institutional sector Table 4.1 Indeterminate response rates for items with more than 10 percent \"don't know\" or \"refused\" Table 4.2   Table 4.3 Table 5.1"}, {"section_title": "Success of online coding procedures: upcoding 41", "text": "Success of online coding procedures: recoding 42 Description of missing data codes 49 Table 6.1 Average weight adjustment factors from the logistic model used to adjust for student location nonresponse Table 6.2 Average weight adjustment factors from the logistic model used to adjust for student refusal nonresponse 61 Table 6.3 Average weight adjustment factors from the logistic model used to adjust for nonresponse other than refusal Table 6.4 Average weight adjustment factors from the logistic model used to adjust for nonresponse to either NPSAS:96 or BPS:96/98, among the respondents to BPS:1996/2001    Tables (continued)   Table 6.9 Comparison of BPS:1996/2001 respondents and nonrespondents Table 6.10 Comparison of BPS:1996/2001 converted refusals and other respondents Table 6.11 Comparison of BPS:1996/2001 late respondents and early respondents Table 6.12 Nonresponse bias before and after weight adjustment for selected variables Table 6.13 Overall BPS:1996/2001 study response rates by type of institution Table 6.14 BPS:1996/2001 response rates by prior response status Table 6.15 Overall response rates for BPS:1996/2001 by type of institution and prior response status Table 6.16 Weighted item nonresponse for items with more than 10 percent nonresponse Table 6.17 Comparison of item respondents and nonrespondents for \"cumulative undergraduate GPA\" Table 6.18 Comparison of item respondents and nonrespondents for \"Lifetime Learning tax credit 1999 (undergraduate)\" 121 Table 6.19 Comparison of item respondents and nonrespondents for \"gross annual salary for current job\" Table 6.20 Comparison of item respondents and nonrespondents for \"gross annual salary or first postenrollment job\" 123 Table 6.21 Comparison of item respondents and nonrespondents for \"gross salary for 2000\" Table 6.22 Comparison of item respondents and nonrespondents for \"spouse's gross salary for 2000\" Table 6.23 Comparison of item respondents and nonrespondents for \"total balance on all credit cards\"       Cumulative percentage of all students who attained a degree by June 2001 Cumulative percentage of all students who were employed: 2001"}, {"section_title": "List of Figures", "text": "Cumulative mean age in the base year of students in 4-year institutions: 2001 Cumulative percentage of students in 4-year institutions who are non-White: 2001 Cumulative percentage of students in 4-year institutions who were enrolled in an undergraduate program in spring 2001 100 Cumulative percentage of students in 4-year institutions who attained a degree by June 2001 101 Cumulative percentage of students in 4-year institutions who were employed: 2001 Chapter 1 Overview of BPS:1996BPS: /2001 This document describes the procedures and results of the full-scale implementation of the Beginning Postsecondary Students Longitudinal Study: 1996(BPS:1996. BPS:1996BPS: /2001 is the second follow-up data collection conducted with students who started their postsecondary education during the 1995-96 academic year. Research Triangle Institute (RTI), with the assistance of MPR Associates, Inc. (MPR), conducted the study for the National Center for Education Statistics (NCES) of the U.S. Department of Education (Contract No. ED-99 -CO-0112), as authorized under Section 404(a) of the National Statistics Act of 1994 . This introductory chapter describes the background, purposes, schedule, and products of BPS:1996/2001. The second chapter describes the design and methods used during the full-scale study. A discussion of data collection outcomes is presented in chapter 3. A description of the procedures implemented to ensure the quality of the BPS data and an evaluation of the quality of the data collected are provided in chapter 4. Data file construction is discussed in chapter 5. Results of sample weighting and variance estimation are presented in chapter 6. Additional materials used during the study are provided as appendices to the report and cited, where appropriate, in the text."}, {"section_title": "A.", "text": "\n"}, {"section_title": "Background and Objectives of BPS", "text": "Each academic year, several million students begin postsecondary education for the first time. The Beginning Postsecondary Students Longitudinal Study (BPS) series provides an opportunity to describe these students during their first year and at multiple time points after their first year. As one of several studies sponsored by NCES to respond to the need for a national, comprehensive database on postsecondary education, the BPS series addresses issues related to enrollment, persistence, progress, attainment, continuation into graduate/professional school, employment, and rates of return to society. Since nearly half of all beginning students enroll at more than one institution during the 5 years after they begin postsecondary education,' being able to monitor the progress of these students across postsecondary institutions has become increasingly important. Through its unique design, the BPS study series makes it possible to trace the paths of first-time beginning students (FTBs) throughout the entire system of postsecondary education over a number of years. Consequently, whereas typical retention and attainment studies of entering freshmen provide Berlcner, L.K., Cuccaro-Alamin, S., and McCormick, A.C. (1996). Descriptive Summary of 1989-90 Beginning Postsecondary Students: 5 Years Later, with an Essay on Postsecondary Persistence and Attainment (NCES-96-155, ED396597). U.S. Department of Education. Washington, DC: National Center for Education Statistics. data at a single institution, BPS allows for the study of student persistence and attainment anywhere. The BPS series is also unlike previous longitudinal studies of high school age cohorts in that its student sample includes nontraditional postsecondary students who delayed continuation of their education after high school for a variety of reasons. The first BPS series, BPS:90, involved data collection at three points in time (see figure 1.1). Base year data collection during the first year of postsecondary study occurred during the 1989-1990 academic year for the 1990 cohort, as part of the 1990 National Postsecondary Student Aid Study (NPSAS:90). Two subsequent data collections took place in the third (BPS:90/92) and fifth academic years (BPS:90/94) following initial enrollment. The current series, BPS:96, collected data at three similar points in time (figure 1.1). Consistent with BPS:90, base year data collection occurred as part of NPSAS:96, the first year of postsecondary study for the 1996 cohort, and the first follow-up (BPS:96/98) occurred 2 years later, during the third academic year following entry. However, unlike BPS:90, the second follow-up of the 1996 cohort (BPS:1996(BPS: /2001 was conducted 6 academic years following entry, rather than 5. This timing allowed for the collection of attainment information for students who completed their degree in either their fifth or sixth year. Only students who had never completed a postsecondary course prior to the 1995-96 academic year were eligible for participation in BPS:96. Questions for FTB-determination, along with items addressing how students and their families pay for postsecondary education, were administered as part of the base year studies (NPSAS:90; NPSAS:96). Items in the first follow-up studies (BPS:90/92; BPS:96/98) focused on issues ofpersistenceacademic progress through the first 3 years of postsecondary studyamong students enrolled in 4-year institutions, and attainment among students enrolled in less-than-2-year and 2-year colleges. Nontraditional students were asked about the reasons they delayed enrollment, their prior employment experience, and their purpose for enrolling. Interviews addressed the differences between those with immediate vocational goals and those intending to earn a bachelor's degree, including those beginning at community colleges. In addition, sets of items identified transfers, stopouts, and dropouts, and the reasons for these enrollment behaviors. Because the second follow-up of the BPS:90 cohort, BPS:90/94, occurred during the fifth academic year and the second follow-up of the BPS:96 cohort, BPS:1996/2001, took place during the sixth academic year since first enrollment, some items in the BPS:1996/2001 interview collected retrospective information about the fifth academic year to allow cross-cohort comparisons. Persistence and attainment among students enrolled in 4-year institutions and employment among students no longer enrolled were the primary topics for the second followup. These studies serve to monitor academic progress over time, allowing assessment of completion rates for 4-year programs in the normal time expected. For students who graduated in the 4-year time period, the BPS:1996/2001 survey occurred 2 years after receipt of the bachelor's degree and addressed issues of attainment, graduate school access, and initial rate of return.   (BPS:1996(BPS: /2001) AY6 NOTE: AY1 through AY6 indicate the first through sixth academic years covered by the longitudinal period for each study. BPS sample members first entered postsecondary education during AY1."}, {"section_title": "EST COPY AVAILABLE", "text": "For those students who terminated their postsecondary education prior to completion of a baccalaureate degree, the BPS:1996/2001 follow-up 6 years after college entry collected more detailed information on continuation and rate of return. It provided information on how many FTBs returned for additional education either in the same or a different field within the limited time period. For those who did not continue, it provided some rate of return information for employment and other societal benefits related to education. By following a cohort of new entrants into postsecondary education (PSE), the BPS series of studies provides a unique perspective of what happens to persons as they enter and pursue education beyond high school. Because it includes both nontraditional and traditional students who entered PSE immediately after high school, BPS permits study of educational aspirations, progress, persistence, and attainment for both groups of students. By providing longitudinal data for a single cohort and trend data across cohorts, the BPS series contributes to our understanding of the value of a student's postsecondary education both to the student and to society, and to the comprehensive national database addressing policy issues at the postsecondary level."}, {"section_title": "B.", "text": "\nData Collection Design 1."}, {"section_title": "Schedule and Products of BPS:1996/2001", "text": "The operational schedule for BPS:1996/2001 is presented in table 1.1. "}, {"section_title": "Respondent Universe", "text": "The respondent universe for the BPS:1996/2001 full-scale study consisted of all students who began their postsecondary education for the first time during the 1995-96 academic year at any postsecondary institution in the United States or Puerto Rico. The sample students were the first-time beginning students (FTBs) who attended postsecondary institutions eligible for inclusion in NPSAS:96 and who were themselves NPSAS-eligible. a."}, {"section_title": "Institution Universe", "text": "Consistent with previous NPSAS studies, institutions eligible for NPSAS:96 and, consequently, eligible for the BPS:96 cohort, were those that satisfied all of the following conditions for the 1995-96 academic year: offered an educational program designed for persons who have completed secondary education; offered more than just correspondence courses; offered at least one academically, occupationally, or vocationally oriented program of study requiring at least 3 months or 300 contact hours of instruction; offered courses that were open to the general public (i.e., not just to specific populations such as prison inmates or members of the organization offering the courses); and were located in the United States or Puerto Rico. U.S. service academies were excluded from participation because of their atypical funding and tuition base. Also ineligible were institutions offering only avocational, recreational remedial, or correspondence courses; institutions not open to the public; hospitals offering only internships or residency programs; institutions offering only noncredit continuing education units (CEUs); schools whose only purpose was to prepare students to take a particular examination (e.g., CPA or Bar exams); institutions offering only programs of study which required less than 3 months or 300 contact hours of instruction; and branch campuses of U.S. institutions in foreign countries. "}, {"section_title": "Student Universe", "text": "Students eligible for the BPS:96 cohort were those students eligible for NPSAS:96 who were FTBs at NPSAS sample institutions during the 1995-96 academic year (except those who were deceased). NPSAS:96-eligible students were enrolled in NPSAS-eligible institutions during the 1995-96 academic year and satisfied all of the following eligibility requirements: were enrolled in a term or course that began between May 1,1995, andApril 30, 1996;' were enrolled in either (a) an academic program; (b) at least one course for credit that could be applied toward fulfilling the requirements for an academic degree; or (c) an occupational or vocational program that required at least 3 months or 300 clock hours of instruction to receive a degree, certificate, or other formal award; were not concurrently enrolled in high school; and were not enrolled solely in a GED or other high school completion program. The NPSAS-eligible students who had never enrolled in a postsecondary institution after completing high school were considered \"pure\" FTBs and were, of course, eligible for the BPS:96 cohort. However, those NPSAS-eligible students who had enrolled for at least one course after completing high school but had never completed a postsecondary course before the 1995-96 academic year were considered \"effective\" FTBs and were also eligible for the BPS:96 cohort."}, {"section_title": "2.", "text": "\n\n\n"}, {"section_title": "Statistical Methodology", "text": "The NPSAS:96 sampling design was a two-stage design in which eligible institutions were selected at the first stage and eligible students were selected at the second stage within eligible, responding sample institutions. The NPSAS:96 sample, the process of identifying and selecting FTBs for the BPS follow-up studies, and the BPS:1996/2001 subsampling procedures are described below. a."}, {"section_title": "NPSAS:96 Institution Sample", "text": "The institution-level sampling frame for NPSAS:96 was constructed from the 1993-94 Integrated Postsecondary Education Data System (IPEDS) Institutional Characteristics (IC) filet. This full year of enrollment is the operational survey population. The ideal target population consists of the terms in the 1995-96 financial aid award year, those beginning between July 1, 1995, and June 30, 1996. The survey year is slightly shifted from the ideal year to allow more timely data collection and dissemination of results. 2 The 1993-94 IPEDS IC file was the latest version available at the time of NPSAS:96 institutional sampling. The following sets of records that did not correspond to institutions eligible for NPSAS:96 were deleted: administrative units (SECTOR=0); U.S. Service academies (OBEREG = 00); U.S. Territories, except Puerto Rico (OBEREG = 09 and STABBR not 'PR'); institutions that offer no programs of at least 300 contact hours, 6 semester or trimester hours, or 12 quarter hours and for which the highest level of offering was a certificate or diploma of less than 1 academic year (PG300 = 2 and HLOFFER 1); institutions offering only correspondence courses (UNITID=249928, 137379, 367644, and 385363);3 and 12 institutions with reported real (not imputed) zero enrollment (based on unduplicated head counts) for the 1992-93 academic year.4 These edits resulted in a sampling frame consisting of 9,468 institutions that appeared to be eligible for NPSAS:96 based on their 1993-94 IPEDS IC data. Sample institutions were selected for NPSAS:96 with probabilities proportional to composite measures of size based on overall sampling rates by type of institution and type of student. The overall institution sample sizes and sampling rates are shown in table 2.1 for each of the nine institutional sampling strata. The expected frequency of selection exceeded unity (1.00) for some institutions because of their relatively large enrollment within their stratum. These institutions were included in the sample with certainty. The numbers of certainty and noncertainty institutions selected are shown for each stratum in table 2.2. Within each of the nine institutional strata, additional implicit stratification was accomplished by sorting the sampling frame for each stratum in a serpentine manner 5 by the following variables: institutional level; the Office of Business Economics (OBE) Region (from the IPEDS IC file) with Alaska and Hawaii moved to Region 9 with Puerto Rico; and the institution measure of size. 3 These were identified by calling the institutions. The calls resulted from searching for \"corr\" in the name of the institution and from checking discrepant/outlier enrollment data. 4 Unduplicated head count data are collected for the academic year prior to the one in which the IPEDS data collection is conducted. 5 Williams, R.L., and Chromy, J.R. (1980). \"SAS Sample Selection MACROs.\" Proceedings of the Fifth Annual SAS Users Group International Conference, 392-396.  :1996: (BPS:1996.  :1996: (BPS:1996. The objectives of this additional, implicit stratification were to ensure proportionate representation of institutions by level for the two strata that include institutions at two levels; to ensure proportionate representation of all geographic regions; and to ensure proportionate representation of small institutions. The effect of the implicit geographic stratification is seen in table 2.3, which shows that the geographic distribution of the sample is comparable to that of the survey population (the eligible institutions in the 1993-94 file)."}, {"section_title": "10", "text": ""}, {"section_title": "27", "text": "BEST COPY AURA ILE \nBIEST COPY AVAILA LE  A comparison of conditional interview rates (i.e., interview given contact) in table 3.2 shows that contacting and interviewing rates varied according to prior response status. The percentage of sample members who were interviewed, given contact, was 96 percent for those interviewed in both NPSAS:96 and BPS:96/98. A 90 percent response rate (given contact) resulted from those sample members who were only interviewed in BPS:96/98, while NPSAS:96only respondents had a response rate of 81 percent. When compared, NPSAS:96 nonrespondents (BPS:96/98-only respondents) were easier to both contact (t = -2.3; p<.05) and interview (t = -2.2; p<.05) than those who responded during NPSAS:96 but not during BPS:96/98. Contacting and interview rates by type of school, presented in table 3.3, show the same general results as in the prior follow-up (BPS:96/98). That is, students who attended private forprofit schools continued to be difficult to contact and students from 4-year institutions tended to be relatively easy to contact. As in the prior follow-up, interviewing rates varied little by institution type, ranging from 92 to 97 percent once the person was contacted.  "}, {"section_title": "NPSAS:96 Student Sample", "text": "Each sample institution was asked to provide a database or hard-copy list of all its NPSAS-eligible students enrolled during the NPSAS year. Students were sampled on a flow basis as the student files and lists were received. Machine-readable lists were unduplicated by student ID number prior to sample selection. Stratified systematic sampling was used to facilitate sampling from both hard-copy and machine-readable lists. For each institution, the student sampling rates, rather than the student sample sizes, were held constant (fixed) for the following reasons: to facilitate sampling students on a flow basis as student lists were received; to facilitate the procedures used to \"unduplicate\" the sample selected from duplicated hard-copy lists; and because sampling at a fixed rate based on the overall stratum sampling rate and the institutional probabilities of selection results in approximately equal overall probabilities of selection within the ultimate student strata. For each sample institution, the student sampling rates were determined for each of four student sampling strata: potential FTBs, other undergraduate students, first professional students, and other graduate students."}, {"section_title": "11", "text": ""}, {"section_title": "28", "text": "BEST COPY AVAIIABLE The institutions were asked to specify the student level (undergraduate, first professional, or other graduate student) based on the student's last term of enrollment during the NPSAS year. Furthermore, they were asked to identify their undergraduate students whose first term of enrollment at the institution was during the NPSAS year, who were freshman or first-year students at that time, and who did not have any transfer credits from another postsecondary institution. Those students were classified as the potential FTBs. The sampling rates depended on the overall population sampling rates for the four types of students, theprobability of selecting the institution, and a requirement for a minimum of 40 sample students per institution whenever possible. NPSAS:96 data collection consisted of computer-assisted data entry (CADE) from records maintained by the institutions (e.g., at the financial aid or registrar's office) for all sample students as well as computer-assisted telephone interviews (CATI) with sample students. Unfortunately, a sample student's FTB status could not be determined until the student's CATI interview had been completed. Therefore, potential FTBs were oversampled in NPSAS in an attempt to yield a sufficient number of BPS-eligible sample members. A total of 12,410 cases were identified as either pure or effective FTBs and thus were eligible for the BPS:96 cohort."}, {"section_title": "c. BPS:1996/2001 Sample", "text": "Of the 12,400 eligible for the BPS:96 cohort, 10,300 completed the BPS:96/98 interview and were verified to be FTBs. The BPS:1996/2001 sample consisted of these BPS:96/98 respondents plus almost 1,800 NPSAS:96 respondents (BPS:96/98 nonrespondents) who were verified to be FTBs. Excluding those cases identified as deceased since their last interview, almost 12,100 sample members eligible for BPS:1996BPS: /2001 To contain costs for the full-scale study, the eligible BPS:96/98 nonrespondents were subsampled. A sample of BPS:96/98 nonrespondents with probabilities proportional to their initial weights was selected. Of these cases, a stratified random subsample was selected to include at the beginning of data collection. The remaining cases were reserved for possible fielding at a later date if necessary and not cost-prohibitive, but ultimately were not included in the BPS:1996/2001 sample. The details of this sampling are described below. The first step entailed defining three nonrespondent subsampling strata based on whether the parent postcard was returned and whether the sample member either matched to the Central Processing System (CPS) database or Telematch produced a good telephone number. It was expected that sample members whose parents returned the postcard were most likely to be located and interviewed. Those whose parents did not return the postcard but who matched to CPS or Telematch were assumed to be somewhat less likely to be located and interviewed. Sample members whose parents did not return the postcard and who did not match to CPS or Telematch were assumed to be least likely to be found and interviewed. These three sampling strata were then subdivided based on institutional strata because FTBs were sampled at different rates at different types of institutions. Preliminary analyses showed that without this subdivision of the sampling strata, the unequal weighting design effects for institutional analysis strata become unacceptably large. A sample allocation was chosen that maximized the unweighted response rates and those rates were then scaled to achieve the desired sample sizes. The sampling strata and sampling rates, are shown in table 2.4. "}, {"section_title": "0.160", "text": "Legend for institutional stratum: 1= public less-than-2-year 2= public 2-year 3= public 4-year non-doctorate-granting 4= public 4-year doctorate-granting 5= private not-for-profit less-than-4-year NOTE: The CPS (Central Processing System) contains locating information for all sample members who applied for federal financial aid for a given year. SOURCE: U.S. Department of Education, National Center for Education Statistics, Beginning Postsecondary Students Longitudinal Study:1996(BPS:1996). 6= private not-for-profit 4-year non-doctorate-granting 7= private not-for-profit less-than-2-year 8= private for-profit less-than-2-year 9= private for-profit 2-year or more 13 30 BEST COPY AVAILABLE Finally, a stratified sample of nonrespondents with probabilities proportional to their initial weights was selected, using the sample allocation computed in the previous step. A stratified random subsample of these cases was selected to include as part of the initial BPS:1996/2001 sample. Due to the high cost of locating these sample members and their relatively low interview rates, the remaining nonrespondents were not added to the sample."}, {"section_title": "3.", "text": ""}, {"section_title": "BPS:1996/2001 Field Cluster Selection", "text": "Field interviewing, discussed in detail later in this chapter, required the selection of geographic clusters. These geographic clusters were selected at the start of data collection to maximize the likelihood of having a high number of sample members in each area. The geographic clusters were defined by the following multistep process: First, a unique zip code was associated with each sample member, based on their \"best address\" available. The U.S. Postal Service's address standardizing service was used to clean addresses and obtain zip codes for as many addresses as possible. Next, RTI's geographic information system (GIS) was loaded with each sample member's zip code. Finally, the GIS plotted each zip code, identifying concentrations of sample members within 50-mile radii. This process resulted in 30 geographic clusters, each containing between 63 and 900 potential field cases. CATI nonrespondents were assigned to one of the 30 geographic clusters based on the latest tracing information available at the time that a sample member was identified for field interviewing. If the most recent locating information fell outside the 30 clusters, the case was treated as a \"hard to reach\" case (described below)."}, {"section_title": "Instrument Development", "text": "The BPS:1996/2001 interviews were conducted using computer-assisted interviewing (CAI) technology to conduct both telephone and in-person interviews. In preparation for the development of the CATI/CAPI instrument, a comprehensive set of data elements was developed from a review of the data elements used for the BPS:90 cohort, their relationship to the NPSAS:96 and BPS:96/98 data elements, the reliability of responses obtained in BPS:90, and their relevance to current research and policy issues. To allow for cross-cohort comparisons with BPS:90/94, the data elements included retrospective information.6 A preliminary set of BPS:1996/2001 data elements was refined with input from the study's Technical Review Panel (TRP; see appendix A for a list of members) as well as from NCES and other Department of Education staff. The final set of data elements is presented in appendix B. Based on the data elements, the BPS:1996/2001 CATI/CAPI instrument was first developed for the field test data coll6ction effort and then, with feedback from NCES and recommendations from the TRP, revised for the full-scale data collection. The instrument was structured by identifying section topics and determining the progression of items within sections. Individual items were designed with several goals in mind: (1) using existing items (that have been previously tested) when feasible; (2) ensuring consistency with NPSAS:96, BPS:96/98, and BPS:90/94 items when items were not identical; and (3) identifying and preparing wording for item verifications and probes as necessary. Detailed instrument specifications were written for each item, including variable names and definitions, skip patterns, and out-of-range limits. Instrument sections were reviewed on a flow basis by NCES. As depicted in figure 2.1, the first section collected information about all postsecondary enrollment since the previous interview.' The next two sections collected information about undergraduate and postbaccalaureate (graduate or additional undergraduate) school, experiences, respectively. Employment, particularly addressing rate of return policy issues, was the focus of the fourth section. This section asked about the first job after leaving school for those who were not asked about first job in the BPS:96/98 interview (because they were still enrolled), as well as current job information. The final section collected background and current status information such as family formation/household composition, income, debts, civic participation, disabilities, and goals. A facsimile interview is provided in appendix C. Despite different data collection methods, the CATI and CAPI interviews were programmed identically, using version 4.3 of the Computer-Assisted Survey Execution System (CASES) software. The CATI/CAPI system presented interviewers with screens of questions to be asked of the respondents, with the software guiding the interviewer and respondent through the interview. Inapplicable questions were automatically skipped based on prior response patterns and preloaded information. Wording for probes was suggested when a respondent provided a response that was out of range for a given item. Help text was provided for each screen in the event that clarification of question intent was required. Online coding programs for IPEDS, enrollment terms, major, financial aid, and occupation/industry were incorporated to allow standard coding of responses. Concurrent with the design and programming of the CATI/CAPI instrument, instrument documentation was entered into an integrated data dictionary system (DDS), which subsequently facilitated production of data files with CATI/CAPI variable documentation. An abbreviated instrument was developed for the purpose of interviewing special respondent groups such as sample members whose primary language is Spanish. The abbreviated instrument, also presented in appendix C, focused on the respondent's postsecondary enrollment history, undergraduate experiences, employment, and family formation.  To minimize the interview burden on respondents, the CATI/CAPI instrument used extant data whenever feasible. Pre loaded values from the locator database and data from the NPSAS:96 and BPS:96/98 interviews were used to confirm the identity of sample members and to reduce data collection time, effort, and cost. The preloaded data dictated the flow of many portions of the interview. Certain questions were asked only if the data were missing from prior interviews. Other questions used the NPSAS:96 and BPS:96/98 preloads to provide context (e.g., \"I'd like to begin by asking you some questions about your school enrollment since the last time we talked to you in 1998. According to our records, you were enrolled at North Carolina State University at that time. Are you still enrolled there?\"). In other questions, respondents were asked to update information since the last interview based on preloaded information (e.g., \"Last time we talked to you, your major or program of study while attending North Carolina State University was electrical engineering. Is that still your major?\"). Once CATI/CAPI programming was completed, test cases were developed and loaded for instrument testing and interviewer training. Project staff systematically tested the CATI/CAPI instrument prior to the start of interviewer training. Finally, preload files containing data from NPSAS:96, BPS:96/98, and the Department of Education databases were prepared and loaded into the CATI/CAPI system to both guide the interview and assist sample member locating efforts. Data collection commenced only after all of these tasks were complete."}, {"section_title": "Locating", "text": "The BPS:1996/2001 sample members were at a stage in their lives where they tended to be highly mobile, having moved at least once, if not multiple times, since they were last interviewed. Consequently, it was a difficult population to locate. The BPS:1996/2001 design involved tracing sample members to their current location and conducting an interview by telephone (CATI) or in person (CAPI) with them about their experiences since their last interview (the BPS:96/98 interview 3 years earlier or the NPSAS:96 interview 5 years earlier). The locating activities, depicted in figure 2.2 and discussed in the following sections, involved advance locating conducted before the start of CATI, locating activities performed by telephone interviewers as part of CATI operations, intensive tracing by RTI's Tracing Operations Unit (TOPS), and field locating. a.\nPre-CATI locating. An important first step in contacting and interviewing BPS:1996/2001 sample members was the updating of address information collected during the BPS:96/98 and NPSAS:96 interviews, as well as any new information collected since the last interview. This new information could have been obtained from annual matches to the Central Processing System for federal financial aid applicants occurring as part of sample maintenance or from batch processing to the NCOA and Telematch databases. In addition, sample members' parents and other individuals identified by the sample member in prior interviews were contacted by mail for address updates for the sample members. Address information was available for parents or other locators for 81 percent of the sample, and address update forms were received from 32 percent of those who were sent the mailing. One week before the start of CATI data collection, a second mailing was sent to inform sample members of the upcoming telephone interview and to request that they correct and return an address update sheet. The prenotification mailing was sent to every sample member with the exception of 38 cases for whom no address information was available. Address update sheets with new or confirmed information were received from 8 percent of those sent the mailing. As shown in table 3.4, contact (t = 13.9; p<.001) and interview (t = 16.0; p<.001) rates were higher for those respondents who returned an address update sheet or had it returned on their behalf.  Longitudinal Study: 1996(BPS:1996."}, {"section_title": "Advance Locating", "text": "Locating information was collected during the NPSAS:96 and BPS:96/98 interviews and incorporated into the locator database. The locating information included the sample members' local and permanent addresses and telephone numbers, the addresses and telephone numbers of parents and friends of sample members, drivers license information, and Social Security Numbers. These locating data were updated by the U.S. Postal Service National Change of Address (NCOA) and by Telematch operations, which provided updated address and telephone number information, respectively. Department of Motor Vehicle (DMV) searches were conducted in the six states containing the largest concentrations of sample members (California, Texas, Florida, New York, Illinois, and Michigan) to obtain additional locating information.  Jur COPY MAILABLE Four months prior to the start of data collection, a mailing, consisting of a letter, a study leaflet, and an address update information sheet, was sent to the parents or other contacts of sample members to update the most recent sample member addresses and to gain cooperation by explaining the purposes of the study. A similar mailing, consisting of a letter, a study leaflet, a call-in card, and an address update information sheet (examples of each are in appendix D), was sent to sample members immediately prior to the start of data collection. The purpose of this mailing was to notify the sample members of the upcoming interview, inform them of their rights as participants, stress the importance of the study and urge their participation, and obtain additional postal service address updates. The mailing also gave sample members the opportunity to complete and return an address update form. New contact information obtained from the mailing was entered into the locator database. To expand efforts to gain parent cooperation, a postcard was mailed to the parents of sample members at the beginning of the data collection period, informing them of the upcoming data collection. The postcard consisted of a note explaining the study as well as a perforated card for the parent to tear off and give to the sample member (see appendix D). The card asked the sample member to call in using the toll-free telephone number shown and complete the interview at his or her convenience. This addressed a problem encountered in the field test and other NCES studies, namely, that parents sometimes acted as \"gatekeepers\" making it difficult to locate and speak with the sample member. Additional pre-CATI tracing was performed for sample members identified as BPS:96/98 nonrespondents, those with insufficient telephone number information, and those for whom we received undeliverable mail returns through RTI's TOPS Unit. TOPS's intensive tracing operations are described below. b."}, {"section_title": "CATI Locating", "text": "In addition to the advance locating activities described above, tracing efforts were undertaken by interviewers in the Telephone Survey Unit (TSU), concurrent with their efforts to gain cooperation from and interview sample members. When assigned a case, the telephone interviewer called the telephone number designated by the system as the best number (i.e., the number among all available locator numbers that appeared to have the greatest potential for contacting the sample member) and attempted to interview the designated sample member. If the person at that number indicated that the sample member could not be reached there, the interviewer requested additional contact information for the sample member. If the person was unable to provide additional information, the interviewer called additional telephone numbers associated with the case in an attempt to locate the sample member. After all possible telephone numbers for the case were exhausted without success, the case was assigned to TOPS for intensive tracing. c."}, {"section_title": "Intensive Tracing", "text": "Intensive tracing was performed by RTI's TOPS unit, which had access to both proprietary and public domain data. TOPS tracers had real-time access to consumer databases that contained current address and phone listings for the majority of consumers with credit histories. In addition to proprietary databases, TOPS had access to various other information sources, such as Dataminers, commercial list-houses, and NCOA via leased line. These sources searched for name, address, neighbor, business, telephone number, and status (decedent, incapacitated, military). A two-tiered intensive tracing plan was used to locate sample members. The first tier involved identifying sample members with Social Security Numbers and processing that information through two credit bureau searches. If the searches generated a new telephone number, that case was returned to TSU for telephone interviewing. If a new address was generated but no telephone number was provided, tracers called directory assistance or queried other databases to obtain telephone numbers for CATI. This first level of effort minimized the time that cases were out of production. The more intensive second tier was implemented for those cases where the first level searches were unsuccessful. This involved the following tracing procedures: (1) checking directory assistance for telephone listings at various addresses; (2) using reverse-match databases to obtain the names and telephone numbers of neighbors and then calling the neighbors; (3) calling persons with the same unusual surname in small towns or rural areas to see if they were related to or knew the sample member; (4) contacting the current or last known residential sources such as the neighbors, landlords, current residents, tax assessors, realtors, and other business establishments related to previous addresses associated with the sample member; (5) calling colleges and military establishments to follow up on leads generated from other sources; and (6) checking various tracing Web sites. Tracers checked new leads produced by these tracing steps to confirm the address and telephone numbers for the sample members. When the information was confirmed, the case was returned to TSU for telephone interviewing. If the information could not be confirmed (e.g., there were no working telephone numbers or numbers for relevant neighborhood sources were unpublished), and the sample member was thought to be located in one of the geographic clusters, the case was assigned to field interviewers for locating. d."}, {"section_title": "Field Locating", "text": "Locating activities were performed by field interviewers, concurrent with their efforts to interview sample members. Since the costs of conducting field locating were high, field locating efforts were implemented only when less costly efforts were exhausted. Sample members were identified as needing field locating/interviewing if they were not located using CATI-locating and centralized intensive tracing. Additionally, sample members who were located by telephone but initially refused to participate were identified as potential field cases. Thirty geographic clusters of sample members were identified and staffed with field interviewers. The interviewers were trained to locate and interview sample members using a laptop computer. Field interviewers were provided with a checklist which included sample questions to help with tracing operations and that demonstrated the correct order in which tracing activities should be performed. The checklist was completed for each case to help identify sources considered to be most useful in locating sample members. Field interviewers documented every telephone call or field contact. Primary tracing sources included: current or former neighbors, postsecondary schools attended, past or present employer, social agencies' records, and city and county offices. Secondary tracing sources included directory assistance, chambers of commerce, public libraries, the U.S. Postal Service, and Departments of Motor Vehicles. Other miscellaneous sources, useful in some cases, included small town police or sheriff's departments, fire departments or emergency rescue squads, local newspapers, public housing authorities, mobile home park managers, motel staff, probation officers, and permit issuing departments at the city level (new construction). A contact script guided interviewers in soliciting information from various sources. 3. Interviewing a."}, {"section_title": "Training of Interviewers", "text": "The training program for telephone and field interviewers was designed to maximize active participation. Training for telephone interviewers and their supervisors, conducted immediately prior to the start of telephone interviewing, consisted of a study overview, review of confidentiality requirements, demonstration interview, question-by-question review of the BPS:1996/2001 instrument, and hands-on practice exercises with the instrument, tracing module, and online coding modules. Interviewers were also trained in techniques for gaining cooperation with sample members, parents, and other contacts, as well as techniques for addressing the concerns of reluctant participants and avoiding refusals. Training for field interviewers andtheir supervisors similarly consisted of lectures, demonstrations, and hands-on practice exercises with the instrument and online coding modules. In addition, field interviewers were trained on fieldspecific operations, including the field management system and field tracing procedures. The BPS:1996/2001 telephone and field interviewer training agendas and the table of contents from their respective training manuals are located in appendix E. b."}, {"section_title": "Telephone Interviewing", "text": "CATI locating and interviewing began in February 2001 upon completion of telephone interviewer training. CATI procedures included attempts to locate, gain cooperation from, and interview study sample members by telephone. Locating information gleaned from the advance locating sources described above and from prior interviews with the sample member was preloaded into the CATI system. Each case had a call roster with names and telephone numbers associated with the sample member (e.g., parents, other contacts such as friends or relatives, sample member) for the interviewers to call. Up to five roster-lines were preloaded with contact information. Additional roster-lines were added when CATI tracing or intensive tracing produced new contact information. An automated call-scheduler, embedded within the CATI software, assigned cases to interviewers. This system allowed calls to be scheduled on the basis of established case priority, time of day, and history of success of prior calls at different times and on different days. Scheduler case assignment was designed to maximize the likelihood of contacting and interviewing sample members. Cases were assigned to various queues for this purpose. Some of the queues included new cases, Spanish language cases, initial refusals, and various appointment queues (e.g., firm appointments set by the sample member, appointments suggested by locator sources, and appointments for cases which were initial refusals). Once located, some cases required special treatment. To deal with those who initially refused to participate (including locator sources who acted as \"gatekeepers,\" preventing access to the sample member), certain interviewers were trained in refusal conversion techniques. Sample members and their locator sources who spoke only Spanish, primarily located in Puerto Rico, were assigned to bilingual interviewers. Results of CATI interviewing were monitored daily through the study Integrated Management System. Daily reports of production, with revised projections of future production to satisfy study requirements, were available to both NCES and contractor staff. Finally, in an effort to increase study response rates, a modest incentive was used with particular types of nonrespondents: (1) cases where the sample member initially refused the interview, (2) sample members for whom intensive tracing yielded a good mailing address, but no telephone number, and (3) cases identified as \"hard to reach\" (i.e., those with 15 or more call attempts, where contact had been established with the sample member but no appointment could be scheduled). The subsample of BPS:96/98 nonrespondents was offered an incentive as well, although because subsample members were expected to be difficult cases, their incentive was offered before any attempt was made to interview them. The incentive mailing consisted of a letter from the project director tailored to the specific type of nonrespondent (i.e., refusal or no telephone number/hard to reach). A $5 bill was included with the letter. Respondents received a check for an additional $15 when they completed the interview. The incentive letters, shown in appendix D, were mailed on a flow basis as respondents met one of the criteria described above. All cases assigned to field interviewers were automatically eligible to receive the incentive. c."}, {"section_title": "C.", "text": "\n"}, {"section_title": "The Integrated Management System", "text": "All aspects of the study were under the control of an Integrated Management System (IMS). The IMS was a comprehensive set of desktop tools designed to give project staff and NCES access to a centralized, easily accessible repository for project data and documents. The BPS IMS consisted of several components, or modules: the management module, the Receipt Control System (RCS) module, and the CATI/CAPI module. The management module of the IMS contained tools and strategies to assist the project staff and the NCES project officer in managing the study. All information pertinent to the study could be found here, accessible via the World Wide Web, in a secure desktop environment. Available on the IMS were the current project schedule, monthly progress reports, daily data collection reports and status reports (available through the Receipt Control System described below), project plans and specifications, key project information and deliverables, instrument specifications, staff contacts, the project bibliography, and a document archive. Also accessible from the management module was a downloadable version of the CATI/CAPI instrument for testing and review. The Receipt Control System (RCS) is an integrated set of systems that was used to monitor all activities related to data collection, including tracing and locating. Through the RCS, project staff were able to perform stage-specific activities, track case status closely, identify problems early, and implement solutions effectively. The RCS's locator data were used for a number of daily tasks related to sample maintenance. Specifically, the mailout program produced mailings to parent/contacts and sample members, the query system enabled administrators to review the locator information and status for a particular case, and the mail return system enabled project staff to update the locator database as mailings or reply sheets were returned or forwarding information was received. Another component of the RCS was the Field Case Management System (FCMS) which controlled field interviewing activities. The FCMS allowed field staff to conduct tracing and interviewing activities, communicate with RTI staff via electronic mail, transmit completed cases, and receive new cases. The RCS also interacted with the TOPS database sendinglocator data between the two systems as necessary. The CATI/CAPI module managed development of the CATI/CAPI instrument within the Data Dictionary System (DDS). The DDS consisted of a set of linked relational files and associated utilities for developing and documenting the instrument. Developing the CATI/CAPI instrument with the DDS ensured that all variables were linked to their item/screen wording and were thoroughly documented. Also included within the CATI/CAPI module was online coding software (\"user exits\") that collected detail on schools attended, enrollment, major, financial aid, occupation, and industry."}, {"section_title": "D.", "text": "\n"}, {"section_title": "The Variable Tracking System", "text": "The central mechanism for constructing input files for the electronic codebook (ECB) developed by NCES is a software application called the Variable Tracking System (VTS). The VTS tracks and stores documentation for both interview and derived variables required for the ECB and Data Analysis System (DAS). This includes weighted and unweighted variable distributions, variable labels and codes, value labels, and a text field describing the development of each variable and the programming code used to construct it. Input files for the ECB and DAS systems are automatically produced by the VTS according to NCES specifications. Successful data collection for BPS:1996/2001 involved several steps: updating existing locating information for the sample member, attempting contacts at the available addresses, initiating intensive locating efforts when contacts failed, and completing the interview. Chapter 3 describes these data collection outcomes and examines the effectiveness of our data collection methods."}, {"section_title": "Response Rates", "text": "Overall contacting and interviewing results for BPS:1996/2001 are presented in figure 3.1. The starting sample consisted of those sample members who participated in the first follow-up, BPS:96/98, plus sample members selected from among the BPS:96/98 nonrespondents. Of those cases, less than 1 percent was excluded because the sample members were deceased. Among the remaining cases, 92.4 percent were successfully contacted and 95.5 percent interviewed, given contact, for an overall unweighted response rate of 88.3 percent. Weighted response rates are presented separately in chapter 6. Table 3.1 shows a distribution of response rates by type of interview completed and prior response status. From the table, 97 percent completed full interviews, while the remaining 3 percent completed less than a full interview, either in the form of a partial interview (i.e., sample members completing at least the interview section on enrollment history) or an abbreviated interview (i.e., a condensed version of the full interview containing key data elements from each of the five sections of the survey). Partial and abbreviated interview response rates have been combined for reporting purposes.  : 1996: (BPS:1996."}, {"section_title": "Respondent Locating and Interviewing Outcomes", "text": "Tracing and locating sample members in any longitudinal study is a complex task, oftentimes requiring the use of multiple sources of information to locate the current address and telephone number of a sample member. Successful completion of the BPS:1996/2001 locating effort required a combination of pre-CATI locating activities (advance tracing, updating the BPS locator database, mailings), telephone tracing during the CATI phase of data collection (tracing activities conducted by telephone interviewers/supervisors), centralized tracing efforts (tracing activities conducted by the TOPS unit), and tracing by field interviewers. Descriptions of these locating activities are presented in detail in chapter 2 and highlighted throughout this section. 1."}, {"section_title": "6", "text": "IntenSive tracing. Intensive tracing was conducted by the TOPS unit at RTI both prior to data collection, for cases with no contact information at all (advance tracing), and during data collection, for cases where all leads were exhausted. A number of locating sources were used to trace sample membersincluding consumer databases, directory assistance, and Internet sourcesin two tiers of tracing; the second, more intensive tier was used when the first failed to locate the sample member. Results of the intensive tracing effort are shown in table 3.5.  : 1996: (BPS:1996. Advance tracing prior to the start of data collection was very successful. Of the cases traced, 81 percent was contacted and of those, 84 percent was interviewed. A total of 31 percent of sample members was traced using the first tier, resulting in 76 percent contacted and 93 percenit (of those contacted) interviewed. The second tier tracing was implemented for 1.7 percent of all cases, 44 percent of whom was contacted and 96 percent of those contacted interviewed."}, {"section_title": "32", "text": ""}, {"section_title": "48", "text": "1E \u00a7T COPY HAMA I,E Percentages are based on the total within the row under consideration. 2 Percentages are based on the number ever in field within the row under consideration. 'Percentages are based on the number contacted within the row under consideration. \"Public less-than-2-year and public 2-year sectors were combined due to the small number of cases. Study: 1996(BPS:1996."}, {"section_title": "SOURCE: U.S. Department of Education, National Center for Education Statistics, Beginning Postsecondary Students Longitudinal", "text": "\n\n"}, {"section_title": "4.", "text": ""}, {"section_title": "Interview Burden and Effort", "text": "This section reviews the burden and effort associated with the BPS:1996/2001 interview. The first section examines the burden on respondents by examining the time required to complete the interview overall and by section. We then consider the effort required to locate and interview sample members for the study by considering the average time and number of calls that were required to complete interviews."}, {"section_title": "1.", "text": "\nOnline Coding and Editing BPS:1996/2001 included one major data collection systemthe CATI/CAPI interview designed to include edit checks to ensure that the data collected were within valid ranges. To the extent feasible, the system incorporated across-item consistency edits. The CATI system also included online coding systems used for the collection of industry, occupation, and major, as well as a coding module used to obtain IPEDS information for new postsecondary institutions attended since the last interview.\nBase Weight for BPS:1996/2001Adjustment for Subsampling of BPS:96/98 Nonrespondents As discussed in chapter 2, a subsample of BPS:96/98 nonrespondents was included in BPS:1996/2001. The subsample, rather than all nonrespondents, was fielded in order to reduce data collection costs. The weight BOlIAWT was adjusted for those students, j, in the subsample by multiplying by the inverse of their selection probabilities. These probabilities take into account the stratification and probability proportional to size (PPS) sampling that was used in selecting the subsample. The adjustment was"}, {"section_title": "Timing", "text": "During instrument development, project staff embedded \"time stamps\" at the start and end of the interview, at the start of each section, and around each interview screen (which could include multiple, related items). The time stamps measured elapsed time, allowing project staff to monitor the time required to complete specific interview items, online coding programs, sections of the interview, and the entire interview. Average time to administer the BPS:1996/2001 interview, overall and by section, is shown in table 3.11. Sections are listed in the table in the order in which they occurred in the interview. The number of cases completing each section fluctuated because some respondents broke off the interview early (partial interview); the timing figures for partial interviews are included through the end of the section prior to the point where the interview was terminated. In addition, sample members enrolled at the time of the interview who considered themselves to be primarily students (rather than employees) were skipped around the section on postenrollment employment. As a result, the number completing that section was low relative to the other sections. Average time by BPS:96/98 response status is presented in table 3.11. BPS:96/98 nonrespondents were asked to provide data back to 1996, the time of the NPSAS base-year interview. Consequently, BPS:96/98 respondents took significantly less time than BPS:96/98 nonrespondents to complete the 2001 interview (t = 3.9; p<.001). As shown in table 3.12, the shortest interview times can, in general, be attributed to those sample members who had no enrollment since their last interview (t = -43.6; p<.001). Those reporting no additional enrollment 34"}, {"section_title": "50", "text": "EST COPY AVAIRABLE skipped most of the section on enrollment history (section B), nearly all of the section on undergraduate enrollment (section C), and half of the section on postenrollment employment (section E), and took, on average, 11.9 minutes to administer, compared with 19.3 minutes for those who had been enrolled since their last interview. Likewise, the short interview times of students in less-than-2-year institutions, presented in table 3.13, can be attributed to their low enrollment rate since the last interview. Excludes respondents who skipped the postenrollment employment section because they were enrolled at the time of the interview and considered themselves to be primarily students. Study: 1996(BPS:1996. Excludes respondents who skipped the postenrollment employment section because they were enrolled at the time of the interview and considered themselves to be primarily students. SOURCE: U.S. Department of Education, National Center for Education Statistics, Beginning Postsecondary Students Longitudinal Study: 1996(BPS:1996."}, {"section_title": "Telephone Interviewer Hours", "text": "A total of 15,291 telephone interviewer hours (exclusive of CATI training, supervision, monitoring, and quality circle meetings) were expended to obtain interviews from the sample members who completed full or partial CATI interviews. On average, telephone interviewers spent 1.91 hours per completed interview. With the average time to administer the interview at 18 minutes, the large majority of interviewer time was spent in other activities, primarily attempting to locate and contact the sample member. Call screening. Interview nonresponse is an increasing problem for CATI and CAPI studies, affecting the cost of data collection and the quality of the resulting data. Call screening, defined as the use of devices such as telephone answering machines, Caller ID, call-blocking, or privacy managers to avoid unwanted telephone calls, is an increasing problem for all studies conducted by telephone. Call screening poses a significant obstacle to contacting sample members and can, in turn, affect the representativeness of the data, lower the response rate, and increase project costs by requiring additional call attempts and interviewer time. Approximately 40 percent of the telephone calls placed for BPS:1996/2001 telephone interviewing reached an answering machine, and nearly three-quarters (74 percent) of the cases had at least one answering machine event. Considerably more calls were required to interview those with answering machines (average of 23 calls per case) than those without (average of eight calls per case; t = 29.4; p<.001). Similarly, cases with no answering machine events had a much lower rate of ever refusing (10 percent) compared to 20 percent with at least one answering machine contact (t = 11.9; p<.001). These figures were captured by the study's computerized receipt control system and are based on calls made by telephone interviewers. Calls made by TOPS (in attempt to locate sample members) and field interviewers are excluded. "}, {"section_title": "36", "text": ""}, {"section_title": "4.7", "text": "I Excludes respondents who skipped the postenrollment employment section because they were enrolled at the time of the interview and considered themselves to be primarily students. NOTE: There is no section A in the instrument. Section A, eligibility determination, was eliminated because eligibility for all sample members was determined in NPSAS:96 or BPS:96/98. SOURCE: U.S. Department of Education, National Center for Education Statistics, Beginning Postsecondary Students Longitudinal Study: 1996(BPS:1996.  : 1996: (BPS:1996.  : 1996: (BPS:1996."}, {"section_title": "5 4", "text": "Chapter 4 Evaluation of Data Quality Evaluations of data quality are effective in identifying problems with the instrument that can inform instrument design for future studies. Several types of evaluations were planned for BPS:1996/2001 as part of the overall study design, including analyses of indeterminate responses, help text accesses, online coding, quality circle meetings, and quality control monitoring of interviews. These are described in the sections below. A."}, {"section_title": "39", "text": ""}, {"section_title": "55", "text": "BEST COPY MAMA LE Six of the items with high rates of combined nonresponse pertained to income and personal finances. Many respondents were reluctant to provide information about personal and family finances (refusals) and, among those who were not, many simply did not know. Grade point average also had more than 10 percent nonresponse, most likely because of respondents' difficulty in recalling this information as well as its sensitive nature. The other two items with more than 10 percent nonresponse asked about the Lifetime Learning tax credit. The high rate of \"don't know\" responses for these items is likely due to respondents' not knowing about the credit. The CATI/CAPI instrument was designed to convert \"don't know\" responses, if possible, for three of these items. Sample members who responded with \"don't know\" to the GPA item were asked to provide a letter grade range (e.g., mostly A's, A's and B's, mostly B's, etc.) instead of a number; their conversion 'rate was 94 percent for an item level response rate of 99 percent.' When offered the opportunity to specify annual salary in terms of an hourly, weekly, twice monthly, or monthly amount, 91 percent of those who answered \"don't know\" to the question of current annual salary, and 92 percent of those who answered \"don't know\" to first postenrollment job salary, were converted, for an item level response rate of 93 and 94 percent, respectively. B."}, {"section_title": "Help Text", "text": "Online help text was available for every screen in the CATI/CAPI instrument. Help text screens included definitions of terms used in the question wording and the type of information requested. Having additional information available at the touch of a function key was beneficial to interviewers, particularly at the beginning of data collection, to immediately minimize respondents' confusion with questions while still on the telephone with a respondent. Counters were used to determine the number of times each help screen was accessed, making it possible to identify items that were confusing to the interviewer or respondent. An analysis of the number of help text accesses revealed only one item for which the rate of help text usage was greater than 4 percent: \"Do you (or your spouse) currently receive any of the following forms of untaxed income? TANF (AFDC), Social Security benefits, workers compensation, disability payments, child support, food stamps.\" It is likely that respondents were unfamiliar with some of these sources of untaxed income. C."}, {"section_title": "Online Coding", "text": "The BPS instrument included tools that allowed computer-assisted online coding of literal responses for postsecondary institution, major field of study, occupation, and industry. Online coding systems were designed to improve data quality by capitalizing on the availability of the respondent to clarify coding choices at the time the coding was performed. To assist with the online coding process, interviewers were trained to use effective probing techniques and given extensive, supervised practice. While the interview was being conducted, interviewers were able to clarify the text string provided and request additional information if it could not be I Conversion of \"don't know\" responses to the GPA item was not attempted in the abbreviated interview (19 \"don't know\" cases). The response rate after conversion, inclusive of abbreviated cases, is 98.8 percent. Excluding abbreviated cases results in a response rate of 99.1 percent. coded on the first attempt. Because both the literal string and selected code were captured in the data file for field of study, occupation, and industry responses, subsequent quality control recoding by a coding expert was easily incorporated into data collection procedures. Institution coding was used to assign a six-digit Integrated Postsecondary Education Data System (IPEDS) identifier for each postsecondary institution the respondent reported attending, other than those collected during the base year and first follow-up interviews. To facilitate coding, the IPEDS coding system asked for the state in which the school was located, followed by the city, and finally the name of the postsecondary institution. The system relied on a look-up table, or coding dictionary, of institutions constructed from the IPEDS institutional database. Additional information in the dictionary, such as institutional level and control, was retrieved for later use (e.g., branching) once an institution was properly coded. Major field of study, occupation, and industry coding utilized a dictionary of word/code associations. The online procedures for these coding operations consisted of four steps: (1) the interviewer keyed the verbatim text provided by the respondent; (2) the dictionary system displayed similar words for those words in the text string that were not in the dictionary, giving the interviewer the option of accepting a word that would help in terms of coding or ignoring a word that was not applicable; (3) standard descriptors associated with identified codes were displayed for the interviewer; and (4) the interviewer selected the appropriate standard descriptor from the list, with the assistance of the respondent as needed. Several steps were taken after data collection to ensure the completion and accuracy of the online coding procedures. The first step was upcoding, where project staff reviewed IPEDS schools, majors, occupations, and industries that interviewers marked as \"uncodeable\" and coded the strings into the appropriate categories, where possible. Table 4.2 presents the proportion of coding attempts that were uncodeable by interviewers but were subsequently coded by project staff. 'Total number of coding at empts may exceed the total number of completed interviews as some items were asked multiple times in an interview. 2Percentages are based on the number originally uncodeable within the row under consideration. 3Percentages are based on the total number of responses coded within the row under consideration. NOTE: IPEDS schools, majors, occupations, and industries were reviewed by project staff and either determined to be uncodable or \"upcoded\" into one of the existing response categories as appropriate. SOURCE: U.S. Department of Education, National Center for Education Statistics, Beginning Postsecondary Students Longitudinal Study: 1996(BPS:1996. Institution coding had the highest rate of uncodeable responses prior to upcoding as well as the lowest rate of successful upcoding. This is due, in large part, to the different methods used in coding: IPEDS coding required a precise match between the name of the institution entered and the IPEDS database, while major, industry, and occupation were coded by assigning verbatim strings to categories. Thus, while major, occupation, and industry strings were simply coded into categories, institutions required an exact match. Two additional factors contributed to this high rate of uncodeable schools: (1) the 1997-98 IPEDS database was used for upcoding, and, while this was the most recent version available, it did not include the newest schools; and (2) foreign institutions were not included in the IPEDS database and thus were not codeable either online or during post-data collection coding procedures. Of the remaining codeable fields, very few literal strings given by respondents were uncodeable. Major, occupation, and industry each had less than 1 percent originally uncodeable. Project staff were successful in upcoding the majority of these initially uncodeable strings. The second step to ensuring data quality was the recoding process. Ten percent of the cases were randomly sampled and their major, occupation, and industry coding results examined. The verbatim strings were evaluated for completeness and for the appropriateness of the assigned codes. Upon review of the string and assigned code, project staff determined whether a different code should be assigned. Table 4.3 shows the results of online coding procedures. Across the entire interview, verbatim strings were recoded for 8 percent of the coding attempts sampled, excluding those which could be upcoded (Table 4.2). The percentage recoded for occupation was higher than expected at 13 percent. Project staff noted that some occupation categories (e.g., \"managers midlevel\" and \"managers supervisory\") were difficult to distinguish. Adding more examples to the descriptions of occupational categories that appear on screen may help to avert this problem in the future.  : 1996: (BPS:1996."}, {"section_title": "CATI Quality Circle Meetings", "text": "Quality circle meetings were a vital component in ensuring data quality and consistency throughout the data collection period. During these regularly scheduled meetings, interviewers, supervisors, and project staff met to discuss issues pertinent to conducting CATI interviews in the most effective manner. Telephone interviewers attended the quality circle meetings on a rotating basis. Helpful tips and summaries of discussions and decisions were prepared and distributed by project staff to all telephone interviewers and their supervisors. Meeting minutes were available both online and in hard copy."}, {"section_title": "42", "text": ""}, {"section_title": "58", "text": "BEST COPY HAMA LE The quality circle meetings were instrumental in providing prompt and precise solutions to problems encountered by the interviewers. Some slight modifications were made to the CATI instrument as a result of these meetings. Examples of issues raised in quality circle meetings included: Revising help text. Help text was modified based on telephone interviewer feedback. Modifications included clarification of definitions and additional information to aid interviewers in coding. Reviewing/entering case-level comments. The importance of reviewing and entering comments pertaining to contacting attempts for each sample member was stressed throughout data collection. Telephone interviewers were encouraged to always check the record of calls to see what happened previously on a particular case. This enabled interviewers to contact the respondent at the appropriate time and telephone number. By entering effective comments, a detailed description of events was created that proved helpful to interviewers who later accessed the case. Problem sheets. Problem sheets were a means for interviewers to report instrument or interviewing problems. Project staff reviewed these problem sheets in order to determine the nature of the problems encountered and resolve them accordingly. Solutions to recurrent problems were addressed in quality circle meetings and in the minutes of these meetings. Coding. Considerable emphasis was placed on properly coding responses. Since most respondents did not provide verbatim responses that exactly matched our response categories, telephone interviewers were instructed on how to fit those responses into the \"best\" possible category. In addition, telephone interviewers and project staff discussed solutions for how best to code items using the online coding system. Changes to the instrument. Telephone interviewers were notified if a change in programming code had to be made to fix a problem with the instrument or supporting screens."}, {"section_title": "E.", "text": ""}, {"section_title": "Quality Control Monitoring", "text": "Monitoring telephone data collection serves a number of goals, all aimed at maintaining a high level of data quality. Monitoring in BPS:1996/2001 helped to meet three important quality objectives: (1) reduction in the number of interviewer errors, (2) improvement in interviewer performance by reinforcement of good interviewing practices, and (3) assessment of the quality of the data being collected. In order to ensure data quality, CATI interviews were evaluated by supervisors using a silent quantitative monitoring system. Monitors listened to and simultaneously viewed the progress of the interview using remote monitoring telephone and computer equipment. Monitors listened to up to 20 questions during an ongoing interview and, for each question, evaluated two aspects of the interviewer-respondent interchange: whether the interviewer (1) delivered the question correctly (error in delivery) and (2) keyed the appropriate response (error in data entry). Measures of question delivery and data entry were developed and daily, weekly, and cumulative reports produced. Monitoring took place during the first 31 weeks of data collection, with a total of 19,962 items monitored during that time. After the 12th week of data collection, monitoring efforts were scaled back due to the lighter caseload being worked by telephone interviewers, the greater experience of the remaining interviewers, and the satisfaction by project staff that the process was proceeding smoothly. Figure 4.1 shows error rates for question delivery; figure 4.2 shows error rates for data entry.2 Both graphs provide upper and lower control limits for these measures.3 The first two weeks reflect the learning curve expected at the start of any study during which interviewers are developing their skills with the instrument. During this time, error rates of up to 2.4 percent were recorded. Throughout the remainder of the monitoring period, error rates remained within acceptable limits, never exceeding 0.6 percent. 2 Weeks 13 through 31 are not shown in the figures due to the low rate of monitoring. Analysis of interviewer behavior based on the few observations from this period is not useful. No errors were recorded after week 11. 3 The upper and lower control limits were defined by three times the standard error of the proportion of errors to the number of questions observed for the period (upper control limit: +3 times the standard error; lower control limit: 3 times the standard error). Week number 9 10 12"}, {"section_title": "6O", "text": "Upper Control Lim it Error Rate ---Lower Control Lim It NOTE: The upper and lower control limits were defined by three times the standard error of the proportion of errors to the number of questions observed for the period (upper control limit: +3 times the standard error; lower control limit: 3 times the standard error). Weeks 13 through 31 are not shown in the figures due to the low rate of monitoring. Analysis of interviewer behavior based on the few observations from this period is not useful. No errors were recorded after week 11. Study: 1996(BPS:1996. Week number  : 1996: (BPS:1996."}, {"section_title": "45", "text": ""}, {"section_title": "61", "text": "1131E \u00a7 1 COPY AVAIIIIABLIE"}, {"section_title": "Chapter 5 Data File Development", "text": "As the third of three interviews with the BPS:96 cohort, the data files for BPS:1996/2001 contain a number of component data files from a variety of sources in addition to those files created from the interview itself. These files are available as a set of restricted research files, fully documented by an Electronic Codebook (ECB), and as a public release Data Analysis System (DAS), which also contains full documentation.' This chapter describes each data file and details the documentation process. A."}, {"section_title": "Overview of the BPS:1996/2001 Data Files", "text": "Data obtained from the BPS:1996/2001 student interview are contained in restricted data files, documented by an ECB, which are available to researchers who have applied for and received authorization from NCES to access restricted research files. Included in the BPS:1996/2001 restricted data are the data files and ECB documentation for eligible first-time beginning students (FTBs) interviewed during the base year interview, NPSAS:96, and for the first follow-up interview, BPS:96/98. 2001 CATI Student Data FileProvides student-level raw data collected from the sample members who responded to the BPS:1996/2001 interview. The file excludes any CATI \"verbatim\" variables, which are on the Verbatim Data File described below. [F01STUD.DAT] 2001 CATI School Data FileProvides institution data obtained from the student interview. Although this is a student-level file, a sample member may have more than one record in the file since there is a separate record for each postsecondary institution attended since the last interview (up to nine institutions). [F01SCH.DAT] 2001 Coding Results FileContains the verbatim text and resulting codes for undergraduate major and, for employed respondents, industry and occupation. "}, {"section_title": "Data Coding and Editing", "text": "The BPS:1996/2001 data were coded and edited using procedures developed and implemented for previous NCES-sponsored studies. These coding and editing procedures were refined during the field test for use in the processing of BPS:1996/2001 full-scale data. The coding and editing procedures fell into two categories: online coding and editing performed during data collection, and post-data-collection editing."}, {"section_title": "Post-Data-Collection Editing", "text": "During and following data collection, the CATI/CAPI data were reviewed to confirm that the data collected reflected the intended skip-pattern relationships. At the conclusion of data collection, special codes were inserted in the database to reflect the different types of missing data. There are a variety of explanations for missing data within individual data elements. Table 5.1 lists the set of special codes used to assist analysts in understanding the nature of missing data associated with BPS:1996/2001 data elements.  : 1996: (BPS:1996. BEST COPY AVAIELA 49 6 Skip-pattern relationships in the database were examined by systematically running cross-tabulations between gate items and their associated nested items. In many instances, gatenest relationships had multiple levels within the CATI/CAPI instrument. That is, items nested within a gate question may themselves have been gate items for additional items. Therefore, validating the gate-nest relationships often required significant iterations and multiway crosstabulations. In some instances, additional across-item consistency checks were performed, although these checks were kept to a minimum since, without recontacting respondents, it was difficult to know which data item was the true source of the inconsistency."}, {"section_title": "steps:", "text": "The data cleaning and editing process for the BPS:1996/2001 consisted of the following Step 1. Review of one-way frequencies for every variable to confirm no missing or blank values. This involved replacing blank or missing data with 9 for all variables in the CATI database and examining frequencies for reasonableness of data values. Step 2. Review of two-way cross-tabulations between each gate-nest combination of variables to check data consistency. This step required using CATI/CAPI source code as specifications to define all gate-nest relationships and replace 9 codes (missing for unknown reason code) with 3 codes (legitimate skip code) as appropriate. Where the two-way cross-tabulations revealed either unusually high numbers of nonreplaced 9 codes, or unusually high numbers of responses for items which should have been skipped, the situation was investigated to ensure skip-pattern integrity. In some instances the inconsistency was due to a backup in the interview that changed the value of the gate question. In other cases resolution involved reprogramming the gate-nest relationship to be consistent with the CATI instrument. In rare instances this check revealed errors in the CATI source code. Step 3. Identifi) and specially code items that were not administered due to a partial or abbreviated interview. This step involved replacing 9 and 3 values with 7 (item not administered) based on the section completion and abbreviated interview indicators. This 7 code, which was used for the first time in BPS:96/98, allows analysts to easily distinguish those items that were not administered to the respondent due to a partial or abbreviated interview from items skipped or left blank unintentionally. Step 4. Identib) items requiring recoding and logical imputations. Standard variable recoding and formatting (e.g., formatting dates as YYYYMM) and standardizing units of time (where an item collected amount of time in a variety of units) were performed during this step. Logical imputations were performed where items were missing but their values could be implicitly determined. For instance, if the respondent did not work in 2000, then the amount earned was imputed to $0 rather than 3 or 9. Items that were skipped because the respondent did not answer the gate question (don't know or refusal) were imputed to the value of the gate question (-1 or 2). Step 5. Identify out-of-range or outlier values. One-way frequencies for all categorical variables and descriptive statistics for all continuous variables were examined. Values determined to be out-of-range or unreasonable were replaced with 6. Concurrent with the data cleaning process, detailed documentation was developed to describe question text, response options, logical imputations, recoding, and the \"applies to\" text for each delivered variable."}, {"section_title": "BPS:1996/2001 Descriptive Report", "text": "The descriptive report, a separate BPS:1996/2001 publication, documents some of the significant results from the longitudinal data collection. It includes an essay on persistence and attainment at 4-year institutions and a table compendium updating key variables for student characteristics, education and employment experiences, finances, and civic participation created using the BPS:1996 Development of statistical analysis weights for the BPS:1996/2001 sample is discussed in section A below. Cross-sectional weights were constructed for analyzing the respondents to BPS:1996/2001. In addition, two longitudinal weights were constructed, one for analyzing the students who participated in all three interviewsNPSAS:96, BPS:96/98, andBPS:1996/2001 and another for analyzing the students who responded to NPSAS:96 andBPS:1996/2001. Analysis procedures that can be used to produce design-unbiased estimates of sampling variances are discussed in section B, including variances computed using Taylor series and balanced repeated replications (BRR) techniques. Section C discusses the accuracy of BPS:1996/2001 estimates in terms of both precision and potential for bias. This section includes survey design effect tables that illustrate the level of precision achieved by the BPS:1996/2001 survey for key analytic outcomes for several important analysis domains. Finally, section D gives weighted response rates. A."}, {"section_title": "Analysis Weights", "text": "The initial file for the BPS:1996/2001 sample contained approximately 10,300 BPS:96/98 respondents and 1,800 BPS:96/98 nonrespondents. As noted in chapter 2, the final BPS:1996/2001 sample consisted of the eligible respondents to BPS:96/98 and a subsample of nonrespondents to BPS:96/98 who were NPSAS:96 respondents. Among these, over 20 were identified as deceased either prior to data collection and after data collection began. A statistical analysis weight was computed to be used for analyzing data from the BPS:1996/2001 respondents. In addition, two longitudinal weights were computed: a weight for analyzing those BPS:1996/2001 respondents who also responded to NPSAS:96 and BPS:96/98, and a weight for analyzing the BPS:1996/2001 respondents who only responded to NPSAS:96 andBPS:1996/2001. The weights for the BPS:96/98 respondents were constructed by applying a series of adjustments for subsampling and nonresponse to the base weights for the 2001 follow-up of the BPS:96 cohort, namely BOHAWTI. Specifically, four adjustments were made: The rationale for the variable name \"BOIIAWT\" is the following:"}, {"section_title": "53", "text": ",6 7 to account for subsampling of the BPS:96/98 nonrespondents; to account for those not located; . to account for refusals, among those who were located; and to account for types of nonresponse other than refusals among those who were located and did not refuse. Construction of the longitudinal weight for those who responded to all three surveys consisted of an additional adjustment for nonresponse to either NPSAS:96 or BPS:96/98. Construction of the analysis weight for those who responded to both NPSAS:96 and BPS:1996/2001, but not to BPS:96/98, consisted of an additional adjustment for nonresponse to NPSAS:96."}, {"section_title": "ADA =", "text": "The weight was calculated as:"}, {"section_title": "B01_100U", "text": "= BOlIAWT * ADJ1, for students in the BPS:96/98 nonrespondent subsample = BOlIAWT for all other students. The weights B01_100U for the students in the subsample were then adjusted so that they summed to the weight sum of BOIIAWT for the BPS:96/98 nonrespondents. This adjustment resulted in the initial sampling weight for the BPS:1996/2001 sample, which is denoted B01 100. B01_100 was further adjusted to produce the BPS:1996/2001 analysis weights, as described below. The weight B01_100 is nonnegative for both the eligible and ineligible (i.e., deceased) students. Weighted response rate tables later in this chapter were computed using B01_100 and were based on the set of eligible students. The eligible students are those with BO1ELIG=1 where BO1ELIG is the eligibility indicator for BPS:1996BPS: /2001 denotes the BPS survey 01 denotes the year 2001 I stands for \"initial\" A stands for \"analysis\" WT stands for \"weight\" 54 2. BPS:1996 Analysis weights were constructed for the respondents to BPS:1996/2001. The weights were constructed by applying adjustments to the base weight B01_100. This section describes each of the adjustment steps, the variables considered for the adjustments, and the variables in the final weight adjustment models. The adjustment for nonresponse was performed in three steps because the predictors of response propensity were potentially different for each of the following outcomes: inability to locate the student, refusal to be interviewed, and other noninterview. Using these three steps of nonresponse adjustment achieved greater reduction in nonresponse bias to the extent that different variables were significant predictors of nonresponse propensity at each step. All nonresponse adjustments were fit using RTI's proprietary generalized exponential modeling procedure (GEM2), which is similar to logistic modeling using bounds for adjustment factors. A key feature and advantage of the GEM software is that the nonresponse adjustment and weight trimming and smoothing are all accomplished in one step. Lower and upper bounds are set on the weight adjustment factors. The bounds can be varied, depending on whether the weight falls inside or outside a range, such as one defined by the bounds (median 3 times the interquartile range, median + 3 times the interquartile range). This allows different bounds to be set for adjustments for weights that are considered high extreme, low extreme, or nonextreme. In this way, the extreme weights can be controlled and the design effect due to unequal weighting reduced. Candidate predictor variables selected were those thought to be predictive of nonresponse and nonmissing for most of the sample (nonrespondents as well as respondents). The candidate predictor variables included age (categorical); typical age for a beginning student (yes or no); race/ethnicity; gender; citizenship status in the base year; attendance status in the base year; level of institution attended in the base year; control of institution attended in the base year; region of institution attended in the base year; size of institution attended in the base year (categorical); applied for financial aid in the base year (yes or no); receipt of federal aid in the base year (yes or no); receipt of Pell Grant in the base year (yes or no); receipt of Stafford Loan in the base year (yes or no); receipt of state aid in the base year (yes or no); receipt of institutional aid in the base year (yes or no); receipt of any aid in the base year (yes or no); previous response status (whether the student was a respondent to both NPSAS:96 and BPS:96/98 versus a nonrespondent to either NPSAS:96 or BPS:96/98); income of independent students and parents of dependent students (collapsed); parents' highest educational attainment; degree completion status in 1998; number of telephone numbers available; number of times an answering machine was encountered (for located students); and whether the student was in a field cluster. To detect important interactions for the logistic models, a Chi-squared automatic interaction detection analysis (CHAID) was performed on the predictor variables. The CHAID analysis divided the data into segments that differed with respect to the response variable (located, did not refuse, or respondent, depending on the model). The segmentation process first divided the sample into groups based on categories of the most significant predictor of response. It then split each of these groups into smaller subgroups based on other predictor variables. It also merged categories of a variable that were found to be nonsignificant. This splitting and merging process continued until no more statistically significant predictors were found (or until some other stopping rule was met). The interactions from the final CHAID segments were then defined. The nonresponse bias for these same variables was estimated, and then a statistical test of whether or not the bias was significant was performed. Tests were performed to identify significant differences between refusal conversions and other respondents; significant differences suggest a potential for nonresponse bias because of the refusal population being different from the other respondents. Additional tests were performed to detect significant differences between late respondents and other respondents; significant differences would suggest a potential for nonresponse bias because of the noncontacts/late-contact population being different from the other respondents. Results and further details of these analyses are given below in section C. The interaction segments and all the main effects were then subjected to variable screening in the GEM logistic procedure. Variables with significant bias were included in each nonresponse model. The models initially included all of the potentially important variables. The interaction segments identified by CHAID were also retained in all of the models. The most nonsignificant variables were deleted sequentially until the deletion of additional variables did not appreciably improve the unequal weighting effect (UWE). Different bounds on the weight adjustments, depending on whether the weight is classified as high extreme, nonextreme, or low extreme, were applied within the NPSAS:96 institutional sampling strata to accomplish nonresponse adjustment, truncation, and smoothing in one step. A large numberof predictor variables in each nonresponse model were kept. This allows the estimates to be calibrated based on the respondents to as many totals as possible that are known for both respondents and nonrespondents. a."}, {"section_title": "Weight Adjustment for Nonrespondents Who Were Not Located", "text": "Of the individuals eligible for the BPS:1996/2001 sample, 92 percent was contacted. An adjustment was performed to the weight B01_100 to adjust for the remaining sample members who did not respond because they were not located. As described above, a CHAID analysis was performed on all of the predictor variables to detect important interactions. All potentially important variables were included in the model. Highly nonsignificant variables were deleted from the model until there was little change in the unequal weighting effect. Table 6.1 presents the final predictor variables used in the logistic model that adjusted the weights for those who were not located, and gives the weighted location rate and the average weight adjustment factors resulting from these variables. The weighting adjustment factor for student j was the reciprocal of the predicted response probability, or ADJ2J = 1/ pL j. The weight, adjusted for those who were not located, was then computed as LOCWT = B01 _ 100 * AJD2 for those who were located = 0 otherwise. b."}, {"section_title": "Weight Adjustment for Nonrespondents Who Refused", "text": "Of the sample members who were eligible and located for the BPS:1996/2001 sample, 3 percent refused. An additional adjustment was performed to the weight that had been adjusted for the not located, LOCWT, to adjust for those who refused. As in the case of the adjustment for the not located, a CHAID analysis was performed on all of the predictor variables to detect important interactions. All potentially important variables were included in the model. Highly insignificant variables were deleted from the model until there was little change in the unequal weighting effect. Table 6.2 presents the final predictor variables used in the logistic model that adjusted the weights for those who refused and gives the weighted nonrefusal rate (for those who were located) and the average weight adjustment factors resulting from these variables. The weighting adjustment factor for student j was the reciprocal of the predicted response probability, or ADJ3i = r NRefj. The weight adjusted for those who refused was computed NREFWT = LOCWT * ADJ3 for those who did not refuse = 0 otherwise. "}, {"section_title": "Attendance status", "text": "Full-time/full year 1 institution 92.7 1.42 Full-time/full year more than 1 institution 96.1 1.05 Full-time/part year 79.8"}, {"section_title": "1.59", "text": "Part-time/full year 1 institution 88.6"}, {"section_title": "1.57", "text": "Part-time/full year more than I institution 60.4"}, {"section_title": "1.55", "text": "Part-time/part year 91.8 58 72 BEST COPY AVAILAIng 59 7 BEST COPY AVAILABLE  : 1996: (BPS:1996.  8 = Prior respondent, 4 or more telephone numbers available, 4-year institution 97.9 1.02 9 = Prior respondent, 4 or more telephone numbers available, 2-year or less-than-96.7 1.04 2-year institution NOTE: Predictor variables are from base year data (NPSAS:96) with the exception of degree status in 1998, telephone numbers available, number of times answering machine was encountered, in field cluster, and certain interaction variables. The weight used is LOCWT. SOURCE: U.S. Department of Education, National Center for Education Statistics, Beginning Postsecondary Students Longitudinal Study: 1996(BPS:1996."}, {"section_title": "77", "text": "c."}, {"section_title": "Weight Adjustments for Located Nonrespondents Who Were Not Refusals", "text": "Of the 9,259 who were eligible, located, and did not refuse, 9,132 responded to the BPS:1996/2001 survey and the remaining 127 did not respond for reasons other than refusal. Next, an adjustment was made to NREFWT to adjust for these 127. As in the case of the other adjustments, a CHAID analysis was performed on all of the predictor variables to detect important interactions. All potentially important variables were included in the model. Highly insignificant variables were deleted from the model until there was little change in the unequal weighting effect. Table 6.3 presents the final predictor variables used in the logistic model that adjusted the weights for those who were interviewed, and gives the weighted interview rate (for those who were located and did not refuse) and the average weight adjustment factors resulting from these variables. The weighting adjustment factor for student j was the reciprocal of the predicted response probability, or ADJ4J = 1/pRd and the weight was computed as: BOlAWT = NREFWT * ADJ4 for the 9,132 who responded, and = 0 otherwise. This final weight was rounded to the nearest integer and is denoted by BOlAWT. This weight is to be used for analyzing the data collected from the 9,132 responses to BPS:1996BPS: /2001 3."}, {"section_title": "Longitudinal Analysis Weights", "text": "Two longitudinal weights were constructed: one weight (B01LWT1) was computed for the 8,934 eligible NPSAS:96 sample members who responded to all three rounds of the survey (i.e., responded to NPSAS:96, BPS:96/98, andBPS:1996/2001); and the second weight (B01LWT2) was computed for the 8,999 eligible NPSAS:96 sample members who responded to both BPS:1996/2001 and NPSAS:96. These two weights were each constructed by applying additional nonresponse adjustments to the final BPS:1996/2001 cross-sectional weight (i.e., BOlAWT).  NOTE: Predictor variables are from base year data (NPSAS:96) with the exception of prior respondent, degree status in 1998, telephone numbers available, number of times answering machine was encountered, and in field cluster."}, {"section_title": "J8", "text": "The weight used is NREFWT. SOURCE: U.S. Department of Education, national Center for Education Statistics, Beginning Postsecondary Students Longitudinal Study: 1996(BPS:1996. The weight for analyzing respondents to all three surveys, NPSAS:96, BPS:96/98, and BPS:1996/2001, was constructed by applying an additional nonresponse adjustment to the final unrounded BPS:1996/2001 cross-sectional weight (unrounded version of BOlAWT). As for the other models, CHAID was used to determine the interaction segments, then the GEM modeling procedure was used to determine the adjustment factor. Table 6.4 presents the final predictor variables used in the logistic model that adjusted the weights for those who were not also interviewed in both NPSAS:96 and BPS:96/98 and gives the weighted interview rate (for those who were interviewed in BPS:1996BPS: /2001 and the average weight adjustment factors resulting from these variables. The final weight was rounded to integer values, and is denoted as BOlLWT1. where Specifically, BOlLWT1 = BOlAWT * ADJ5 for those who responded to all three surveys, and = 0 otherwise, ADJ5 = 1/P96-98-01j is the reciprocal of the predicted response probability. The weight for analyzing respondents to both NPSAS:96 and BPS:1996/2001 was also constructed by applying an additional nonresponse adjustment to the final BPS:1996/2001 crosssectional weight, following the same steps as for the other adjustments. Table 6.5 presents the final predictor variables used in the model and the weighted response rates and adjustment factors. The final weight was rounded to integer values and is denoted as BOlLWT2. Specifically, BO1LWT2 = BOlAWT * ADJ6 for those who responded to both NPSAS:96 and BPS:1996/2001, and = 0 otherwise, where ADJ6 = 1/p96-o1j is the reciprocal of the predicted response probability. The distributions of the weight adjustment factors for the BPS:1996/2001 analysis weights and the two longitudinal weights are presented in table 6.6. Table 6.7 presents the distributions of the initial, intermediate, and final weights along with their unequal weighting design effects.  : 1996: (BPS:1996."}, {"section_title": "70", "text": "8 4  : 1996: (BPS:1996."}, {"section_title": "72", "text": "BEST COPY AVAILABLE  : 1996: (BPS:1996.  : 1996: (BPS:1996."}]