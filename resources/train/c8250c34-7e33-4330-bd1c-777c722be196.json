[{"section_title": "Abstract", "text": "Alzheimer's Disease (AD) is the most common type of dementia. In all leading countries, it is one of the primary reasons of death in senior citizens. Currently, it is diagnosed by calculating the MSME score and by the manual study of MRI Scan. Also, different machine learning methods are utilized for automatic diagnosis but existing has some limitations in terms of accuracy. In this paper, we have proposed some novel preprocessing techniques that have significantly increased the accuracy and at the same time decreased the training time of various classification algorithms. First, we have converted the ADNI dataset which was in 4D format into 2D form. We have also mitigated the computation costs by reducing the parameters of the input dataset while preserving important and relevant data. We have achieved this by using different preprocessing steps like grayscale image conversion, Histogram equalization and selective clipping of dataset. We observed a highest accuracy of 97.52% and a sensitivity of 97.6% in our testing dataset."}, {"section_title": "INTRODUCTION", "text": "Alzheimer's disease is a neurodegenerative disease that leads to gradual memory loss in human beings. In most cases, Alzheimer's disease leads to dementia [1] . Although there is no definite and effective cure for Alzheimer's disease, early detection and treatment can reduce the severity of symptoms which can alleviate the sufferings of the patient. Medications and treatments are most effective when the disease is detected in its early stages. For instance, early treatment can slow down memory loss for some period. [2] . Alzheimer's report two common abnormalities in the brain of this patient, \"1. Dense layers of protein deposited outside and between the nerve cells. 2. Areas of damaged nerve fibers, inside the nerve cells, which instead of being directly had become tangled\". Moreover, these plaques and tangles have been used to help diagnose AD [3]. To overcome these problems, Diagnosing Alzheimer's needs careful medical assessment, as well as patient records, mental state examination (MMSE), and many neurobiological and physical examinations[4]. Besides, s-MRI (structural magnetic resonance imaging) and rs-fMRI (resting-state functional magnetic resonance imaging) is the most common method of analyzing the regular changes, different activities in the brain [5]. , present a paper, they present a categorization structure to accurately utilize the complementarity in the different input dataset. Features from many modalities are then collected using a nonlinear graph mixture process, which produces a fused graph for final classification. Using these fused graphs, they got a classification area under the curve (AUC) of the receiver-operator attribute of 98.1% between normal controls (NC) images and AD images, 82.4% between MCI images and NC images and 77.9% in the overall classification. Tijn M. Schouten, et al. [9] , uses different techniques to take out features from the diffusion Magnetic Resonance Images. First, they use the pixel-wise distribution tensor calculations that have been frame worked using region-based spatial statistics. Second, they clustered the pixel-wise distribution calculations using ICA and they calculated the fusion of these features. Here, Table 1 contains the major issues covered by different researchers on the ADNI dataset, where we include the methods applied by different authors on the same dataset and what accuracy they got after these methods. "}, {"section_title": "LITERATURE REVIEW", "text": ""}, {"section_title": "METHODOLOGY", "text": ""}, {"section_title": "Dataset Description", "text": "In this paper, we acquired data from Alzheimer's Disease Neurological Initiative (ADNI). The ADNI image dataset contains 1917 images belonging to AD (Alzheimer's Disease) class and 1775 images belonging to NL (Normal person) class. Thus the total images are 3692."}, {"section_title": "Dataset preprocessing and Training", "text": ""}, {"section_title": "Image conversion:", "text": "Input: List of 3D Images Output: List of 2D Images\nThe dataset consists of various cross-section layers of brain MRI scans of the healthy and diseased person. In this paper, we have tried to modify the dataset. For the maximum efficiency of our model, we have clipped the dataset for the crosssectionswhich are nearer to the edge of the skull. The images located near the edge of the skull are not suitable for extracting any meaningful features that can help the network. Hence, these images were clipped from the training and testing dataset. We have further improved our dataset by modulating the pixel intensities using the Histogram Equivalence method. It helped us to intensify the overall contrast of the dataset, hence assisting in extracting the features of the images. The number of parameters in the dataset was reduced by converting the RGB images into grayscale versions. Reducing parameters won't result in any significant data loss since the images are already present in Black and White format. Since the input dataset consists of images in black and white.\nThe detailed methodology is described in the Figure-3 below. "}, {"section_title": "RESULTS AND CONCLUSION", "text": "The models were created and trained on Kaggle kernels. The model was trained for a 40 epochs. We calculated the accuracy and sensitivity for assessing the correctness of our models. Also, the confusion matrix was calculated as shown in Figure for further assessment. The best test results produced an Accuracy of 97.52% and a sensitivity of 97.60%.The test accuracy and test loss was also plotted as shown in table 2. Table 3 presents the calculation of computation time of these algorithms. From figure-5, we can observe that the accuracy of 97.52% can be achieved using simple CNN model with 2 convolution layer. So we can easily conclude that rather than applying more complex models on ADNI dataset we can go with very simple CNN model after applying strong pre-processing like histogram equalization, segmentation, type conversion etc. With the help of data preprocessing, we were able to reduce the model complexity by reducing the number of parameters and hence, with the same amount of data, we were not only able to get a better accuracy but also reduced the training time. Our method can also be used in similar applications having insufficient amount of training data."}]