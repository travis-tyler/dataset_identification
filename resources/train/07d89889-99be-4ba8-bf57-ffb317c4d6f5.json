[{"section_title": "Abstract", "text": "Abstract-Recently, a sparse inverse covariance estimation (SICE) technique has been employed to model functional brain connectivity. The inverse covariance matrix (SICE matrix in short) estimated for each subject is used as a representation of brain connectivity to discriminate Alzheimers disease from normal controls. However, we observed that direct use of the SICE matrix does not necessarily give satisfying discrimination, due to its high dimensionality and the scarcity of training subjects. Looking into this problem, we argue that the intrinsic dimensionality of these SICE matrices shall be much lower, considering 1) an SICE matrix resides on a Riemannian manifold of symmetric positive definiteness matrices, and 2) human brains share common patterns of connectivity across subjects. Therefore, we propose to employ manifold-based similarity measures and kernel-based PCA to extract principal connectivity components as a compact representation of brain network. Moreover, to cater for the requirement of both discrimination and interpretation in neuroimage analysis, we develop a novel preimage estimation algorithm to make the obtained connectivity components anatomically interpretable. To verify the efficacy of our method and gain insights into SICE-based brain networks, we conduct extensive experimental study on synthetic data and real rs-fMRI data from the ADNI dataset. Our method outperforms the comparable methods and improves the classification accuracy significantly.\nIndex Terms-Alzheimer's disease (AD) classification, brain network, kernel principal component analysis (PCA), preimage estimation, rs-fMRI, symmetric positive definite (SPD) kernel."}, {"section_title": "", "text": "regions of AD patients differs from that of normal aging. For example, compared with the healthy, AD patients have been found decreased functional connectivity between hippocampus and other brain regions, and MCI patients have been observed increased functional connectivity between the frontal lobe and other brain regions [4] . Therefore, detecting these abnormal alterations in functional connectivity of AD can bring significant benefits in identifying novel connectivity-based biomarkers to improve the diagnosis confidence and revealing the mechanism of AD to help the development of therapies.\nConstructing and classifying functional brain networks based on resting-state functional magnetic resonance imaging (rsfMRI) [5] holds great promise for functional connectivity analysis [6] , [7] . rs-fMRI focuses on the low-frequency (< 0.1 Hz) oscillations of blood-oxygen-level-dependent signal, which presents the underlying neuronal activation patterns of brain regions [8] [9] [10] . Many methods have been proposed to model brain connectivity based on the covarying patterns of rs-fMRI time series across brain regions. Two issues are generally involved: identifying network nodes and inferring the functional connectivity between nodes. The network nodes are often defined as anatomically separated brain regions of interest (ROIs) or alternatively as latent components in some data-driven methods, e.g., independent component analysis [11] , [12] , and clustering-based methods [13] , [14] . Given a set of network nodes, the functional connectivity between two nodes is conventionally measured by the correlation coefficient of time series associated with the two nodes (e.g., the averaged time series from all voxels within a node) [15] [16] [17] , and the brain network is then represented by a correlation matrix.\nHowever, it has been argued that partial correlation could be a better choice since it measures the correlation of two nodes by regressing out the effects from all other nodes [18] . This often results in a more accurate estimate of network structure in comparison with those correlation-based methods. Sparse inverse covariance estimation (SICE) is a principled method for partial correlation estimation, which often produces a stable estimation with the help of the sparsity regularization [19] . The result of SICE is an inverse covariance matrix, and each of its off-diagonal entries indicates the partial correlation between two nodes. It has been widely used to model functional brain connectivity in [20] [21] [22] . For brevity, we call it \"SICE matrix\" throughout this paper.\nSICE matrices can be used as a representation to classify brain networks. A direct approach could be to vectorize each SICE matrix into a feature vector, as in [16] . However, when using it to train a classifier to separate AD from normal controls (NC), the problem of \"the curse of dimensionality\" arises since the dimensionality of the vector (at the order of d \u00d7 d 1 for a network with d nodes, for example, d = 90 in our study) is usually much larger than the number of training subjects, which is often only tens for each class. This usually leads to poor performance of classification. An alternative approach is to summarize a d \u00d7 d SICE matrix into lower dimensional graphical features, such as local clustering coefficient (LCC) [17] or hubs [23] . Nevertheless, these approaches have the risk of losing useful information contained in the SICE matrices. This paper aims to address the high dimensionality issue of these SICE matrices by extracting compact representation for classification.\nAs an inverse covariance matrix, an SICE matrix is symmetric positive definite (SPD). This inherent property restricts SICE matrices to a lower dimensional Riemannian manifold rather than the full d \u00d7 d dimensional Euclidean space. In medical image analysis, the concept of Riemannian manifold has been widely used for DTI analysis [24] , shape statistics [25] , and functional-connectivity detection [6] . Moreover, considering the fact that brain connectivity patterns are specific and generally similar across different subjects, the SICE matrices representing the brain connectivity should concentrate on an even smaller subset of this manifold. In other words, the intrinsic degree of freedom of these SICE matrices shall be much lower than the apparent dimensions of d \u00d7 d. These two factors motivate us to seek a compact representation that better reflects the underlying distribution of the SICE matrices.\nPrincipal component analysis (PCA), the commonly used unsupervised dimensionality reduction method, is a natural option for this task. However, a linear PCA is not expected to work well for manifold-constrained SICE matrices. Recently, advances have been made on measuring the similarity of SPD matrices considering the underlying manifold that they reside. In particular, a set of SPD kernels, e.g., Stein kernel [26] and Log-Euclidean kernel [27] , have been proposed with promising applications [28] , [29] . These kernels implicitly embed the Riemannian manifold of SPD matrices to a kernel-induced feature space F. They offer better measure than their counterparts in Euclidean spaces and require less computation than Riemannian metric, as detailed in [26] . In this paper, we take advantage of these kernels to conduct a SPD-kernel-based PCA. This provides two advantages: 1) It produces a compact representation that can mitigate the curse of dimensionality and, thus, improves classification.\n2) The extracted leading eigenvectors in F can reveal the intrinsic structure of the SICE matrices and, hence, assist brain network analysis.\nWhile our approach introduced above could significantly improve the classification accuracy, another problem arises: how to interpret the obtained compact representation anatomically, or more specifically, can we visualize the principal connectivity components identified by a SPD-kernel PCA? This is important in neuroimage analysis, as it could possibly help to reveal the disease mechanisms behind. Since SPD-kernel PCA is implicitly carried out in the kernel-induced feature space F, the ex- 1 To be precise, the dimensionality of the vector is\nbecause the SICE matrix is symmetric and its diagonal entries are not used. tracted eigenvectors in F are not explicitly known and, therefore, cannot be readily used for anatomical analysis. A kernel preimage method has to be employed to recover these eigenvectors in the original input space. However, estimating the preimages of an object in F is challenging. Existing preimage methods [30] , [31] require the knowledge of an explicit distance mapping between an input space and the feature space F. Unfortunately, such an explicit distance mapping is intractable for SPD kernels and, thus, the existing preimage methods cannot be applied to our case. To solve this problem, we further propose a novel preimage method for the SPD kernels and use it to gain insight into SICE-based brain network analysis.\nTo verify our approach, we conduct an extensive experimental study on both synthetic dataset and rs-fMRI data from the benchmark dataset ADNI. 2 As will be seen, the results well demonstrate the effectiveness and advantages of our method. Specifically, the proposed compact representation obtained via the SPD-kernel PCA achieves superior classification performance to that from linear PCA and the graphical feature LCC. Also, the proposed preimage method can effectively recover in the original input space the principal connectivity components identified in a feature space and enables the visualization and anatomical analysis of these components.\nIn addition, we would like to point out that besides SICE matrices, the proposed method can be seamlessly applied to the correlation matrices previously mentioned, because they are also SPD. We focus on SICE matrices in this paper because SICE matrices model the partial correlations, which enjoy theoretical advantages, and generally admit more stable connectivity in comparison with correlation [32] . This paper is an significant extension of our previous work reported in a workshop paper [33] . The extension is made in three aspects: 1) More SPD kernels are investigated in this version. As demonstrated, different SPD kernels consistently achieve superior classification performance, which indicates the generality of the proposed method; 2) new experiments are conducted on a specifically designed synthetic data to show the characteristics of the proposed preimage method and its effectiveness; 3) in addition to the k-nearest neighbor (k-NN) classifier, this version includes support vector machines (SVM) as a classifier to evaluate the classification performance.\nThe rest of the paper is organized as follows. Section II reviews the SICE algorithm and the manifold structure of SPD matrices. Section III details the proposed SPD-kernel PCA and the preimage method. Section IV presents the experimental results on synthetic and real rs-fMRI datasets, and finally, Section V concludes this paper."}, {"section_title": "II. RELATED WORK", "text": ""}, {"section_title": "A. Constructing Brain Network Using SICE", "text": "Let {x 1 , x 2 , . . . , x M } be a time series of length M , where x i is a d-dimensional vector, corresponding to an observation of d brain nodes. Following the literature of SICE [19] , [21] , x i is forms a closed, self-dual convex cone, which is a Riemannian manifold in the Euclidean space R d \u00d7d [26] . (b) To measure the distance between two SICE matrices A and B, Euclidean distance is not accurate since it does not consider the special geometry of the manifold structure. Instead, geodesic distance, which is defined as the shortest curve connecting A and B on the manifold, is more accurate.\nassumed to follow a Gaussian distribution N (\u03bc, \u03a3). Each offdiagonal entry of \u03a3 \u22121 indicates the partial correlation between two nodes by eliminating the effect of all other nodes. \u03a3 \u22121 ij will be zero if nodes i and j are independent of each other when conditioned on the other nodes. In this sense, \u03a3 \u22121 ij can be interpreted as the existence and strength of the connectivity between nodes i and j. The estimation of S = \u03a3 \u22121 can be obtained by maximizing the penalized log-likelihood over positive definite matrix S (S 0) [19] , [21] :\nwhere C is the sample-based covariance matrix; det(\u00b7), tr(\u00b7), and || \u00b7 || 1 denote the determinant, trace, and the sum of the absolute values of the entries of a matrix. ||S|| 1 imposes sparsity on S to achieve more reliable estimation by considering the fact that a brain region often has limited direct connections with other brain regions in neurological activities. The tradeoff between the degree of sparsity and the log-likelihood estimation of S is controlled by the regularization parameter \u03bb. Larger \u03bb makes S * more sparse. The maximization problem in (1) can be efficiently solved by the off-the-shelf packages, such as SLEP [34] ."}, {"section_title": "B. SPD Matrices", "text": "The resulting SICE matrix S * obtained by (1) is SPD since it is an estimation of inverse covariance matrix. Let Sym\nAs illustrated in Fig. 1(a) , Sym + d forms a closed self-dual convex cone, which is a Riemannian manifold in the Euclidean space R d\u00d7d [26] . To effectively measure the similarity between two SICE matrices, as in Fig. 1(b) , methods that respect the geodesic distance rather than Euclidean distance should be used [27] . To directly measure the geodesic distance for SPD matrices on the manifold, affine-invariant Riemannian metrics (AIRMs) were proposed in [35] and [24] . However, there are two issues: 1) The computational cost of AIRMs is high because it intensively uses matrix inverse, square roots, and logarithms [27] , [28] ; 2) more importantly, the linear algorithms, e.g., SVM, that are developed in Euclidean spaces cannot be directly applied to SPD matrices lying on a manifold [29] . To address these issues, kernel method [26] , [27] , has been adopted to measure the similarity between SPD matrices. It measures the similarity by implicitly mapping the Riemannian manifold of SPD matrices onto a high-dimensional kernel-induced feature space F, where linear algorithms can be generalized. The manifold structure is well incorporated in the mapping by utilizing distance functions that are specially designed for SPD matrices. Also, kernel methods are often computationally more efficient than AIRMs because the intensive use of matrix inverse, square roots, and logarithms in AIRMs can be avoided or reduced [27] ."}, {"section_title": "III. PROPOSED METHOD", "text": ""}, {"section_title": "A. SICE Representation Using SPD-Kernel Based PCA", "text": "In spite of individual variation, human brains do share common specific connectivity patterns across different subjects. Therefore, the SICE matrices used to represent brain networks shall have similar structures across subjects. This makes them be further restricted into a small subset of the Riemannian manifold of SPD matrices, with a limited degree of freedom. Inspired by this observation, we aim to extract a compact representation of these SICE matrices for better classification and analysis. PCA is a commonly used technique to generate a compact representation of data by exploring a subspace that can best represent the data. Therefore, PCA is a natural choice for our task. However, linear PCA is not expected to work well for the SICE matrices because it does not consider the manifold structure. Consequently, we adopt kernel PCA [37] and integrate SPD kernels for similarity measure. This effectively accounts for the manifold structure of SICE matrices when exploring the subspace of the data. Our method is elaborated as follows.\nThe SICE method is applied to N subjects to obtain a train-\n, where S i is the SICE matrix for the ith subject. We define the kernel mapping \u03a6(\u00b7): Sym + d \u2192 F, which cannot be explicitly solved but implicitly induced by a given SPD kernel. As an extension of PCA, kernel PCA generalizes linear PCA to a kernel-induced feature space F. For the self-containedness of this paper, we briefly describe Kernel PCA as follows and the details can be found in [37] . Without loss of generality, it is assumed that \u03a6(S i ) is centered, i.e., N i=1 \u03a6(S i ) = 0, and as in [37] , this can be easily achieved by simple computation with kernel matrix. Then, a N \u00d7 N kernel matrix K can be obtained with each entry\n. Kernel PCA first performs the eigen decomposition on the kernel matrix: K = U\u039bU . The ith column of U, denoted by u i , corresponds to the ith eigenvector, and \u039b = diag( \u03bb 1 , \u03bb 2 , . . . , \u03bb N ), where \u03bb i corresponds to the ith eigenvalue in a descending order. Let \u03a3 \u03a6 denote the covariance matrix computed by {\u03a6 (S i \nThe ith eigenvector of \u03a3 \u03a6 can be expressed as\nwhere\nAnalogous to linear PCA, for a given SICE matrix S, \u03a6(S) can then be projected onto the top m eigenvectors to obtain an m-dimensional principal component vector\nwhere\nWith the kernel trick, it can be computed as\nwhere\nOnce \u03b1 is obtained as a new representation for each SICE matrix, an SVM or k-NN classifier can be trained on \u03b1 with class labels.\nIn this paper, we study four commonly used SPD kernels, namely, Cholesky kernel (CHK) [29] , power Euclidean kernel (PEK) [29] , Log-Euclidean kernel (LEK) [27] , and Stein kernel (SK) [26] . The four kernels are all in a form of\nwhere d(\u00b7, \u00b7) is a kind of distance between two SPD matrices. Different definitions of d(\u00b7, \u00b7) lead to different kernels, and the distance functions in the four kernels are Cholesky distance [36] , power Euclidean distance [36] , Log-Euclidean distance [27] , and root Stein divergence [26] , respectively. They are introduced as follows."}, {"section_title": "1) Cholesky Distance:", "text": "Cholesky distance measures the difference between S i and S j by\nwhere (S) is a lower triangular matrix with positive diagonal entries obtained by the Cholesky decomposition of S, that is, S = (S)(S) and || \u00b7 || F denotes the Frobenius matrix norm.\n2) Power Euclidean Distance: Power Euclidean distance between S i and S j is given by\nwhere p \u2208 R. Note that S, as a SPD matrix, can be eigen decomposed as S = U\u039bU , and S p can be easily computed by S p = U\u039b p U . In this paper, we set p = 0.5 since it achieves the best result in the literature [29] , [36] and our experiments.\n3) Log-Euclidean Distance: Log-Euclidean distance is defined as\nwhere log(S) = U log(\u039b)U and log(\u039b) applies logarithm to each diagonal element of \u039b to obtain a new diagonal matrix."}, {"section_title": "4) Root Stein Divergence:", "text": "Root Stein divergence is the square root of Stein divergence, which is defined as \n2 , +\u221e) to guarantee SK to be a Mercer kernel [26] .\nThe four distance functions and the corresponding kernels are summarized in Table I . They will be applied to SPD-kernel PCA to produce the principal component vector \u03b1."}, {"section_title": "B. Preimage Estimation", "text": "As will be shown in the experimental study, the principal components \u03b1 extracted by the above SPD-kernel PCA offer promising classification performance. Note that \u03b1 is fundamentally determined by the m leading eigenvectors v 1 , . . . , v m , which capture the underlying structure of SICE matrices and can be deemed as the building blocks of this representation of brain connectivity. Therefore, analyzing these eigenvectors is important for the understanding and interpretation of the obtained principal connectivity patterns. However, the eigenvectors are derived in F via the implicit kernel mapping \u03a6(\u00b7) and, thus, are not readily used for analysis in the input space Sym + d . To tackle this issue, we aim to develop a method that can project a datapoint in the subspace spanned by the m leading eigenvectors in F back to the input space. This will allow the visualization of the principal connectivity patterns in the input space for interpretation. This is known as the \"preimage\" problem of kernel methods in the literature [30] , [31] , [38] . Unfortunately, existing preimage methods, such as those in [30] and [31] , cannot be applied to our case, because they require an explicit mapping between the Euclidean distance in F and the Euclidean distance in the input space, which is unavailable when the SPD kernels are used. In the following, we develop a novel preimage method for the SPD kernels to address this issue.\nLet \u03a6 m (S) denote the projection of \u03a6(S) into the subspace spanned by the m leading eigenvectors in F, that is\nwhere\nOur aim is to find a preimage\u015c in the original input space (that is, Sym Specifically,\u015c is estimated as follows. First, we find a set of\nby sorting 3 Using linear combination of neighbors may restrict the search space of preimage and could affect the reconstruction accuracy. Here, we use it for three reasons: 1) our experiment on synthetic data (with ground truth) has demonstrated good reconstruction result; ii) using linear combination can significantly simplify the optimization problem of preimage estimation; iii) by using linear combination of neighbors, we can better enforce the constructed preimage to follow the underlying distribution of training samples. \n, +\u221e SK the following distance:\nThis distance can be easily computed because it is fully represented by the kernel functions. Second, we model the preimage\u015c by a convex (linear) combination of its neighbors a\u015d\nwhere S j \u2208 \u03a9, w j \u2265 0, and L j =1 w j = 1. This convex combination guarantees the SPD of\u015c and also makes it be effectively constrained by its L neighbors. Defining w = [w 1 , w 2 , . . . , w L ] , we seek the optimal w by solving\nwhere\nby applying (10) and (11) . This optimization problem can be efficiently solved using gradient descent based algorithms. Note that (12) can be used to compute the preimage of any datapoint \u03a6 m (S) in F. In addition, when estimating the preimage of a specific eigenvector v i , we can simply set \u03a6 m (S) as v i and solve the same optimization problem in (12) . In this case, the objective function reduces to\nAlgorithm 1 outlines the proposed preimage algorithm. "}, {"section_title": "IV. EXPERIMENTAL STUDY", "text": ""}, {"section_title": "A. Data Preprocessing and Experimental Settings", "text": "Rs-fMRI data of 196 subjects were downloaded from the ADNI website 4 in June 2013. Nine subjects were discarded due to the corruption of data, and the remaining 187 subjects were preprocessed for analysis. After removing subjects that had problems in the preprocessing steps, such as large head motion, 156 subjects were kept, including 26 AD, 44 early MCI, 38 late MCI, 38 NC, and ten significant memory concern labeled by ADNI. We used the 38 NC and the 44 early MCI in this paper because our focus in this paper is to identify MCI at very early stage, which is the most challenging and significant task in AD prediction. The IDs of the 82 (38 NC and 44 early MCI) subjects are provided in the supplementary material. The data are acquired on a 3-T (Philips) scanner with TR/TE set as 3000/30 3 . The preprocessing is carried out using SPM8 5 and DPARSFA [40] . The first ten volumes of each series are discarded for signal equilibrium. Slice timing, head motion correction, and MNI space normalization are performed. Participants with too much head motion are excluded. The normalized brain images are warped into automatic anatomical labeling (AAL) [41] atlas to obtain 90 ROIs as nodes. By following common practice [15] [16] [17] , the ROI mean time series are extracted by averaging the time series from all voxels within each ROI and then bandpass filtered to obtain multiple subbands as in [17] .\nThe functional connectivity networks of 82 participants are obtained by the SICE method using SLEP [34] , with the sparsity levels of \u03bb = [0.1 : 0.1 : 0.9]. For comparison, constrained sparse linear regression (SLR) [17] is also used to learn functional connectivity networks with the same setting. Functional connectivity networks constructed by SICE and SLR are called \"SICE matrices\" and \"SLR matrices,\" respectively. To make full use of the limited subjects, a leave-one-out procedure is used for training and test. That is, each sample is reserved for test in turn, while the remaining samples are used for training. Both SVM and k-NN are used as the classifier to compare the classification accuracy of different methods. The parameters used in the following classification tasks of this rs-fMRI dataset, including the sparsity level \u03bb, the subband of the time series, the number of eigenvectors m, and the regularization parameter of SVM are tuned by fivefold cross validation on the training set. \u03b8 in all the four SPD kernels is empirically set as 0.5, and the k of k-NN is set as 7."}, {"section_title": "B. Experimental Result", "text": "The experiment consists of three parts: 1) Evaluating the classification performance when the original SICE or SLR matrices are used as the features; 2) evaluating the classification performance when the compact representation of SICE or SLR matrices is used as the features; 3) investigating the effectiveness of the proposed preimage method.\n1) Classification Using Original SICE or SLR Matrices: By applying the SICE or SLR method to the rs-fMRI data, we can obtain the SICE or SLR matrices as the representation of brain networks. These matrices can be directly used as features to train a classifier. A straightforward way is to vectorize the matrices into high-dimensional vectors as features as in [16] , which are then used to train a linear SVM or k-NN with linear kernel 5 http://www.fil.ion.ucl.ac.uk/spm/software/.\nas the similarity measure to search NN to perform classification. Note that linear kernel is Euclidean distance-based similarity measure. As shown in the second and third columns in Table II (labeled by \"linear kernel\"), this method produces poor classification performance (lower than 60%) on both SICE and SLR matrices, be it k-NN or linear SVM is used as the classifier. Specifically, it only achieves 53.7% for the k-NN classifier using SLR matrices. When SICE matrices are used, the classification performance is only 57.3% too. The result does not change much when a linear SVM is used. The poor classification performance of this method is largely due to two issues: 1) The vectorization ignores the underlying structure of SICE matrices, and the linear kernel in SVM and in the k-NN classifier cannot effectively evaluate their similarity and distance; and 2) the \"small sample size\" problem occurs because the dimensionality of the resulting feature vectors is high, while the training samples are limited.\nIn order to effectively consider the manifold geometry of SICE matrices, we employ the four aforementioned SPD kernels to evaluate the similarity between SICE matrices and adopt k-NN and SVM classifiers with these kernels to perform classification. As seen in the columns under \"LEK,\" \"SK,\" \"CHK,\" \"PEK\" in Table II , the classification accuracy with respect to each SPD kernel is above 60%, which clearly outperforms that of their linear counterparts. In particular, PEK obtains 65.9% with SVM as the classifier, achieving an improvement of 8.6 percentage points over linear SVM. This well verifies the importance of considering the manifold structure of SICE matrices for the classification. Note that because SLR matrices are not necessarily SPD, the SPD kernels cannot be applied. Therefore, no classification result is reported in the row of \"SLR\" in Table II ."}, {"section_title": "2) Classification Using the Compact Representation:", "text": "In this experiment, we compare the classification performance of the compact representation obtained by the proposed SPD-kernel PCA, linear PCA, and the method computing LCC [17] . LCC, as a measure of local neighborhood connectivity for a node, is defined as the ratio of the number of existing edges between the neighbors of the node and the number of potential connections between these neighbors [42] . In this case, LCC can map a network, represented by a d \u00d7 d adjacency matrix, to a d-dimensional vector, where d is the number of nodes in the network. Table III shows the classification results when using the compact representation of SICE or SLR matrices using k-NN with Euclidean distance and linear kernel SVM. LCC achieves 65.9% for both SICE and SLR matrices with k-NN as the classifier. It is better than the result (53.7% and 57.3% in the second column of Table II ) of directly using the original matrices, and is comparable to the result (65.9%) of applying PEK-SVM, the best one obtained in Table II . When linear PCA is applied to the vectorized SICE or SLR matrices to extract the top m principal components as features, the classification accuracy increases to 67.1% for both SICE and SLR matrices. This performance is better than LCC and all the methods in Table II . Such a result indicates the power of compact representation and also preliminarily justifies our idea of exploring the lower intrinsic dimensions of the SICE matrices. By further taking the SPD property into account and using the proposed SPD-kernel PCA to extract the compact representation, the classification accuracy is significantly boosted up to 73.2% for both SK-PCA and PEK-PCA, with SVM as the classifier. This achieves an improvement of 4.9 percentage points (73.2% versus 68.3%) over linear PCA and 7.3 percentage points (73.2% versus 65.9%) over LCC. These results well demonstrate that: 1) The obtained compact representation can effectively improve the generalization of the classifier in the case of limited training samples.\n2) It is important to consider the manifold property of SICE matrices in order to obtain better compact representation. Cross referencing the SICE results in Tables II and III , SPD-kernel PCA achieves the best classification performance, i.e., 73.2%, obtaining an improvement of 15.9 percentage points over the linear kernel method (57.3%, in Table II) ."}, {"section_title": "3) Investigating the Proposed Preimage Method:", "text": "The two goals of the preimage method, which is shown in Algorithm 1, is to estimate the preimage of 1) \u03a6 m (S), which is the projection of \u03a6(S) into the m leading eigenvectors in F and 2) one single eigenvector v i of SPD-kernel PCA in F.\nThe motivation of the first goal to recover the preimage of \u03a6 m (S) is inspired by the property of PCA. It is known that projecting data into the m leading eigenvectors discards the minor components, which often correspond to the data noise. Therefore, when an SICE matrix S is contaminated by noise (and it makes \u03a6(S) noisy), \u03a6 m (S) can be regarded as a \"denoised\" version of \u03a6(S). As a result, if the proposed preimage method really works, the recovered preimage shall be closer to the true inverse covariance matrix than S is. In the literature, such a property has been extensively used for data and image denoising [43] .\nThe proposed preimage method is performed on the real rsfMRI data. Here, we aim to investigate if the preimages can boost the classification performance in comparison with the original SICE matrices based on the assumption that the preimage of \u03a6 m (S) can bring some kind of denoising effect. We first estimate the preimages of \u03a6 m (S i ), S i \u2208 {S i } 82 i=1 and redo classification using two methods: 1) Linear kernel method. As what we did in the second column of Table II , k-NN classifier is directly applied to the obtained preimages with linear kernel as the similarity measure; ii) LCC method. As what we did in the second column of Table III , LCC is extracted as a feature from the obtained preimages and apply k-NN classifier to LCC with Euclidean distance. The number of leading eigenvectors m is selected by cross validation from the range of [1 : 5 : 80] on the training set while the number of neighbors L is empirically set as 20. In our experiment, we observe that 1) A larger L will make the optimization significantly more time consuming, while the performance of the method remains similar; 2) the selected value of m is usually in the range of . Table IV shows the classification result on the preimages of\n, obtained on the real rs-fMRI data. The classification performance with the preimages when SK, LEK, and PEK are used can consistently outperform the classification performance with original SICE or SLR matrices using either linear kernel method or LCC method. Specifically, the performance of linear kernel method on SICE matrices is boosted to 68.3% (the fourth column, with preimages when LEK is used) from 57.3% (the third column). We believe that the improvement is due to that, by estimating the preimages of \u03a6 m (S i ) in F, the resulting matrices are more reliable than the original SICE matrices. Recall that the leading eigenvectors v i in F capture the underlying structure of SICE matrices and can be deemed as the building blocks of the representation for brain connectivity. Thus, we estimate the preimage of top eigenvectors v i in F for anatomical analysis. In this experiment, the preimages of the top two eigenvectors, which pose the most significant variance of SICE matrices in F, are visualized in Fig. 2 . The lobe, index, and name of each ROI in AAL [41] atlas are listed in Table V . We observe that: 1) compared with the eigenvectors in linear PCA, the eigenvectors obtained in the SPD-kernel PCA capture richer connection structures. Specifically, as seen from Fig. 2(a) , the first eigenvector in linear PCA only presents very weak intralobe connections in frontal and occipital lobes. In contrast, the first eigenvector obtained by each of the SPD-kernel PCA well captures the intralobe connections in all the lobes. Especially, as indicated in Fig. 2(c) , (e), (g), and (i), there are strong connections at orbitofrontal cortex ( ROI index : 8, 19-22) , rectus gyrus (23, 24) , occipital gyrus (43) (44) (45) (46) (47) (48) , temporal gyri (53-58), hippocampus (65-66), and temporal pole (69-72). Respecting the second eigenvector, the eigenvectors obtained by the SPD-kernels PCA [see Fig. 2(d) , (f), (h), and (j)] incorporate both intralobe and inter-lobe connections while the eigenvector in linear PCA [see Fig. 2(b) ] mainly captures only intraobe connections in occipital lobe; 2) the preimages obtained when different SPD kernels are used, as seen in Fig. 2(c)-(j) , are very similar with each other with slight variation. This is expected since they all reflect the underlying manifold structure of SICE matrices. Further exploration of their clinical interpretation will be included in our future work."}, {"section_title": "C. Evaluation of the Preimage Method Using Synthetic Data", "text": "To further investigate the efficacy of the proposed preimage method, a synthetic dataset is specially designed for evaluation. The synthetic dataset is used for two purposes: 1) It allows the comparison between the recovered preimage of \u03a6 m (\u00b7) and the ground truth inverse covariance matrix, which is not available for real rs-fMRI data; ii) by adjusting the parameters used to generate the synthetic data, the behavior of the proposed preimage method can be demonstrated. The synthetic data are generated by mimicking the following data generation process in practice.\n1) Generate a set of 82 covariance matrices of the size of 90 \u00d7 90, by sampling a Wishart distribution 6 [44] . Let \u03a3 i (i = 1, . . . , 82) be the ith covariance matrix and its inverse \u03a3 \u22121 i will be used as a ground truth inverse covariance matrix; 2) A set of 130 vectors are randomly sampled from each normal distribution N (0, \u03a3 i ), where i = 1, . . . , 82; 3) Gaussian noise is added to each set of 130 vectors to simulate that the data are contaminated. The noise level is denoted by \u03b4; 4) A sample-based covariance matrix C is computed by using each set of the (noisy) 130 vectors and 82 covariance matrices are obtained in total. They are denoted as C 1 , C 1 , . . . , C 82 ; 5) Apply the SICE method to each C i to obtain the SICE matrix, and they are collectively denoted by\n. These SICE matrices form the synthetic dataset. Note that they are affected by the noise added in Step 3. From the synthetic dataset {S i } 82 i=1 , every S i is selected in turn as the test data and the remainder are used as the training set. Algorithm 1 is then applied to estimate the preimage\u015c i for \u03a6 m (S i ). Then, the recovered preimage\u015c i and the test data S i are compared, respectively, with the ground truth inverse covariance matrix \u03a3 i . Given a pair of SPD matrices \u03a3 1 and \u03a3 2 , KL divergence measures the similarity of two Gaussians N (\u03bc 1 , \u03a3 1 ) and N (\u03bc 2 , \u03a3 2 ). It can 6 The Wishart distribution is used as \u03a3 i \u223c W 90 (\u03a3 0 , n), where \u03a3 0 \u2208 Sym + 90 is set as a block-wise covariance matrix for a better illustration of the result, and n is the degree of freedom set as 1000. Fig. 3 . Performance of the proposed preimage method on synthetic dataset. (a) Averaged KL divergence between the ground truth inverse covariance matrix \u03a3 \u22121 and the original SICE matrix S (labeled by \"original\") or the preimages\u015c when four SPD kernels are used (labeled by \"CHK,\" \"PEK,\" \"LEK,\" and \"SK\", respectively) at various noise levels with m and L set as 5 and 20, respectively. As indicated, the resulting KL divergence values corresponding to the four SPD kernels are consistently smaller than K L(\u03a3 \u22121 , S) at all noise levels. Moreover, the improvement of\n, becomes more significant with increase of \u03b4. Note that the KL divergence values corresponding to the four kernels are similar and overlapped in the figure; (b) the iImprovement of the proposed preimage method (using SK) with various number of leading eigenvectors m when L is set as 20, and (c) the improvement of the proposed preimage method (using SK) with various number of neighbors L when m is set as 5. be used to measure the similarity between the two SPD matrices by relating them to the covariance matrices and setting the means as zero. KL divergence in our case is expressed as KL(\u03a3 1 , \u03a3 2 ) = tr(\u03a3 The result is shown in Fig. 3 . As seen in Fig. 3(a) , KL(\u03a3 \u22121 ,\u015c) (averaged over all 82 test cases and with m and L set as 5 and 20, respectively.) is consistently lower than KL(\u03a3 \u22121 , S) for all the different noise levels and the SPD kernels used in the kernel PCA. This result suggests that the obtained preimage\u015c is closer to the ground truth inverse covariance matrix \u03a3 \u22121 in comparison with the original SICE matrix S. Relating back to the idea that we use to design this experiment, this result shows that the proposed preimage method indeed works. Also, the improvement of KL(\u03a3 \u22121 ,\u015c)\nover KL(\u03a3 \u22121 , S), i.e., KL(\u03a3 \u22121 , S) \u2212 KL(\u03a3 \u22121 ,\u015c), becomes more significant with the increase of the noise level \u03b4 introduced in Step 3 of the synthetic data generation process. To demonstrate the result obtained by the proposed pe-image method, an example is given in Fig. 4 , where Fig. 4(a) shows a ground truth inverse covariance matrix \u03a3 \u22121 , Fig. 4(b) plots the estimated SICE matrix S and Fig. 4(c) shows the preimage\u015c of \u03a6 m (S). As seen,\u015c is more similar to \u03a3 \u22121 in comparison with S. As indicated in Algorithm 1, the number of leading eigenvectors m and the number of neighbors L are two important parameters. We evaluate how the performance of the proposed preimage method will change with these two parameters. SK is taken as an example. Fig. 3(b) and (c) shows the improvement, i.e., KL(\u03a3 \u22121 , S) \u2212 KL(\u03a3 \u22121 ,\u015c), of our method with different m and L, respectively. As seen in Fig. 3(b) , when L is set as constant 20, the improvement first increases with m and then decreases, achieving the highest value when m is five. This is because the first several leading eigenvectors v i in F represent the dominant network structures of the network while the following ones intend to characterize more detailed structures which are vulnerable to noise. As a result, with the increasing value of m, the components often correspond to noise. Therefore, when m > 5, noisy components could be included, and this reduces the magnitude of the improvement. At the same time, note that the improvement does consistently hold although its magnitude is reduced. Fig. 3(c) shows that, when m is fixed at 5, the improvement with the increase of L becomes saturated when L = 20. This is because the constraint of L j =1 w j = 1 in\u015c = L j =1 w j S j (11) imposes the sparsity of w j , limiting the actual number of neighbors S j used to estimate\u015c. Based on our experience, a relatively large initial number of L is recommended, e.g., one fourth of the number of training samples, and the constraint of L j =1 w j = 1 will implicitly and automatically select a small set of S j by setting most w j as zero."}, {"section_title": "V. CONCLUSION", "text": "Recently, SICE has been used as a representation of brain connectivity to classify AD and NC. However, its high dimensionality can adversely affect the classification performance. Taking advantage of the SPD property of SICE matrices, we use SPD-kernel PCA to extract principal components to obtain a compact representation for classification. We also propose a preimage estimation algorithm, which allows visualization and analysis of the extracted principal connectivity patterns in the input space. The efficacy of the proposed method is verified by extensive experimental study on synthetic data and real rs-fMRI data from the ADNI.\nIn this paper, we specifically focus on unsupervised learning to explore compact representation without using class label information. Note that our framework can readily be extended to supervised case, such as kernel linear discriminant analysis, to explore discriminative representation. This will be studied in our future work."}]