[{"section_title": "Abstract", "text": "Abstract. In this paper, we propose a framework for automatic classification of patients from multimodal genetic and brain imaging data by optimally combining them. Additive models with unadapted penalties (such as the classical group lasso penalty or 1-multiple kernel learning) treat all modalities in the same manner and can result in undesirable elimination of specific modalities when their contributions are unbalanced. To overcome this limitation, we introduce a multilevel model that combines imaging and genetics and that considers joint effects between these two modalities for diagnosis prediction. Furthermore, we propose a framework allowing to combine several penalties taking into account the structure of the different types of data, such as a group lasso penalty over the genetic modality and a 2-penalty on imaging modalities. Finally, we propose a fast optimization algorithm, based on a proximal gradient method. The model has been evaluated on genetic (single nucleotide polymorphisms -SNP) and imaging (anatomical MRI measures) data from the ADNI database, and compared to additive models [13, 15] . It exhibits good performances in AD diagnosis; and at the same time, reveals relationships between genes, brain regions and the disease status."}, {"section_title": "Introduction", "text": "The research area of imaging genetics studies the association between genetic and brain imaging data [8] . A large number of papers studied the relationship between genetic and neuroimaging data by considering that a phenotype can be explained by a sum of effects from genetic variants. These multivariate approaches use partial least squares [16] , sparse canonical correlation analysis [17] , sparse regularized linear regression with a 1 -penalty [10] , group lasso penalty [12, 11] , or Bayesian model that links genetic variants to imaging regions and imaging regions to the disease status [9] .\nBut another interesting problem is about combining genetic and neuroimaging data for automatic classification of patients. In particular, machine learning methods have been used to build predictors for heterogeneous data, coming from different modalities for brain disease diagnosis, such as Alzheimer's disease (AD) diagnosis. However, challenging issues are high-dimensional data, small number of observations, the heterogeneous nature of data, and the weight for each modality.\nA framework that is commonly used to combine heterogeneous data is multiple kernel learning (MKL) [6] . In MKL, each modality is represented by a kernel (usually a linear kernel). The decision function and weights for the kernel are simultaneously learnt. Moreover, the group lasso [2, 3] is a way to integrate structure inside data. However, the standard 1 -MKL and group lasso may eliminate modalities that have a weak contribution. In particular, for AD, imaging data already provides good results for its diagnosis. To overcome this problem, different papers have proposed to use a 1,p -penalty [7] to combine optimally different modalities [13, 14] .\nThese approaches do not consider potential effects between genetic and imaging data for diagnosis prediction, as they only capture brain regions and SNPs separately taken. Moreover, they put on the same level genetic and imaging data, although these data do not provide the same type of information: given only APOE genotyping, subjects can be classified according to their risk to develop AD in the future; on the contrary, imaging data provides a photography of the subject's state at the present time.\nThereby, we propose a new framework that makes hierarchical the parameters and considers interactions between genetic and imaging data for AD diagnosis. We started with the idea that learning AD diagnosis from imaging data already provides good results. Then, we considered that the decision function parameters learnt from imaging data could be modulated, depending on each subject's genetic data. In other words, genes would express themselves through these parameters. Considering a linear regression that links these parameters and the genetic data, it leads to a multilevel model between imaging and genetics. Our method also proposes potential relations between genetic and imaging variables, if both of them are simultaneously related to AD. This approach is different from the modeling proposed by [9] , where imaging variables are predicted from genetic variables, and diagnosis is predicted from imaging variables.\nFurthermore, current approaches [13, 14, 15] do not exploit data structure inside each modality, as it is logical to group SNPs by genes, to expect sparsity between genes (all genes are not linked to AD) and to enforce a smooth regularization over brain regions for imaging modality. Thus, we have imposed specific penalties for each modality by using a 2 -penalty on the imaging modality, and a group lasso penalty over the genetic modality. It models the mapping of variants into genes, providing a better understanding of the role of genes in AD.\nTo learn all the decision function parameters, a fast optimization algorithm, based on a proximal gradient method, has been developed. Finally, we have evaluated our model on 1,107 genetic (SNP) and 114 imaging (anatomical MRI measures) variables from the ADNI database 1 and compared it to additive models [13, 15] ."}, {"section_title": "Model set-up", "text": ""}, {"section_title": "Multilevel Logistic Regression with Structured Penalties", "text": ", and x k I \u2208 R |I| (imaging data) and y k \u2208 {0, 1} (diagnosis). Genetic, imaging and genetic-imaging cross products training data are assumed centered and normalized.\nWe propose the following Multilevel Logistic Regression model:\nwhere \u03b1 0 (x G ) is the intercept and \u03b1(x G ) \u2208 R |I| is the parameter vector. On the contrary of the classical logistic regression model, we propose a multilevel model, for which the parameter vector \u03b1(x G ) and the intercept \u03b1 0 (x G ) depend on genetic data x G . This is to be compared to an additive model, where the diagnosis is directly deduced from genetic and imaging data put at the same level. We assume that \u03b1 and \u03b1 0 are affine functions of genetic data x G : Figure 1 summarizes the relations between parameters.\nThe disease status y is predicted from imaging data xI and the parameters \u03b20(xG), \u03b2(xG) (which are computed from genetic data xG)\nThe parameters W, \u03b2 I , \u03b2 G , \u03b2 0 are obtained by minimizing the objective:\nand\nGenetic data are a sequence of single-polymorphism nucleotides (SNP) counted by minor allele. A SNP can belong (or not) to one gene (or more) and therefore participate in the production of proteins that interact inside pathways. We decided to group SNPs by genes, and designed a penalty to enforce sparsity between genes and regularity inside genes. Given that some SNPs may belong to multiple genes, the group lasso with overlap penalty [4] is more suitable, with genes as groups. To deal with this penalty, an overlap expansion is performed. Given x \u2208 R |G| a subject's feature vector, a new feature vector is created\nby the concatenation of copies of the genetic data restricted by group G . Similarly, the same expansion is performed on\n. This group lasso with overlap penalty is used for the matrix W and for \u03b2 G .\nFor imaging variables, the ridge penalty is considered:\n. In particular, brain diseases usually have a diffuse anatomical pattern of alteration throughout the brain and therefore, regularity is usually required for the imaging parameter. Finally, \u2126 is defined by:\nFrom now on, and for simplicity reasons, W, \u03b2 and x are respectively denoted as W, \u03b2 and x. Let \u03a6 be the function that reshapes a matrix of M |I|,|G| (R) to a vector of R |I|\u00d7|G| (i.e. W i,g = \u03a6(W) i|G|+g ):\nWe will estimate \u03a6(W) and then reshape it to obtain W. The algorithm developed is based on a proximal gradient method [1, 5] .\nare updated with:\nThe idea is to update w (t+1) from w (t) with a Newton-type algorithm without the constraint \u2126 given a stepsize \u03b5, and then to project the result onto the compact set defined by \u2126. Regarding the stepsize \u03b5, a backtracking line search [5] is performed. Let G w (t) , \u03b5 = 1 \u03b5 w (t) \u2212 w (t+1) be the step in the proximal gradient update. A line search is performed over \u03b5 until the inequality is reached:\nThe minimization algorithm stops when S w\n, where \u03b7 = 10 \u22125 . The whole algorithm is summarized below:\nAlgorithm 1: Training the multilevel logistic regression\n2 Initialization: W = 0, \u03b2 I = 0, \u03b2 G = 0, \u03b20 = 0 and continue = True ; 3 while continue do\nAlgorithm 2: Parameter update\n3 Experimental results"}, {"section_title": "Dataset", "text": "The ADNI1 GWAS dataset from ADNI studied 707 subjects, with 156 Alzheimer's Disease patients (denoted AD), 196 MCI patients at baseline who progressed to AD (denoted pMCI, as progressive MCI), 150 MCI patients who remain stable (denoted sMCI, as stable MCI) and 201 healthy control subjects (denoted CN). In ADNI1 GWAS dataset, 620,901 SNPs have been genotyped, but we selected 1,107 SNPs based on the 44 first top genes related to AD (from AlzGene 2 ) and on the Illumina annotation using the Genome build 36.2. Group weighting for genes is based on gene size: for group G , the weight \u03b8 G = |G | ensures that the penalty term is of the order of the number of parameters of the group.\nThe parameter \u03bb G influences the number of groups that are selected by the model. In particular, the group G enters in the model during the first iteration if\nThis inequality gives an upper bound for \u03bb G . The same remark can be done for \u03bb W . Regarding MRI modality, we used the segmentation of FreeSurfer which gives the volume of subcortical regions (44 features) and the average cortical region thickness (70 features). Therefore, there are 1, 107\u00d7114 = 126, 198 parameters to infer for W, 114 parameters for \u03b2 I and 1, 107 parameters for \u03b2 G ."}, {"section_title": "Results", "text": "We ran our multilevel model and compared it to the logistic regression applied to one single modality with simple penalties (lasso, group lasso, ridge), to additive models ( [13] , [15] EasyMKL with a linear kernel for each modality, and the model p(y = 1|x G , x I ) = \u03c3 \u03b2 I x I + \u03b2 G x G + \u03b2 0 with our algorithm under the constraint \u03b2 G = 0), and to the multiplicative model with W only, where p(y = 1|x G , x I ) = \u03c3 x G W x I + \u03b2 0 . We considered two classification tasks: \"AD versus CN\" and \"pMCI versus CN\". Four measures are used: the sensitivity (Sen), the specificity (Spe), the precision (Pre) and the balanced accuracy between the sensitivity and the specificity (BAcc). A 10-fold cross validation is performed. The parameters \u03bb W , \u03bb I , \u03bb G are optimised between [10 \u22123 , 1]. Classification results for these tasks are shown on table 1. It typically takes between 5 and 8 minutes to learn the parameters.\nRegarding MRI features, the most important features (in weight) are the left/right hippocampus, the left/right Amygdala, the left/right entorhinal and the left middle temporal cortices. Regarding genetic features, the most important features in weight are SNPs that belong to gene APOE (rs429358) for both tasks \"AD versus CN\" and \"pMCI versus CN\".\nRegarding the matrix W, the couples (brain region, gene) learnt through the task \"pMCI versus CN\" are shown on Fig. 2 . It can be seen that W has a sparse structure. Among the couples (brain region, gene) that have non null We noticed that genes and brain regions strongly related to AD are captured by the vectors \u03b2 G and \u03b2 I , whereas genes less strongly related to AD are captured by the matrix W. Coming back to original formulation described in section 2.1, the contribution of the function \u03b1 0 : x G \u2192 \u03b2 G x G + \u03b2 0 is much smaller (in terms of weights) than the function \u03b1 : Fig. 2 shows that genetic data x G tend to express through W, and thereby participate in the modulation of the vector \u03b1(x G ).\nWe compared our approach to [13, 15] , for which the codes are available. The features that are selected by [13, 15] are similar to ours for each modality taken separately. For instance, for [13] and the task \"AD versus CN\", SNPs that have the most important weights are in genes APOE (rs429358), BZW1 (rs3815501) and MGMT (rs7071424). However, the genetic parameter vector learnt from [13] or [15] is not sparse, in contrary of ours. Furthermore, for [15] , the weight for the imaging kernel is nine times much larger than the weight for the genetic kernel. These experiments show that the additive model with adapted penalties for each modality provides better performances than [15] , but our additive, multiplicative and multilevel models provide similar performances. "}, {"section_title": "Conclusion", "text": "In this paper, we developed a novel approach to integrate genetic and brain imaging data for prediction of disease status. Our multilevel model takes into account potential interactions between genes and brain regions, but also the structure of the different types of data though the use of specific penalties within each modality. When applied to genetic and MRI data from the ADNI database, the model was able to highlight brain regions and genes that have been previously associated with AD, thereby demonstrating the potential of our approach for imaging genetics studies in brain diseases."}, {"section_title": "A Probabilistic formulation", "text": "This section proposes a probabilistic formulation for the model. The conditional probability is given by p(y = 1|x G ,\n-For each region i \u2208 I and gene G , W i,G \u223c M-Laplace(0, \u03bb W ) (M-Laplace stands for \"Multi-Laplacian prior\"). In other words: \nThe maximum a posteriori estimation is given by:\n( W, \u03b2 I , \u03b2 G , \u03b2 0 ) \u2208 argmax W,\u03b2 I ,\u03b2 G ,\u03b20\np(W, \u03b2 I , \u03b2 G , \u03b2 0 |Y, X I , X G ; \u03bb W , \u03bb I , \u03bb G , G, \u03b8 G ) \u2208 argmax W,\u03b2 I ,\u03b2 G ,\u03b20\np(W, \u03b2 I , \u03b2 G , \u03b2 0 , Y, X I , X G ; \u03bb W , \u03bb I , \u03bb G , G, \u03b8 G )\nIt is equivalent to minimize the function S defined by:\nS(W, \u03b2 I , \u03b2 G , \u03b2 0 ) = \u2212 log p(Y, W, \u03b2 I , \u03b2 G , \u03b2 0 , X I , X G ; \u03bb W , \u03bb I , \u03bb G , G, \u03b8 G ) = R N (W, \u03b2 I , \u03b2 G , \u03b2 0 ) + \u2126(W, \u03b2 I , \u03b2 G )"}]