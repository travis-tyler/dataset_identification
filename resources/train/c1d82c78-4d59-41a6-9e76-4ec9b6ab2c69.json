[{"section_title": "Abstract", "text": "Abstract. We consider the problem of using high dimensional data residing on graphs to predict a low-dimensional outcome variable, such as disease status. Examples of data include time series and genetic data measured on linear graphs and imaging data measured on triangulated graphs (or lattices), among many others. Many of these data have two key features including spatial smoothness and intrinsically low dimensional structure. We propose a simple solution based on a general statistical framework, called spatially weighted principal component regression (SWPCR). In SWPCR, we introduce two sets of weights including importance score weights for the selection of individual features at each node and spatial weights for the incorporation of the neighboring pattern on the graph. We integrate the importance score weights with the spatial weights in order to recover the low dimensional structure of high dimensional data. We demonstrate the utility of our methods through extensive simulations and a real data analysis based on Alzheimer's disease neuroimaging initiative data."}, {"section_title": "Introduction", "text": "Our problem of interest is to predict a set of response variables Y by using highdimensional data x = {x g : g \u2208 G} measured on a graph \u03b6 = (G, E), where E is the edge set of \u03b6 and G = {g 1 , . . . , g m } is a set of vertexes, in which m is the total number of vertexes in G. The response Y may include cognitive outcome, disease status, and the early onset of disease, among others. Standard graphs including both directed and undirected graphs have been widely used to build complex patterns [10] . Examples of graphs are linear graphs, tree graphs, triangulated graphs, and 2-dimensional (2D) (or 3-dimensional (3D)) lattices, among many others ( Figure 1 ). Examples of x on the graph \u03b6 = (G, E) include time series and genetic data measured on linear graphs and imaging data measured on triangulated graphs (or lattices). Particularly, various structural and functional neuroimaging data are frequently measured in a 3D lattice for the understanding of brain structure and function and their association with neuropsychiatric and neurodegenerative disorders [9] . The aim of this paper is to develop a new framework of spatially weighted principal component regression (SWPCR) to use x on graph \u03b6 = {G, E} to predict Y. Four major challenges arising from such development include ultra-high dimensionality, low sample size, spatially correlation, and spatial smoothness. SWPCR is developed to address these four challenges when high-dimensional data on graphs \u03b6 share two important features including spatial smoothness and intrinsically low dimensional structure. Compared with the existing literature, we make several major contributions as follows:\n\u2022 (i) SWPCR is designed to efficiently capture the two important features by using some recent advances in smoothing methods, dimensional reduction methods, and sparse methods.\n\u2022 (ii) SWPCR provides a powerful dimension reduction framework for integrating feature selection, smoothing, and feature extraction.\n\u2022 (iii) SWPCR significantly outperforms the competing methods by simulation studies and the real data analysis."}, {"section_title": "Spatially Weighted Principal Component Regression", "text": "In this section, we first describe the graph data that are considered in this paper. We formally describe the general framework of SWPCR."}, {"section_title": "Graph Data", "text": "Consider data from n independent subjects. For each subject, we observe a q \u00d7 1 vector of discrete or continuous responses, denoted by y i = (y i,1 , . . . , y i,q ) T , and a m \u00d7 1 vector of high dimensional data x i = {x i,g : g \u2208 G} for i = 1, . . . , n. In many cases, q is relatively small compared with n, whereas m is much larger than n. For instance, in many neuroimaging studies, it is common to use ultrahigh dimensional imaging data to classify a binary class variable. In this case, q = 1, whereas m can be several million number of features. In many applications, G = {g 1 , . . . , g m } is a set of prefixed vertexes, such as voxels in 2D or 3D lattices, whereas the edge set E may be either prefixed or determined by x i (or other data)."}, {"section_title": "SWPCR", "text": "We introduce a three-stage algorithm for SWPCR to use high-dimensional data x to predict a set of response variables Y. The key stages of SWPCR can be described as follows.\n\u2022 Stage 1. Build an importance score vector (or function) W I : G \u2192 R + and the spatial weight matrix (or function) W E : G \u00d7 G \u2192 R.\n\u2022 Stage 2. Build a sequence of scale vectors\nranging from the smallest scale vector s 0 to the largest scale vector s L . At each scale vector s , use generalized principal component analysis (GPCA) to compute the first few principal components of an n \u00d7 m matrix X = (\n\u2022 Stage 3. Select the optimal 0 \u2264 * \u2264 L and build a prediction model (e.g., high-dimensional linear model) based on the extracted principal components A(s * ) and the responses Y.\nWe slightly elaborate on these stages. In Stage 1, the important scores w I,g play an important feature screening role in SWPCR. Examples of w I,g = W I (g) in the literature can be generated based on some statistics (e.g., Pearson correlation or distance correlation) between x g and Y at each vertex g. For instance, let p(g) be the Pearson correlation at each vertex g and then define\nIn Stage 1, without loss of generality, we focus on the symmetric matrix W E = (w E,gg ) \u2208 R p\u00d7p throughout the paper. The element w E,gg is usually calculated by using various similarity criteria, such as Gaussian similarity from Euclidean distance, local neighborhood relationship, correlation, and prior information obtained from other data [21] . In Section 2.3, we will discuss how to determine W E and W I while explicitly accounting for the complex spatial structure among different vertexes.\nIn Stage 2, at each scale vector s = (s E, , s I, ), we construct two matrices, denoted by Q E, and Q I, based on W E and W I as follows:\nwhere \nto extract 'significant' vertexes. There are various ways of constructing Q E, . For instance, one may set Q E, as\nwhere s E, = (s E, ;1 , s E, ;2 ) T and D(g, g ) is a graph-based distance between vertexes g and g . The value of s E, ;2 controls the number of vertexes in {g \u2208 G : D(g, g ) \u2264 s E, ;2 }, which is a patch set at vertex g [18] , whereas s E, ;1 is used to shrink small |w E,gg |s into zero.\nAfter determining Q E, and Q I, , we set \u03a3 c = Q E, Q I, Q T I, Q T E, and \u03a3 r = I n for independent subjects. Let X be the centered matrix of X. Then we can extract K principal components through minimize the following objective function given by\nIf we consider correlated observations from multiple subjects, we may use \u03a3 r to explicitly model their correlation structure. The solution (U , D , V ) of the objective function (4) at s is the SVD of X R, = XQ E, Q I, . The we can use a GPCA algorithm to simultaneously calculate all components of (U , D , V ) for a fixed K as follows. In practice, a simple criterion for determining K is to include all components up to some arbitrary proportion of the total variance, say 85%. For ultra-high dimensional data, we consider a regularized GPCA to generate (U , D , V ) by minimizing the following objective function\nwhere u k, and v k, are respectively the k-th column of U and V . We use adaptive Lasso penalties for P 1 (\u00b7) and P 2 (\u00b7) and then iteratively solve (5) [1] . For each k 0 , we define\nT k, and minimize\nsubject to u T k0, u k0, \u2264 1 and v T k0, v k0, \u2264 1. By using the sparse method in [12] , we can calculate the solution of (6), denoted by (d k0, ,\u00fb k0, ,v k0, ). In this way, we can sequentially compute\nIn Stage 3, select * as the minimum point of the objective function (5) or (6) . let Q F, * = Q E, * Q I, * V * D \u22121 * and then K principal components A(s * ) = XQ F, * . Moreover, K is usually much smaller than min(n, m). Then, we build a regression model with y i as responses and A i (the i-th row of A(s * )) as covariates, denoted by R(y i , A i ; \u03b8), where \u03b8 is a vector of unknown (finite-dimensional or nonparametric) parameters. Specifically, based on {(y i , A i )} i\u22651 , we use an estimation method to estimate \u03b8 as follows:\nwhere \u03c1(\u00b7, \u00b7, \u00b7) is a loss function, which depends on both the regression model and the data, and P 3 (\u00b7) is a penalty function, such as Lasso. This leads to a prediction model R(y i , A i ; \u03b8). For instance, for binary response y i = 1 or 0, we may consider a sparse logistic model given by logit(P (\nGiven a test feature vector x * , we can do predictions from our prediction model as follows:\n\u2022 Center each component of x * by calculatingx * = x * \u2212\u03bc x , in which\u03bc x is the mean and learnt from the training data;\n\u2022 Optimize an objective function based on R(y,x * T Q F, * ; \u03b8) to calculate an estimate of y, denoted by\u0177 * .\nOur prediction model is applicable to various regression settings for continuous and discrete responses and multivariate and univariate responses, such as survival data and classification problems."}, {"section_title": "Importance Score Weights and Spatial Weights", "text": "There are two sets of weights in SWPCR including (i) importance score weights enabling a selective treatment for individual features, and (ii) spatial weights accommodating the underlying spatial dependence among features across neighboring vertexes on graph. Below, we propose the strategy of determining both importance score weights and spatial weights.\nImportance Score Weights As discussed in Section 2.3, at each vertex g, w I,g , such as the Pearson correlation in (1), is calculated based on a statistical model between x g and Y in order to perform feature selection according to each feature's discriminative importance. Statistically, most existing methods use a marginal (or vertex-wise) model by assuming\nwhere \u03b2 = (\u03b2(g) : g \u2208 G) and \u03b2(g) is introduced to quantify the association between y i and x i,g at each vertex g \u2208 G. At the g\u2212th vertex, w I,g is a statistic based on the marginal model\nHowever, those w I,g s largely ignore complex spatial structure, such as homogenous patches defined below, across all vertexes on graph.\nFor a graph \u03b6 = (G, E), it is common to assume that \u03b2(g) across all vertexes are naturally clustered into P homogeneous patches, denoted by {G l : l = 1, . . . , P }, such that P << m, G = \u222a\nTo learn the homogeneous patches, a general framework of Multiscale Adaptive Regression Model (MARM) developed in [13] is to maximize a sequence of weighted functions as follows:\nwhere \u03c9(g, g ; h) characterizes the similarity between the data in vertexes g and g with \u03c9(g, g; h) = 1. If \u03c9(g, g ; h) \u2248 0, then the observations in vertex g do not provide information on \u03b2(g). Therefore, \u03c9(g, g ; h) can prevent incorporation of vertexes whose data do not contain information on \u03b2(g) and preserve the edges of homogeneous regions. Let D 1 (g, g ) and D 2 (\u03b2(g; h s\u22121 ),\u03b2(g ; h s\u22121 )) be, respectively, the spatial distance between vertexes g and g and a similarity measure between\u03b2(g; h s\u22121 ) and\u03b2(g ; h s\u22121 ). The \u03c9(g, g ; h s ) can be defined as (8) where K 1 (\u00b7) and K 2 (\u00b7) are two nonnegative kernel functions and \u03b3 n is a bandwidth parameter that may depend on n. See the detailed algorithm of MARM in [13] . After the iteration h s , we can obtain\u03b2(g; h S ) and its covariance matrix, denoted by Cov(\u03b2(g; h s )), across all g \u2208 G and \u03c9(g, g ; h s ) for all g \u2208 E g (h s ) and g \u2208 G. Finally, we calculate statistics w I,g based on\u03b2(g; h s ) and Cov(\u03b2(g; h s )), such as the Wald test, and then we use a clustering algorithm, such as the Kmean algorithm, to group {\u03b2(g; h s ) : g \u2208 G} into several homogeneous clusters, in which\u03b2(g; h s ) varies very smoothly in each cluster. Moreover, each homogenous cluster can be a union of several homogeneous patches.\nSpatial Weights As discussed in Section 2.3, w E,gg often characterizes the degree of certain 'similarity' between vertexes g and g . The locally spatial weight-ing matrix consists of non-negative weights assigned to the spatial neighboring vertexes of each vertex. It is assumed that\nin which \u03c9(g, g ; h s ) is defined in (8) . Therefore, w E,gg = 0 for all g \u2208 E g (h s ) and g \u2208G w E,gg = 1. The weights K 1 (D 1 (g, g ) /h s ) give less weight to vertex g \u2208 E g (h s ), whose location is far from the vertex g. The weights K 2 (u) downweight the vertex g with large D 2 (\u03b2(g; h s ),\u03b2(g ; h s )), which indicates a large difference between\u03b2(g ; h s ) and\u03b2(g; h s ). Moreover, by following [4, 13, 15, 16], we set K 1 (x) = (1 \u2212 x) + and K 2 (x) = exp(\u2212x). Although m is often much larger than n, the computational burden associated with the local spatial weights is very minor when h s is relatively small."}, {"section_title": "Simulation Study", "text": "In this section, we conducted one set of simulation study corresponding to binary responses, in order to examine the finite-sample performance of SWPCR in the high-dimensional classification analysis. We demonstrate that SWPCR outperforms many state-of-the-art methods for at least in the simulated dataset. We simulated 20 \u00d7 20 \u00d7 10 (x \u00d7 y \u00d7 z) 3D-images from a linear model given by shown in Figure 2 . Voxels in the red cuboid region have the maximum difference 1 between classes 0 and 1. The dimension of red cuboid is 3 \u00d7 3 \u00d7 4 and contains 36 voxels. In this case, m = 4, 000 and we set n = 100 with 60 images from Class 0 and the rest from Class 1. We consider three types of noise i (g) in (10) . First,\ni (g) were independently generated from a N (0, 2 2 ) generator across all voxels\n(g )/m g were generated from\n(1)\ni (g) in order to introduce the short range spatial correlation, where m g is the number of voxels in the set { g \u2212 g \u2264 1}. Third, to introduce long range spatial correlation, (3) i (g) were generated according to (3) i (g) = 2 sin(\u03c0g 1 /10)\u03be i,1 + 2 cos(\u03c0g 2 /10)\u03be i,2 + 2 sin(\u03c0g 3 /5)\u03be i,3 + (1) i (g), where \u03be i,k for k = 1, 2, 3 were independenly generated from a N (0, 1) generator. Moreover, the noise variances in all voxels of the red cuboid region equal 4, 4/6, and 4{sin(\u03c0g 1 /10) 2 + cos(\u03c0g 2 /10) 2 + sin(\u03c0g 3 /5) 2 } + 4 for Type I, II, and III noises, respectively. Therefore, among the three types of noise, Type III noise has the smallest signal-to-noise ratio and Type II noise has the largest one. We ran the three stages of SWPCR as follows. In Stage 1, let {h = 1.2 , = 0, 1, . . . , S = 5}, and for each g \u2208 G,\nwhere p(g) is the p-value of Wald test B 1 (g) = 0 in (7) (\u03b2(g) = (B 0 (g), B 1 (g)) T ) for each voxel g. The spatial weight W E is given by (9) . Here we haven't used the simple Pearson correlation (1) for computing weights because it neglects the spatial correlation of the data set. In Stage 2, for each h , we define Q E, = W E and generate Q I, through (2) and (3), where s I, thresholds out the w I,g with p(g) < 0.01. Then we extract different K principal components of GPCA to reconstruct the low dimensional representations of simulated images and then do classification analysis. The results are very stable for different number of principal components and here we let K = 5. In Stage 3, we tried different classification methods, including linear regression, k-Nearest Neighbor (k-NN) [11] and support vector machine (SVM) [14] , on these low dimensional spaces. Based on the misclassification error for the leave-one-out cross validation, the linear regression is slight better than others. The linear regression uses class label y i as dependent variable and principal components as explanatory variables. If the prediction value is less than 0, the image is classified as 0. Otherwise, the image is classified as 1.\nWe compared SWPCR with other state-of-the-art classification methods. The leave-one-out cross validation is used here to calculate the misclassification rates of the different methods. Other classification methods considered here include sparse linear discriminant analysis (sLDA) [6] , sparse partial least squares (SPLS) analysis [5] , sparse logistic regression (SLR) [20] , SVM, and regularized optimal affine discriminant (ROAD) [8] . These methods are well known for their excellent performance in various simulated and real data sets. Inspecting Table  1 reveal that except SWPCR, all classification methods perform pretty poor, when the signal-to-noise ratio is low in those simulated datasets with Type I and II noises. Except SPLS, PCA, and SWPCR, all other methods are seem to be sensitive to the presence of the long-range correlation structure in Type III noise."}, {"section_title": "Real Data Analysis", "text": ""}, {"section_title": "ADNI PET Data", "text": "The real data set is the baseline fluorodeoxyglucose positron emission tomography (FDG-PET) data downloaded from the Alzheimer's Disease Neuroimaging Initiative (ADNI) web site (www.loni.ucla.edu/ADNI). The ADNI1 PET data set consists of 196 subjects (102 Normal Controls (NC) and 94 AD subjects).\nThere are three subjects, missing the gender and age information. Among the rest of the subjects, there are 117 males whose mean age is 76.20 years with standard deviation 6.06 years and 76 females whose mean age is 75.29 years with standard deviation 6.29 years. The dimension of the processed PET images is 79 \u00d7 95 \u00d7 69. Left panel in Figure 3 shows some selected slices of the processed PET images from 2 randomly selected AD subjects and 2 randomly selected NC subjects."}, {"section_title": "Binary Classification", "text": "Our first goal is to apply SWPCR in classifying subjects from ADNI1 to AD or CN group based on their FDG-PET images. Such goal is associated with the second primary objective of ADNI aiming at developing new diagnostic methods for AD intervention, prevention, and treatment. Similar as in Section 3 , SWPCR contains the three detailed stages that will not be repeated again. The right panel in Figure 3 is the three view slices of the weight matrix Q I, at the coordinate (40, 57, 26) in the stage 2 of SWPCR. The red region in three slices corresponds to the large important score weight and contains the most classification information.\nWe compared SWPCR with six other classification methods including sLDA, SPLS, SLR, SVM, ROAD, and PCA. We used their leave-one-out cross validation rates. Table 2 shows the classification results of all the seven methods. sLDA Fig. 3 . ADNI1 pet data and the important score weight matrix Q I, in SWPCR. In the left panel, one row sequence of 2-D images belongs to one subject. The first two rows respectively belongs to AD subjects and the rest belongs to NC subjects. In the right panel, the three plots (left -right-bottom) are three view slices of the weight matrix Q I, at the coordinate (40, 57, 26). The red region corresponds to large weight score and contains the most classification information.\nperforms much worse than all other six methods. ROAD performs slightly better than PCA. SPLS and SVM are comparable with each other, but they outperform SLR and ROAD. SWPCR outperforms all six classification methods. It suggests that the classification performance can be significantly improved by incorporating spatial smoothness and simple dimension reductions methods, such as PCA."}, {"section_title": "Age Prediction", "text": "Our second goal is to apply SWPCR in predicting subjects' age based on their FDG-PET images. The response variable y is the age of the subjects and the explanatory variables are the latent scores, extracted from image data. It is very interesting to use memory test scores as the response variable y. However, the data set here contains no such information. The three subjects without the age information are deleted and then we have 193 images left. y i in model (10) becomes age of the subjects. Here we will not repeat the detailed stages of SWPCR again, which is similar as in Section 3. The slight difference is stage 3. Here we run regression rather than classification methods between age and the SWPCR latent scores. First, we compared SWPCR with three other dimensional reduction methods including PCA, weighted PCA (WPCA) [17] , and supervised PCA (SPCA) [2] . We used the leave-one-out cross validation to compute the prediction errors of all the four methods. Let\u0177 i be the fitted response value based on the regression model, and the prediction error is defined as |\u0177 i \u2212 y i |/|y i |. Subsequently, we calculated the error difference between SWPCR and all three other methods across different numbers (K = 5, 7, 10) of principal components. Panels (a)-(c) in Figure 4 show the boxplots of the error difference between SWPCR and PCA, WPCA, and SPCA, respectively. The error differences are almost always less than 0 (under the dashed line) and these results show the better performance of SWPCR in dimension reduction. Second, we compared SWPCR with several other high-dimensional regression methods including penalized regression (PR) [19] , sure independence screening (SIS) regression [7] , support vector regression (SVR) [3] , and SPLS [5] . Panel (d) in Figure 4 shows the boxplots of the prediction error difference between SWPCR and all the other regression methods. The analysis results further confirm the better performance of SWPCR in regression."}, {"section_title": "Discussion", "text": "SWPCR enables a selective treatment of individual features, accommodates the complex dependence among features of graph data, and has the ability of utilizing the underlying spatial pattern possessed by image data. SWPCR integrates feature selection, smoothing, and feature extraction in a single framework. In the simulation studies and real data analysis, SWPCR shows substantial improvement over many state-of-the-art methods for high-dimensional problems."}]