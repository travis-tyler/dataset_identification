[{"section_title": "Abstract", "text": "Abstract-In this paper, we explore the use of Censored Likelihoods in Gaussian Process Regression when predicting bounded clinical scores from neuroimaging data. The standard approach, which uses a Gaussian Likelihood, does not respect the fact that the clinical scores are bounded, and so may produce suboptimal models. Conversely, Censored Likelihoods explicitly model the restricted range of such clinical scores and carry this property through inference. We apply both the standard approach and the Censored Likelihood approach to the prediction of the MMSE score from structural MRI. Overall, we find small improvements in mean squared error when using the Censored Likelihood and in addition, the censored models are more favoured from a Bayesian perspective. We also discuss the qualitative nature of the predictions of the two approaches."}, {"section_title": "I. INTRODUCTION", "text": "There has been substantial interest in recent years in using multivariate regression models to predict clinical and psychometric scales from neuroimaging MRI [1] .\nOne attractive framework for learning the predictive models for such measures is Gaussian Process Regression (GPR). Gaussian processes are flexible Bayesian methods for model estimation that have recently gained popularity for building predictive neuroimaging models for regression and classification [2] , [3] , [4] . The standard Gaussian Likelihood used for GPR , however, does not respect the fact that the clinical score y is often bounded below y \u2208 [a, \u221e], above y \u2208 [\u221e, b] or both y \u2208 [a, b], where a, b \u2208 R. This can mean that some of the attractive properties of Gaussian Processes, such as automatic hyperparameter estimation, become compromised resulting in sub-optimal models and poorer predictions.\nIn this work we explore the use of GPR with Censored Likelihoods when the clinical score we aim to predict is bounded. The use of Censored Likelihoods enables the modelling procedure to explicitly take into account the restricted range of the the clinical score during model building and inference. Using imaging data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database, we show that the use of GPR with Censored Likelihoods can result in better models for the data from a Bayesian perspective, whilst also improving prediction accuracy."}, {"section_title": "II. MATERIALS", "text": "The dataset consisted of the MP-RAGE images of 592 unique subjects from the ADNI database (adni.loni.usc.edu). The data was preprocessed using SPM12 (http://www.fil. ion.ucl.ac.uk/spm/software/spm12/) by performing grey matter segmentation and group-wise registration with Dartel to a study-specific template. The aligned images were transformed to the 2mm MNI template and smoothed with a Gaussian kernel of 2mm FWHM. A mask was applied to select voxels that had a probability of being grey matter above 0.025, giving a set of images that provide the 157026 image features x in the matrix X. In our experiments, the grey matter image features are used to predict the 'Mini-Mental State Examination' (MMSE) which is thus the target variable y. The MMSE is commonly used to diagnose and assess dementia and tests subject performance in areas such as arithmetic, comprehension, and basic motor skills. Subjects can achieve a minimum of zero and a maximum of thirty for the MMSE score, ie. y \u2208 [0, 30]. Figure 1 shows the distribution of MMSE score over the 592 subjects used in our analysis, and we can see that a large proportion of the scores 'pile up' at the maximum possible score of 30. "}, {"section_title": "A. Gaussian Process Regression", "text": "Given a set of n observations x i \u2208 R p with associated target variables y i , Gaussian processes impose a multivariate Gaussian prior on a set of latent variables f i , where the mean 978-1-4673-6530-7/16/$31.00 \u00a92016 IEEE and covariance of the prior are functions of the inputs x i [5] :\nWe assume a zero mean function m(x i ) \u2261 0 throughout this work. The covariance function k(x i , x j ), also referred to as the kernel function, describes how the values of the latent variables covary across the input space, and it will have a set of associated kernel parameters \u03b8. We employ a linear covariance function with a bias term in our analysis:\nThe posterior for the latent variables f i is given by:\nwhere the likelihood function P (y i | f i , \u03c3) relates the observed targets to the latent variables, and has hyperparameters \u03c3. In order to perform inference, we firstly need to estimate the complete set of kernel and likelihood hyperparameters {\u03c3, \u03b8}, and this is performed by maximizing the marginal likelihood P (y | X, \u03b8, \u03c3) which is the normalizing factor in equation (3). Once we have the optimised hyperparameters {\u03b8, \u03c3}, we can then perform inference for a set of test points X * to derive the predictive distribution for the test targets y * . We now describe the Gaussian and Censored Likelihoods used for modelling in this paper and how inference proceeds in each case."}, {"section_title": "1) Gaussian Likelihood:", "text": "The model underlying the Gaussian Likelihood is\nwhere \u223c N (0, \u03c3 2 ) and \u03c3 > 0 is the standard deviation of the noise. This gives rise to the Gaussian Likelihood\nFor a Gaussian Likelihood, the predictive distribution of the target y * of a test point x * , given the training data, has the closed form\nin which K is the n \u00d7 n matrix of training set covariances and k * is the n-dimensional row vector of test-training covariances. Typically, the mean of the distribution,\u0233 * , is taken to be the prediction of the target at test point x * .\nNote that the Gaussian Likelihood in equation (5) gives non-zero probabilities for every possible value of the target variable, given the latent variable. This means that even if the target variable y is bounded within [a, b], this is not taken into account during inference. One consequence of this is that the mean predictions\u0233 * may lie outside [a, b] . In this work, we ensure that all predictions using the Gaussian Likelihood lie within the appropriate range by taking the final predictions to be the closest value within [a, b] to the mean predictions.\n2) Left and Right Censored Likelihood: Here the targets y i are explicitly bounded below ('right censoring') and above ('left censoring'), y \u2208 [a, b]. The underlying model is\nwith \u223c N (0, \u03c3 2 ). The likelihood for y i is then\nwhere \u03a6 is the cumulative distribution function for the normal distribution,\nComparing this with the Gaussian Likelihood in equation (5), we can see that the Censored Likelihood is identical to the Gaussian Likelihood for values of the target y i \u2208 (a, b), but differs at the bounds a, b. The resulting likelihood also gives zero probabilities for values of the target y i that lie outside [a, b] , ensuring that the range of the targets is explicitly included in the model and is carried through inference. Note that our main motivation for using a Censored Likelihood when predicting clinical scores is to respect their bounded nature: We are not using it to directly address any possible skewness in the distribution of target variables, such as that seen in figure 1, although skewness may naturally arise with bounded targets. Indeed, a Gaussian Likelihood may still be appropriate for a skewed distribution of non-bounded target variables.\nInference with a Censored Likelihood cannot be performed analytically as with the Gaussian Likelihood, and so approximations must be used. In this work we test both the Laplacian approximation for Censored Likelihoods, described in [6] , and the Expectation Propogation (EP) approach from [7] . These methods are essentially different ways of approximating the posterior distribution of the latent variables given in equation (3), enabling the marginal likelihood to be optimized, and inference to be performed. In each case, this results in gaussian predictive distributions for the latent variables of the test points:\ngiving rise to the following predictive distribution for y * :\n(10) The mean prediction\u0233 * using the Censored Likelihood is then given b\u0233\nin which\n, \u03c3 * = \u03c3 2 + \u03c3 2 f * , and \u03c6 is the probability density function of the standard normal distribution."}, {"section_title": "B. Model Evaluation", "text": "We evaluate the different GPR models described in section III-A when predicting the MMSE score using the imaging features as predictors. In addition to those models, we also perform predictions with naive models that consist solely of a constant bias term, using both a Gaussian Likelihood and Censored Likelihood with EP inference. These models do not use the image features during inference and serve as reference models for the other approaches. Model performance was determined using 5-fold cross validation, repeated over 10 different splits of the data. In each case all features were standardised to mean zero and unit variance using the training folds only, and the predictions were rounded to the nearest whole number. We calculate the mean-squared error (MSE), the mean-absolute error (MAE), and Kendall's coefficient \u03c4 of each model for each of the 10 splits of the data, and then average them to give summary measures of predictive performance. In addition to the above metrics, which are calculated using only point estimates of the predictions, we determine extra measures that are often used when assessing Bayesian modelling approaches. Firstly, we calculate the mean negative log predictive density (MNLPD) for each model. The log predictive density (LPD) essentially describes how probable the real targets, ie. the MMSE scores, are, according to the predictive distribution given by a particular model [5] ie.\nLP D = log P (y * |X, y, x * )\nUnlike the MSE and MAE, the LPD takes into account the uncertainty in the predictive distribution. We take the negative log predictive density to turn this into a loss function, and average over each observation and split to give the summary measure MNLPD of how well the model has captured the predictive distribution of the data. Lower values of MNLPD will then correspond to better fits of the predictive distribution. Lastly, we determine the optimised Log Marginal Likelihood, log Z, using the whole dataset for training. Since log Z is the optimized value we correct for the number of model hyperparameters to enable model comparison using the Bayesian Information Criterion:\nHere n is the size of the dataset, and p is the number of model hyperparameters (equal to the number of kernel parameters plus 1 likelihood parameter). Table I summarizes the predictive accuracies for the different models. Firstly, we can see that all the models that use the image features outperform the corresponding naive models that utilize a covariance function containing only a bias term. We can also see that the Censored Likelihood model with EP inference gives slightly more accurate predictions than the model using the standard Gaussian Likelihood, according to both the MSE and MAE. The Censored Likelihood model using Laplace inference gives a smaller improvement in MSE than the one using EP inference. The Kendall correlation coefficient appears similar for the different approaches. It is possible that the use of the more appropriate Censored Likelihood produces the improvements in MSE and MAE compared to the Gaussian likelihood model. We found that the censored models improved the MSE for all 10 splits of the data, providing further evidence that the slight improvements in accuracy were due to the modelling approach. As the Censored Likelihood with EP inference outperformed that with Laplace inference, we mostly focus on that approach in the following discussion. Figure 2 shows the predicted MMSE scores against the true MMSE scores when using the Gaussian Likelihood, and the Censored Likelihood with EP inference, averaged over the 10 splits. The dashed red lines indicate predictions for a model that has zero test error. The first thing we can notice is that the predictions of subjects with low MMSE scores tend to be too high for all the models, as shown by the large number of points to the left of the red line. However, we can also observe subtle differences in the qualitative behaviour of the different approaches over the range of the targets. The principal apparent difference is that the predictions of the censored approach (bottom) do not appear to push predictions of high MMSE scores \u2265 27 as strongly to the maximal score of 30 as those with the Gaussian Likelihood (top). This can be further seen in table II, where we show the average errors over the 10 splits using the Gaussian Likelihood and Censored Likelihood (EP) within each quartile of the targets: Q 1 : y \u2264 26, (n = 179); Q 2 : 27 \u2264 y \u2264 28, (n = 152); Q 3 : y = 29, (n = 131); Q 4 : y = 30, (n = 130). Here we can see that the 'softening' of predictions for subjects with high MMSE scores using the Censored Likelihood seems to have mostly affected the predictive accuracy of subjects with the highest MMSE score, as the Censored Likelihood gives smaller MSE and MAE for each quartile apart from Q 4 . In terms of the overall predictive distribution, the censored models perform better than the Gaussian Likelihood as shown by the lower MNLPD values for these approaches. Calculation of the Log Bayes Factors log K of pairs of models, given by the difference between the corresponding values of log Z BIC , gives very strong support for both of the censored models over the Gaussian Likelihood model (log K = 129, 130). Given that the Censored Likelihood explicitly restricts the range of the target variable to [0, 30] , it is perhaps not suprising that these models are so strongly favoured over those using the Gaussian Likelihood, according to the Bayes Factors and MNLPD."}, {"section_title": "IV. RESULTS AND DISCUSSION", "text": "For illustration, figure 3 shows the (unnormalized) weight images for the different approaches when training with the complete dataset. A positive weight at a voxel v indicates an increase in the prediction of the latent function (but not the predicted target variable for the Censored Likelihood models) as This figure shows the predicted MMSE scores against the true MMSE scores for different models, averaged over 10 splits. Some jittering has been added to the true scores for visualisation purposes. The top shows the predictions using the Gaussian Likelihood, while in the bottom we see the predictions using the Censored Likelihood EP approach. The dashed red lines indicate predictions for a model that has zero test error. "}, {"section_title": "V. CONCLUSION", "text": "In this work we have explored using Gaussian Process Regression with a Censored Likelihood when predicting clinical scores from neuroimaging data. We compared this approach to Gaussian Process Regression with a Gaussian Likelihood, which is the standard model used when predicting scores that have a large range. We found that the use of the Censored Likelihood with both Laplace and EP inference gave small improvements in the MSE compared to the standard model, although they did not appear to improve Kendalls \u03c4 correlation coefficient. The use of a Censored Likelihood also appeared to improve the overall predictive distribution for those models, and the Bayes factors gave strong supporting evidence that they were more appropriate for the data.\nWhilst we saw modest improvements in prediction accuracy using the Censored Likelihood, it is possible that further improvement may be obtained by adding complexity to the model so that non-linear relationships within the range of target values [a, b] can be captured, while simultaneously enforcing censoring of the likelihood. It would also be interesting to investigate performance using different kernels rather than the single linear kernel utilized in this work. Further experiments, using datasets with different properties and dimensionality and comparison with methods such as Gaussian Process Ordinal Regression [3] will be performed to investigate the behaviour of these different approaches."}]