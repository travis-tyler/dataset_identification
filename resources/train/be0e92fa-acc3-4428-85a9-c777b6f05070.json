[{"section_title": "Abstract", "text": "Abstract-In spite of the growing interest for grids and cloud infrastructures among scientific communities and the availability of such facilities at large-scale, achieving high performance in production environments remains challenging due to at least four factors: the low reliability of very large-scale distributed computing infrastructures, the performance overhead induced by shared facilities, the difficulty to obtain fair balance of all user jobs in such an heterogeneous environment, and the complexity of large-scale distributed applications deployment. All together, these difficulties make infrastructure exploitation complex, and often limited to experts. This paper introduces a pragmatic solution to tackle these four issues based on a serviceoriented methodology, the reuse of existing middleware services, and the joint exploitation of local and distributed computing resources. Emphasis is put on the integrated environment ease of use. Results on an actual neuroscience application show the impact of the environment setup in terms of reliability and performance. Recommendations and best practices are derived from this experiment."}, {"section_title": "I. INTRODUCTION", "text": "Distributed Computing Infrastructures (DCIs) are being increasingly exploited for tackling the computation needs of large-scale applications. DCI middleware helps users in exploiting seamlessly large amounts of computing resources. However, executing large-scale applications on a DCI faces several well-identified problems often causing poor application performance, either under-performing execution time or complete application failure. In particular, the following showstoppers are recurrently reported in the literature dealing with large-scale distributed applications enactment:\n\u2022 Low reliability of the infrastructure causing high failure rates and severe performance losses.\n\u2022 High latency of computing tasks submitted to production batch systems causing low performance.\n\u2022 Unfair balance between shorter and longer computation tasks.\n\u2022 Complex deployment of distributed computing applications. Users often face critical decisions that require expert skills to setup and tune a workable solution for their particular application among available middleware services. In this paper, we propose a comprehensive and integrated execution environment designed to tackle simultaneously the low reliability, high latency, unfair balance, and complex deployment issues.\nThe designed software architecture is reusing as much as possible existing stable middleware services. Their integration is far from trivial as different services may be tackling different issues but they are not necessarily compatible nor interoperable. The resulting framework does not only deliver to users a uniform interface to manipulate and execute their applications. It also shields users from the technical issues of the underlying infrastructure. It is exploited by neuroscientists to study neurodegenerative diseases such as Alzheimer's. The users' main concern is completely non-technical in this area.\nThis proposition assumes that the applications to execute are described as scientific workflows. Workflows formalize the description of distributed computing processes and they have been widely adopted among computational scientific communities. Workflows are decomposed into many application services with inter-dependencies which define ordering constraints at execution time. In scientific workflows, input datasets are usually composed of many independent data items to be processed, hence implying a high level of data parallelism. The workflow services are invoked multiple times, to process all received data segments. An application can then be seen as an orchestration of collaborating services deployed within a Service-oriented Architecture (SOA).\nOur methodology is heavily based on SOA principles for their high versatility. As shown in section III, the provision of both middleware and application components as services makes it possible to fine tune application deployment and configuration, thus tackling the problems of reliability, fair balancing, and application deployment. Furthermore, highlatency concerns are addressed by advanced tasks management services. To take advantage of the service-oriented principles while being non-intrusive for the existing application services, our implementation includes an intermediate layer between the infrastructure stack of technologies and the user interfaces.\nThe rest of the paper is structured as follows. First, a requirements analysis is conducted and relevant technical solutions are reviewed in section II. Then, section III describes a model to decide computation tasks dispatching integrating local resources to DCIs. Section IV presents the execution framework and describes all its software components. This section also highlights optimization methods to address the failure recovery, resource reservation, and scalability of the framework. An experimental validation is presented in section V. Finally conclusion and some recommendations derived from the experiments are developed in section VI."}, {"section_title": "II. ADDRESSING PRODUCTION DCIS SHORTCOMINGS", "text": "A lot of research efforts have been invested in dealing more or less independently with the well-known issues of largescale infrastructures outlined above [1] , [2] , [3] , [4] . The goal of this work is to design an end-to-end execution framework simultaneously tackling the most critical showstoppers commonly encountered. Specifically, four issues are addressed to improve the application performance on production DCIs within the framework: 1) Failure recovery. Networking and computing infrastructures are subject to random resource failures. The likeliness of failures increases with the number of physical entities, as seen in large-scale distributed systems today [5] , [6] . Recovering from failures becomes a critical issue to improve the reliability of the infrastructure, preventing the correct completion of many application runs. Numerous works addressing this issue have been proposed in the literature including check-pointing, live migration [7] , [8] , job replications [2] and submission strategies [4] . On general purpose production infrastructures, job resubmission is often the only general failure recovery solution available, as check-pointing and migration usually either make restrictive assumptions on the computational processes or they require application instrumentation. The final makespan could be increased, specifically with longer applications, but resubmission ensures that the application execution can always continue and finish successfully. This approach is implemented in the framework by controlling the status of submitted jobs and defining a resubmission policy when a failure occurs. 2) Lowering latency. The splitting of an application's computation logic in many tasks lends towards more parallelism but the gain may be easily compensated by the time needed to handle all tasks generated in a competitive production batch system. In the case of a workflow-based application with inter-dependencies between tasks, the sequential submission of tasks to long batch queues will be highly penalizing. Addressing the high latency issue, many works study multiple submissions approaches [1] , [2] , [4] . The results of these studies confirm that submitting tasks several times increases application performance. However, users who do not use multiple submission are penalized. Furthermore, without considering the capacity of batch schedulers, high number of submissions can overload the batch schedulers and then degrade the overall system performance. Alternatively, pilot jobs systems help users in reserving a pool of computing resources during the execution of the application [9] , being considered as a bridge between batch systems and systems supporting resources reservation. A pilot job is submitted to a workload manager to reserve a computing resource. User jobs are then pulled from the job queue to computing nodes by successfully started pilot jobs. Each pilot job can thus process sequentially several user jobs without introducing delay between two of them. Each pilot is subject once to the workload manager queuing time but the jobs they process are not. Another advantage of pilot jobs to the classical submission approach include the sanity checks of the running environment before assigning resources for execution. They also allow users to create a virtual private network of computing resources reserved for executing their tasks, and they implement effectively the pull scheduling paradigm. Our execution framework extensively uses pilot jobs reducing latency and making executions more reliable because broken resources are filtered by the pilot jobs. 3) Task fairness. The very complex tuning of large-scale submission systems, involving meta-brokers and many schedulers, makes it extremely difficult to achieve fair balance between short and long tasks in a computation process. Yet, production infrastructures are not only used for long running jobs processing data-intensive applications but they are also frequently used for processing shorter jobs. Statistical results shows that more than 50% of the jobs take less than 30 minutes for execution [3] . While the high latency has less impact on long running jobs, short jobs are heavily penalized if they have long waiting times before execution. The larger the computing time discrepancy between tasks, the higher the impact. Users therefore require a mechanism of resource fair sharing to avoid that long jobs monopolize the whole computing resources, and delay the completion of other users' (short) jobs. Pilot jobs also improve handling of short jobs as they reduce individual jobs queuing time. However, although dedicated to a specific user, pilot job systems usually do not implement fairness among the user's jobs and pilots may be overloaded by the processing of longer jobs similarly to a Grid meta-scheduler. Therefore, our approach combines more dedicated resources out of a distributed infrastructure with the capacity of DCIs to improve handling of short jobs. Local resources are more reliable since the user is administrator of computing nodes, thus failures coming from the software dependencies are lowered. Executing applications locally reduces the number of job submissions remotely removing the submission phase and delays of middleware initialization. This then reduces the waiting time of other jobs in the queue for obtaining computing resources on remote infrastructures. Nevertheless, as the number of computing resources in the local server is limited, the more jobs submitted locally, the longer is the execution time needed to finish all jobs. We define a decision model in section III to decide whether a task is executed on local resources or submitted to a DCI. 4) Deployment & scalability. Beyond middleware parameterization, the deployment of application services may have a strong impact on application performance as servers easily become overloaded in large-scale runs [10] . Some initiatives like GASW [11] or LONI Pipeline [12] propose tools to reuse scientific applications on DCIs but they have scalability limitations or interoperability constraints respectively. Concerning Web service-related projects, tools such as GEMLCA [13] , and gRAVI [14] manage services lifecycle at different levels, enabling dynamic deployment and/or supporting of nonfunctional concerns. However, their adoption involves the use of an homogeneous middleware. Our execution framework relies on a legacy application code wrapper that both provides a standard Web service interface to all application computing components, and helps managing the complete lifecycle of the resulting services.\nIn practice multiple services containers, acting as a proxy between users and the production DCI, may be configured in the framework, as described in section IV.\nEach container naturally has a limited capacity to process concurrent services. When the size of input dataset increases, the number of services submitted concurrently may exceed its capacity. The replication of servers into the system (scaling out) resolves this limitation. It increases the performance without modifying the framework architecture.\nThe execution framework described below addresses simultaneously the production DCIs shortcomings by combining advanced job submission strategies, services replication, and including the use of local resources during workflow enactment. The implementation of job resubmission improves the reliability by instantiating a system capable of error overcoming from remote executions. Then the adoption of pilot jobs for multilevel scheduling ensures the reduction of latency. Pilot jobs represent a new approach to overcome long queues of batch schedulers reusing computing resources efficiently. In order to tackle the unfair balancing resulting from the competition of short/lightweight application tasks with the long/heavyweight ones, a decision model dispatching tasks among local and remote resources is implemented. The deployment of services provides transparent mechanisms of applications reallocation, over local and remote resources, holding back technical details far from final users. Finally, the scalability heedfulness ensures large-scale experiment campaigns by enabling services resiliency.\nThe delivery of an integrated execution environment is eased by the application of SOA principles, made possible by the workflow formalism used to model distributed applications. SOA has been adopted to a large extent in middleware design [15] . For instance, the Swift workflow management system [16] provides an integrated working environment for job scheduling, data transfer, and job submission. It is built on top of a uniform implementation based on Globus toolkit. Yet, production infrastructures hardly ever comply to a homogeneous middleware stack, nor adopt a single communication standard for all core and community services. Conversely, traditional workflow management systems like Taverna [17] , or Triana [18] support service invocation enabling interoperability but they do not natively execute code on DCIs. In our architecture, both middleware and application components are deployed as services. The application code is instrumented non-invasively to comply to this model through a Web service builder aware of DCIs computing capability [19] . Using an SOA approach allows users to scale the execution of their applications and flexibly extend the execution framework according to the computation needs."}, {"section_title": "III. MODEL FOR EFFICIENT USE OF LOCAL RESOURCES", "text": "In spite of the large number of computing resources available on DCIs, the waiting time of a job to obtain a computing resource may increase considerably with a big number of jobs simultaneously submitted to the infrastructure. This latency is particularly not negligible for short-execution jobs. Using local resources may then complement DCIs resources. By reducing the number of short jobs executed remotely, they reduce the management time of jobs processed on the DCI thus improving the reliability and performance of the application. However, a strategy is required to ensure that local resources are not overloaded when many jobs are executed. In view of this, a decision model is defined to dispatch incoming jobs for local execution or for submission to a DCI based on expertises captured from the application. It makes the assumption that each workflow activity i among the k used activities {i \u2208 Z + | i k} is consuming a fixed amount of resources when executing (i.e., r i memory space, and t i execution time). It is also assumed that the target DCIs are large enough to handle simultaneously all computation tasks triggered by the invocation of the application services at runtime.\nLet R denote the memory consumed on the local resource for all running services including r j which would be an incoming service of type j executed locally at a given time. The value of R is computed according to Equation 1, where n i denotes the number of services of type i. The volume of assigned memory must not exceed R MAX , the available memory installed on the local resource (R R MAX ).\nMaking the hypothesis that production infrastructures have sufficient computing resources to execute all submitted services, the execution time of a scientific workflow T MAX would be the longest path of its representation as a graph (aka critical path). Therefore, the execution time in the local resources T must be shorter than this theoretical threshold in order to avoid penalizing the final execution time of the workflow (T T MAX ). The value of T , as shown in Equation 2, represents the sequential execution time of all services running on local resources distributed on all available processor units, where N CPU denotes the number of CPU cores.\nAlgorithm 1 shows the procedure to decide whether a job is executed locally or submitted remotely. The estimation of R and T is performed each time an incoming service is enacted by the workflow manager. Meanwhile, the value of n j is updated for accounting. \nservice is executed locally n j = n j + 1 else service is submitted to a DCI end if"}, {"section_title": "IV. EXECUTION FRAMEWORK", "text": "This section describes a highly configurable, and standardsbased service architecture enabling reproducible and scalable experimentation. From a user's point of view, an end-toend system enabling the exploitation of DCIs should provide high expressiveness to describe applications; design and enact applications composition making use of consistent interfaces; and transparent access to DCI resources. The framework architecture, based on MOTEUR [20] and jigsaw [19] components, is pictured in Fig. 1 .\nMOTEUR, a scientific workflow environment, is the frontend component that connects the user to the rest of the framework. It includes a workflow designer and an execution monitor. A MOTEUR client interacts with the MOTEUR server at runtime to execute the application considering a specific input dataset. The MOTEUR server is responsible for invoking each application service through generic interfaces. Before each invocation, it executes the decision model algorithm to define whether the job should be executed locally or submitted remotely addressing the task fairness described in the third DCI shortcoming of section II. In our architecture, application services invoked by MOTEUR are packaged and instrumented using the jigsaw framework. Jigsaw is a legacy application code wrapper which exposes legacy application components as standard Web services, and provides a complete mechanism to handle the invocation of application code locally, on the server hosting the service, or remotely, on the DCI.\nJigsaw enables the definition of multiple execution profiles corresponding to different execution platforms, and it is responsible for monitoring the job status and it implements resubmission policies when an execution fails resolving the first DCI shortcoming concerning failure recovery. It integrates multiple submission back-ends such as GASW [11] , to target various execution infrastructures. It can dispatch executions on the local server (local resource), PBS clusters (not represented here) or the European Grid Infrastructure (EGI) through its Workload Management System interface (WMS). In addition, the DIRAC pilot management system [9] is integrated in the framework ahead of the default WMS and its computing resources. The DIRAC architecture consists of numerous cooperating distributed services and light agents built within the same framework following distributing security standards. It implements a multilevel scheduling using pilot jobs that drastically improves reliability of jobs submission and significantly reduces task latency. This addresses the second shortcoming, and overcomes the full list of identified strategies to improve the applications performance on production DCI environments.\nJigsaw instruments the legacy code execution interfaces using WS-compliant messages for services invocation. The execution description, packaged along with the legacy codes in portable artifacts, are deployed and published in a services container such as Apache Tomcat [21] , thus tackling the complete services lifecycle, from creation to deployment and then invocation. The jigsaw services may be hosted and replicated in several containers. It makes possible to scale the system out enforcing the availability of services, and distributing the workload associated to the invocation of services involved in task dispatching. Together, jigsaw and the services container, provide an integrated solution to the deployment and scalability arguments mentioned in the fourth shortcoming of the requirements analysis.\nComplementary modules are integrated in the framework to address non-functional requirements of users' authentication and data management. User's credentials may be required for authentication during execution. Therefore, each framework component can connect to a myProxy server [22] . The user is just required to provide the login and password of the credential stored on the myProxy server. The validity of the proxy is checked each time an operation is performed. An expired proxy will be automatically renewed without interrupting the application execution. Concerning data transfers, the VL-e Toolkit [23] is also integrated in the framework. It provides a unified view of heterogeneous file systems. VL-e Toolkit supports several protocols such as gridFTP or HTTP, and file schemes like the grid LFN or local file systems. It is used for service deployment, and file staging on the services container to provide the data inputs to the service instances enforcing the interoperability.\nThe framework targets a coherent integration of a datadriven approach to manipulate complex data structures through high level interfaces. The MOTEUR client provides to users a graphical application to configure services and describe the semantics of dataflows, achieving transparent parallelism. The description is represented by the GWENDIA workflow language [24] , that supports the required expressiveness to represent services composition. While the MOTEUR client offers design tools to build workflows and configure an execution environment, the MOTEUR server provides asynchronous invocation and orchestration of services, for optimizing the execution of data-intensive workflows. The integration of jigsaw with MOTEUR provides a full range of functionality, making them suitable for the purpose of applications reuse and large-scale experimentation."}, {"section_title": "V. RESULTS", "text": "Experiments have been designed to validate the submission model and the execution framework. The framework is stress tested using a real application related to the progression of Alzheimer's disease. The experimental setup aims at quantifying the endured by, and the speedup of the application."}, {"section_title": "A. Case study", "text": "Neuro-degenerative diseases like Alzheimer's disease are characterized by a co-occurrence of different pathological phenomenas which eventually cause brain cells loss over time.\nMonitoring the structural changes of the brain provides a way to track the evolution of the disease. The evaluation of changes in time from serial data of the same subject acting as his own control (i.e., longitudinal analysis) is useful for detecting the subtle changes related to the biological processes. This original processing was developed at the Asclepios Research Project from Inria Sophia-Antipolis [25] . Its scientific workflow representation is shown in Fig. 2 . It includes pre-treatment steps for reorientation, and registration of images against an atlas of the brain. Then the intensity of images is normalized, and a non-rigid registration is performed to look for anatomical differences between pairs of images. Finally, a quantification of the longitudinal brain atrophy is derived from the Jacobian matrix of deformations as an average measuring the volume change in comparison to a reference mask. In terms of execution the longitudinal atrophy detection in Alzheimer's disease is a good example for the validation of the decision model because services composing the workflow are heterogeneous in terms of average execution time and memory consumption as shown in Table I "}, {"section_title": "B. Experimental setup", "text": "Three experiment types were defined to test different submission approaches supported by jigsaw and validate the framework scalability: 1) Execution on grid. The workflow is executed by submitting jobs directly to the EGI WMS. This is the default behavior when working on production environments and it is considered as baseline performance. 2) Multilevel scheduling execution. The workflow is executed using DIRAC. It represents a basic environment considering pilot jobs. 3) Efficient execution. The workflow is executed implementing all optimization mechanisms of the framework. Two services containers are deployed to instrument the decision model. One server manages executions using local resources, and the second one serves the job submissions on EGI through DIRAC. Several patients could be processed in parallel without performance loss assuming availability of resources on the DCI. For each experiment type, the workflow was executed with patients' datasets which size grows exponentially from 1 to 256, and with 2 to 5 images associated to each patient. This leads to an average of 25 service executions per patient submitted to the infrastructure. The experiments were performed on the European Grid Infrastructure, using inputs of the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. For the local executions a server with 2 quad-core processors at 2.67 GHz and 16 GB of memory was used.\nWe are mainly interested in the final workflow execution timespan for the speedup calculation and the average latency of all job submissions. Nevertheless, the execution failure rate is also reported as it is an important element in the reliability analysis."}, {"section_title": "C. Latency", "text": "Table II details the statistical results of all three experiment types for the latency analysis including the average latency (x), the standard deviation (\u03c3), the median absolute deviation (MAD), and the interquartile range (IQR). The MAD and IQR are robust statistics that are not significantly affected by outliers. The variable workload of a production environment like EGI is noticed obtaining higher values of \u03c3 in the grid execution type. The robust statistics are preferred for the analysis, in the context of executions on EGI, because outliers have exhibited a high impact on latency due to load variability [4] . Globally, we observe a sustained latency increment when the input dataset size increases. This behavior is expected as the increasing number of jobs loads the submission queues (e.g., from 5 concurrent executions for 1 patient up to 962 for 256 patients). Nevertheless, the introduction of multilevel scheduling reduces significantly the average latency. Focusing on the largest dataset runs, we verify that the multilevel scheduling optimization reduces the latency of 85.25% compared to grid execution for 256 patients. The latency reduction of the efficient execution is equivalent to the multilevel one reaching 86.13%, confirming the benefit effect of pilot jobs in both execution types. This behavior is verified by the MAD, a variability measure comparable to \u03c3. In addition, the MAD exhibits the repercussions of introducing the decision model into the framework for job submission fairness improvement. The values of MAD for efficient executions are reduced to slight or null values in efficient executions compared to the multilevel execution. Similarly, the range of average latency is attenuated significantly. We observe a reduced variability of latency reflected with lower values of IQR. However, the use of limited local resources shows up with large datasets obtaining similar IQR values in multilevel and efficient executions. TABLE II: Statistics summary for submission latency: average latency over 3 executions (x), standard deviation (\u03c3), median absolute deviation (MAD), and interquartile range (IQR).\nIn Fig. 3 , we present the timeline diagrams for the execution of the three identified experiment types. Graphically, we observe the time evolution of a saturated workload manager during the grid execution that results in all tasks having a similar waiting time. This latency is gradually reduced during the multilevel scheduling execution. Finally, we observe a scaled latency attenuation effect when local resources are used in the efficient execution type. However, the dynamic resource acquisition is exhibited once the pilot jobs are enabled in multilevel and efficient executions."}, {"section_title": "D. Speedup", "text": "Two types of speedup are considered to evaluate the impact of the execution framework on the application performance: the traditional speedup and the workflow speedup. The traditional speedup S is defined as the ratio of a reference, sequential running time of the application over the timespan measured during a parallel run. This value assesses the interest of using DCIs for executing large-scale applications instead of running applications sequentially. It varies with the parallel execution mode considered and the input data set processed. On the other hand, we determine the workflow speedup S w = p \u00d7 T 1 /T p , where p is the number of patients, and T i is the execution time for i patients in a given execution type. In practice, we obtain the value of S w with regard a constant reference time T 1 of the grid execution. Acting this way, S w represents a good comparator between execution types. Table III presents the statistical results of the timespan, failure rate, and the computed speedups for each execution type. For all executions, the timespan grows when the size of input dataset increases. However, an exception is verified in case of multilevel scheduling execution of 128 patients, which is longer than the 256-patient run. This is due to its high failure rate leading to the resubmission of 9.09% of total number of tasks. This behavior exhibits the dynamic workload of production Grid infrastructures.\nConcerning speedups, we observe that they are effective from one patient (S 1). The increasing speedup verifies the several levels of parallelism (data, service, pipeline) implemented in the workflow enactment system. The speedup increases significantly even if the latency increases showing the relevance of the resources availability on DCIs. The workflow enactment enables concurrent executions improving the final execution timespan specially for large number of patients. In the best case of efficient execution, the traditional speedup reaches a factor of 120.915 in comparison to the sequential execution. When comparing S w of the multilevel scheduling execution to the grid one, we observe that pilot jobs reduce significantly the execution timespan. The workflow speedup attains a factor of 102.918 for the largest input dataset.\nCombining the local resources to the execution framework lightly reduces the final execution timespan. It confirms that the use of local resources does not reduce the final execution timespan significantly, but it has a clear influence on latency and reliability as expected in a high throughput environment. Therefore, S w of the efficient execution is relatively the same as the multilevel one."}, {"section_title": "E. Reliability", "text": "We observe in Table III a significant failure rate within each execution type. These rates are influenced by several factors on the production environment, namely full storage elements, temporal unavailability of middleware services such as the file catalog server or the proxy certificates manager, unexpected timeouts while storing data, or application specific errors due to incompatibilities with OS computing elements and/or missing system libraries. For instance, the failure rate of the default grid execution type is high, up to 31.86% in the worst case. The reduction of the failure rate in the multilevel scheduling execution (up to 13.71% in the worst case) is possible thanks to the sanity check mechanism of pilot jobs, and broken computing resources filtering before jobs pulling. When executing jobs on local resources, we also eliminate the errors concerning the incompatibilities with OS computing elements and/or missing system libraries. The failure rate "}, {"section_title": "VI. CONCLUSION", "text": "In spite of more than a decade spent in active research and development of DCIs middleware, large-scale infrastructures often remain accessible to experts only. The inherent complexity of such systems is causing large overheads, numerous failures, and complex application control environment. This paper proposed pragmatic solutions to deploy and control execution of large-scale applications. The solution proposed is based on state-of-the-art middleware components, glued together in a coherent service-oriented environment that interfaces these components so as to balance the features provided by each of them. As a result, complex scientific experiments such as the study on Alzheimer's exemplified in this paper can be reliably enacted and high performing. The computing environment was built from well-established software component in a general setting to make it as reusable as possible."}]