[{"section_title": "Abstract", "text": "In an increasing number of neuroimaging studies, brain images, which are in the form of multidimensional arrays (tensors), have been collected on multiple subjects at multiple time points. Of scientific interest is to analyze such massive and complex longitudinal images to diagnose neurodegenerative disorders and to identify disease relevant brain regions. In this article, we treat those problems in a unifying regression framework with image predictors, and propose tensor generalized estimating equations (GEE) for longitudinal imaging analysis. The GEE approach takes into account intra-subject correlation of responses, whereas a low rank tensor decomposition of the coefficient array enables effective estimation and prediction with limited sample size. We propose an efficient estimation algorithm, study the asymptotics in both fixed p and diverging p regimes, and also investigate tensor GEE with regularization that is particularly useful for region selection. The efficacy of the proposed tensor GEE is demonstrated on both simulated data and a real data set from the Alzheimer's Disease Neuroimaging Initiative (ADNI)."}, {"section_title": "Introduction", "text": "isting methods perform the prediction using only the baseline data, ignoring data at the follow-up time points that often contain useful longitudinal information. Recently, a small group of researchers started to use longitudinal imaging data for individual-based classification (Misra et al., 2009; Davatzikos et al., 2009; McEvoy et al., 2011; Hinrichs et al., 2011 ) and for cognitive score prediction , whereas a limited number of studies regressed longitudinal image responses on a collection of covariates, first one voxel at a time then spatially smoothing the parameters (Skup et al., 2012; Li et al., 2013) . In general, longitudinal imaging analysis is challenging, due to both the ultrahigh dimensionality and the complex spatial structure of images, while the longitudinal correlation adds another layer of complication.\nSince the seminal work of Liang and Zeger (1986) , there has been a substantive literature on statistical analysis of longitudinal data. See Prentice and Zhao (1991) ; Li (1997) ; Qu et al. (2000) ; Xie and Yang (2003) ; Balan and Schiopu-Kratina (2005) ; Song et al. (2009); Wang (2011) , among many others. There is also a line of research studying variable selection for longitudinal models, including Pan (2001) ; Fan and Li (2004) ; Ni et al. (2010) ; Xue et al. (2010) ; Wang et al. (2012) . However, all those studies take the covariates as a vector, whereas in imaging regression, covariates take the form of multi-dimensional arrays. Naively turning an array into a vector would result in extremely high dimensionality. For instance, a 32 \u00d7 32 \u00d7 32 MRI image would require 32 3 = 32, 768 parameters. Moreover, vectorization destroys inherent spatial information in images. There have been some recent developments of statistical regression models for image/tensor covariates; for instance, Caffo et al. (2010) ; Reiss and Ogden (2010) ; Wang et al. (2014) . In particular, Zhou et al. (2013) proposed a class of tensor regression models by imposing a low rank tensor decomposition on the coefficient tensor. Although those methods directly work with a tensor covariate, none has taken longitudinal tensors into account, and thus none is immediately applicable to our longitudinal imaging study.\nIn this article, we propose tensor generalized estimating equations for longitudinal imaging analysis. Our proposal consists of two key components: a low rank tensor decomposition and generalized estimating equations (GEE). Similar to Zhou et al. (2013) , we choose to impose a low rank structure, the CANDECOMP/PARAFAC (CP) de-composition (Kolda and Bader, 2009) , on the coefficient array in GEE. This structure substantially reduces the number of free parameters and makes subsequent estimation and inference feasible. But unlike Zhou et al. (2013) , we incorporate this low rank structure in estimating equations to accommodate longitudinal correlation of the data. We have chosen GEE over another popular approach, the mixed effects model, for longitudinal imaging analysis. This is because the GEE approach only requires the first two marginal moments and a working correlation structure for the scalar response variable.\nBy contrast, a mixed effects model requires specification of a distribution for the parameters, which turns out to be a formidable task for a tensor covariate. Within the tensor GEE framework, we develop a scalable computation algorithm for solving the complicated tensor estimating equations. Next we establish the asymptotic properties of the solution of tensor GEE, including consistency and asymptotic normality under two large sample scenarios: the number of parameters is fixed and the number of parameters diverges along with the sample size. In particular, we show that the tensor GEE estimator inherits the robustness feature of the classical GEE estimator, in that the estimate is consistent even if the working correlation structure is misspecified. Finally, we investigate regularization in the context of tensor GEE. Regularization is crucial when the number of parameters far exceeds the sample size, and is also useful for stabilizing estimates and incorporating prior subject knowledge. For instance, employing an L 1 penalty in our tensor GEE in effect finds subregions of brains that are highly relevant to the clinical outcome. This region selection is of scientific interest itself, and corresponds to the intensively studied variable selection problem in classical regressions with vector-valued predictors.\nOur contributions are two-fold. First of all, our proposal offers a timely response to the increasing availability of longitudinal imaging data along with the growing interest of their analysis. To the best of our knowledge, there has been very few systematic statistical methods developed for such an analysis. Second, our work generalizes both the GEE approach from vector-valued covariates to tensor-valued image covariate, as well as the tensor regression model of Zhou et al. (2013) from independent imaging data to longitudinal imaging data. Such a generalization parallels the extension in classical regressions with vector predictors. This extension, however, is far from trivial. Owing to the intrinsic complexity of both spatially and temporally correlated observations as well as the huge data size, longitudinal imaging analysis is much more challenging than both longitudinal analysis with vector-valued predictors and imaging analysis at a single time point. Given that the results of this kind are rare, our proposal offers a useful addition to the literature of both longitudinal and imaging analysis.\nThe rest of the article is organized as follows. Section 2 proposes tensor GEE for longitudinal imaging data, along with their estimation and regularization. Section 3 presents the asymptotic results for the tensor GEE estimates. Simulation studies and real data analysis are carried out in Sections 4 and 5, respectively, followed by a discussion in Section 6."}, {"section_title": "Tensor Generalized Estimating Equations", "text": "\nThe GEE method has been widely employed for analyzing correlated longitudinal data since the pioneer work of Liang and Zeger (1986) . It requires specification of the first two moments of the conditional distribution of the response given the covariates, Following Liang and Zeger (1986) , we assume Y ij is from an exponential family with canonical link. Then\nwhere \u00b5(\u00b7) is a differentiable canonical link function, \u00b5 (1) (\u00b7) is its first derivative, \u03b8 ij is the linear systematic part, and \u03c6 is an over-dispersion parameter. In this article we simply set \u03c6 = 1 while the extension to a general \u03c6 is straightforward. \u03b8 ij is associated with the covariates via the relation\nwhere \u03b3 is the coefficient vector associated with the covariate vector Z, including the intercept, and B is the coefficient tensor of the same size as X that captures effects of every array element of X.\nThe GEE estimator of B, \u03b3 is then defined as the solution of\nwhere\nthe response covariance matrix of the i-th subject. The first component in (3) It thus becomes crucial to reduce the number of estimating equations.\nToward that end, we impose a low rank structure on the coefficient array B. More specifically, we assume B in model (2) follows a CP structure in (1),\nwhere\nThen the systematic part in (2) becomes\nAdopting (4), we propose the tensor generalized estimating equations estimator of B, \u03b3, defined as the solution of\nwhere \u03b2 B = vec(B 1 , . . . , B D ), and the subscript B is to remind that \u03b2 is constructed based on the CP decomposition of a given coefficient tensor B = B 1 , . . . , B D . Comparing to the classical GEE (3), the derivative is now with respect to [\u03b2 Examining (5), the true intra-subject covariance structure V i is usually unknown in practice. The classical GEE adopts a working covariance matrix, specified through a working correlation matrix R. That is,\n, where A i (B, \u03b3) is an m \u00d7 m diagonal matrix with \u03c3 2 ij (B, \u03b3) on the diagonal and R is the m-by-m working intra-subject correlation matrix. Some commonly used correlation structures include independence, autocorrelation (AR), compound symmetry, and unstructured correlation, among others. The correlation matrix R may involve additional parameters, which can be estimated using residual-based moment method.\nBy both adopting this working covariance/correlation idea, and explicitly evaluating the derivative in (5), we finally arrive at the formal definition of the tensor GEE estimator, which is the solution ( B,\u03b3) of the following estimating equations\nwhere R is an estimated correlation matrix, vec(\nhas been canceled by the diagonal of the matrix A \u22121 i due to the property of canonical link. For ease of presentation, we denote the left hand side of equation (6) as s(B, \u03b3), and write the tensor GEE (6) as s(B, \u03b3) = 0."}, {"section_title": "Notations and Preliminaries", "text": "Suppose there are n training subjects, and for the i-th subject, there are observations over m i time points. For simplicity, we assume m i = m and the time points are the same for all subjects. The observed data consist of {(Y ij , X ij , Z ij ), i = 1, . . . , n, j = 1, . . . , m},\nwhere, for the i-th subject at the j-th time point, Y ij denotes the target response, Z ij \u2208 IR p 0 is a conventional predictor vector, and\narray that represents the image covariate. The array dimension D depends on the image modality. With an image at a single time point, for EEG, D = 2, for MRI and PET, D = 3, and for fMRI,\nA key attribute of longitudinal data is that the observations from different subjects are commonly assumed independent, but the observations from the same subject are correlated. That is, the intra-subject covariance matrix, Var(Y i ) \u2208 IR m\u00d7m , is not a diagonal matrix but with some structure.\nNext we review some key notations and operations of multidimensional array that will be used throughout this article. The inner product between two tensors is defined as\nwhere the vec(B) operator stacks the entries of a tensor B \u2208 IR p 1 \u00d7\u00b7\u00b7\u00b7\u00d7p D into a column vector. The outer product, \ndecomposition (Kolda and Bader, 2009) \nwhere\n. . , D, r = 1, . . . , R, are all column vectors, and B cannot be written as a sum of less than R outer products. The decomposition (1) is often represented by a shorthand, B = B 1 , . . . , B D , where\nIf a tensor B \u2208 IR p 1 \u00d7\u00b7\u00b7\u00b7\u00d7p D admits a rank-R decomposition (1), then\nwhere denotes the Khatri-Rao product (Rao and Mitra, 1971) \nand \u2297 denotes the Kronecker product."}, {"section_title": "Estimation", "text": "Directly solving the tensor generalized estimating equations (6) with respect to (B, \u03b3)\ncan be computational intensive, as the mean function of the response given the covariates is nonlinear in the parameters and the Jacobian matrices J 1 , . . . , J D also depend on the unknown parameters. We propose to iteratively solve the sub-GEE for B 1 , . . . , B D , along with \u03b3, one at a time, while keeping all other components fixed. When updating\nwhere X ij(d) is the mode-d matricization of the tensor X ij . As such, the systematic part\nand depends on the covariates and fixed parameters only. Consequently, each step reduces to a standard GEE problem with Rp d parameters, which can be solved using standard statistical softwares.\nA problem of practical interest is to choose the rank R for B in its CP decomposition.\nThis can be viewed as a model selection problem. Pan (2001) proposed a quasi-likelihood independence model criterion for the classical GEE model selection, by evaluating the likelihood under the independence working correlation assumption. In our tensor GEE setup, we use the following BIC-type information criterion\nwhere ( B(R),\u03b3; I m ) is the log-likelihood evaluated at the tensor GEE estimator\u03b3 and B(R) with a working rank R and the independence working correlation structure I m .\nFor simplicity, we call this criterion BIC, as the term log(n) is used. Because the CP decomposition itself is not unique, but can be made so under some minor conditions (Zhou et al., 2013) , the actual number of estimating equations, or the effective number of parameters, is of the form:\nfor D > 2. We choose R that minimizes this criterion among a series of working ranks.\nWe will briefly illustrate its use in Section 4.1."}, {"section_title": "Regularization", "text": "Even after introducing a low rank structure in our tensor GEE, regularization can still be useful, as the number of subjects is often limited in a neuroimaging study. In this section, we consider a general form of regularized tensor GEE that includes a variety of penalty functions. Then in Section 4.3, we will illustrate with a lasso penalty that is capable of identifying sub-regions of brains associated with the clinical outcome. Specifically, we consider the following regularized tensor GEE\nwhere P \u03bb (|\u03b2|, \u03c1) is a scalar penalty function, \u03c1 is the penalty tuning parameter, \u03bb is an index for the penalty family, \u2202 \u03b2 P \u03bb (|\u03b2|, \u03c1) is the subgradient with respect to argument \u03b2, and the subscript p e of 0 is a reminder of the number of estimating equations to solve. Some widely used penalties include: power family (Frank and Friedman, 1993) ,\n, and in particular lasso (Tibshirani, 1996) \nand ridge (\u03bb = 2); elastic net (Zou and Hastie, 2005) , in which\nand SCAD (Fan and Li, 2001) , in which \u2202/\u2202|\u03b2|P \u03bb (|\u03b2|, \u03c1) = \u03c1 1 {|\u03b2|\u2264\u03c1} + (\u03bb\u03c1 \u2212 |\u03b2|) + /(\u03bb \u2212 1)\u03c11 {|\u03b2|>\u03c1} , \u03bb > 2, among many others.\nThanks to the separability of parameters in the regularization term, the alternating updating strategy still applies. When updating B d , we solve the penalized sub-GEE "}, {"section_title": "Theory", "text": "In this section, we study the asymptotic properties of the unregularized tensor GEE estimator as the number of subjects n goes to infinity, while we assume the true rank of the tensor coefficient is known. We investigate two scenarios: the number of parameters is fixed in Section 3.1, and the number of parameters diverges in Section 3.2. For ease of exposition, we omit the vector-valued covariates Z and the associated parameters \u03b3, while the results can be easily extended to incorporate them. Our development builds upon and extends the previous work of Xie and Yang (2003); Balan and Schiopu-Kratina (2005); Wang (2011) from classical vector GEE to tensor GEE, while we spell out the similarity as well as difference in asymptotics when comparing the vector and tensor GEE. We show that tensor GEE estimator inherits the key advantage of the classical GEE estimator in that it remains consistent even if the working correlation structure is misspecified. On the other hand, we note that, although one can generalize the classical GEE asymptotics by directly vectorizing the tensor, it would have to require a more stringent set of conditions. By contrast, we could achieve the robustness in consistency for our tensor GEE based on a weaker set of conditions, and we achieve this by imposing and exploiting the special structure of the coefficient tensor."}, {"section_title": "Asymptotics for Fixed Dimension", "text": "We begin with the list of regularity conditions for the asymptotics of tensor GEE with a fixed number of parameters.\n(A1) The elements of X ij , i = 1, . . . , n, j = 1, . . . , m, are uniformly bounded by a finite constant.\n(A2) The true value B 0 of the unknown parameter lies in the interior of a compact parameter space B and follows a rank-R CP structure defined in (1).\n. It is assumed that there exist two positive constants c 1 < c 2 such that\nover the set {B : ||\u03b2 B \u2212 \u03b2 B 0 || \u2264 n \u22121/2 } for some constant > 0, where \u03bb min and \u03bb max are smallest and largest eigenvalue, respectively. It is also assumed that on the same set I(B) has a constant rank.\n(A4) The true intra-subject correlation matrix R 0 has bounded eigenvalues from zero and infinity. The estimated working correlation matrix satisfies\n, where \u00b7 F is the Frobenius norm,R is some positive definite matrix with bounded eigenvalues from zero and infinity, andR = R 0 is not required.\n(A5) For some constant \u03b4 > 0 and\n) has sub-Gaussian tails for all i = 1, . . . , n, j = 1, . . . , m.\n(A7) The elements of \u2202\u03b8 ij (\u03b2 B 0 )/\u2202\u03b2 B 0 , i = 1, . . . , n, j = 1, . . . , m, are uniformly bounded by a finite constant.\n, where \u03b8 ij is the linear systematic part evaluated at the GEE solution B. It is assumed that \u00b5 (1) (\u03b8 ij ) are uniformly bounded away from zero and infinity, and \u00b5 (k) (\u03b8 ij ) are uniformly bounded by a finite constant, over the set {B : ||\u03b2 B \u2212 \u03b2 B 0 || \u2264 n \u22121/2 }, for some constant > 0, i = 1, . . . , n, j = 1, . . . , m, and k = 2, 3.\nis the Hessian of the linear systematic part \u03b8 ij under tensor structure. There exist two positive constants c 3 < c 4 such that\nover the set {B : ||\u03b2 B \u2212 \u03b2 B 0 || \u2264 n \u22121/2 } for some constant > 0, i = 1, . . . , n and j = 1, . . . , m.\nA few remarks are in order. Conditions (A2) and (A3) are required for model identifiability of tensor GEE (Zhou et al., 2013) . We observe that, the matrix\nand thus (A3) is much weaker than the nonsingularity condition on the design matrix if one were to directly vectorize the tensor covariate.\nCondition (A4) is commonly imposed in the GEE literature. It only requires a consistent estimator R of someR, in the sense\n.R needs to be well behaved in that it is positive definite with bounded eigenvalues from zero and infinity, but R does not have to be the true intra-subject correlation R. This condition essentially leads to the robust feature in Theorem 1 that the tensor GEE estimate is consistent even if the working correlation structure is misspecified. Conditions (A5) and (A6) regulate the tail behavior of the residuals so that the noise cannot accumulate too fast, and we can employ the Lindeberg-Feller central limit theorem to control the asymptotic behavior of the residuals. Condition (A7) states the gradients of the systematic part evaluated at the truth are well-defined. Condition (A8) concerns the canonical link and generally holds for common exponential families, for example, the binomial distribution with \u00b5(\u03b8 ij ) = exp \u03b8 ij /(1 + exp \u03b8 ij ), and the Poisson distribution with \u00b5(\u03b8 ij ) = exp \u03b8 ij .\nCondition (A9) ensures that the Hessian matrix of the linear systematic part, which is highly sparse, is well-behaved in a neighborhood of the true value.\nBefore we turn to the asymptotics of the tensor GEE estimator, we address two components involved in the estimating equations: the initial estimator and the correlation estimator. Recall the tensor GEE estimator B is obtained by solving the equations in (6). After dropping the covariate vector Z, the tensor estimating equations become\nwhere R is any estimator of the intra-subject correlation matrix satisfying the condition (A4). We still denote the left hand side by s(B). Note that (9) involves the unknown correlation R, and its estimate R is often obtained via residual-based moment method, which in turn requires an initial estimator of B. Next, we examine some frequently used estimators of B and R.\nA customary initial estimator B in the GEE literature is the one that assumes an independent working correlation. That is, one completely ignores possible intra-subject correlation, and the corresponding tensor GEE becomes\nDenoting the equations as s init (B) = 0, and the solution as B init , the next Lemma shows that it is a consistent estimator of the true B 0 .\nLemma 1. Under conditions (A1)-(A3) and (A5)-(A9), there exists a root B init of the equations s init (B) = 0 satisfing that\nHere \u03b2 B = vec(B 1 , . . . , B D ), and is constructed based on the CP decomposition of a given tensor B = B 1 , . . . , B D , as defined before.\nGiven a consistent initial estimator of B 0 , there exist multiple choices for the working correlation structure, e.g., autocorrelation, compound symmetry, and the nonparametric structure (Balan and Schiopu-Kratina, 2005 ). We will investigate those choices in our simulations and real data analysis.\nNext we establish the consistency and asymptotic normality of the tensor GEE estimator from (9). \nThe key message of Theorem 1, as implied by condition (A4), is that the consistency of the tensor coefficient estimator B does not require the estimated working correlation R being a consistent estimator of the true correlation R. This protects us from potential misspecification of the intra-subject correlation structure. Such a robustness feature is well known for GEE estimator with vector-valued covariates. Theorem 1 confirms and extends this result to the tensor GEE case with image covariates. We also remark that, although the asymptotics of the classical GEE can in principle be generalized to the tensor data by directly vectorizing the coefficient array, the ultrahigh dimensionality of the parameters would have made the regularity conditions such as (A3) unrealistic. By contrast, Theorem 1 ensures that one could still enjoy the consistency and robustness properties, by taking into account the structural information of the tensor coefficient under the GEE framework.\nUnder condition (A4), we defin\u1ebd\nAs we will show in the appendix,M n (B) approximates the covariance matrix of s(B)\nin (9), whileD n1 (B) approximates the leading term of the negative gradient of s(B)\nwith respect to \u03b2 B . Then the next theorem gives the asymptotic normality of the tensor GEE estimator."}, {"section_title": "Theorem 2. Under conditions (A1)-(A9), for any vector", "text": "By Theorem 2 and Cram\u00e9r-Wold theorem, one can derive the sandwich covariance estimator of Var(\u03b2 B ), and carry out the subsequent Wald inference. Specifically, it is easy to see that the variance of the GEE estimator can be approximated by the asymptotic\nn1 (B 0 ). Since it involves the unknown terms B 0 , R 0 and R, we plug in, respectively, B, n\nand R, which leads to the sandwich estimator,\nThis sandwich formula in turn can be used to construct asymptotic confidence interval or asymptotic hypothesis testing through the usual Wald inference."}, {"section_title": "Asymptotics for Diverging Dimension", "text": "We next study the asymptotics when the number of parameters diverges. We assume that p d \u223c p n for d = 1, . . . , D, where a n \u223c b n means a n = O(b n ) and b n = O(a n ).\nWe also assume that the rank R is fixed in the tensor GEE. Next we list the required regularity conditions. Since the conditions (A1), (A2), (A5)-(A7) are the same as in Section 3.1, we only list the conditions that are different, while we relabel those same conditions as (A1 * ), (A2 * ), (A5 * )-(A7 * ), respectively.\n(A3 * ) There exist two positive constant c 1 < c 2 such that\nover the set {B : ||\u03b2 B \u2212 \u03b2 B 0 || \u2264 p n /n} for some constant > 0. It is also assumed that I(B) has a constant rank on the same set.\n(A4 * ) The true intra-subject correlation matrix R 0 has bounded eigenvalues from zero and infinity. The estimated working correlation matrix satisfies\nwhere \u00b7 F is the Frobenius norm,R is some positive definite matrix with bounded eigenvalues from zero and infinity, andR = R 0 is not required."}, {"section_title": "(A8", "text": "* ) It is assumed that \u00b5 (1) (\u03b8 ij ) are uniformly bounded away from zero and infinity, and \u00b5 (k) (\u03b8 ij ) are uniformly bounded by a finite constant, over the set {B : ||\u03b2 B \u2212 \u03b2 B 0 || \u2264 p n /n}, for some constant > 0, i = 1, . . . , n, j = 1, . . . , m, and k = 2, 3.\n(A9 * ) There exist two positive constants c 3 < c 4 such that\nover the set {B : ||\u03b2 B \u2212 \u03b2 B 0 || \u2264 p n /n} for some constant > 0, i = 1, . . . , n and j = 1, . . . , m.\nComparing the two sets of regularity conditions for the fixed and diverging number of parameters, the main difference is that the conditions are imposed on the set {B :\n||\u03b2 B \u2212 \u03b2 B 0 || \u2264 p n /n} when the number of parameters diverges. This is due to the slower convergence rate of the tensor GEE estimator with a diverging p n . In addition,\nwe note that I(B) and H(B, X ij ) are no longer matrices with fixed dimensions when p n diverges. Correspondingly, we impose conditions (A3*) and (A9*) on the bounded eigenvalues, which are similar to the sparse Riesz condition for vector covariates. The latter condition has been frequently employed in the current literature of inference with diverging dimensions (Zhang and Huang, 2008; Zhang, 2010) .\nNext we present the asymptotics for the tensor GEE estimator with a diverging p n .\nTheorem 3. Under conditions (A1*)-(A9*), and p n = o(n 1/2 ), there exists a root B of the equations s(B) = 0 satisfying that\nIt is important to note that, if one directly vectorizes the tensor covariate and applies the asymptotics of the classical GEE as in Wang (2011), the conditions for the consistency would require\n). This rate can be much more stringent for a tensor covariate. Theorem 3, instead, states that the consistency still holds with p n = o(n 1/2 ), after imposing and exploiting the low rank tensor structure on the coefficients array.\nThe asymptotic normality can also be established for a diverging p n . \nSimilarly, for the asymptotic normality to hold, the condition would have become p n = o(n 1/(3D) ) if one directly vectorizes the tensor covariate. By contrast, the tensor GEE requires p n = o(n 1/3 )."}, {"section_title": "Simulations", "text": "We have carried out extensive simulations to investigate the finite sample performance of our proposed tensor GEE approach. We adopt the following simulation setup. We generated the responses according to the normal model\nwhere\n2 is a scale parameter, and R 0 is the true m\u00d7m intra-subject correlation matrix. We have chosen R 0 to be of an exchangeable (compound symmetric) structure with the off-diagonal coefficient \u03c1 = 0.8. The mean function is of the form\nwhere Z ij \u2208 IR 5 denotes the covariate vector, with all elements generated from a standard normal distribution, and \u03b3 \u2208 IR 5 is the corresponding coefficient vector, with all elements equal to one; X ij \u2208 IR 64 \u00d764 denotes the 2D matrix covariate, again with all elements from standard normal, and B \u2208 IR 64 \u00d764 is the matrix coefficient. B takes the value of 0 or 1, and contains a series of shapes as shown in Figure 1 , including \"square\", \"T-shape\", \"disk\", \"triangle\", and \"butterfly\". Our goal is to recover those shapes in B by inferring the association between Y ij and X ij after adjusting for Z ij ."}, {"section_title": "Signal Recovery", "text": "As the true signal in reality is hardly of an exact low rank structure, the tensor model (4) and the associated tensor GEE (5) essentially provide a low rank approximation to the true signal. It is thus important to verify if such an approximation is adequate. We set n = 500, m = 4, and show both the tensor GEE estimates under various ranks and the corresponding BIC values (7) in Figure 1 . We first assume that the correlation structure is correctly specified, and will study potential misspecification in the next section. In this setup, \"square\" has the true rank equal to 1, \"T-shape\" has the rank 2, and the remaining shapes have the highest possible rank 64. It is clearly seen from the figure that the tensor GEE offers a sound recovery of the true signal, even for the signals with high rank or natural shape, e.g., \"disk\" and \"butterfly\". In addition, the BIC seems to identify the correct or best approximate rank for all the signals."}, {"section_title": "Effect of Correlation Specification", "text": "We have shown that the tensor GEE estimator remains asymptotically consistent even when the working correlation structure is misspecified. However this describes only Figure 1: True and recovered image signals by the tensor GEE with varying ranks. n = 500, m = 4. The correlation structure is correctly specified. TR(R) means estimate from the rank-R tensor model. the large sample behavior. In this section, we investigate potential effect of correlation misspecification when the sample size is small or moderate.\nWe chose the \"butterfly\" signal and fitted the tensor GEE model with three different working correlation structures: exchangeable, which is the correct specification in our setup, autoregressive of order one (AR-1), and independent. Table 1 reports the averages and standard errors out of 100 replicates of the squared bias, the variance, and the mean squared error (MSE) of the tensor GEE estimate. We observe that the estimator based on the correct working correlation structure, i.e., the exchangeable structure, performs better than those based on misspecified correlation structures. When the sample size is moderate (n = 100), all the estimators have comparable bias, while the difference in MSE mostly comes from the variance part of the estimator. This agrees with the theory that the choice of the working correlation structure affects the asymptotic variance of the estimator. When the sample size becomes relatively large (n = 150), all the estimators perform similarly by the scaling term of n \u22121/2 on the variance. When the sample size is small (n = 50), all the estimators have relatively large bias, while the independence working structure yield similar results as the exchangeable structure. This suggests that, when the sample size is limited, using a simple independence working structure is probably preferable compared to a more complex correlation structure.\nNevertheless, we should bear in mind that the above observations are for the average behavior of the estimate. Figure 2 shows two snapshots of the estimated signals under the three working correlations at n = 100. The top panel is one replicate where the estimates are \"close\" to the average in the sense that the bias, variance and MSE values for this single data realization are similar to those averages reported in Table 1 . Consequently, the visual qualities of the three recovered signals are similar. The bottom panel, on the other hand, shows another replicate where the estimates are \"far away\" from the average.\nThen the quality of the estimated signal under the correct working correlation structure is superior than the ones under the incorrect specifications. Such an observation suggests that, as long as the sample size of the study is moderate to large, a longitudinal model should be favored over the one that totally ignores potential intra-subject correlation."}, {"section_title": "Regularized Estimation", "text": "We implemented the regularized tensor GEE with a lasso penalty, which extends the penalized GEE method of Wang et al. (2012) from vector to array covariate. It can identify relevant regions in images that are associated with the outcome, and this region selection problem corresponds to variable selection in classical vector covariate regressions. We studied the empirical performance by adopting the simulation setup described at the beginning of Section 4, but varying the sample size. The estimates of three shapes, \"T-shape\", \"triangle\", and \"butterfly\", with and without regularizations, are shown in Figure 3 . For the regularized tensor GEE, the penalty parameter \u03bb was selected based on the prediction accuracy on an independent validation set. It is clearly seen from the plot that, while increasing sample size improves estimation accuracy for both tensor GEE and regularized tensor GEE, regularization leads to a more accurate recovery, especially when the sample size is limited. As such we recommend the regularized tensor GEE for longitudinal imaging data analysis in practice."}, {"section_title": "Real Data Analysis", "text": ""}, {"section_title": "Alzheimer's Disease", "text": "Alzheimer's Disease (AD) is a progressive and irreversible neurodegenerative disorder and the leading form of dementia in elderly subjects. It is characterized by gradual impairment of cognitive and memory functions, and it has been projected to quadruple in its prevalence by the year 2050 (Brookmeyer et al., 2007) . Amnestic mild cognitive impairment (MCI) is often a prodromal stage to Alzheimer's disease, and individuals with MCI may convert to AD at an annual rate as high as 15% (Petersen et al., 1999) .\nAs such there is a pressing need for accurate and early diagnosis of AD and MCI, as well as monitoring their progression. The data we analyzed was obtained from the place, the immediate and delayed recall of three words, the attention and calculations, language, and visuoconstructional functions (Folstein et al., 1975) , and is our response variable. A detailed description of acquiring MRI data from ADNI and the preprocessing protocol can be found in Zhang et al. (2012) . There are two scientific goals for this study.\nOne is to predict the future clinical scores based on the data at previous time points, which is particularly useful for monitoring disease progression. The second is to identify brain subregions that are highly relevant to the disorder. We fitted tensor GEE to this data for both score prediction and region selection."}, {"section_title": "Prediction and Disease Prognosis", "text": "We downsized the original 256 \u00d7 256 \u00d7 256 MRI images to 32 \u00d7 32 \u00d7 32 via interpolation for computational simplicity. We first fitted tensor GEE using the data from baseline to 12-month, and used prediction of MMSE at 18-month to select the tuning parameter \u03bb.\nThen we refitted the model using the data from baseline to 18-month under the selected \u03bb, and evaluated the prediction accuracy of all subjects using the \"future\" MMSE score at 24-month. The accuracy was evaluated by the rooted mean squared error (RMSE),\n, and the correlation, Corr(Y im ,\u0176 im ). This evaluation scheme is the same as that of Zhang et al. (2012) . Table 2 summarizes the results. It is seen that, for this data set, the best prediction was achieved under an AR(1) working correlation structure with L 1 regularization. The corresponding RMSE and correlation were 2.270 and 0.747, which are only slightly worse than the best reported RMSE 2.035 and correlation 0.786 in Zhang et al. (2012) . Note that Zhang et al. (2012) used multiple imaging modalities and additional clinical covariates, which are supposed to improve the prediction accuracy, while our study utilized only one imaging modality."}, {"section_title": "Region Selection", "text": "We applied the lasso regularized tensor GEE to this data, and Figure 4 shows the estimate (marked in red) overlaid on an image of an arbitrarily chosen subject, with three (Braak and Braak, 1991; Desikan et al., 2009; Yao et al., 2012) . With AD, patients experience significant widespread damage over the brain, causing shrinkage of brain volume (Yao et al., 2012; Harasty et al., 1999) and thinning of cortical thickness (Desikan et al., 2009; Yao et al., 2012) . The affected brain regions include those involved in controlling language (Broca's area) (Harasty et al., 1999) , reasoning (superior and inferior frontal gyri) (Harasty et al., 1999) , part of sensory area (primary auditory cortex, olfactory cortex, insula, and operculum) (Braak and Braak, 1991; Lee et al., 2013) , somatosensory association area (Yao et al., 2012; Tales et al., 2005; Mapstone et al., 2003) , memory loss (hippocampus) (den Heijer et al., 2010) , and motor function (Buchman and Bennett, 2011) . It is interesting to note that these regions are affected starting at different stages of AD, indicating the capability of the proposed method to locate brain atrophies as disease progresses. Specifically, hippocampus, which is highly correlated to memory loss, is commonly detected at the earliest stage of the disease. Regions related to language, communication, and motor functions are normally detected at the later stages of the disease. The fact that our findings are consistent with the results reported in previous studies demonstrates the efficacy of our proposed method in identifying correct biomarkers that are closely related to AD/MCI. Figure 4 : The ADNI data: regularized estimate overlaid on a randomly selected subject."}, {"section_title": "Discussions", "text": "We have proposed a tensor GEE approach for analyzing longitudinal imaging data.\nOur method combines the powerful GEE idea for handling longitudinal correlation and the low rank tensor decomposition to reduce the vast dimensionality of imaging data.\nThe proposed algorithm scales well with imaging data size and is easy to implement using existing statistical softwares. Simulation studies and real data analysis show the advantage of our method for both signal recovering and prediction.\nIn the current paper, we have considered an image covariate together with a conventional vector covariate. Extending to joint multi-modality imaging analysis is conceptually easy: simply adding more array covariates into the systematic component (4).\nHowever this brings up other issues such as joint selection of ranks for multiple array covariates, properly defining interactions between tensor covariates, and even higher volume of data. These important yet nontrivial questions deserve further investigation."}, {"section_title": "Appendix: Technical Proofs", "text": "Outline of the proofs\nWe prove the results for the diverging case (Theorem 3 and Theorem 4) in the appendix.\nOne can prove the results for the fixed case (Theorem 1 and Theorem 2) by using the same techniques below and replacing p n with a fixed positive constant.\nThe proof of Lemma 1 is similar to the one of Theorem 3 by dropping the terms involving the working correlation matrix and thus is omitted here.\nTo facilitate the proof, we introduce the following notations. Denote \u03b2 n = \u03b2B and\nRecall that the CP decomposition ensures that B is uniquely determined\n, and note that under tensor structure\nRecall the generalized estimating equations without vector covariates can be written as\nThe main technique to prove Theorem 3 is the sufficient condition for existence and consistency of a root of equations proposed in Ortega and Rheinboldt (2000) . To check this condition, the following Lemma 2 -4 are proposed. Lemma 2 provides a useful approximation to the generalized estimating equations s n (\u03b2 0 ) based on the Condition (A4*) of the working correlation matrix. This facilitates the later evaluations of the moments of the generalized estimating equations by treating the intra-subject correlation as known. Lemma 3 further establishes the approximation to the negative gradients of the generalized estimating equations. Lemma 4 refines this approximation to the negative gradients at one more step, providing the foundations for the Talyor expansion of generalized estimating equations at the true value.\nBased on Theorem 3, the proof of Theorem 4 is straightforward by evaluating the covariance matrix of the generalized estimating equations and applying the LindebergFeller central limit theorem.\nProof of Lemma 2. Consider\nDenote {r i,j } 1\u2264i,j\u2264m the (i, j)-th element of\nfor some constant C > 0 by Condition (A1*), (A2*) and (A7*). Since\nthe proof is complete.\nlishes the approximation to the negative gradients of the estimating equations.\nLemma 3. Under Conditions (A1*)-(A9*), for any > 0,\nProof of Lemma 3. Similar to Lemma C.1. of Wang (2011) , it can be shown by direct calculation thatD\nwith\nim (\u03b2 n ) ,\nj length m vector with j-th element 1 and 0 everywhere else, and H(\u03b2 n , X ij ) = \u2202J T (\u03b2 n )vec(X ij )/\u2202\u03b2 T n . Let D ni (\u03b2 n ) be defined the same asD ni (\u03b2 n ), but withR replaced by R, for i = 1, . . . , 4. It is sufficient to prove\nFor i = 1, we have\nBy Condition (A4*) and (A6*), |u\nFor i = 2, we have\nwhere\nFor J n1 , by Cauchy-Schwarz inequality for matrices with Frobenius norm,\nwhere \u03b2 n is between \u03b2 n and \u03b2 0 . By Conditions (A3*), (A4*) and (A8*), sup ||\u03b2n\u2212\u03b2\nSimilarly to J n1 , it can be shown sup ||\u03b2n\u2212\u03b2 0 ||\u2264 \u221a pn/n\nFor J n22 , similar to the decomposition of A 1/2 i (\u03b2 n ), we can further decompose those terms involving \u03b2 n into terms that only depend on \u03b2 0 and four other terms involving \u03b2 n .\nOn the set {\u03b2 n : ||\u03b2 n \u2212 \u03b2 0 || \u2264 p n /n}, similar to J n1 , under Conditions (A1*)-(A9*), all those terms involving \u03b2 n can be shown to be O p ( \u221a p n n). To complete the evaluation of J n22 and hence J n2 , it suffices to show\nDenote L n (\u03b2 0 ) the left side of (10). Recall that ij (\u03b2 0 ) = \u03c3 \u22121 ij (\u03b2 0 )(Y ij \u2212 \u00b5 ij (\u03b2 0 )). We have \nUsing similar decompositions, we can verify the results for D n3 and D n4 , which completes the proof.\nBased on Lemma 3, we can further approximateD n (\u03b2 n ) byD n1 (\u03b2 n ), which is easier to evaluate. Lemma 4 provides this approximation. ForD n2 (\u03b2 n ), we have the decompositio\u00f1 D n2 (\u03b2 n ) =D n2 (\u03b2 0 ) + for some constant C > 0. This implies that I n11 = p n /nO p ( \u221a np n ) = O p (p n ). For I n12 , by Lemma 2, I n12 \u2264 ||\u03b2 n \u2212 \u03b2 0 || \u00b7 ||s n (\u03b2 0 ) \u2212s n (\u03b2 0 )|| = o p (p n ).\nTherefore, I n1 is dominated in probability by I n11 .\nFor I n2 ,we decompose it into\nwhich completes the proof of (14).\nTo prove Theorem 2, note that by the fact s n ( \u03b2 n ) = 0, we have s n (\u03b2 0 ) = D n (\u03b2 * n )( \u03b2 n \u2212 \u03b2 0 ) for some \u03b2 * n between \u03b2 n and \u03b2 0 . Hence, min (M n (\u03b2 0 )) = O(n \u22121 ), it can be seen that\nn n \u22121/2 ).\nUsing the stronger assumption that p = o(n \u22121/3 ), I n1 = o p (1). Similarly, by Lemma 4, we have I n2 = o p (1) and I n3 = o p (1). Therefore J n1 has the same asymptotic distribution as in (14), which completes the proof."}]