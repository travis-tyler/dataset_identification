[{"section_title": "Abstract", "text": "Abstract. In the context of Alzheimer's disease (AD), state-of-the-art methods separating normal control (NC) from AD patients or CN from progressive MCI (mild cognitive impairment patients converting to AD) achieve decent classification rates. However, they all perform poorly at separating stable MCI (MCI patients not converting to AD) and progressive MCI. Instead of using features extracted from a single temporal point, we address this problem using descriptors of the hippocampus evolutions between two time points. To encode the transformation, we use the framework of large deformations by diffeomorphisms that provides geodesic evolutions. To perform statistics on those local features in a common coordinate system, we introduce an extension of the K\u00e4rcher mean algorithm that defines the template modulo rigid registrations, and an initialization criterion that provides a final template leading to better matching with the patients. Finally, as local descriptors transported to this template do not directly perform as well as global descriptors (e.g. volume difference), we propose a novel strategy combining the use of initial momentum from geodesic shooting, extended K\u00e4rcher algorithm, density transport and integration on a hippocampus subregion, which is able to outperform global descriptors."}, {"section_title": "", "text": "Abstract. In the context of Alzheimer's disease (AD), state-of-the-art methods separating normal control (NC) from AD patients or CN from progressive MCI (mild cognitive impairment patients converting to AD) achieve decent classification rates. However, they all perform poorly at separating stable MCI (MCI patients not converting to AD) and progressive MCI. Instead of using features extracted from a single temporal point, we address this problem using descriptors of the hippocampus evolutions between two time points. To encode the transformation, we use the framework of large deformations by diffeomorphisms that provides geodesic evolutions. To perform statistics on those local features in a common coordinate system, we introduce an extension of the K\u00e4rcher mean algorithm that defines the template modulo rigid registrations, and an initialization criterion that provides a final template leading to better matching with the patients. Finally, as local descriptors transported to"}, {"section_title": "Introduction", "text": "Large scale population studies aim to improve the understanding of the causes of diseases, define biomarkers for early diagnosis, and develop preventive treatments. For Alzheimer's disease, several classification strategies have been proposed to separate patients according to their diagnosis. These methods can be split into three categories: voxel-based [1] [2] [3] [4] [5] [6] [7] , cortical-thickness-based [1, 8, 9] and hippocampus-based [10] [11] [12] methods. While decent classification rates can be achieved to separate AD from CN or CN from progressive MCI (MCI patients converting to AD), all methods perform poorly at separating stable MCI (MCI patients non converting to AD) and progressive MCI. A recent review comparing these methods can be found in [13] .\nIn this paper, we investigate the use of longitudinal evolution quantifiers either local or global to separate between stable MCI and progressive MCI. To extract information between two successive time-points, we use a one-to-one deformation mapping the first image onto the second one. Different registration algorithms are available to compute plausible deformations in this context. However, only one, the large deformations via diffeomorphisms (LDD) [14] , provides a Riemannian setting that enables to represent the deformations using tangent vectors: initial velocity fields or equivalently initial momenta. This can be used in practice to retrieve local information and to perform statistics on it as presented in [15, 16] . In this direction, it is worth mentioning paper [17] which shows the correlation between principal modes of deformations and diagnosis. In order to compare this information among the population, we need to define a common coordinate system. This implies (1) the definition of a template and (2) a methodology for the transport of the tangent vector information.\nIn the literature, point (1) is addressed via different methods [18, 19] . Combining geodesic shooting algorithm presented in [20] , we chose to develop a K\u00e4rcher method to average a set of shapes. A first approach has been presented in [21] . A natural requirement on the K\u00e4rcher average is that it could be invariant with respect to rigid transformations of each subject of the population. However, this is not the case in [21] . One of the contribution of the present paper is to propose a methodology to define such invariant K\u00e4rcher averages. We also use a finer strategy to update the deformations at each iteration of the algorithm. Point (2) benefited in each different settings from various contributions that go beyond the standard transport actions. The key-point in our application is that inter-subject variability is much higher than the longitudinal variation so that one expects the statistical results to be strongly influenced by the choice of the transport. To address this issue, parallel transport has been proposed in the LDD setting in [22] and it has been applied to longitudinal data discrimination, very similar to our problem, in [23] . Note that parallel transport preserves the norm of the velocity field and since this norm is not invariant with respect to rescaling, the population variability is still contained in the parallel transport of the tangent information. For other frameworks, such as Log-demons, Schild's ladder approach has been introduced in [24] to extend parallel transport to their Lie-group setting. In any case, we consider the question of transporting tangent information as still open and this motivates us to compare different transport strategies in the classification step. Section 2 introduces the global pipeline used to perform statistics from local descriptors of hippocampus deformations, with details on geodesic shooting, population template and transport. Section 3 presents the data used and the numerical results. Section 4 concludes the paper."}, {"section_title": "hal-00764163, version 1 -12 Dec 2012", "text": "Title Suppressed Due to Excessive Length 3 2 Methods\n4 Authors Suppressed Due to Excessive Length vector fields. The deformation \u03a6 is given by the flow of v t . Namely, for any x \u2208 \u2126 the domain of the images,\nThe first term in formula (1) is a similarity term controlling the matching quality whereas the second one is a smoothing term controlling the deformation regularity. The minimization problem (1) can be reformulated using a shooting formulation on an initial condition which is the so-called initial momentum denoted P (0), a scalar field in the image space, as follows:\nsubject to the geodesic shooting equations:\nwhere I is the source image, v t the velocity field, P the momentum, K the translation-invariant kernel of the reproducing kernel Hilbert space, and the convolution operator. In order to solve the new optimization problem (2), we use the methodology described in [20] . Note that the choice of the kernel matters for retrieving plausible deformations and we refer to [25] for an extensive discussion on the parameter choices. For each patient, a two-step process was performed to encode the deformations of the hippocampus shape evolution, as described in Fig. 2 . First the hippocampus from the second time point was rigidly registered back to the first time point. Second, the geodesic shooting was performed from the first time point towards the registered second time point. Inital momenta and initial velocity fields from different patients are local descriptors that were used to compare hippocampus evolutions."}, {"section_title": "Global pipeline", "text": "Let us assume we have a population of patients and the binary segmentation of their hippocampus at two different time points. Let us also assume that all patients have the same diagnosis at the first time point, and only a part of them have converted to another diagnosis at the second time point. Our goal is to compare patient evolutions, and classify them with regard to stable diagnosis versus progressive diagnosis.\nWe use the pipeline summarized in Fig. 1 to compute descriptors of patient evolutions in a common space. First, the evolution descriptors is computed locally for each patient (independently). To be able to compare these descriptors, one needs to transport them in a common space. To do so, a population template is computed, towards which all the local descriptors are transported. Finally, classification is performed to separate progressive from stable patients. Several local descriptors were tested: initial momentum and initial velocity field of geodesic shooting. The use of subregion and integration were also introduced. As for global shape deformation descriptors, volume variation and relative volume variation were tested. (1) the local descriptors are computed for each patient independently, (2) a population template is computed, (3) all local shape deformation descriptors are transported towards this template, (4) SVM classification is performed for local descriptors on the whole domain and on a subregion, and local descriptors integrated on the whole domain and on a subregion, and compared with results using global descriptors."}, {"section_title": "Geodesic shooting", "text": "The LDD framework introduces the following minimization problem:\nwhere I is the source image, J the target image, and v t is a time dependent velocity field that belongs to a reproducing kernel Hilbert space of smooth enough"}, {"section_title": "Population template", "text": "As mentioned in section 2.1, local descriptors of hippocampus evolutions need to be transported in a common space prior to any statistical analysis. We introduce two modifications of the standard K\u00e4rcher mean algorithm [21] : (1) the population template is computed up to rigid transformations and (2) the template is regenerated from a reference image at every iteration. We propose the following optimization problem:K = argmin\nStep 1. Rigid registration\nStep 2. Shooting Input:\n-source image: J (follow-up, t = t0 + 12 months) -target image: I (screening, t = t0) Output:\n-source image: I -target image: R(J)\nOutput:\n-initial momentum P 0 Fig. 2 : For each patient, the initial momentum encoding the hippocampus evolution is computed in a two-step process. First, the second time point (t = 12 months) is rigidly registered to the first time point (t = 0). Second, the geodesic shooting is computed from the first time point to the previously rigidly registered second time point.\nwhere {I 1 , . . . , I n } are the population images, {R 1 , . . . , R n } are the rigid transformations registering the I i towards K.\nThe proposed associated algorithm initializes the deformation field generating the template from the reference as the identity: u 0 = Id. Each K\u00e4rcher iteration is then composed of four steps described in Fig. 3 .\nThe first modification is introduced because current implementations of LDD algorithms are not invariant by the action of the group of orthogonal linear transformations, so the resulting template reflects the orientation variability of the population. Numerically speaking, such a modification can potentially help avoiding convergence issues of the geodesic shootings (if the template estimate and the image are too far with respect to the LDD metric), and reduce computation time. The second modification of the standard K\u00e4rcher algorithm is purely motivated by numerical considerations. The standard formulation smoothes the template estimate at each iteration (as K t+1 is computed by shooting from K t ). In our algorithm, the template is regenerated from the reference image at each step, so that the template estimate keeps sharp boundaries."}, {"section_title": "Tangent information and associated transport", "text": "The local descriptors computed for each patient as explained in section 2.2 need to be transported in a common coordinate space: the space of the K\u00e4rcher average defined in section 2.3. We chose transport rules that only depend on the final deformation (it does not include parallel transport which depends on the chosen path). A two-step process was then used to transport local descriptors (2) geodesic shootings from the current K\u00e4rcher estimate K t towards all the registered images are computed (3) geodesic shooting from K t using P 0 = 1 n i P 0 i generates a deformation field u tmp , and (4) the composed deformation field u t+1 = u tmp \u2022 u t is used to compute the updated estimate from the reference image.\nof hippocampus evolutions to the template space (Fig. 4) . First, the screening hippocampus was registered towards the template rigidly [26] then non-rigidly [27] . The resulting deformation is denoted by \u03c6. Second, this transformation was used to transport the local descriptors of hippocampus deformations towards the template. The transport itself depends on the nature of the quantity transported. For instance, we call image transport the standard transformation of an image by the action of a diffeomoprhism \u03c6:\nFrom the mathematical point of view, the momentum is an adjoint variable to the image. As a consequence, it is transported by the adjoint action of the group which reduces to the standard transport for a density n, defined by:\nOn the theoretical side, this transport preserves geodesics, due to the Ad invariance of momentum maps: we refer the reader to [28] for that property. Note that this action preserves the global integration of the density by a simple change of variable. Last, we present the transport via the standard conjugation of a velocity field defined by:\nhal-00764163, version 1 -12 Dec 2012\nTitle Suppressed Due to Excessive Length 7\nAll those transport methods were tested in the classification step. We did not include parallel transport in this study since no public implementation is available and its implementation is rather involved, especially in the case of images. "}, {"section_title": "Material and Results", "text": ""}, {"section_title": "Data", "text": "A dataset of 206 hippocampus binary segmentations from 103 patients enrolled in ADNI 1 [29] has been used to estimate the efficiency of local and global descriptors of hippocampus evolution with regard to disease progression. For each patient, 'screening' and 'month 12' were the two time points selected. All patients were MCI at the screening point, 19 became AD by month 12, and the remaining 84 stayed MCI."}, {"section_title": "Experiments", "text": "First, all screening images were resampled to a common isotropic voxel size 1.0 \u00d7 1.0 \u00d7 1.0 mm, similar to their original size. Rigid transformations aligning the month 12 hippocampus towards the screening ones were computed using [26] . The geodesic shootings [20] were performed 2 using a sum of three kernels (sizes 1, 3 and 6 mm, with respective weights 2, 1 and 1), and 200 gradient descent iterations. All the 103 source images deformed by the shooting had a final Dice score (i.e. overlap score) with their target images over 0.85."}, {"section_title": "Authors Suppressed Due to Excessive Length", "text": "To compute the template, a subset of 20 images was used. This subset and the initialization was based on a shape volume criterion. Four K\u00e4rcher iterations were performed, with respectively 200, 150, 150 and 100 gradient descent iterations in the geodesic shootings. To compute the transformations from the screening hippocampi towards the template, rigid [26] then non-rigid [27] registration algorithms were applied with their default parameters.\nTo classify from local descriptors, a mask computed by dilating the template was used. To compute classification on subregions, each hippocampus (left and right) from the template was dilated. The bounding box was cut equally in thirds along the longest axis, and intersections were used as masks.\nUsing a leave-10%-out scheme, training and test sets were created. With training features equally distributed among classes, SVM classifiers were computed (the Matlab functions from the Bioinformatics Toolbox were used). All the patients were then classified. The Gaussian kernel was used and 20 kernel widths were tested. This procedure was repeated 50 times and classification accuracy averaged. From the numbers of true/false positives/negatives (TP, FP, TN, FN), four indicators were used to measure classification accuracy: specificity Spec = T N T N +F P , sensitivity Sens = T P T P +F N , negative predictive value N P V = T N T N +F N , and positive predictive value P P V = T P T P +F P ."}, {"section_title": "Results", "text": "Using the modified K\u00e4rcher mean algorithm and the criterion mentioned above, the average Dice score between the 103 registered patients and the template was 0.87 \u00b1 0.02, whereas it was only 0.44 \u00b1 0.11 when matching to a template computed using a criterion based on the distance to the L2 mean.\nRegarding descriptors of hippocampus evolutions, the local descriptors did not perform as well as global descriptors, when used directly as input features (Fig. 5) . However, when integrated on the whole domain, the performances were similar. When integrated on some subregion, they can outperform the global descriptors. Detailed results are displayed in Fig. 5 , and Table 1 displays the four unbiased indicators when the sum of specificity and sensitivity is maximized."}, {"section_title": "Discussion and conclusion", "text": "We have studied the use of global, semi-local and local descriptors of hippocampus evolutions to predict AD conversion for MCI patients, using a dataset of binary segmentations provided by ADNI. This study focuses on shape evolutions between two time points, whereas (to the best of our knowledge), studies in this application field usually extract features from a single time point and perform diagnosis classification.\nThe proposed extension of the K\u00e4rcher mean algorithm with a subpopulation and initialization criterion based on shape volume improved the matching quality to the template (average Dice of 0.87 \u00b1 0.02 instead of 0.44 \u00b1 0.11) without the need of modifying the default registration parameters. : Classification performance (depending on the SVM Gaussian kernel width \u03c3) for global descriptors (5a, 5b), local (5c, 5f, 5i), local integrated on the whole image (5d, 5g, 5j) and local integrated on a subregion (5e, 5h, 5k). Higher for Spec + Sens (in cyan blue) is better.\nhal-00764163, version 1 -12 Dec 2012 In our experiments, the local descriptors did not perform as well as global descriptors such as volume difference when they were directly used as input features of the SVM classifiers. However, when integrated over the whole domain, classification performances were similar. When integrated on a subregion, they could even outperform the global descriptors. The method we propose combines (1) the use of initial momentum of geodesic shooting, (2) an extended version of the K\u00e4rcher mean algorithm, (3) the use of density transport and (4) the integration on a subregion. On our dataset, this method was the only one able to outperform the global descriptors. It should be noted that in our study the definition of the subregion was sub-optimal and used as a proof-of-concept. The most promising perspectives are (1) developing a strategy to define subregions maximizing the classification results and (2) adding more time-points to the study using the geodesic regression method introduced in [30] or cubic spline interpolation in [31] ."}]