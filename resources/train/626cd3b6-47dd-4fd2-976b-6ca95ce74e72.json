[{"section_title": "Abstract", "text": "Abstract. In recent studies of Alzheimer's disease (AD), it has increasing attentions in identifying mild cognitive impairment (MCI) converters (MCI-C) from MCI non-converters (MCI-NC). Note that MCI is a prodromal stage of AD, with possibility to convert to AD. Most traditional methods for MCI conversion prediction learn information only from MCI subjects (including MCI-C and MCI-NC), not from other related subjects, e.g., AD and normal controls (NC), which can actually aid the classification between MCI-C and MCI-NC. In this paper, we propose a novel domain-transfer learning method for MCI conversion prediction. Different from most existing methods, we classify MCI-C and MCI-NC with aid from the domain knowledge learned with AD and NC subjects as auxiliary domain to further improve the classification performance. Our method contains two key components: (1) the cross-domain kernel learning for transferring auxiliary domain knowledge, and (2) the adapted support vector machine (SVM) decision function construction for cross-domain and auxiliary domain knowledge fusion. Experimental results on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database show that the proposed method can significantly improve the classification performance between MCI-C and MCI-NC, with aid of domain knowledge learned from AD and NC subjects."}, {"section_title": "Introduction", "text": "Alzheimer's disease (AD) is the most common form of dementia in elderly people worldwide. Early diagnosis of AD is very important for possible delay of the disease. Mild cognitive impairment (MCI) is a prodromal stage of AD, which can be further categorized into MCI converters (MCI-C) and MCI non-converters (MCI-NC). The former will convert into AD in follow-up time, while the latter will not convert. Thus, accurate diagnosis of MCI converters is of great importance. Nowadays, many machine learning methods have been proposed for the classification of AD or MCI [1] [2] [3] . More recently, an increasing number of studies in AD research begin to address MCI conversion prediction, i.e., the classification between MCI-C and MCI-NC based on the baseline imaging data [2] [3] [4] [5] [6] [7] . One challenge in MCI conversion prediction is that the number of MCI (including both MCI-C and MCI-NC) subjects available for training is generally very small and thus the generalization ability of the classifier is limited. The same problem also exists in the classification between AD and NC, where the number of AD and NC subjects are also limited. Recently, to enhance the classification between AD and NC, several studies have used semi-supervised learning (SSL) methods [8] in AD diagnosis, where MCI subjects are treated as unlabeled data to aid AD classification [9] [10] [11] . Semi-supervised learning methods can efficiently utilize unlabeled samples to improve classification and regression performance, but it requires unlabeled samples and labeled samples coming from the same data distribution [8] , which is usually not satisfied in practice. For example, Fig. 1 plots the distributions of AD, MCI-C, MCI-NC and NC from ADNI. As can be seen from Fig. 1 , the distribution of MCI is different from that of AD and NC.\nOn the other hand, in the machine learning areas, a new learning methodology called transfer learning has been developed for dealing with problems involving cross-domain learning. Unlike SSL, transfer learning does not assume the auxiliary data (unlabeled data in SSL) have the same distribution as target data (labeled data in SSL), and can effectively adopt those auxiliary data from the related domain for improving classification performance, as validated in several successful applications in computer vision [12] [13] . However, to the best of our knowledge, transfer learning has not been used for addressing brain disease classification problems in medical imaging, although intuitively the inclusion of additional AD and NC subjects as auxiliary data may be beneficial for the classification between MCI-C and MCI-NC. For example, as we can see from Fig. 1 , although the distributions of MCI-C and MCI-NC are different from those of AD and NC, they are very related and the domain knowledge for diagnosing AD and NC may help diagnose MCI conversion.\nIn this paper, we propose a new transfer learning method called Domain Transfer Support Vector Machines (DTSVMs) to classify MCI-C and MCI-NC, with the aid from auxiliary AD and NC subjects. Specifically, we first construct a cross-domain kernel to transfer auxiliary domain knowledge, and then we utilize the adapted SVM to fuse cross-domain and auxiliary domain knowledge for better helping MCI-C and MCI-NC classification. We validate our method on single-modality and multimodality biomarkers, including magnetic resonance imaging (MRI), fluorodeoxyglucose positron emission tomography (FDG-PET), and quantification of specific proteins measured through cerebrospinal fluid (CSF), from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. "}, {"section_title": "Domain Transfer Support Vector Machines (DTSVMs)", "text": "Transfer learning aims to apply knowledge learned from one or more related auxiliary domain to improve the performance on the target domain. In this section, we propose a new method called DTSVM for MCI-C and MCI-NC classification. There are two main steps in DTSVM, i.e., cross-domain kernel learning for transferring auxiliary domain knowledge, and the adapted SVM decision function construction for crossdomain and auxiliary domain knowledge fusion. Fig. 2 shows the flow chart of our proposed DTSVM classification method."}, {"section_title": "Fig. 2. Flow chart of the proposed DTSVM classification method", "text": ""}, {"section_title": "Cross-Domain Kernel for Transferring Knowledge of the Auxiliary Domain", "text": "Because of distribution difference between the auxiliary and the target domains, training with samples from the auxiliary domain may degrade the classification performance in another target domain. So, we cannot directly add auxiliary data to target domains for training. In cross-domain learning, it is crucial to reduce the difference between data distributions of the auxiliary and target domains. To avoid such differences, we use cross-domain kernel learning for transferring auxiliary domain knowledge [13] . Briefly, the auxiliary and target domains are first mapped to the Reproducing Kernel Hilbert Space (RKHS), and then the cross-domain kernel approach is used to build a new kernel function that can reduce the difference between data distributions of the auxiliary and target domains.\nIn the following, we formally address the problem of the cross-domain kernel approach by building a new kernel function. Firstly, we define the kernel matrices from the auxiliary domain A and from the target domain T as:\nand , respectively, where and are samples in the auxiliary domain A, and is the number of samples in domain A. Then, we define the cross-domain kernel matrices from the auxiliary domain to the target domain and from the target domain to the auxiliary domain as: and , respectively, where and are samples in the target domain T, and is the number of samples in domain T. Finally, the kernel matrix is obtained as:\nWhere ."}, {"section_title": "Adapted SVM for Knowledge Fusion", "text": "Assume we have samples and corresponding class labels in the auxiliary domain as , where is a sample and is the class label (i.e., AD as 1 and NC as -1). Also, assume we have samples and corresponding class labels as , where is a sample and is the class label (i.e., MCI-C as 1 and MCI-NC as -1). For the convenience of description in the following, we can also define . According to Section 2.1, we can learn the cross-domain kernel from both auxiliary and target domains. In addition, since AD and NC subjects are sitting in the auxiliary domain while MCI-C and MCI-NC subjects are sitting in the target domain, we can assign the cross-domain label so that the MCI-NC label will be the same as the NC label and the MCI-C label will be the same as the AD label, considering that the learning tasks for the MCI-C vs. MCI-NC classification and the AD vs. NC classification are very related [9] . Here, we adopt the adapted SVM method in [12] to learn the cross-domain classifier , which is the ultimate decision function of our adapted SVM method. This ultimate decision function is first learned from the cross-domain classifier as:\nWhere is a kernel-induced nonlinear implicit mapping, is the parameter vector of the classifier, is the bias term, and x is the cross-domain data obtained by . Also, denotes the transpose of . Because the ultimate decision function can also be learned from the auxiliary domain classifier , Eq. 2 can thus be written as:\nTo learn the weight vector in Eq. 3, we use the following objective function, similar to the SVM:\nis the -th sample in the cross-domain with range of , is the corresponding slack variable, and represents the total number of samples in as indicated above.\nAccording to [12] , we can solve this objective function (in Eq. 4) to obtain the solution for the weight vector . Then, we can obtain the final solution for ."}, {"section_title": "Experiments", "text": "In this section, we evaluate the effectiveness of our proposed DTSVM method on multimodal data, including MRI, PET and CSF, from the Alzheimer\u00cas disease Neuroimaging Initiative (ADNI) database."}, {"section_title": "Experimental Settings", "text": "In our experiments, the baseline ADNI subjects with all corresponding MRI, PET, and CSF data are included, which leads to a total of 202 subjects (including 51 AD patients, 99 MCI patients, and 52 normal controls (NC)). For 99 MCI patients, it includes 43 MCI converters and 56 MCI non-converters. We use 51 AD and 52 NC subjects as auxiliary domains, and 99 MCI subjects as target domains. The same image pre-processing as used in [1] is adopted here. First, for all structural MR images, we correct their intensity inhomogeneity by the N3 algorithm, perform skull-stripping, and remove the cerebellum. Then, we use the FSL package to segment each structural MR image into three different tissue types: gray matter (GM), white matter (WM), and cerebrospinal fluid (CSF). We further use an atlas warping algorithm [14] to partition each structural MR brain image into 93 ROIs. For each of the 93 ROIs, we compute GM volume in that ROI as a feature; For PET image, we use a rigid transformation to align it to its corresponding structural MR image of the same subject, and then compute the average PET value of each ROI as a feature. Accordingly, for each subject, we can acquire 93 features from the structural MR image, another 93 features from the PET image, and 3 features (A\u03b2 42 , t-tau, and p-tau) from the CSF biomarkers.\nTo evaluate the performance of different classification methods, we use a 10-fold cross-validation strategy to compute the classification AUC (areas under the ROC curve), accuracy, sensitivity and specificity. Specifically, the whole set of subject samples are equally partitioned into 10 subsets, and then one subset is successively selected as the testing samples and all remaining subsets are used for training classifiers. This process is repeated 10 times. The SVM classifier training of standard SVM and DTSVM are implemented using LIBSVM toolbox [15] , with a linear kernel and a default value for the parameter C (i.e., C=1). For comparison, LapSVM is also adopted in this paper. LapSVM is a typical semi-supervised learning method based on manifold hypothesis [8] . For LapSVM settings, we use linear kernel, and the graph Laplacian with nodes are connected using k (i.e., k=5) nearest neighbors, and their edge weights are calculated using the Euclidean distance among samples. We use both multi-modality and single-modality biomarkers to validate our method. For combining multimodality data in DTSVM, standard SVM, and LapSVM methods, we specifically use a linear multi-kernel combination technique, with the L weights learned from the training samples through a grid search, using the range from 0 to 1 at a step size of 0.1. Also, for features of each modality, the same feature normalization scheme as used in [1] is adopted here."}, {"section_title": "Results", "text": "We compare DTSVM with LapSVM and standard SVM (i.e., SVM) for both multimodality and single-modality cases. Table 1 shows the classification performance measures of DTSVM, LapSVM, and SVM on different modalities. Note that Table 1 shows the averaged results of 10 independent experiments. As we can see from Table 1 , DTSVM can consistently achieve better results than LapSVM and SVM methods on each performance measure, which validates the efficacy of our DTSVM method on using AD and NC subjects as auxiliary domains for helping classification. Specifically, for multi-modality case, DTSVM can achieve a classification accuracy of 69.4%, which is significantly better than LapSVM and SVM that achieve only 60.8% and 63.8%, respectively. In addition, for multi-modality case, if using the leave-one-out evaluation strategy, DTSVM can achieve a classification accuracy of 70.7%.\nOn the other hand, AUC and ROC are further used to validate the classification performance of DTSVM, LapSVM, and SVM. Table 1 also gives the AUC values of the three methods, and Fig. 3 plots their ROC curves with respect to different modalities. Both Table 1 and Fig. 3 indicate that DTSVM is superior to LapSVM and SVM for MCI-C and MCI-NC classification. Specifically, in the multi-modality case, the AUC value is 0.736 for DTSVM, while the AUC values for LapSVM and SVM are only 0.626 and 0.683, respectively. Finally, in Fig. 4 , we compare DTSVM with LapSVM for classification accuracy, with respect to the use of different number of subjects in the auxiliary domain. As we can see from Fig. 4 , in most cases, the performance of DTSVM is significantly better than LapSVM as the number of subjects in the auxiliary domain increases. This validates the usefulness of adopting domain transfer learning method (i.e. DTSVM) to learn AD and NC domain knowledge, compared to the semi-supervised learning method (i.e., LapSVM). This result further demonstrates that the distribution of MCI is different from that of AD and NC."}, {"section_title": "Conclusion", "text": "This paper addresses the problem of exploiting the use of auxiliary domain data (AD and NC subjects) for helping MCI-C vs. MCI-NC classification. By integrating the cross-domain kernel learning and the adapted SVM methods, we propose a Domain Transfer SVM classification method, namely DTSVM. Our method does not require the auxiliary domain data and the target domain data to be from the same distribution. Experimental results on ADNI dataset validate the efficacy of our proposed method."}]