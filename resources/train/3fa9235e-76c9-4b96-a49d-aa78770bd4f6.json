[{"section_title": "TABLE OF CONTENTS", "text": "Temporal and spatial variability in the northwest Atlantic sound scattering layers from a decade of weekly surveys"}, {"section_title": "Introduction", "text": "Oysters have been a valuable resource for the people around Narragansett bay for over 2000 years, providing a source of food, and in recent years, playing a major role in the local economy. With the rise of aquaculture and efforts to restore wild oyster populations in the bay, water quality monitoring is becoming increasingly more important. Temperature and salinity are both very influential on mortality and growth of oysters (Leonhardt et al., 2017;Lavaud et al., 2017). Temperature affects clearance rate, or how quickly the oysters consume the particles in the surrounding water, and oxygen consumption (Casas, 2018). Salinity generally has a lesser negative effect on clearance rate, but also correlates with a decreased food quality, causing an overall decrease in filtration energy efficiency (Casas, 2018;Lavaud, 2017). Increases in temperature have been shown to decrease susceptibility of oysters to viral infections (Delisle, 2018). Salinity and temperature have both been found to be related to algal blooms, both harmful and non-harmful (Fricke et al., 2018;Lavaud et al., 2017). Current water quality monitoring efforts typically use stationary monitoring points to determine environmental parameters at select locations around a body of water. Stationary water monitoring points ascertain temporal variation quite clearly, but are limited in their determination of spatial variation. Previous research has attempted to calculate spatial variation using a variety of analysis techniques and numerical models (Hajigholizadeh, 2017;Obade, 2018). The proposed method of this project uses an autonomous surface vehicle (ASV) to provide spatially continuous field data. The ASV is a battery-powered kayak body with a central computer that controls an electric motor. It possesses hull-attached sensors that can measure salinity, temperature, dissolved oxygen, turbidity, colored dissolved organic matter (CDOM), and chlorophyll-a along a predetermined path. Depending on the path it follows, it can provide several different types of spatiotemporally continuous data.\nCoastal parks are being affected by sea level rise (slr) and storm surge, and need sea level rise scenarios and storm surge maps to help plan adaptation strategies (Beavers et al., 2016). Of the 417 park units, approximately 25% of these units are on or close to the coastline (Caffrey et al., 2018). These parks protect biodiverse habitats such as salt marshes, estuaries, dunes, coastal forests, and beaches as well as cultural resources (e.g. historic monuments and archeological sites). As sea levels rise, these habitats may experience flooding or more powerful storm surge. To best guide the protection strategies of these habitats, it is essential to have accurate climate change projections at hand. The National Park Service (NPS) supported a study to provide consistent sea level rise and storm surge projections for all its coastal park units. This study, referred to here as the Servicewide Study, used the NOAA based SLOSH model (Sea, Lake, and Overland Storm from Hurricanes) to make sea level rise and storm surge projections. SLOSH is a program that estimates storm surge for a given area (Caffey et al., 2018). The SLOSH model offers two different products to calculate storm surge; Maximum Envelope of Water (MEOW) and Maximum of MEOWs (MOMs). The MEOW gives a worst-case possible projection of the storm surge produced by a given hurricane based on the parameters selected: slosh basin (area of landfall), category of hurricane, speed and direction upon landfall, and tide level. The MOM method consolidates all the MEOWs into one worst case scenario for each category of hurricane that a basin can experience (NOAA, 2018). The Servicewide Study used MOM produced projections for different categories of hurricanes based on the record of each park; the rule was to model one category higher than the highest hurricane on record in each park (Caffrey, et al. 2018). After Hurricane Sandy hit the east coast of the United States, a second study was conducted, also using SLOSH, modeling sea level rise and storm surge projections in three of the parks most affected by Hurricane Sandy. These three parks were Gateway National Recreation Area (GATE), Assateague Island National Seashore (ASIS) and Fire Island National Seashore (FIIS). This study, referred to here as the Sandy Study, was led by the Environmental Data Center at the University of Rhode Island and Applied Science Associates (Bradley et al., 2018). The MOM method from the SLOSH model was also used to make their sea level rise and storm surge projections for all four categories of hurricanes in the three selected parks. Both studies provided sea level projections combined with storm surge for GATE, ASIS, and FIIS. While these projections were calculated using the MOM method of the SLOSH model, the method in how each study modeled the combination of storm surge and sea level rise differed. The SLOSH program offers a default mode that enables the user to use pre-calculated MOM storm surge projections for current day sea levels. The Servicewide Study used these current day MOM storm surge values through default setting in the SLOSH model, without having to rerun the model which would have been computationally intensive to do for all coastal parks, and then added on the sea level rise projections after. The Sandy Study ran the SLOSH model on their future projections of sea level rise instead of using the default current sea level. This produced combined storm surge and sea level rise projections that would be expected to be more accurate in modeling how storm surge at future sea levels will act. The purpose of this research was to investigate where it would be necessary to employ this more computationally involved method and to evaluate how much of a difference is made by using one method of another. Two other differences between the Servicewide Study and the Sandy Study were also taken into consideration to ensure that the sources of the differences found between both studies would be identified correctly. The digital elevation models (DEMs) used by each study were different in all three parks; the Sandy Study used the 2014 National Geodetic Survey Lidar data for all three parks with the addition of the 2014 USGS New York CMGP (Coastal and Marine Geology Program) for Staten Island in GATE (Bradely et al., 2018). The digital elevations used by the Servicewide study were: NY_OKX1_GCS_5m_NAVDm_2011 & NJDEPA_PHI_GCS_10m_NAVDm_2011 for GATE and FIIS, and VA_AKQ_EasternShore_GCS5m_NAVD88m_2011 and MD_PHI_AKQ_GCS_10m_NAVDm_dis_2011 for ASIS . The Servicewide Study used DEMs from 2011 which were before Hurricane Sandy and the Sandy Study used ones from 2014 which were from after Hurricane Sandy. We can expect that the topography of each park has changed from 2011 to 2014, especially in the beach and dune areas. This introduces the possibility that some of the differences between the two studies' projections could have been caused by different land elevation values. Another difference was that the two studies used slightly different sea level rise projections. The sea level rise projections used by the Servicewide Study were based on the Intergovernmental Panel on Climate Change (IPCC) models downscaled through Regional Climate Models local to each park. These projections included four scenarios of carbon emissions ranging from the business as usual scenario to drastic global carbon reductions; the values of each scenario varied from each other within one or two centimeters of sea level rise. Using these sea level rise projections and the SLOSH generated storm surge values, the Servicewide Study issued four different sets of projections for the time horizons of 2030, 2050, and 2100 (Caffrey et al., 2018). The Sandy Study created two sets of projections of sea level rise for 2030 and 2050; a low and high projection varying by 15.2 cm (0.5 ft) (Bradley et al., 2016). This research analyzed the differences of the sea level rise and storm surge projections by the two studies. This is done to assist the NPS decide when it is necessary to use the more computationally involved modeling method as used by the Sandy Study and how much of a difference it makes by using it. The goal of this research is to generalize findings so they can be used in future decision for the parks focused on and for coastal parks not included in the study.\nPer-and polyfluoroalkyl substances (PFASs) are \"fluorinated substances\" that contains the perfluoroalkyl moiety CnF2n+1 - (Buck et al., 2011). The chemical structure of the strong C-F bond made PFASs persistent to natural degradation and bioaccumulative in food webs; thus, these ubiquitous environmental contaminants have drawn much attention at the global scale (Conder et al., 2008;Castiglioni et al., 2015). The first usage of PFASs can be traced back to 1950 in industrial and commercial productions (Buck et al., 2011). Due to their chemical stability, surface tension lowering properties, and water and oil-repelling features, PFASs have been incorporated extensively in metal plating, textile coating, fire-fighting foams, petroleum, and coal product manufacturing (Prevedouros et al., 2006). Products found containing PFASs range from fire-fighting foam (AFFF) to consumer goods such as pizza box, paper, and carpet cleaner. PFASs are key ingredients in surfactants and polymers productions, and they have been produced and used all over the world, including in the US, for several decades. There are two main ways of manufacturing processes. Electrochemical fluorination (ECF), the most important process for manufacturing PFASs, rearranges carbon chains of raw organic material by electrolysis; long chain (6-, 8-, 10-carbon) PFASs, such as perfluoro-n-octanoic acid (PFOA) and perfluoro-1-octanesulfonate PFOS, are produced with this technology (Buck et al., 2011). Telomerization is another technology to produce PFASs commercially to yield long perfluorinated chains CmF2m+1 (CF2CF2) (Buck et al., 2011). PFOA is an essential \"processing aid\" in the manufacture of fluoropolymers. The global emission of total cumulative emission of PFOS-based products was estimated to be 3200-7300 tons between 1951-2002and 820-7180 tons between 2003(Prevedouros et al., 2006Wang et al., 2014). An estimated yearly registered production of 2,3,3,3-tetrafluoro-2-(1,1,2,2,3,3,3-heptafluoropropoxy) propanoic acid (GenX), a PFOA replacement chemical, was 10-100 tons in Europe in 2013. In China, it is estimated that the output of PHxSF-or PBSF-derivatives, ingredients of surfactants treating products, will exceed 1000 tons per year in 2015-2020 (Wang et al., 2013). Despite using traditional processing aid such as ammonium or sodium perfluorooctanoate, many producers developed their own alternatives, such as ADONA and GenX (Wang et al., 2013). PFASs have been detected in animals, humans, and the environment. For example, in the Netherlands, PFOS level in sediments samples collected from the Rhine River system ranged from 0.5-8.7 ng g -1 (Kwadijk, 2010). Additionally, in US surface water, concentrations of PFASs were reported to be 43-244 ng L -1 between 2000 and 2009 (Zhang, 2016). In a study conducted in Rhode Island and New York area, the highest PFAS concentration found in Passaic River, NJ was 56 ng L -1 of PFOA (Zhang, 2016). Other PFASs concentrations found in that area included 26 ng L -1 of PFOS and 43 ng L -1 of PFHxS (Zhang, 2016). Besides industrial and manufacturing sites, airports and firefighting training areas with high usage of aqueous film forming foam (AFFF) are the most important sources of PFASs to the environment (Buck et al., 2011;Kwadijk 2014). For example, in Amsterdam (Netherlands), a concentration of up to 490 \u00b5g L -1 of PFOA was detected in surface water and 14 \u00b5g kg -1 in soil, downstream from a spill (Kwadijk, 2014). Water downstream from a fluorochemical production plant in the Netherlands had concentrations of PFCAs and PFSAs ranging from 36-65 ng L -1 (Gebbink, 2017). The GenX concentration detected at the first downstream sampling site was 13 times higher than the level of PFCA and PFSA together (Gebbink, 2017). PFAS levels decrease as water flows further downstream from a PFASs production plant. As the PFASs are widely used and produced, they are detected in all environmental compartments, including in humans. Humans can be exposed to PFASs directly by drinking water and indirectly by consumer goods that are used every day. The indirect way is through the environment including air, water, and soil that have been contaminated by manufacturing waste. Food intake and drinking water are important PFASs exposure sources; some studies showed elevated PFOA level in serum (Vestergren and Cousins, 2009;Hoffman et al., 2011). PFASs were detected in 98% of human blood samples, and exposure to PFASs has been correlated to a variety of adverse health effects (Calafat et al., 2007). Particularly PFOA, listed in the Candidate List of Substance of Very High Concern by the European Chemicals Agency (ECHA), and PFOS both are persistent and accumulative in human bodies (Wang et al., 2013). Daily exposure can affect thyroid hormone, immune response, obesity, whereas high exposure can be linked to kidney cancer and testicular cancer (Grandjean et al., 2012;Vaughn et al., 2013;Lewis et al., 2015;Braun et al., 2016). With the increasing concerns on the use of PFASs, many regulations have been proposed, though mostly on an advisory level. The European Union restricted the use of PFOS in products at a concentration above 0.0001 wt% with few exemptions (EU 757/2010). The drinking water regulation is neither established in the USA or Europe. The US Environmental Protection Agency (EPA) issued a 70 parts per trillion advisory for PFOA and PFOS in 2016. The European Food Safety Authority recommended 0.15 \u00b5g kg -1 body weight (bw) for PFOS and 1.5 \u00b5g kg -1 bw for PFOA (EFSA, 2008). Since 2000, some of the manufacturers have begun to phase out the production of PFOS and PFOA, replacing long chains with short chains PFAS (Buck et al., 2011;Wang 2013). The EPA 2010/2015 Stewardship made an agreement with 8 major leading companies in PFASs industries to eliminate PFASs usage by 2015 (EPA). With the increasing concerns on the use of PFASs, many regulations have been proposed, though mostly on an advisory level. The European Union restricted the use of PFOS in products at a concentration above 0.0001 wt% with few exemptions (EU 757/2010). The drinking water regulation is neither established in the USA or Europe. The US Environmental Protection Agency (EPA) issued a 70 parts per trillion advisory for PFOA and PFOS in 2016. The European Food Safety Authority recommended 0.15 \u00b5g kg -1 body weight (bw) for PFOS and 1.5 \u00b5g kg -1 bw for PFOA (EFSA, 2008). Since 2000, some of the manufacturers have begun to phase out the production of PFOS and PFOA, replacing long chains with short chains PFAS (Buck et al., 2011;Wang 2013). The EPA 2010/2015 Stewardship made an agreement with 8 major leading companies in PFASs industries to eliminate PFASs usage by 2015 (EPA). In many countries, the sources of PFASs in the environment have been investigated. However, due to the nature of funding and other reasons, data and studies are limited to only assessing bigger production companies, such as 3M. Hence, the effects of potential small sources of PFASs are still unclear. While it is easy to ignore the small contamination sources because of their size, it would be naive to overlook their combined effects since there are more small PFASs companies than big ones. Therefore, this research looks into whether adverse environmental effects can be attributed to small PFASs sources. Here we compare samples collected up-and downstream of a local fluoropolymer-using company. To assess the PFASs levels, in total, nine samples were collected directly from a stream that flows by the production plant (freshwater) and into Narragansett Bay (seawater). Samples were filtered, extracted using a solid-phase extraction (SPE) technique and cleaned up using activated carbon. Thirty PFASs, including 13 perfluorocarboxylic acids (PFCAs), 5 perfluoro sulfonates (PFSAs), 3 polyfluorotelomer sulfonates (FTSs), 3 perfluorooctane sulfonamide (FOSA/Es), 2 alkyl-perfluorooctane sulfonamido acetic acids (FOSAAs), 1 fluorinated telomer unsaturated acid (FTUA), 1 disubstituted polyfluorinated phosphate ester (di-PAP) and 3 new alternatives -ADONA, GenX and F-53B, were separated and detected using liquid chromatography coupled to mass spectrometry (LC-MS).\nInterferometric synthetic aperture radar (InSAR) has become an important tool for measuring surface deformation associated with natural hazards such as earthquakes and volcanoes and their mechanism (Rosen et al., 2000;Burgmann et al., 2000;Massonnet and Feigl, 1998). However, most people use uniform crust structure to invert fault slip (Wei et al., 2009;Barnhart et al., 2013;Elliott et al., 2015), which might introduce errors. The goal of this project is to incorporate layered crust structure to geodetic slip inversion and evaluate whether it will improve the fit to data.\nLake Azuei is a brackish lake located in the Cul-de-Sac basin in Haiti about 30 km east of the capital city of Port-au-Prince. The lake is endorheic (with no outflow to other basins or the ocean), which means that its level is extremely sensitive to variations in precipitation. In the past 10 years there has been ~5-meter rise in lake level (Moknatian et al., 2017). This rise submerged roads, houses, and farmland surrounding the lake. The southern portion of Lake Azuei is presumably cut through by the eastern extension of the Enriquillo-Plantain Garden Fault (EPGF). The EPGF is one of two left-lateral strike slip fault trending E-W, which demarks the boundary between the Caribbean and North American plates. However, the trace of the fault is not well imaged in the surrounding landscape, leading to the publication of competing models, some advocating a purely strike-slip fault through Lake Azuei (e.g. Mann et al., 1995), and other a transpressional fault (Saint Fleur et al., 2015;Symithe and Calais, 2016). The January 12, 2010 M7.0 earthquake occurred along the previously unmapped L\u00e9og\u00e2ne fault, a North-dipping fault that runs subparallel to and abuts the sub-vertical EPGF (Calais et al., 2010). This report contributes new results relevant to the climate history and seismic risk of eastern Haiti by studying sediment cores and seismic reflection profiles collected in Lake Azuei in 2017.\nCoastal and near-shore fisheries are important contributors to many local economies around the world, including in New England. Fishery yield and fishing effort can be highly variable among seasons, years and even decades. Some of this variation is potentially linked to changes in habitat, including water quality. A key diagnostic of water quality is the dissolved oxygen (DO) concentration, which is closely tied to nutrient balance and water stratification. The lack of DO, also known as hypoxia, is a threat to benthic dwellers such as sessile or slow-moving shellfish (Melrose et al., 2007;Levin et al., 2009). DO concentrations have been measured in many coastal regions, but extensive spatial and temporal sampling is required to better understand the variability of DO and its relationship with benthic physical processes in space and time. While certain processes are established drivers of hypoxia, the link between benthic physical processes and hypoxia can be more informed by more investigation. To explore these relationships, we designed a low-cost, autonomous benthic weather station (LoBSTAS 1 ) to monitor seabed dynamics visually and with measurements. Although the LoBSTAS was initially designed to be deployed with lobster traps to include observations of benthic organism interaction with the cages, the system can also be attached to other frames or structures. While underwater imaging systems have been widely used in biological studies and habitat mapping, the LoBSTAS takes a additional approach in focusing on changes of physical processes and hypoxia in space and time. An onboard camera records time-lapse imagery and video segments to reveal benthic currents, sediment evolution, benthic organism activity, and particle suspension in sync with local dissolved oxygen measurements. A goal of the LoBSTAS is to make underwater monitoring affordable for potential use by researchers, managers or the public with limited finances. The low cost also enables production and deployment of a LoBSTAS fleet to understand spatial and temporal variations. With the total cost around $380 and a base cost of $100-150 including housing, computer, and camera, the LoBSTAS offers an imaging system more affordable and compact than industrial towed or dropped imaging systems or low-cost units such as MBARI's SeeStar (Cazenave et al., 2015). The base cost of the LoBSTAS which includes a computer, LED lighting, and a camera also rivals that of single action cameras commonly used underwater such as the GoPro camera (Struthers et al., 2015), which has limited battery life and customization. In this report, we specify details of the underwater housing, Raspberry Pi controller, dissolved oxygen sensor, camera specifications and lighting. The results from several test deployments are discussed in Section 3, followed by an exploration of future improvements and possibilities."}, {"section_title": "Materials and Methods", "text": "Three different patterns were used for analysis with the ASV in Potter Pond (41.395\u00b0, -71.538\u00b0), a salt pond in Rhode Island (Figure 1). The first was a grid based pattern in upper Potter Pond typically used for bathymetric surveys. The second was a transect between the upper and lower pond, passing by the access point to the adjacent pond. The third and final pattern was a repeated loop around an oyster farm in the lower pond. Additionally, pressure loggers and tilt meters were placed around the oyster farm to determine water depth and current speed at different locations. 2.1 Grids The first analysis pattern used was a grid in the upper pond ( Figure 2). The grid was composed of transects crossing the pond several meters apart. The entire process took approximately 2.5 hours, and was ended shortly before the end of the planned mission due to batteries dying. \n\n\nDesigns were made with goal that the LoBSTAS must be inexpensive and easily built by a person with limited tools or technical experience. Using off-the-shelf components and open source software in construction enabled a modest total cost of approximately $380 USD. For the latest prototype, the total can be broken down into the housing ($12) and core components ($90) including Raspberry Pi controller, LED ring, 5200 mAh power bank, and camera which sums up to a base cost of approximately $110. We chose to include a dissolved oxygen sensor, which adds $270 to the base cost. Other sensors could possibly be added, and this is discussed more in Section 2.2."}, {"section_title": "Transects", "text": "The second analysis pattern involved transects between the upper and lower pond (Figure 1), each time passing by the only connector between the salt pond and Pt Judith Pond. Transects were performed when traveling to or from the lower pond in order to conduct research around the oyster farm. "}, {"section_title": "Loops", "text": "The final analysis pattern was a repeated loop around an oyster farm in the lower pond ( Figure  3). Each loop took approximately 10-15 minutes and was repeated for 2-3 hours. Loops were started at low tide and ran until around high tide in the pond. Two days of data were collected, but compass and GPS malfunctions did affect the performance of the vehicle, resulting in poor line following control.\nThe temperature in the loop around the farm started with very little variance, with a difference of around 1\u2103 around the entire farm ( Figure 10). As high tide was approaching, the western edge warmed up considerably, then the increase in temperature extended up the north and south sides of the loop. At the same time, the northernmost edge cooled rapidly, resulting in a 3\u2103 difference around the farm. At the beginning of the loop measurements, salinity varied only be 1 psu around the farm (Figure 11). As the tide flowed in, the salinity in the eastern corner decreased, while the northern edge increased, in the same section that experienced a dip in temperature. DO experienced very large amounts of spatial and temporal change during the tidal cycle ( Figure  12). At the beginning, there was only 210-310 micromoles/L around the entire farm. As the tide"}, {"section_title": "Results", "text": "\n"}, {"section_title": "Grid", "text": "At the time of the survey, there was a 2.5\u2103 difference throughout the pond, with a warmer section around the center of the pond and to the north (Figure 4). There was a much larger variance in salinity, with a change of approximately 6 psu. The highest salinity was near the center, in the same area that had the higher temperature, with a much lower salinity near the southwest corner of the pond ( Figure 5). This low salinity can be attributed to input from Fresh Pond. Dissolved oxygen varied by as much as 120 micromoles/L and was also highest in the central patch, where the water was warmer and more saline ( Figure 6).  "}, {"section_title": "Transect", "text": "The transects had large amounts of temporal and spatial variation. Temperature ranged from 20.5 to 26.5\u2103, with the coldest point around (0,0) in each transect ( Figure 7). Salinity ranged from 27 to 31.5 psu. The points of highest salinity were at the same locations as the lower temperatures ( Figure 8), suggesting this water recently came from the inlet to Pt Judith Pond. Dissolved oxygen ranged from 190 to 310 micromoles/L. It was consistently high at the same locations as the temperature minima and salinity maxima. There were also areas with high DO that had no apparent relation to the temperature or salinity (Figure 9). There was a noticeable variation among transects, with some having much less variance, higher minima, and lower maxima ( Figure 7; Figure 8; Figure 9)."}, {"section_title": "Discussion", "text": "The observed large decrease in salinity at the southwest corner of upper pond is due to inputs from Fresh Pond, a freshwater source with an access point in that part of the pond. Each transect has a section near (0,0) that is colder and more saline than the rest. This point is where water from Point Judith pond, a larger salt pond that connects to the ocean, and the only access point for Potter Pond. The water that flows in from Point Judith creates collections of water that are higher salinity and lower temperature. There were two very distinct temperature and salinity patterns. Transects 1, 2, and 4 are much more spatially variable, with much colder, saltier water at the access point than anywhere else on the pond. Transects 3 and 5 are less variable and although they do have the colder ocean water near the access point, temperature and salinity are much more consistent. This is likely related to high and low tide. At high tide, the water from Point Judith flows into the pond, creating a cold salinity front. At low tide, it all flows out, causing the warmer, fresher water from afar in the pond to cover that area. The DO plots suggest that there are areas with much higher productivity than others, leading to highly oxygenated water fronts traveling around the farm. There is no apparent cause for this to occur. The loops show clear spatial variation around the farm throughout a tidal cycle. As the tide rises, temperatures in the southwest corner rise, while the northern edge cools. Water in the southwest corner flows northeast, causing the warmer water deeper in the pond to travel towards the center of the farm. The cooling on the northern sides are likely caused by the cold saline front that forms as the tide rises seen in the transects. The water around the farm is very saline and doesn't change drastically. There is a salinity increase in the north, which is likely related to the cold saline front from Point Judith pond. The dissolved oxygen around the farm is very low at low tide. As the tide rises, the northeastern edge rapidly increases, likely due to incoming water from Pt Judith Pond. This water is also more productive than anywhere else in the pond, with much higher numbers than upper pond or the section that connects the two ponds.\nThe amount of misfit decreased by 10.76% for the M7.3 earthquake in the Ascending image. In the descending image for that earthquake, the misfit decreased even more, by 34.14%. The misfit did decrease by adding a layered geology by a significant amount for each image. In the descending residual data, it shows a representation of no visual data with an even more significant decrease in misfit, showing a well modeled fault with the predicted data matching the observation. The slip distribution plot showed an improvement for the fault model after adding a layered crust structure. There are less residual data which are likely separate areas of deformation, unrelated to the earthquake slip. This is often referred to as noise, which is mitigated in the plot through the addition of data in the center, making a wider slip range and fewer data on the edges. California did not show as much decrease in misfit, as it went down by only 2.04% after applying a layered geology. California is small scale in which the maximum amount of slip is just over 0.01 m and the residual plot shows a variance around 0.005 m of subsidence. The predicted data are not a perfect fit for the observation data, resulting in visual residual data. The slip distribution plot for California shows much more data after running a layered geology. More of the focused slip is along the surface which is correlated to the type of motion for this fault. The data being of higher magnitude and regionally centered indicated a more accurate slip distribution plot for the fault."}, {"section_title": "References", "text": "Brownlee, E., Sellner, S., & Sellner, K. (2005). Prorocentrum minimum blooms: Potential impacts on dissolved oxygen and Chesapeake Bay oyster settlement and growth. Harmful Algae,4(3), 593-602. doi:10.1016/j.hal.2004.08.009 Casas, S., Filgueira, R., Lavaud, R., Comeau, L., Peyre, M. L., & Peyre, J. L. (2018. Combined effects of temperature and salinity on the physiology of two geographically-distant eastern oyster populations. Journal of Experimental Marine Biology and Ecology,506, 82-90. doi:10.1016/j.jembe.2018.06.001 Delisle, L., Petton, B., Burguin, J. F., Morga, B., Corporeau, C., & Pernet, F. (2018. Temperature modulate disease susceptibility of the Pacific oyster Crassostrea gigas and virulence of the Ostreid herpesvirus type 1. Fish & Shellfish Immunology,80, 71-79. doi:10.1016/j.fsi.2018.05.056 Fricke, A., Pey, A., Gianni, F., Lem\u00e9e, R., & Mangialajo, L. (2018. Multiple stressors and benthic harmful algal blooms (BHABs): Potential effects of temperature rise and nutrient enrichment. Marine Pollution Bulletin,131, 552-564. doi:10.1016/j.marpolbul.2018.04.012 Lavaud, R., Peyre, M. K., Casas, S. M., Bacher, C., & Peyre, J. F. (2017. Integrating the effects of salinity on the physiology of the eastern oyster, Crassostrea virginica, in the northern Gulf of Mexico through a Dynamic Energy Budget model. Ecological Modelling,363, 221-233. doi:10.1016/j.ecolmodel.2017.09.003 Leonhardt, J. M., Casas, S., Supan, J. E., & Peyre, J. F. (2017. Stock assessment for eastern oyster seed production and field grow-out in Louisiana. Aquaculture,466, 9-19. doi:10.1016Aquaculture,466, 9-19. doi:10. /j.aquaculture.2016.034"}, {"section_title": "Methods", "text": ""}, {"section_title": "Comparison of inundation depth", "text": "Since the Servicewide study did not provide projections for all categories of hurricanes in each park, we were limited to only comparing sea level rise and storm surge projections for a category 2 hurricane. The time horizon of 2050 was selected as it was performed by both studies and was where the differences between the available sea level rise projections were the smallest. The sea level rise projections used by the Servicewide Study ranged from 25 cm for the reduced carbon emission scenario to 27 cm corresponding to the business as usual scenario of carbon emission for GATE; 26 cm and 27 cm respectively for FIIS and 27 cm and 28 cm respectively for ASIS (Caffrey et al., 2018). The low and high carbon emission scenarios for sea level rise projections used by the Sandy Study were respectively 23 cm to 60 cm for both GATE and FIIS and 21 cm and 58 cm for ASIS (Bradley et al., 2016). In order to create the closest comparison possible, the lowest projection of sea level rise was taken for the Sandy Study and compared to the highest sea level rise scenario from the Servicewide study (this was the only projection available in the data). Despite matching both studies' sea level rise projections as closely as possible, there was still a slight difference; 4 cm in GATE and FIIS, and 7 cm in ASIS. All projections were done at mean high water for each park by both studies (Caffrey et al., 2018;Bradley et al., 2018). These conditions were selected to eliminate as many differences as possible to obtain the unbiased comparison. The two comparisons that were decided upon were the projections of inundation extent and the inundation depth between both studies. Both comparisons consisted of two parts: 1) calculating the amount of agreement and differences between the sea level rise and storm surge projections between the two studies and 2) analyzing in which geomorphic land type the differences were found in. In each comparison, it was calculated that the percentage of the differences between the two studies could have been caused by the differences of DEMs and the sea level rise differences in the sea level rise projections. All comparative analyses were performed in the program Arc.GIS. The first comparison looked at the different projections of inundation extent or amount of flooding in each park. The first step taken was to divide each park based on different geographical units. The divisions created for FIIS are shown in figure 1. The units of each park and their respective areas are listed in Tables 1, 2, and 3.   8,348,337.99 4,990,780.65 6,566,814.75 10,389,253.80 7,960,286.23 Jamaica Bay unit was divided into three subunits as each part was detached from the others geographically. We expected that this difference in location and land type could affect how each storm surge was projected in each area. The Fire island unit consisted of the barrier island of the FIIS park limits. The bay islands were grouped separately in the Bay islands division. The mainland unit represent the Floyd estate and the headquarters and ferry terminals in Patchogue.  34,516,338.77 28,942,189.63 2,926,452.20 5,975,113.41 Assateague National Seashore sits on the state boundary of Virginia and Maryland. The projections were divided into south and north components by the Servicewide Study (Caffrey et al., 2018). We kept this division, and further divided each side into Barrier Island and Bay Islands. The next step in calculating the differences in the extent of inundation was to perform an intersect of both inundation files for each park unit. An intersect is an ArcGis tool that calculates the amount of agreement between two given layers; this is the amount of land that both studies projected would be inundated. The 2050 category 2 hurricane storm surge projections of the Servicewide Study were chosen to be one layer and the other layer to be the respective match in the Sandy Study. After calculating the amount of agreement for each geographical division, an intersect was calculated for each park as a whole to create an average value. What was not included in the intersect of the two inundation extent projections were considered the differences between the projections of the two studies. These differences occurred when one study projected more inundation than the other. The differences between the two studies were calculated by performing a symmetrical difference of the Sandy Study and the Servicewide Study. A symmetrical difference is a tool in ArcGIS that subtracts one layer from another. Whatever one study projected differently from the other was shown in the product of this test. These differences were then separated based on which study had caused them. This was done by performing another symmetrical difference between the intersect of the two projections and the individual projections of each study; subtracting what was in common between both studies by each study\u00b4s individual projections. The NPS provides geomorphic resource inventories for GATE, FIIS, and ASIS. For GATE and FIIS, the Geomorphological-GIS Map of the Gateway National Recreation Area was adopted (Psuty et al., 2015;NPS 2016). For ASIS, this data was provided in the Digital Geomorphic Map of Assateague Island National Seashore, Maryland and Virginia from the NPS Geologic Resources Program (Psuty et al., 2018;NPS, 2009). Each park was divided into different geomorphic categories using these inventories: beach, dunes, wetlands, developed land, and elevated surfaces. Table 4 lists these categories for each geomorphic feature. The percentage of the inundation extent differences found in each geomorphic land types was then calculated. The DEMs used by the Sandy Study were subtracted by the DEMs used by the Servicewide Study. The means of these differences as well as the standard deviation were evaluated to understand the difference between the DEMs."}, {"section_title": "Comparison of Inundation Depth", "text": "The second comparison looked at the different values for the depth of inundation projected for each park. Depth projections were formatted as raster files whereas inundation extent projections were two-dimensional polygon shapes. Raster files represent three dimensional values through two-dimensional pixel with different gradients of color to measure its magnitude. This difference in format required different techniques to measure the inundation depth projection differences from those used in the first comparison. The differences between the depth projections were measured by comparing them against two thresholds. The first threshold was the 4 to 7 cm differences taken from the difference between the different sea level rise projections used. The other threshold was 30.5 cm (1 foot) to measure the magnitude of the differences between the two depth projections in each park. To do this, the Sandy Study\u00b4s inundation depth projections were subtracted by the inundation depth projections of the Servicewide Study. The ArcGIS tool used to subtract these two raster layers was the raster calculator which is a tool that allows simple mathematical operations to be performed on raster files. This raster calculator operation produced a new three-dimensional layer where depth differences caused by the Sandy Study projected deeper values were represented as positive and similarly the differences caused by the Servicewide Study were represented as negative. An absolute value greater than or equal to the 4 to 7 cm differences were taken to see how much of the differences between the depths of inundation could have been caused by the difference in the sea level rise projections. Likewise, this was done for the second threshold of 30.5 cm. The value obtained from each of the executed absolute value operations was used to calculate the percentage of difference between the inundation depths of the two studies. This was done for both thresholds in each geographical division of all three parks. A park average for both thresholds was also calculated. Just as was done in the first comparison, we mapped out where these differences fell in each geomorphic land type. The percentages of the depth differences were then calculated for each geomorphic land type. Inundation depth differences were double checked with the differences between the DEMs used by each study. The means and the standard deviations of the inundation depth differences were compared with those of the difference between the DEMs. DEM difference was calculated with the raster calculator the same way it was done for the inundation depth differences.\n"}, {"section_title": "Comparison of Inundation Extent", "text": "Storm surge at higher sea levels would hypothetically travel further in land than storm surge modeled at current day sea level. The inundation extent projections for FIIS are shown in fig 3 for the Sandy Study. In addition, it was theorized that areas exposed to a higher amount of storm surge would experience greater differences in inundation extent projections between both studies. This is seen in FIIS (table 5) and ASIS (table 6) where the largest variance between the two studies' extent of inundation projections occurred in the ocean side divisions. However, there is not as strong of a correlation in GATE (table 8). Furthermore, Staten Island, Sandy Hook, and Breezy Point have the largest contrast between the extents of inundation projections compared to the other divisions.  3.2 Agreement and differences between inundation extent projections. In the comparison of inundation extent, the two studies agreed with each other over 80% in each park (figure 4 for FIIS). FIIS had the lowest park average for inundation extent agreement, 83.36% whereas ASIS saw an agreement of 96% for inundation extent between studies (Table  8). We theorize this difference could have been caused in part by ASIS having been less hit directly by hurricane Sandy whereas GATE and FIIS received more of the storm. The topography in GATE and FIIS would have changed more due to this event compared to the changes in the topography of ASIS. The use of different DEMs could be the source of this difference between parks and led to lower levels of agreement for FIIS and GATE. Fig. 4. Agreement (pink) and difference (yellow) of inundation extent between both studies at a breach at Fire Island National Seashore. "}, {"section_title": "ASIS 96%", "text": "Focusing on the individual geographical divisions in each park, it was found that the oceanside units generally showed less inundation extent agreement between both studies when compared to the bayside units and mainland units ( fig 5). This correlation is easily seen in FIIS and ASIS and less present in GATE. FIIS and ASIS, both parks situated on barrier islands, were into oceanside, bayside, and mainland. However, GATE consists of different geographical features ( fig 5). The distinct geographical features of GATE complicate the comparison between the other two barrier island parks. The agreement of inundation extent for each geographical division in GATE, FIIS, and ASIS are shown in Tables 9, 10, and 11, respectively. These differences were split up and attributed to the study that had projected them differently from the other study. The split differences represented how much each study deviated from the amount of agreement in inundation extent between the two studies. It was found that the studies that projected more inundation produced the majority of differences between the two studies. Figure 6 represents these differences in one of the breaches caused by Hurricane Sandy in FIIS. Shaded in pink is the inundation extent agreement between both studies, in yellow the differences generated by the Sandy Study projecting more inundation, and in red the inundation extent differences of the Servicewide Study. Percentages values for these split differences for GATE, FIIS, and ASIS are shown in tables 12, 13, and 14 respectively. The two places that represented exceptions to this finding were the bay island and mainland divisions in FIIS.  3.3 Inundation extent differences by geomorphic land type The percentages of inundation extent differences in each geomorphic land type were calculated. There was no correlation between differences of inundation extent and geomorphic land type that could be made for all three parks. In ASIS the majority of the differences happened in the Dune and Elevated surfaces and ridges land type. The geomorphic land type data available for FIIS only mapped the barrier island part of the Park. For this reason, percentages were only evaluated for the oceanside division of the park, which showed that most of the inundation extent differences occurred in the beach land type. Sandy Hook in GATE showed the most of inundation extent differences in the dunes land type, whereas for Staten Island and Jamaica Bay, most of the differences were found in developed and elevated surface land types. The distinct features of each park make drawing a correlation between geomorphic land type and inundation extent differences across all parks impossible. Fig 7 shows the differences of inundation extent in each geomorphic land type. The only observed trend was that the wetland areas for all three parks had minimal to no differences of inundation extent because the wetlands in both were almost entirely inundated. The percentages of inundation extent in each geomorphic land type for GATE, FIIS, and ASIS are shown in tables 15, 16, and 17 respectively.  3.4 Differences between the Digital Elevation Models used Digital elevation models used in each study were subtracted from one another (fig 8). The average calculated differences show that for GATE and FIIS, these differences were less than 2 cm (table 18). The park average for ASIS is skewed by a high elevation differences that was observed in the South Bay Islands unit (table 26). This small difference in inundation extent suggests inundation extent differences were mostly not in part due to the DEM differences. The mean DEM differences for each park unit are discussed in more details in the second comparison of inundation depth. .5 Limitations and future work Future analysis would be to duplicate this comparison for the 2050 category 1 and 2050 category 3 hurricane in each park to explore if these findings are similar for different hurricane strengths. These would also be done with the 2100 sea level rise scenarios for all category hurricanes to see how a larger amount of sea level rise affects differences in the two modeling techniques. To expand on the analysis of the geomorphic land type, the percentage of split inundation extent differences as shown in tables 8-10 should be calculated for each geomorphic land type. A more extensive analysis upon the magnitude of the differences between the DEM models used would help better understand the possible effect on the inundation extent differences. To directly compare the depth differences of the DEMs would require ArcGIS tools for raster files that were beyond the scope of this study."}, {"section_title": "Differences between inundation depth projections", "text": "It was found that there were more differences between inundation depth projections between the two studies than agreements. Figure 9 shows the magnitude of the absolute value differences of inundation depths between the two studies in FIIS at the threshold of 4 cm. At the sea level rise difference threshold (4 cm for FIIS and GATE, and 7 cm for ASIS), 95% of the depth differences in each park were greater than the sea level rise difference. This means that 95% of the depth differences between the two studies in each park could not solely have been caused by the difference in sea level rise projections used. The second threshold of 30.5 cm revealed that over 65% of the depth differences in each park were greater than 30.5cm between each study. FIIS experiences the most inundation depth differences as shown by both thresholds measurements. Averaged inundation depth differences for each park are listed in table 19 for the sea level rise projection threshold and in table 20 for the 30.5 cm threshold.  Inundation depth differences slightly varied from the park average in each geographical unit. It was observed that most of the bayside units experienced greater differences in inundation depths than the oceanside units. The Staten Island unit in GATE, the bayside unit in FIIS, and the northern bay islands unit in ASIS showed the highest inundation depth differences in all three parks. The exception to this trend were the bay islands unit in Jamaica Bay which showed the lowest inundation depth differences for all three parks at the 30.5 cm threshold ( fig.10). Inundation depth differences for geographical units for GATE, FIIS, and ASIS are show in tables 21, 22, and 23.  Means, standard deviations and maximum values were calculated for the depth differences in each park unit to measure the distribution of the depth differences ( fig. 11). It was observed that in FIIS and ASIS, the projections of inundation depth were slightly deeper for the Sandy Study projections (tables 20 and 21); this is shown by the positive mean values of the difference of inundation depth. For GATE, the Servicewide projected inundation depths were slightly deeper than in the Sandy Study.  4.2 Inundation extent differences in each geomorphic land type The percentages of inundation depth differences in each geomorphic land type were calculated. Inundation depth differences for each geomorphic land type for FIIS is depicted in Figure 12. The percentages of inundation depth differences in each geomorphic land type for GATE, FIIS, and ASIS are shown in tables 27, 28, and 29, respectively  4.3 Difference between digital elevation models The means of the differences of the DEMs were calculated and then compared with the means and standard deviations calculated from the inundation depth differences. In GATE and FIIS the means of the DEM differences were significantly smaller than those of the inundation depth differences. This suggest that the grand majority of the inundation depth differences were not caused by the DEM differences. The high means of the DEM differences for the south part of ASIS could be due to the dynamic coastal process of Tom\u00b4s cove (the southernmost part of ASIS). Mean values for DEM differences and inundation depth differences shown for GATE, FIIS, and ASIS in tables 30, 31, and 32 respectively. 4.4 Shortcomings and future work Additional extensions for the comparison of the inundation depth projections would be to separate the absolute value differences for each threshold. Absolute values can be split into two positive and negative boundaries. The positive boundaries would only measure the inundation depth differences that occurred when the Sandy Study projected deeper inundation values than the Servicewide Study. The negative boundary would represent those values for the Servicewide study. These split inundation depth differences would be mapped in each geomorphic unit to see if they map out equally or differently."}, {"section_title": "Conclusions", "text": "The method of combining sea level rise with storm surge produces different projections from those of adding sea level rise to current day storm surge. When modeling inundation extent, this difference is small, and using the sea level rise plus current day storm surge method is sufficient for accurate modeling. However, when modeling inundation depth, it is advisable to model storm surge at future sea levels as was done by the Sandy Study. Per-and polyfluoroalkyl substances (PFASs) are bioaccumulative, potentially toxic chemicals widely produced and used in industrial and commercial products. Due to their chemical stability and water-and oil-repelling features, PFASs have been incorporated in various products globally for decades. PFASs are detected in all environmental compartments and humans, and exposure to PFASs has been correlated with immune dysfunction, developmental disorder, elevated cholesterol, endocrine disruption, and can be linked to cancers under high exposure. This study aimed to assess how a small scale manufacturer was affecting the PFAS concentrations in the environment (particularly in water) by comparing concentration up-and downstream of a local production plant in Narragansett, Rhode Island. A total of nine samples were collected directly from a stream that flows by the production plant (freshwater) and into Narragansett Bay (seawater). Samples were filtered, extracted using a solid-phase extraction technique and cleaned up using activated carbon. Thirty PFASs were separated and detected using liquid chromatography coupled to mass spectrometry (LC-MS). The highest PFAS concentrations detected across the sites were perfluoro-n-butanoic acid (PFBA, 102 ng L -1 ) and perfluoro-noctanoic acid (PFOA, 50 ng L -1 ). We found elevated PFASs concentrations across sampling sites, and the elevation was attributed to the local production plant and other sources in the Narragansett area. However, more information on PFASs in air, soil, sediment, and drinking water sources in the area is needed to determine how significant the site of interest is to the area.\nWe sampled 9 water samples in a vicinity of a local fluoropolymer company at Narragansett Bay and we analyzed 30 poly-and perfluoroalkyl substances in these samples. The total concentrations of target PFSAs at the site of interest was 237 ng L -1 which is 30 times higher by comparison with background site in the Narragansett area. Moreover, the sites downstream the company have elevated concentrations of analyzed PFAS (\u2211PFASs was five times higher downstream than upstream). However, the concentrations of some of PFASs increased furthermore downstream which indicates other PFAS sources in the area. To assess how significant the impact of the site of interest is to the Narragansett area, the analyses of PFASs in air, soil, sediment as well as drinking water sources in the area should be utilized in the future.\nWe chose the M7.3 earthquake near Halabjah, Iraq in November 12, 2017, the M6.1 near Mashhad, Iran on April 5, 2017, and a slow earthquake in August 2017 in California for our study. We used local seismic velocity models to constrain the crustal layers. For the M7.3 Iraq earthquake, the misfit between observations and model predictions decreased by10.8% for the descending image and 34.1% for the ascending image using a layered crust structure. For the slow earthquake in California, only descending images were used and the misfit decreased by 2.0%. The Geology in Iran is more stratified to a deeper depth, likely causing larger decrease in misfit after adding a layered geology when compared to California. Running SDM using a layered geologic structure can improve the fault model when the layering geology is significant. Wei, M., Sandwell, D., & Fialko, Y. (2009). A silent M w 4.7 slip event of October 2006 on the Superstition Hills fault, southern California. Journal of Geophysical Research, 114(B7), 1-15. http://doi.org/10.1029/2008JB006135 1. Introduction\nWe have described the initial findings from a custom gas equilibration mass spectrometer (GEMS), a deployable spectrometry method with the intent to continuously measure the molar concentration of dissolved 20 Ne, 21 Ne, and 22 Ne in water. While operating at an ionization voltage of -80 eV and an emission current of 0.5 A, the chemical getter decreased the ion current of N2 by 8.78 \u00d7 10 11 Amps, O2 by 8.29 \u00d7 10 11 Amps, and 40 Ar by 8.61 \u00d7 10 11 A. Further work needs to be conducted to comprehend how the getter affects the composition and quantity of air that passes into the GEMS system.\n"}, {"section_title": "Chemicals and Standards.", "text": "Methanol, ammonium hydroxide, LC/MS water were obtained from Fisher Chemicals, while LC/MS methanol was obtained from Honeywell Research Chemicals. Ammonium acetate was purchased from Amresco. Standard solution, a mixture of 30 perfluoroalkyl compounds including perfluoroalklycarboxylic acid (PFCAs), perfluoroalkylsulfonates (PFASs), perfluorooctane-sulfonamides (FOSAs), perfluorooctanesulfonamidoethanols (FOSEs), perfluorooctane-sulfonamidoacetic acids (FOSAAs), fluorinated telomer alcohols (FTOHs), polyfluorotelomer unsaturated acid (FTUA), fluorinated telomer sulfonates (FTSs), disubstituted polyfluorinated phosphate esters (di-PAP) and PFCAs alternatives (ADONA, GenX and F-53B), at concentration of 1 \u00b5g mL -1 , were obtained from Wellington Laboratories, Canada."}, {"section_title": "Sample Collection and Site Description.", "text": "Water samples were collected from a stream in Narragansett and the Narragansett Bay in Rhode Island at approximate 0.1 meter below the surface in May 2018 ( Figure 1). A volume of 0.5 L water was collected and stored in 0.5 L wide-mouth high-density polyethylene bottles precleaned with 3% ammonia hydroxide in methanol, LC/MS methanol, and LC/MS water. The GPS coordinates, pH, air temperature, etc. were recorded in the sampling sheet for each sampling (Table SI 1)."}, {"section_title": "Figure 1 Map of water samples; Stream sites (S1 to S6) and Bay sites (B1 to B3)", "text": "When collecting samples from the stream, each bottle was rinsed with stream water once before collection, and the rinse water was dumped towards downstream. When collecting from the bay, the bottles were rinsed with ocean water, and samples were collected shortly after the rinse water was released. Samples were stored at 4 \u00b0C on the day it was sampled and were analyzed within two weeks of collection."}, {"section_title": "Sample Extraction.", "text": "Prior to extraction, the water samples were filtered on glass fiber filters (Whatman 0.7 \u00b5m GF/C). The solid part (particles) and the liquid part (water) of each sample were processed separately (result of particle sample extraction is not included in this report). Solid phase extraction (SPE) technique was used to extract PFASs from water samples. Prior to the analyses, all samples were spiked with surrogate standards (SI Table 5) which were used for the calculation of recoveries. Waters OASIS WAX SPE cartridges were pre-cleaned with 3% ammonia hydroxide in methanol and conditioned with 4 mL of LC/MS methanol and 4 mL of LC/MS water. Water samples were passed through SPE cartridges at a rate of 1 drop per second with a limited vacuum. Cartridges were dried by placing them in pre-cleaned falcon tubes, and centrifuged (International Clinical Centrifuge Model CL) at 4000 RMP for ten minutes. Water samples were eluted with 4 mL of 1% ammonia hydroxide in methanol to the pre-cleaned (3% ammonia hydroxide in methanol) and pre-weighted (Mettler Toledo AG245 balance) falcon tubes. Particle samples were transferred into pre-cleaned falcon tubes, spiked with surrogate standards, and soaked in 1% ammonia hydroxide in methanol. The tubes with soaked samples were placed in a sonic bath (Cole-Parmer 8851) for two hours. After that, methanol was transferred to cleaned falcon tube and the extraction was repeated. Both extracts were combined. 2.4 Sample Preparation. Falcon tubes containing water and particle extracts were placed in a water bath (Dubnoff Metabolic Shaking Incubator) at 30 \u00b0C, and evaporated to approximately 0.5 mL under Nitrogen. Extracts were run through Super Clean ENVI-CARB cartridges to collect polar compound. Autosampler inserts were pre-cleaned with 3% ammonia hydroxide in methanol and LC/MS methanol and then placed into autosampler vials. Using the Microman M100 pipet, 60 \u00b5L of 4 mM ammonium acetate in LC-MS water and 60 \u00b5L of the sample were pipetted into each insert. All extracts were mixed (Scientific Industries Mixer) and stored at 4\u00b0C until analysis."}, {"section_title": "Instrumental Analysis.", "text": "Thirty PFASs, including 13 perfluorocarboxylic acids (PFCAs), 5 perfluoro sulfonates (PFSAs), 3 polyfluorotelomer sulfonates (FTSs), 3 perfluorooctane sulfonamide (FOSA/Es), 2 alkylperfluorooctane sulfonamido acetic acids (FOSAAs), 1 polyfluorotelomer unsaturated acid (FTUA), 1 di-substituted polyfluorinated phosphate ester (di-PAP) and 3 new alternatives -ADONA, GenX and F-53B were separated and detected by Liquid Chromatography coupled to Mass Spectrometry (LC-MS/MS) (Table SI 2). Briefly, the mass spectrometer (AB Sciex 4500 QTRAP) operated in negative ionization mode (ESI-) under optimized conditions (Table SI 3). The liquid chromatograph (Shimadzu Prominence UFLC) was equipped with C18 BEH\u00ae UPLC column (130\u00c5, 1.7 \u00b5m, 2.1 mm ID X 50 mm; WATERS) and both, water and methanol used as mobile phases, contained 2 mM ammonium acetate. A gradient elution with an initial content of 40% methanol was applied and is described in details in Table SI 4. The flow rate of mobile phase was 0.2 mL min -1 . The injection volume of both, calibration standards and samples, was 20 \u03bcL. The identification and quantification of target compounds were based on the calibration curve ranging between 0.004 to 40 ng mL -1 . Target compounds in calibrations, samples, and blanks were quantified using isotope dilution quantification or internal standard quantification if a labelled analog was not commercially available (Table SI 5)."}, {"section_title": "Results and Discussion", "text": "\nThe plot of N2 indicates that applying the 200 \u00b0C getter reduced the ion current from 8.9358 \u00d7 10 -11 to 1.5462 \u00d7 10 -12 Amps (Figure 8). Applying the room-temperature getter reduced the ion current from 8.9358 \u00d7 10 -11 Amps with no getter to 1.4414 \u00d7 10 -12 Amps. Together, these showed promising results as the N2 concentration entering the ion source of the mass spectrometer should be reduced after passing through the getter. However, the difference in ion current between the room-temperature getter and the 200 \u00b0C getter was only 1.0480 \u00d7 10 -13 Amps. Based upon the manufacturing handbook distributed by SAES Getters Group, the 200 \u00b0C getter should have proved more efficient at removing N2. Not only was the difference in ion current small, but the heated getter's graphic line was above the room temperature's. Additionally, there appears to be a general trend upwards in the room temperature's data line. This is likely due to gas escape through the metal walls of the system. Plotting 40 Ar indicates that applying the getter also greatly reduced its detectability despite 40 Ar being a trace gas supposed to be unaffected by getter absorption (Figure 9). Without a getter, the ion current read 8.6194 \u00d7 10 -11 Amps but dropped to 5.5688 \u00d7 10 -14 Amps when the 200 \u00b0C getter was added. Similarly, the ion current decreased to 1.2760 \u00d7 10 -13 Amps after the roomtemperature getter was installed. In fact, the decrease in ion current for 40 Ar appeared to be roughly the same as experienced by N2. Due to the extensive decrease in ion currents across noble gases, neither plot of neon could be created. Seeing that the ion current of 20 Ne + and 20 Ne 2+ was already low with no getter attached, application of the getter reduced their ion currents to the edge of detectability by the mass spectrometer. Therefore, 40 Ar was plotted in place of neon to represent the trend that a trace gas exhibits when the getter is installed. In this case, the efficiency gap between the heated and cold getter was slightly more substantial at 7.1915 \u00d7 10 -14 Amps, but still below the desired separation.  Upon closer investigation of the ion current changes in N2 and 40 Ar, it appears as though both currents decreased by roughly the same amount. To formulate an accurate comparison, an average ion current value for each species was calculated ( Table 3). Note that the general decrease in ion current values when the getter was installed decreased 20 Ne + and 20 Ne 2+ below a detectable range resulting in negative ion current values for both getter scenarios. Using the average values and the ion current differences between getter applications, a percent change table was generated to directly compare how the species were affected in relation to each other (Table 4). In both the hot and cool getter cases, each species experienced nearly a 100% decrease in ion current. This implies that the getter material did not distinguish between gas species and reduced all concentrations of gas by about the same amount. One possible explanation for this behavior could be that the getter material is inhibiting the flow of air into the mass spectrometer, creating a pressure difference. Instead of changing the composition of air that enters the ion source, the getter may have created a pressure difference that reduced the amount of air that passes, thus reducing the ion currents. More investigation is needed to understand this behavior. Moving forward, we would like to explore the contrast in efficiency between the heated and room temperature getter. It is possible that our getter does not contain enough material to discern a visible increase in efficiency when heated. A similar study conducted by Visser et al. (2013) used 830 cm 2 of getter material heated to 400 \u00b0C compared to our 90.13 cm 2 at 200 \u00b0C. Future experiments will involve an extended getter housing to store more sintered pellets and be heated beyond 200 \u00b0C. Another possible extension to boost the signal strength of 20 Ne + and 20 Ne 2+ is to increase the cathode voltage beyond -100 eV. In an experiment to track the ion current of 20 Ne 2+ at varying ionization energies, Chung and Cho (2001) found that 20 Ne 2+ reached its maximum ion current at about -250 eV. In our setup, the maximum voltage that can be achieved is -100 eV; however, some third-party software attachments can allow this limit to be increased to -120 eV.\nWe conducted twelve test deployments at the URI Graduate School of Oceanography (GSO) dock at depths down to 6m. Out of the twelve deployments, two deployments experienced minor leaks: one was because the expansion plug was not fully tightened with a wrench, and the other was due to leaks in the sensor probe port on the square head expansion plug. Thankfully, the main board was not damaged as it was elevated on a cardboard platform and wrapped in anti-static pouches. However, in one of the two leaks the power bank at the bottom was the casualty. Our second core prototype with a 5200 mAh power bank had a total deployment time of seventy hours with a sampling interval of one hour. For this duration, the LoBSTAS did not experience biofouling on the window that degraded image quality. We did not test longer sampling intervals to explore maximum battery life due to time constraints. We confirmed that a combination of a 30-second video and two images is ideal to capture movements for several wave cycles. However, 30 seconds was insufficient time for the DO sensor to stabilize and take accurate readings. Due to lack of time and the leak in the sensor probe's porthole, we were unable to take more DO readings in water. In all our test deployments, the LoBSTAS was attached to rocks or a lobster cage during the test deployments. These are then retrieved with a rope tied to the dock. While the rock drops were most compact and convenient, the lobster cage provided a sense of scale in captured imagery with its 3D grid. We also determined that the best position to attach the LoBSTAS to the cage was at the side near one of the entry points, as this provides a view of the sediment, vertical water column, and organism activity inside the cage. Other trapattached possibilities include a position on the net opening inside the cage, and a top down view from the outside. From a single image captured we were able to characterize sediment type, vegetation cover, amount of suspended particles, benthic organism presence, and water turbidity. Additionally, the videos played a key role in providing information on water dynamics. Local water velocity can be gauged from particle motion as well as vegetation and sediment movement. Changes of these characteristics in time can be seen with time lapse imagery where images a few hours, days, or weeks apart are compared (Fig. 6). Spatial variation of benthic characteristics can also be compared with imagery from slightly different locations (Fig. 7). In dark environments, foreground particles' back-scattering hindered a good view of the benthic environment (Fig. 8a). All the foreground particles were unfocused as the standard camera lens' focus was set at infinity. When we adjusted the focus to be clear at 3 inches from the lens, the depth of view became smaller and the field of view became narrower. While fisheye lens captured several reflections of the LED ring, its larger depth of field enabled us to see well defined particles and nearby sediment. We were even able to monitor hermit crab activity through a video (Fig. 8b). We also realized that in dark, turbid waters, the object of interest should be less than 1 foot away for sufficient reflected to reach the camera.  To address the issues back-scattering and reflections from single-housing units, we tested the twin and paired configuration. In the pair configuration (Fig. 10a), one camera with lighting was placed on top of the cage looking down to capture activity inside. The second camera was oriented on the side of the cage near one of the two entry points, to monitor not only in-cage activity but also the sediment and water column. With these placements, the inside of the cage was visible, but the light source was not powerful enough to light the sediment outside the cage. Thus, for the current LED ring the best pair configuration was to move the side camera inside the cage (on the net) to focus on in-cage activity. In the twin configuration, an additional LoBSTAS was mounted on top of the base unit to provide light (Fig. 10b). The bottom camera captured the benthic environment while the top camera captured particle motion and provided light. The top unit was positioned with an inclination of 20-30 \u2022 ; this is the best angle as the camera is pointed at the seafloor about one foot away and would minimize back-scatter for the other camera. Its elevated height also provided a bigger view of the seafloor (Fig. 7b) which is good for studies focusing on sediment evolution. The twin configuration enabled the bottom camera to capture a good image of the sediment and water column in low light (9a) and a completely dark environment (9b). To synchronize image capture of both units, we set the same wake up time with the Witty Pi and set a longer video recording length on the top unit. Strobe flashes of the LED ring would be much harder to synchronize due to slight variations in boot time (40\u00b11 s, processing speed (if both units are not the same Raspberry Pi board), and RTC variability (1-3 s) in both units."}, {"section_title": "Detection frequency.", "text": "In nine samples of surface (6 samples) and sea water (3 samples), 25 of 30 analysed PFASs were detected, including 13 perfluorocarboxylic acids (PFCAs), 5 perfluoro sulfonates (PFSAs), 2 polyfuorotelomer sulfonates (FTSs), 2 perfluorooctane sulfonamide (FOSA/Es), 1 alkylperfluorooctane sulfonamido acetic acids (FOSAAs), 1 polyfluorotelomer unsaturated acid and 2 new alternatives -ADONA (NaDONA) and GenX (HPFO-DA) ( Figure 2; Table SI 6)."}, {"section_title": "Figure 2 Detection frequencies of all PFASs at studied sites", "text": "Among the PFCAs, C4-C10 PFCAs were detected consistently in the samples; all of the short chain PFCAs had a detection frequency of around or above 90%. On the contrary, the long chain PFCAs (with >10 perfluorinated carbons) were not as detectable in water samples, C11-C16 were detected in fewer than 30% of samples except for PFODA (C18), which was detected in all samples. All short chain PFSAs with even perfluorinated carbons in a chain (C4, C6, and C8) were detected in all samples in contrast to C7 which were detected only in 60% of samples and C10 which has only 30% detection frequency. From the precursors, FOSA and 6:2 FTS, which are the volatile precursors of PFOS, were detected in all samples. Three new alternatives were also analyzed in the set of samples, and two of them were detected at more than 50% of samples. For the next data analysis, only the substances with detection frequencies higher than 30% were considered."}, {"section_title": "Concentration overview.", "text": "Twenty of thirty PFASs were detected in at least 3 sampling sites. The detected concentrations of these PFASs are shown at Figure 3. The localities are divided into groups based on their location (upstream and downstream of the site of interest). Concentrations of \u221120 PFASs varied and ranged from 5 to 237 ng L -1 . The lowest concentration in surface water was detected upstream the site of interest (S1) where no significant sources of PFAS are located. The overall lowest concentrations of \u221120 PFASs were detected in Bay samples which might be affected by various sources (wastewater treatment plant, households,) however the contamination was diluted. The PFASs levels at the site of interest were significantly elevated compared to the upstream, background, site. Among the 20 PFASs included in the data analysis, all were detected at the site of interest, and 14 of them reached their highest concentration at this sampling site. The concentrations detected downstream were higher than those detected upstream. This occurrence indicates the likeliness that the production site was a dominant source of PFASs in this area."}, {"section_title": "PFAS Patterns in the Stream.", "text": "To compare the concentrations and pattern of various PFASs at all sites, Figure 4 (a&b) to Figure 6 (a&b) were constructed. The pattern of the PFAS concentrations in the stream varied across upstream (S1), the site of interest (S2), and downstream (S3-S6). Whereas the elevated downstream concentrations indicate the prevalence of direct PFAS sources from the site of interest, it is interesting that the downstream compositions of different groups of PFASs (PFCA, PFSA, and precursors) did not align with those of the site of interest. As can be seen in figures 4a and 5a, there was a 40-fold increase in \u03a3PFCAs, and 18-fold increase in \u03a3precursors at the site of interest compared to the upstream site. Also, the pattern (Figure 4b, 5b, and 6b) at the site of interest was different from the ppattern at the upstream site, which means that there are different sources of PFASs at these two sites. Furthermore, downstream (S3-S6), the concentrations of both, \u03a3PFCAs and \u03a3precursors were also increased relative to the upstream site. Moreover, the pattern in these samples was changing. In contrast to the dominant PFCA at the site of interest, PFBA at about 50%, PFOA became the predominant PFCA in the downstream samples, particularly at S4 at 38%. Likewise, the precursors N-MeFOSAA had a considerable elevation in both concentration and composition (Figure 6a&b). The change in the pattern indicated that there might be another PFAS source in the area. To our knowledge, there is no other direct source by the site of interest and upstream, yet there are households between S3 and S5. Hence, we infer that the source is attributed to the wastewater from the households. Figure 6a showed a 7-fold increase in \u03a3PFSAs at the site of interest in contrast to the upstream site. However, the concentrations at sites downstream (S3-S6) did not show the same trend as PFCAs and precursors at those sites. In addition, the pattern (Figure 6b) was different at the site of interest with the dominant PFOS (up to 90%) and the sites downstream (up to 50% of PFHxS at site Stream 3). The maximum concentrations found in this study were similar to or lower than the maximum concentrations in RI and NY Region (Table 2) and comparable with concentrations detected by the fluoropolymer production in the Netherlands. The highest detected concentration of PFAS in our study in the stream water, PFBA was measured at 102 ng L -1 . It was also detected in all samples, and the concentration range was wide, from 1 to 102 ng L -1 . PFOA was another dominant PFAS detected in all samples; as the second highest detected concentration, its level reached 50 ng L -1 . This is a result worth noting because the maximum level of PFOA, found in such a small scale study, was only 6 ng L -1 lower than the maximum PFAS concentration (also PFOA) measured in the RI and NY Region (Zhang, 2016)."}, {"section_title": "PFAS Alternatives at the Site of Interest.", "text": "Substantial levels of short-chain PFCAs (PFOA alternatives) were measured at the site of interest. In particular, PFBA reached 102 ng L -1 , two times higher than the maximum PFBA level detected at AFFF impacted site (38 ng L -1 , groundwater), and five times higher than the maximum level detected in wastewater treatment plant effluent (19 ng L -1 ) (Houtz, 2016;Dauchy, 2017). Besides, PFBA contributed 43% of the total PFAS concentration at the site of interest. It is noteworthy that the level of PFBA (102 ng L -1 ) was twice that of the PFOA (50 ng L -1 ). The short chain PFAS dominating the overall concentration reflects the phase-out of longchain PFASs since 2000 (Buck et al., 2011). Moreover, the new PFAS alternatives HFPO-DA (GenX) and ADONA were detected in this site at concentrations of 3 and 6 ng L -1 , respectively.  In this study, there was no significant increase in the PFAS concentrations measured at the bay sampling sites downstream from the stream discharging water affected by the local fluoropolymer company. At this Narragansett Bay site (B3), the \u22116 PFCAs, \u22115 PFSAs, and \u2211precursors were detected at about the same levels as at the site upstream from the local fluoropolymer company (B1). Interestingly, it appears that the levels of PFCAs and precursors detected at site B2, which is about half a kilometer south from B1, are elevated considerably. To our knowledge, there are no typical PFAS sources close by, so the sources of this increase in concentration are unclear to us. This phenomenon could be potentially studied in the future.  "}, {"section_title": "Methodology", "text": "GSO  We used Sentinel-1A InSAR data from the European Space Agency. We downloaded them through the Alaska SAR Facility website using the ASF data protal. We processed these data using the open source software GMTSAR (Sandwell et al. 2011). We used the 90 m resolution Shuttle Radar Topography Mission (SRTM) digital elevation map (DEM) to remove the topographic signal (USGS2004). We gathered the orbital data for each image on the POD Precise Orbit Ephemerides site for Sentinal-1 (https://qc.sentinel1.eo.esa.int/aux_poeorb/).We used a Gaussian filter with a wavelength of 200m to filter the phase image and software SNAPHU (Chen & Zebker 2002) to unwrap it, both were built-in in GMTSAR. We used SDM (Steepest Decent Method) software to invert fault slip. SDM is a FORTRAN code package for inverting co-seismic surface deformation data (GPS, InSAR, etc.) for fault slip distribution (Wang et al., 2009). It can handle both a uniform or layered structure and can use arbitrarily curved fault geometry, which would produce a more realistic model. SDM has a-priori constraint on the variation range of rake angle and uses fast optimization algorithm based on the steepest descent method. For the M7.3 earthquake, the fault trace is not known. We developed our own fault model based on the spatial characters of the deformation signal. The deformation pattern of the M7.3 earthquake is very distinct and consistent with a northwest strike fault dipping to the northeast. For the California slow earthquake, slip rupture near the surface and the fault trace is well known. We ran inversion for both uniform and layered structure. We down-sampled the data to reduce the calculation time. The layered structure for the Iran and Iraq earthquakes ( Figure 1) was based on local studies (Afsari et al., 2011;Rezaeifar et al., 2016). The structure for the California case ( Figure 1) was from the SCEC Community Velocity Model -Harvard (CVM-H; https://scec.usc.edu/scecpedia/CVM-H). We compared the misfit to observations to determine which method matched the observations better. Misfit was defined as = \u221a , where residual is the difference between observation and model prediction at selected sample locations and Npoints is the total number of observation samples used in inversion. "}, {"section_title": "Data", "text": "We selected three earthquakes for this study. For the Iran and Iraq earthquake, we used data from both ascending and descending directions ( Table 1). The repeating data were acquired soon after the earthquake to limit the effect of post-seismic slip. For the slow earthquake in California, only descending data were used. We stacked seven interferograms to increase the signal-to-noise ratio. "}, {"section_title": "4.1", "text": "The November 12, 2017 M7.3 earthquake in Iraq The M7.3 earthquake occurred on November 12 th 2017 near the Iraq and Iran border, 29 km from the town of Halabjah. The ascending data show a decrease of line-of-sight (LOS) with a maximum of 0.8 m. The descending data show a decrease of LOS up to 0.5 m on the northeast side and an increase of LOS up to 0.4 m on the southwest side. These observations suggest a northwest trending fault that dips northeast with a significant right-lateral strike-slip component. We used a fault model of average strike 351\u00ba, dip angle 16 \u00ba, and rake range of 118 to 138 degrees. The top of the fault is 5 km with a width of 60 km. We used a smoothness parameter of 0.05. The best fitting model for both uniform and layered model both fits the observations well, with a few cm of misfit (Table 2). However, the misfit of the ascending data decreases by 10.76% using the layered model, and that of the descending data decreases by 34.14%. This suggests that the layered model fits the observations better than the uniform model. The fault slip of uniform and layered crust looks very similar. Most slip concentrated between depth of 10-15 km with a width of 35 km. The maximum slip is 4 m. However, the slip model of uniform crust has some small slip patches on the top left corner, which seems not realistic, whereas the layered structure does not have that. This also suggests that using a layered structure is better. 4.2 The slow earthquake in August 2017 in southern California The slow earthquake occurred in August of 2017 in southern California on the Superstition Hills Fault in Salton Trough, California. The data come from multiple compiled descending images which show a total increase of LOS of up to 0.02 m on the southwest side and a decrease of LOS of up to 0.01 m on the northeast side. These observations suggested a northwest trending fault that dips northeast with a right-lateral strike-slip component. We used a fault model of average strike 310\u00ba, dip angle 89 \u00ba, and rake range of -5 to 5 degrees. The top of the fault is 0 km and has a width of 5 km. We used a smoothness parameter of 0.05. The best fitting model for both uniform and layered models fits the observations well, with a few mm of misfit ( Table 2). The data are along a descending path and show a decrease in misfit by 2.04% once adding a layered geology. This suggests that the layered model fits the observation better than the uniform model. The fault slips of uniform and layered crust look significantly different. In the uniform model, most of the slip is focused around a depth of 0-1 km for a width of about 10 km. The uniform model shows a maximum amount of slip at the surface as 0.04 m in a region of 0.25 km in width at a depth of 0.5 km. There is a region of slip on the east side with a maximum slip of 0.02 m. With the layered slip model however, there is more noticeable slip along both the surface and the eastern side of the fault. There is the main region of slip along the surface that reaches the maximum slip of 0.04 m and extends for 5 km at a depth of 1 km. There is also more observable slip on the eastern side reaching a higher overall magnitude, with a maximum at 0.04 m which covers a larger area than it did in the uniform model. There are more data in the uniform model, reaching the maximum slip in more areas and it appears to be more focused around the surface which is characteristic of a slow moving strike slip fault. This suggests that using a layered structure is better.  Misfit is a way to measure the change in residual data. The model should decrease in the amount of misfit after adding a layered geology."}, {"section_title": "Accelerated Glacial Melt", "text": "Global sea level rise is one of the consequences of climate change. The overall increase in ocean levels is attributed to two basic consequences that result from global warming: thermal expansion and glacial melt (Rignot et al, 2011). As oceans warm, the particles within the water obtain additional energy causing the boundaries volume to expand, and thus sea levels rise. Additionally, as air temperatures increase, terrestrial glaciers melt at a heightened rate causing large amounts of freshwater to be deposited into oceans therefore raising its total mass. The melting freshwater from glaciers not only directly contributes to an escalated global sea level, but it also pushes down heavier salt water altering the Thermohaline Circulations (THC) around the ice shelf margin and nearby coastal oceans. Warmer water from around the world mixes with fresh water as it follows the path of the Antarctic Circumpolar Current (ACC) as shown in Figure 1. The currents close proximity with Western Antarctica and the Antarctic Peninsula exposes its ice to higher temperature waters. The West Antarctic ice shelf coastline is also in danger because it rests on a continental landmass situated below sea level. Due to this, the ice sheets lack any kind of natural boundary and are in direct contact with the coastal ocean. The rate of basal melt is much higher in these areas as water melts the lower layers of ice shelves causing a rapid glacial retreat at the coastal margin.  (Thompson, 2008). Tracking meltwater routes deposited from ice shelves is crucial for not only predicting the melt rate of grounded ice sheets, but to also obtain a better understanding of the ocean's role in the stability of the West Antarctic ice sheet. Acquiring more accurate and up to date forecasts of ice sheet growth and recession can also help scientists generate more precise sea level rise predictions. Applying this research to predict the ocean's response to increased glacier melting, and thereby better understanding the cascade of processes brought on by climate change, is necessary to future predictions of the climate system."}, {"section_title": "Utilizing Neon as a Chemical Tracer", "text": "Established methods used to differentiate glacial meltwater from the sea water include identifying temperature, salinity, and dissolved oxygen concentration differences between the two. Although useful, relying on these methods can lead to biased results. For example, Biddle et al. (2017) show that oxygen concentration can fluctuate as a result of submarine photosynthetic and respiratory cycles (Figure 2). This may produce the appearance of freshwater presence by some methods that isn't supported by others. Dissolved neon is a more favorable indicator for glacial meltwater since it is inert and unwilling to interact with other atoms involved in biological or chemical ocean processes. Additionally, neon is 970% supersaturated in pure glacial meltwater (Hohmann, 2002). It can be detected down to a dilution factor of 1000:1. In other words, dissolved neon is ten times more concentrated in glacial meltwater than it is in the atmosphere at the ocean's surface. The traditional methods of collecting discrete samples for laboratory-based analysis to indicate neon presence is limited to a small sample size. The process requires several hundred samples to be collected individually and each to be analyzed for about $500 each (Manning et al, 2016). This can be costly and encourages researchers with a limited budget to utilize less accurate methods."}, {"section_title": "Gas Equilibration Mass Spectrometers", "text": "The methods of mass spectrometry have advanced such that dissolved gases including neon can be measured directly from water in the field (Holocher et al, 2003). By analyzing water directly, the need to transport individual samples and pay for subsequent analysis is eliminated. This new method, often given the title gas equilibration mass spectrometer (GEMS), is an on-site measurement for Ne and other noble gas mole ratios in water (Manning et al, 2016). Within the GEMS method, the instrument that distinguishes the mass-to-charge ratio of Ne from other ions is the quadrupole mass spectrometer. After the GEMS system separates the dissolved gas atoms from the water, the atoms are sent into the mass spectrometer (Manning et al, 2016). The exact mole ratio of neon is separated from the other atoms in water through select ion monitoring and calibration with the atmosphere which has stable neon mole ratios. (US Standard Atmosphere, 1976). As the atoms pass into the quadrupole mass spectrometer, they are struck by a stream of electrons calibrated to a particular cathode voltage, changing the atoms into positive ions. The ions are then sent through an electric field generated from four charged rods fed by AC current. An artist rendition of the quadrupole mass spectrometer can be seen in Figure 3."}, {"section_title": "Figure 3:", "text": "Quadrupole mass spectrometer apparatus displaying the path of an ideal mass to charge ratio particle through the system (From Micek, 2008)."}, {"section_title": "Chemical Getters", "text": "In vacuum systems, chemical getters are a common, yet important addition. Getters are manufactured in many forms, but all are defined as deposits of reactive material that chemically combine with or absorb gas molecules. After a vacuum has been established, the getter material can continuously remove residual gases released from the inner surfaces of the system (Mattox, 2010). This allows the system to achieve a higher vacuum than the pump can reach alone. Chemical getters are also commonly installed near the atmospheric input to many mass spectrometers to alter the composition of the particles that pass into the vacuum (Manning et al, 2016). To avoid atmospheric contamination, many getter materials need to be introduced to a vacuum system in an inactive state and later activated by heat inside the system. Common forms of a \"bulk\" getter are sintered pellets. Sintered pellets are typically made from nonevaporable getter zirconium powder and bound under high temperature in a vacuum (Jousten, 2008). This is the particular kind of gettering material used in this research experiment (Figure 4)."}, {"section_title": "Figure 4: St171 sintered getter pellets by SAES Getters", "text": "Group beside a US one-cent coin for scale."}, {"section_title": "Setup Installation and Experimental Preparations", "text": "The benchtop quadrupole mass spectrometer we used is designed for atmospheric intake only. Its configuration is comprised of 5 crucial sections ( Figure 5). Initially, air enters through a 4-meter long glass capillary and passes through a 3-way Swagelok valve. On the opposite side of the intake stem is a Swagelok bellows valve capable of restricting airflow into the getter. The Lesker getter (B) housing has available space to be filled with gettering material. The white fiberglass wrapping is used to insulate the getter housing as the getter material is activated. The Arduino UNO board (C) controls the temperature of the getter and receives live temperature data from the OMEGA RTD sensor (D) on the surface of the getter housing. After passing through the sintered pellets, the atmospheric gases pass into the Pfeiffer vacuum quadrupole mass spectrometer system (E). Additional air in the system is channeled to the right and passes through the Edwards turbo valve (E). This valve acts as a backstop to trap air within the system and allows the user to conduct static air tests. Finally, after passing the turbo valve, the air is drawn from the tubes by a Pfeiffer vacuum pump (F). Before experimentation could begin, the chemical getter material needed to be installed and the ion source tuned to boost the detection levels of neon as high as possible. Firstly, the Lesker getter nipple was filled completely with 95 St171 sintered getter pellets. Using the average mass of each pellet, it was estimated that the total mass was 28.12 g. The total available surface area of pellets was determined to be 90.13 cm 2 . The getter nipple was then installed in-line with the mass spectrometer's ion source and the capillary intake stem. Once installed, the vacuum system was turned back on and the pressure pumped down to 3.2 \u00d7 10 -4 torr. To allow the vacuum system to achieve a higher pressure, the getter was conditioned. According to SAES, the sintered getter material must exceed a minimum temperature of 450 \u00b0C in order to be activated. Figure 6 outlines the recommended activation procedures given various temperature and time variables. The heating coil was turned on and gradually increased to ensure time for heat to transfer from the getter housing into the getter material. Initially, the getter was held at 250 \u00b0C for 10 minutes. The temperature was then increased by 50 \u00b0C after each 10minute interval. Following the timed heating at 400 \u00b0C, the temperature was increased to 475 \u00b0C where it remained for 5 hours and 10 minutes. After conditioning, the vacuum pressure had lowered to 5.8 \u00d7 10 -6 torr. By tuning several of the ion sources within Quadera, particularly the cathode voltage, the signals of less concentrated gases such as neon could be amplified. To observe the cathode voltage that generated the largest ion current for 20 Ne + and 20 Ne 2+ , an ionization voltage test was conducted at several cathode voltages between an emission current of 0.5 A and 1.0 A for both species. Using the built-in Faraday Cup test on Quadera, the emission current was set to 0.5 A and the cathode voltage to -100 eV. After the ion current for both species of neon stabilized out to the onehundredths place value, the cathode voltage was decreased by 20 eV to -80eV. Once the ion current leveled out, the cathode voltage was decreased in increments of 20 eV until a test had been conducted at -40 eV. This entire procedure was then repeated for an emission current of 1.0 A. Within Python TM , an average ion current for both neon species was calculated for the test time at each voltage. Error bars were defined by the standard deviation using this average over each period. Figure 7 presents the ion source tuning results as described above. As the cathode voltage grew, the ion current of neon increased at an almost linear rate. Therefore, to maximize neon's signal to the mass spectrometer, a cathode voltage of -80 eV was chosen because it had a much lower error margin than the -100 eV setting. Additionally, an emission current of 0.5 Amps was selected since 1.0 Amps drew significantly more power but showed little increase over the ion currents displayed at an emission current of 0.5 Amps. The final ion source settings agreed upon in Quadera's Ion Source Editor for all following experimental procedures is displayed in Table 1 below.  "}, {"section_title": "Dynamic Measurements", "text": "In order to understand how the getter affects the ion current of air as it passes into the quadrupole mass spectrometer, a dynamic test while heating the surface of the getter housing to 200 \u00b0C was conducted. A dynamic test was conducted by allowing all system valves to remain open so air could flow through the 4-meter capillary, into the mass spectrometer, and out the vacuum pump continuously. To begin, the bellows and turbo valve were opened as the getter began to heat up to 200 \u00b0C from 25.5 \u00b0C. To ensure proper time for heat transfer, the getter was given 30 minutes to heat after the RTD sensor detected 200 \u00b0C. Following the heating period, we recorded the \"background\" emission current valves of each species with all valves closed to level out as the pump evacuated air from the system. When the ion currents remained stable out to the onehundredths place value, the 3-way valve was opened and the dynamic test began. The system was drawing air for a total of 41 minutes before the test ended. In an effort to understand how the efficiency of the getter changes post-activation when exposed to heat, another dynamic test was conducted with the getter at room temperature (about 25.5 \u00b0C). After allotting time for a background test, the 3-way valve was opened to allow air to flow freely through the system. After 30 minutes, the test concluded as the 3-way valve was shut. Lastly, to generate a baseline for comparison with room temperature and 200 \u00b0C getter data, a dynamic test was conducted with the getter removed. Running the vacuum with just the 4-meter column connected directly to the ion source, the SEM template was selected. Background ion current data were observed by opening the bellows and turbo valve. The 3-way valve remained shut. Once the species' ion currents had leveled out, the filament and SEM were temporarily turned off to open the 3-way valve. The system drew air for 4 hours and 53 minutes before ending the dynamic test. Ion current data collected during each of the three dynamic tests (room temperature getter, 200 \u00b0C getter, and no getter) were compiled onto the same graph in Python TM . After plotting, the ion current axis was changed to a logarithmic scale for ease of interpretation and comparison."}, {"section_title": "Geologic Setting", "text": "Lake Azuei is a large (10 km by 25 km), brackish lake located ~30 km from the capital city of Port-au-Prince. The lake is relatively shallow, about 30 meters deep at its depocenter. Lake Azuei lies along the boundary between the North American and Caribbean plates (Figure 1), which are moving at a rate of 19 mm/yr relative to each other (e.g., Benford et al., 2012). Two major transform fault systems define this broad plate boundary, the Enriquillo-Plantain Garden Fault (EPGF) and the Septentrional Fault. These fault systems are both left-lateral and separated by about 200 km (e.g. Mann et al., 1995;Symithe and Calais, 2016). The January 2010 M 7.0 earthquake ruptured along the L\u00e9og\u00e2ne fault, which abuts the EPGF. The EPGF is well defined throughout most of southern Haiti, where it travels through the southern edge of Port-au-Prince, and across the Cul-de-Sac basin before presumably plunging below Lake Azuei (Symithe and Calais, 2016;Saint Fleur et al., 2015). The geological setting of Lake Azuei is controlled by transpressional tectonics. Not only are the Southern and Northern parts of Hispaniola sliding past each other in a left-lateral motion along the EPGF and the Septentrional fault, they are also being squeezed together. Indeed, GPS monitoring across Hispaniola document an overall NE-SW transpressional motion (e.g., Benford et al., 2012;Calais et al., 2010). The main goal of the Lake Azuei Project is to determine how this transpressional motion is accommodated between strike-slip and contractional structures using seismic reflection methods. Lake Azuei is located ~60 km east of the epicenter of the 2010 earthquake, which surprised the scientific community by rupturing an unknown transpressional fault rather than the well mapped EPGF. Several folds are mapped around Lake Azuei (Symithe and Calais, 2016), as well as beneath Lake Azuei . These folds could either be en echelon drag folds or faultpropagation folds. En echelon drag folds are surficial features which develop along strike-slip fault zones, and are oriented slightly oblique to the fault. Fault-propagation folds are surface expressions of a shallow-dipping blind thrust fault, and strike parallel to the fault (where a thrust fault does not reach the surface, it is referred to as a blind fault). In the case where the Lake Azuei folds would be fault-propagation folds, the associated blind thrust would be a southdipping low-angle thrust fault which projects into the Cul-de-Sac Basin from the Massif de la Selle (Symithe and Calais, 2016). Although this is the preferred model, en echelon drag fold cannot be ruled out based on existing data (Saint Fleur et al., 2015). "}, {"section_title": "Paleoshorelines", "text": "A paleoshoreline is an ancient shoreline that is preserved above or below present water level. As such, it can provide a useful marker for prior horizontality and water level in an area where tectonic deformation occurs. These features are commonly mapped on land as \"marine terraces\" (e.g., Armijo et al., 1999;Muhs et al., 2014), but can also be imaged underwater using seismic reflection methods (Sloan et al., 2017) and/or high resolution multibeam bathymetry (e.g., Cormier et al., 2006). The marker of a paleoshoreline is the pronounced \"shoreline angle\", a sharp transition from flatter to a steeper slope. In order to groundtruth a paleoshoreline detected from seismic reflection and/or multibeam bathymetry, one needs to collect sediment samples across that paleoshoreline. The samples would then be inspected for shell hash, pebbles, or other characteristics of a beach facies. In practice, since paleoshorelines are markers of original horizontality, they can be used to determine the amount of relative vertical deformation affecting an area due to tectonic activity, and thus provide useful constraints to assess the seismic hazard of an area."}, {"section_title": "Data and Methods", "text": ""}, {"section_title": "CHIRP data analysis", "text": "GSO Technical Report 18-01 94 A three-week expedition to Lake Azuei in January of 2017 collected high-resolution subbottom seismic reflection (CHIRP) data, multichannel seismic reflection data, and three sediment cores . In order to visualize and interactively interpret the seismic reflection data, we utilized Opendtect (https://www.opendtect.org). Opendtect is a free, open source seismic interpretation system, which can display seismic profiles in 2D and 3D, highlighting the spatial relations of the profiles relative to one another (Figure 2). Using this software, sedimentary layers, called seismic horizons, were digitally tracked laterally across CHIRP profiles. Particularly, turbidite layers, deformed layers, and erosional surfaces were tracked throughout the lake bed, with the goal to gain insight into vertical deformation and fault geometry.  (Hearn et al., 2017) After digitizing the horizons, several imaging softwares were used in order to graphically represent the data and horizons. These included GMT (https://www.soest.hawaii.edu/gmt/), and Fledermaus (http://www.qps.nl/display/fledermaus)."}, {"section_title": "Gas Front and Paleo-shoreline identification", "text": "A gas front beneath the lake bed is clearly identifiable in the CHIRP profiles. The gas front is a bright reflector, which obscures most of the geological layers below it (Figure 3). The interaction of gas bubbles in shallow sediment with seismic signal results in \"acoustic turbidity\", the strong attenuation of the signal and ensuing wipe-out of underlying reflectors (Hagen and Vogt, 1999). Gas found in shallow marine sediments is generally of biogenic origin, formed by bacterial reduction of organic matter in the upper 10s to 100s of meter of sediment (e.g., Hagen and Vogt, 1999). The gas front was traced throughout all of the CHIRP profiles in order to assess the thickness of gas-free sediments throughout the lake. The 11-m paleoshoreline was picked previously based on shoreline angle in CHIRP profiles (Lucier, 2017;Sloan et al., 2017). The ~11-m paleoshoreline generally exhibits a well-defined shoreline angle, as well as three other characteristics: 1) a transitional from acoustically opaque to acoustically transparent strata; 2) a near constant ~11-m water depth; 3) the presence, locally, of a thin (~30 cm), acoustically transparent layer draped across it (Figure 4). There is however human error associated with picking the travel-time and coordinates of the paleoshoreline. For this reason, we tried to remain consistent with parameters for picking the paleoshoreline, looking for all the criteria mentioned above. However, it was difficult to pick exactly at this point, and the locations may be off by a few tenths of a millisecond, which would correspond to an estimated error of up to 30 cm. It was also impossible to pick paleoshorelines in the Southern portion of the lake due to the steepness of the slope.  Three sediment cores were taken from the lake bed. Cores 1 and 2 sampled the same location due to problems with collection of core 1. The coordinates of the cores along with associated depths are included in Table 1. The core locations were decided in the field based on the CHIRP profiles just acquired, and are shown on these profiles in Figure 5. Both sediment cores seem to sample layers identifiable in CHIRP that extend laterally below the ~11-m paleoshoreline, implying the layer is older than the paleoshoreline. The upper portions of the two cores were dated in 2017 using 210 Pb on the upper 10 cm of the cores. 210 Pb is a radiometric dating method applicable to the last ~150 years of sediment accumulation. Modern sedimentation rates for both cores was calculated to be ~1 mm/yr . To the best of our knowledge, the only other sediment core collected from Lake Azuei sampled the deepest part of the lake and indicated a modern sedimentation rate of ~6 mm/yr (Eisen-Cuadra et al., 2013); this is consistent with our rates, as our two cores sampled the lake slopes where rates of sediment accumulation are expected to be lower. The rates cannot be extrapolated to the lower portion of the core due to presumed changes in sedimentation rates, as suggested from the studies of other lakes in Hispaniola (e.g., Curtis and Hodell, 1993;Lane et al., 2014). Organic material from the lower portion of the core is currently being dated using the Accelerator Mass Spectrometry (AMS) Radiocarbon Dating method with Beta Analytic. In order to prepare the samples for AMS Carbon-14 dating, the sediment was processed following a methodology similar to that used for other lakes in Hispaniola (e.g., Curtis and Hodell, 1993). The sediment was scooped from half of a 1 cm-thick interval in the split core using a steel spatula. Some particularly sticky samples were soaked overnight in surfactant (sodium hexametaphosphate) to help breakdown the clumped sediment before sieving. These samples were stored in a tube with the surfactant after being shaken through a centrifuge or a \"vortex genie.\" The samples were then washed through a 2-mm (#10) brass frame and stainless steel cloth sieve using deionized water ( Figure 6). The remaining samples were then further washed through a 1-mm plastic sieve using deionized water, and then a 125-\u00b5m (#120) brass frame and stainless steel cloth sieve. The remaining material (<125 \u00b5m) was not examined for material useful in AMS C-14 dating, as plentiful gastropods and plant material were isolated with the three sieves. The samples were then placed in dishes according to grain size and kept wet with deionized water. A reflected light binocular microscope was used to examine the samples. The most abundant specimen available were gastropods, but plant material including wood was also found. The specimens were picked from the dishes using a fine brush and/or tweezers and stored in a separate dish according to their nature (wood, plant, or gastropod). The number of whole gastropods, gastropod fragments, and plant material was recorded for each sample ( Table 2). The samples were then dried overnight at 100\u00b0C and weighed to ensure that at least 20 mg of dry material was available for a reliable AMS C-14 dating. The description and weights of the samples selected for C-14 dating are listed in Table 3. "}, {"section_title": "Preliminary Results", "text": "The research performed this summer will provide some needed constraints about several thought-provoking questions on the geological history of Lake Azuei. Namely, what is the origin of the 11-m deep paleoshoreline? Why is the paleoshoreline not detecting any vertical deformation in the presumed extension of the EPGF? How fast is the west side of the lake uplifting?"}, {"section_title": "Maximum age of the 11-m deep paleoshoreline and possible origin", "text": "We are in the process of dating material from the bottom of the core using Carbon-14 dating methods. We anticipate the resulting dates will give insight into the climate history of Haiti. One of our five samples, the plant material from Core 2 at 63-64 cm, could not be dated due to carbon replacement. Two hypotheses could explain the origin of the ~11-m paleoshoreline. The first is that this shoreline represents a period of maximum aridity. Multiple papers (e.g. Curtis and Hodell, 1993;Caffrey et al., 2013;Lane et al., 2014) have reported a period of maximum aridity in the Caribbean. This period is also known as the \"Terminal Classic Drought\" and may be responsible, in part, for the collapse of the Mayan civilization, and ended around ~1000 years BP. A future project to test this hypothesis could be to collect cores across the paleoshoreline to precisely date its formation and, possibly, detect a signature of aridity in the sediment. However, this hypothesis cannot really explain why the paleoshoreline would have stabilized at a depth of 11 meters. If this depth corresponds to a period of maximum aridity, why did the lake not continue to dry up? Figure 7: Location of possible outlet to Lake Azuei in past, which is now obstructed by an alluvial fan (clearly visible near 71\u00b051'W). Another possible explanation for this ~11-m paleoshoreline is a period of increased precipitation. Sustained wet conditions may lead to the lake spilling over the lowest surrounding elevation (sill), with its level being maintained at that sill depth. Although there is currently no outlet to the lake, there may have been one in the past. A possible location for the sill depth that caused the ~11-m paleoshoreline is the Southeast corner of Lake Azuei, which is currently 33 meters above sea level (Figue 7). Because the 11-m deep paleoshoreline is actually at an elevation of ~10 m above sea level, ~23 m of sediment accumulation would be needed to explain the present topography. This does not seem impossible in this tectonic location since the 33-m-high sill is located on the eastern extension of the EPGF fault zone. Seismic activity, such as thrust faulting and/or earthquake-triggered landslides, and/or mudslides could cause the blockage. Such blockages have been reported in several other sites due to landslides and/or growth of alluvial fans (Atwater et al., 1996;Dai et al., 2005;Keefer, 1999); furthermore, earthquake-triggered landslides are common in Haiti (Gorum et al., 2013;Harp et al., 2016) The 33-m sill is located within a narrow corridor which connects Lake Azuei to Lake Enriquillo and is currently blocked by an alluvial fan. The city of Jimani is built on this alluvial fan, and experienced extensive flooding in 2004, suggesting that similar conditions in the past could have facilitated extensive flash floods and the rapid growth of that alluvial fan."}, {"section_title": "Distribution of biogenic gas in lake sediment and tectonic activity", "text": "Gas fronts were imaged throughout most of the CHIRP profiles, and in many cases, they were seen to rise very close to the lake bed (Figure 8). This gas front obscures most of the geological layers beneath it. Such gas front is commonly imaged in subaqueous sediment and is interpreted to be of biogenic origin, formed by bacterial reduction of organic matter in the upper 10s to 100s of meters of sediment. However, there are two environments in Lake Azuei where a gas front is deeper or is simply absent (see Figure 8): 1) The gas front is deeper in tectonically active areas (folds and faults), presumably because fractures and fissures in deformed sediments provide pathways for biogenic gas to escape (e.g., Leithold et al., 2018). This includes the monoclinal fold on the west side of the lake where a gas front seems to be absent and the smaller folds east and south of the lake; 2) CHIRP penetration exceeds 8 ms (~6 m) at the lake depocenter, below 29-30 m water depth (lines 401 and 602). MCS profiles collected along the same tracks also reveal deeper penetration in this area. This observation is compatible with anoxic conditions in the lake depocenter as it would preclude bacterial reduction of organic matter and, therefore, would preclude the formation of biogenic gas. CHIRP profiles, therefore, may provide a useful tool to estimate the spatial extent of anoxic conditions. Another line of support for anoxic conditions at the depocenter is metal geochemistry performed on a 50 cm-long sediment core collected there in 2011. That core had a negative cerium anomaly, which is typical of sediments deposited under anoxic conditions (Eisen-Cuadra et al., 2013). If anoxic conditions do occur at the depocenter of Lake Azuei, this area may contain finely laminated sediments, potentially providing a detailed, undisturbed record of climate, as well as of prior earthquake events (seismites). Additionally, the high sedimentation rate of ~6 mm/yr at the depocenter (Eisen-Cuadra et al., 2013) would favor a good stratigraphic separation between events."}, {"section_title": "Figure 8:", "text": "Thickness of gas-free layer throughout Lake Azuei. Difference between the bathymetry and the depth to the gas front, when gas is present."}, {"section_title": "Areas of tectonic deformation and their style", "text": "The ~11-m paleoshoreline has a remarkably constant depth around the lake bed (Figure 9). However, in certain portions the paleoshoreline is raised 1-2 m. Interestingly, the paleoshoreline is deformed away from the presumed fault locations and undeformed where the fault is presumed to be. Further analysis is needed to determine why. Possibly, the 1-2 m of uplift in the western region of the lake results from a single large earthquake on the subjacent blind thrust fault (Hearn et al., 2017), while no similarly large earthquake has affected the southern section of the lake since the formation of the paleoshoreline. Sediment deformation was found throughout Lake Azuei, and was prevalent in the Southern portion of the lake in the form of folds ( Figure 10). There were also signs of liquefaction in the western portion of the lake, above a large monoclinal fold imaged with the MCS data ( Figure  11).  There were no gas flares imaged in the water column in the CHIRP profiles. In contrast, some recent studies along other submerged active faults have imaged pervasive gas seeps, suggesting that faults provide ready pathways for gas and fluids (e.g. Dupre et al., 2015;Kluesner et al., 2016). In that respect, the lack of any gas flares beneath Lake Azuei is an unexpected result. Possibly, since the lake is so shallow (about 30 m deep) and possibly, relatively young, there is not enough gas accumulation within the sediment infill to sustain gas flares. Further results from continued analysis will be presented at the 2018 fall meeting of the American Geophysical Union in Washington DC (Cormier et al., 2018)."}, {"section_title": "Impact to other studies (HaitiDRILL)", "text": "There is currently an international initiative to promote scientific drilling in Haiti and specifically in Lake Azuei (project HaitiDRILL). Due to the heightened seismic risk in the area, there is a critical need for further exploration and understanding of the transform fault zone. One way to increase this knowledge is to drill into the fault zone and surrounding areas. A deep drilling site has been proposed for Lake Azuei to the International Continental Drilling Program (ICDP); the main objective of that international project (Haiti-DRILL) would be to drill in close proximity to the EPGF and constrain its tectonic evolution. The newly acquired seismic reflection profiles, especially multichannel seismic, would provide the critical information needed for selecting a drilling site in the lake, especially with respect to safety issues. For example, we imaged a lot of gas throughout the lake bed in our CHIRP profiles, and it would not be good to drill into a pocket of pressurized gas. Through the use of MCS profiles, which have deeper penetration, it would, for example, be possible to confirm the absence of pockets of high gas concentration. Additionally, structures can be imaged to see what would be worthy of drilling into. Hopefully, the results from our project will aide in the funding of the HaitiDRILL project. That project would not only advance our understanding of the tectonic evolution of the region, but deep drilling would also provide the data needed to unravel the timing of the closure of a presumed open seaway between the Atlantic and Caribbean (Mann et al., 1995), provide an estimate of earthquake recurrence in the area (paleoseismology), and potentially help establish the longest sedimentary record of climate change in the northern Caribbean (e.g., Hodell et al., 1991). Lastly, it could nail down the timing of the arrival of the first inhabitants in Hispaniola, the Arawak (Taino) people through their impact on the environment, for example by dating the appearance of maize pollen and evidence of increasing fire (e.g. Lane et al., 2008Lane et al., , 2009Lane et al., , 2014."}, {"section_title": "On-going Work", "text": "We are currently in the process of analyzing our paleoshoreline data to interpret tectonic deformation rates in the lake bed. We are waiting for the AMS C-14 dates to come back from processing in order to confine the age of the ~11-m paleoshoreline.   The eastern oyster, Crassostrea virginica, is both ecologically and economically important. Oyster reefs filter water and act as diverse habitats, and oyster aquaculture operations provide jobs and food to coastal communities. The protozoan parasite Perkinsus marinus causes Dermo disease which decimates oyster populations. The microbiome has been implicated in the health status of a variety of animals, as it has been shown to help absorb nutrients, and fight off pathogens. However, little is known about the composition and function of the oyster microbiome. This study aimed to examine the relationship between the oyster microbiome and severity of infection with Perkinsus marinus using 16S rRNA sequencing. Tissue samples (gills, mantle, gut, hemolymph and pallial fluid) were collected from oysters sampled from Ninigret Pond, Charlestown, RI. Following the identification of disease status, oysters with a range of infection levels were selected for DNA extraction and PCR amplification of the V4 region of the 16S rRNA. The PCR amplicons were sequenced with an Illumina MiSeq platform and analyzed with QIIME2, PICRUSt, and LEfSe. Overall, 1,722 amplicon sequence variants were identified, with dominant phyla including the Proteobacteria, Fusobacteria, Tenericutes, Spirochaetes, and Bacteroidetes. Results indicate that there are differences in the alpha and beta diversity between tissue types with the gut beta diversity being distinct from other tissues. Additionally, functional differences were identified in the shell swab, mantle, and gut samples. Future research and sequencing efforts can be dedicated to exploring the existence of a core oyster microbiome. (The rest of this section intentionally left blank) While a number of studies have examined coastal water quality changes (e.g., hypoxia), questions remain about the driving processes and benthic responses. A Low-cost Benthic Sensing Trap-Attached System (LoBSTAS) has been developed for multi-day image and video observations of benthic physical processes and their relationship to local dissolved oxygen levels. The prototype LoBSTAS unit contains a camera and dissolved oxygen sensor to monitor hypoxia in relation to pictures and videos of sediment suspension, particle motion, organic matter concentration, as well as water-column movement and seafloor reworking by physical processes or benthic organisms. Using off-the-shelf components and open source software in construction makes LoBSTAS affordable for potential use by researchers, managers or the public with limited finances. The low cost also enables deployment of a LoBSTAS fleet to understand spatial and temporal variations. While the LoBSTAS was initially designed to be deployed with lobster traps to evaluate benthic organism interactions with the cages, the system can also be attached to other frames or structures. At a depth of five meters, the current prototype can capture one image and a thirty-second video each hour, for a duration of seventy hours with a 5200 mAh power bank. Illumination and power consumption were significant hurdles in capturing long-duration, high quality day and night imagery in turbid waters. Future work may include computer vision algorithms for motion detection to capture benthic organisms, automatic lighting adjustment, and particle image velocimetry to deduce local benthic currents."}, {"section_title": "Underwater Housing", "text": "The cylindrical shell which houses the core components is made with widely accessible 3-inch diameter schedule 40 PVC Pipe with a length of 6-10 inches (approximately 15-25 cm). For the compact \"Mini\" version, a 2-inch diameter PVC pipe may be used instead to increase portability and reduce cost by almost 50%. The main body length can also be shortened or extended to fit extra components. Schedule 40 thickness is sufficient, as a thicker-walled Schedule 80 does not necessarily increase operational depth rating (Bergshoeff et al., 2017). On one end of the cylindrical housing is a tight-fitting square head expansion plug, and on the other end is the window which is glued to the main body with PVC cement. The expansion plug is made of plastic and a porthole can be made on the square head for sensors. The window, a 1/4 or 3/16-inch-thick acrylic disk, was placed in between the body and PVC coupling. This configuration provides a protecting frame for the acrylic window and creates two sealing points: the acrylic-coupling contact, and the body-acrylic contact which is the most important. PVC Cement was generously used in the body-acrylic contact, and waterresistant double-bubble epoxy was used to cover the coupling-acrylic contact as well as any imperfect edges on the acrylic circle. Clear PVC was used for the window material in an earlier prototype, but a later prototype used acrylic for lesser deterioration of image quality. Note that acrylic-PVC may not weld as well as PVC-PVC if the PVC cement used does not contain key ingredients: Methyl Ethyl Ketone (MEK) and acetone. The housing was left overnight for the adhesives to cure before leak tests at a depth of 17ft (5m). The expansion plug (especially the 3-inch plug) required a wrench to fully tighten to prevent leakages. "}, {"section_title": "Core Controller: Raspberry Pi", "text": "The core includes a camera module, LED ring, and optional sensor controlled by the Raspberry Pi computer. The first prototype uses a Raspberry Pi 3 board which has a quadcore 1.2GHz CPU, 1GB RAM, USB ports, HDMI port, Wifi, Bluetooth, and GPIO pins. Built-in wifi served as the main entry point to control the Pi and access stored data through Secure Shell (SSH) from a smartphone or computer. The Pi controls the other components with open-source Python scripts in a Linux environment, and is able to simultaneously collect camera and sensor data with multi-threading. 2 In a subsequent prototype, a Raspberry Pi Zero W was used. This version is 60% smaller, 70% cheaper, and has approximately 50% lesser power consumption (Fig. 2)."}, {"section_title": "Power Management", "text": "The goal of the battery life was at least 3-4 days to match the common deployment cycles of lobster cages. In continuous data-taking mode (10s sampling interval), the Pi 3 was estimated to last about 9-11 hours with an Anker 5200 mAh power bank (5V 2A output) via a microUSB port. Sampling interval did not significantly affect the battery life, since the Pi remained on. Thus a solution was to extend battery life by shutting down the Pi in between samples. Although the Pi can enter a 'sleep' state by halting all processes, it cannot 'wake' itself by means of code. Additionally, once power is cut, the Pi computer loses track of time as it does not have a built-in Real-time Clock (RTC). To address this challenge, we added an optional Witty Pi power management board which fits on the Pi board's GPIO pins; it keeps time with an RTC and can be scheduled to wake the Pi up at pre-coded intervals (Fig. 3). It also has the option to add a dummy load so that the power bank does not shut down due to low current draw. The Witty Pi 2 for the Pi 3 and Witty Pi Mini for the Pi Zero adds about $20-23 USD to the total cost. With this power management board, total battery life can be varied as a function of sampling interval, since a longer interval equals to a longer down time of the Pi.  "}, {"section_title": "Camera Specifications", "text": "From the wide range of camera modules compatible with the Pi that can be easily installed on the board's Camera Serial Interface (CSI) port, the Sainsmart FOV160 \u2022 5-Megapixel Camera Module was selected. The Sainsmart module uses the same OV5647 sensor as the official Raspberry Pi v1 camera but have additional embedded wide-angle lens with adjustable focus. This enables us to focus on nearby imagery, since turbid waters greatly reduce viewing distance. While the camera can take 1920\u00d71080 resolution JPEG images (approximately 2MB per image) and raw H264 videos at a frame rate of 30fps, we chose a frame rate of 15 fps and a resolution of 1296\u00d7972 (850kB per image) to take advantage of 2\u00d72-pixel binning that increases the signal by four times in low-light conditions. A lower frame rate increases maximum shutter time and thus improves low-light imaging. However, lower frame rate introduced motion blur and so we set 15 fps as the minimum frame rate. We attempted to manually set the camera's sensitivity to maximum (ISO 800) and set longer shutter speeds to push the limits of low-light imaging, but preliminary tests suggest that automatic exposure settings fared better. Of all the exposure settings in the picamera Python package, the exposure setting named nightpreview gave the best results in lowlight conditions."}, {"section_title": "Lighting", "text": "To aid the camera in low-light conditions and make night imaging possible, we added a NeoPixel 16 \u00d7 5050 RGB LED Ring. The LED ring is connected to board pins 2, 9, 12 for power (5V), ground, and data signal respectively. The window caused light reflections in the image, which were especially significant for objects further than two feet away. Use of the wide-angle lens captured more reflection than the standard lens, but this can be addressed by using a larger-radius LED ring or putting the LED ring further behind the camera. A potential solution was to use separate windows or housing for the camera and lighting; this will be discussed in greater detail in the results section. In addition to reflection challenges, prototypes with the light ring and Clear PVC window captured images with blue tints due to the higher opacity of the material in comparison with acrylic. Although setting the LED light to be yellow can counter this blue tint, we decided that for same-house lighting, acrylic is the best window material."}, {"section_title": "Dissolved Oxygen Sensor", "text": "The LoBSTAS was designed to be modular and highly customizable, where extra features or sensors can be added on. We chose to add an Atlas Scientific sensor to measure dissolved oxygen (DO) since DO is a key diagnostic of water quality. We changed the sensor's data protocol from UART to i2c mode, and connected power (5V), ground, SDA, and SCL to pins 4, 6, 3, and 5 respectively (Fig. 4a). When not taking data, we set the sensor to sleep mode which reduces current draw by 95%. Since the device is very sensitive to electrostatic interference, we recommend enclosing it in anti-static pouches. We drilled a 0.6inch diameter hole in the square head expansion plug and potted the probe with epoxy. For a single sensor, this method gives the probe mechanical stability; if there are multiple sensors, there is insufficient space on the square head and we will need to pot the wire cables instead. The galvanic membrane DO sensor has a range of 0.01 to 100 mg/L with an accuracy of 0.05 mg/L. Although the shortest response time is one reading per second, when the probe is first powered up it needs approximately 40-60 seconds for readings to increase from zero to actual value and stabilize. Given that oxygen diffuses through the sensor membrane and is reduced at the cathode, the sensor consumes small amounts of oxygen and requires small water movement (60 ml/min recommended) to take accurate readings. The sensor is capable of dual point calibration with zero and atmospheric DO concentration, but we only did atmospheric calibration as zero-point calibration is only needed for measurements less than 1 mg/L. Temperature, salinity, and pressure compensation have default values of 20 \u2022 C, 0 Siemens, 101.3 kPa and can be user defined, but only salinity and pressure settings are retained when power is cut. Atlas Scientific notes that for depths of less than 10 meters and water conductivity less than 2.5 millisiemens, the pressure and salinity compensation values are irrelevant. Thus, we did not set compensation values for our initial test deployments at <6m depth."}, {"section_title": "4 Future Work", "text": "We have started development of the third prototype 'Mini Wye' which uses a 'Y' PVC connector (Fig. 8c). One branch would be the camera window, and the other branch would provide the light. While this is slightly more expensive than a single unit, it is less costly than a twin configuration which uses two units. This new housing will enable the LoBSTAS to take high quality images and videos any time of the day. Improvements on the sensor probe porthole will also need to be made to guarantee a watertight housing. To better quantify turbidity, robust computer vision algorithms will be needed to calculate average color of a benthic image, total suspended matter in a given area, or distinguish image contrast of an underwater Secchi disc. An algorithm which adjusts the LED lighting levels based on the brightness of the scene will also be useful as current lighting system is based on fixed sunrise and sunset times. This will aid imaging on dark cloudy days or deeper waters, and account for varying daylight hours across seasons. In the future, after testing the maximum pressure the housing can withstand in a hydrostatic pressure chamber, we hope to deploy the LoBSTAS in different coastal environments such as the Wickford Shipyard in Rhode Island (USA), Densu estuary (Ghana), and potentially polar coasts."}, {"section_title": "Conclusion", "text": "To better understand benthic physical processes and its link to water quality, we developed a Low-cost Benthic Sensing Trap-attached system (LoBSTAS) for use by communities with limited financial means. With a 5200 mAh power bank and sampling interval of one hour, the current prototype can be deployed for three days. From images and videos of the benthic region, we were able to characterize sediment type, sediment suspension, particle motion, organic matter concentration, as well as seafloor currents and movement patterns of benthic organisms. Although the LoBSTAS was initially design to monitor the benthic environment near shellfish traps, this design can be adapted to be used in many other observational studies of marine organisms."}, {"section_title": "Exploring the Effects of Turbulence on Microzooplankton Growth and Grazing", "text": "Anna Ward*, Gayantonia Franz\u00e8, Susanne Menden-Deuer *Scripps Institution of Oceanography, University of California, San Diego; Graduate School of Oceanography, University of Rhode Island"}, {"section_title": "Motivation", "text": "Plankton are key players in the ocean's biogeochemical cycles; they serve as biomass resources for higher trophic levels and contribute to carbon sequestration and vertical material transport through respiration and sinking flux [1]. The amount of primary production consumed by microzooplankton ultimately affects ecosystem structure and function; thus, it is pivotal to understand how environmental factors drive microzooplankton physiological rates [1,2,3]. Turbulent environments might alter trophic dynamics by enhancing or reducing the encounter rate between microzooplankton and their prey [4]. With this study we investigated the influence of turbulence on microzooplankton growth and grazing rates. Given changing climate conditions, it is critical to understand how plankton dynamics will be affected, altering energy and matter fluxes in coastal waters. \u27a2Turbul ence affects microzooplankton grazing by facilitating the encounter rate between the predator and their prey. The response in growth rates appears highly variable, with significant intraspecific differences. \u27a2I ncreased grazing in highly turbulent environments could lead to a faster depletion of resources affecting population growth and consequently affecting biomass and carbon availability/transfer to higher trophic levels."}]