[{"section_title": "Abstract", "text": "Motivation: Second generation sequencing technologies are being increasingly used for genetic association studies, where the main research interest is to identify sets of genetic variants that contribute to various phenotypes. The phenotype can be univariate disease status, multivariate responses and even high-dimensional outcomes. Considering the genotype and phenotype as two complex objects, this also poses a general statistical problem of testing association between complex objects. Results: We here proposed a similarity-based test, generalized similarity U (GSU), that can test the association between complex objects. We first studied the theoretical properties of the test in a general setting and then focused on the application of the test to sequencing association studies. Based on theoretical analysis, we proposed to use Laplacian Kernel-based similarity for GSU to boost power and enhance robustness. Through simulation, we found that GSU did have advantages over existing methods in terms of power and robustness. We further performed a whole genome sequencing (WGS) scan for Alzherimer's disease neuroimaging initiative data, identifying three genes, APOE, APOC1 and TOMM40, associated with imaging phenotype. Availability and Implementation: We developed a C \u00fe\u00fe package for analysis of WGS data using GSU. The source codes can be downloaded at https://github.com/changshuaiwei/gsu. Contact:"}, {"section_title": "Introduction", "text": "The ongoing sequencing studies allowed researchers to comprehensively investigate the role of a deep catalog of human genome variations in complex diseases (Cirulli and Goldstein, 2010) . Although these studies hold great promise for uncovering novel diseaseassociated variants, the massive sequencing data bring tremendous computational and statistical challenges to data analysis. Sequencing data are characterized with high-dimensionality and sparsity, where a large portion of genetic variants are rare variants with minor allele frequency (MAF) smaller than 5%. Even with a large effect size, a rare variant is hard to detect because of its low MAF. Moreover, the massive number of rare variants raises computational burden and multiple comparison issue.\nThe common strategy is to perform a joint association test, namely, testing the joint effect of a set of single nucleotide variants (SNVs) on a genomic region, a functional unit (e.g. a gene) or a functional pathway. By combining multiple SNVs, the association information is aggregated and the number of tests is greatly reduced. Among these approaches, methods based on variance component score tests (VCscore) are widely used (Lin, 1997; Wu et al., 2011) . The methods considered the effects of the multiple variants as a random effect, and then test the effect by testing the variance component under the framework of the linear mixed model or the generalized linear mixed model.\nThere are also increasing interests in studying shared genetic contribution to multivariate phenotype. The multivariate phenotype can be multiple measurements evaluating different aspects of a disease, which better reflect the underlying biological mechanism of the disease. It can also be multiple disease phenotypes that used for studying co-morbid genes or pleiotropic gene (Dick and Agrawal, 2008) . A few methods can test the association of SNV-set with multivariate phenotype, yet, most of the current methods cannot handle multivariate phenotype when the outcome variables are of different types (e.g. some variables are binary while others are continuous). Besides conventional multivariate phenotype, modern data types, such as shapes, images and trees, are emerging in biomedical researches. These complex objects are difficult to be integrated in traditional statistical frameworks, whose primary interests are variables in vector spaces (e.g. continuous, ordinal and categorical variables). Yet, it is relatively easy to define distance metric or similarity metric for complex objects. As a consequence, many distance and similarity based methods have been proposed for modern data analysis.\nIn this paper, we present a similarity-based test using U statistic, referred to as the Generalized Similarity U test (GSU). GSU can be used to test the association of high-dimensional and sparse predictors with univariate, multivariate or complex-object responses from sequencing association studies or other association studies. We first studied the theoretical properties of GSU in a general setting in Section 2, where we investigated the finite-sample properties and asymptotic properties of the test. In Section 3, we then focused on the applications of GSU to genetic sequencing studies. Extensive simulation studies were conducted to evaluated the performance of GSU in Section 4, followed by a whole genome sequencing (WGS) data application in Section 5.\n2 Generalized similarity U"}, {"section_title": "General setting and rational", "text": "We start with a formal set-up. Let \u00f0X; A; P\u00de and \u00f0W; G; d\u00de be a probability space and a metric space, respectively. Consider random elements Y and G taking values on metric space \u00f0W Y ; G Y ; d Y \u00de and \u00f0W G ; G G ; d G \u00de with distribution P Y and P G , respectively. Here, the random elements can be random variables (e.g. W \u00bc R), random vectors (e.g. W \u00bc R p ), random matrix (e.g. W \u00bc R p\u00c2p ), random graph (e.g. trees) or random objects (e.g. shapes).\nLet y and g denote the realization of the random response element Y and random predictor element G. Given a sample of data f\u00f0y i ; g i \u00deg 1 i n , we are interested in testing the association of response Y and predictor G. Since Y and G may not live in a natural vector space, it is not straight forward to construct a regression model, such as E\u00f0YjG\u00de \u00bc f \u00f0G\u00de. However, it is easier to construct similarity measurement for pairs f\u00f0y i ; y j \u00deg i6 \u00bcj and f\u00f0g i ; g j \u00deg i6 \u00bcj with the distance metrics d Y and d G . Intuitively, if Y and G are associated, then high similarity between g i and g j should lead to high similarity between y i and y j .\nThe similarity measurement can be defined by a real-valued function quantifying the similarity between two elements. For example, we can define the similarity between y i and y j as h\u00f0y i ; y j \u00de \u00bc e \u00c0dY \u00f0yi;yj\u00de , so that the closer y i and y j are in the metric space, the more similar y i and y j are. Other possible transformations from distance to similarity include inverse transformation h\u00f0y i ; y j \u00de \u00bc \u00f0d Y \u00f0y i ; y j \u00de \u00fe e\u00de \u00c01 (for some e > 0) and thresholding transformation h\u00f0y i ; y j \u00de \u00bc 1 fdY \u00f0yi;yj\u00de < eg (for some e > 0). Loosely speaking, any monotonically nonincreasing function can be used to transform distance to similarity.\nHere, we list some examples of similarity measurements. Example 1 (vector similarity): Let y i ; y j 2 R p . We can use Gaussian kernel (h\u00f0y i ; y j \u00de \u00bc exp\u00f0\u00c0jjy i \u00c0 y j jj 2 2 \u00de) or cross-product kernel (h\u00f0y i ; y j \u00de \u00bc < y i ; y j >) to measure similarity. Here, < y i ; y j > can be considered as a transformation from Euclidean distance jj \u00c1 jj 2 2 , using the fact that < y i ;y j >\u00bc \u00c01=2\u00f0jjy i \u00c0 y j jj 2 2 \u00c0jjy i jj 2 2 \u00c0jjy j jj 2 2 \u00de. Example 2 (graph similarity): Let y \u00bc \u00f0V; E\u00de be a graph with adjacency matrix A, where V is the set of vertices and E is the set of edges. For any two graphs y i and y j , we can construct a product graph y i y j , with adjacency matrix A i A j . The similarity between the two graphs can be calculated using random walk on the product graph, h\u00f0y i ; y j \u00de \u00bc P k w k q T W k p (Vishwanathan et al., 2010) , where k is the length of the random walk, w k is the weight for size-k random walk, p is the initial probability for vertices on y i y j ; W is the transition probability obtained from A i A j , and q is the stopping probability for vertices on y i y j . Beside random walk, graph similarity can also be calculated using graphlet and subtree pattern.\nExample 3 (image similarity): Image similarity can be calculated from local features and global features of the images by using traditional computer vision techniques such as scale invariant feature transformation (SIFT) and histogram of gradients (HOG). Both SIFT and HOG are human designed feature extraction. With large datasets, we can use modern machine learning methods, such as deep neural network (LeCun et al., 2015) , to automatize the feature extraction, and construct more meaningful image similarity from high level representation of image."}, {"section_title": "A motivating model", "text": "Given the predictor elements and the response elements for the subjects i and j, we denote their response similarity S i;j by S i;j \u00bc h\u00f0y i ; y j \u00de;\nand denote their predictor similarity K i;j by K i;j \u00bc f \u00f0g i ; g j \u00de . The similarity measurements h\u00f0\u00c1;\u00c1\u00de : W Y \u00c2 W Y ! R and f \u00f0\u00c1;\u00c1\u00de : W G \u00c2 W G ! R can be of a general form as long as they satisfy the finite second moment condition, i.e. E\u00f0h 2 \u00f0Y 1 ;Y 2 \u00de\u00de < 1 and E\u00f0f 2 \u00f0G 1 ;G 2 \u00de\u00de < 1, where Y 1 and Y 2 (G 1 and G 2 ) are independent identical copy of Y (G). We center the response similarityS i;j \u00bch\u00f0y i ;y j \u00de b\u1ef9 h\u00f0y i ; y j \u00de \u00bc h\u00f0y i ; y j \u00de \u00c0 E\u00f0h\u00f0y i ; Y j \u00de\u00de \u00c0 E\u00f0h\u00f0Y i ; y j \u00de\u00de \u00fe E\u00f0h\u00f0Y i ; Y j \u00de\u00de; (1) and center the predictor similarity,K i;j \u00bcf \u00f0g i ; g j \u00de, in the same manner. Based on the definition of the centered similarity, we can show that E\u00f0f \u00f0G i ; G j \u00de\u00de \u00bc 0 and E\u00f0h\u00f0Y i ; Y j \u00de\u00de \u00bc 0 (Supplementary Appendix S1).\nWe can investigate the relationship of the two similarities using a similarity regression model (Elston et al., 2000; Tzeng et al., 2009) , E\u00f0S r jK r \u00de \u00bc bK r 8r 2 f\u00f0i; j\u00de; i < jg:\nSince the similarities have been centered, the regression has zero intercept. The association can then be evaluated by testing null hypothesis\nAs we shall see soon, U b is in the same form as the generalized similarity U."}, {"section_title": "Weighted U statistic", "text": "The generalized similarity U (GSU) is defined as the summation of the centered response similarities weighted by the centered predictor similarities,\n(2) whereK i;j is considered as the weight function andS i;j is considered as the U kernel. In our definition of GSU, the role of response similarity and predictor similarity are interchangeable. In other words, we can also treatS i;j as the weight function andK i;j as the U kernel. Under the null hypothesis, when the predictor element G is independent of response element Y (i.e. Y? ?G), we have\nUnder alternative hypothesis, when Y is associated with G, we expect that the response similarity is concordant with the predictor similarity. In other words, the positive response similarities are weighted heavier and the negative response similarities are weighted lighter, leading to a positive value of U statistic. A statistical test can be formed to test the association, and P-value can be calculated by P \u00f0U > U obs \u00de under null hypothesis, where U obs is the observed value of U.\nDefine a population parameter l U as\nIt is easy to show that GSU is an unbiased estimator of l U , i.e. E\u00f0U\u00de \u00bc l U . In addition, knowing that l U \u00bc E\u00f0f \u00f0G 1 ; G 2 \u00deh\u00f0Y 1 ; Y 2 \u00de\u00de \u00c0E\u00f0f \u00f0G 1 ; G 2 \u00de\u00deE\u00f0h\u00f0Y 1 ; Y 2 \u00de\u00de; we can consider l U as a population covariance. In this sense, a scale invariant 'correlation', U c , can be calculated\nas an indicator of strength of association."}, {"section_title": "Strongly positive definite similarity", "text": "We have already shown that Y? ?G ) l U \u00bc 0, which ensures the correct Type I error. It is of interest now whether l U \u00bc 0 ) Y? ?G, so that we can control Type II error (i.e. improve power) and reject null hypothesis whenever ? ? = . The establishment of l U \u00bc 0 ) Y? ?G needs additional assumptions on the similarity measurements and metric spaces. For the completeness, we first introduce several preliminaries. Define a 'kernel' as a real symmetric function h : W \u00c2 W ! R. A kernel is called positive definite if P n i;j c i c j h\u00f0y i ; y j \u00de ! 0; 8 c i ; c j 2 R and 8 y i ; y j 2 W. A kernel is called negative definite if P n i;j c i c j h\u00f0y i ; y j \u00de 0; 8 c i ; c j 2 R; y i ; y j 2 W and P i c i \u00bc 0. A positive definite kernel is called strictly positive definite when the equality P n i;j c i c j h\u00f0y i ; y j \u00de \u00bc 0 implies c i \u00bc 0 8i. The kernel function here can be used to define similarity measurement. To consider l U \u00bc 0 ) Y? ?G; however, we need the kernel function to exhibit a property of 'strong' positive definiteness in the integral form. Using similar notions of Rachev et al. (2013) , we define a strongly positive definite kernel as follows.\nDefinition 1: Let Q be a finite positive measure on \u00f0W; G; d\u00de and q be a function integrable with respect to Q. We say h is strongly positive definite if it is positive definite and the equality \u00d0 W \u00d0 W h\u00f0x; y\u00de q\u00f0x\u00deq\u00f0y\u00dedQ\u00f0x\u00dedQ\u00f0y\u00de \u00bc 0 implies q \u00bc 0 a.e. 8 Q.\nLet # be a finite signed measure dominated by Q s.t. d# \u00bc qdQ. For strongly positive definite kernel h, the equality \u00d0 W \u00d0 W h\u00f0x; y\u00ded#\u00f0x\u00de d#\u00f0y\u00de \u00bc 0 implies # \u00bc 0 8 #. Now let # \u00bc P GY \u00c0 P G P Y be a measure on W G \u00c2 W Y , we can show (in Supplementary Appendix S1) that\nIf the tensor product kernel \u00f0f h\u00de\u00f0\u00f0g 1 ; y 1 \u00de; \u00f0g 2 ; y 2 \u00de\u00de \u00bc f \u00f0g 1 ; g 2 \u00de h\u00f0y 1 ; y 2 \u00de is strongly positive definite, then\nIn fact, we can show l U \u00bc 0 ) Y? ?G as long as f and g are both strongly positive definite.\nTheorem 2:\nAssume both f \u00f0\u00c1; \u00c1\u00de and h\u00f0\u00c1; \u00c1\u00de are strongly positive definite. Leth\u00f0Y 1 ; Y 2 \u00de andf \u00f0G 1 ; G 2 \u00de be the centered similarities as defined in (1)\nThe proof is given in Appendix A by employing measures embedding into the reproducing kernel Hilbert space. Many popular kernels such as radial basis kernel h\u00f0y 1 ;y 2 \u00de \u00bc exp\u00f0\u00c0jjy 1 \u00c0 y 2 jj q \u00de (0 < q 2) are strongly positive definite kernel on R p (Sriperumbudur et al., 2010) . However, the cross product kernel h\u00f0y 1 ;y 2 \u00de \u00bc< y i ;y j > is not strongly positive definite on R p , by observing that"}, {"section_title": "Asymptotic test", "text": "For high-dimensional data, it is computationally expensive to calculate P-values P\u00f0U > U obs \u00de using permutation. Here, we derive the asymptotic distribution of GSU under null hypothesis. By considering the predictor similarity as the weight function and the response similarity as the U kernel, GSU is a weighted U statistic (Lindsay et al., 2008; Wei et al., 2016) . More specifically, because its kernel satisfied Var\u00f0E\u00f0h\u00f0Y 1 ; Y 2 \u00dejY 2 \u00de\u00de \u00bc 0 (Supplementary Appendix S1), GSU is a degenerated weighted U statistic. To derive the limiting distribution of GSU, we can decompose the centered response similarity by,h\u00f0y 1 ; y 2 \u00de \u00bc P 1 s\u00bc1 k s / s \u00f0y 1 \u00de/ s \u00f0y 2 \u00de; where fk s g and f/ s \u00f0\u00c1\u00deg are eigenvalues and eigenfunctions of the U kennelh\u00f0\u00c1; \u00c1\u00de, and all the eigenfunctions are orthogonal, i.e. \u00d0 / s \u00f0y 1 \u00de/ s 0 \u00f0y 1 \u00dedF\u00f0y 1 \u00de equals 0 if s 6 \u00bc s 0 and equals 1 if s \u00bc s 0 . Similarly, we can decompose the centered predictor similarity by,f \u00f0G i ; G j \u00de \u00bc P 1 t\u00bc1 g t u t \u00f0g 1 \u00deu t \u00f0g 2 \u00de. We can then rewrite the GSU as,\nUsing the form above, we can show that the limiting distribution of GSU is a weighted sum of independent v 2 random variables. This is the result of Theorem 3, which is proved in Appendix B.\nTheorem 3: Assume E\u00f0h\u00f0Y; Y\u00de\u00de < 1; E\u00f0f \u00f0G; G\u00de\u00de < 1, and\nUsing the similar techniques, we can show that a weighted V statistic in the following form, V \u00bc 1"}, {"section_title": "Power and sample size", "text": "In this subsection, we derive the asymptotic distribution of GSU under the alternative hypothesis, and provide asymptotic power and sample size calculations for association analysis.\nAssume under the alternative hypothesis that l U > 0 and f 1 > 0. Using the Hoeffding projection, we can show that GSU asymptotically follows a normal distribution, with mean l U and variance 4f 1 =n. This is the result of Theorem 4, which is proved in Supplementary Appendix S4.\nTheorem 4: Leth\u00f0Y 1 ; Y 2 \u00de andf \u00f0G 1 ; G 2 \u00de be the centered similarities as defined in (1). Suppose Y is associated with G, and the following conditions are satisfied:\nand U\u00f0\u00c1\u00de is the CDF of a standard normal distribution. The sample size required to achieve power b can be calculated by solving\n3 Generalized similarity U for sequencing association"}, {"section_title": "Settings for sequencing data analysis", "text": "In a sequencing association study, the response element is called phenotype and the predictor element is called genotype. Common forms of phenotype and genotype are scalars or vectors. Suppose that n subjects are sequenced in a study, where we are interested in testing the association of L phenotypic variables (y i;l ; 1 i n,\nFor each subject i, we observe a phenotype vector y i (y i \u00bc \u00f0y i;1 ; y i;2 ; . . . ; y i;L \u00de) and a genotype vector g i (g i \u00bc \u00f0g i;1 ; g i;2 ; . . . ; g i;M \u00de). In the special case when L \u00bc 1 (or M \u00bc 1), it is simplified to a univariate analysis (or a single-locus analysis). When L > 1 (or M > 1), it extends to a multivariate analysis (or a multi-locus analysis). Here, we allow multiple phenotypes to be of different types (e.g. continuous or categorical), and do not assume any distribution of phenotypes. The number of genetic variants M and the number of phenotypes L can be larger than the sample size. For example, the genetic data can be sequencing data (high-dimensional genotype) and the phenotype data can be imaging data (high-dimensional phenotype)."}, {"section_title": "Similarity measurement", "text": "The choices for the phenotype similarity h\u00f0\u00c1; \u00c1\u00de and the genetic similarity f \u00f0\u00c1; \u00c1\u00de are flexible. According to different types of genetic variants and the purpose of the analysis, we can choose different types of phenotype similarities and genetic similarities.\nFor phenotype similarity, one popular approach is to use a cross product kernel, i.e. h\u00f0y i ; y j \u00de \u00bc < y i ; y j > (Tzeng et al., 2009 ). Yet, as discussed in previous theoretical analysis, cross product kernel may not fit for robust association analysis. Here, we propose a similarity measurement for both categorical and continuous phenotype using radial basis kernel with L1 norm [Laplacian Kernel (LK)],\nx l jy i;l \u00c0 y j;l j\u00de;\nwhere x l represents the weight for the lth phenotypes given based on prior knowledge. If there is no prior knowledge, we can use an equal weight, x l \u00bc 1=L. The LK-based phenotype similarity can be modified to take the correlation among the phenotypes into account,\nwhere d ij \u00bc \u00f0jy i1 \u00c0 y j1 j 0:5 ; . . . ; j y iL \u00c0 y jL j 0:5 \u00de T . C can be chosen to reflect the correlations among the phenotypes. For example, we can define C as C \u00bc 1 n P n i\u00bc1 y i y T i \u00c00:5\n:\nFor the categorical SNVs data, the popular way of measuring genetic similarity is to use IBS function or the weighted IBS function (Lynch and Ritland, 1999) . Assuming the genetic variants (g i;m ; 1 i n; 1 m M) are coded as 0, 1 and 2 for AA, Aa and aa, respectively, the IBS-based genetic similarity is defined as\nAlternatively, the weighted-IBS (wIBS) genetic similarity can be defined to emphasize the effects of rare variants, K wIBS\nwhere w m represents the weight for the mth SNV in the SNV-set, and ! is a scaling constant, defined as ! \u00bc P M m\u00bc1 w m . w m is usually defined as a function of MAF (denoted as c m ). For example, the weight w m can be calculated using inverse variance, i.e. w m \u00bc 1= ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi c m \u00f01 \u00c0 c m \u00de p . However, IBSbased similarity cannot be used for other genetic data, such as copy number variation (count) or expression data (continuous). Here, we propose a unified LK-based genetic similarity by generalizing wIBS,\nwhere g i;m can be categorical, count or continuous variables, and w m can be calculated as function of variance r 2 m of g m , i.e. w m \u00bc 1=r m .\nThus, we defined a unified measurement for genetic similarity and phenotype similarity with LK exp\u00f0\u00c0j \u00c1 \u00c0 \u00c1 j\u00de. Since LK is strongly positive definite, we know that (from Theorem 2) the corresponding GSU has the property l U \u00bc 0 () G? ?Y, so that it can control Type II error for the detection of any types of association. Since LK is bounded similarity measurement, i.e. 0 h\u00f0\u00c1; \u00c1\u00de 1 and 0 f \u00f0\u00c1; \u00c1\u00de 1, we know the regularity conditions in Theorem 3 is satisfied and the asymptotic test for corresponding GSU is robust against distribution assumptions (for large sample size)."}, {"section_title": "Computation and covariates adjustment", "text": "Let S \u00bc fS i;j g n\u00c2n and K \u00bc fK i;j g n\u00c2n be the matrix form of the phenotype similarity and genetic similarity, the centered similarity matri-cesS andK can be obtained by,S \u00bc \u00f0I \u00c0 J\u00deS\u00f0I \u00c0 J\u00de; and K \u00bc \u00f0I \u00c0 J\u00deK\u00f0I \u00c0 J\u00de; where I is an n-by-n identity matrix, and J is an n-by-n matrix where all elements are 1=n (Supplementary Appendix S5). Then GSU can be expressed as, U \u00bc \u00f01=n\u00f0n \u00c0 1\u00de\u00de P i6 \u00bcjK i;jSi;j : In this form, U can be viewed as a sum of the element-wise product of the two matrices,K 0 andS 0 , which are obtained by assigning 0 to the diagonal elements of matricesK andS.\nTo allow for covariates adjustment, we can perform two sided projection on the zero-diagonal centered similarity matrices,K 0 and S 0 . Suppose that there are P covariates that need to be adjusted. Let X \u00bc fx i;p g n\u00c2P represents the covariate matrix, we can calculate the covariate centered similarity matrices by (Supplementary Appendix S6), b\nS \u00bc \u00f0I \u00c0 X\u00f0X T X\u00de \u00c01 X T \u00deS 0 \u00f0I \u00c0 X\u00f0X T X\u00de \u00c01 X T \u00de; and b K \u00bc \u00f0I \u00c0 X \u00f0X T X\u00de \u00c01 X T \u00deK 0 \u00f0I \u00c0 X\u00f0X T X\u00de \u00c01 X T \u00de: The covariate adjusted GSU can be expressed as\nWe include the diagonal terms in the covariate-adjusted similarities because they also contain the similarity information after the adjustment. In fact, the covariate-adjusted GSU is a weighted V statistic, and its asymptotic distribution can be attained similarly as weighted U statistic. We use matrix eigen-decomposition to approximate the eigen-values in function decomposition. Let f b k s g and fb g t g be the eigen-values for matrices b K and b S, respectively, the limiting distribution of U is given by (Supplementary Appendix S7) ,\nwhere fv 2 st g are independent v 2 random variables with 1 degree of freedom. The P-value can be calculated by using the Davies' method (Davies, 1980) , the Liu's method (Liu et al., 2009) or the Kuonen's method (Kuonen, 1999) . To facilitate the high-dimensional data analysis, we developed a C \u00fe\u00fe package based on GSU (https:// github.com/changshuaiwei/gsu)."}, {"section_title": "Simulation study", "text": ""}, {"section_title": "Simulation method", "text": "To mimic real genetic structure, we used genetic data from the 1000 Genome Project (Abecasis et al., 2010) . Based on the genetic data, we then simulated phenotype values. In particular, we used a 1 Mb region of the genome (Chromosome 17: 7344328-8344327) from the 1000 Genome Project. For each simulation replicate, we randomly chose a 30-kb segment from the 1 Mb region and formed a SNV-set for the analysis, in which only rare variants (i.e. MAF <0.05) are used except otherwise specified. From the SNV-set, we set a proportion of the SNVs as causal. A number of individuals were randomly chosen from the total 1092 individuals as the simulation sample to study the performance of the methods. We set sample size n \u00bc 50 by default.\nTo investigate the robustness against different phenotype distributions, we simulated four types of phenotypes:"}, {"section_title": "A binary-distributed phenotype (denoted as B), by logit\u00f0P\u00f0Y", "text": "Here, Y i and G i were the phenotype value and the genotype vector (coded as 0, 1 and 2) for the ith individual, respectively. We set l i \u00bc 0 except otherwise specified. b were the effects of the SNVs, which were sampled from a uniform distribution with a mean of l b and a variance of r 2 b . Three sets of simulations were performed. In Simulation I, we considered a single phenotype; in Simulation II, we considered multivariate phenotype; in Simulation III, we considered multivariate phenotype under the influence of confounding effects. Details of simulation settings are in Supplementary Appendix S8.\nWe evaluated the performance of GSU by comparing it with variance component score (VCscore) test under univariate or multivariate linear mixed model (Wu et al., 2011; Maity et al., 2012) . For each simulation, we created 1000 simulation replicates to evaluate Type I error and power. Type I error rates and powers are calculated using percentage of P-values smaller than a given threshold (e.g. 0.05) under null models and alternative models, respectively."}, {"section_title": "Result for Simulation I", "text": "The Type I error rates and powers are summarized in Table 1 . GSU had a well-controlled Type I error (around 0.05) for all four phenotypes, while VCscore had an inflated Type I error rates (0.113) for Cauchy-distributed phenotype and over-conservative Type I error rates (0.005) for Binary-distributed phenotype.\nFor the disease model where half of the causal SNVs were deleterious (Table 1) , GSU had slightly lower power than VCscore for Gaussian-distributed (0.258 versus 0.345) and Poisson-distributed phenotype (0.506 versus 0.651), but had significantly higher power than VCscore for Cauchy-distributed (0.503 versus 0.21) and Binary-distributed phenotype (0.402 versus 0.083). The same comparison was observed for the second disease model in which a majority of the SNVs were deleterious.\nWe performed additional simulations by including both common and rare variants ( Supplementary Table S4 ). Under this setting, the power of VCscore increased significantly for Binary phenotype (0.764), though still lower than that of GSU (0.807). GSU attained higher power than VCscore for Poisson (0.813 versus 0.795) and Cauchy (0.885 versus 0.573) phenotype. Nevertheless, GSU was still less powerful than VCscore for Gaussian phenotype (0.853 versus 0.878)."}, {"section_title": "Result for Simulation II", "text": "The Type I error rates and powers for the multivariate analysis are summarized in Table 2 . Similar to the results of the univariate analysis, GSU can correctly control Type I error at the level of 0.05 (Table 2) , while VCscore had inflated Type I error when the phenotype contained variables with heavy tailed distribution (e.g. CGG and BCG). GSU attained higher power than VCscore for BBG, CGG and BCG phenotypes, and similar power as VCscore for BPP phenotype.\nWe examined the Type I error rates at more stringent significance levels (Table 3) by simulating 1 million replicates. In general, GSU can control the Type I error better than VCscore. For example, at 5 \u00c2 10 \u00c03 and for BPP phenotype, GSU had Type I error near 5 \u00c210 \u00c03 (i.e. 7:4 \u00c2 10 \u00c03 ), while, VCscore had Type I error much higher than 5 \u00c2 10 \u00c03 (i.e. 1:1 \u00c2 10 \u00c02 ). While simulation demonstrated robustness of GSU over VCscore on controlling Type I error, we observe GSU has slightly inflated Type I errors at 5 \u00c2 10 \u00c03 level. We suspect this is because of the small sample size. We therefore conducted another set of simulation with sample size of 200, and the results showed Type I errors of GSU are better controlled for stringent significant levels under larger sample size ( Supplementary  Table S5 ).\nTo separate influences of different distributions, we also compared GSU and VCscore when phenotype have the same distributions (i.e. BBB, CCC, GGG, PPP). The results ( Supplementary Table  S6 ) are similar to those for univariate phenotype. In general, GSU can control Type I errors better than VCscore. GSU had slightly lower power than VCscore for GGG phenotype (0.882 versus 0.958) and PPP phenotype (0.862 versus 0.966), but attained significantly higher power for BBB phenotype (0.862 versus 0.26) and CCC phenotype (0.724 versus 0.284). We further increased the dimension of phenotype to 10 for each type, and the comparisons showed that GSU have better control of Type I error and attain higher power for most cases ( Supplementary Fig. S1 )."}, {"section_title": "Result for Simulation III", "text": "We summarized the Type I errors in Table 4 . Without covariates adjustment, both methods had inflated Type I errors. With covariates adjustment, GSU showed robustness against confounding effects for all 4 multivariate phenotypes, with Type I errors ranging from 0.056 to 0.061. VCscore can control Type I error for BBG phenotype (0.054), but had inflated Type I errors, ranging from 0.174 to 0.322, for the other 3 multivariate phenotypes.\nIn Figure 1 , we generated the power curves by plotting the powers of the two methods against different sample sizes (50-200). GSU has higher power than VCscore for different sample sizes and multivariate phenotypes, except for BPP phenotype. The 'higher power' of VCscore for BPP phenotype is due to the fact that VCscore has inflated Type I error (i.e. 0.322, as shown in Table 4 )."}, {"section_title": "Real data application", "text": "We analyzed the WGS data from Alzherimer's disease neuroimaging initiative (ADNI) using the GSU C \u00fe\u00fe package. ADNI is a largescale longitudinal study that collects and utilize various predictors of Alzherimer's disease (AD), including 3D brain imaging, cognitive measurements and genetic data. The sample with WGS data contains 808 individuals, with 280 normal controls (NC), 234 early mild cognitive impaired (EMCI) patients, 246 late cognitive impaired (LMCI) patients, and 48 AD patients at study baseline.\nWGS was performed on autosomal chromosomes for each subject. To form SNV-set, we group the genetic variants based on the gene range list from GRch37 assembly, where we only used the nonoverlapping genes. For genetic variants outside of gene ranges, we group them by evenly spacing the remaining genome with windows of 50 kb. After completing quality control (e.g. delete variants with high-missing rate) and grouping process, about 21 million genetic variants remained for analysis, forming 61 683 SNV-sets.\nWe were interested in testing the association of the SNV-sets with brain imaging summary matrices considered important to cognitive impairment. In particular, we used six variables: 18F-fluoro-2-deoxyglucose, Hippocampus, Entorhinal, 8F-florbetapir (AV45), Fusiform and Ventricles measurements at base-line, as multivariate phenotype. The phenotype similarity is calculated using weighted LK, S LK i;j \u00bc exp\u00f0\u00c0 P L l\u00bc1 x l jy i;l \u00c0 y j;l j\u00de. We 'fished' the weight x l from the case-control status. In particular, we regressed the case-control status on the scaled multivariate phenotype and obtained regression coefficient b l for lth variable, where we assigned x l \u00bc jb l j ( Supplementary Table S7 ). In order to adjust the potential confounding effects, we included age, gender, race and top 20 genome principle components as covariates in the analysis. Two sets of whole genome association analysis were performed. For the first scan, we include both common and rare variants, while for the second scan we only include rare variants. The Quantile-Quantile plots ( Supplementary Figs S2 and S3 ) showed no systematical bias after adjusting covariates. We listed the top 5 SNV-sets for each scan in Table 5 . When both common and rare variants were considered, 4 SNV-sets (i.e. APOE, Ch19-45389309-45439308, APOC1, TOMM40) pasted the Bonferroni threshold, among which the genes APOE and TOMM40 has been reported in previous studies. As a comparison, we also performed the analysis using VCscore ( Supplementary Table S10 ). VCscore attained similar results for the top association findings, though with less significant P-values (e.g. P-value \u00bc1:98 \u00c2 10 \u00c026 for APOE).\nWhen only rare variants are considered, no SNV-set past the Bonferroni threshold. Interestingly, the gene APOC1 was listed as one of the top 5 associated genes from both analyses. Further investigation will be needed to study its role in AD. More detailed results are in Supplementary Tables S8 and S9 . We further calculated the P-value of the top SNV-sets using AD case-control status instead of multivariate phenotype with 6 intermediate measurements. The univariate analysis attained less significant result ( Supplementary Table  S11 ). For example, the P-value of APOE is 3:44 \u00c2 10 \u00c08 from analysis using AD case-control status, less significant than 2:77 \u00c2 10 \u00c048 from analysis using brain imaging matrices."}, {"section_title": "Discussion", "text": "Many genetic studies collect multiple secondary phenotypes, or use intermediate biomarkers, to study complex diseases. By considering multiple phenotypes that measure the different aspects of underlying diseases, the power of the association analysis can potentially be improved (Zhang et al., 2010; Maity et al., 2012) . Several methods were recently developed to detect the joint effect of genetic variants on multivariate phenotype (Tao et al., 2015; Wang et al., 2015) . Most were built on parametric framework that poses certain assumptions on phenotype distribution. In this paper, we proposed a non-parametric test, GSU, based on similarity measurement. Simulation study showed that our methods can can control Type I error for multiple different phenotypes and moderate level of confounding effects. In most cases, GSU also attained higher power than the parametric method. Although the simulation results depend on the simulation settings, and should always be interpreted in the context of the simulation setting, we believe the results reflect the advantage of GSU in a broader sense, because (i) the genetic data used in the simulation comes from the 1000 Genome Project, which reflects the LD pattern and the allele frequency distribution in the general population; and (ii) we simulated a wide range of disease models, including univariate phenotype and multivariate phenotype with different distributions, to mimic real disease scenarios.\nThe test statistics in VCscore is a quadratic form,\nY is the standardized residual under null, and K is the genetic similarity matrix. If we rewrite T as T \u00bc\nIn this respect, VCscore can be considered as a special case of GSU. Nonetheless, there are several key differences: (i) GSU allows general forms of similarity and thus can be used for association analysis of elements in general metric space; (ii) for multivariate association analysis, GSU with LK-based similarity has the ability to detect any types of association (strongly positive definite similarity) and its asymptotic test is robust against distribution assumptions (bounded similarity); (iii) for covariates adjustment, GSU used a centralized similarityS 0 and then perform two sided projection, i.e. b S \u00bc \u00f0I \u00c0 X\u00f0X T X\u00de \u00c01 X T \u00deS 0 \u00f0I \u00c0 X\u00f0X T X\u00de \u00c01 X T \u00de, while, VCscore performed two sided projection on original similarity (S \u00bc YY T ), i.e. b S \u00bc \u00f0I \u00c0 X\u00f0X T X\u00de \u00c01 X T \u00deS\u00f0I \u00c0 X\u00f0X T X\u00de \u00c01 X T \u00de; (iv) Asymptotic distribution of GSU is in the form of P 1 t\u00bc1 g t P 1 s\u00bc1 k s v 2 st , where distribution of VCscore is in the form of P s k s v 2 s ; (v) for multivariate phenotype with L variables, the dimension for similarity matrix is n \u00c2 n in GSU and nL \u00c2 nL in VCscore.\nIn simulation studies, we observed higher power of GSU over VCscore. This is mainly due to the fact that GSU is equipped with strongly positive definite kernel which can detect any type of association while the cross product kernel in VCscore does not have this property. We performed another set of simulations by generating dependence structure via rotation operator (Supplementary Appendix S9). In particular, we first generate two i.i.d. multimodal continuously distributed variables and then rotate the vector with angle h 2 \u00f00; p=4\u00de ( Supplementary Fig. S4 ). The data generated thus does not have first-order dependence structure (correlation) nor second order dependence structure. The result ( Supplementary Fig. S5 ) showed that GSU (with LK-based similarity) had power of 1 for large enough sample size, while VCscore (with cross-product kernel) cannot detect any association regardless of different sample sizes. Though the 'toy' simulation may not represent common scenarios in genetic association studies, it empirically explains the reason why GSU attained higher power than VCscore. To further investigate the influence of different kernels, we performed simulations using five different kernels for GSU, including three strongly positive definite kernels. The result shows that GSU with strongly positive definite kernels have higher powers for the most of the time, among which GSU with LK kernel have highest power ( Supplementary Fig. S6 ). In general, we recommend to use LK kernel for GSU. Nevertheless, its performance may not guaranteed to be optimal. In this case, we can perform kernel selection, for example, by using the procedure proposed by Wu et al. (2013) . Besides the choice of kernel, different choices of weights can also influence the power of GSU for multivariate phenotype. In principle, we should use weights that represent their relative importance with respect to the underlying 'true phenotype'. For example, in real data analysis, we obtained the weights based on their contributions to the AD disease status. Here in this paper, we only considered the joint effect of SNV-sets. If gene environment interaction effects are to be considered, we can calculate Table 5 . Top association findings from the multivariate analysis of ADNI whole genome sequencing data"}, {"section_title": "SNV-set", "text": "Chr Size P-value a composite similarity using both the genetic information and environmental information (Tong et al., 2016; Wei et al., 2016) , and then construct GSU with the composite similarity and the phenotype similarity.\nThe asymptotic test for GSU (with LK-based similarity) is shown to be robust to distribution assumption. This is because the LK is bounded between 0 and 1, and the resulting similarities h\u00f0\u00c1; \u00c1\u00de and f \u00f0\u00c1; \u00c1\u00de thus satisfy the regularity condition of asymptotic test, i.e. E\u00f0h\u00f0Y; Y\u00de\u00de < 1 and E\u00f0f \u00f0G; G\u00de\u00de < 1. However, cross-product kernel does not have this property. As a result, we observed that in simulation studies GSU had more robust Type I errors than VCscore. Nevertheless, we still observed slightly inflated Type I error with stringent significant level (e.g. 5 \u00c2 10 \u00c03 ) when n \u00bc 50. This is because the asymptotic null distribution cannot approximate the actually null distribution well when sample size is small compared with when sample size is large ( Supplementary Fig. S7 ). One way to improve the robustness for small sample size is to take an rank transformation for each variable [i.e. r i;j \u00bc \u00f0rank\u00f0y i;j \u00de \u00c0 0:5\u00de=n] before calculating the similarity. We performed additional simulation for GSU with rank transformation for n \u00bc 50 using same setting as Simulation II. The results showed that GSU with rank transformation (GSU-rk) can control Type I error well even with more stringent significant level for small sample size ( Supplementary Table  S12 ). Nevertheless, rank transformation can cause loss of information, which might lead to lower power.\nIn simulations, we observe that VCscore, although designed for Gaussian distributed phenotype, appears to be able to control Type I error appropriately and attain slightly higher power for Poisson phenotype. This may be due to that Poisson distribution can be reasonably approximated by Gaussian distribution when its mean is moderate to large. We performed additional simulation using heavily right skewed Poisson distribution, and the results showed VCscore had lower power for one simulation and inflated Type I errors for another simulation ( Supplementary Table S13 ). We can use rank transformation to improve the robustness of VCscore (Wei et al., 2016) . We performed additional simulation to compare GSUrk to VCscore test with rank transformation (VCscore-rk). The result ( Supplementary Table S14 ) showed that VCscore-rk can control Type I errors under various setting. However, VCscore-rk still had lower powers than GSU-rk for most cases.\nFor the analysis of multivariate phenotype, the difference on the dimension of similarity matrix for GSU and VCscore influenced the computation efficiency especially when the number of variables in multivariate phenotype increases. The key reason is the cost of the eigen decomposition. For analysis of L-variable multivariate phenotype in a sample of size n, GSU needs to decompose a n \u00c2 n matrix, while VCscore needs to decompose a Ln \u00c2 Ln matrix. The time used for matrix decomposition are O\u00f0n 3 \u00de for GSU and O\u00f0L 3 n 3 \u00de for VCscore. For example, in real data application when L \u00bc 6, the average time to analyze one SNV set is 36.75 s for VCscore and 1.3 s for GSU. For high-dimensional setting (e.g. L ) n), VCscore is computationally infeasible. An additional simulation shows that GSU is well behaved when the dimension of phenotype increase to 100 ( Supplementary Fig. S8 ). Nevertheless, noises in high-dimensional phenotype or genotype may reduce the power of GSU. In this case, dimension reduction techniques, such as variable selection and principle component analysis, can be used to increase power.\nThe covariate adjustment proposed in the paper is a heuristic approach for adjusting confounding effect. Accurate adjustment of confounding effects requires additional assumptions on the distributions and the functional forms between responses and covariates. In the paper, we showed GSU works well when the confounding effects are moderate. Nonetheless, the heuristic covariate adjustment in GSU should always be used with caution. If there is a strong confounding effect, the heuristic approach might not control Type I error very well. For this paper, covariate adjustment is not the primary focus, and the issue will be investigated in future studies.\nBesides confounding effects, the correlation among variables in multivariate phenotype may also influence the performance of association testing. This is particularly important for regression based methods, since it handles multivariate phenotype by stretching the phenotype matrix to a long phenotype vector. Without considering correlations among variables in phenotype, the test will lead to inflated Type I error. Nevertheless, GSU do not have this issue, since its similarity matrix is calculated on subject level and its inference only assume independence between subjects. We performed additional simulations by introducing additional correlation in the multivariate phenotype ( Supplementary  Table S15 ). The results showed that, in general, GSU can control Type I error and attain higher power than VCscore ( Supplementary Fig. S9 ).\nIn recent years, U-statistic-based methods became popular in genetic data analysis, and have shown their robustness and flexibility for analyzing genetic data (Li et al., 2011; Schaid et al., 2005; Wei and Lu, 2015; Wei et al., 2016) . GSU is a general framework of association analysis and is based on similarity measurements and U statistics. In this paper, we have focused on the association analysis between multivariate phenotype and categorical sequencing data (i.e. SNV data). GSU can easily be applied to analyze other types of genetic data, such as count data (CNV data) and continuous data (expression data) with unified LK-based similarity (Section 3.2). With appropriate similarity measurement (Section 2.1), GSU can also be used for association testing of modern data types, such as imaging, curves and trees. van der Vaart and Wellner, 2000) . Therefore, the empirical process, 1 ffiffi n p P n i\u00bc1 g ? t \u00f0G i \u00de/ ? s \u00f0Y i \u00de, converges weakly to the Gaussian process Z s;t with mean zero and covariance function, cov\u00f0Z s;t ; Z s 0 ;t 0 \u00de \u00bc E\u00f0g ?\nt \u00f0G 1 \u00de/ ? s \u00f0Y 1 \u00deg ? t0 \u00f0G 1 \u00de/ ? s0 \u00f0Y 1 \u00de\u00de. With this uniform convergence (for all s > 1 and t > 1), we can show that,\nwhere v 2 st are i.i.d v 2 random variables with a df of 1."}]