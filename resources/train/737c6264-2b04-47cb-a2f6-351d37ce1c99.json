[{"section_title": "Introduction", "text": "Over the last 30 years, structural and functional brain imaging has become a powerful and widespread tool for clinical and basic neuroscience. However, it is only now that some brain imaging studies have started to join the ranks of ''big data'' science. Whereas most neuroimaging studies continue to have modest sample sizes (number of subjects, N < 50) and modest amounts of data collected per subject, a small number of studies are now starting to embrace ''big'' imaging in a number of ways-with subject numbers in tens of thousands, or taking advantage of huge increases in the quantity of imaging and non-imaging data collected on each subject. For example, the Human Connectome Project (HCP) (Van Essen et al., 2013) has recently completed imaging of >1,000 young adults, with an impressive 4 hr of scanning per subject, and utilizing vast improvements in the spatial and temporal resolutions of the acquired data. In a complementary manner, UK Biobank (UKB) (Miller et al., 2016) is acquiring more modest amounts of imaging data per subject but is scanning 100,000 volunteers, and this imaging is just part of the much larger overall UKB project that includes genetics, biological and lifestyle measures, and health outcomes from 500,000 subjects. In yet another approach to big data imaging, the ENIGMA consortium is amassing imaging and genetic data from tens of thousands of subjects by pooling many existing studies, using advanced metaanalysis techniques to overcome restrictions on sharing individual-subject data (Bearden and Thompson 2017) .\nIn this brief NeuroView, we highlight some of the challenges that ''big data neuroimaging'' brings. While there is no hard definition, we may consider an imaging study ''big'' if it has 1,000 or more subjects and/or collects a substantially larger set of measurements than usual or expands on traditional measurements (e.g., with greater resolution or duration than typical). We focus on MRI-based brain imaging, though many of the points made are also relevant for valuable complementary techniques such as EEG, MEG, and PET. All aspects of neuroimaging data are rapidly becoming ''bigger.'' Spatial resolution and number of images are constantly increasing, enabled by enhancements in MRI hardware (higher field strength and more coil channels) and software (advanced acquisition and reconstruction techniques). Additionally, the proliferation of research scanners means that study designs are becoming more ambitious, with longer scan durations, multiple scan sessions, and large subject numbers. For example, if one considers the increases in state-of-the-art fMRI imaging spatial resolution, temporal resolution, and receive-coil numbers, data size has increased quite in line with Moore's law (a doubling every 2 years, i.e., a 1,000-fold increase in the last 20 years).\nAt the heart of an imaging study lies the image processing and statistical modeling, a major goal of which is to work against the ''big''-to reduce the raw data to meaningful and concise information that allows the original experimental question to be answered. For example, in UKB, in addition to requiring image processing to remove artifacts and align images across modalities and individuals, imaging-derived phenotypes (IDPs) are computed-currently 2,500 distinct measures of brain structure and function. Examples of IDPs include the volume of specific brain structures, the strength of connectivity between pairs of brain regions, and the dispersion of fibers in specific white-matter tracts. As a result, the 20 PB of raw neuroimaging data from 100,000 subjects will be converted into 300 TB of pre-processed image data and finally into 1 GB of IDPs (i.e., a total compression ratio of 20 million). However, it is also possible for image analysis to greatly increase data size. For example, HCP resting-state functional MRI (rfMRI) time series data are under 2 GB per subject, but once this is processed to generate the voxels 3 voxels matrix of ''dense'' brain connectivity, this expands 20-fold to 33 GB.\nThese numbers immediately illustrate perhaps the most obvious difficulty with big data neuroimaging -data sizes can quickly become hard to manage, both in terms of long-term storage and compute speed and memory. However this is just one of the challenges.\nBig Sample Sizes: Small Effect Sizes A primary reason that prospective epidemiological health studies (such as UKB) need very large subject numbers is to be able to acquire data on subjects in advance of later disease development, hence allowing researchers to learn the earliest markers of (and possibly mechanisms for) disease; the large subject numbers are needed as there is no single disease focus, and only later can patient-control subsets be identified. However, another reason for acquiring data from large numbers of subjects is to allow the identification of subtle effects that are not statistically detectable in smaller groups. This has been the scenario in most genome-wide association studies, where individual genetic effects tend to be very small. The associated downside is of course exactly the same point-that statistically significant effects could be very small in terms of the biological effect (percentage variance explained in the data, Figures 1 and 2 ). For example, in an analysis of almost 15,000 UKB subjects, even after Bonferroni correction for 14 million association tests between IDPs and non-imaging variables (such as cigarette consumption or cognitive test scores), statistical significance can be reached with much less than 1% variance in the nonimaging variables explained by the IDP (Figure 3) .\nHence, in the worst case scenario, having huge N renders ''significant'' individual associations meaningless. However, where many variables are considered simultaneously, reasonable total percentage of variance prediction may be found. For example, multivariate modeling from the whole genome can account for almost 20% of variance in hippocampal volume, whereas individual genetic variants only explain up to 1% (Bearden and Thompson, 2017) . Similarly, analysis of the first 5,000 UKB subjects found that population variance explained in pairwise associations (between IDPs and non-imaging variables) reached maxima of around 5%, while multivariate analyses reached up to 50% variance explained (Miller et al., 2016) .\nBig Sample Sizes: Big Confound Trouble A more insidious problem associated with huge N-again closely related to the upside of having high sensitivity to real effects-is high sensitivity to artifactual associations due to confounding effects. Just tiny amounts of shared confounding factors that feed into two (otherwise independent) variables of interest can induce false associations; even when a real association exists, confounds can bias the estimate of the correlation. For example, socioeconomic status might influence cheese consumption and (independently) rate of cognitive decline in aging as reflected in the volume of the hippocampus; this would induce an apparent association between cheese and hippocampus volume that is not causal but induced by a common factor.\nConfound effects are particularly problematic in big imaging studies because of the huge variety of potential imaging artifacts, including many factors that can affect both the imaging and non-imaging variables of interest. Just some of the confounding effects are: head motion, head size, changes in breathing rate and depth, and scanner hardware and software changes that are much more likely to occur in big, longer-term imaging studies (Table 1; Figure 2 ). It is essential therefore to develop sophisticated image processing methods for removing the effect of such confounds from imaging data; two very effective examples are structured noise removal from rfMRI (Salimi-Khorshidi et al., 2014) and diffusion MRI (dMRI) data .\nThere can also be interpretability problems caused by changes in the brain. For example, structural brain changes can easily cause misinterpretation of results in other modalities through partial volume effects. This happens where voxels contain a mixture of tissue types, and atrophy (or other structural variation over time or between subjects) changes the mixture of tissues within voxels. Because MRI voxels are on the order of millimeters while the cortex is only a few millimeters thick, this is a common occurrence, in particular at the gray-white boundary. This leads to structural changes being incorrectly attributed to a change in functional activity in gray matter in fMRI, or white matter microstructure in dMRI.\nThere are also factors that both induce imaging confounds and also relate to the effect of interest, making their removal from the data even more complex. For example, blood pressure can be a health factor of direct interest but causes confounding effects in resting-fMRI functional connectivity estimates. In addition, blood pressure can lead to differences in dMRI measures in white matter that are driven by vascular changes rather than alterations to the fiber architecture of interest. Similarly, aging effects in the brain may be of interest, but age-related structural atrophy may also act as an imaging confound (for example, changing partial-volume effects); additionally, in an aging population, age may be a dominant source of intersubject variability and hence could be viewed as an important mediator in any imaging versus non-imaging association, or could be seen as a significant source of unwanted variance.\nAnother concern is when a confounding effect is identified but can only be The plot shows the sample size N needed to attain 80% power to detect 1 true association while controlling the familywise error rate (the chance of one or more false positives) over K tests. Effect size is measured in terms of percentage of variance explained (r 2 ), and is shown for three small values, 1% (corresponding to a correlation of r = 0.1), 0.1%, and 0.01%. While large sample sizes are needed for just 1 test, as N increases K grows exponentially. Roughly, squaring the number of tests requires only a doubling of the sample size. measured/estimated with error; in this case the adjustment is incomplete and increasing N just increases sensitivity to the un-adjusted variance that ''leaks'' through; addressing this requires measures of confound uncertainty and a more complex modeling approach that propagates this uncertainty through the modeling process (Westfall and Yarkoni, 2016) . Another potentially useful approach is to generate an expanded set of confounds by adding transformed versions of those available (e.g., nonlinear transformations that suppress or exaggerate outliers). Adjusting the data with such an expanded set will allow more than just linear effects of confounds to be captured.\nAnother method for dealing with confounds is to sub-select subjects, i.e., identify a matched control for each patient or exposed individual (e.g., match for sex and age rather than by regressing those confounds out of the data). Doing this exactly is often impossible, but a soft version can be accomplished with propensity score matching (Rosenbaum and Rubin, 1983) . However, such matching leaves out data, and so it might seem better to just ''adjust for everything,'' that is, include all possible confounds. However, if a variable being considered as a confound is actually a consequence of two otherwise independent variables, adjustment for this confound artifactually induces an association, in an effect known as Berkson's paradox or ''collider bias.'' For the same mathematical reasons, false associations can also arise from selection (or ascertainment) bias, relating to the recruitment of subjects into a study. For example, it has been noted that individuals in UKB smoke less and hold more educational qualifications than the UK population as a whole; if it were the case that these two features actually causally influenced whether subjects participated in the UKB, a bias could be induced in any association measured between smoking and education (Fry et al., 2017; Munaf\u00f2 et al., 2017) . Similarly, large meta-analyses can also be biased by the ''file drawer problem,'' where the literature is biased The % signal change color overlay shows the fMRI signal change associated with the faces-shapes contrast, masked by significant voxels from mixed-effects modeling of the group-average signal (all maps shown here conservatively corrected for multiple comparisons across 1.8 million voxels using Bonferroni correction, p < 0.05). This threshold excludes only 19% of voxels, i.e., showing a significant response to the task in most of the brain. The maximum statistical effect size (Cohen's d) is 1.57, equivalent to a one-group T statistic of 176.2. The Sex overlay shows significant correlation of the faces-shapes effect with the confound factor of sex; orange/blue coloring shows correlation estimated after controlling for head size, while copper/green coloring is without this adjustment. The Head size overlay shows significant correlation with volumetric head size; orange/blue coloring shows correlation estimated after controlling for sex, and copper/green coloring is without adjustment. Because sex and head size are highly correlated (r = 0.63), adjustment makes a great difference, eliminating a significant effect in some regions. Over all five results shown here, the minimal detectable correlation was r = 0.049. (Image intensities truncated for presentation, percentage of signal change truncated at \u00b10.5%, correlation intensities at \u00b10.08; full ranges listed in the figure.) away from the null due to non-publication of null findings."}, {"section_title": "Multivariate Modeling and Machine Learning: Finding Population Patterns, Making Predictions", "text": "With big imaging datasets, the potential loss of statistical sensitivity associated with huge numbers of tests (across space, imaging modalities, and thousands of non-imaging variables) is not the only challenge. With millions of tests comes also the challenge of interpretation of potentially thousands of significant individual results. Interpretation can be easier with multivariate analyses, where many variables are considered simultaneously; such analyses can jointly model imaging variables/voxels against a nonimaging variable (healthcare outcomes, behavior, life factors). Alternatively, in doubly-multivariate analyses, many imaging and non-imaging variables can be jointly modeled. These methods produce a small number of dominant patterns of association that can be much easier to interpret compared to millions of univariate association tests.\nMachine learning uses multivariate methods to make predictions about individual subjects and discover patterns in data. All machine learning methods must balance complexity and generalizability-the more flexible the model, the greater the chance of overfitting the data. Feature selection is one approach to managing complexity; for example, penalized regression methods (such as LASSO or Elastic Net) automatically drop variables from the model, leaving just the most important ones. However, for imaging data, where many thousands of voxels/variables feed into sparse regression models, performance can be further improved with additional pre-selection, feeding into the regression only the most promising variables (Le Floch et al., 2012) . ''Univariate filtering'' with a target variable is one such supervised feature selection method. For example, when building a predictive model of IQ from functional connectivity, univariate filtering consists of individually correlating IQ with each connectivity ''edge,'' and only those edges with some evidence for an association are retained for building a multivariate predictive model.\nAnother approach to managing complexity is feature extraction, where variables are combined in some way to create a new, more informative variable. PCA (principal component analysis) is a common feature extraction method that is unsupervised (does not use knowledge of the target variable). However, for such unsupervised data-driven feature In the Manhattan-style plot, 5,456 non-imaging variable are arranged on the x axis, with 16 groups of variable types. For each variable, 7 -log 10 p values are plotted-the most significant association of that variable with each of 7 different classes of imaging-derived phenotypes (IDPs). Approximately 100,000 associations are FDR significant, and 15,000 are Bonferroni significant. The histograms show the distributions of correlation size (across all 14 million tests); depending on thresholding method, the minimum detectable r is 0.03-0.05, meaning that for FDR thresholding an association with 0.1% variance explained is detectable. reduction, it can be useful to have already reduced the data in sensible ways; PCA on millions of voxels uses no prior knowledge, and may be less successful than dimension reduction based on PCA of thousands of IDPs that were explicitly designed to each reflect a meaningful quantity. For example, when looking for imaging versus non-imaging associations in the HCP, Smith et al. (2015) used PCA to reduce 20,000 IDPs to 100 variables. Big imaging studies naturally start with many more measurements (potentially billions of voxels/time points across multiple modalities) than typical non-imaging studies, hence the particular importance of such data reduction. PCA is an example of a linear feature extraction method, which has the advantage that each feature can easily be mapped back to the original variables; this is particularly important for big imaging studies where imaging confounds/artifacts can easily dominate the results (e.g., head motion in case-control studies). Nonlinear or highly reductive feature extraction methods (e.g., graph theory metrics) can be harder to interpret, including the question of whether results are confound driven or not.\nWhile multivariate methods typically relate multiple predictors to a single target variable, ''doubly-multivariate'' methods relate multiple variables from multiple sources. For example, Miller et al. (2016) applied canonical correlation analysis to relate thousands of IDPs to thousands of non-imaging life-factor and biological measures. Such an analysis finds common patterns of population variance that exist in two datasets, which can be fundamentally different in character (e.g., imaging data and health outcomes). However, such data-driven analysis may not necessarily result in biologically interpretable modes (patterns of population covariance). Here, independent components analysis (ICA) can be a powerful tool to transform the modes (e.g., from PCA or CCA) to make them more biophysically meaningful. For example, in Miller et al. (2016) , 9 significant CCA modes were ''unmixed'' into 9 ICA modes with enhanced reproducibility and biological interpretability, for example, relating measures of blood pressure, alcohol intake, and changes in white matter cell microstructure to each other, with up to 20% variance explained, in 5,000 subjects.\nEvery multivariate or predictive model is in danger of being affected by overfitting, for example, leading to over-optimistic estimates of prediction accuracy. To unambiguously demonstrate that the predictive performance is above chance, a careful null analysis is required, such as can be achieved through permutation testing. Unbiased estimates of variance explained or predictive accuracy must be assessed with rigorous cross-validation based on held-out data and, ideally, replication with another large and distinct dataset."}, {"section_title": "Significance Testing: Multiple Comparisons, Fishing Trips", "text": "Since there is currently much buzz about the potential of machine learning methods, it is reasonable to consider what role existing neuroimaging statistical tools will play with big data. Currently, the workhorse tools of neuroimaging are ''mass univariate,'' where a regression model is fit separately at each voxel. Inferences over the image are corrected for multiple testing using random field theory (RFT) or nonparametric resampling methods (e.g., permutation or bootstrap). We believe that these tools will continue to be relevant to provide stringent control of false positives, particularly for highly focused hypotheses. RFT, an approximate analytical method, is in its element with large N; permutation methods are extremely computationally intensive with large N but can be practical with permutation distribution approximation methods (Winkler et al., 2016a) .\nA serious problem is when researchers fly through a growing collection of imaging modalities, preprocessing options, and alternative models, searching for significance. Unless using stringent multiple testing or validation with held out data, such explorations quickly become ''p-hacking.'' Big open imaging datasets are explicitly designed to allow for a wide range of hypotheses to be tested, and so raise this danger further. It is vital that researchers arm themselves with detailed analysis plans that address multiplicityideally included as part of study plan preregistration-otherwise little confidence can be placed in the results found after such fishing expeditions. In particular, nonparametric (e.g., permutation-based) combining can be essential to account for controlling a search for effects over several modalities (Winkler et al., 2016b) . Large studies may include genetically related subjects accidently or by design or may use a complex sampling structure. Whether using mass univariate or multivariate methods, it is essential to control for this structure, for example with hierarchical models or constrained permutation approaches . For example, the HCP includes many twins and siblings; ignoring twin structure in HCP will lead to inflated significance on heritable brain phenotypes. Similarly the ABCD study (Volkow et al., 2017) uses a stratified design, sampling schools around the country, and children within each school, which must be accounted for when analyzing its data.\nAs large, open datasets become widely used, new complexities arise. Consider the Alzheimer's Disease Neuroimaging Initiative (ADNI), first started in 2004, which has produced a repository of multimodal MRI for predicting the course of Alzheimer's disease (AD) (Mueller et al., 2005) . ADNI has been very successful, and, for example, now nearly every publication on constructing imaging biomarkers to predict AD conversion (from mild cognitive impairment) is based on ADNI. However, as more and more researchers base findings on the very same ADNI data, generalizability becomes a concern: will the 100 th ADNI paper on AD conversion reflect over-fitting to idiosyncrasies of this sample? Part of the solution is the continued creation of new big imaging resources, so that each new method and scientific finding can be validated on alternate and diverse populations."}, {"section_title": "Conclusions", "text": "Imaging can be a powerful way to identify phenotypes that are more ''intermediate'' than the measures of ultimate interest (such as health outcomes or cognitive scores), but which can be more quantitative and sensitive (Duff et al., 2015) . Combining this strength of imaging with advanced analysis methods in largescale imaging studies will allow us to find population patterns that more closely map to underlying disease mechanisms and cognitive behavior, for example as proposed in the Research Domain Criteria (RDoC) paradigm in psychiatry (Insel et al., 2010) . To harness these opportunities, researchers will need to equip themselves with the tools of computer scientists and epidemiologists, learning to scale up existing tools and develop new ones that grapple with the confounds that will be as potent as the subtle effects of interest now accessible with these enormous datasets."}]