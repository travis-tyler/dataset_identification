[{"section_title": "", "text": "The spread of infectious diseases such as the novel coronavirus disease 2019 epidemic caused by SARS-CoV-2 is a major challenge for modern societies. Reported case number trajectories reflect in principle the epidemic dynamics, and are thus used within the scientific community to infer its evolution (1, 2) , but are also publicly reported and debated (3) (4) (5) , along with the effectiveness of governmental interventions in lowering them. However, the number of reported cases may be influenced by multiple factors, notably depending on the public policy regarding testing, such that its relationship to epidemic dynamics needs to be clarified. To investigate this question, we consider a SIR model of the epidemic, and characterize the dynamics at a given time t by the instantaneous exponential growth rate \u03bb(t) of the prevalence I(t) (i.e. the total number of infectious individuals at that time) (6, 7) (see Materials and Methods, section SIR Model). In the initial phase of an epidemic wave, we assume \u03bb(t) is approximately constant and equals \u03bb 0 , reflecting the baseline disease transmission rate before the onset of governmental response, such that the prevalence evolves linearly in logarithmic scale with slope \u03bb 0 , as illustrated in Fig. 1D . After the onset of social distancing measures a constant causal effect \u03b8 0 is assumed to be subtracted from \u03bb 0 such that overall the log-prevalence evolves as a piecewise linear function of time, the absolute change in slope, \u03b8, quantifying the causal effect of containment measures on the transmission rate (see Fig. 1D and Materials and Methods, section SIR Model). To understand how this change is reflected in the number of confirmed cases, we follow the same abstraction principles as age-structured population models with continuous age distribution (8) and define a risk-structured population model as follows. We consider an idealized large-scale testing mechanism, in which a risk score r is attributed to each individual. This risk score accounts for observable factors officially used by the government to determine an order of priority for testing (e.g. symptoms, contacts with positive cases,...), as well as for unobserved in which all and only individuals with a risk score above r 0 are tested (orange rectangle) while testing capacity is larger (purple rectangle). (B) Risk-score density of infectious individuals for a given prevalence I 0 (light pink area) and the double (dark pink area). The area from r 0 to r max reflects case numbers for panel A. A drop in the proportion of infectious discovered at low risk score affects the evolution of case numbers if excessive testing is performed (black and purple arrow). (C) Causal graph representing the effect of testing and social distancing measures on the reported rate of confirmed cases in the AT regime. The red arrow represents the causal effect of measures on the epidemic dynamics we wish to evaluate. (D) Schematic time course of prevalence (black), number of tests (orange) and of confirmed cases (cyan), for the AT regime. Dashed line indicates the onset of more stringent containment measures, inflecting the initial growth rate \u03bb 0 of the prevalence. (E) Same as A for the limiting factor (LF) regime, in which low testing capacity constrains number of tested individuals (purple arrow) to a minimum risk score \u03c1(t) above the threshold r 0 set for candidates (green rectangle). Increasing testing capacity may additionally lead to the excessive testing regime described in B. (F) Same as C for the LF regime, with the number of tests now influencing cases numbers. (G) Same as D for the LF regime. Violet dashed line indicates the drop in the growth rate of case numbers, resulting from excessive testing (see B). factors influencing the likelihood of a given individual to be tested (e.g. location specific limitation of testing capacity). We assume at any time the population actually tested corresponds to all individuals above a given minimum risk score \u03c1(t). Moreover, public policy may define a risk threshold r 0 , not necessarily equal to \u03c1(t), such that individuals above this threshold form the pool of candidates that should \"in principle\" be tested. Three paradigmatic cases are of interest: in the so-called adapted testing (AT) regime, the risk threshold is kept fixed such that \u03c1(t) = r 0 at all times t. This implies the testing capability allows to handle the possibly larger pool of candidates above the risk threshold as the epidemic spreads. The rate of cases thus remains uninfluenced by the number of tests performed, which simply adapts to the state of the epidemic in order to fulfill the testing threshold r 0 set by public policy. To understand the mechanisms underlying the observed data, we use the formalism of causal inference, offering a principled framework for evaluating causal effects and learning mechanisms from data (9, 10) . Influences between variables for the adapted testing regime are summarized by the causal graph ( Fig. 1C) showing that the true prevalence, which is unobserved, can potentially be inferred from its effect on the observed case numbers Y (t), further allowing to infer the causal effect of government response on prevalence through the above mentioned SIR model. In this regime, under simplifying assumptions provided in Materials and Methods, section Large scale testing models, the total number of positive cases is proportional to the prevalence of the disease,  where T (t) is the rate of testing at time t and \u03ba is a multiplicative constant. As a consequence of this relation, the number of confirmed cases shown in Fig. 1G completely misrepresents the epidemic dynamics characterized by the prevalence, and the effect the policy has on it, due to the apparently increasing growth rate. Finally, the above limiting factor regime can evolve into an excessive testing regime as the testing capability increases during the course of the epidemic, such that individuals achieving increasingly smaller risk score are progressively being tested. Indeed, in this situation the marginal number of additionally positively tested candidates may drop as the risk of the addi-tionally tested individuals decreases (see illustration by the purple arrow in Fig. 1B) , reflecting a smaller proportion of infectious cases achieving such risk score. The excessive testing regime results in an drop of the probability of a single test to detect a true positive, leading to a milder increase of the number of cases compared to the prediction by the limiting factor regime model of equation 1 (see dashed purple line in Fig. 1G ), again misrepresenting the epidemic dynamics. In order to investigate how empirical data can untangle the mechanisms of large-scale testing, we introduce a family of population level models reflecting the variety of regimes described above, and evaluate their validity based on their ability to predict the death rate trajectories in two datasets: the testing dataset of Our World in Data 1 (11) containing time resolved evaluations of the number of tests and confirmed cases for many countries across the world, and the data from the COVID Tracking Project 2 that contains similar data for all US states. As described in the schematic of Fig The testing models notably include the regimes described in Figs. 1A and 1E: on the one hand the adapted model is our baseline, in which the rate of cases Y (t) reflects I(t) up to a multiplicative factor; on the other hand, the limiting factor model implies that the rate of cases is proportional to the product of I(t) and T (t) (based on equation 1). We additionally checked intermediates between these two models, accounting for a possible saturation of the growth rate of case numbers Y (t) as the number of tests becomes too large (the excessive testing regime described in Fig. 1B Methods, section Large scale testing models). All models are associated with a testing function f m (where m indicates the model), parameterized by one scalar coefficient in the case of both saturated models, and linking the number of confirmed cases to prevalence through The functions associated to different testing models are illustrated on Fig. 2B , and their mathematical basis is provided in Materials and Methods, section Large scale testing models. We exploit this framework to compare testing models based on their associated estimates of prevalence, and their ensuing ability to predict the observed death rate D(t), with estimate D(t). For that we perform cross-validation of these models across countries, optimizing for putative free parameters of each model, as described in Materials and Methods section Validation of testing models. Optimization of the parameters was implemented using the automated differentiation capabilities of the pyTorch library (14) and using the BFGS algorithm (see Materials and Methods section Optimisation of testing models). We estimated the prediction error of the logarithmic death rate of each model using 10-fold cross-validation across countries. Cross-validation errors of each models (yielding one error value per country) were then compared using paired Wilcoxon signed rank tests. Figure In order to assay testing models at a different spatial scale, we performed the same anal- ). This supports the existence of saturation phenomena that make the testing model depart from the purely linear limiting factor regime, such as excessive testing. However, the strong heterogeneity of the World's countries may explain why these parameteric models fail to improve prediction on this dataset, emphasizing the robustness of the limiting factor model in this context. To improve the predictive power of parametric models on the World's countries dataset, we investigated ways to handle country heterogenity. While fitting a separate model to each country is prone to overfitting, we allowed the model to adapt to the country through the influence of country specific covariates on the parameter of the model, as illustrated in Fig Since these results imply that the adapted testing regime inaccurately estimates the prevalence of the disease, we next studied the impact of choosing this assumption for growth rate and policy effect estimations, in comparison to the more accurate limiting factor model. Using the OWID testing dataset (11) we investigated epidemic growth rate trajectories around two periods of instatement of stronger social distancing measures, that we call \"lockdown\" for the sake of conciseness, although the additional measures taken are not officially qualified as lockdown in every country. These periods respectively start in the first (called \"first lockdown\") and fourth (called \"second lockdown\") quarters of 2020 in many countries, Tables S1-S2. son regression (see Materials and Methods) on two time intervals: before and after instatement of more stringent social distancing measures, and using both adapted and limiting factor testing assumptions. While the estimated growth rate is high for the pre-lockdown period (median: .23 days \u22121 ) using the adapted testing model, it is significantly smaller (median: .09 days \u22121 ) when using the limiting factor testing regime assumption (p = 6.1 \u00d7 10 \u22125 ; N = 15). This reflects the overestimation of the growth rate before lockdown, due to a concurrent increase in test rates (see Supplementary Fig. S3 ), which is corrected for by the limiting factor test- We next ran the same analyses for the second lockdown, in the fourth quarter of 2020 ( Fig. 3D-F) . While the overall initial growth rate prior to lockdown has lower magnitude compared to the first lockdown (Fig. 3D) , likely reflecting the higher initial stringency of measures as shown in Fig. 3G , using the limiting factor testing assumption again reduces the magnitude of the initial growth-rate (p = 1.9 \u00d7 10 \u22123 , N = 11), suggesting again that the increase of testing prior to lockdown tends to over-evaluate the growth of the epidemics at the beginning of One limitation of our results is that upon change of testing policy the relevant model may change, and the proposed regimes are simple approximation that can be improved by an accurate and time resolved documentation on the testing process (for example by assessing the evolution of testing capacity). More generally, as more epidemic data will be gathered, more accurate models of large-scale testing may be discovered. This may be achieved through further characterization of the testing process through detailed modeling of the risk score used in different countries and the distribution of infectious and non-infectious individuals according to this score. Finally, these results suggest that changes in public policy regarding testing must be wellthought-out and documented in order to maintain reliable assumptions on the testing mechanisms, and thus a precise and timely evaluation of epidemic dynamics. Awareness of this aspect 14 of testing at the demographic level are moreover particularly relevant in order to establish optimal epidemic control policies that mitigate economical impacts (23 \u2022 Tables S1 to S3 We use a simplified model of exponential spread of the disease within a country, neglecting the influence of imported cases. We start with the Susceptible-Infectious-Recovered (SIR) model where \u03b3 is the recovery rate, assumed constant, and \u03b2(t) is the disease transmission rate which might be influenced by time varying social distancing measures. We assume that I(t) (the current number of contagious individuals in a country) as well as R(t) remain small with respect to the total population, as supported by empirical evidence (24, 25) , such that variations of S(t) relative to its initial value remain small. This approximation leads to a homogeneous first order differential equation for I(t) where the sign of \u03bb(t) = \u03b2(t)S(0) \u2212 \u03b3 determines whether the outbreak goes on spreading or diminishes. The rate \u03bb is closely related to the reproduction number R 0 , which can be defined as R 0 = \u03b2S(0) \u03b3 in the SIR model (26) . To infer the epidemic dynamics, we focus on estimating \u03bb(t) which is directly related to I(t) as is logarithmic derivative, using equation 4, such that Notably, this leads to the exponential form of the prevalence, further exploited in the section Estimation of causal effects, Risk-based testing framework Following the same lines as age-structured population models with continuous age distribution (8, 27) , we consider the population as a continuum of individuals, for which we can define densities of individuals satisfying particular properties. More specifically, we consider a riskstructured population in which the selection of candidates for testing is done through a random risk variable R(k) that assigns to each individual k a positive risk. Note that for the remainder of the supplementary information, R will always denote the random risk, and not the number of recovered individuals of the above SIR model. The higher the risk, the higher is the priority of the individual for being tested. This risk is assumed continuous valued and reflects not only the observed information (symptoms, contact cases,...) but also exogenous influences that will affect the probability of the individuals for being tested. R(k, t) depends on the actual state of the individual \u03c3(k, t) (infectious, \u03c3(k, t) = 1, or not, \u03c3(k, t) = 0), and on the overall state of the epidemic (I(t) large makes individuals more likely to be a contact case, thus to have a high risk). As a consequence, using the continuum assumption, R(k, t) is distributed across the population according to a density of the form p(r|I(t), \u03c3(k, t)) such that the probability of an arbitrary individual in state \u03c3(k) to have a risk in the small interval [r, r + dr] is p(r|I(t), \u03c3(k))dr. The average number of infectious individuals in the total population with risk in [r, r + dr] is given by in the same way, the number of susceptible with a given risk is given by where N (t) is the total number of non-infectious individuals in the above SIR model, i.e., It is reasonable to combine the two compartments because many infections 20 are not detected so that it is not known if an individual recovered and the testing procedures often do not depend on an earlier infection. Note that N (t) + I(t) = N (0) = S(0) is constant. While for each value of the risk, I(t) intervenes as a multiplicative factor for the above conditional quantities, I r and N r do not necessarily evolve linearly with I(t) without further assumptions due the dependency on p(r|I(t), \u03c3). We however make the following additional assumptions in line with the above section of Supplementary Methods on SIR modeling. \u2022 N (t) remains close to N (0) for all times, i.e., I(t) is always much smaller than N (t) reflecting that only a small proportion of the population is infected at any time, \u2022 I r remains small in comparison to N r at any given score value r (reflecting the low proportion of infectious within the population, and that the risk value achieved by an individual does not provide strong evidence for her being infectious), \u2022 the modulation by I(t) of p(r|I(t), \u03c3) is small with respect to its marginal p(r|\u03c3), such that p(r|I(t), \u03c3) = p(r|\u03c3) + u(r, I(t), \u03c3), for small and |u(r, I(t), \u03c3)| < p(r|\u03c3) Inference on the growth rate \u03bb(t) based on equation 4 requires estimation of the true number of infectious I(t). In practice only the time evolution of confirmed cases Y (t) is known but this is not necessarily a good indicator of I(t) as it also depends on the amount of testing T (t). Taking the above risk based model, we assume the pool of T tested individuals is chosen as the T having highest risk. This leads to the relation between T and threshold risk \u03c1(T ) (taking the expectation) We assume moreover that testing is \"ideal\" such that every test is 100% reliable, performed only once per individual and instantaneous (putative lags are discussed in section Reporting p(r|\u03c3 = 1)dr (6) and due to counting nature of the number of confirmed cases, we assume that Y (t) follows a Poisson distribution. We then consider the following testing regimes. Adapted testing (baseline). The baseline model assumes testing has no influence on observed cases and I and Y are proportional This can be put in the above framework under the \"adapted testing \" assumption stating that: T (t) is chosen in order to test all candidate individuals up to a fixed risk r 0 , fixed by government response once and for all. Indeed, under the approximation of equation 6, this leads to yielding the above linear relationship with \u03ba = rmax r 0 p(r|\u03c3 = 1)dr, which does not depend on On the other hand, to understand how the amount of testing is modulated, we can solve for T \u03c1(T ) = r 0 leading to In this last approximation, the first term is constant, while the next two terms may vary as a function of the prevalence I(t), in principle with comparable magnitude and also at least comparable to the variations in the number of confirmed cases of equation 8. Notably, variations in the second term, reflecting the number of non-infectious, may be caused by contact tracing measures, leading, when I(t) gets large, to a larger number of non-infectious contacts assigned high values of r, and thus being tested. The third term trivially reflects the shift in risk-structure due to shifting from the non-infectious structure to the infectious structure as I(t) increases. This justifies that the adapted testing assumption entails a putative dependency of the number of test T (t) on I(t), justifying the causal graph of Fig. 1B (top) . Limiting factor testing. Due to material or organizational limitations of the testing procedure, all individuals with risk above r 0 may not be tested. We model this regime by assuming that tests are performed following the risk of individuals in decreasing order, until the testing capacity, fixed to T (t), is completely exploited. Contrary to the above example, T (t) is now influencing p(r|I(t), \u03c3 = 1)dr . We use additional assumptions to simplify this relation. Specifically, we assume a constant ratio p(r|I(t), \u03c3 = 0)/p(r|I(t), \u03c3 = 1) = \u00b5 0 for r \u2265 \u03c1(T (t)) at all times. This leads to such that the \u03c1(T (t)) dependent term in the expectation of confirmed cases writes p(r|I(t), \u03c3 = 1)dr = leading to (neglecting the order two term in I(t)) Thus the expected number of cases takes the form of a mass action law, with proportionality to both the number of tests T and to the number of infections I that governs the probability that a randomly tested individual is infected, i.e., in our general testing model framework Given detecting a new case relies on both testing the subject and that the subject is infected, the above approximation can be interpreted as a law of mass action for testing, that induces a multiplicative effect of the rate of new tests on the rate of new confirmed cases. Up-saturating testing model. Based on a similar idea as in the previous model we in addition assume that with increasing test numbers the probability of a test to be positive decreases, i.e., the quotient I r /N r increases as the risk r decreases. We relax the assumption that p(r|\u03c3 = 1) is constant and instead consider the following parametric form This expression is almost constant and equal to \u03bd 1 for r > r max \u2212 \u03c9 0 but then decays quickly accounting for saturation of the testing policy. Using manipulations similar to (11) we obtain the following expression for the risk threshold The expected number of confirmed cases then reads Upon reparametrizing \u03ba = \u03bd 1 \u03c9 0 and \u03b1 = N (0)\u03bd 0 \u03c9 0 we obtain the following testing model Note that for small numbers of tests (i.e., T (t) < \u03b1) this model behaves similarly to the limiting factor testing model f l , i.e., every test has the same probability to be positive and confirmed cases are proportional to I(t)T (t) while for large numbers of tests Y (t) \u2248 \u03baI(t) and the model approaches the baseline f a . Thus this model interpolates between the two limiting regimes. Down-saturating testing model. We investigate one further model that accounts for an increasing ratio I r /N r . The idea of this testing model is that a certain fraction of strongly symptomatic patients and very close contacts are always discovered almost independently of the testing numbers. In our framework this can be formalized by assuming that the distribution p(r|\u03c3 = 1) contains a point mass for r = r max , i.e., P(r = r max |\u03c3 = 1) = \u03b4. In addition we assume that p(r|\u03c3 = 1) = \u03bd 1 and p(r|I(t), \u03c3 = 0) = \u03bd 0 as a special case of the limiting factor testing model. Based on similar calculations as before we obtain . For a justification of the names of the testing we refer to the plot in Figure 2A . Death statistics of the Covid 19 pandemic are often assumed to be reported more accurately than the number of infections. Therefore they can be used to check the validity and compare different testing models. In this section we describe the methodology underlying our validation approach. We modeled the relation between the number of infections I(t) and the number of deaths D(t) following (13) . We assume that the expectation of the death rate D(t) relates to the time series of daily incidence i(t) through the equation where \u03a0 denotes a filter representing the distribution of the time between infection and death, called onset-to-death distribution, ifr the infection fatality rate and * the discrete convolution. We use the parametric model of onset-to-death distribution in days, provided by (13) based on the experimental results of (12), and discretize it to obtain one point per day, using \u03a0(t) = P(\u03a0 \u2208 [t \u2212 0.5, t + 0.5]). We will assume that ifr remains constant in time but may differ between countries. The actual value is not relevant to our analysis. We can now combine the relation between infections and deaths and the testing model and assess how well different testing models can explain the observed evolution of cases, deaths, and tests. Recall that we assume for some testing model f . We can infer an unbiased estimate of I a\u015d Since our testing model involves the prevalence while the expression for the expected mortality rate involves the incidence we need to connect incidence and prevalence in the SIR model. In the SIR model the incidence agrees with the gain term i(t) = S(0)\u03b2(t)I(t). Discretization on a daily level leads to the expression where \u03b3 is the recovery rate in the SIR model. Combining the last to equations implies The recovery rate \u03b3 in the SIR model corresponds to the inverse of the mean generation interval which we assume to be 5.0 days based on (28) , thus \u03b3 = 0.2 days. Our results are not sensitive to the value of \u03b3 and we can also use the prevalence I as a proxy for the incidence i(t). We use testing time series T (t) = T C (t) and confirmed cases Y (t) = Y C (t) from some country C to estimate infection numbers using (18) , i.e.,\u00ce(t) = Y (t)/f m (T (t)). Then we infer an estimate for the expected number of deaths using the relations (15) and (19) . We assess the quality of a testing model by its ability to predict the observed time series of deaths. We measure the distance between D C and D C est by where the time series is restricted to days with at least 10 observed deaths. We remark that the variance does not depend on the unspecified constant of proportionality given by if r. We average this across countries using the expression The normalisation by the variance of ln(D C ) ensures that the results of different countries are comparable and the error is not dominated by countries with bad data quality. Here we describe how the parameters of the testing models are optimized such that the prediction error of equation 20 is minimized. Our error measure is insensitive to multiplicative factors such that we do not need to optimize the factor \u03ba contained in all testing models. This allows the testing models to adapt for different infection fatality rates due to varying age distributions and health systems. The further parameters involved in the testing model are assumed to be independent of the country and optimized using 10-fold cross validation and automatic differentiation and the BFGS optimizer using the pyTorch library (14) . We also allow the parameter \u03b1 of the testing model to be a linear function of a set of n covariates X i of the countries, i.e., which can be optimized for n + 1 dimensional vector \u03b2. This results in a step function effect on the growth rate with \u03bb 0 the baseline value of \u03bb(t) under normal conditions (no social distancing), related to \u03b2 0 the baseline disease transmission rate, and \u03b8 0 is the causal effect of social distancing. As a consequence, the logarithmic number of infected evolves in time as leading to a piecewise linear time course as illustrated on Fig. 1D , where the change in slope reflects directly the causal effect of social distancing. We now include the testing model relation between I, Y , and T in thea piecewise linear evolution of log I(t) as in (21) and we obtain log E[Y (t)] = log I(t) + log f (T (t)) = log 29 The evolution of log E(Y (t)) is thus guaranteed to reflect the piece-wise linear trajectory of I(t) only if the correction accounting for varying testing is included in the model. Given the log-linear form of the testing models of the theoretical value of Y (t) (based on equation 2, an appropriate statistical framework for estimating \u03bb(t) is Poisson regression (29), which is a particular form of Generalized Linear Model. In this setting, observations of the number of confirmed cases Y (t) are assumed Poisson distributed, with expectation parameter in the uncorrected case (adapted testing), such that the regression coefficient b, which is the slope of the curve schematized in Fig. 1D , provides an estimate of \u03bb(t) on time intervals where it is assumed constant. The model dependent correction for the varying testing rates can also be incorporated as in (22) in the Poisson regression as an exposure term, leading to the corrected with f the function associated to the testing model (f is identity for the limiting model, and constant for the adapted model). These statistical models allow inferring the disease dynamics from observational data. We use the Our World in Data Covid 19 dataset available online at ourworldindata.org and published in (11) . This dataset contains data for over 200 entities and provides various time resolved data related to Covid 19. Our analysis relies on the number of confirmed cases, testing information and fatality numbers. The available data typically contains a time lag \u03b4 between the date of a test and the date the result of this test is reported. This implies that the testing models actually relate test statistics of time t with the prevalence I(t \u2212 \u03b4) at an earlier point. We account for this in the analysis of causal effects by excluding some days from the regression (see below). In the validation analysis we assume that for each country the reporting delay for fatalities and tests is similar such that the offsets cancel. Moreover, this delay is small in regard to the time scale of the death rate trajectory. For the model validation based on the death prediction we use all countries for which at least weekly testing information is available and which had at least 1000 fatalities attributed to Sars-Cov 2 such that death prediction is meaningful. In addition we removed China from the data because testing started in a late stage of the epidemics there. This left 58 countries for the data analysis. We also investigated the dependence of the testing model on additional country covariates. The covariate data was taken from the DELVE Global COVID-19 Dataset 4 (?) and was originally collected by the World Bank.Data was available for 55 of the previously selected countries. We started the time series when at least 20 cases were reported because some of the testing models notably the limiting factor model become unstable for very small case numbers. We base our analysis on reports of daily new tests and confirmed cases. In case daily updates are missing for n days, we perform linear interpolation of the logarithm of the cumulative number of test and cases, respectively, and use the daily difference to approximate the daily updates. In this way, the cumulative number remains consistent with the observations, while the daily updates are interpolated. For the analysis of lockdown effects we also relied on the Our World in Data dataset. The lockdown dates for the spring period was determined based on information reported in the BBC article Coronavirus: The world in lockdown in maps and charts 5 where the earliest date among \"national recommendation\" and \"national lockdown\" was chosen. From those countries we selected all countries that had at least 10 cases reported 7 days before the beginning of the lockdown and at least weekly testing information. The 15 selected countries and lockdown dates can be found in Table S1 . For the pre-lockdown Poisson regressions we considered the time interval spanning from the first day with at least 10 cases until 5 days after the onset of the lockdown taking into account that due to the incubation period and delays in case reporting this time interval captures infections before the lockdown. For the Poisson regressions during the lockdown we considered the interval starting 10 days after the lockdown onset and ending when the mobility reduction was less than 80% of the maximal reduction in mobility as measured by the Google mobility reports 6 where we used weekly averages of the sum of retail, transit stations, and workplace indicators. Many European countries ordered a second lockdown in the fall of 2020. As the measures were installed more gradually the transition between pre-lockdown and lockdown was less sharp. Therefore we determined the onset of the second lockdown based on the stringency index from the Oxford COVID-19 Government Response Tracker (OxCGRT) (15) which was designed to measure the stringency of the governmental responses to the Covid-19 pandemic on a scale from 0-100. We defined the beginning of the lockdown as the first day where the stringency index was above 50. The data was taken from the Our World in Data dataset. Using this definition of the second lockdown 11 countries issued a second lockdown (some countries were excluded because they never lifted the stringency of their measures below 50) and their lockdown dates can be found in Table S2 . We remark that while the definitions of the two lockdowns do not agree using the stringency based definition would only change the lockdown dates of the first lockdown by a few days. For the pre-lockdown regressions for the estimation of causal effects we used the 3 weeks before the lockdown and for the regressions during lockdown we used all data from 10 days after lockdown onwards (based on the latest version of the dataset from 2020- [11] [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] . We performed a similar analysis for the states of the USA based on data provided by the COVID Tracking project 7 . The lockdown dates for the US states were chosen as the day of the 'stay at home order'. Dates were taken from from Wikipedia 8 . There were 30 states that had sufficient data for the lockdown analysis. They can be found along with their lockdown dates in Table S3 . For the validation analysis we used the same criteria as above which were satisfied by 33 states. We did not pursue the addition of covariates for the parametric testing models. Country wise cross validation prediction error for the death rate for different testing models. Blue box-plots indicate regression without covariates. For orange box plots, the testing models' parameter was optimized using a linear dependence with respect to the country's Gross Domestic Product (GDP) per capita, population density, and the percentage of urban population as covariates. Note that both boxplots agree for the case of the limiting testing model which has no parameters to optimize. Statistical significance was assessed using a paired Wilcoxon signed rank test (N = 55). Slopes of the testing number Pre-lockdown During lockdown Figure S3 : Testing statistics, related to Fig. 3 . Statistics of the exponential growth rate of the number of tests estimated by Poisson regression for each country individually, separately for time periods before and during application of social distancing measures for the first and second lockdown. Significance was assess using Wilcoxon signed rank tests on the sign of the median of the regression coefficients or their difference (when above a line connecting two quantities). 37 days from lockdown onset  Estimated 1st lockdown effect Figure S4 : Impact of testing assumptions on growth rate and policy evaluation for US states, related to Fig. 3. (A) Number of daily new confirmed cases for each country of the first lockdown. Black and red solid line represent the average of Poisson regression models across country, before and during social distancing, respectively. (B) Same as A for daily new tests per thousands (right). (C) Statistics of the exponential growth rate of the prevalence estimated by Poisson regression for each country individually, separately for time periods before and during application of social distancing measures for the first lockdown. Testing is corrected for according to two testing models: adapted testing (left) and limited testing (right). (D) Estimate of the effect of social distancing on transmission rate, with or without correction for testing. Indicated p-values correspond to Wilcoxon signed tests on the sign of the median of the regression coefficients or their difference (when above a line connecting two quantities)."}]