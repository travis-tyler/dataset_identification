[{"section_title": "Abstract", "text": "The last decade has seen an extraordinary amount of effort devoted in biomedical research to the field of biomarkers. There have been some notable successes with novel markers being adopted into clinical practice bringing clear clinical benefit to some patients -particularly with the increasing numbers of medicines being approved with companion diagnostics. However, it is fair to say that there has not yet been the numbers of clinically valuable biomarkers brought to medical practice that the research effort would seem to warrant. This paper evaluates examples of successful biomarkers, markers which might be considered partial successes and a few problematic examples and argues that more effort spent in the validation phase of marker development, and less in the discovery phase might be a more efficient way to allocate research resources."}, {"section_title": "Introduction", "text": "oday, research into the use of biomarkers to discover, develop and rationally prescribe medicines is dynamic and productive. The Food and Drug Administration (FDA) website lists 14 different biomarkers (analytes) which can be used as \"companion diagnostics\" to identify patients for treatment with 16 different therapeutics [1] . The science of companion diagnostics can be very refined so that some biomarkers, for example mutations within a particular gene, can be subdivided associating specific therapeutics with individual single nucleotide variations and different therapeutics with other mutations in that same gene (e.g. the T790M mutation within the EGFR gene indicates therapy with osimertinib, but safety and efficacy for that drug has not yet been established for other mutations: e.g. G719X, exon 19 deletions, L858R, exon 20 insertions, S768I, and L861Q -mutations for which other EGFR inhibitors are indicated). More generally in the USA, 137 different medicines have pharmacogenomics biomarker information included in the drug label text, many with more than one associated marker [2] ; these markers enable the better use of the medicines providing information on: drug exposure and clinical response variability, risk for adverse events, genotype-specific dosing, mechanisms of drug action, and polymorphic drug target and disposition genes. It is estimated that perhaps three quarters of the drugs in clinical development in the major pharmaceuticals companies utilise biomarkers in their clinical studies.\nHowever, the broader field of biomarker discovery and development to create novel tools for identifying and characterising patients, e.g. tools for screening, diagnosis, prognosis, prediction, and assessment of disease activity [3] , clearly lags far behind the increasingly successful efforts to marry biomarkers with drugs. This is not to dispute the tremendous utility of currently available and medically very important bio-T markers (e.g. troponins in the diagnosis of myocardial infarction, d-dimers in the exclusion of deep venous thrombosis, and dozens of other markers in routine clinical use) but rather it is to recognise that the great efforts being made in biomarker discovery are not yet translating into commensurate numbers of biomarkers which have been adopted into widespread clinical practice.\nAt the end of 2015, a search of the PubMed database using the term \"biomarker\" revealed the startling figure of 753,760 publications in this database, of which 83,523 were review articles. Searches on the terms \"precision medicine\" identified 17,800 publications and \"personalized medicine\" 26,032. Figure 1 shows the extraordinary growth in the numbers of publications -with more than 50,000 new articles being published per year in the recent period.\nIt has been estimated that at least 100,000 different \"biomarkers\" have been described but the number of biomarkers applied in clinical practice is perhaps only around 100. What is the explanation for this disparity? The remainder of this article will consider lessons which can be learned from recent successfully developed biomarkers and from recent problematic results. It will be argued that diverting some research resources away from biomarker discovery to more rigorous and disciplined validation efforts for those few biomarkers which show strong associations with specific clinical conditions or outcomes might be ultimately a more effective overall strategy for the field. It will also be argued that the path from discovering a biomarker to eventual clinical utility will be smoother and more efficient if coordinated efforts across different research groups are made using a single reliable and reproducible technical platform so that results can easily be compared across studies. Perhaps less discovery science and more pragmatic development effort is needed. Figure 1 . This is a plot the numbers of publications registered each year in PubMed identified using the search term \"biomarker'. There does seem to have been acceleration in the annual numbers since the middle part of the last decade. More than 50,000 papers are now being published each year featuring biomarkers."}, {"section_title": "Recent Successes", "text": "It is important to recognise that there have been key advances in the last few years, especially in the oncology therapeutic area, despite the pessimism expressed as recently as 2010 [4] and the genuine challenges involved [5] . This recent progress is also beginning to spread more broadly into other therapeutic areas.\nOne very good example is the increasing adoption of gene expression profiles in oncology for prognosis and to some extent for treatment guidance. Such profiles (or signatures) are used to identify patients with a high risk of early progression who require aggressive therapy (and conversely to identify low-risk patients who do not need this therapy and for whom avoiding the side effects of aggressive therapy is important [6\u22129] which have been undertaken for these assays. After thorough evaluation of the analytical validity of the signatures, they have been subject to careful clinical validation involving hundreds of patient samples from multiple collections, and have been shown to affect treatment decisions for patients in beneficial ways ( Table 1) .\nThe development programmes for these biomarkers have been as much as a decade long in order to generate the data required for the assays to be incorporated into treatment guidelines [15] and to gain reimbursement [16] . From a purely scientific perspective there are some interesting observations about these expression profiles: firstly, although the intended clinical uses for some of the above tests seem quite similar and the gene signatures seem to have been identified in similar ways, the actual lists of genes included in the signatures show relatively little overlap (although maybe represent similar overall pathways) and the tests use rather different technical platforms (RT-PCR, microarray, NanoString) [17] ; secondly, the tests do not always identify exactly the same patients for classification as low or high risk for disease recurrence [18, 19] . This could suggest that the utility of the tools derives not from a deep scientific understanding of the analytes (the genes, their relative degrees of expression and the actions of their products) nor even from a complete knowledge of the link between the underlying Clinical Validation \u2022 Sensitivity and specificity of the assay -clinical accuracy (in the intended patient population) \u2022 Assay failure rates (and reasons)\n\u2022 Assay \"no-call\" rates, i.e., indeterminate results\n\u2022 Use blinded, retrospective analyses of prospectively collated samples with known outcomes \u2022 Evaluate assay performances in different labs and in different patient populations [11] [12] [13] Clinical Utility \u2022 Does the assay provide medically useful information which improves patient outcomes or reduces health-care costs?\n\u2022 Use prospective randomised clinical studies to show the assay improves outcomes [11, 14] molecular pathology and the patient's eventual clinical outcome, but rather from the thorough, pragmatic and methodical development programmes pursued by the originators of these tools.\nAnother interesting observation about these expression profiles is that they all had large effect sizes when first discovered (i.e. when a sample was positive for the assay that patient had a much increased chance of showing the relevant clinical outcome, i.e. early disease recurrence or specific histopathological subtype) and the probability that the signature was a chance observation was very low [20, 21] . A review of multiple biomarkers across a broad range of indications showed that for many markers, the highly-cited paper which originally identified the marker often overestimated the effect size in comparison with larger studies and in comparison with subsequent meta-analyses [22] . One interpretation of these findings might be that only biomarkers originally identified with large effect sizes subsequently survive further validation studies.\nIn the cardiovascular field amongst many important traditional biomarkers, brain (derived) natriuretic protein (BNP) and N-terminal proBNP (NT-proBNP) have recently proven to be very useful markers to diagnose or exclude cardiac failure in acutely dyspnoeic patients, to provide some prognostic information in cardiac disease and to distinguish between cardiac and pulmonary disease [23] . For these markers to have become part of clinical practice, it has taken almost thirty years of effort (BNP was first identified in 1988 and the related ANP was identified even earlier). Clinical validation and clinical utility have been established by the study of several thousands of patients in multiple small and large studies.\nDevelopment of natriuretic peptides as biomarkers has unfortunately been hampered by the use of different assays with different performance characteristics. Development has also been difficult because the markers show many of the characteristics of typical serum biomarkers: normal physiological variation and strong influences of patient characteristics (e.g. the markers are lowered in obese individuals, but increased with impaired renal function and by concomitant diseases such as atrial fibrillation). The example of the natriuretic peptides illustrates perfectly the substantial effort required to clinically validate novel biomarkers but perhaps also suggests that the marker might have been developed more efficiently if a single optimised assay had been used across multiple centres and clinical studies.\nThe therapeutic area of inflammatory diseases provides another example of a recent hard-won potential success. With so many new medicines for Rheumatoid Arthritis recently approved and in development, there has been a substantial effort to identify biomarkers which might identify patients who will respond to a specific medicine or mechanism. Despite the explosion of knowledge around cytokines, other inflammatory mediators and the roles of cell adhesion molecules it has proven extremely tough to find any disease activity markers more informative than the traditional measure of C-reactive protein (CRP) and straightforward assessment of patient symptoms and signs. Similarly, predictive markers to identify potential responders or non-responders have not been forthcoming. However, recently a panel of 12 serum proteins together with an algorithm to interpret the results of measures in this panel (Vectra DA TM ) may have finally been shown to improve upon traditional measures [24] . Across a range of patient populations with Rheumatoid Arthritis this score correlates with traditional measures of disease activity, but most im-portantly changes in the score over as little as two weeks of therapy predicted future clinical responses (measured at 6 weeks) in one study. If this finding is real and repeatable, the biomarker could provide two significant benefits: firstly the ability to avoid long and expensive trials of therapy in patients who will not gain benefit from a new drug, and secondly to simplify and considerably shorten phase 2 exploratory clinical studies with new drugs. Convincing validation of this finding is awaited with great interest."}, {"section_title": "Recent Partial Successes: Biomarker Qualification Programmes", "text": "The evidence that biomarker development can be very difficult was further illustrated by the FDA Biomarker Qualification Programme (for drug development) [25] . There have been 19 submissions to this programme since 2008 of which: 1) 9 are for qualification of preclinical safety markers 2) 3 are for measures of clinical response (FDG PET for tumour effects; tumour volume measures; and MRI cartilage thickness measures for arthritis) 3) 6 are for prognostic markers (cerebrospinal fluid (CSF) markers for the progression of Alzheimer's Disease, hippocampal volume for the same condition, dopamine transporter PET for Parkinson's Disease, total kidney volume for autosomal polycystic kidney disease, fibrinogen levels for chronic obstructive airways disease (COPD), and chronic kidney disease marker 273 for diabetic nephropathy), and 4) 1 marker is for patient adherence (breath 2 butanone). Six data packages are under review or have been approved; three for preclinical toxicology groups of markers, and three for optimising patient selection for clinical trials (for invasive aspergillosis, COPD and polycystic kidney disease). The clinical packages comprise a great deal of data but the \"approvals\" are for only very modest reductions in the numbers of patients required in phase 3 clinical studies. These examples perhaps again stress the need to focus on markers which provide large effect sizes (and therefore merit the effort of their development and validation).\nAlzheimer's Disease is a condition for which there has been an enormous multi-programmatic effort to identify prognostic and predictive biomarkers. This effort has been driven by the desire to prevent progression of the disease, perhaps prevent dementia ever developing and even perhaps to reverse the disease process. Governments, regulatory authorities, pharmaceuticals industries, charities and academic researchers have come together to deliver several major coordinated programmes to discover and qualify biomarkers, e.g. ADNI (I & II) and AIBL [26\u221228] . These programmes have required the collection of multiple samples of blood and CSF over many years from cohorts of normal individuals, individuals with early cognitive decline at risk for disease and patients with frank Alzheimer's whilst monitoring the cognitive function of these individuals. Great efforts have been made to analyse the serum and CSF samples using standardised techniques from only a small number of laboratories. Hundreds of patients have also provided positron emission tomography (PET) and volumetric MRI scans and through these efforts specific protocols for imaging conduct and analysis have been developed and standardised.\nAmongst the many successes of these programmes has been agreement on biomarker driven diagnostic criteria for both full-blown Alzheimer's Disease and for incipient disease on the basis of CSF analytes (A beta 42, phospho-tau, total tau), amyloid PET neuro-imaging, specific cognitive deficits and APOE4 genotype [29\u221232] . Amyloid binding PET ligands have been brought to the market as a way of excluding patients from the diagnosis of Alzheimer's Disease and as a method of identifying patients for clinical trials of novel medicines intended to arrest the progression of Alzheimer's Disease at an early (possibly even preclinical) stage. Assay kits have been developed for blood and CSF analytes -though it is fair to report that preanalytic quality control remains an issue as does \"trueness\" across the various kits -especially for blood tests.\nThese biomarker programmes have been astonishingly successful in characterising, and analytically and clinically validating, specific markers. On the other hand, despite the programmes having been underway for more than a decade, the eventual clinical utility of the markers is still not clearly determined. Certainly while amyloid PET ligands can help exclude a diagnosis of Alzheimer's Disease as the cause of dementia, their approval by regulators was not met by reimbursement. A major study is currently underway (the IDEAS study) to demonstrate the clinical utility of amyloid PET Imaging [33] . Of the other markers, APOE4 genotype (and other much rarer dominantly inherited genotypes) potentially provide prognostic information about the age at which Alzheimer's Disease might develop. However, there are no currently available interventions which have been shown to delay or reverse progression of the disease. In this disease area, the biomarkers have, unusually, been developed in advance of therapies rather than vice versa for the examples given elsewhere in this article."}, {"section_title": "Some Recent Problematic Results", "text": "Brookes [5] provides a good theoretical description as to why it is so difficult to develop and validate biomarkers and Diamandis [4] provides a series of examples of lessons that can be learned from high-profile failures of validation. Amongst the many important points made by these authors four issues stand out: 1) Exaggeration of the effect size of a putative biomarker in early discovery studies i. often by poor selection of the control tissue (or patient) samples; for example, not truly matching for age, gender, and other diseases of the same tissue ii. sometimes through poor case selection; for example, using end-stage disease 2) Failure to fully disclose the methodology and failure to replicate the result in at least one other laboratory early in the validation process 3) Not having a clear plan for the \"intended use\"\nfor the biomarker i. what decision is information from the biomarker intended to support? ii. not recognising the harm that can follow from an incorrect decision 4) Failing to understand the importance disease prevalence plays in the usefulness of a screening test and so underestimating the difficulties caused by false positive results (to patients and to healthcare providers). To these issues it is also possible to add (at least) another two:\n\u2022 retrospectively attempting to analytically and clinically validate complicated biomarkers during the course of extensive clinical studies of novel drugs\n\u2022 developing assays for recently discovered analytes with complex and only partially understood function A very good example of the fifth issue concerns recent efforts to develop drugs, typically biologics, as therapeutics for subsets of patients with asthma [34, 35] , patients whose asthma is thought to have a molecular pathology consistent with the mechanism of action of the novel therapeutic.\nGiven the recent successes in developing targeted therapies in oncology, at least a few of which were found to be associated with predictive biomarkers during the course of their clinical development (eg gefitinib, crizotinib), it has been tempting for pharmaceuticals companies to embark on large clinical development programmes incorporating inadequately characterised biomarkers in the expectation that associations between the markers and response will emerge from those clinical studies. The thinking behind the choice of these biomarkers is naturally related to the supposed mechanism of action of the novel targeted therapeutic.\nHowever, it is important to recognise that retrospective identification of predictive biomarkers only really works when it is easy to identify patients who are experiencing a profound clinical response, not seen in other patients. The link between a biomarker being present in profound responders and not present in non-responders is usually very obvious, and qualitative rather than quantitative (i.e. a mutation or translocation is present or it is not). Conversely, the biomarkers used in asthma studies described above (blood and sputum eosinophil levels, so-called TH2 gene expression profile signatures, levels of inflammatory mediators such as periostin) are quantitative and complex, and derived from assays with limited analytical validation. Clinically meaningful cut-offs for positive, negative and intermediate results have not been fully established in prior extensive untreated population cohort studies. Similarly, responsive patients in asthma studies are difficult to distinguish from less well responding patients -there is a certain level of placebo response and natural variation in disease severity over time. Therefore, making the link between response and a biomarker level is inherently extremely difficult and mathematically complex, with neither measures of disease severity nor of the biomarker providing well-founded data. It remains conceivable that a meaningful biomarker (beyond blood eosinophil levels) linking efficacy to a given drug might emerge from these asthma studies which have already included thousands of patients but this does not as yet seem to have been an efficient use of clinical research resources.\nA very good example of the sixth issue (incompletely understood analytes) concerns attempts to develop individual microRNAs or panels of these ana-lytes as biomarkers, for example in heart failure and other cardiovascular diseases [36\u221238] . MicroRNAs are small molecules which serve to regulate transcription, usually each microRNA of which more than 1000 have been identified, regulates multiple genes. They have definite potential as biomarkers as they are found \"stabilised\" in circulating blood either in exosomes or bound to protective proteins. Their potential utility as biomarkers was recognised in the cardiac and oncology therapeutic areas more than a decade ago; RT-PCR is a viable technology to measure these markers and next generation sequencing offers a more advanced potential platform. However, because the precise role of any given microRNA is incompletely understood (and often is only inferred rather than proven) it has been extremely difficult to decide which molecules provide the most relevant information in any given situation and replication of findings across laboratories and across assay technologies remains problematic. This problem is exacerbated when searching for panels of microRNAs with the increased risks of false discovery. It is reassuring to note that the titles of two publications [36, 37] end with a query, and there is also an observation in another report that it is not clear that a new panel of four microRNAs provides information beyond that already available from traditional symptoms, signs and currently available markers [38] . Bearing in mind that it has taken almost three decades to validate natriuretic peptides in these same indications, it seems that microRNA research is still in its relative infancy."}, {"section_title": "Discussion", "text": "For biomarkers to be adopted into widespread clinical practice they need reliably to provide useful clinical information. But, showing that this is true for any given marker is much more difficult than many discovery orientated scientists imagine. The first step for a biomarker is to show an association between the marker and a clinical condition. This step seems to get the scientific recognition and the highly-cited publications. Unfortunately, this is only the very first step. Years, sometimes decades, of subsequent work is then required: to establish an analytical platform that is reliable and robust; to confirm the finding of the association and to provide some kind of clinically meaningful quantification or cut-offs; and to demonstrate that the new finding provides information not otherwise readily available to the clinician and that the additional information influences the healthcare and health outcome for the patient in a meaningful way.\nHow can scientists be incentivised to conduct this validation work? Here are a few suggestions: 1) Journals (editors and reviewers) should avoid publishing biomarker \"discoveries\" until the discovery has been replicated in at least one other laboratory i. perhaps academic laboratories could form collaborative groups to attempt replications of each other's findings? 2) Biomarker discoveries should always be accompanied with a full revelation of the details of the assay methodology and with the full raw data being made available i. a particular issue can be the details of any algorithms used to generate scores from panels of biomarkers, sometimes even the underlying computer code can be important for replication 3) Journals should encourage the publication of validation studies, both analytical and clinical. Studies which fail to replicate previous reports should be recognised as being just as important as those which do replicate. Both kinds of results are vital to scientific progress and merit accompanying editorials in those journals. For efficient redirection of resources, perhaps validation, or early de-validation, deserves as much recognition as biomarker discovery in answering the question, \"What makes a good biomarker?\""}]