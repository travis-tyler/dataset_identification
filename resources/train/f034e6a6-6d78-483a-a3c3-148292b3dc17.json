[{"section_title": "Abstract", "text": "Abstract. Observation or sampling error in population monitoring can cause serious degradation of the inferences, such as estimates of trend or risk, that ecologists and managers frequently seek to make with time-series observations of population abundances. We show that replicating the sampling process can considerably improve the information obtained from population monitoring. At each sampling time the sampling method would be repeated, either simultaneously or within a short time. In this study we examine the potential value of replicated sampling to population monitoring using a density-dependent population model. We modify an existing population time-series model, the Gompertz state-space model, to incorporate replicated sampling, and we develop maximum-likelihood and restricted maximum-likelihood estimates of model parameters. Depending on sampling protocols, replication may or may not entail substantial extra cost. Some sampling programs already have replicated samples, but the samples are aggregated or pooled into one estimate of population abundance; such practice of aggregating samples, according to our model, loses considerable information about model parameters. The gains from replicated sampling are realized in substantially improved statistical inferences about model parameters, especially inferences for sorting out the contributions of process noise and observation error to observed population variability."}, {"section_title": "INTRODUCTION", "text": "The sizes of ecological populations are often estimated rather than censused, producing variability in the observed population abundances. This variability exists over and above the random population fluctuations due to environmental processes, and is variously termed ''measurement error,'' ''sampling error,'' or ''observation error.'' A growing body of research has indicted observation error in population monitoring as a cause of serious degradation of the inferences, such as estimates of density-dependence strength, population trend, or extinction risk, that ecologists and managers typically want to make with such data (Solow 1990 , Shenk et al. 1998 , Holmes 2001 , Holmes and Fagan 2002 , Staples et al. 2004 , Freckleton et al. 2006 . The degradation takes two main forms. First, if observation error is not properly accounted for in the statistical models of population growth, then the estimates of model parameters (and functions of parameters such as probability of extinction) can be seriously biased. Second, if observation error is somehow adequately incorporated into statistical population models, then the observation variance and process variance parameters can be nearly nonidentifiable and difficult to estimate separately, producing ridge-shaped likelihoods and large confidence limits (Dennis et al. 2006 , Knape 2008 .\n''State space'' population models incorporating both process noise and observation error have great potential for improving population studies Hastings 2002, Clark and Bj\u00f8rnstad 2004) . The inferential approaches in previous applications of state-space models have been largely Bayesian (e.g., Clark and Bj\u00f8rnstad 2004) . Recent developments, both analytical (Staples et al. 2004 , Dennis et al. 2006 ) and computational (de Valpine 2003 , Ionides et al. 2006 , Lele et al. 2007 , are helping to make statistical inferences possible for state-space models in the conventional frequentist sense. However, the problem remains for Bayesian and frequentist approaches alike that variability due to observation error is often difficult to disentangle from the natural population fluctuations (process noise) in the time-series abundance data.\nOne idea for improving the information obtained in population monitoring is to replicate the sampling process. Instead of one observation per time unit, the sampling method would be repeated, either simultaneously or within a short time, at each sampling time. Such replication could take such forms as another transect of river snorkeled, another mark-recapture sample conducted, another backcountry road driven and spotlighted at night, or another night of light trapping. Ideally, the replication would be designed to draw from all sources of variability that go into the observation error in the monitoring data. Although such replication might potentially improve the information that can be obtained from population monitoring, in fact such replication is almost nonexistent in reported ecological studies.\nIn a recent review of the sampling-error problem, Freckleton et al. (2006) offered the idea of replicated sampling and discussed results from two such studies in which the sampling-error component could be estimated separately. One was a study of shorebirds by Spearpoint et al. (1988) , who estimated the variability in counts between two observers measuring the same populations simultaneously, as well as long-term (1981) (1982) (1983) (1984) (1985) (1986) (1987) (1988) variability in numbers. The other was by Cunningham et al. (1999) who analyzed replicated censuses of 65 species of Australian woodland birds. In these studies, however, the sampling replication was a one-time study and not a routine part of the long-term population monitoring.\nIn a large study featuring many bird species, Link et al. (1994) analyzed what they termed ''within-site variability'' of bird-count data. They designed and carried out replicated sampling at a variety of sites in the North American Breeding Bird Survey (BBS; Peterjohn 1994), and they constructed variance-components models to estimate the amount of variability attributable to sampling. They found that in at least 14 out of 98 species more than half of the variation in bird counts was attributable to sampling variability. However, like the studies by Spearpoint et al. (1988) and Cunningham et al. (1999) , the Link et al. (1994) study did not encompass multiple years, so that combining the information gleaned from the replicated sampling with time-series modeling of population abundances is not straightforward.\nIndeed, we have not been successful in locating an example data set in which the sampling process has been intentionally replicated through time. Spatially distinct populations have been sampled simultaneously, but such data must usually be used to estimate separate process-and sampling-error parameters (albeit with possible covariation among the populations). Yet, given the degradation of parameter estimation associated with the presence of observation error, the idea of replicated sampling is intriguing. After all, population monitoring consumes much time, personnel, and resources of the agencies involved; is the information thereby gained useful for its intended purposes? How much can the information from population monitoring be improved by replicating the sampling procedures?\nIn this paper we introduce a state-space population model featuring observations from replicated sampling. The model can be used for analyzing replicated sampling data and for studying the potential value of replicated sampling in population monitoring. We modified an existing time-series population model, the Gompertz state-space model, to include replicated sampling. We describe maximum-likelihood and restricted maximumlikelihood parameter estimation for the model. Our computer simulations of parameter estimation reveal that substantial gains in parameter precision can be obtained with replicated sampling; such gains can be weighed by managers against the costs of the extra sampling. Some sampling programs already have replicated samples, but the samples are aggregated or pooled into one estimate of population abundance; such practice of aggregating samples, according to our results, loses considerable information about model parameters. We explain the appropriate statistical procedures for using the model, and we provide an R program for such use. We suggest that ecologists and managers might find replicated sampling worthwhile to undertake in a wide variety of settings."}, {"section_title": "METHODS", "text": ""}, {"section_title": "Replicated sampling model", "text": "We first briefly review the Gompertz state-space (GSS) model for a single time series with unreplicated observations. Additional details can be found in Dennis et al. (2006) . The GSS model takes a discrete-time, stochastic Gompertz model to represent the densitydependent growth of the population:\nHere N t is population abundance at time t (t \u00bc 0, 1, 2, . . .), and E t ; normal(0, r 2 ), with E 1 , E 2 , . . . uncorrelated. The noise process E t represents environmentally induced fluctuations in the logarithmic population growth rate. The stochastic Gompertz assumes population abundance is known without error, with the random fluctuations being driven by ecological processes. The stochastic Gompertz forms the basis of densitydependence tests (Pollard et al. 1987, Dennis and Taper 1994) , models of multiple sites (Langton et al. 2002) , and models of multiple interacting species (Ives et al. 2003 ).\nThe GSS model further takes population abundance to be estimated with error, with the estimated or observed population O t at time t given by\nwhere F t ; normal(0, s 2 ), and F 1 , F 2 , . . . are uncorrelated with each other and uncorrelated with E 1 , E 2 , . . . . The lognormal error exp(F t ) is a model of population estimation error under heterogeneous ob-serving or sampling conditions (Dennis et al. 2006) . For data analysis, the model cast on the logarithmic scale is more convenient:\nand Y t is an observed or estimated value of X t . It should be noted that Y t is an unbiased estimate of X t . The GSS model has four parameters: r 2 is the process-noise variance, s 2 is the sampling-error variance, c is inversely related to the strength of density dependence, and a scales the stationary mean (given by a/(1 \u00c0 c)) of log-population abundance (r 2 . 0, s 2 . 0, \u00c01 , c , \u00fe1, a . 0). Data-analysis methods for a single time series using the GSS model are based on a multivariate normal distribution (see Dennis et al. 2006 Here f(y t j y 0 , y 1 , . . . , y t\u00c01 ) is a normal distribution pdf with mean m t and variance v 2 t that are calculated with two simultaneous recursion equations containing the parameters and previous observations:\nIf the process is assumed to be stationary when the initial value y 0 was recorded, then the recursions are\nIf the initial population, however, was far from equilibrium, the recursions are initiated at m 0 \u00bc x 0 , v 2 0 \u00bc s 2 , with the underlying initial population size x 0 becoming another unknown parameter. The recursions are a special case of four matrix equations for conditional moments known as the ''Kalman filter'' (for instance, Harvey 1993), used for multivariate Gaussian time series. The parameter values that jointly maximize the likelihood function are the ML (maximum-likelihood) estimates. The maximization must be done numerically (SAS program given in Dennis et al. [2006] ).\nA special case of the model representing densityindependent population growth occurs with c \u00bc 1. The model becomes a stochastic version of exponential growth or decay, with process noise and observation error (see Holmes 2001 , Staples et al. 2004 , Dennis et al. 2006 ). The likelihood function for the densityindependent case is multivariate normal, but can be calculated using the above Kalman recursion equations with the value of c fixed at 1. Population abundance under the density-independent model does not have stationary behavior, so the recursions must be initiated at m 0 \u00bc x 0 , v 2 0 \u00bc s 2 , with x 0 treated as an unknown parameter.\nThe extension of the GSS model to replicated sampling is as follows. Suppose at sampling time t, the sampling process is replicated p t times, producing\n0 of the observations (as random variables) at time t, and denote by y t the p t 3 1 column vector [y 1t , y 2t , . . . , y p t t ] 0 of the recorded outcomes (data values) of the random variables in the vector Y t at time t. The number of replications p t can vary for different sampling times, but to apply the methods reported here, p t must be at least 1 for each sampling time. We write j t for a p t 3 1 column vector of 1's, J t for a p t 3 p t matrix of 1's, and I t for a p t 3 p t identity matrix. The Gompertz state space, replicated sampling (GSS-RS) model consists of the underlying population process joined with a multivariate sampling process:\nHere F t is a p t 3 1 random vector having a multivariate normal distribution with a mean vector of 0's and a variance-covariance matrix s 2 I t . The form of the variance-covariance matrix corresponds to the assumption that the observations at each sampling period are independent replicates (given the value of X t ) with constant variance s 2 . The elements in F t are additionally assumed to be uncorrelated through time and uncorrelated with E t ."}, {"section_title": "Maximum-likelihood estimation", "text": "The basic result needed to form the likelihood function is the joint distribution of\nThat distribution is a multivariate normal (MVN) distribution, with a mean vector m t and a variance-covariance matrix V t that are calculated recursively from the previous data, similar to the univariate case. We write\nFor this replicated sampling model, the Kalman recursion equations do not simplify much, and all four equations are needed:\nHere the recursions are started at\nfor the stationary case, and l 0 \u00bc x 0 , u 2 0 \u00bc 0 for the non-stationary case. The scalar quantities l t and u 2 0 are, respectively, the conditional mean and variance of the underlying process X t , given the previous observations\nis that of a multivariate normal distribution with mean vector m t and variancecovariance matrix V t :\nwhere m t and V t are calculated from parameters and observation history using the Kalman recursion relationships. In parallel with the univariate case, the distribution of the initial observation vector Y 0 is multivariate normal, with mean vector m 0 and variance-covariance matrix V 0 . The likelihood function under the GSS-RS model is formed as the product of the conditional multivariate normal pdfs:\nThe log-likelihood, used for the statistical calculations, then becomes\nHere r \u00bc p 0 \u00fe p 1 \u00fe \u00c1 \u00c1 \u00c1 \u00fe p q is the total number of observations, and q \u00fe 1 is the total number of times at which the population has been sampled. The likelihood function contains the additional unknown parameter x 0 when the non-stationary assumption is used. The recursion equations and likelihood function under replicated sampling remain valid for the densityindependent case with c \u00bc 1. The recursions are initiated at the nonstationary conditions l 0 \u00bc x 0 , u 2 t \u00bc 0, with x 0 treated as an unknown parameter.\nThe likelihood function L(a, c, r 2 , s 2 ) can be shown to be that of a single multivariate normal distribution for the vector formed by stacking the vectors y 0 , y 1 , . . . , y q of observations. The above Kalman filter representation represents a decomposition of that likelihood. The multivariate normal distribution follows from linearly transforming the X t 's and F t 's in the definition of the Y t 's and is derived and displayed in the Appendix.\nThe ML estimates of unknown parameters for the GSS-RS model are the values that jointly maximize the likelihood function. There are no closed-form formulas for such estimates, because the likelihood is too complex for simple calculus maximization. Numerical optimization routines available in R, MATLAB, and various other computational software packages are easy to use (Appendix). Although the ML calculations for the univariate GSS model can be performed with mixed-effects analysis of variance programs (such as PROC MIXED in SAS; see Dennis et al. 2006 ), there appears to be no such computational resource for the GSS-RS model."}, {"section_title": "Restricted maximum-likelihood estimation", "text": "While ML estimates are asymptotically unbiased in statistical theory, for random-effects models it is known that ML parameter estimates can often retain a substantial bias for seemingly large yet finite samples. The recommended improvement of restricted maximumlikelihood (REML) estimates has now become standard practice (Searle et al. 1992) . REML estimation transforms the data linearly so as to remove the fixed-effects parameters, leaving the variance components to be estimated through the covariance structure of the transformed data.\nREML estimates are formed by using a linear transformation of the observations that has as a mean a vector of 0's. The procedure eliminates uncertainty in the estimate of the mean vector from the estimation of the variance components; the information in the data is concentrated toward estimating parameters in a variance-covariance matrix. One REML transformation of the observations in replicated sampling can be defined as follows. Multiply each element in the vector y t (t \u00bc 1, 2, . . . , q) by p t\u00c01 (the size of the previous vector), and then subtract the sum of the elements in the previous vector y t\u00c01 , that is,\n0 . Then the transformed observations in w 1 , w 2 , . . . , w q , stacked into a column vector, are from a multivariate normal distribution with a mean vector of 0 and a variance-covariance matrix having elements that depend on the unknown parameters (derived and displayed in the Appendix). Numerical maximization is required but is straightforward with matrix-based programming languages (Appendix).\nML estimates remain important, for calculating confidence intervals based on profile likelihoods, and for likelihood-based model selection. In particular, model selection with the Akaike information criterion (AIC; Akaike 1973 , Sakamoto et al. 1986 ) and its variants requires maximized log-likelihood functions for all the models under consideration, using the same data. The likelihood used for REML estimates applies to the transformed observations (w it 's) and is not comparable to likelihoods for other models fitted to the untransformed observations (y it 's).\nWe compared ML estimation and REML estimation with computer simluation (see Simulations, below)."}, {"section_title": "Disaggregating data", "text": "Although replication of samples might seem onerous or prohibitively costly, it might not be in practice depending on sampling protocols. In some situations such replication might already have been accomplished unwittingly, and no extra data need be collected. A notinfrequent practice in population-monitoring studies is to aggregate subsamples, say of areas or transect portions, into one overall estimate of population abundance. The subsamples may be legitimate replicates. Other studies, such as the BBS in North America or the Common Birds Census (CBC) in the United Kingdom, feature simultneous estimates from spatially separated samples of possibly the same populations. Whether such samples can be considered replicates is a scientific judgment to be made case by case, but if the samples can be analyzed as replicates, considerable gains in parameter estimation might be realized. The question is, how much information about model parameters might be gained by retaining and analyzing the subsamples as replicated samples rather than aggregating them?\nThe GSS-RS model can be used to study the effects of disaggregating data into replicated samples. Suppose for example there are the same number, p, of samples each sampling time, and suppose that the sample observations Y 1t , Y 2t , . . . , Y pt arise from a GSS-RS model with parameters a, c, r 2 , and s 2 . With lognormal sampling error, it would be natural to combine data on the logarithmic scale in order to obtain an unbiased estimate of X t , and so we define the aggregated population estimate as the sample mean of the log-abundances in the subsamples:\nIt is straightforward to show that\u0232 0 ,\u0232 1 , . . . ,\u0232 q follow a univariate GSS model with parameters a, c, r 2 , and reduced sampling variance s 2 /p. Thus, parameter estimates obtained with the disaggregated observations can be contrasted with the estimates of the same parameters obtained with the aggregated observations. We view results of a computer simulation in the next section."}, {"section_title": "SIMULATIONS", "text": "Data sets simulated with the Gompertz state-space, replicated sampling (GSS-RS) model show the striking effects of sampling variability (Fig. 1) . Each simulation in Fig. 1 has two observations per sampling time. The numerical values of the parameters a, c, r 2 , and s 2 used in the first simulation (Fig. 1A) were maximumlikelihood (ML) estimates for a time series in the Breeding Bird Survey, and so the values reflect an actual field setting (see Dennis et al. 2006 : Table 1 and Fig. 1 ). The original time series, consisting of counts of American Redstart (Setophaga ruticilla) at a BBS location, had one observation at each sampling time. The ML estimates suggest that the larger proportion of the variability in the counts was due to observation error; the estimate of the sampling-variability parameter s 2 is twice as large as that of the process variability parameter r 2 . Although the simulated true population abundances (solid line) show considerable variability, the observations (circles) in the top panel of Fig. 1A are by comparison widely scattered. Decreasing the value of s 2 to half of the value of r 2 (Fig. 1B) reduces the sampling-based scattering, but the graph still gives the visual impression that observation error is obscuring the population signal. The observations resemble the true population abundances when the value of s 2 is decreased to one-tenth the value of r 2 (Fig. 1C) What can replicated sampling contribute to parameter identifiability? Profile log-likelihoods for parameters of the GSS-RS (replicated sampling) model suggest that replicated sampling essentially fixes the estimation problems (Fig. 2) . The profile log-likelihoods in Fig. 2 , calculated using the simulated data plotted in Fig. 1A , are unimodal and approximately parabolic. The parabolic shapes indicates that the distributions of the ML estimates are nearly normal, suggesting that the estimates are converging swiftly to the theoretical asymptotic normal distributions (e.g., Pawitan 2001) for ML estimates. The parabolic shapes also suggest that the parameters are statistically identifiable. In addition, the parabolic shapes render the ML estimates easy to calculate with numerical optimization techniques. The locations of the profile peaks are somewhat far from the true values of the parameters, however, suggesting the presence of a bias in the ML estimates. The bias likely could be corrected with parametric bootstrapping (Ospina et al. 2006) or penalized likelihood (Pawitan 2001) ; alternatively restricted maximum-likelihood (REML) estimates can be used (simulations area described later in this section). The R program in the Supplement to this paper produces ML and REML estimates as well as profile log-likelihood plots for the GSS model; the program contains the example data and reproduces the plots in Fig. 2 .\nHow much information is gained toward parameter estimation from replicated sampling? We calculated profile log-likelihoods for the GSS model for two simulated data sets, one set having 20 sampling times (q \u00bc 19 \u00bc the number of sampling times minus 1) with two replicated observations per sampling time, and the other set having 40 sampling times (q \u00bc 39) with just one replication per sampling time (Fig. 3) . The data sets conceptually represent the same amount of expended sampling effort. The difference is striking. The longer univariate time series (dashed line) produced wider profiles with extra shoulders or maxima, while the shorter time series with replicated samples (solid line) produced profiles with regular parabolic shapes. Thus, the same total sampling effort spent in replicated sampling can produce better information about the parameters in less time.\nWe simulated ML and REML estimation under replicated sampling (Fig. 4) . The simulations incorporated two replicate samples for each sampling time, with lengths of 31 (q \u00bc 30; Fig. 4A ) and 101 (q \u00bc 100; Fig. 4B ) sampling times. The 101 sampling times, while unrealistically long in current ecological research, allow assessment of statistical convergence. The parameters from generated from the GSS-RS model, and 2000 sets of parameter estimates were obtained by fitting the model (i.e., numerically maximizing the likelihood or restricted likelihood) to the generated data. Each box plot depicts 2000 parameter estimates divided by the reference parameter value, so that relative variability and bias can be compared across different parameters. For 31 sampling times, substantial median biases were evident in the ML estimates of a and c (Fig. 4A) . The ML estimates of a were biased upward and were substantially more variable than the ML estimates of the other parameters. The ML estimates of c were biased downward but had relatively small variability. The biases in ML estimates of a and c persisted for 101 sampling times (Fig. 4B) . It has been noted (Dennis et al. 2006 ) that the GSS model is a type of random-effects model, and ML estimation in such models often has persistent finite-sample bias due to the dependence of the observations. However, the near-parabolic shapes of the log-likelihoods in the GSS-RS model (Fig. 2) indicate statistical regularity and suggest that the biases could be routinely corrected by bootstrapping. In addition, ML estimates of certain functions or transformations of the parameters in the GSS model, such as the stationary mean a/(1 -c), turn out to be nearly unbiased (Dennis et al. 2006) .\nREML estimates, in contrast to ML estimates, were well centered. Little median bias was evident in REML estimates of any parameters for either 31 sampling times (Fig. 4A) or for 101 sampling times (Fig. 4B) . The variability of the REML estimates for 101 sampling times was of course reduced from that of 31 sampling times, although the relatively modest amount of reduction seems disproportionate to the large increase in sample size. Such slow gains from increasing sample sizes are common in dependent data problems. The greatest gains tend to occur by altering sampling protocols, such as in the present case by taking two samples instead of one at each sample time.\nIf samples have been aggregated into one population estimate, what is the effect of disaggregating them into replicated samples? Note that disaggregation is a much different question than the question of whether to undertake additional samples. Aggregating samples holds out the promise of reducing the sampling variability in a univariate time series; recall that the sampling-variability parameter becomes s 2 /p (where p is the number of samples in each sampling time) in the GSS model. Can better parameter estimates instead be obtained by disaggregating, that is, by simply analyzing the data differently? We generated data under the GSS-RS model, with two replicate samples per sampling time, using the parameter values from Fig. 1A . Each of the replicate sample pairs were then aggregated as sample averages on the logarithmic scale, forming a univariate time series. We compared parameter estimates obtained by fitting the GSS-RS model (parameters a, c, r 2 , and s 2 ), to the replicated sampled with estimates obtained by fitting the GSS model (parameters a, c, r 2 , and s 2 /p, where p \u00bc 2) to the aggregated samples. Fig. 5 shows logprofile likelihoods calculated for a, c, r 2 , and s 2 under both the GSS-RS (solid curves) and GSS (dashed curves) approaches, for a representative simulated data set. For each parameter, using the replicated samples and the full GSS-RS model produces steeper profiles, indicating greater information in the data. In the cases of a and c, the GSS-RS profiles are only slightly steeper. Note, however, that the GSS profiles have ''shoulders'' indicating the possibility of another local maximum. The shoulders in the GSS profiles for r 2 and s 2 are pronounced. The GSS-RS profiles for r 2 and s 2 are considerably steeper than the GSS profiles, showing that disaggregating into replicated samples is especially advantageous for sorting out the sources of variability in population data. Fig. 5 and Fig. 3 portray different concepts, although they look similar. While Fig. 3 contrasts taking one sample rather than two, the second contrasts pooling two replicated samples rather than keeping them disaggregated."}, {"section_title": "DISCUSSION", "text": ""}, {"section_title": "Improvement in inferences", "text": "Replicating the samples potentially offers a substantial gain of information in return for the costs involved. Disentangling sampling variability and process variability in unreplicated time series is difficult (Dennis et al. 2006) . Very long time series are necessary for clean separation of the two variance components. The ridgeshaped, multimodal likelihood functions require caseby-case attention, and the analyses are hard to automate for bootstrapping or processing multiple data sets. Estimation of the other parameters in population models, and functions of parameters, depends on adequate estimation of sampling variability. Thus, the inferences about the populations, such as whether they are declining, growing, or stable, and whether changes in the population processes have occurred, are degraded without adequate information about the contribution of sampling variability. In particular, estimates of firstpassage properties such as persistence probabilities in population viability analysis are known to be sensitive to sampling variability (Holmes 2001, Holmes and Fagan 2002) .\nMost noteworthy about our replicated-sampling simulations was that they were numerically well behaved. Even for 31 sampling occasions, the profile log-likelihood functions were smooth and unimodal (Fig. 2) . The log-likelihood shapes indicate that replicated samples contain sharper information for estimating the parameters, in particular for separating the contributions of process noise and observation error."}, {"section_title": "Alternative model forms", "text": "Many alternatives to the Gompertz state-space, replicated sampling (GSS-RS) model exist that could accommodate replicated sampling. Alternative statespace models would incorporate changes in either the sampling component, or the population abundance component, or both.\nA state-space model could use, for instance, a Poisson sampling model, in which counts Y 1t , Y 2t , . . . , Y ptt at time t arise from independent Poisson distributions, each with mean kN t , where k is a parameter and the underlying population abundance N t is governed by some stochastic population model. The probability of a count y under Poisson sampling is given by However, many field situations are characterized by heterogeneous sampling conditions leading to overdispersion in the observations. The lognormal sampling component in the GSS-RS model is a model of overdispersed sampling, but it is a continuous distribution in which 0's are not allowable outcomes. An alternative overdispersed sampling model for count data is the negative binomial distribution. One formulation of the negative binomial for state-space modeling takes the probability of a count of y to be\nfor outcomes y \u00bc 0, 1, 2, . . . , where a and b are positive parameters and C(\u00c1) is the gamma function. This negative binomial model differs from the above-mentioned Poisson sampling model by allowing k to have a gamma distribution with probability density function\ne \u00c0bk /C(a). Link and Sauer (1998) apply the negative binomial sampling model toward estimating population trends in various Breeding Bird Survey time series, although they do not describe the underlying process with a dynamic population model.\nAlternatives for the population-abundance component include various forms of density dependence and process noise, such as stochastic versions of the Ricker, Beverton-Holt, theta-Ricker, or Hassell growth models (Bellows 1981) . While extensive analyses undertaken with thousands of time-series data sets (Woiwod and Hanski 1992 , Zeng et al. 1998 , Sibly et al. 2005 , Brook and Bradshaw 2006 have suggested that particular model forms describe nature more often, none of these analyses have adequately accounted for sampling variability in the data. Certainly, no replicated sampling studies are yet known that might allow careful evaluation of different forms for both the sampling component and the population-abundance component.\nOne potential advantage of some alternative models is that the weak identifiabilities of the sampling and process variabilities might be sidestepped. For instance, if one can justify using the above Poisson sampling model with k \u00bc 1, then an entire parameter is eliminated. Similar structures can be built into process variability to bypass its estimation (e.g., Sullivan 1992 , Newman 1998 , Besbeas et al. 2002 . However, such approaches if used should be based firmly on the biological and methodological properties of the system at hand, and not contrived for convenience. Too few parameters can be as misleading as too many parameters, in that bias can be large when models are misspecified."}, {"section_title": "Computing for alternative models", "text": "Alternative models to the GSS-RS, in which either the population-abundance component or the sampling component is altered, pose additional computing problems for parameter estimation. The essential problem is that the likelihood function in such ''hierarchical models'' (models with random components, in this case, population abundance N t ) is a multidimensional integral that generally cannot be written down in closed form. The Gompertz growth model and the lognormal sampling model conveniently combine on the logarithmic scale to produce a multivariate normal likelihood, but no such consolidation occurs for nonnormal models. The recent ''data cloning'' algorithm (Lele et al. 2007 ) is a promising method for calculating ML estimates in hierarchical models, including state-space models. The data cloning method re-directs the computer-intensive Bayesian MCMC algorithms, ordinarily used for calculating posterior distributions in Bayesian statistics, toward calculating ML estimates and asymptotic frequentist confidence intervals. Other frequentist approaches for state-space models (de Valpine and Hastings 2002 , de Valpine 2003 , Ionides et al. 2006 ) are computer intensive as well. Alternatively, full Bayesian inferences can be undertaken (see Clark and Bj\u00f8rnstad 2004) , provided one understands the substantial conceptual differences between the Bayesian and frequentist approaches (Dennis 1996 , Lele and Dennis 2009 ). We point out that the references cited in this subsection all contain numerically computed examples of alternative, realistic state-space models."}, {"section_title": "Correlated observations", "text": "Situations exist for which the observations in replicated samples might be correlated. Some examples are: (1) if each year a different observer performs the sampling, but within each year replicated observations receive a bias from that observer's technique; (2) observation conditions vary substantially from year to year, but are similar for the within-year replicated samples performed close together in time; and (3) sampling protocols vary from year to year. The main feature of such correlation is that a different sampling bias is added each year essentially as a random effect. If the replications are modeled as independent when sampling correlation is present, the result could be spuriously high accuracy in estimation. One potential advantage of aggregation might be to avoid such sampling correlation. Alternatively, the sampling correlation could be modeled. Modeling correlation between replicated samples is straightforward, but estimation has not been studied. One might anticipate that difficulties with parameter identifiability could arise. Investigators at present should try to avoid sampling correlation by designing the replications to avoid such within-year biases. Ideally, all possible sources of variability in sampling should be embodied in each replicate."}, {"section_title": "Concluding remarks", "text": "The importance of biological monitoring data cannot be over emphasized. A high-quality data set recording a population's abundances, long-term, can be a canary in the mine. The responses of ecological communities to global climate change are projected to be large, fast, and extensive (for instance, Rehfeldt et al. 2006) . Protecting species and communities from extinction, and sustaining the ecological services derived by humans from the earth's biota, will require major modifications to human economic activities. Policy decisions about economic activities will hinge on the changes observed in biological monitoring data and on the scientifically inferred causes of those changes.\nYet, inadequate attention to the design of a monitoring program risks losing the very signal that the program exists to monitor. Ecologists now know, for instance, that conventional ecological sampling methods can contribute more than enough variability to cloud or bias conclusions about population abundance and dynamics. Replicating samples may be expensive or not, depending on whether existing sampling designs can be disaggregated. In either case sample replication should be considered as a survey design issue where the goal is to maximize the amount of useful information gained within the constraints of allowable cost. As managers, let us not waste money or the future of the species in our charge. We suggest checking the canary again, using replicated sampling."}]