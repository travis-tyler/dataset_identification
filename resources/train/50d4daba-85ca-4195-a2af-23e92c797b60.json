[{"section_title": "Abstract", "text": "As a principled method for partial correlation estimation, sparse inverse covariance estimation (SICE) has been employed to model brain connectivity networks, which holds great promise for brain disease diagnosis. For each subject, the SICE method naturally leads to a set of connectivity networks with various sparsity. However, existing methods usually select a single network from them for classification and the discriminative power of this set of networks has not been fully exploited. This paper argues that the connectivity networks at different sparsity levels present complementary connectivity patterns and therefore they should be jointly considered to achieve high classification performance.In this paper, we propose a subject-adaptive method to integrate multiple SICE networks as a unified representation for classification. The integration weight is learned adaptively for each subject in order to endow the method with the flexibility in dealing with subject variations. Furthermore, to respect the manifold geometry of SICE networks, Stein kernel is employed to embed the manifold structure into a kernel-induced feature space, which allows a linear integration of SICE networks to be designed. The optimization of the integration weight and the classification of the integrated networks are performed via a sparse representation framework. Through our method, we provide a unified and effective network representation that is transparent to the sparsity level of SICE networks, and can be readily utilized for further medical analysis. Experimental study on ADHD and ADNI data sets demonstrates that the proposed integration method achieves notable improvement of classification performance in comparison with methods using a single sparsity level of SICE networks and other commonly used integration methods, such as Multiple Kernel Learning. \nAs a principled method for partial correlation estimation, sparse inverse covariance estimation (SICE) has been employed to model brain connectivity networks, which holds great promise for brain disease diagnosis. For each subject, the SICE method naturally leads to a set of connectivity networks with various sparsity. However, existing methods usually select a single network from them for classification and the discriminative power of this set of networks has not been fully exploited. This paper argues that the connectivity networks at different sparsity levels present complementary connectivity patterns and therefore they should be jointly considered to achieve high classification performance.\nIn this paper, we propose a subject-adaptive method to integrate multiple SICE networks as a unified representation for classification. The integration weight is learned adaptively for each subject in order to endow the method with the flexibility in dealing with subject variations. Furthermore, to respect the manifold geometry of SICE networks, Stein kernel is employed to embed the manifold structure into a kernel-induced feature space, which allows a linear integration of SICE networks to be designed. The optimization of the integration weight and the classification of the integrated networks are performed via a sparse representation framework. Through our method, we provide a unified and effective network representation that is transparent to the sparsity level of SICE networks, and can be readily utilized for further medical analysis. Experimental study on ADHD and ADNI data sets demonstrates that the proposed integration method achieves notable improvement of classification performance in comparison with methods using a single sparsity level of SICE networks and other commonly used integration methods, such as Multiple Kernel Learning."}, {"section_title": "Introduction", "text": "Functional Magnetic Resonance Imaging (fMRI) has emerged as a powerful tool to investigate the functional connectivities between different regions of human brain. A large amount of the investigations center around constructing and analyzing functional brain networks based on restingstate fMRI (rs-fMRI). Rs-fMRI focuses on the low frequency (<0.1Hz) oscillations of blood-oxygen-level-dependent signal which presents the task-free neuronal activation patterns of brain regions [1, 2, 3, 4] . Rs-fMRI based brain network analysis holds great promise for brain disease diagnosis [5] . For example, Attention Deficit Hyperactivity Disorder (ADHD) affects at least 5-10% of school-age children and, unfortunately, there does not exist well-known biological diagnosis. Recent studies have found increased connectivities between the right pulvinar and bilateral occipital regions and reduced connectivities between bilateral pulvinar and right prefrontal regions for ADHD patients in comparison with healthy controls [6, 7, 8] . Another example is Alzheimer's disease (AD). Compared with the healthy, AD patients have been found decreased functional connectivity between hippocampus and other brain regions [9] .\nEmail addresses: jz163@uowmail.edu.au (Jianjia Zhang), lupingz@uow.edu.au (Luping Zhou), leiw@uow.edu.au (Lei Wang) With these disease-related connectivity differences, diagnosis of brain diseases could be achieved by modeling and classifying brain connectivity networks.\nIn recent years, several methods have been proposed to model brain connectivity networks based on the co-varying patterns of rs-fMRI time series across brain regions. The network modeling can be roughly split into two stages: i) identifying network nodes, and ii) inferring the functional connectivity between nodes. The network nodes are often defined as anatomically separated brain regions of interest (ROIs) or alternatively as latent components in some data-driven methods, e.g. independent component analysis [10] and clustering-based methods [11, 12] . Once network nodes are identified, the functional connectivity strength between a pair of nodes is conventionally measured as the correlation of the averaged time series across the brain regions associated with the two nodes [13] . In this way, the brain network is represented by a correlation matrix. Recently, it has been argued that compared with correlation, partial correlation could be a better choice since it regresses out the effects from all other nodes [14, 15] . This often results in a more accurate estimate of network structure in comparison with those correlation-based methods. Sparse inverse covariance estimation (SICE) is a principled method for partial correlation estimation, and it often produces a stable estimation with the help of the sparsity regularization [16] . The resulting brain network modeled by SICE method is an inverse covariance matrix, and each of its off-diagonal entries corresponds to the partial correlation between two nodes.\nPrevious studies have shown the great potential of SICE networks for analyzing brain connectivity patterns [17, 18, 19, 20] and diagnosing brain diseases [21, 22, 23, 24] . For each subject, the SICE method naturally produces a set of connectivity networks by specifying different sparsity regularization parameters. Monotonic increase of this parameter would lead to gradually sparser networks. Sparse connectivity networks present the stronger connectivities while dense ones also include weaker connectivities. In this case, at which sparsity level the SICE network should be used for classification becomes a critical issue, because different connectivity patterns could possess different discriminative power. A common practice is to select one sparsity level from this set of networks [20, 22, 23] and ignore other sparsity levels. However, there are at least two drawbacks with this approach: 1) It does not fully exploit the information contained in these ignored sparsity levels; 2) The selection of the most appropriate sparsity level is usually carried out by multi-fold cross-validation, which often has high computational complexity and becomes unreliable when the sample size is small.\nWe argue that SICE networks with different sparsity levels could provide complementary information that is of great value for classifying the SICE networks. They should be jointly considered in order to improve the discriminative power. Also, this will circumvent the rigid selection of a single sparsity level. To this end, a straightforward method might be to concatenate the features extracted from these SICE networks. However, this method suffers from two limitations: 1) not all the extracted features are sufficiently discriminative; and 2) this method treats each network separately and indiscriminately, failing to integrate them in an inherent and adaptive manner. Another appealing method may be multiple kernel learning (MKL). However, although MKL combines multiple kernels from different channels, it ignores the inter-channel information, as will be explained in Section 3.5.1.\nIn this paper, we propose a learning based framework that integrates a set of SICE networks with the aim of attaining more discriminative power. Our framework has at least four contributions.\n1. It makes use of the whole spectrum of SICE matrices at different sparsity levels, which not only improves the classification performance, but also circumvents the need of presetting the employed sparsity level. 2. The proposed framework provides subject-adaptive integration of SICE networks. It is noticed that some sparsity levels that are useful for one subject could become less useful for another due to the variation of subject-specific characteristics, e.g., disease phase, age and gender. In this case, we allow the integration to be subject-adaptive, and achieve this through a sparse representation framework. 3. Our integration of SICE networks respects the specific geometric property of SICE matrix. As known, SICE matrices are symmetric position definite (SPD) and form a Riemannian manifold [25, 26] , which makes a linear combination of them in the input (Euclidean) space improper. To address this issue, we propose to embed the Riemannian manifold into a reproducing kernel Hilbert space (RKHS), where linear operations become sufficiently good to handle SICE matrices. This embedding is achieved through SPD-matrix based kernels, such as the Stein kernel [27] . Following that, a linear combination of multiple SICE networks is optimized in this kernel induced feature space through the sparse representation framework mentioned above. 4. By the integration, our learning framework provides a unique, enhanced, and new network representation for each subject. Although the integration takes place in a kernel induced feature space, it is feasible to project the integration result back into the original network space for visualization and further medical analysis. This could help to understand the underlying pathophysiology of brain diseases.\nThe rest of the paper is organized as follows: Section 2 reviews the SICE algorithm, the properties and measurements of SICE networks and the sparse representation technique. Section 3 details the proposed subject-adaptive integration method and discusses several issues regarding the proposed method. Section 4 presents the experimental results on ADHD and AD-NI rs-fMRI data sets. And Section 5 concludes this paper."}, {"section_title": "Related Work", "text": ""}, {"section_title": "Constructing Brain Networks Using SICE", "text": "Let {x 1 , x 2 , \u00b7 \u00b7 \u00b7 , x M } be a time series of length L, where x i is a d-dimensional vector, corresponding to an observation of d brain nodes. Following the literature of SICE [16, 21] , x i is assumed to follow a Gaussian distribution N(\u00b5, \u03a3). Each offdiagonal entry of \u03a3 \u22121 indicates the partial correlation between two nodes by eliminating the effect of all other nodes. \u03a3 \u22121 i j will be zero if nodes i and j are independent of each other when conditioned on other nodes. In this sense, \u03a3 \u22121 i j can be interpreted as the existence and strength of the connectivity between nodes i and j. The estimate of \u03a3 \u22121 , denoted as S, can be obtained by maximizing the penalized log-likelihood over positive definite matrix S (S 0) [16, 21] :\nwhere\u03a3 is the sample-based covariance matrix; det(\u00b7), tr(\u00b7) and || \u00b7 || 1 denote the determinant, the trace and the sum of the absolute values of the entries of a matrix, respectively. Here ||S|| 1 imposes sparsity on S to achieve more reliable estimation by considering the fact that a brain region often has limited direct connections with other brain regions in neurological activities. The tradeoff between the degree of sparsity and the loglikelihood estimation of S is controlled by the regularization parameter \u03bb. A larger \u03bb makes S * sparser. The maximization problem in Eq. (1) can be efficiently solved by the off-the-shelf packages, such as SLEP [28] . In the following, we may use SICE matrix to refer the SICE network estimated by Eq. (1). \n. It forms a closed, self-dual convex cone, which is a Riemannian manifold [27] . A set of SICE networks with various sparsity can be obtained for a single subject by varying \u03bb (in Eq. (1)), as illustrated in Figure 2 (a). The operations and algorithms that are developed for Euclidean spaces can not be directly applied to SPD matrices residing on such a manifold [26] . For example, the average of two SPD matrices could result in an undesirable swelling effect [25] : the determinant of the Euclidean mean of two SPD matrices can be larger than the determinants of the original two matrices, which is physically unrealistic in many applications, such as diffusion tensor interpolation. To address this issue, methods that respect the manifold geometry have been developed [25] . Kernel method is one of the most effective methods and has been adopted in [29, 26, 30] to measure the similarity between SPD matrices. It implicitly maps the Riemannian manifold of SPD matrices onto a high-dimensional kernel-induced feature space F , where linear algorithms developed for Euclidean spaces can be applied. At the same time, the manifold structure is well incorporated into the mapping by utilizing distance functions that are specially designed for SPD matrices. In addition, kernel methods are often computationally efficient."}, {"section_title": "Kernel functions for SICE networks", "text": "Various kernel functions have been proposed to map the Riemannian manifold of SPD matrices onto a feature space F , such as Cholesky kernel [31] , Log-Euclidean kernel [25, 26] etc. A recently proposed Stein kernel [27] has shown promising performance in a variety of applications [29, 32, 33, 34] . It is expressed as\nwhere \u03b8 is a tunable positive scalar. S (X, Y) is called SDivergence and it is defined as\nThe S-Divergence has the following desirable properties: 1) It is invariant to affine transformations applied to the input SPD matrices, such as rotation, scaling; 2) The square-root of SDivergence is proven to be a metric on Sym \n2 , +\u221e). Readers are referred to [27] for more details.\nBesides Stein kernel, there are other commonly used SPD kernels, including Cholesky kernel [31] , Euclidean kernel [31] , Log-Euclidean kernel [25, 26] , and Power Euclidean kernel [31] . All the four kernels share the same form of:\nwhere d(\u00b7, \u00b7) is a kind of distance between two SPD matrices. Different definitions of d(\u00b7, \u00b7) lead to different kernels. The distance functions in the four kernels are introduced as follows.\n\u2022 Cholesky distance. Cholesky distance measures the difference between S i and S j by\nwhere chol(S) is a lower triangular matrix with positive diagonal entries obtained by the Cholesky decomposition of S, that is, S = chol(S) chol(S) and || \u00b7 || F denotes the Frobenius matrix norm.\n\u2022 Euclidean distance. Euclidean distance between S i and S j is given by\n\u2022 Power Euclidean distance. Power Euclidean distance between S i and S j is given by\nwhere p is a non-zero scalar. Note that S, as a SPD matrix, can be eigen-decomposed as S = U\u039bU , and S p can be easily computed by: S p = U\u039b p U .\n\u2022 Log-Euclidean distance. Log-Euclidean distance is defined as\nwhere log(S) = U log(\u039b)U and log(\u039b) applies logarithm to each diagonal element of \u039b to obtain a new diagonal matrix. \nThe properties of all these SPD kernels mentioned above are summarized in Table 1 . All the kernel functions in this table will be investigated in the experimental study."}, {"section_title": "Sparse Representation", "text": "Sparse representation [35] of signals have received intensive attention over the last decade due to its effectiveness in modeling signals and the robustness in dealing with corrupted data. Sparse representation aims to search for the sparsest representation of a signal by using a linear combination of atoms in an overcomplete dictionary. Specifically, given a signal x \u2208 R n and a set of atoms\nn as a dictionary, the idea of sparse representation is to reconstruct signal x by using a few atoms in dictionary D. The sparse representation coefficient of x, denoted as \u03b1 = [\u03b1 1 , \u03b1 2 , \u00b7 \u00b7 \u00b7 , \u03b1 N ] , can be obtained by solving the following problem:\nwhere the first term of Eq. (9) is the reconstruction error of x, and the second term is used to control the sparsity of the coefficient \u03b1. The parameter \u03b3 is the tradeoff parameter used to balance the reconstruction error and sparsity. A larger \u03b3 leads to a sparser solution. Kernel-based sparse representation (K-SR) [36] extends the concept of sparse representation from a vector space to a kernel-induced feature space F . KSR shares the same form of Eq. (9) except that both the signal and the atoms become the mapped features in F ."}, {"section_title": "Proposed method", "text": ""}, {"section_title": "Problem Formulation", "text": "As previously mentioned, the SICE representation naturally leads to a set of networks with different sparsity levels. Each sparsity level of the network captures specific connectivity patterns, which are different from that of other sparsity levels. Therefore, when the final aim is to classify different groups of subjects, such as subjects with ADHD versus healthy controls, multiple sparsity levels of connectivity networks should be jointly considered and integrated. By doing so, the complementary and discriminative information of the connectivity patterns from multiple sparsity levels could be fully explored to boost the classification performance.\nTo integrate multiple sparsity levels of SICE networks, a natural approach is linear combination. However, as previously mentioned, linearly combining manifold data in a Euclidean space fails to respect the non-linear structure of the Riemannian manifold and could lead to spurious result, such as the swelling effect [25] . Accounting for the manifold structure of SICE networks, we propose a method that conducts such an integration in a kernel-induced feature space F . Firstly, multiple sparsity levels of SICE networks from the same subject are mapped into F by a non-linear mapping function \u03c6(\u00b7) : Sym\nThis \u03c6(\u00b7) mapping brings at least two advantages. 1) The Riemannian manifold geometry has been well considered by using distance functions specially designed for SPD matrices, such as the Stein divergence; 2) The images of SICE matrices can be linearly processed in the kernel-induced feature space F . After the mapping, we utilize a convex combination to integrate multiple \u03c6-mapped SICE networks in F . Convex combination constrains the resulting network to lie in the convex hull of these \u03c6-mapped SICE networks, implicitly making the solution better comply to the local distribution of these networks. Specifically, let\n, denote the SICE networks of one subject at M different sparsity levels and f (X) denote the combined network of X with implicit function f . The convex combination is carried out in the following manner:\nwhere\nis the combination coefficient of function f . Instead of applying a uniform \u03b2 to all the subjects, we assign a subject-adaptive \u03b2 for each subject. We argue that the discriminative power of SICE networks at different sparsity levels may not necessarily be uniform across all the subjects. This can be intuitively understood by the following considerations. 1) Although sharing the same class label, the patient subjects could be experiencing different stages of the disease. As a result, disease-induced alterations in the brain networks will be different across subjects.\n2) The brain networks may demonstrate differences across subjects due to different age, gender, etc. Thus, we propose to adaptively optimize \u03b2 for each subject to handle such variation as follows.\nTo solve \u03b2, this paper extends the kernel-based sparse representation (KSR) [36] . The original KSR is used to obtain the sparse representation coefficient \u03b1 only. In this paper, we develop it to optimize the integration coefficients \u03b2 and the sparse representation coefficient \u03b1 simultaneously. The proposed method is called sample-adaptive SICE network integration (SASNI).\n, denote a dictionary with N SPD matrices. Note that X i and D j are both SICE matrices, however, X i denotes the i-th SICE matrix of a test subject while D j denotes the j-th atom of the dictionary, which is formed by SICE matrices from training subjects in this paper. The objective function of SASNI is formulated as follows.\narg min\nThe first term of Eq. (11) is the reconstruction error of \u03a6, and the second term is used to control the sparsity of \u03b1. Figure 2 illustrates the proposed method: (a) a set of SICE networks with various sparsity (denoted by green dots) residing on a Riemannian manifold can be obtained for a single subject; (b) the SICE networks are mapped to a kernel-induced feature space F by using SPD kernels. Then the mapped networks in F are convexly combined as a unified representation (denoted by a red dot); (c) the unified representation is sparsely represented by the atoms (denoted by blue dots) in F . How to solve the combination coefficient \u03b2 and the sparse representation coefficient \u03b1 and how they can be used for classification will be elaborated in the following parts."}, {"section_title": "Solving The Optimization Problem", "text": "As known in kernel methods, the kernel mapping \u03c6 is usually too complicated to be explicitly computed. Therefore, we need to bypass the mapping \u03c6 in Eq. (11) . With the kernel property of k( A, B) = \u03c6( A) \u03c6(B), the first term in Eq.(11) can be expanded as:\nNote that, as shown in Eq.(12), the objective in Eq. (11) is a function of both \u03b1 and \u03b2. It can be proved that the objective function is jointly convex on both of \u03b1 and \u03b2. To find the optima for \u03b1 and \u03b2, we devise an iterative procedure that monotonically decreases the objective by alternately optimizing one variable at a time while keeping the other fixed. Specifically, at the t-th iteration, we first optimize the objective in Eq.(11) over \u03b1 with \u03b2 fixed. In this case, \u03b2 K(X, X)\u03b2 reduces to a constant and Eq.(11) can be rewritten as follows:\n13) The objective of Eq. (13) is a quadratic function and convex with respect to \u03b1. This optimization problem can be efficiently solved via existing packages, such as SPAMS [37] . After obtaining \u03b1 (t) , \u03b2 can be updated in a similar manner by optimizing the following problem:\nThis optimization problem can be solved by the reduced gradient descent method as in [38] to handle the convex constrain. \u03b1 and \u03b2 are optimized alternately until a stopping criterion is satisfied. There are two commonly used stopping criteria: i) a predefined threshold on the difference of consecutive objective function values is met; and ii) the maximum iteration number is reached. Note that as the original KSR [36] , there is no training stage in the proposed method, and the training subjects are used as the atoms in the dictionary D. \u03b1 and \u03b2 are optimized individually for each test subject, so they are adaptive for different test subjects. This subject-adaptivity of \u03b2 enhances the flexibility of SASNI to integrate multiple networks across subjects, and in turn improves the effectiveness of SASNI."}, {"section_title": "Classification Based on SASNI", "text": "With the obtained sparse representation coefficient \u03b1, a test sample can be classified in multiple manners. If the atoms in the dictionary are not associated with class labels, the sparse representation coefficient can be treated as a feature vector, and a classifier, such as support vector machine (SVM), can be trained with the feature vectors of training samples. The class label of a test sample is then predicted by feeding its feature vector to the trained classifier. If the atoms in the dictionary are associated with class labels, i.e., the atoms are just the training samples, the sparse representation coefficient can be directly used to classify a test sample through calculating class-specific residue. This approach is denoted as sparse representation based classification (SRC) in [35] and also used in this paper. SRC takes the assumption that the samples from the same class do lie in a subspace. In this case, any new (test) sample from the same class shall approximately lie in the span of the training samples and can therefore be sufficiently represented by the training samples from the same class. Then the classspecific reconstruction residue can be calculated by using the sparse representation coefficients and the training samples associated with each class. Eventually, the test sample is assigned to the class with the minimal class-specific reconstruction residue. SRC has been used in some generic image classification tasks, such as face recognition [35] and object categorization [29, 30] , and has demonstrated its effectiveness and high efficiency.\nSpecifically, in this paper, once the combination coefficients \u03b2 and the sparse representation coefficient \u03b1 are obtained by optimizing Eq.(11), a new (test) sample X will be assigned to the class with the minimum reconstruction residue. The reconstruction residue of \u03a6 for class i can be computed as follows:\nwhere l( j) is the label of the j-th atom D j , \u03b4(k) is the Kronecker delta function, which is one when k is zero, and zero otherwise,\n, with indicating element-wise product. Based on Eq. (15), the final label of X is determined as follows:\nAlgorithm 1 outlines the proposed method SASNI.\nAlgorithm 1 Proposed subject-adaptive SICE network integration (SASNI). Update Ob j t according to Eq.(11); 7: until t \u2265 T or | Ob j t\u22121 \u2212 Ob j t |\u2264 \u03c4;"}, {"section_title": "Projection Back to The Original Space", "text": "The integrated network provides a unified network representation for each subject and therefore it is worthy to visualize for future analysis. However, as shown above, the integration takes place in a kernel-induced feature space. As a result, it needs to be projected back into the original space. For this purpose, we use a kernel pre-image estimation method to recover the integrated network in the original input space. The idea is to model the pre-image, denoted as\u015c, of the integrated network \u03a6 by a linear combination of its neighboring SICE matrices in Sym + d , i.e.,\u015c = S j \u2208\u2126 w j S j , where \u2126 denotes a set of neighboring SICE networks and w j denotes the combination weight. Note that the neighboring SICE networks can be found by measuring similarities between \u03a6 and \u03c6 (S i ) in the kernel-induced feature space. The optimal w can be obtained by solving the following optimization problem using gradient descent based algorithms.\nThe objective function in Eq. (17) can be further reorganized into terms of kernel matrices as in Eq. (18) .\n3.5. Discussion 3."}, {"section_title": "Comparison between MKL and SASNI", "text": "Multiple kernel learning (MKL) has been regarded as a promising technique for integrating multiple data sources [39] . It can also be used to integrate multiple sparsity levels of brain networks. Here we discuss the similarity and difference between MKL and the proposed SASNI. The idea of MKL is to search for an integration of base kernel functions that maximizes a generalized performance measure, such as structural risk minimization [39] . Formally, let X = {X 1 , X 2 , \u00b7 \u00b7 \u00b7 , X M } denote M data sources. MKL uses the concatenation of mapped features as the integrated feature representation:\nwhere\nThe dot product in the integrated feature space gives the integrated kernel:\nIn contrast, the integration of SASNI is carried out in the following manner:\nand the dot product in the integrated feature space is:\nBy cross-referring Eq. (19) and Eq. (20), we can identify that: 1. The similarity of MKL and SASNI is that both of them adopt multiple data sources in a kernel-induced feature space F . 2. The difference between MKL and SASNI is that MKL only considers the similarity between samples from the same data source, i.e. X At the same time, note that the proposed method specially caters for brain network data where cross-source data can be compared, and MKL is a more general method for combining different data sources."}, {"section_title": "Convergence Analysis and Computational Complexity", "text": "As outlined in Algorithm 1, the optimization problem defined in Eq. (11) is solved by a commonly used alternate optimization strategy which alternately minimizes the objective function with respect to one of the two variables, i.e. the combination coefficient \u03b2 and the sparse representation coefficient \u03b1. Considering the two facts 1) these two variables are iteratively optimized to decrease the same objective function monotonically; and 2) the objective function is lower bounded by zero, the optimization problem defined in Eq. (11) is guaranteed to converge. Also, it can be proved that the objective function is jointly convex over both \u03b2 and \u03b1. Therefore, the final solution is guaranteed to be a global optimum. The experimental results show that employing this optimization strategy has already been able to achieve promising performance.\nThe main computational cost of the proposed method is twofold: kernel computation and parameter optimization. Table 1 . It can be seen that when the Stein kernel (SK) is used, the time complexity involved in calculating the kernel matrices K(X, X), K(D, X) and Therefore, the overall time complexity of SASNI for each test\n373 . In our experiment, a single subject can be classified in less than one second on a desktop computer with 3.6 GHz CPU and 32 GB memory. Table 1 ."}, {"section_title": "Non-subject-adaptive variants of SASNI", "text": "In SASNI, the integration coefficient \u03b2 is adaptively assigned to each subject to handle the variation of the utility of different sparsity levels across subjects. Then a question arises naturally: how the classification performance will be if a uniform \u03b2 is applied for all the subjects to integrate multiple SICE networks at different sparsity levels? To answer this question, we explore two non-subject-adaptive integration variants and conduct a comparison between them and the proposed subject-adaptive counterpart in our experiment (see Section 4.2.5). The first variant is to treat each sparsity level equally and set each entry of \u03b2 to 1/M, where M is the total number of sparsity levels to integrate. This method, called 'MeanC', can be intuitively understood as using the geometric center of the networks in the kernel-induced feature space F as the representation of the network set. The second method is to learn a fixed \u03b2 based on the training data set. To do this, the training data set is split into two folds, one fold as dictionary and the other fold as the validation set. A fixed \u03b2 is learned in a similar manner as introduced in Section 3. The only difference is the reconstruction error defined in Eq. (11) is calculated over the subjects in the validation set only. Therefore, the objective function becomes: arg min\nwhere P denotes the total number of subjects in the validation set. Eq. (21) can be solved in the same way as introduced in Section 3. Once a fixed \u03b2 is learned, it will be applied to all the subjects in the test sets to perform classification. This method is called 'FixedTrainC'."}, {"section_title": "Experimental Study", "text": ""}, {"section_title": "Data Preprocessing and Experimental Settings", "text": "Two rs-fMRI data sets are used to verify the effectiveness of the proposed method SASNI. One data set is ADHD-200 provided by the Neuro Bureau for differentiating Attention Deficit Hyperactivity Disorder (ADHD) from healthy control subjects. ADHD-200 consists of 768 training subjects and 197 test subjects 1 collected from eight independent imaging sites. The summary of this data set is provided in Table 2 . The rs-fMRI data are processed with Athena pipeline. Specifically, the first four echo-planar imaging (EPI) volumes are removed for signal equilibrium and then slice timing, orientation and motion correction are performed. Each rs-fMRI image is co-registered to T1 image and warped into MNI space at 4\u00d74\u00d74 mm 3 resolution. The time series of 90 brain nodes in gray matter are extracted from the preprocessed data using the automated anatomical labeling (AAL) [40] atlas. Detailed preprocessing descriptions and the processed time series are available at Neuro Bureau website 2 . The other data set is ADNI data set downloaded from the following website http://adni.loni.usc.edu with the aim of identifying Mild Cognitive Impairment (MCI), which is very early stage of Alzheimer's disease, from healthy controls. There are 38 healthy controls and 44 MCIs. The data are acquired on a 3 Tesla (Philips) scanner with TR/TE set as 3000/30 ms and flip angle of 80\n\u2022 . Each series has 140 volumes, and each volume consists of 48 slices of 64 \u00d7 64 dimensional image matrices at 3.31\u00d73.31\u00d73.31 mm 3 resolution. The preprocessing is carried out using SPM8 3 and DPARSFA [41] . The first 10 volumes of each series are discarded for signal equilibrium. Similar with ADHD-200, slice timing, head motion correction and MNI space normalization are performed. Participants with too 1 The labels of 26 subjects from Brown University in the test set are not released yet. They are not included in our performance evaluation.\n2 http://neurobureau.projects.nitrc.org/ADHD200/ Introduction.html 3 http://www.fil.ion.ucl.ac.uk/spm/software/ much head motion are excluded. The normalized brain images are warped into AAL atlas to obtain 90 ROIs as nodes. The ROI mean time series are extracted by averaging the time series from all voxels within each ROI and then band-pass filtered to obtain the most discriminative frequency band as in [42] . For both of the ADHD-200 and ADNI data sets, the functional connectivity networks are obtained by the SICE method using SLEP [28] . Nine sparsity levels of SICE matrices are obtained for each subject by setting \u03bb = [0.1 : 0.1 : 0.9]. For the ADHD-200 data set, the predefined training/test sets are used while a leave-one-out procedure is used for ADNI data set to make full use of the limited subjects. For both of the data sets, the dictionary is made up by all the SICE matrices of the training subjects. The parameters used in the classification tasks of these two data sets, including \u03b8 in the SPD kernels, \u03b3, and the regularization parameter of SVM are tuned by using five-fold cross-validation on the training set."}, {"section_title": "Experimental Result", "text": "This experiment consists of the following parts: data set Before applying the proposed method to integrating different sparsity levels of brain networks, we first evaluate the performance of a single sparsity level of brain network. In this case, the brain network of one subject is represented as a single SPD matrix, which can be used to train a classifier in multiple manners. The following three common manners are evaluated in this experiment. A straightforward way is to extract graphical features from the network, such as the local clustering coefficent (LCC) feature in [42] , and then train SVM with these features. LCC, as a measure of local neighborhood connectivity for a node, is defined as the ratio of the number of existing edges between the neighbors of the node and the number of possible connections between these neighbors [43] . In this case, LCC can map a network, represented by a d \u00d7 d adjacency matrix, to a d-dimensional vector, where d is the number of nodes in the network. The second manner is employing one of the five SPD kernels in Table 1 to directly evaluate the similarity between SICE matrices and adopt SVM classifier with these kernels to perform classification. The third manner is using SRC instead of SVM as the classifier with the same SPD kernel. Figure 3 shows the classification results of these methods on each of nine sparsity levels of SICE networks. As seen, the feature of LCC (labeled by 'SVM (LCC)') produces poor classification with most sparsity levels. When SPD kernels are used, either with SVM or SRC, the classification performance can be improved. In particular, the highest accuracy is achieved by SRC with the Stein kernel (SK), reaching over 69% on the 7th sparsity level.\nThis demonstrates that: 1) graphical feature of LCC does not sufficiently convey the discriminative information contained in SICE matrices; 2) SPD kernels can achieve reasonably good classification performance by considering the manifold property; 3) using SRC as the classifier and SK as the SPD kernel admits promising classification performance. The good performance verifies the effectiveness of SRC when dealing with brain network classification.\nIt is also worth noting that the discriminative power of different sparsity levels are not same. The sparser (4th-9th) levels are generally more discriminative than denser (1st-3rd) ones. This consolidates our motivation that combining the complementary information of different sparsity levels could benefit the classification performance."}, {"section_title": "Evaluation of the proposed SASNI on ADHD-200 data set", "text": "SRC with SK achieves the best classification performance when a single sparsity level is used. It is considered as a baseline and compared to the proposed SASNI method. In this experiment, we would like to investigate whether the classification performance can be improved by integrating multiple sparsity levels via SASNI. Note that, SK is also used as the SPD kernel in SASNI. As seen in Figure 4 , the yellow bars indicate the classification performance of SASNI when different sparsity levels of brain networks are integrated. The tick ' [1, n] ' on the x-axis means that the n densest levels of brain networks are integrated. Compared with SRC (with SK) using a single sparsity level, SASNI can consistently boost the classification performance for all integration settings. When the top four dense levels are integrated, SASNI achieves an accuracy more than 71%. When the top seven dense levels of SICE matrices are integrated, SASNI achieves an accuracy of over 72.5%, obtaining an improvement of three percentage points over SRC with the best single sparsity level.\nThis demonstrates that integration of multiple sparsity levels could attain more discriminative power and in turn improve the classification performance. As for which sparsity levels to be integrated, they can be selected by cross-validation on the training data set. An alternative way is just integrating all the sparsity levels since, as seen in Figure 4 , the performance of SASNI is insensitive to the combination range when more than six sparsity levels are integrated. To the best of our knowledge, in the literature, the highest classification accuracy on ADHD-200 data set is reported in [44] . That work computed the pairwise correlation of the time series between brain voxels to model the brain network and extracted multiple kinds of features from the network. Then a PCA-LDA based classifier is trained with the extracted features. Table 3 provides the classification performance obtained in the literature [44, 45] and by our proposed SASNI on the whole test set from all imaging sites. As seen, both the proposed SASNI and [44] outperform [45] by a large margin in terms of the overall classification performance. Our proposed SASNI method further exceeds [44] by about three percentage points on the overall test set. In particular, SASNI achieves better performance on OHSU and Peking test sets and perform equally on NeuroImage and NYU test sets in comparison with [44] . On the KKI and Pittsburgh test sets, the performance of SASNI is worse than that of [44] . Note that the number of subjects in K-KI and Pittsburgh test sets is only 11 and 9, respectively. The absolute difference is only 1\u223c2 subjects. On other larger test sets, SASNI consistently achieve better or equal performance in comparison with [44] . The classification performance of [45] on NYU and Pittsburgh data sets are not reported. Also note that the overall accuracy is the accuracy directly calculated over the 171 subjects from all the six data sets."}, {"section_title": "4.2.4.", "text": "Comparison with other integration schemes on ADHD-200 data set An experiment is carried out to compare the classification performance of the proposed SASNI method with other integration methods on ADHD-200 data set. These include MKL and a straightforward concatenation of LCC features from different sparsity levels (denoted as LCC in Figure 5 ). As reported in Figure 5 , SASNI consistently outperforms both LCC feature concatenation method and five MKL methods (using each of the five SPD kernel functions in turn) once the top three or more sparsity levels are integrated.\nAnother integration approach is a late fusion method where a mixture of classifiers are used and each one of them looks at only one source (a single value of \u03bb). Although extensive literature [46] has demonstrated that early fusion, e.g. feature selection and feature combination, often outperforms late fusion methods, e.g. major voting, we conduct additional experiments to compare the proposed SASNI, as an early fusion method, with two late fusion methods, including:\n1. Majority Voting. An SRC classifier is trained for each value of \u03bb and the final predicted label is determined by major voting; 2. 2-layer SVM. An SVM classifier is trained for each value of \u03bb and the prediction scores for multiple \u03bbs from the same subject are concatenated as a feature vector to train a second layer SVM for final classification. Top seven sparsity levels are used in this experiment. As seen in Table 4 , the proposed SASNI outperforms the other two late fusion methods by a large margin. "}, {"section_title": "4.2.5.", "text": "Comparison with non-subject-adaptive integration variants on ADHD-200 data set In this experiment, the proposed SASNI is compared with two non-subject-adaptive variants of SASNI, i.e. 'MeanC' and 'FixedTrainC' introduced in Section 3.5.3, to investigate the effectiveness of subject-adaptive integration. For 'FixedTrainC', the training data set is split into two equal-sized folds as the dictionary and the validation set, respectively. And the partition of the training data set is repeated 10 times to accumulate statistics. The averaged classification performance is reported in Figure 6 in comparison with that of the 'MeanC' and the proposed SASNI method. As seen, the SASNI method achieves the best classification performance in comparison with the two non-subject-adaptive integration methods. This verifies the advantage of the subject-adaptive mechanism. Note that each of the non-subject-adaptive integration methods outperforms SR-C using the best single sparsity level, which indicates that the improved performance of the proposed SASNI method is attributed to both integration of multiple sparsity levels and the subject-adaptive mechanism."}, {"section_title": "Evaluation of the proposed method on ADNI data set", "text": "As previously mentioned, a leave-one-out procedure is used in the classification on ADNI data set, and the averaged accuracy is reported. As seen in Figure 7 , the superiority of the proposed SASNI over a single sparsity level based SRC is confirmed again on ADNI data set. Specifically, when all the nine sparsity levels are integrated, SASNI outperforms the best single sparsity level based SRC by a large margin of over five percentage points."}, {"section_title": "Visualization of the integrated brain networks", "text": "An example of recovered pre-image is shown in Figure 8 . The 90 ROIs of AAL atlas are grouped into eight lobes according to the anatomical structure. As seen, evident block-wise structures are presented. This is expected since the ROIs in the same lobe have higher chances to be anatomically and functionally connected. Also, most of the functional abnormalities identified in the literature [45, 8] are within lobes. Especially, as indicated in Figure 8 , the most strongest connections include connections within frontal gyrus, e.g. Superior frontal gyrus dorsolateral-Middle frontal gyrus (4, 8) , Middle frontal gyrus orbital part-Inferior frontal gyrus opercular part (9, 11) and connections between Hippocampus (37\u223c40), Amygdala (41\u223c42) and Calcarine (43\u223c44). An interesting thing this figure presented is that the same parts in left and right brain are often closely functionally connected. For example, the connections between Inferior frontal gyrus orbital part in left brain and Inferior frontal gyrus orbital part in right brain (15, 16) , and the connection between Rolandic operculum in left brain and Rolandic operculum in right brain (17, 18) . This is probably due to the extensive cooperation between the left and right brain in many functions. Since the experimental study has demonstrated that the integrated network possesses more discriminative power in comparison with a network corresponding to a single sparsity level, the recovered pre-image in the original space may reveal more disease-related connectivity patterns. In this sense, the visualization of the integrated brain networks provides medical specialist a new perspective to conduct analysis of the brain networks and this may promote understanding the underlying pathophysiology of brain diseases."}, {"section_title": "Convergence Evaluation", "text": "As discussed in Section 3.5.2, the alternate optimization of the sparse representation coefficient \u03b1 and the combination coefficient \u03b2 is guaranteed to converge. Here, we would like to verify the evolution of the objective function defined in Eq. (11) . The evolution of the objective values averaged over all samples in the test set is plotted in Figure 9 . As seen, the objective is monotonically decreased by optimizing \u03b2 and \u03b1 alternately. Moreover, the objective value decreases significantly in the first few iterations and quickly becomes convergent. This result experimentally demonstrates that the proposed optimization method can be effectively and efficiently solved.\nSparsity levels [ Figure 6 : Comparison between non-subject-adaptive integration methods and the proposed SASNI on ADHD-200 data set. 'MeanC' indicates average combination while 'FixedTrainC' indicates that a set of fixed integration coefficients is learned with the training data set and uniformly applied to all test subjects.\nSparsity levels "}, {"section_title": "Extension of Application", "text": "In order to demonstrate the generality of the proposed SASNI method, we apply it to another data set on action recognition, which is called MSR-Action3D human action data set. MSRAction3D contains 20 categories of actions from ten subjects. Each action is performed two or three times by each subject. For this data set, only skeleton data are used. Following the literature on human action recognition [47] , the cross-subject test setting is used, i.e., the odd-indexed subjects for training and the even-indexed ones for test. Figure 10 presents the comparison of classification performance between single sparsity level SVM/SRC, MKL and the proposed SASNI on this data set. As seen, SRC achieves higher classification performance than SVM on most single sparsity levels. When multiple levels are combined, SASNI can further boost the accuracy of SRC and outperform MKL. Note that, this paper focuses on analyzing brain network data, as indicated by its title. Therefore, the results on human action data here are only used to demonstrate the generality of our proposed methods."}, {"section_title": "Conclusion", "text": "Recently, sparse inverse covariance estimation (SICE) has been widely employed to model brain connectivity networks and naturally leads to a set of connectivity networks with different sparsity for each subject. To explore the complementary information in the set of networks, we propose a learning framework that integrates brain networks and respects the underlying manifold structure of the SPD-based network representations. The proposed framework conducts a subject-adaptive integration via a kernel sparse learning scheme, and the obtained integrated network representation can be projected back in the original space for medical-related exploration. The effectiveness of the proposed method is verified on both ADHD and ADNI data sets. The experimental results demonstrate that the proposed integration method considerably outperforms a single network based methods and other commonly used integration methods. Our future work will follow two directions. The first direction is to apply the proposed method to other brain disease diagnosis and the second direction is to explore nonlinear integration of a set of networks. "}]