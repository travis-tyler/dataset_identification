[{"section_title": "Abstract", "text": "Abstract The volume, diversity and velocity of biomedical data are exponentially increasing providing petabytes of new neuroimaging and genetics data every year. At the same time, tens-of-thousands of computational algorithms are developed and reported in the literature along with thousands of software tools and services. Users demand intuitive, quick and platform-agnostic access to data, software tools, and infrastructure from millions of hardware devices. This explosion of information, scientific techniques, computational models, and technological advances leads to enormous challenges in data analysis, evidence-based biomedical inference and reproducibility of findings. The Pipeline workflow environment Some of the data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.ucla.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.ucla.edu/wp-content/uploads/ how_to_apply/ADNI_Acknowledgement_List.pdf. Brain Imaging and Behavior (2014) 8:311-322 DOI 10.1007/s11682-013-9248-x provides a crowd-based distributed solution for consistent management of these heterogeneous resources. The Pipeline allows multiple (local) clients and (remote) servers to connect, exchange protocols, control the execution, monitor the states of different tools or hardware, and share complete protocols as portable XML workflows. In this paper, we demonstrate several advanced computational neuroimaging and genetics case-studies, and end-to-end pipeline solutions. These are implemented as graphical workflow protocols in the context of analyzing imaging (sMRI, fMRI, DTI), phenotypic (demographic, clinical), and genetic (SNP) data."}, {"section_title": "Introduction", "text": "Process understanding is frequently the core research question in many biomedical, health and environmental applications. As we rarely know the exact process characteristics, we collect data (observations) which is used as proxy of the underlying physiological, physical or environmental phenomena. As such, the observed information (data) becomes the pivotal aspect of the scientific inquiry. The data variability, complexity and heterogeneity directly affect the scientific inference, accuracy of the results and reproducibility of findings.\nThree data characteristics make contemporary biomedical data different, challenging and powerful. These are the data volume (size), typically in the petabyte range (1 PB=10   15 bytes), data heterogeneity, including (un)formatted, ASCII/ Binary, (un)structured, and the data velocity, or data derivative, which captures the change, transfer, and discovery of raw and derived data (Foster et al. 2011; Howe et al. 2008; Lynch 2008) . Table 1 illustrates the Kryder's law for exponential increase of the volume of data (Walter 2005) . Using two decades of data, this law predicts that the density of information on hard drives, areal density, increases by a factor of 1,000 every 10-11 years. This storage rate increase is driven by the rapid expansion of data volume and velocity and translates into doubling of data size each 12-13 months. Both Moore's and Kryder's laws indicate similar exponential increase (of computational power and data storage, respectively) over time (Sood et al. 2012) .\nThere are thousands of software tools for acquisition, processing, storage/databasing, service, migration, mining, analysis, visualization, annotation, and data-driven process understanding. For example, the field of biomedical imaging includes hundreds of different types of image processing algorithms and filters. For each type of process, there may be dozens of concrete software products (instance implementations). More specifically, the Neuroimaging Informatics Tools and Resources Clearing House (NITRC) (Luo et al. 2009 ) lists over 500 openly shared neuroimaging software tools. For each openly shared tool, there may be dozens of proprietary or less commonly used analogues. Similarly, in genomics and bioinformatics there are over 200 data and cloud computing service providers, and hundreds of public, private and non-profit organizations that provide thousands of stand-alone tools (Eliceiri et al. 2012) . Resource organization, classification, discovery, traversal and utilization of these software products require flexible human and machine interfaces (Dinov et al. 2008) .\nAnother computational challenge is the proliferation of millions of hardware devices. According to Cisco (Cisco Systems Inc 2012) , by the end of 2012, the number of mobile-connected devices will exceed the number of people on Earth and there will be over 10 billion mobile-connected devices in 2016; i.e., there will be more than 1.3 mobile devices per capita worldwide. These include phones, tablets, laptops, handheld gaming consoles, e-readers, in-car entertainment systems, digital cameras, and \"machine-to-machine modules.\" There is a clear need for bridges between these mobile devices and for efficient connections to distributed databases, clients, servers, compute-nodes, web-services, variety of interfaces."}, {"section_title": "Methods", "text": "The LONI Pipeline environment (http://Pipeline.loni.ucla.edu) (Torri et al. 2012; Dinov et al. 2011 ) is a graphical workflow middleware providing an interface to computational libraries, informatics resources, computational expertise and cloud services (e.g., cloud data storage, cloud computing services). The Pipeline facilitates the design, validation, execution, monitoring and sharing of advanced heterogeneous computational protocols as graphical workflows. It also mediates the tool discovery and interoperability and provides distributed computing infrastructure for en masse data processing. The Pipeline's user-friendly interface enables access to disparate data, services, hardware infrastructure, computational expertise and cloud computing services (Torri et al. 2012) .\nAlternative infrastructures to the Pipeline environment that also facilitate visual informatics and computational genomics include Taverna (Oinn et al. 2004) , Kepler (Lud\u00e4scher et al. 2006) , Triana (Taylor et al. 2006) , Galaxy (Goecks et al. 2010) , AVS (Lord 1995) , VisTrails (Freire et al. 2006) , Bioclipse (Spjuth et al. 2007) , KNIME (Berthold et al. 2008) , NyPipe (Gorgolewski et al. 2011) , PSOM (Bellec et al. 2012 ) and others. The choice of a workflow environment depends on the specific research domain, scientific application and computational need. The Pipeline environment provides In 2011, the size of the genetics data is estimated to be 30 TBs (based on 10,000 human genomes) (Grossman and White 2012; Marusina 2012) . As the total number of complete human genomes sequenced by the end of 2011 worldwide was >10,000, this figure may be orders of magnitude smaller than the real genomics data size. Furthermore, data derived from genome sequencing of other species and 'partial genomes' (e.g., exome capture sequencing, RNA sequencing and chromatin immunoprecipitation sequencing) is not included in this estimate. By 2015 more than a 10 6 human genomes will be sequenced (Grossman and White 2012) . Assuming each genome takes about 10 11 B (100 GB) this translates into a total data volume of 10 17 B (100 PB). Some of the sequences may be whole-genome 100\u00d7 depth/coverage acquisitions, and some may be acquired at lower depth D Data volume may be increasing at a faster pace compared to the well-established growth of computational power, Moore's law (Fuller and Millett 2011a; Rupp and Selberherr 2011) some advantages over the alternative architectures. These include distributed client-server architecture, an array of scheduler grid plug-ins, external lightweight data manager, easy incorporation of new software tools and libraries, and dynamic workflow design, validation, execution, monitoring and dissemination of complete end-to-end computational solutions (Dinov et al. 2010 ).\nThe main types of computational tools available in the Pipeline library include software for neuroimaging and genetics data processing and visualization. For each of these types there are 3 categories of resources -data, atomic modules, and workflows. These resources can be explored via the Pipeline Navigator (http://pipeline.loni.ucla.edu/explore/librarynavigator/) and can be tested via the guest-access Pipeline Web-Start server (http://pipeline.loni.ucla.edu/PWS). Many interesting end-to-end computational workflow solutions (pipelines) are documented online (http://pipeline.loni.ucla. edu/explore/pipeline-workflows/). There are also many video tutorials, screencasts, and training materials (http:// pipeline.loni.ucla.edu/learn/basic-videos/), which illustrate the basic and advanced features of the pipeline client-server architecture, and the protocols for workflow design, execution and management.\nNeuroimaging processing tools There are several hundred atomic neuroimage processing tools, from a variety of software suites available in the LONI pipeline library, Fig. 1a . These tools may be used for analysis of structural brain images (e.g., AFNI (Cox 1996) , ROBEX (Iglesias et al. 2011) , MDT Atlasing (Wang et al. 2005; Tang et al. 2010) , BrainParser (Tu et al. 2008) , SVPASEG (Tohka et al. 2007 (Tohka et al. , 2010 , AIR (Woods et al. 1999) , FSL (Smith et al. 2004) , BrainSuite (Shattuck and Leahy 2000) , SSMA (Leung 2011; Leung et al. 2008) , ANTS (Avants et al. 2009 ), ITK (Pieper et al. 2006) , MINC (Evans 2002) ), functional brain data (e.g., FLIRT (Smith et al. 2005) , AFNI (Cox 1996) , WAIR (Dinov et al. 2002) , Matlab (Hanselman and Littlefield 1997) ), diffusion data (e.g., DTK (Wang et al. 2007a) , DIRAC (Patel et al. 2010a) , MiND (Patel et al. 2010b) ), statistical analyses (e.g., R (Ihaka and Gentleman 1996) , GAMMA (Chen and Herskovits 2005) , SOCR (Che et al. 2009; Dinov 2006) , SPM (Hu et al. 2005; Friston et al. 2011) ), shape and surface modeling (e.g., sulcal analysis (Joshi et al. 2012) , local and global shape analyses (Dinov et al. 2010) , shape mapping DHM (Shi et al. 2007 ), FreeSurfer surface extraction, and cortical thickness (Fennema-Notestine et al. 2006; Fischl and Dale 2000a) ).\nInformatics and genomics computational library The breadth of genomics tools available as pipeline modules and workflows is illustrated by the variety of sequence alignment solutions (Torri et al. 2012) , Fig. 1b . Some different categories of informatics and genomics computing software tools available in the Pipeline library include: sequence alignment (Mosaik (Smith et al. 2008) , MAQ (Li et al. 2008) , PERM (Chen et al. 2009 ), BWA/BWA-SW Durbin 2009, 2010) , Bowtie (Langmead and Salzberg 2012) , Novoalign (Li and Homer 2010) , SOAPv2 (Li et al. 2009a) , BLAST (Kent 2002) ), indexing (mrFAST/mrsFAST (Hach et al. 2010) ), genome-wide association studies (GWASS (Marchini et al. 2007 ), PLINK (Purcell et al. 2007 )), basic"}, {"section_title": "Neuroimaging tools", "text": "Genomics tools a b Fig. 1 Examples of classes of tools available in the Pipeline computational library and advanced quality control (SAMTools (Li et al. 2009b) , GATK (McKenna et al. 2010) ), CNV calling (CNV/CNVR (Stranger et al. 2007; Wang et al. 2007b) ), annotation (Artemis (Rutherford et al. 2000) ), de novo assembly (Trinity (Grabherr et al. 2011) , Velvet (Zerbino and Birney 2008) ), molecular biology (EMBOSS (Olson 2002) ), population genetics (GENEPOP (Raymond and Rousset 1995) ), and many others.\nBackend pipeline servers Pipeline web-start server (PWS) uses Java Web-Start technology enabling guest users to test the LONI Pipeline application from a web browser without the installation of either a pipeline client or a server. The PWS server provides access to all of the functions and features included in the downloadable version. PWS is accessible via an anonymous guest login or user-authentication to connect to remote Pipeline servers, e.g., http://ucla.in/GRSc8a. Several alternative Pipeline servers provide secure access-controlled connections to independent computational infrastructures. Examples include LONI Genomics Server (Genomics.loni. ucla.edu, 1 TB RAM/40-core), Cranium Server (Cranium. loni.ucla.edu, 16GB RAM/core, 1,200 cores) and Medulla Server (Medulla.loni.ucla.edu, 24GB RAM/core, 4,300 slots). The Distributed Pipeline Server infrastructure (http://pipeline. loni.ucla.edu/DPS) facilities the deployment of independent disparate Pipeline services on available hardware resources, including Amazon EC2 (http://pipeline.loni.ucla.edu/ products-services/pipeline-server-on-ec2/).\nBig data Modern protocols for imaging and genetics data collection generate enormous amounts of data. Table 2 illustrates some of the data-management, storage and processing challenges associated with common neuroimaging and genetics analysis protocols. Figure 2 shows an example of the multi-channel imaging brain data typically acquired in traumatic brain injury studies."}, {"section_title": "Applications and results", "text": "To demonstrate the Pipeline management of heterogeneous neuroimaging, genetics, phenotypic and clinical data, and the diversity of computational data processing tools available through the Pipeline library, we have chosen three complementary applications. These include studies of imaging-based genome-wide association, hippocampal morphometry, persistent pain and irritable bowel syndrome. Each of these three applications demonstrates exemplary solutions to the resourcescalability and processing-efficiency challenges related to the data complexity (size, heterogeneity and velocity), software tools interoperability and diversity of hardware devices. Specifically, these case-studies demonstrate (1) how seemingly incongruent imaging, phenotypic and clinical data can be jointly processed and analyzed in an integrated computational workflow protocol; (2) how pipeline workflows can wrap independent software tools to make them interoperate; and (3) how these data and computational resources (tools and services) can be accessed via different client devices (e.g., desktop or laptop computers or mobile devices running different operating systems and browser configurations).\nADNI imaging-genetics GWAS study The Alzheimer's disease data used in this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.ucla.edu). ADNI is the result of efforts of many co-investigators from a broad range of academic institutions and private corporations, and subjects have been recruited from over 50 sites across the U.S. and Canada. The initial goal of ADNI was to recruit 800 subjects but ADNI has been followed by ADNI-GO and ADNI-2. For up-to-date information, see www.adni-info.org. The Alzheimer's Disease Neuroimaging Initiative (ADNI) (Jack et al. 2008; Mueller et al. 2005; Shen et al. 2010 ) data was screened and from 589 study participants, 188 qualified for an Alzheimer's Disease (AD) diagnosis at baseline, 401 had mild cognitive impairment (MCI). Among them, 9 were early-onset (EO) AD (Male: 4, Female: 5) and 27 were earlyonset MCI (Male: 15, Female: 12). Subjects (ages 55 to 65) were divided into two groups: EO-AD and EO-MCI. Individual ADNI genotype and imaging data were downloaded and merged to form a single dataset containing genome-wide information for 36 individuals. Genetic analysis, including quality control, were performed using PLINK version 1.09. All the genetic processing was done via the LONI Pipeline environment. The 20 most significant single nucleotide polymorphisms (SNPs) were chosen by Manhattan plot and were associated with specific neuroimaging biomarkers. The structural ADNI data (1.5 T MRI) was parcellated using BrainParser, and the 15 most important neuroimaging markers were extracted by the Global Shape Analysis (GSA) Pipeline workflow.\nThe goal of this application is to demonstrate the use of the pipeline environment for genome-wide association study (GWAS) using early-onset ADNI data including cognitive impairment measures, neuroimaging and genetics biomarkers. After standard SNP quality control (Hibar et al. 2011; Rimol et al. 2010) , the raw SNP data (630K SNPs) was reduced to 360K SNPs. A new pipeline workflow was designed to integrate the global shape analysis, tensor-based morphometry and SOCR multivariate regression analyses. The results of the automated pipeline workflows included significant correlations between SNPs and various neuroimaging biomarkers in the EO subjects and discriminated between EO-AD and EO-MCI cohorts, Fig. 3 . A connectomics diagram can be used to illustrate the strength of the associations between the 15 derived neuroimaging biomarkers and the top 20 SNP genetic markers. In this case-study, the small sample-size (N=36) has a negative effect on the (statistical) power to detect significant associations between the biomedical imaging markers (e.g., regional volume and shape metrics) and the genetic traits (SNPs/chromosomes). However, the same computational pipeline workflows can be used to analyze similarly larger cohorts (e.g., N>700), where sufficient power may be available to detect interactions between imaging and genetics effects (after Bonferonni correction for multiple testing). The imaging, genetics and clinical data used in this example were directly imported into the Pipeline workflow environment from the ADNI database using the Pipeline's IDAGet module. This pipeline workflow protocol can be designed on one client, and execution may be initiated on a user-specified pipeline server from another pipeline client, and the workflow "}, {"section_title": "--", "text": "A Relative to the mouse brain, the field of view of human brain imaging data is several orders of magnitude larger (Sultan and Braitenberg 1993) . Diffusion imaging of mouse brain may reach 1.9 GB (7\u00d7512\u00d7256\u00d7256 points with real and imaginary parts, represented as 4 bits float numbers) (Jiang and Johnson 2010) , and correspondingly diffusion spectral or high-angular resolution images may exceed 10 GB per human subject and session (Scholl et al. 2011) . The Global Shape Analysis pipeline workflow (Dinov et al. 2010 ) includes about 100 processing steps and depending on the server load and the number of subjects provided as input may take 7 days to complete on the LONI Pipeline Medulla cluster (4 TB RAM, 3,000 slots) B Many computationally intensive neuroimaging processing tools require significant hardware resources including storage, memory and CPU cycles (Dinov et al. 2011 ) C In 2011, many alternative commercial DNA sequencing platforms generated whole genome sequences of size 100-600 GB (Glenn 2011) , which require days of computations on powerful grid systems. For example, our experience shows that Trinity whole-genome de novo assembly (Zhang et al. 2011; ) takes over 14 days of calculations on the LONI Pipeline Genomics server (1.4 TB RAM, 40-core) D The infrastructure needs of cohort-based and multi-institutional studies increase linearly with the increase of the number of cases that require processing. Thus, a brain study of 1,000 subjects (e.g., Chinese Probabilistic Brain Atlas (Xing et al. 2013) , vGWAS (Hibar et al. 2012) ) or a computational genetics study of 1,000 whole-genome sequences (e.g., prostate cancer (Gudmundsson et al. 2012) , autism spectrum disorder (Buxbaum et al. 2012) ) may require Terabytes of storage and extensive infrastructure for data management, processing and interrogation. Longitudinal neuroimaging studies add another layer of complexity, as these typically require baseline as well as several (1+) follow up scans, which increases proportionately the volume of the imaging data progress, and final result inspection, may be monitored or examined on a different client device.\nGenetic associations with hippocampal function and shape A recent study investigated the genetics effects (single-nucleotide polymorphisms, SNP, associated with FKBP5 gene regulation, rs1360780) related to attention, behavioral, and hippocampal morphometrics (Fani et al. 2013 ). The FKBP5 gene regulates glucocorticoid receptor sensitivity and is associated with hypothalamic-pituitary-adrenal axis functioning and stress-related psychiatric disorders (Binder 2009 ). In this cross-sectional study using fMRI/MRI, African American cohort of adults (N=103) separated into 2 groups by genotype: Group 1 included carriers of the rs1360780 T allele, associated with increased risk for posttraumatic stress disorder; Group 2 included non-carriers. The study used the local shape analysis pipeline workflow to identify attention bias toward threat (F 1,90 =5.19, p=0.02), and revealed alterations in the hippocampal shape for TT/TC compared with the CC genotype groups. Figure 4 shows part of the computational protocol implemented as a pipeline workflow and the exemplary result from this morphometric analysis.\nPersistent pain and irritable bowel syndrome (IBS) A UCLA IRB approved study recruited 328 female normal controls (NC) and IBS subjects. A diagnosis of IBS was made using the ROME III symptom criteria (Sperber et al. 2007; Drossman and Dumitrascu 2006) "}, {"section_title": "EO Subject demographics Results", "text": ""}, {"section_title": "Fig. 3", "text": "Early Onset (EO) ADNI Imaging-Genetics GWAS Study using the pipeline environment Fig. 4 Example of using the pipeline environment to complete a neuroimaging genetics study of FKBP5 gene (rs1360780) association with attention, measured through behavioral response (dot probe task) and hippocampal morphometrics. The superior and inferior vies of the hippocampal surface map illustrate the vertex locations, on the mean left hippocampus, where FKBP5 carriers (group 1) and non-carriers (group 2) showed significant shape differences by one of 4 gastroenterologists experienced in the diagnosis of functional bowel disease and the exclusion of organic disease. A subject's medical history and physical examination were obtained by a gastroenterologist. IBS patients with all types of predominant bowel habit were included. Subjects with a history of any chronic functional symptom or syndrome, or symptoms suggestive of disordered mood or affect, by history or by questionnaire, were excluded. In addition, potential subjects are excluded if by either history or questionnaire they a) have a serious medical condition or are taking medications which may interfere with interpretation of the brain imaging or physiological measures (other than IBS); b) have an ongoing major psychiatric diagnosis or psychotropic medication use over the past 6 months (subjects are not excluded for lifetime incidence of psychiatric disorder, or for intake of low dose tricyclic antidepressant for non-psychiatric indication); c) have a positive symptom score on the Hospital Anxiety and Depression Scale consistent with depression or anxiety d) do excessive physical exercise (i.e., marathon runners). Brain images were obtained from all 328 subjects (107 IBS, 221 NC) using 1.5 and 3 T MRI scanners (Jiang et al. 2013) . We collected phenotyping data on catastrophizing (Coping Strategies Questionnaire) (Geisser et al. 1994) , early life trauma (Early Trauma Inventory) (Bremner et al. 2000) , state anxiety and depression (Hospital Anxiety and Depression Scale) (Zigmond and Snaith 1983) , health status (12-Item Short-Form Health Survey) (Ware et al. 1996) , trait anxiety scores (State Trait Anxiety Inventory) (Spielberger 2005) and IBS symptom severity and duration (Bowel Symptoms Questionnaire) (Talley et al. 1995) .\nAs a first step for shape-based neuroimage analysis, we reconstruct surface representation of anatomical structures of interest. Then, we analyze both cortical and subcortical structures. The cortical surfaces, including both white matter and pial surfaces, are reconstructed from T1-weighted MR images using FreeSurfer (Fischl and Dale 2000b) . For sub-cortical structures, we applied the LONI BrainParser (Tu et al. 2008) to automatically segment the T1-weighted MR image into fifty-six regions. Using masks generated by BrainParser, accurate surface representations of the segmented regions are reconstructed with a novel algorithm we developed recently. This tool can remove segmentation artifacts without volume shrinkage and guarantees all surfaces guaranteed have the correct topology. All surfaces are represented as triangular meshes with spherical topology. The global shape analysis (GSA) pipeline workflow was used to identify regional differences between the NC and IBS subjects using the 56 regions of interest (ROIs) on 6 different volumetric and shape metrics (average mean curvature, surface area, volume, shape index, curvedness, and fractal dimension). Figure 5 shows the 3 steps in this analysis (data inputs, pipeline workflow and results of regional group differences)."}, {"section_title": "Conclusions", "text": "Although there are a number of useful software discovery and navigation frameworks (Dinov et al. 2008; Kennedy 2006; Tenenbaum et al. 2011) , the protocols for tool interoperability Left Lateral View Right Lateral View Fig. 5 Analyzing IBS/NC regional differences: (Left) raw sMRI data, (Middle) GSA workflow including data processing, surface reconstruction, 3D parcellation, and statistical analysis, (Right) Statistically significant ROI between-differences rendered as 3D scenes (left cuneus is green, and right angular gyrus is gray; the red cingulate gyrus and the blue insula are shown for orientation only) continue to present significant biomedical computing challenges. There are considerable design differences between independent software suites. Furthermore, the varieties of computer programming languages for algorithm implementation, the substantial diversity of compilers and optimization strategies, and the gamut of hardware resources present additional hurdles in biomedical computing. Mediating these computational issues, coping with the enormous amounts of incongruent data, and handling a wide spectrum of devices require a paradigm shift of how we manage, process, interrogate and utilize biomedical and health related data.\nThe evidence is clear that we are in the front of an enormous storm of exponentially increasing wave of data, processing power and resource diversity. Multidisciplinary science efforts, technologies like Hadoop (White 2012) , OpenStack (Wen et al. 2012) , Elastic Cloud Computing (Ostermann et al. 2010) , Pipeline workflow systems (Dinov et al. 2010; Heinis 2010 ) and super high-bandwidth networking (Chowdhury et al. 2010; Wang and Guo 2012) will be critical for riding this storm and uncovering novel biomedical knowledge. Embracing the science interactome (the multidisciplinary interactions between biomedical, computational and basic scientific areas, which often lead to new discoveries) will also be essential for establishing, maintaining and expanding the cyclical flow from Biomedical Challenges \u2194 Scientific Models \u2194 Data Analysis \u2194 Computational Infrastructure \u2194 Sustainable Education.\nIn this manuscript, we presented evidence of the rapid increase of the volume, diversity and velocity of biomedical data (e.g., neuroimaging and genetics (Novak et al. 2012; Van Essen et al. 2012; Thompson et al. 2013) ), and the growth of computational models, algorithms, software tools, services and electronic devices that manipulate these data (Toga et al. 2012; Berger et al. 2013; Meir and Rubinsky 2009) . There is evidence that software tool expansion always occurs within the limits of the available hardware infrastructure (Fuller and Millett 2011b) . This close connection between the Moore's law for increase of computational power facilitates the observed expansion of new and more powerful software tools (e.g., Software as a Service (SaaS) (Hashizume et al. 2012 ), Platform-as-a-Service (PaaS) (Truong and Dustdar 2012) ). For example, in 1993, Windows NT OS 3 consisted of 5-million lines of code, which 10-years later grew 10-fold to 50-million lines in Windows, Server OS 2003 (Maraia 2005 . Similarly, from 2000 to 2007, the Linux Debian OS grew from 59-million to 280-million lines of code (Matell\u00e1n Olivera 2012). Web and mobile applications, or webapps, are software systems running on portable devices, which have significantly grown since 2005 into a multi-billion dollar business (Minelli and Lanza 2013) . The explosion of webapp software development can be measured in terms of pure source code, usage of third-party APIs, and historical data. Studies of lines of code in specific areas indicate that over the past few decades there is an exponential increase of software development efforts (Knobloch 2013; German et al. 2013 ). This advancement of the software tool capabilities in turn pushes the introduction of more efficient and omnipotent hardware devices (e.g., Infrastructure as a Service (IaaS) and Virtual Machines (VMs) (Alarifi and Wolthusen 2013)).\nThe Pipeline workflow environment is one of many solutions that provide a distributed and platform-independent management of heterogeneous resources using dispersed clients and servers, elaborate exchange protocols, and flexible mechanisms for control, execution, monitoring and sharing of complete computational protocols. We demonstrated three advanced end-to-end computational pipeline solutions for neuroimaging, genetics and computational morphometry. These solutions are implemented as graphical workflow protocols in the context of analyzing imaging (sMRI, fMRI, DTI), phenotypic (demographic, clinical), and genetic (SNP) data."}]