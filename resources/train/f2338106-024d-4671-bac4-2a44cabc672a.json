[{"section_title": "Abstract", "text": "High resolution magnetic resonance (MR) images are desired for accurate diagnostics. In practice, image resolution is restricted by factors like hardware, cost and processing constraints. Recently, deep learning methods have been shown to produce compelling state of the art results for image superresolution. Paying particular attention to desired hi-resolution MR image structure, we propose a new regularized network that exploits image priors, namely a low-rank structure and a sharpness prior to enhance deep MR image superresolution. Our contributions are then incorporating these priors in an analytically tractable fashion in the learning of a convolutional neural network (CNN) that accomplishes the super-resolution task. This is particularly challenging for the low rank prior, since the rank is not a differentiable function of the image matrix (and hence the network parameters), an issue we address by pursuing differentiable approximations of the rank. Sharpness is emphasized by the variance of the Laplacian which we show can be implemented by a fixed feedback layer at the output of the network. Experiments performed on two publicly available MR brain image databases exhibit promising results particularly when training imagery is limited."}, {"section_title": "INTRODUCTION", "text": "High resolution MR images can provide rich structural information about bodily organs which is critical in analyzing a given medical condition. Often, the quality of these images is restricted by factors like imaging hardware, sensor noise, cost and time constraints. In such scenarios, the spatial resolution of these images can be enhanced by a well-designed mathematical algorithm. Simple and fast interpolation methods like bilinear, bicubic [1] have been widely used for increasing the size of low resolution (LR) medical images. In many cases, these methods are known to introduce blurring, blocking artifacts, ringing and are thus unable to recover sharp details of an image. To alleviate this problem, an alternative approach known as super-resolution (SR) was introduced in [2] . Current literature on SR can be classified into two categories: multi-image SR and single-image SR. This work is supported by NIH Grant R01HD085853. In multi-image SR [2, 3] , a HR image is generated by exploiting the information from multiple LR images which are acquired from the same scene with a slightly shifted field of view. However, these methods are likely to fail if an adequate amount of LR images from the same scene are not available. As an alternative approach, single image SR was introduced wherein multiple LR images from the same scene are not required to obtain a HR image. In this approach, a mapping between LR and HR images is learned by constructing examples from a given database [4] [5] [6] [7] [8] .\nRecently, deep learning methods have been shown to produce compelling state of the art results [9] [10] [11] [12] [13] [14] [15] for image SR. Invariably though, the training burden of deep networks, i.e. the number of example LR and HR images (or patches), is quite significant. In some medical diagnosis problems, generous LR and HR pairs is not a problem but there are compelling real-world problems such as enhancing 3T MR to 7T MR images [8] , where the paucity of training has been recognized. There has been encouraging recent application of deep networks for MR image SR [16, 17] but the methods remain training intensive. An outstanding open challenge for deep MR image superresolution is the development of methods that exhibit a graceful degradation with respect to (w.r.t.) the number of training LR and HR image pairs.\nMotivation and Contributions: Our approach to improve deep MR image superresolution, even in the face of limited training is via the exploitation of suitable prior information pertinent to MR images. In [18] , a model based SR approach is presented that uses low-rank (approximated by nuclear norm) and total variation regularizers. The authors in [18] validate that MR images from various parts of the body can be reconstructed with a peak signal-to-noise ratio (PSNR) of close to 50 db by retaining only half of the singular values obtained by a singular value decomposition (SVD) of the image matrix. Despite this promise, using a rank or even its nuclear norm relaxation in a deep network for SR presents a stiff analytical challenge since neither is a differentiable function of the image matrix (and hence the network parameters). Our contribution includes incorporating a suitable approximation to the rank, which is smooth, differentiable and amenable for learning in a deep CNN framework. Additionally, recognizing the need for sharp well formed edges in diagnosis, we propose a sharpness prior realized via a variance of the laplacian measure which adds to the network structure at the output as a fixed (non-optimizable) feedback layer. We use a CNN for super-resolution (SRCNN) as described in [9] as our base network. Our SR method is then called deep network with structural priors (DNSP)."}, {"section_title": "DEEP LEARNING FOR MR IMAGE SUPER-RESOLUTION 2.1. Notation:", "text": "Let X \u2208 R M\u00d7N represent the LR image where M and N are the width and height of the image respectively. Let Y \u2208 R sM\u00d7sN be the output HR image and s is the desired scale to which X needs to be upscaled and Y g \u2208 R sM\u00d7sN is the ground truth HR image for X. Let W l k \u2208 R m\u00d7n\u00d7d be the k th convolutional filter in layer l where m, n and d represent the width, height and depth of the filter respectively. Similarly, let b l k \u2208 R be the k th bias coefficient of layer l. The objective of the network is to learn W l k and b l k so that the output of the network Y is a close representation of the ground truth\nTo make the size of input and output of the network the same, we first upscale X by a factor of s using bicubic interpolation and use this upscaled X s \u2208 R sM\u00d7sN as input to the network. Finally, let the mapping function of the network be represented by F where F(X s , \u0398) = Y ."}, {"section_title": "Deep CNNs For SR", "text": "Deep learning methods are a class of machine learning methods which are inspired from biological neural networks. In general, a cascade of many nonlinear processing units are used to learn features to represent data effectively for a given task. In particular, a deep CNN for image SR usually consists of two or more convolutional layers (each layer essentially is a combination of filters followed by an activation function) which are used to learn an end-to-end mapping between sample HR and LR images. For example, Figure 1 illustrates the SRCNN network [9, 16] for super-resolution. Each convolutional layer in the network consists of several learnable filters, which are convolved with output from the previous layer. For a given layer, outputs obtained by convoluting with each filter are combined to form a data cube which is passed through a nonlinear activation function and then forwarded as an input to next layer [19] . Most commonly used activation function in recent times is the Rectified linear unit (Relu) [20] . The input to the first layer is the image obtained after bicubic interpolation and the output of the last layer is the expected HR image. The filters are learned to minimize the loss function given by: "}, {"section_title": "DEEP NETWORKS WITH STRUCTURAL PRIORS", "text": "As discussed in Section 1, we integrate two priors into the learning of the CNN. Note that both the priors are to be applied on Y as it represents the desired output HR image. The two priors are as follows: Low Rank Prior: It has been argued recently [18] that MR images are naturally rank deficient. However, the rank of a matrix is a non-differentiable function w.r.t. its input and therefore cannot be used as regularizer in a CNN. Most of the optimization problems with a low-rank constraint are solved by minimizing the nuclear norm of the matrix which is a convex relaxation for the low-rank constraint. However, this relaxation cannot be used in a CNN as the nuclear norm is also a non-differentiable function. Recently, a function which is smooth and differentiable was proposed in [21] that can approximate the rank of a matrix accurately. It is defined as:\nwhere \u03b4 is a tunable parameter that affects the measure of approximation error in finding the rank 1 \ngives the number of singular values of Y which are zero. sN) . Now, the function R \u03b4 (Y ) is differentiable and its gradient w.r.t. Y is given by:\nSharpness Prior: HR images look much sharper compared to LR images. The main reason can be attributed to blurriness of the LR images. The pursuit of quantifying sharpness begins by computing the Laplacian (\u2207 2 Y ) of the image [22] . The laplacian of a smooth/blurred image is more uniform compared to the laplacian of a sharp image. The variance of the Laplacian is hence an indicator of sharpness. As shown in Figure 2 , an MR brain image is degraded by a gaussian filter with different blur parameters and plotted against the variance of laplacian. It can be observed that the variance of laplacian decreases as the blur parameter increases. Therefore, we propose to use V (Y ) = var(\u2207 2 Y ) as a regularizer to encourage the CNN to yield sharper HR images. V (Y ) is a quadratic function in Y and therefore a differentiable function which can be easily integrated into the CNN learning. Note that the laplacian of an image can be implemented by wellknown linear filters [22] , which are also easily integrated into the CNN via a filtering layer at the output as shown in Fig. 3 . Network Structure: We incorporate the aforementioned two priors into the the basic SRCNN [9] framework and the proposed Deep Network with Structural Priors (DNSP) is shown in Fig. 3 . Note that our priors can be integrated into any other deep SR network as well. There are 3 layers in DNSP: the first layer has 64 9\u00d79\u00d71 filters, the second layer has 32 1\u00d71\u00d764 filters while the final layer has one 5 \u00d7 5 \u00d7 32 filter. The output of each layer except that of the final layer is fed into ReLU to generate a nonlinear activation map [23] . We also use a 3\nafter the final layer to compute the Laplacian and subsequently find the variance of Laplacian. Finally, the loss function of DNSP to be minimized is given by:\nwhere, Y = F(X s , \u0398), \u03b1 and \u03b2 are positive regularization parameters 2 , note that negative sign before V (Y ) is to increase the variance of Laplacian. Note that the SRCNN loss function in Eq (1) is a special case of Eq. (5). We learn \u0398 by minimizing E(\u0398) using a stochastic gradient descent method [24, 25] .\nIn particular, weights are updated by the following equation: 2 We chose \u03b1 = .1 and \u03b2 = 5 \u00d7 10 \u22125 by cross validation.\nwhere, t represents the iteration number, \u03b7 represents the learning rate, and \u0398 t represents the values of weights at previous iteration. As \u0398 = {W l k , b l k }\u2200l, k, following gradients are to be computed:\n, where w l k denotes an arbitrary scalar entry in filter W l k . For simplicity, let output image Y be of dimension N \u00d7 N. The equation for computing the gradient of weight w l k in layer l \u2208 {1, 2, 3} is given by:\nwhere between two matrices A and B is defined as \u2211 i, j A i, j B i, j ,\nThe complete expression for D V is given by:\nwhere P = [p i, j ], and P is obtained by convolving Y with a 3 \u00d7 3 laplacian operator L. Expression for p i, j is given by: p i, j = y i, j \u2212 1 4 (y i\u22121, j + y i+1, j + y i, j\u22121 + y i, j+1 ), and Y = [y i, j ] Detailed derivations for the above equations are reported in a technical report at [26] . Note that the gradient for bias terms are also updated in a similar fashion. The partial derivative \u2202Y \u2202 w k l is obtained by a standard back propagation rule [24, 25] ."}, {"section_title": "EXPERIMENTAL EVALUATION", "text": "Databases: We evaluate the proposed DNSP on two publicly available MR brain image databases. The first database is 20 simulated T1 brain image stacks from Brainweb (BW) 3 .\nAxial slices of these 20 stacks are distributed evenly for training and evaluation purposes. From each stack, we extract 40 slices making a total of 400 images for training and 400 images for evaluation. The second database we work with is from the Alzheimer's Disease Neuroimaging Initiative (ADNI) 4 . The training and evaluation configuration for this database is same as that of the BW database. LR image simulation: Consistent with [18] , we simulate training LR images by applying a gaussian blur and factor of 2 downsampling. These LR images are then upscaled by bicubic interpolation. To speed up the training process, we further extract patches of size 40 \u00d7 40 from these bicubic enlarged LR training images. Note that this is also a standard procedure used for training a typical deep SR network [9] [10] [11] . Methods and Metrics for Comparison: Two standard metrics PSNR and structural similarity index (SSIM) [27] are used for evaluation. We compare against: 1.) Bicubic interpolation (Bb), 2.) a competitive model based approach with low-rank and total variation (LRTV) regularizers [18] , 3.) example based super-res via sparse weighting (SRSW) [4] -a state-of-the art sparsity based method and 4.) SRCNN [9, 16] that is the most popular embodiment of a deep SR network. Table 1 shows PSNR and SSIM values for all competing methods. Two trends emerge from the results 1) DNSP outperforms the competition, and 2) Overall, deep SR methods, i.e. SRCNN and DNSP perform better. To confirm this statistically, we performed a 2-way Analysis of variance (ANOVA) on PSNR values for all the methods across the two datasets which is illustrated in Fig. 4 . It may be inferred from Fig.  4 that deep learning methods are statistically well separated from the traditional methods and further DNSP is well separated from SRCNN indicating the effectiveness of using prior information. Figure 5 illustrates the results of top 3 methods w.r.t. PSNR on a sample image from the BW database. DNSP performs better in recovering finer image detail. SRSW, 2) The performance degradation of DNSP is more graceful. For example, PSNR values for SRCNN and SRSW dropped by almost close to 1db whereas for DNSP, the drop is around .5db, when the training drops to 25 percent. Note that LRTV is excluded for this comparison since this is model based and not an example/learning based technique [18] . "}, {"section_title": "CONCLUSION", "text": "We present a novel regularized deep network structure for MR image superresolution, which excels in varying training regimes. This is accomplished by using two structural priors on the expected output HR image: 1) a low-rank prior, and 2) a sharpness prior. While we demonstrate improvements by employing SRCNN [9] as our base network, our proposal is versatile and the proposed priors can be applied to extend other deep SR networks [10, 12, 14, 15] as well."}]