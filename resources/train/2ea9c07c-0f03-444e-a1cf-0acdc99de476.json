[{"section_title": "Abstract", "text": "In recent years, deep convolutional neural networks (CNNs) have shown record-shattering performance in a variety of computer vision problems, such as visual object recognition, detection and segmentation. These methods have also been utilised in medical image analysis domain for lesion segmentation, anatomical segmentation and classification. We present an extensive literature review of CNN techniques applied in brain magnetic resonance imaging (MRI) analysis, focusing on the architectures, pre-processing, data-preparation and post-processing strategies available in these works. The aim of this study is three-fold. Our primary goal is to report how different CNN architectures have evolved, now entailing state-of-the-art methods by extensive discussion of the architectures and examining the pros and cons of the models when evaluating their performance using public datasets. Second, this paper is intended to be a detailed reference of the research activity in deep CNN for brain MRI analysis. Finally, our goal is to present a perspective on the future of CNNs, which we believe will be among the growing approaches in brain image analysis in subsequent years. Figure 1: Number of publications per year in IEEE-Xplore containing \"deep learning\" and \"medical imaging\" keywords from 2010 to 2017. (Queried: June 6th, 2017).\nwidely used in brain MRI for preprocessing data [15] , detecting and segmenting lesions [16, 17, 18, 19, 20, 21] and segmenting tumours [22, 23, 24] , whole tissue [25, 26, 27] and sub-cortical structures [28, 29] .\nAlthough in medical image classification tasks, there are usually fewer classes compared to large-scale semantic image recognition, there are two major difficulties hindering achieving similar accuracy to human raters. First, there is a lack of sufficiently labelled training data. Essentially, generating highly accurate labels and finding sufficient preprocessed and representative data require large amount of time. Second, medical image annotation is carried out by experts which is subjective and error-prone [30, 31] . Learning a model from a less accurate representation of training samples degenerates the algorithm accuracy.\nBrain MR image analysis has traditionally been an important area of research, attracting researchers to work on different tasks, such as lesion detection and segmentation, tissue segmentation and brain parcellation on neonatal, infant and adult subjects [32, 33, 34, 35] . Several public brain MR image datasets are available to the community, especially those organised by the Medical Image Computing and Computer Assisted Intervention (MICCAI 1 ) society, strongly encouraging research and publications in the field. These standard evaluation frameworks have been proposed for quantitatively comparing tissue segmentation algorithms under the same directives and conditions. Consequently, the number of publications using CNNs in brain MRI has been increasing: of 23 works collected from PubMed, Scholar, IEEE Xplore and Scopus databases for this review work, 20 of them were published in the period from 2015 to May 2017.\nDeep learning methods have been extensively reviewed in recent years [36, 37, 38, 39, 40] and discussed for particular applications [41, 42, 43, 44] . To the best of our knowledge, this work is the first to present a detailed review and discussion of deep CNN techniques applied to brain MRI analysis. We intend to comprise all publications in this field published from 2012 to June 2017.\nThe rest of this review is structured as follows. The general concepts in CNNs are given in Section 2. Note that an experience reader could skip this section. Afterwards, the different methods published in the literature on brain MRI are surveyed and analysed in terms of advantages and disadvantages in Section 3. In Section 4, evaluations and comparisons of the works are performed, based on the reported numerical results. We conclude this work with a discussion, indicating future trends in this research field.\nFor years, conventional supervised machine-learning techniques were built using automatic learning techniques and well-engineered algorithms. The approach consisted in taking the raw data, describing its content with lowdimensional feature vectors -using specific prior knowledge of the addressed scenario -and inputting the vectors 1 http://www.miccai.org/"}, {"section_title": "Introduction", "text": "Convolutional neural networks (CNNs), an outstanding branch of deep learning applications to visual purposes, have earned major attention in the last years due to its breakthrough performances in varied computer vision applications, such as in object recognition, detection and segmentation challenges [1, 2, 3] , in which they have achieved astonishing performances [4, 5, 6, 7, 8] . Their success has not been limited to reach top positions in different tasks, but also to achieve competent personnel levels in visual object recognition tasks [9] and, perhaps more importantly, sensitive medical applications [10] .\nCNNs have been used in medical imaging applications since 1990s in areas such as lung structure and nodule detection [11, 12] and breast tissue classification [13] . However, due to the lack of labelled training data and computational power limitations by that time, it was not possible to train deep CNNs without over-fitting. As a result, proposals in this regard were discontinued in the field for some years. With the time, large annotated training datasets and more powerful graphic processing units (GPUs) have been created, enabling researchers to continue working in the area. This trend can be observed in Fig. 1 in which the number of papers using these strategies has increased year after year from 2010. In brain image analysis, the pioneering work appeared in multiple sclerosis (MS) by Maleki et al. [14] , after Krizhevsky et al. [4] rekindled research on CNNs in 2012. Nowadays, deep CNN architectures are Figure 2 : Generic architecture of convolutional neural networks. The output of each convolution operation at each layer is activated using activation functions before applying pooling operations. The convolution operation produces different numbers of feature maps, depending on the numbers of filters used. The pooling operations reduce the spatial dimensions of each feature map. After convolution and pooling the layers, the future maps are flattened in the fully connected layer before a prediction is made using linear classifiers.\nto a trainable classifier. While the classifier was certainly useful for other purposes, the features were not necessarily generic. Indeed, the overall accuracy of the method would depend on how appropriately designed were the heuristics [45] .\nRepresentation learning appears as an alternative to this drawback: discover automatically suitable detection and classification representations from the input data. One of the first successful attempts using this strategy took place in 1998 when LeCun et al. [46] presented a 5-layer fully-adaptive architecture for addressing handwritten digit recognition. Despite its accuracy results (1% error rate and 9% reject rate from a dataset of around 1000 samples), the authors were able to apply neural networks on a real world task. However, these authors were not able to scale a large number of hidden layers and larger images, mainly due to computational resource constraints and the \"vanishing gradient\" problem. This latter situation being the case when the gradients of the network's output with regard to the parameters in early layers becomes negligible [47] .\nTwo initial solutions were proposed in the literature to address the \"vanishing gradient\" problem. The first one was proposed by Hinton et al. [48] , in which training was conducted in two stages. The first stage consisted in training each layer in an unsupervised, greedy manner, essentially initialising the parameters with better values than random or uniform values. In the second stage, the network was fine-tuned using labelled data. This work rekindled interest in current deep learning strategies. The second solution appeared with the introduction of the ReLU activation function [49] , which showed successful results in large-scale image recognition tasks [4] . Additionally, the availability of fast GPUs dramatically increased the research in deep neural networks, in which many computer vision tasks have outperformed their shallower counterparts by large margins.\nOne of the most widely adopted approaches of deep neural networks is the convolutional neural networks in which are able to process array-like data [40] , such as images or video sequences. From a high-level perspective, the idea behind CNN is to identify compositional hierarchy features which objects from the real world exhibit: low level features (e.g. edges) form patterns and these specific patterns form more intrinsic structures (e.g. shapes, textures). Further information of the building blocks of CNN is provided in following sections."}, {"section_title": "Building blocks of CNN 2.1.1. Convolutional layer", "text": "CNNs learn the relationships among the pixels of input images by extracting representative features using convolution and pooling operations. The features detected at each layer using learnt kernels vary in terms of their complexity, with the first layer extracting simple features, such as edges, and the later layers extracting more complex and highlevel features. The convolution operation in CNNs has three main advantages. First, the weight-sharing mechanism helps to deal with high dimensional data, either 2D images or 3D data, such as videos and volumetric images. Second, local connectivity of the input topology can be exploited using 2D or 3D kernels. Finally, slight shift invariance is achieved using pooling layer. The typical architecture of a 2D CNN is shown in Fig. 2 .\nVery deep CNN architectures were recently proposed to replace the conventional convolutional layer with modules with more powerful representation and using less computational resources. For instance, Szegedy et al. [50] introduced inception modules that could extract multi-scale features from the input feature maps and could efficiently reduce the number of parameters. This module was further improved in [9] and [51] . The recent version of the inception module in [6] was created to have a more uniform simplified architecture than previous versions and thus could achieve top performance in large-scale image classification tasks."}, {"section_title": "Non-linearity layer", "text": "The above convolutional layer is usually followed by non-linearity operations. Non-linearity is achieved using a specific family of functions called activation functions. These activation functions ensure that the representation in the input space is mapped to a sparse one and, hence achieving (i) a certain invariance to data variability and (ii) a computationally efficient representation [49] . The former situation refers to the fact that sparse representations are more resilient to slight modifications than dense ones. In the past, sigmoid and hyperbolic tangent functions were commonly used for this purpose. However, for large-scale image recognition, novel activation functions are being continuously proposed. We categorise the commonly used activation functions into three broad families.\nRectified linear units (ReLUs) and variants: which are expressed in a general form as\nwhere z xy lk is the input value at position (x, y) on the k th feature map at the l th layer and \u03b1 is the slope of the negative linear function. There are five special cases distinguished depending on the conditions over \u03b1. First, if \u03b1 = 0, the expression results in the so-called ReLU [4, 52] which is the one of the most commonly used activation functions [40] . Despite its computationally efficient implementation, this method presents some drawbacks due to its gradient discontinuity at the origin in terms of gradient update and empirical performance [6, 53] . Second, if \u03b1 is a small constant, the variant is referred as Leaky ReLU (LReLU) [54] . This approximation enables to cope with the problem of zero gradient. Third, if \u03b1 is tuned up in the training process along with other parameters using back-propagation, the approach is referred as Parametric ReLU (PReLU) [53] . Fourth, in [55] , the parameter \u03b1 is sampled from a uniform distribution for each example, and this approach is called Randomised ReLU (RReLU). Although using a slope parameter for the negative part in ReLU showed improvements in performance, RReLU showed better performance than the other ReLU variants, based on the evaluation in [55] on image classification tasks. Fifth, recently, Jin et al. [56] proposed a new type of activation function, called S-shaped ReLU (SReLU), in which the essential idea is to consider a piecewise function composed of three linear functions, i.e.\nwhere a r , a l , t r and t l are learnable parameters. According to the experiments in the paper, the SReLU is able to learn both convex and non-convex functions. This activation improved the performance of well-known CNN architectures in MNIST [57] and ImageNet [58] datasets, compared to ReLU, LReLU and PReLU. Maxout and variants: Maxout [59] was proposed in particular to improve the optimisation and model averaging performance of dropout training. This activation function is also a generalisation of ReLU. It is computed by calculating the maximum across K affine feature maps, i.e. f z xy lk = max k\u2208 [1,K] z xy lk .\nOne major drawback of this technique is that the number of weights to be learned in each layer is increased by factor of K. A workaround to this situation was proposed by Springenberg and Reidmiller [60] in which a probabilistic sampling procedure to compute the maximum across feature maps, generalising maxout is considered. This activation function called Probout empirically matched or improved the performance of maxout [60] .\nExponential Linear Units (ELU) and variants: ELU [61] are similar to the variants of ReLU since they employ an identity for positive inputs. Unlike ReLU variants, they provide saturated output for negative inputs. The saturation in the negative regions of the function is reported to be beneficial for expediting the learning and improving the performance of very deep CNNs. It is defined as f z xy lk = max z xy lk , 0 + \u03b1 \u00b7 min e z xy lk \u2212 1, 0 .\nTrottier et al. [62] defined parameters controlling different aspects of the ELU function and proposed learning them with gradient descent during training. This parametric ELU (PELU) further improved the speed and performance of training deep networks. Using off-the-shelf ResNet [5] , PELU performed better than ELU and ReLU in image classification tasks on MNIST, CIFAR-10/100 and ImageNet datasets [62] ."}, {"section_title": "Pooling and sub-sampling layer", "text": "Typical convolutional layers consists of three steps. First, the layer performs several convolution to produce feature maps. Second, non-linear activation functions are used on the resulting maps. Third, the output is modified by the pooling layer before reaching the next convolutional layer. The idea of a pooling function is to extract a summary statistics of non-overlapping neighbourhoods -usually -to (i) reduce the number of parameters in the following layers, (ii) control over-fitting, and (iii) achieve slight translation invariance [63] .\nAmong several pooling methods, max pooling and average pooling are widely used types. Their operations are similar except that the former uses the maximum of the activations, and the latter uses the average of them. Max pooling is the most common due to its empirical performance [64, 65] . Apart form these two approaches, other pooling methods are proposed to achieve greater performance.\nStochastic Pooling: Zeiler et al. [66] proposed stochastic pooling to regularise the convolutional layers and, hence, overcome the overfitting problem of average pooling and max pooling. In this method, the output of the activation from each pooling region is selected first by computing the probability distribution p by normalising the activations; then, the output activation is chosen by sampling from the multinomial distribution based on p. The authors showed that this pooling strategy improved the accuracy on MNIST, CIFAR-10/100 and street view house numbers image recognition (SVHN) [67] datasets.\nSpatial pyramid pooling: Another pooling method called spatial pyramid pooling [68] , is proposed to work with any input image size. It is added after the last pooling layer, immediately before the fully connected layer, to generate fixed length representation regardless of the image size or scale. By simple modification of the off-the-shelf CNN architectures with a spatial pyramid pooling layer, this pooling scheme was reported to improve the performance of CNN models.\nDeformation pooling: Despite max pooling and average pooling being useful in handling deformations, they are incapable of learning deformation constraints or geometric models from object parts. Deformation pooling (referred as def-pooling) was specifically designed to overcome this drawback for object detection applications [69] .\nCombination of max and average pooling: Lee et al. [70] proposed a generalisation of max and average pooling operations that allowed them to be combined and adjusted in the training process. The combination operation was performed using mixed max-average and gated max-average pooling. The latter approach improved the characteristics of the region being pooled. The learning is performed during a combined pooling operation using a binary tree in which each leaf of the tree is associated with a learnt pooling filter. This strategy is called tree pooling. An experiment in [70] reported that this method boosted the performance of AlexNet [4] and GoogLeNet [50] models on MNIST, CIFAR-10 and SVHN datasets."}, {"section_title": "Fully connected layer", "text": "Unlike convolutional layer, the Fully Connected (FC) layer has a full connection to all of the units in the previous layer, as shown in Figure 2 . This layer changes the previous layers 2D structure features into a predefined one dimensional feature vector. Essentially, the main task of the FC layer is to mine the incoming features to extract information about the content of the input image. The process usually consists in flattening the feature maps coming from convolutional layers, to achieve a one dimension feature vector representation, and, then, inputting it into the FC layer. The output of this layer could either be the predict class labels [4] or an intermediate layer [71] (consecutive FC layers can be stacked together).\nImplementing FC layers usually require a large number of parameters -compared to other layers -since each cell of a feature map is fully connected to all elements in the previous layer. In addition, there are two drawbacks of this kind of layers: (i) a single output is produced -if it is used as output layer -and (ii) accepts fixed-size inputs. The former issue means that a single input image receives a single output label. This situation is computationally inconvenient if the CNN is intended to be used in segmentation tasks rather than classification ones. The second issue relates to problems in extensibility of the network: either the input images are scaled to fit the requirements of the network or the network is re-factorised to be able to process the new data. A solution to these problems lies on the fact that FC layers can be converted to convolutional layers of 1 \u00d7 1 kernels [72] (1 \u00d7 1 \u00d7 1 kernels in case of 3D). In this way, the model keeps the fully connected functionality while accepting arbitrary input size image and making dense predictions. This type of architectures, called fully convolutional network (FCNN), has been gradually introduced in the literature [16, 21, 23, 24, 28, 72] ."}, {"section_title": "Loss function", "text": "CNNs are well-known for their ability to extract discriminative features using learned weights in each layer. The learning process is reinforced by employing appropriate loss functions. Loss functions are designed to encourage intra-class similarity and inter-class separability.\nIn image classification tasks, most CNNs employ softmax loss, which is a combination of the softmax function and cross-entropy loss, mainly because of simplicity and the probabilistic interpretation of softmax classifiers. Hinge loss is another type loss function, which in conjunction with margin-based classifiers was reported to perform better than cross-entropy loss in a standard image classification challenge [73] . More recently, the research in [74] proposed a Large-Margin Softmax Loss (L-softmax), which is a modification of the softmax loss with a distance margin constraint. This loss was reported to boost the performance of deeply learned features in visual classification and verification tasks.\nOne of the main issues during the training phase is the disproportion among class observations (i.e. the number of available samples per class varies dramatically) since the resulting classifier may be biased towards the majority class. For instance, the region of interest to be analysed occupies only a small part of the scan. This problem can be addressed in the training phase by (i) undersampling the majority class [23, 25] , (ii) merging or subdividing classes [27] and (iii) penalising sample misclassification based on the reciprocal frequency [75] ; or throughout the loss function. Brosch et al. [76] considered an approach for brain lesion segmentation in which the weighted sum of the mean squared difference of the lesion voxels (sensitivity) and non-lesion voxels (specificity) was used. Also, Milletari et al. [77] proposed an objective function based on Dice Similarity Coefficient (DSC) for a two-class problem. According to the authors, this loss function yielded higher performance than the same architecture using softmax.\nOther loss functions are task specific, such as contrastive losses, which are mostly used to measure similarities between two data points. Su et al. [78] used a combination of softmax and contrastive losses for face recognition. The purpose of the softmax loss was to classify faces into different classes, thus encouraging inter-class separation. Then, the contrastive losses were applied for face verification to enforce intra-class similarity. To obtain more discrimination of features, a distance constraint was introduced to the contrastive loss in [79] ."}, {"section_title": "Regularisations", "text": "We observed the appealing performance of deep CNN methods in different domains, although they use enormous numbers of parameters. Unless trained on a large, labelled training datasets, proper regularisation should be employed to mitigate over-fitting. There are several regularisation methods widely used in the community, such as L 1 or L 2 regularisation approaches encouraging sparsity and small weight magnitude; early stopping [63] forcing the training to stop when there is a sign of over-fitting (also widely used to select hyper-parameters of the model); batch normalisation [9] in which each batch is preprocessed to achieve mean equal to zero and standard deviation equal to one; and dropout [80, 81] in which some feature map units are skipped. This last approach being the dominant since (i) it is computational inexpensive and (ii) prevents co-adaptation among feature map units by encouraging independent contributions to the prediction task from units at each layer.\nDifferent improvements in dropout have been proposed. For instance, Wang et al. [82] proposed a method to expedite training using dropout by sampling from a Gaussian approximation, instead of repeatedly sampling a random subset of input features. Another research in [83] proposed Dropconnect, which drops a subset of weights within the network instead of subset of activation within each layer. This approach showed better generalisation than the original dropout in standard image recognition tasks. Data augmentation is another approach to improve the generalisation of CNNs by increasing the training dataset by artificially generating data. Although different image transformations [4, 50, 53, 84] and colour perturbation [85] have been used and proved effective in recognition tasks, one limitation with these methods is that there is no theoretical background that indicates the group of transformations helping to improve the generalisation capability of a model."}, {"section_title": "Optimisation", "text": "Deep CNNs are learnt by searching appropriate values for model parameters optimising the loss function. Gradient descent methods, in which the parameter update is performed using a back-propagation algorithm, are widely used for minimisation [86] . This parameter update is undertaken by computing the loss function for a single, small subset, or the whole training set. Each of these cases is referred in the literature as stochastic, mini-batch and batch gradient descent, respectively. Updating values using the whole training set can be computationally expensive and, hence, mini-batch gradient descent is ubiquitously used in the community, leading to smoother parameter updating and more stable convergence.\nThe major difficulty when optimising deep CNNs using gradient descent-based algorithms is the non-convex nature of the loss function. Non-convex functions have several local minima, in which gradient descent methods could get easily trapped [87] . Dauphin et al. [88] argued that saddle points, where the gradient vanishes at nonlocal optimum places, are much more of an issue than local optima in optimising non-convex loss functions. The momentum variable was introduced to the stochastic gradient descent (SGD) update method to avoid oscillation in local optima. Further, momentum-based SGD could be improved to have some prescience about the next update direction, referred to as Nesterov accelerated gradient descent [89, 90] . Recent works have also been proposed to escape saddle points during the optimisation process [91, 92] .\nOne major drawback of the momentum-based SGD methods is to select an adequate learning rate a parameter determining how large a change in the update should be made. This parameter is commonly set globally to be equal for all parameters. Much work has been carried out on tuning the global learning rate adaptively based on the gradient of each parameter. One method in [93] , called adaptive gradient algorithm (Adagrad), scales the learning rate of each parameter according to the sum of the previous gradients. The amount of updating differs based on the sparsity of the parameters in each gradient update. Adadelta [94] and Root Mean Square Propagation (RMSProp) [95] improved the drawbacks of Adagrad unstable gradient updating by adopting a more stable learning rate scheme. Kingma and Ba [96] proposed Adam, which improved the gradient computation of both Adadelta and RMSProp. They also empirically showed improved performance in non-convex optimisation problems in different machine learning tasks."}, {"section_title": "Weight initialisation", "text": "Weight initialisation is crucial for non-convex optimisation algorithms. Initialising all the parameters to the same small value results in the undesired case in which all weights are updated the same during back-propagation. Another option that could break the symmetry is to initialise the weights with small values taken from a Gaussian distribution [4] . Glorot and Bengio [97] proposed a method known as \"Xavier\" initialisation that normalises the variance of each neuron's output to one. There are two beneficial outcomes from this approach: (i) this avoids the variance in each layer and (ii) keeps the weights from exploding to large values or vanishing. This approach is later improved in [53] to account for the non-linearity of ReLU. This allows to train very deep networks with better convergence than the \"Xavier\" initialisation. Another recent work from He et al. [53] proposed an initialisation by explicitly modelling the non-linearity of rectifiers (ReLU/PReLU), which helped the convergence of extremely deep models."}, {"section_title": "Normalisation", "text": "It is a common practice to use mean-centred training datasets by applying contrast normalisation to train deep CNNs [4, 22, 23, 24, 98] . This simple preprocessing technique improves the convergence speed of SGD algorithms. Ioffe et al. [9] argued that training deep networks can be slow since the distribution of parameters across hidden units changes dynamically during training, which is a phenomenon called the internal covariate shift. Their proposal consisted in normalising the data input in each layer, with a technique known as batch normalisation. This method provides any layer in a network with inputs that come from a unit Gaussian distribution and also enables the use of a high learning rate with a less careful weight initialisation choice. More recently, in [99] , the authors introduced weight normalisation. With simple weight re-parametrising at each layer, they were able to show improved conditioning of the optimisation problem and faster convergence of SGD methods. "}, {"section_title": "CNN methods for brain image analysis", "text": "Automatic segmentation in brain MR has been widely investigated throughout the years to support medical tasks, such as diagnosis and patient monitoring. In the literature, most of the deep learning-based papers for brain MRI analysis have focused on tumour, tissue and sub-cortical structure segmentation. This trend is primarily attributed to the different MICCAI challenges. Each year, the number of participants in these challenges employing deep learning approaches has been increasing. In addition, people have applied CNNs for segmentation of MS lesions, cerebral micro-bleeds and deep brain anatomical structures. In this section, the proposed methods in this domain are discussed in more detail, based on their CNN architectures. Additionally, the description comprises the considered pipelines: pre-processing, data preparation and post-processing techniques. There is no specific CNN architecture that is only suitable for a particular application; rather, a CNN model that is proposed for tumour segmentation could work for structure segmentation, and vice versa, with little or no modification.\nThe segmentation methods proposed in the literature could be seen from a top level perspective as presented in Fig. 3 . The overall pipeline consists of four stages: preprocessing, data preparation, classification, and postprocessing. In the preprocessing stage the different pipelines consider noise filtering techniques, inter and intrapatient normalisation and skull-stripping methods -when necessary. Then, the data is prepared to be processed by the classifier. For instance, data preparation could contemplate augmenting the data or, in patch-based strategies, extracting patches from the input volumes. According to the literature, the more the considered modalities, the better the segmentation due to complementary contrast information. After that, the classification of the instance takes place. Finally, once the segmentation results are obtained, they could be refined by removing small isolated areas by selecting the biggest groups only or smoothing regions. It is important to note that some works do not specify any preprocessing or post-processing methods. In the following sections, we will discuss each of these blocks emphasising on the CNN strategies."}, {"section_title": "Pre-processing", "text": "Pre-processing in MRI is a vital step for subsequent segmentation task. In training supervised models such as CNNs, the input training data hugely influences the performance of the model, so having preprocessed and wellannotated data is a crucial step in achieving good performance. Acquired brain MRI volumes incorporate non-brain tissue parts of the head, such as eyes, fat, spinal cord or the skull. The process of extracting the brain tissue from non-brain one is referred in the literature as skull stripping. An example of an original volume and its corresponding skull stripped output is presented in Fig. 4a and 4b, respectively. This step has direct consequences on the performance of automated methods, as the inclusion of skull or eyes as brain tissue may lead to unexpected results in tissue classification [100, 101] , while unintended removal of the cortical surface may result in underestimation of the cortical thickness [102] . Among the different methods proposed in the literature for skull-stripping [103, 100, 104] , methods such as BET [105, 106] , BSE [107] , ROBEX [108] and BEaST [109] are commonly used. In the literature, the methods used in clinical trail datasets employed BET [21, 25, 110] and ROBEX [23] . Similarly, Zhang et al. [26] applied a paediatric brain skull stripping algorithm known as LABEL [111] . The public dataset images from the Brain Tumor Image Segmentation Challenge (BRATS) 2013 2 , 2014 3 and 2015 4 are preprocessed in this regard beforehand. : An example of two preprocessing methods: skull stripping and bias field correction [112] . In the figure, a T1-w slice is displayed in (a), brain tissue after removing non-brain areas in (b), estimated bias field in (c), and preprocessed brain tissue in (d).\nInherent characteristics of the MRI acquisition process such as differences in the magnetic field, bandwidth filtering of the data or eddy currents driven by field gradients usually result in image artefacts that may also have a negative impact on the performance of the methods [113] . There is the need to remove spurious intensity variations due to inhomogeneity of the magnetic fields and coils. In these cases, intensity correction of the MRI images is performed either before tissue segmentation, or as an integrated part of the tissue segmentation pipeline. A common technique to address this problem is to use bias-field correction [114] . The estimated bias field and the corrected version of Fig. 4b are depicted in Fig. 4c and 4d, respectively. Among the available strategies [115, 116] , the non-parametric non-uniform intensity normalisation (N3) [117] and N4ITK [118] methods are currently the most widely used. Zhang et al. [26] and Yoo et al. [110] employed N3 algorithms on their clinical dataset. Similarly, Pereira et al. [22] used them in both BRATS 2013 and 2015 Challenges, Lyksborg et al. [119] in BRATS 2014, and Zikic et al. [120] in BRATS 2013.\nBrain MRI datasets might have volumes acquired from different scanner vendors and also from the same scanner but with different protocols. As a result, the volumes may exhibit non-uniform intensity representation for the same tissue types, i.e. interclass variability. To correct this problem, image normalisation algorithms are utilised. According to the literature, this intensity normalisation can be driven in two ways: (i) histogram matching [121, 122, 123, 15, 22] and (ii) normalise data to achieve zero mean and unit variance [22, 24, 27, 28, 124] . In the former case, Urban et al. [122] and Kleesiek et al. [15] considered matching the histogram of all volumes to a subject in the training set, which may result in fused grey levels, while Pereira et al. [22] -based on the normalisation method proposed by Nyul et al. [125] -considered mapping to a virtual grey scale learnt directly from the data, so the undesired fusion of grey levels is avoided. Naturally, both normalisation strategies can be used one after the other one to improve the segmentation results. According to the results reported by Pereira et al. [22] , the preprocessing step improved their result, obtaining a mean gain of 4.6%.\nIn addition to the above discussed pre-processing methods, image registration between different MRI modalities is important depending on the dataset analysed. Image registration transforms different modalities of MRI into a common coordinate space. The authors of [14, 21, 23 , 76] 5 applied image registration algorithms on their clinic trial dataset. For instance, Brosch et al. [21] applied a six degree-of-freedom intra-subject registration using one of the 3 mm scans as the target image to align the different modalities. Additionally, Kamnitsas et al. [23] applied affine atlas based registration."}, {"section_title": "Data preparation", "text": "By data preparation, we refer to all of the operations performed before feeding the data into the CNN, such as data augmentation and patch extraction. Although this stage in most pipelines is considered to be pre-processing, we include it as a separate step since (i) the preprocessing steps are generic while these ones are particular for the CNN approaches and also (i) to provide more details.\nData augmentation is mainly employed to increase the training samples to mitigate over-fitting as discussed in Section 2.1.6. It is a common practice to use data augmentation in computer vision tasks in which (i) the CNN architectures are very deep, and (ii) obtaining enormous amounts of labelled training data is difficult. Moreover, unless the dataset is large enough to correctly train the network (which is not the common case [24, 119, 123] ), the high disproportion between the cardinality of the majority and the minority classes may lead to biased classifiers. Data augmentation could be used in these cases to alleviate and improve the overall performance of the model. Pereira et al. [22] used data augmentation for dual purposes: (i) to increase the training data by applying rotation at different angles and (ii) to introduce class balance by adding more data from the minority class. According to their experiments, by augmenting using rotation, they achieved better delineation of the complete tumour types, as well as of the intratumoural structures, reporting a mean gain of up to 2.6%. In addition, by oversampling the minority class, they stated a mean gain of 1.9%. In contrast, Havaei et al. [24] employed data augmentation by flipping the training data but had no success in improving the accuracy of the model. Furthermore, Chen et al. [27] used data augmentation as input, in addition to the given multi-modal images. The augmentation was carried out by subtracting a Gaussian smoothed version and applying histogram equalisation using the Contrast-Limited Adaptive Histogram Equalisation (CLAHE) [126] for enhancing local contrast [27, 127] . Afterwards, they used the generated and original volumes to train their network. Nevertheless, the actual effect of this type of augmentation was not reported in their work.\nThe patch extraction can be performed on a single plane (referred as 2D architectures), from three anatomical planes (referred as 2.5D architectures) or directly from 3D. Also, the patches can be acquired from different imaging modalities -if available. The advantages of one approach or the other one are discussed in the following section. After extracting the patches of training images, the data is normalised by subtracting the mean intensity and dividing by the standard deviation [21, 22, 26, 61, 110, 128] . The resulting zero mean and unit variance training data helps to expedite the convergence of SGD. It is also recommended practice to shuffle the samples randomly, specially when training using a mini-batch gradient decent algorithm [129] ."}, {"section_title": "CNN architectures", "text": "The classification of CNN architectures in medical imaging can be grouped around to five aspects: (i) number of interconnected operating modules, (ii) number of input modalities, (iii) input patch dimension, (iv) number of predictions at a time and (v) implicit and explicit contextual information. All these variants, their advantages and disadvantages are described in the following sections. A summary of the reviewed methods in the literature in terms of pre-processing, post-processing and target of the segmentation are described in Table 1 ."}, {"section_title": "Interconnected operating modules", "text": "According to the number of interconnected operating modules, the strategies in the literature can be classified into: single-path and multi-path architectures. The single-path architectures corresponds to the cases in which there is a unique flow of information: the input data is processed by convolutional, pooling and non-linear rectifier layers; the feature maps are then mined in the FC layers; and afterwards used for predicting the label in the output layer. An example of 2D single-path architecture is shown in Fig. 2 . This category was popular in the literature [14, 16, 22, 29, 76, 120, 128] perhaps due to its fast computation and simplicity compared to multi-path architectures. In contrary to single-path networks, the multi-path architectures are a type of CNN in which independent operative networks are integrated into a single model to capture a more varied set of features. The modules can be arranged either in parallel or in series. In the former approach the idea is to achieve a consented label between different sources of information while in the latter approach the key is to reprocess the output of one network using another one.\nThe parallel multi-path architectures are composed of different CNNs designed to operate in parallel to capture more comprehensive features. Each network uses different versions of the same target area. There might be two possible variants of this type of architecture. The first variant aggregates features extracted from separate networks before making predictions, which can be understood as a process of information fusion that could occur at two levels: first, at a data level, where feature maps are merged to form a larger feature map; and second, at a feature level, where the extracted final features are concatenated before making the prediction. Fig. 5a graphically depicts a multipath architecture in which different features are processed using separate networks and are concatenated to form new features before making the prediction. This concatenation could also be performed at the convolutional layers, where feature maps from different networks form a larger feature map in its third dimension. The second variant of multipath architectures consists of learning ensembles of more than two networks to improve prediction. Predictions made by each network for a given target voxel are merged probabilistically.\nThere are several variants of the parallel multi-path category within the literature (i) multi-scale/multi-resolution [23, 25, 28, 122, 133] -in which different views of the same ROI are used as input -(ii) 2.5D [119] -in which the data is extracted from the three anatomical planes -and (iii) 2.5D+3D [132, 136] -in which the different modules process 2.5D and 3D patches and operate simultaneously to provide consented labels. The fusion of different information sources allows the network to obtain contrast, local detailed and implicit contextual information.\nThe authors in [133] and [25] used multi-scale (i.e., different views of the same target area) 2D CNNs using three separate networks and combining the features immediately before the prediction step. According to the authors, the multi-scale design allowed the network to obtain fine-grained local and general information -presumably spatial context as well -to produce the classification of the target pixel. Similarly, Lyksborg et al. [119] trained three networks separately, using 2D slices from axial, sagittal and coronal planes. Unlike in [133] and [25] , the ensemble of the three networks is performed after the prediction step and it was based on majority vote. The ensemble in this case was used twice: to segment the tumour area and to perform a within-tumour segmentation.\nAnother work, presented by Kamnitsas et al. [23] , proposed a network called DeepMedic that used a small 3\u00d73\u00d73 kernel to increase depth of the network to 11 layers, and they used two scales to train two separate networks for brain lesion segmentation. The output of each network is fused on the fully connected layer. The 11 layer 3D CNN produces a soft segmentation map. They extended a conditional random field [137] to a fully connected 3D CRF and used it as post-processing to impose a regularisation constraint and produce the final hard segmentation labels. One challenge when increasing the depth and path of the networks is that the computational complexity and memory requirements increase.\nBrebisson and Montana [132] were able to train eight networks arranged in parallel, with each network having a seven layer CNN architecture for whole brain anatomical segmentation. The inputs of the network captured information at different scales around the voxel of interest: 3D and orthogonal 2D intensity patches captured a local spatial context, while downscaled, large 2D orthogonal patches and the distances to the regional centroids enforced global spatial consistency. They reported over-fitting, mainly due to the enormous number of parameters of the overall model and the few training images provided in the MICCAI 2012 challenge, which was a multi-atlas labelling challenge.\nThe second variant of multi-path architectures is the series arrangement. In this variant of architecture, different CNN networks are cascaded in series, with information from the previous network used as input for the latter, as shown in Fig. 5b . It also renders the overall CNN deeper and, as a result, increases the expressing power of the model.\nHavaei et al. [24] trained a cascaded architecture, starting by training a two-path CNN. Then, they fixed the parameters of these two-path networks and used the already trained models in the cascaded architecture. They evaluated the performance of concatenating feature maps from the last layer of the first network as feature maps for the second network at three different layers. From their three architectures submitted to the BRATS 2013 brain tumour challenge, they reported the best performance by concatenating the last layer feature maps of the first network, with the actual inputs used as input for the second network.\nValverde et al. [124] presented a cascaded 3D CNN approach for addressing MS lesion segmentation. Due to the conditions of the scenario (i.e. lack of large number of samples and data imbalance), the authors opted for performing the segmentation in two steps. In the first step, the network is trained using all the positive samples and the same number of negative samples (i.e. the negative class is under-sampled). Once the classification is performed, the probabilities for each voxel to belong to the positive class are obtained. In the second step, the network is trained with all the positive voxels and the same number of misclassified negative samples. In the paper, no details regarding the improvement of the cascade approach over the single-path architecture are quantitatively discussed. Nevertheless, with this cascaded architecture, the authors were able to outperform the rest of participants on the MICCAI 2008 challenge.\nChen et al. [27] proposed a 3D CNN, VoxResNet, for brain tissue segmentation. The network is a 3D extension of the ResNet network [5] with a shallower depth. The authors also integrated segmentation maps from VoxResNet (used as the context information) together with the original volumes (i.e. appearance information) to train a new classifier called Auto-context VoxResNet. In this sense, possibly misclassified voxels are reprocessed and probably refined. It is important to understand that more than an \"auto-context\" technique, the procedure corresponds to a cascaded CNN similar to the ones described in [24, 124] . Although this classifier was intended to iteratively refine the segmentation, according to the authors, the improvement was marginal and, hence, a single iteration was considered.\nIn general, training multiple networks, rather than a single network, could lead to better performance. The networks can be integrated in parallel or series. The former approach permits the incorporation of different pieces of information, which help in labelling the central pixel. Different networks, which extract various useful features, could be aggregated to perform the desired segmentation. Each network arranged in parallel might need different input data. Training all of the networks arranged in parallel at the same time might demand much memory. In addition, managing individual networks, training processes, selecting the best hyper-parameters and avoiding over-fitting also require large amounts of time. The latter approximation, networks arranged in series, allows for refining the output of one network using another network, which requires a careful design and training process. Generally, this type of architecture performs better than single networks [24] ."}, {"section_title": "Input modalities", "text": "The strategies in the literature can be also grouped according to the number of modalities that are processed at the same time. The categories are two: single-and multi-modality. The former case is certainly more adaptable to different scenarios since a single modality is commonly provided in datasets for tissue and sub-cortical structure segmentation (mainly T1). The latter case contemplates processing different sources of information. As stated by Zhang et al. [26] , it is important to consider fusing information from different imaging modalities -if possiblesince it has been proven beneficial to discriminate when lacking contrast among tissues. The imaging sequences can be processed in parallel using multi-path configurations (e.g. one branch processes T1 and another one, FLAIR) or can be appended altogether to have multiple channels [26, 27, 119] . The difference between these two approaches is that in the former case specific kernels are adapted to process each modality but, at the same time, more number of parameters are expected than in the latter case."}, {"section_title": "Patch dimension", "text": "As mentioned previously, the different architectures can be also classified according to the dimension of the input patches. The categories in this case are three: 2D, 2.5D and 3D.\nTwo-dimensional architectures consider patches from a single plane (i.e. axial, sagittal or coronal). This architecture is widely used in the literature since it is adaptable to different image domains and segmentation tasks. In semantic computer vision, this type of networks is very common. Compared to the earlier proposal from AlexNet [4] , the depth of the recently proposed networks has shown a significant increase [5, 6, 50, 138] . In MRI brain analysis, the architecture remains very similar despite being shallower, mainly because of limited available training data. It seems also unnecessary to design a very deep network if it does not improve the performance and, perhaps more importantly, it can actually be detrimental, due to potential (severe) over-fitting if too many network weights are required to be adjusted.\nDiagnosing abnormalities employing a binary classifier is mostly the first step toward the localisation and segmentation of regions of interest. To the best of our knowledge, pioneering work using deep learning for brain image analysis was first reported in the detection of MS lesions by Maleki et al. [14] . They used a conventional CNN architecture to extract features from 2D MR FLAIR images and a three layer neural network as a classifier. A similar classification algorithm principle could be extended to patch-wise image segmentation algorithms. Usually incorporating multi-modal images during segmentation as a stack of 2D images improves performance. Zikic et al. [120] used 2D slices from T1, T1c, T2 and FLAIR images to classify small 19 \u00d7 19 patches into five classes in the BRATS 2013 dataset. Their simplified CNN architecture consisted of 2D convolution, ReLU, max-pooling and softmax classifier layers.\nPixel-based labelling schemes that predict the label for a single voxel at a time have been a popular choice in training MR images using CNNs. Despite the success of this approach, scanning each voxel using a sliding window renders it computationally slow. To overcome this limitation, Dvorak and Menze [128] fed their network several versions of the whole 2D slice input image, shifted on the X and Y axes, and they merged the outputs accordingly. They used images from multi-modal volumes to train a CNN for the prediction of extended label patches.\nAs mentioned earlier, in the context of brain MRI, building a very deep network is a rare practice because of the difficulty of obtaining sufficient representative and accurate data for training. In spite of the performance advantages, deeper networks usually have a larger number of parameters and they may tend to suffer from over-fitting. Training on large datasets with proper regularisation algorithms is a common practice in the community. Pereira et al. [22] proposed a deep architecture, eleven layers, with 2D patches extracted from T1, T1c, T2 and FLAIR for gliomas segmentation. They made use of small 3 kernels which allowed designing a deeper architecture [138] . A CNN built using those small kernels has less parameters, and therefore reduces the over-fitting problem. In addition, they also took advantage of data augmentation using rotation to increase their original training dataset. Their 2D CNN consisted of multiple 2D convolutions, followed by LeakyReLU and max-pooling layers.\nIn the reviewed literature, we observed that the conventional CNN models are popular choices of MRI analysis researchers, perhaps because of the authors' interest in utilising the already developed 2D CNN architectures, in addition to reducing computational complexity. This approach is in contrast to the actual 3D nature of the volume.\nTri-planar or 2.5D architectures are provided with patches from the three anatomical planes (i.e. axial, sagittal and coronal), commonly using a multi-path design. As explained by Lyksborg et al. [119] , 2.5D information provides a better understanding of the 3D scenario than 2D-based networks since it exploits the 3D nature of MR images and, consequently, brings up contextual information. Moreover, although 2.5D architectures are more computationally expensive than 2D variants, these networks still consider 2D convolutions and, hence, are expected to be less costly than 3D alternatives [128, 130] .\nLyksborg et al. [119] presented a 2.5D multi-path approach in which the patches from different planes were processed independently by similar operative modules. By using probabilistic methods, the knowledge coming from the three branches was combined after the prediction layer. Although the approach is reliable, it somehow assumes that the contribution among the three branches is the same, which may not be the case at some point (i.e. due to the anisotropic nature of the data and/or to the fact that regions of interest maybe more visible in specific planes). A workaround to this situation is to perform the merging step before the prediction layer and, hence, the weight of the features to be merged are specifically fixed for the case of study since the network determines the best configuration based on their contribution to the final segmentation.\nThree-dimension architectures consists in taking 3D segments directly from the MRI volume. This architecture utilises 3D convolution kernels, which seems to be a more appropriate solution for fully exploiting the spatial contextual information in volumetric data. The main constraint of a 3D CNN approach lies in its expensive computational cost, memory requirements and computational time. Utilising the whole volume at a time is computationally very expensive; therefore, small, three dimensional patches are extracted for training and testing. For instance, Urban et al. [122] used four (multi-modal) 9 3 voxels as input to segment a tumour in the BRATS 2013 dataset. Figure 6 shows a general 3D CNN architecture, in which the input is volumetric data, a 3D brain MR image in this case, and 3D kernels are used to perform the convolution.\nLai and Rueckert [130] investigated the use of three different convolutional network architectures for patch-based segmentation in ADNI hippocampus MRI dataset. They evaluated stacked 2D patches (one around the voxel in question, one in parallel and above it, and one in parallel and below it), tri-planar patches and 3D patches. According to their experiments, 3D convolutional architectures performed the best despite being the most computationally expensive.\nTo summarise, most of the algorithms, 10 of the 20, reviewed in the literature used a 2D CNN architecture. They were based on the two-dimensional convolutions used extensively in computer vision applications on natural images. This fact eases the model development process for practitioners by customising the popular CNN architectures and exploiting the available frameworks. Although patch-wise implementation of this type of architectures is very slow, converting the last output layer into a convolutional layer helps to expedite the inference time [24] . Simple implementation of this type of architecture using a single network might not result in good performance, demonstrating that deeper networks that use smaller kernels achieve better performance, compared to using larger kernels and shallower architecture [22] . Two-dimensional CNNs disallow for exploiting of the actual 3D nature of the MRI data. In this sense, 2.5D and 3D CNNs were introduced mainly to overcome this limitation. Although both 2.5D and 3D architectures require more processing than 2D ones, the former type is less computationally expensive than the second one: 2D convolutional layers are used in 2.5D while 3D convolutional are used in 3D. Indeed, the main impediment of working with 3D CNNs consists of the memory requirements, expensive computational costs and slow speed. Moreover, 3D CNNs use a larger number of parameters. Processing the whole 3D volume requires large CPU or GPU memory, and processing will be slow. However, with the rapid development of GPUs, this situation may not represent a drawback in the coming years."}, {"section_title": "Predictions at a time", "text": "The approaches can be also grouped according to the number of predictions they perform at a time into: CNN and FCNN [28, 23, 135] . The former case corresponds to the traditional approach in which a single patch is processed by the network and a single output is provided as response. Since the idea at the end is to obtain segmented areas, this approach can be slow in practice. The latter case corresponds to the architectures for which the fully connected layers are replaced by 1 \u00d7 1-kernel (or 1 \u00d7 1 \u00d7 1-kernel) convolutional layers to obtain a dense prediction. Additionally, some approaches consider upsampling layers to increase the number of predictions at a time. Naturally, the main drawback of these approaches is that a larger number of parameters are required to be set up (hence, more training samples to train the network properly are needed) in comparison to CNN strategies.\nDolz et al. [28] implemented a 3D FCNN architecture which was used to segment sub-cortical structures. Apart from the FCNN implementation, the authors considered multi-resolution information by extracting feature maps from high resolution layers and merging them with low resolution information, coming from the main information flow, before the 1 \u00d7 1 \u00d7 1-kernel convolutional layers. As explained by Bharath [139] , this kind of connections allowed the network to learn from semantic -coming from deeper layers -as well as fine-grained localisation informationcoming from shallow layers.\nRonneberger et al. [140] proposed a multi-path 2D FCNN architecture called the U-Net. The network was successfully tested on the 2D electron microscopy [141] and ISBI 2015 cell tracking 6 challenges, achieving the first position in both of them. There were two key components on this FCNN. First, up-sampling layers were used to increase the resolution of the output. Thus, the volumes could be rapidly processed. In the paper, the authors were able to produce an output map of 388 \u00d7 388 from an input image of size 572 \u00d7 572. Second, localisation and context were retained in the network by merging features maps from high-resolution layers with the up-sampled ones. On the other hand, it is important to highlight that this type of network, as stated by its authors, requires setting up a large number of parameters. Indeed, the original implementation required data augmentation techniques (for example, non-linear deformations generated on-the-fly during training) to achieve desired segmentation results.\nSeveral approaches in different scenarios have been inspired by the U-Net architecture. Milletari et al. [77] and Yu et al. [142] considered a 3D extension of the U-Net along with skip and/or residual connections for addressing the MICCAI Prostate MR Image Segmentation (PROMISE12). According to their experiments, the use of connections improved the accuracy of the methods and also proved effectiveness of information propagation. Also, the 3D variant has been implemented for confocal microscopic data [143] and liver and lesion segmentation in CT images [144] . Although none of these papers are directly applied to brain MRI, we consider that they could achieve relevance for brain MRI segmentation as well.\nBrosch et al. [21] used 3D convolutional encoder networks for MS lesion segmentation. In addition to learning deconvolution and un-pooling they introduced short-cut connections from the first to the last deconvolution layers inspired by the U-Net architecture [140] . The network in the convolutional encoder path was initialised using weights pre-trained on a stack of convolutional restricted Boltzmann machines. With this approach, they achieved top performance in public MS lesion segmentation challenge datasets."}, {"section_title": "Contextual information", "text": "The fact that the discussed architectures process patches separately and not the whole volume has some implications. For instance, the spatial distribution of brain structures is not directly encoded. Although some implicit contextual information is encoded in 2D, 2.5D and 3D patches their information is limited to the size of the patches. The bigger the size of the patch, the more information the network is able to take into account to produce the prediction; but the more the parameters to be trained. When considering the architecture of Havaei et al. [24] and the work of Kamnitsas et al. [23] , it seems to show that the context is important, but to preserve spatial correspondence of the multi-scale features is crucial. This could explain why a simple approach as [22] could outperform [24] .\nRecently, Wachinger et al. [29] introduced explicit within-brain location information through Cartesian and spectral coordinates [145, 146] to help the classifier to discriminate one class from another. The Cartesian coordinates were obtained by taking the XYZ-coordinates of each of the voxels while the spectral coordinates came from the eigenvectors of the Laplacian matrix of the graph representing the brain mask. As explained by the authors in [147] , the explicit spatial information is useful to (i) discriminate between patches from different brain hemispheres since patch-based strategies loss spatial context and (ii) help in the segmentation of tissues exhibiting low contrast. In this latter approach, it is important to have in mind that (i) considering XYZ-coordinates on the image plane makes sense only if the volumes are registered and (ii) the distribution of the eigenvectors depends highly on the shape of the brain mask, i.e. these features may not be reliable when the shape of the region of interest differs considerably from one volume to another.\n3.3.6. Summary of pros and cons of the different strategies Table 2 summarises the main advantages and disadvantages of the approaches discussed previously in this section. Larger number of parameters to be set up Generally, requires more training samples (data augmentation is commonly adopted in this sense) Contextual information Implicit\nNo additional calculations are required Context depends on the size of the patch Explicit Discriminative features are provided to the network Sense of within-brain positioning Subject to registration accuracy or shape of the region of interest"}, {"section_title": "Post-processing", "text": "Post-processing is the step to refine or improve the results of the segmentation. The purpose of post-processing could be either to create hard segmentation labels [23] or to remove false positives [16, 26, 22, 24, 122, 130] from segmentation results, using different algorithms such as connected component analysis and Conditional Random Fields (CRF). The former strategy consists in keeping only the k largest areas from the segmentation to remove spurious outputs. Nevertheless, in sub-cortical structure segmentation these spurious areas may have larger volume than the small structures of interest and, hence, it becomes impractical. The latter strategy, CRF model, considers not only the given voxel but also its neighbours to produce a refined label, especially around the edges. Although CRF-based methods require learning parameters, they could bring regularisation properties to the segmentation since they are (a) (b) (c) (d) Figure 7 : Two input slices in which low contrast between the region of interest and the surrounding areas is exhibited. In the image (a) the focus is on the brainstem which is described by the purple area in (b). In (c) the focus is on caudate, putamen, pallidum and thalamus structures presented in green in (d).\nbased on the minimisation of an energy equation. This stage helps to improve the overall performance of the model at the expense of adding computational complexity.\nContrast-driven post-processing techniques may lead to unexpected/unsatisfactory smoothing results. For instance, sub-cortical structures and some parts of the brainstem could be shrunk in this step due to low contrast with surrounding areas as shown in Fig. 7 . However, this does not signify that the method is not capable of refining the segmentation since both the intensity features and the prior probabilities take part in the process.\nIn addition to CRF and connected component analysis, Lyksborg et al. [119] utilised an algorithm called Grow cut [148] . It was initially proposed as a continuous state cellular automata method for automated segmentation based on user labelled seed voxels. The authors used it to refine the initial segmentation results of tumour from healthy tissue. The post-processing algorithm was applied iteratively until a stable segmentation was obtained."}, {"section_title": "Evaluation", "text": "Brain MRI volume segmentation approaches are usually evaluated using different quantitative measurements. The described CNN models were tested in public and clinical research trial datasets. Regarding the works reviewed, both the most common datasets and the typical measurements used for the evaluation are described in this section. In addition, the results obtained by the analysed methods on the standard datasets are presented."}, {"section_title": "Public evaluation frameworks and datasets", "text": "Several tissue segmentation algorithms have been proposed during the last years in the literature. However, determining which of them achieves the best performance can be complicated in principle since not all the algorithms are publicly available. A way to address this situation is by creating standard evaluation frameworks, such as the grand challenges taking place on relevant international conferences. Thus, before delving into a detailed discussion about the methods, we describe some of the most widely used public datasets for brain lesions, structures and anatomical segmentation.\nSome of the most commonly used public datasets in brain image MRI analysis are summarised in Table 3 . The reviewed works primarily used MICCAI datasets for MS lesions, tumours, tissue and structure segmentation. For MS lesion segmentation, the challenge was created in conjunction with MICCAI 2008 7 . The challenge was part of the 3D segmentation in the Clinic Grand Challenge II and consisted of 54 brain MRI images, 20 of which are available for training with their corresponding ground truth. For brain tumour segmentation, the series of multi-modal brain tumour segmentation (BRATS) challenges are commonly used. These challenges started in 2012 with 80 cases of real and synthetic data and, every year, the size of the training and testing data has been enlarged (up to 300+ Table 3 : Summary of commonly used public datasets in MS, tumour, tissue and structure segmentation. The table is structured as follows. The different datasets are listed in the first column and their corresponding information: number of scans, offered modalities, acquisition scanner, segmentation tasks in which it is used, and image information such as reconstruction matrix and pixel spacing are detailed from column two to seven."}, {"section_title": "Name", "text": "Number cases in total). For sub-cortical structure segmentation, the MICCAI 2012 8 challenge in multi-atlas labelling and the Internet Brain Segmentation Repository (IBSR) 9 are widely used. It is important to note that MICCAI 2012 challenge is initially intended to evaluate algorithms on whole-brain structure segmentation, but naturally the labels can be ignored and merged to obtain the sub-cortical structures only. Additionally, the IBSR dataset can be used for both sub-cortical structures and brain tissue segmentation. Other datasets widely adopted to evaluate algorithms for tissue segmentation are two the MICCAI Grand Challenges MRBrainS13 10 and NeoBrainS12 11 . The difference between the two datasets is that in the former case, the algorithms are evaluated using adults exhibiting WM lesions and, in the latter case, the data comes from neonatal subjects.\nThe main evaluation measures for the challenges mentioned previously are DSC, specificity, sensitivity, positive predictive value (precision), average surface distance (ASD), average volumetric difference (AVD) and modified Hausdorff distance (MHD)."}, {"section_title": "Evaluation measurements", "text": "Regarding the application, we found that the CNN models were applied for MS lesion segmentation, brain tumour segmentation and structure segmentation. Although there are public datasets available for these applications, researchers still prefer to work on their own databases. One main reason might be the limitation of the number of training samples available in the public datasets. As stated before, working with a small dataset affects the performance of deep CNN models enormously and imposes restrictions on practitioners from fully exploiting the capacity of those algorithms.\nThe evaluation measurements compare the output of segmentation algorithms with ground truth in either a pixelwise or a volume-wise basis. For a more robust assessment, more than one expert might be involved in generating the ground truth volumes to avoid inter-observer variability. In the reviewed literature, the results were evaluated using different evaluation metrics. In cases of MS and tumour segmentation, the most common evaluation measurements are DSC, precision, recall, the true positive rate (TPR) and the positive predictive value (PPV). In addition, absolute volume difference, lesion-wise true positive rate (LTPR) and the lesion-wise false positive rate (LFPR) are also employed. Similarly, for brain tumour and structure segmentation applications, the DSC score, precision, and recall are among the widely used measurements. One main reason for this choice is that the challenge organisers primarily rely on these results. Table 4 summarises the types of databases, numbers of samples, modalities considered, evaluation measurements applied and corresponding results reported of the surveyed works. Table 4 : Summary of results in the reviewed papers. The acronyms for the sequences stand for: susceptibility-weighted imaging (SWI), quantitative susceptibility mapping (QSM), proton density (PD), fractional anisotropy (FA), magnetisation prepared rapid gradient echo (MPRAGE) and gradient echo (GE). The acronyms in the measurements stand for: modified Hausdorff distance (MHD), positive predictive (PP) and average symmetric surface distance (ASSD). The acronyms in results are: grey matter (GM), white matter (WM), cerebrospinal fuild (CSF), dataset from University of North Carolina (UNC) and dataset from Children's Hospital Boston (CHB). "}, {"section_title": "Discussion and future directions", "text": "Deep CNNs have made a large impact in a broad range of application domains. Today, they are the first choice to solve many problems in computer vision, speech recognition and natural language processing. Taking computational advantage of working with small 2D and 3D patches, rather than the entire slice or volume, researchers in brain MR image analysis are able to train deep CNNs to obtain accurate segmentation algorithms. This success has received overwhelming acceptance by the community, in which shallow architectures were the dominant approaches. Currently, most CNN architectures have many layers, including additional normalisation layers, such as batch normalisation. Furthermore, each architecture is becoming increasingly more sophisticated, employing ideas from optimisation and probabilistic models.\nWith regard to the architectures and their performances, the majority of the proposed works in Table 1 used 2D architectures. Although working with each architecture has its own computational advantages and drawbacks, to obtain good generalisation requires an architecture with optimised layers, considering the class imbalance and selecting the best hyper-parameters and advanced training procedures. From the results shown in Table 4 , methods with 2D CNN architecture with sufficient depth [22] , cascade [24] , \"short-cut connections\" [21] and parallel networks [25, 132] showed top performance in their respective applications. After all, there is no universal architecture, and the ongoing research contributes to obtaining a model that can learn to provide a good representation of the underlying input image without suffering from significant over-fitting.\nIn the reviewed works, we observed that most of the proposed methods emphasised the shortcomings of working with deep CNN models. First, there is the computational requirement. Analysing, manipulating and processing each voxel in a volume is computationally very expensive. The enormous amount of memory needed to store the extracted patches and the large amount of time required to process them constitute a difficulty. The deep learning software libraries used to implement layers of deep CNNs have either parallel or distributed frameworks, which help researchers to train their models in multi-core architectures or GPUs. Second, training CNN models for brain image analysis is demanding because of the data imbalance problem, especially with small lesion or structure segmentation. For instance, in tumour or MS lesion segmentation, with most of the lesions having very small sizes compared to the entire volume, obtaining good generalisation is a challenge. Two-phase training with datasets with different tumour to healthy tissue ratios [24] , careful patch selection [22, 25, 130] and manipulation of the cost function [21] are among the proposed methods to overcome this problem. Although we observed the success of CNNs, their full capacity has not yet been fully leveraged in brain MRI analysis. The improved convolution modules [5, 6, 98] , recently proposed activation functions such as SReLU [56] , Probout [60] , ELU [156] and PELU [62] and emerging architectures for large scale object recognition [157] , scene labelling and segmentation application [158, 159] have yet to be investigated with the expected improvements in performance.\nIt is becoming very common practice in the computer vision community to release source codes to the public. This practice helps to expedite the research in the field. The most commonly used deep learning libraries for MRI segmentation are Theano 12 , Caffe 13 , and Keras 14 . Another recommended practice is validating the model on different datasets. Few works [21, 22, 23, 25] have reported their results on three different public datasets. This practice opens the door to design a robust model that can be applied on datasets of similar applications but with different types of MRI scanners, imaging modalities and numbers of training cases.\nWith the lack of training data, the poor spatial resolution of MR images and the need for a short prediction time, it has been impossible to train very deeply the enormous CNNs widely used in computer vision fields. To train such networks, much work is needed in designing faster methods to perform convolutions. FFT algorithms [21] and faster matrix multiplication methods [160] have been used to improve the computation speed of CNNs, but there is still enormous room for improvement in the training algorithms of deep CNNs using variants of SGD [161] and their parallelised and distributed implementations. The new algorithms to come are expected not only to improve the performance of deep CNNs but also highly to be optimised, with less or no hyper-parameters, which constitute one of the major bottlenecks for most users to tune."}]