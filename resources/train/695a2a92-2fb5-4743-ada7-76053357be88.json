[{"section_title": "Abstract", "text": "With advances in data collection technologies, tensor data is assuming increasing prominence in many applications and the problem of supervised tensor learning has emerged as a topic of critical significance in the data mining and machine learning community. Conventional methods for supervised tensor learning mainly focus on learning kernels by flattening the tensor into vectors or matrices, however structural information within the tensors will be lost. In this paper, we introduce a new scheme to design structure-preserving kernels for supervised tensor learning. Specifically, we demonstrate how to leverage the naturally available structure within the tensorial representation to encode prior knowledge in the kernel. We proposed a tensor kernel that can preserve tensor structures based upon dual-tensorial mapping. The dual-tensorial mapping function can map each tensor instance in the input space to another tensor in the feature space while preserving the tensorial structure. Theoretically, our approach is an extension of the conventional kernels in the vector space to tensor space. We applied our novel kernel in conjunction with SVM to real-world tensor classification problems including brain fMRI classification for three different diseases (i.e., Alzheimer's disease, ADHD and brain damage by HIV). Extensive empirical studies demonstrate that our proposed approach can effectively boost tensor classification performances, particularly with small sample sizes."}, {"section_title": "Introduction", "text": "Supervised learning is one of the most fundamental data mining tasks. Conventional approaches on supervised learning usually assume, explicitly or implicitly, that data instances are represented as feature vectors. However, in many real-world applications, data instances are more naturally represented as second-order (matrices) or higher-order tensors, where the order of a tensor corresponds to the number of modes or ways. For example, in computer vision, a grey-level image is inherently a 2-D object, which can be represented as a second-order tensor with the column and row modes [21] ; in medical neuroimaging, an MRI (Magnetic Resonance Imaging) image is naturally a third-order tensor consisting of 3-D voxels [3] . Supervised learning on this type of data is called supervised tensor learning, where each instance in the input space is represented as a tensor. With the rapid proliferation of tensor data, supervised tensor learning has drawn significant attention in recent years in the machine learning and data mining communities.\nA straightforward solution to supervised tensor learning is to convert the input tensors into feature vectors, and feed the feature vectors to a conventional supervised learning algorithm. However, tensor objects are commonly specified in high-dimensional space. For example, a typical MRI image of size 256 \u00d7 256 \u00d7 256 voxels contains 16, 777, 216 features [23] . This makes traditional methods prone to overfitting, especially for small sample size problems [4] . On the other hand, tensorial representations retain the information about the structure of the high-dimensional space the data lie in, such as about the spatial arrangement of the voxel-based features in a 3-D image. When converting tensors into vectors, such important structural information will be lost. In particular, the entries of a tensor object are often highly correlated with surrounding entries. For example, in MRI image data, adjacent voxels usually exhibit similar patterns, which means that the source images contain redundant information at this voxel. It is believed by many researchers that potentially more compact and useful representations can be extracted from the original tensor data and thus result in more accurate and interpretable models. Therefore, supervised learning algorithms operating directly on tensors rather than their vectorized versions are much desired.\nFormally, a major difficulty in supervised tensor learning is how to build predictive models that can leverage the naturally available structure of tensor data to facilitate the learning process. In the literature, several solutions have been proposed. Previous work on supervised tensor learning mainly focuses on linear models [1, 5, 6, 19, 23] , which assume, explicitly or implicitly, that data are linearly separable in the input space. However, in practice this assumption is often violated and the linear decision boundaries do not adequately separate the classes. Recently, several approaches try to exploit the tensor structure with nonlinear kernel models [16, 17, 22] , which first unfold the tensor along each of its modes, and then use these unfolded matrices to construct nonlinear kernels for supervised tensor learning as shown in Figure 1 (b). However, these methods can only capture the relationships within each single mode of the tensor data, because the structural information about inter-mode relationships of tensor data is lost in the unfolding procedures.\nIn this paper, we study the problem of supervised tensor learning with nonlinear kernels which can adequately preserve and utilize the structure of the tensor data. The major research challenges of supervised tensor learning with structure-preserving kernels can be summarized as follows:\n\u2022 High-dimensional tensors: One fundamental problem in supervised tensor learning lies in the intrinsic high dimensionality of tensor objects. Traditional supervised learning algorithms assume that the instances are represented as vectors. However, in the context of tensors, each data object is usually not represented as a vector but a high-dimensional multi-mode (also known as multi-way) array. If we reshape the tensor into a vector, the number of features is extremely high. Both computability and theoretical guarantee of the traditional models are compromised by this ultra-high dimensionality.\n\u2022 Complex tensor structure: Another fundamental problem in supervised tensor learning lies in complex structure of tensors. Conventional tensor-based kernel approaches focus on unfolding tensor data into matrices [16, 17, 22] which can only preserve the one-way relationships within the tensor data. However, in many real-world applications, the tensor data have multi-way structures. Such prior knowledge about multi-way relationships among features should be incorporated to build more accurate and interpretable models, especially in the case of high dimensional tensor data with \nInput space (b) Conventional tensor kernels"}, {"section_title": "Input space", "text": "Feature space small sample size.\n\u2022 Nonlinear separability: In real-world applications, the data is usually not linearly separable in the input space. Conventional supervised tensor learning methods which can preserve tensor structures are often based upon linear models. Thus these methods cannot efficiently solve nonlinear learning problems on tensor data.\nIn this paper, we propose a novel approach to supervised tensor learning, called DuSK (Dual Structurepreserving Kernels). Our framework is illustrated in Figure 1 (c). Different from conventional methods, our approach is based upon kernel methods and tensor factorization techniques that can fully capture the multiway structures of tensor data. We first extract a more compact and informative representation from the original data using a tensor factorization method, i.e., CANDECOMP/PARAFAC (CP) [10] . Then we define a structure-preserving feature mapping to derive the DuSK kernels in the tensor product feature space, used in conjunction with kernel machines to solve the supervised tensor learning problems. Empirical studies on real-world tasks (classifying fMRI images of different brain diseases, i.e., Alzheimer's disease, ADHD and HIV) demonstrate that the proposed approach can significantly boost the classification performances on tensor datasets."}, {"section_title": "PRELIMINARIES", "text": "Before presenting our approach, we introduce some related concepts and notation of tensors. Table 1 lists some basic symbols defined in this study. We first give a formal mathematical definition of the tensor, which provides an intuitive understanding of the algebraic structure of the tensor that tensor object has the tensor product structure."}, {"section_title": "Definition 1. (Tensor)", "text": "An N th-order tensor is an element of the tensor product of N vector spaces, each of which has its own coordinate system.\nWe use A = (a i1,i2,...,i N ) \u2208 R I1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7I N to denote a tensor A of N order. For n = 1, 2, \u00b7 \u00b7 \u00b7 , N , I n is the dimension of A along the n-th mode. Based on the above definition, we define inner product, tensor norm, tensor product, and rank of a tensor and give CP model as follows: Definition 2. (Inner product) The inner product of two same-sized tensors A, B \u2208 R I1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7I N is defined as the sum of the products of their entries:"}, {"section_title": "Definition 3. (Tensor norm) The norm of a tensor", "text": "A is defined to be the square root of the sum of all entries of the tensor squared, i.e., (2.2)\nAs we see the norm of a tensor is a straightforward generalization of the usual Frobenius norm for matrices and of the l 2 norm for vectors."}, {"section_title": "Definition 4. (Tensor product)", "text": "The tensor product A \u2297 B of tensors A \u2208 R I1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7I N and B \u2208\nfor all values of the indices. denotes the inner product in some feature space\nis the rank of tensor A \u03c6(.) denotes the feature mapping \u03ba(., .)\nrepresents a kernel function\nIt is worth mentioning that a rank-one tensor, is still analogously to the matrix case, a tensor that is a tensor product of vectors (N th-order tensor requires N vectors). Additionally, notice that for rank-one tensors"}, {"section_title": "Definition 5. (Tensor rank) The rank of a tensor", "text": "A is the minimum number of rank-one tensor to fit A exactly."}, {"section_title": "Definition 6. (CP factorization) Given a tensor", "text": "A \u2208 R I1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7I N and an integer R, if it can be expressed as\nwe call it CP factorization (see Figure 2 for graphical representations). For convenience, in the following we write"}, {"section_title": "APPROACH", "text": "In this section, we first formulate the problem of tensorbased kernel learning and then elaborate on our DuSK. For the sake of brevity, hereafter we restrict our discussion to classification problems."}, {"section_title": "Problem statement Considering a training set of M pairs of samples", "text": "for binary tensor classification problem, where X i \u2208 R I1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7I N are the input of the sample and y i \u2208 {\u22121, +1} are the corresponding class labels of X i . In [6] , it was noted that the problem of tensor classification can be stated as a convex quadratic optimization problem in the framework of the standard linear SVM. Based on this result, we show how it can be modeled as a kernel learning problem.\nSuppose we are given the optimization problem of Figure \nWhere W is the weight tensor of the separating hyperplane, b is the bias, \u03be i is the error of the ith training sample, and C is the trade-off between the classification margin and misclassification error.\nObviously, the optimization problem in (3.6)-(3.8) is the generalization of the problem of the standard linear SVM to tensor patterns in tensor space. When the input samples X i are vectors, it degenerates into the standard linear SVM. As such, based on the kernel method for the extension of linear SVM to the nonlinear case\u2212by introducing a nonlinear feature mapping \u03c6 :\nH , we develop a nonlinear extension of (3.6)-(3.8) in the following, which is critical for the derivation of the model for tensor-based kernel learning.\nGiven a tensor X \u2208 R I1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7I N , we assume it is mapped into the Hilbert space H by (3.9) \u03c6 : X \u2192 \u03c6 (X ) \u2208 R H1\u00d7H2\u00d7\u00b7\u00b7\u00b7\u00d7H P .\nNote that the project tensor \u03c6 (X ) in space H may have different order with X , and each mode dimension is higher even an infinite dimension depending on the feature mapping function \u03c6(.). Such a Hilbert space is called the high-dimensional tensor feature space or simply a tensor feature space. According to the same principle as the construction of linear classification model in the original tensor space, we construct the following model in this space:\nFrom the viewpoint of high-dimensional tensor feature space, this model is a linear model. However, from the viewpoint of the original tensor space, it is a nonlinear model. When the input samples X i are vectors, it degenerates into the standard nonlinear SVM. When the feature mapping function \u03c6(.) is an identical function, i.e., \u03c6(X ) = X , it is the same as that in (3.6)-(3.8). Thus, we say that the optimization model (3.10)-(3.12) is the nonlinear counterpart of (3.6)-(3.8).\nLet us now show how this model can be exploited to obtain tensor-based kernel optimization model. Using Lagrangian relaxation method [2] , it is easy to check that the dual problem of (3.10)-(3.12) is\nWhere \u03b1 i are the Lagrangian multipliers and \u03c6 (X i ) , \u03c6 (X j ) are the inner product between the mapped tensors of X i and X j in the tensor feature space.\nThe advantage of formulation (3.13)-(3.15) over (3.10)-(3.12) is that the training data only appear in the form of inner products. Based on the fundamental principle of kernel method, by substituting the inner product \u03c6 (X i ) , \u03c6 (X j ) with a suitable tensor kernel function \u03ba (X i , X j ), we thus get the tensor-based kernel model. The resulting decision function is"}, {"section_title": "DuSK", "text": "From the above statement, we can see that tensor-based kernel learning degenerates into the study of kernel function, and the success of kernel methods depends strongly on the data representation encoded into the kernel function. Now we propose the DuSK. Our target is to leverage the naturally available structure of the tensor to facilitate kernel learning.\nTensors provide a natural and efficient representation for multi-way data, but there is no guarantee that such representation will be good for kernel learning. Since learning will only be successful if the regularities that underlie the data can be discerned by the kernel. As with the previous analysis for the characteristics of tensor object, we know that the essential information in the tensor is embedded in its multi-way structure. Thus, one important aspect of kernel learning for such complex objects is to represent them by sets of key structural features easier to manipulate, and design kernels on such sets.\nAccording to the mathematical definition of tensor, we can gain a further understanding of the structure\n. . . Figure 3 : Dual-tensorial mapping of the tensor that tensor object has the tensor product structure. In previous work, it was found that CP factorization is particularly effective for extracting this structure. Motivated by these observations, we investigate how to exploit the benefits of CP factorization to learn a structure-preserving kernel in the tensor product feature space. More specifically, we will represent each tensor object as a sum of rank-one tensors in the original space and map them into the tensor product feature space for our kernel learning. In the following, we illustrate how to design the feature mapping.\nWe start by defining the following mapping on a rank-one tensor\nrespectively. By using the concept of the kernel function, we see that the kernel can be defined directly with inner product in the feature space. Thus, when R = 1, based on the above mapping and Eq. 2.4, we can directly derive the naive tensor product kernels, i.e.,\nDespite this, many authors has demonstrated that a simple rank-one tensor cannot provide compact and informative presentation for original data [24] . The key point is how to design feature mapping when the value of R is more than one.\nBased on the definition of the kernel function, it is easy to find that the feature space is a highdimensional space of the original space, equipped with the same operations. Thus, we can factorize tensor data directly in the feature space the same as original space. This is formally equivalent to performing the following mapping:\nIn this sense, it corresponds to mapping tensors into high-dimensional tensors that retain the original structure. More precisely, it can be regarded as mapping the original data into tensor feature space and then conducting the CP factorization in the feature space. We call it the dual-tensorial mapping function (see Figure 3) . After mapping the CP factorization of the data into the tensor product feature space, the kernel itself is just the standard inner product of tensors in that feature space. Thus, we derive our DuSK:\nFrom its derivation, we know such a kernel can take the multi-way structure flexibility into account. In general, the DuSK is an extension of the conventional kernels in the vector space to tensor space, and each vector kernel can be used in this framework for supervised tensor learning in conjunction with kernel machines."}, {"section_title": "Efficiency", "text": "We consider the case of Gaussian RBF kernel in our framework, which is one of the most popular kernels that have been proven successful in many different contexts. Assume that a set of tensor data {(X i , y i )} M i=1 is given, where X i \u2208 R I1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7I N . The time complexity of computing a Gaussian RBF kernel matrix is O M 2 N n=1 I n and our method DuSK is thus O M 2 R 2 N n=1 I n . A typical characteristic associated with tensor data is very high dimensional while R is often very small, which indicates our proposed method is significantly more efficient than its vector counterpart. It is also worth mentioning that our method depends on CP factorization technique, but it is backed with rapid implementation [13] . The storage complexity is reduced from O M N n=1 I n to O M N n=1 I n , where the data is compressed without quality loss and can be recovered quickly. Furthermore, since the constituent kernels are Gaussian RBF kernels, we can thus reformulate Eq. 3.20 to\nwhere \u03c3 is used to set an appropriate bandwidth. We denote this kernel as DuSK RBF ."}, {"section_title": "Experiment Evaluation", "text": "In this study, we validate the effectiveness of the DuSK RBF kernel within standard SVM framework for tensor classification, which we refer to as DuSK RBF . As an application we consider an example of neuroimaging mining."}, {"section_title": "Data collection", "text": "We use three real-world fMRI datasets in our experimental evaluation as follows.\n\u2022 Alzheimer's Disease (ADNI): The first dataset is collected from the Alzheimer's Disease Neuroimaging Initiative 1 . The dataset consists of records of patients with Alzheimer's Disease (AD) and Mild Cognitive Impairment (MCI). We downloaded all records of resting-state fMRI images and apply SPM8 toolbox 2 to preprocess the data. We deleted the first ten volumes for each individual, and functional images were realigned to the first volume, slice timing corrected, and normalized to the MNI template and spatially smoothed with an 8-mm FWHM Gaussian kernel. Resting-State fMRI Data Analysis Toolkit (REST 3 ) was then used to remove the linear trend of time series and temporally band-pass filtering (0.01 \u2212 0.08 Hz). The average value of each subject over time series was calculated within each of those boxes, thereby resulting in 33 samples and a sum total of 61 \u00d7 73 \u00d7 61 = 271633 voxels (or features). We treat the normal brains as negative class, and AD+MCI as the positive class. Each individual is linearly rescaled to [0, 1]. Feature normalization is an important procedure, since the brain of every individual is different.\n\u2022 Attention Deficit Hyperactivity Disorder (ADHD): The second dataset is collected from ADHD-200 global competition dataset 4 . The dataset contains records of resting-state fMRI images for 776 subjects with 58 \u00d7 49 \u00d7 47 = 133574 voxels, which are labeled as real patients (positive) and normal controls (negative). The original dataset is unbalanced, we randomly sampled 100 ADHD patients and 100 normal controls from the dataset for performance evaluation and the average over time series is conducted. Such dataset are quite special, all algorithms perform bad with normalization, we use non-normalized dataset.\n\u2022 Human Immunodeficiency Virus Infection (HIV): The third dataset is collected from the Department of Radiology in Northwestern University [20] . The dataset contains fMRI brain images of patients with early HIV infection (positive) as well as normal controls (negative). The same preprocessing steps as in ADNI dataset were given. This contains 83 samples with 61 \u00d7 73 \u00d7 61 = 271633 voxels."}, {"section_title": "Baselines and Metrics", "text": "In order to establish a comparative study, we use seven state-of-the-art methods as baselines, each representing a different strategy. We here focus on SVM classifier, since it has been proven successful in many applications.\n\u2022 Gaussian-RBF: a Gaussian-RBF kernel-based SVM, which is now the most widely used vector-based method for classification. In the following methods, if not stated explicitly, we use SVM with Gaussian RBF kernel as the classifier.\n\u2022 Factor kernel: a matrix unfolding based tensor kernel, which is recently proposed in [16] and the constituent kernels belong to a class of Gaussian RBF kernels.\n\u2022 K 3rd kernel: a class of vector-based tensor kernels, aiming at representing the tensor in each vector space to capture structural information and have been applied to analyze fMRI images in conjunction with Gaussian RBF kernel [14] .\n\u2022 Linear SHTM: a linear support higher-order tensor machine [6] , which is one of the most effective methods for tensor classification that generalizes linear SVM to tensor pattern using CP factorization and can be regarded as a special case of DuSK, namely the constituent kernels are linear kernels. This baseline is used to test the ability of our proposed method to cope with complex (possibly nonlinear) structured data.\n\u2022 Linear kernel: linear SVM has also been increasingly used to handle fMRI data. In some cases, it outperforms SVM using nonlinear kernels.\n\u2022 PCA+SVM: Principal component analysis (PCA) is a vector-based subspace learning algorithms, which are commonly used for dealing with high-dimensional data, in particular fMRI data.\n\u2022 MPCA+SVM: Multilinear principal component analysis (MPCA) [12] is a natural extension of PCA to tensors, which are used to handle high-dimensional tensor data.\nThe first three baselines are used to show the improvement of our proposed method over current kernel approaches to tensor classification. The last two baselines are used to test the effectiveness of our proposed method compared to unsupervised methods for tensor classification.\nThe effectiveness of an algorithm is always evaluated by test accuracy, we utilize it as metrics in the experiments. For our proposed method and linear SHTM, we choose the most popular and widely used enhanced linear search method [13] as its CP factorization strategy. All of the related methods select the optimal tradeoff parameter from C \u2208 {2 \u22125 , 2 \u22124 , \u00b7 \u00b7 \u00b7 , 2 9 } and kernel width parameter from \u03c3 \u2208 {2 \u22124 , 2 \u22123 , \u00b7 \u00b7 \u00b7 , 2 9 }. Considering the fact that there is no known closed-form solution to determine the rank R of a tensor a priori [9] , and rank determination of a tensor is still an open problem [18] , in our method and linear SHTM, we use grid search to determine the optimal rank and the optimal trade-off parameter together, where the rank R \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , 12}. The influence of different rank parameters on the classification performance of our method is also given.\nAll the experiments are conducted on a computer with Intel Core2 TM 1.8GHz processor and 3.5GB RAM memory running Microsoft Windows XP."}, {"section_title": "Classification Performance", "text": "In our experiments, we first randomly sample 80% of the whole data as the training set, and the remaining samples as the test set. This random sampling experiment was repeated 50 times for all methods. The average performances of each method are reported. Table 2 shows the average classification accuracy and standard deviation of seven algorithms on three datasets, where the best result is highlighted in bold type.\nFrom the experimental results in Table 2 , we can observe that the classification accuracy of each method on different dataset can be quite different. However, the best method that outperforms other methods in all datasets is DuSK RBF , especially for ADNI dataset. It is worth noting that in neuroimaging task it is very hard for classification algorithms to achieve even moderate classification accuracy on ADNI dataset since this data is extremely high dimensional but with small sample size. While we can observe an 20% gain over comparison methods. Based on this result, we can conclude that operation on tensors is much more effective than on matrices and vectors for high-dimensional tensor data analysis.\nSo far we have demonstrated that our proposed method is effective for tensor classification. However, it is still interesting to show how the data structure for tensor is actually used in our method. We focus on ADNI dataset to conduct an analysis. Figure 6 shows the visualization of original ADNI object and reconstruction result from our chosen CP factorization. As illustrated, CP factorization can fully capture the multi-way structure of the data, thus our method take it into account in the learning process."}, {"section_title": "Parameter Sensitivity", "text": "Although the optimal rank parameter R , the optimal trade-off parameter C and kernel width parameter \u03c3 are found by a grid search in DuSK RBF , it is still important to see the sensitivity of DuSK RBF to the rank parameter R. For this purpose, we demonstrate a sensitivity study over different R \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , 12} in this section, where the optimal trade-off parameter and kernel width parameter are still selected from C \u2208 {2 \u22125 , 2 \u22124 , \u00b7 \u00b7 \u00b7 , 2 9 } and \u03c3 \u2208 {2 \u22124 , 2 \u22123 , \u00b7 \u00b7 \u00b7 , 2 9 } respectively. According to the aforementioned analysis, we know that the efficiency of DuSK RBF is reduced when R is increased because a higher value of R implies that more items are included into kernel computations. Thus, we only demonstrate the variation in test accuracy over different R on three datasets. As shown in Figure 5 , we can observe that the rank parameter R has a significant effect on the test accuracy and the optimal value of R depends on the data, while the optimal value of R lies in the range 2 \u2264 R \u2264 5, which may provide a good guidance for selection of the R in advance.\nIn summary, the parameter sensitivity study indicates that the classification performance of DuSK RBF +SVM relies on parameter R and it is difficult to specify an optimal value for R in advance. However, in most cases the optimal value of R lies in a small range of values as demonstrated in [6] and it is not time-consuming to find it using the grid search strategy in practical applications."}, {"section_title": "Related Work", "text": "From the conceptual perspective, two topics can be seen as closely related to our DuSK approach: supervised tensor learning and tensor factorization. This section gives a short overview of these areas and distinguishes DuSK from other existing solutions.\nTensor factorizations: Tensor factorizations are higher-order extensions of matrix factorization that elicit intrinsic multi-way structures and capture the underlying patterns in tensor data. These techniques have been widely used in diverse disciplines to analyze and process tensor data. A thorough survey of these techniques and applications can be found in [10] . The two most commonly factorizations are CP and Tucker. CP is a special case of Tucker decomposition which forces the core array to a (super)diagonal form. It is thus more condensed than that of Tucker. In the supervised tensor learning setting, CP is more frequently applied to explore tensor data because of its properties of uniqueness and simplicity [6, 8, 19, 23] . However, in these applications, CP factorization is used either for exploratory analysis or to deal with linear tensor-based models. In this study, we employ the CP factorization to foster the use of kernel methods for supervised tensor learning.\nSupervised tensor learning: Supervised tensor learning has been extensively studied in recent years [1, 5, 11, 19, 23] . Most of previous work has concentrated on learning linear tensor-based models, whereas the problem of how to build nonlinear models directly on tensor data has not been well studied. A first attempt in this direction focused on second-order tensors and led to a non-convex optimization problem [15] . Subsequently, the authors claimed that it can be extended to deal with higher-order tensors at the cost of a higher computational complexity, and proposed a factor kernel for tensors of arbitrary order except for square matrices based upon matrix unfoldings [16] . In the context of this proposal, Signorette et al. [17] introduced a cumulantbased kernel approach for classification of multichannel signals. Zhao et al. [22] presented a kernel tensor partial least squares for regression of lamb movements. A drawback of the approaches in [16, 17, 22] is that they can only capture the one-way relationships within the tensor data, because the tensors are unfolded into matrices. The multi-way structures within tensor data are already lost before the kernel construction process. Different from these methods, we aim to directly exploit the algebraic structure of the tensor to study structurepreserving kernels.\nAnother recent work by Hardoon et al. [7] , although not directly performs supervised tensor learning, is worth mentioning in this context. They introduced the so-called tensor kernels to analyze neuroimaging data from multiple sources, which demonstrated that the tensor product feature space is useful for modeling interactions between feature sets in different domains. In this study, we make use of the tensor product feature space to derive our kernels in vivo the incorporation of CP model. The tensor kernels can be cast as a special case of our framework."}, {"section_title": "Conclusion and Future work", "text": "In this paper we have introduced a new tensor-based kernel methodology and first operate directly on tensors. We have applied our method on the problem of fMRI classification. The results indicate that the prior structural information can indeed improve the classification performance, particularly with small-sample size. As previous work limited on learning with matrices and vectors, this paper provides a new insight into the understanding of the principles and ideas underlying the concept of tensor.\nIn the future, we will investigate the reconstruction techniques of tensor data, so that our method can handle high-dimensional vector data more effectively. Another interesting topic would be to design some special method to address the parameter problem. Further study on this topic will also include many applications of DuSK kernels in real-world unsupervised learning with tensor representations."}]