[{"section_title": "Abstract", "text": "We develop a statistical framework for simulating natural hazard events that combines extreme value theory and geostatistics. Robust generalized additive model forms represent generalized Pareto marginal distribution parameters while a Student's t-process captures spatial dependence and gives a continuous-space framework for natural hazard event simulations. Efficiency of the simulation method allows many years of data (typically over 10 000) to be obtained at relatively little computational cost. This makes the model viable for forming the hazard module of a catastrophe model. We illustrate the framework by simulating maximum wind gusts for European windstorms, which are found to have realistic marginal and spatial properties, and validate well against wind gust measurements."}, {"section_title": "Introduction", "text": "Natural hazard events can have devastating and widespread effects on society. In 2014, natural hazards are estimated to have caused USD 106 and 29 billion of economic and insured loss, respectively. Over the past few decades, European windstorms, for example, have been the second biggest cause of insured loss from natural hazards globally, after US hurricanes. Windstorms Christian, Xavier, Dirk and Tini, which struck over the 2013/2014 winter, caused insured losses totalling USD 3.3 billion. To ensure their solvency, insurance companies must have accurate understanding of their potential losses.\nEvents that cause significant loss are rare, resulting in a lack of vital data and other relevant knowledge. One method for overcoming data scarcity is to simulate events. This can help build a probabilistic view of loss, in addition to providing information on types and strengths of defences required to offer sufficient protection.\nOne strategy for producing probabilistic estimates of losses from natural hazard events is known as catastrophe modelling (see Grossi & Kunreuther [1] for an overview). Such models are usually formed by linking hazard, vulnerability, damage and loss modules. When combined these characterize the extent of the hazard event, the property susceptible to damage, the damage caused to the property, given the hazard, and the subsequent loss. Often catastrophe models are used to estimate loss distributions, of which extreme quantiles are typically important. (For example, the Solvency II directive 1 is based on the 99.5% quantile of a company's annual loss distribution.) Hazard modules can be used to produce arbitrarily many synthetic events from which losses can be calculated, to improve precision of extreme quantile estimates. Various contrasting approaches to the hazard module exist: translations, distortions or parsimonious parametrizations of historical events, which can fail to capture the full population variation in events; physically based simulation models similar to climate models, which can under-represent processes, such as wind gust speeds [2, 3] , and, in turn, underestimate losses; or multivariate statistical models that incorporate extreme value statistics, which have extended from bivariate dependencies [4, 5] to max-stable processes [6] .\nThe area affected by a natural hazard event can vary considerably in size: heatwaves might affect entire continents, whereas flooding events may only span a few metres. To capture events entirely, simulation domains may be very different in size. Local variation of natural hazards can also vary considerably: relative variations in temperature over a small domain are typically much less than those of rainfall. European windstorms, however, can affect large areas and have high local variability; adequate simulations of these must represent a large domain at high resolution. A robust simulation model for natural hazard events must allow various different combinations of domain size, resolution and variability amounts on different spatial scales.\nTo meet these simulation criteria, we propose a framework that couples extreme value and geostatistical methods. Works by Casson & Coles [7] , Cooley et al. [8] and Sang & Gelfand [9] consider a similar coupling, although here we focus on a geostatistical model for residuals-an approach sometimes referred to as anamorphosis. Our approach allows excesses corresponding to exceedances of a high threshold to be simulated, and provides an alternative benchmark for catastrophe models for the following reasons: geostatistical models can give highly efficient simulations of high-resolution random fields, compared with fully multivariate models; various forms for dependence over space and time exist for geostatistical models, which have well represented various types of environmental phenomena (see Diggle & Ribeiro [10] ); and statistically sound marginal estimates for extremes are used. Furthermore, we can quickly implement the method on a personal computer, thus requiring less computational time and resource than other types of hazard module, such as those built similarly to climate models.\nThe following section describes a spatial, extreme value framework for simulating natural hazard events. Section 3 presents a model for simulating extreme windstorm events for a large part of Europe together with some simulations from the model. Section 4 provides a summary of the framework presented and of its performance for simulating windstorm events."}, {"section_title": "Method", "text": "This section gives details of the framework proposed to give realistic simulations of natural hazard events. Throughout let Y(s, t) represent values of a natural hazard process at location s \u2208 S and time t = 1, . . . , T. The simulation model generates excesses above a high threshold, which are assumed to follow a generalized Pareto distribution (GPD). The threshold can either be chosen or estimated, but must be sufficiently high that the GPD assumption is valid. For loss estimation, it is useful if the threshold is below any value of the natural hazard process above which damage can occur; for example, around 25 m s \u22121 for wind gusts. The simulation model must also represent spatio-temporal dependence realistically and include an estimate of the threshold exceedance rate to allow simulations to represent specific time periods."}, {"section_title": "(a) Marginal threshold excess model", "text": "We assume that excesses of some spatially varying threshold u(s), for location s, follow the GPD, so that\nwhere \u03c8 u (s) > 0 and \u03be (s) are scale and shape parameters, respectively, for {y : 1 + \u03be (s)y/\u03c8 u (s) > 0}; the scale parameter may vary with u(s). We consider generalized additive model (GAM, [11, 12] ) forms for GPD parameters, summarized as ( , ) . We focus on GAM forms and propose specific types that are suitable for simulating many different hazard types. These are found to be more flexible than parametric forms, such as those described in Coles [13, ch. 6] , and simpler to fit than nonparametric forms, such as Casson & Coles [7] and Cooley et al. [8] . We represent f 0 ( , ) as a thin plate regression spline [14] , as longitude and latitude have the same units; for this reason, we distinguish longitude and latitude from other covariates. For other covariates, where interactions may occur and scales and units may differ, we propose additive and tensor-product forms [15] . The GPD models with parameters with spline forms can be fitted in various ways. Chavez-Demoulin & Davison [12] propose an approach based on maximum likelihood that combines Newton-Raphson and generalized ridge regression steps. This can be used to simultaneously estimate both regression and smoothing parameters. Their orthogonal parametrization guarantees convergence. For interpretability, we use spline forms for log \u03c8 u (s) and \u03be (s), which are not orthogonal. We estimate smoothing parameters using generalized (approximate) cross-validation [16] , where at each iteration we find GPD parameters by numerically maximizing the penalized likelihood [17, 18] ."}, {"section_title": "(b) Threshold estimation", "text": "Here we consider models for the spatially varying threshold, u(s). A constant threshold may not suit large domains, especially if marginal distributions exhibit large variation. We use quantile regression to estimate u(s), which is assumed to have GAM form. Put briefly, u(s) is chosen to minimize the so-called tilted loss function, subject to some additional roughness penalty terms, which depend on the spline choices (see Koenker [19, ch. 7] for complete details). This approach to modelling and estimating u(s), coupled with a GPD model for excesses of u(s), was proposed by Northrop & Jonathan [20] . We extend the additive spatial GAM forms for longitude and latitude used in Northrop & Jonathan [20] to thin plate regression splines forms. Link functions may also be considered to relate u(s) to GAM terms.\nFor simulations to represent given time periods, the rate of upcrossing of the threshold must be taken into account, which we denote \u03b6 (s) = Pr(Y(s, t) > u(s)). If the quantile used in quantile regression is fixed at \u03b6 , then \u03b6 (s) = \u03b6 for all s \u2208 S, t = 1, . . . , t. Alternatively, the threshold can be fixed and logistic regression used to estimate \u03b6 (s), using a similar additive form to equation (2.2) (see Wood [21] for inference details in this case). "}, {"section_title": "(c) Residual dependence model", "text": "By considering marginal and dependence models separately, the model can be related to copulabased approaches. Disadvantages to this separation, identified in Mikosch [22] , are potential bias in stochastic dependence estimates, a lack of supporting statistical theory and potentially poor representation of multivariate extremes or, by virtue of being static, temporal dependence. Here the advantages, given in \u00a71, of fast, high-resolution simulations, robust dependence forms and extreme-value margins are seen to outweigh these disadvantages. To efficiently achieve highresolution simulations, we restrict attention to residual models based on spatially continuous stochastic processes, which are widely used in geostatistics.\nGaussian processes are commonly used in geostatistics and may be used for anamorphosis. They are robust and can give efficient simulations on high-resolution grids using circulant embedding methods [23] , or for irregularly spread locations. The Gaussian process model imposes asymptotic independence between different locations s and s at time t: if y + is the upper endpoint of the distribution of Y(s, t) and \u03c7 = lim y\u2192y + Pr{Y(s, t) > y | Y(s , t) > y}, then \u03c7 = 0 [24] ; when \u03c7 > 0 asymptotic dependence occurs. This assumption may be inappropriate for some natural hazards (see Coles et al. [25] for environmental examples). The Student's t-process can be seen as a generalization. It imposes asymptotic dependence when its degrees of freedom, \u03bd, are finite, and asymptotic independence when infinite, as the latter can be parametrized as a Gaussian process. We treat \u03bd as an unknown parameter.\nWe consider a Student's t-process tail model: a Student's t and continuous space extension of the multivariate Gaussian tail model of Bortot et al. [26] . The marginal model of \u00a72a can be used to give uniformly distributed residuals on [ [27] [28] [29] . We focus on anamorphosis-based models for their interpretability, relative to widely used geostatistical methods and simulation efficiency. Related findings are presented in \u00a73b(iii), d(iii)."}, {"section_title": "(d) Event simulation", "text": "Values of a natural hazard process are simulated for a fixed number of locations S = {s 1 , . . . , s S * }, which may be regularly or irregularly spread. Let T be the time domain of the event. For example, if daily values are simulated for a 3-day event centred at time t * , T = {t * \u2212 1, t * , t * + 1}. The following algorithm details simulation of a single event, and assumes that the threshold, marginal excess and residual dependence models have been fitted.\nRepeat steps (i)-(iii) to give the required number of simulated events. Extensions for the case where simulated values of non-exceedances of the threshold u(s) are also required are proposed in \u00a74."}, {"section_title": "Simulation of extreme windstorm events", "text": "This section illustrates the framework by simulating windstorm events for a large part of Europe."}, {"section_title": "(a) Data", "text": "The windstorm data are maximum daily surface wind gust speeds (in metres per second), measured by anemometer and extracted from the National Climatic Data Center global summary of the day database. 2 These are supplemented with MIDAS land surface station observations [30] , to improve coverage for Norway and the UK, and ECA&D data [31] , to improve coverage for Spain. The period 1 January 1994 to 31 December 2014 is studied. Only winter (defined as December, January, February) storms are modelled. Locations of data stations used are shown on figure 1 together with a plot representing their elevations. Figure 2 shows plots of wind gust speeds for seven stations. These are chosen as they are the nearest two to London and Paris, and the nearest to Madrid, the Ruhr and Milan (which are the five most populated metropolitan areas within the domain studied, in descending order 3 ). Whether the gusts speeds are asymptotically dependent or independent is assessed. These probabilities are estimated empirically and uncertainty quantified by bootstrap re-sampling the data. Two stations are chosen near to London and Paris to help assess asymptotic dependence for these proximate and highly populated station pairs (see also \u00a73b(iii)). Between the London and Paris stations, dependence tends to be higher across the range of gust speeds, compared with between stations from different areas. However, the rarity of concomitant exceedances for the highest gust speeds, such as above 25 m s \u22121 , makes it inconclusive from figure 2 whether the gust speeds are asymptotically dependent or independent. Further analysis is given in \u00a73b(iii) by considering extremal index estimates."}, {"section_title": "(b) Model specification and estimates", "text": "This section gives details of the model specifications used to estimate and then simulate extreme wind gust speeds."}, {"section_title": "(i) Threshold estimation", "text": "We use a spatially varying threshold. A constant threshold did not suit the large domain, primarily because it was not possible to find a single threshold for which the GPD assumption was valid for all stations. The high proportion of missing data, which is typical of European wind gust measurements, presents a significant obstacle to estimating the threshold. Estimating the threshold at a prespecified quantile using only available measurements leads to bias. This is because of a tendency towards the start of the study period for stations to only record high wind gust measurements; ignoring this, and assuming that measurements are missing at random, leads to severe overestimation of high quantiles. We overcome this with an infilling procedure based on a regression model: its response is Box-Cox transformed wind gust measurements using \u03bb = 0.2 [32] ; its covariates are taken from temporally complete ERA-Interim reanalysis data [33] , such as gust and wind speeds and mean sea-level pressure. A multivariate Gaussian model is then fitted to the model's residuals. The infilled data are obtained by simulating residuals for the missing data, conditional on residuals derived from available data. Residuals and predictions from the ERA-Interim regression model are combined, and the infilled gust speeds obtained by inverting the Box-Cox transformation. These infilled values are only used to estimate the threshold, and not the excess model.\nAs described in \u00a72b, and proposed by Northrop & Jonathan [20] , the threshold is estimated by non-parametric quantile regression. The 98th percentile is estimated for both practical and theoretical reasons. Klawa & Ulbrich [34] found this to perform well for estimating financial loss from European windstorms over Germany, while it is sufficiently high that tests show the GPD assumption for threshold excesses to be valid, as supported by figures 6 and 7. We find estimation of both the threshold and excesses models unreliable at higher thresholds, due to too few data, and that GPD estimates incur systematic spatial bias for thresholds below the 95th percentile. Empirical estimates of the 98th percentile for each station show variation with elevation and mean winter wind speed. 4 Consequently, the quantile regression model for u(s) takes the form\nwhere f 0 ( , ) is a thin plate regression spline, elev(s) and mws(s) are elevation and mean winter wind speed at location s, respectively, and f 12 ( , ) is formed by a tensor-product of P-splines (which negates the need for f p ( ) terms). Figure 3 shows that elevation influences the threshold estimate most; mean winter wind speed has a small, but much lesser, effect. The thin plate spline captures that windstorms tend to track north of the UK and Norway. Relative to the rest of the region studied, wind gust data for the Balkans are scarce; their increased estimate is accompanied by large uncertainty.\n(ii) Excess model\nWe then model excesses of the estimated 98th percentile as GPD. The following GAM forms are used: where f 1 ( ) and f 2 ( ) are P-splines. Estimates of log \u03c8 u (s) and \u03be (s) are shown in figure 4 , with contributions from each of the GAM components shown in figure 5 . Elevation appears to have the greatest effect. In general, increased elevation coincides with higher values of log \u03c8 u (s) and lower values of \u03be (s), although for log \u03c8 u (s) this relationship is not entirely monotonic increasing. Increased mean winter wind speed also coincides with higher log \u03c8 u (s) values, but to a much lesser extent than elevation. The thin plate spline contributions show that log \u03c8 u (s) decreases smoothly from the northwest to the southeast of the region studied; a similar-but lesser-effect can be seen for \u03be (s), although its increased values around the Balkan states are most apparent. Uncertainty in the \u03be (s) estimate around the Balkan states is, however, again large, due to few data. Quantile plots for assessing the GPD fits are shown in figure 6 . These are achieved by omitting the data for the nine validation sites (A, C, E-K) to give estimates of a reduced model, which is used to predict sites' GPD parameters. Data for these sites are included when estimating the final model. For the nine sites considered, which coincide with the nine most populated metropolitan areas, and are highlighted in figure 1 , adequate agreement between model-based and empirical estimates of the distribution of excesses of u(s) can be seen. Slight signs of higher model-based estimates than observed gust speeds are apparent. We suspect that careful quality control on measurements could offer slight improvement, as, due to a scarcity of data, we have taken a conservative approach to quality control. We have also only considered fairly "}, {"section_title": "Figure 5. (a) Additive contributions of f 0 (lon(s), lat(s)) (i), f 1 (elev(s)) (ii) and f 2 (mws(s)) (iii) to (log) GPD scale parameter estimate. (b) Contributions of f 0 (lon(s), lat(s)) (i) and f 1 (elev(s)) (ii) to GPD shape parameter estimate.", "text": "simple covariate choices. Other similarly simple covariates, such as mean winter temperature, were also considered. Of those considered, we have chosen the best fitting model. However, allowing less simple covariates may improve the model's fit, in particular covariates derived from reanalysis data. We also collectively assess the fits for the stations using the Kolmogorov-Smirnov (K-S) test. Ties in the data make the K-S test in its standard form unreliable. Instead, we use a Monte Carlo technique to obtain p-values, which involves simulating sets of excesses from estimated GPDs, rounding the simulated excesses, and then computing the K-S test statistic for each set of simulated data. The p-values, shown for each station in figure 7 , result from comparing the K-S test statistic for the gust speed data with the test statistics from the simulated data. For 40 of 789 stations ( 5.1%), the p-value is below 5%, which indicates adequate fit of the GPD across all stations. Importantly, there are no signs of systematic spatial deviations, such as large parts of the study region where the GPD fit is inadequate."}, {"section_title": "(iii) Residual model", "text": "We build a residual model based on an anisotropic correlation function. This is formed by considering a transformed space with coordinatess = ARs, where Figure 6 . GPD quantile-quantile plots for nine stations identified as A, C, E-K in figure 1.\ntracks, which typically causes dependence between gust speeds to be greatest along the track. Angle \u03b8 allows the prevailing direction of the tracks to deviate from perfectly following the longitude or latitude axes. The correlation function includes a nugget, 1 \u2212 \u03c4 , to allow measurement error, and is written\nwhere \u03b4( ) is the Kronecker delta function and d = s \u2212s for locationss ands in transformed space. We choose the Whittle form for \u03c1( ), so that\nfor \u03ba > 0 and K \u03ba the modified Bessel function of the second kind. The Whittle form is chosen for compatibility with the storm model of Cox & Isham [36] , which follows in \u00a73c(i); otherwise Mat\u00e9rn or powered exponential forms might be considered. We are required to estimate \u03c4 , \u03c6 1 , \u03c6 2 , \u03b8 and \u03ba. Figure 7 shows pairwise estimates of the extremal index for the nine validation sites of figure 6 based on the censored method of Schlather & Tawn [37] , with 95% uncertainty bounds estimated by profiling the likelihood. The maximum-likelihood estimates generally equal 2 (and uncertainty bounds all include 2), which indicates asymptotic independence. This is confirmed when we fit the tail Student's t-process, under the assumption of no temporal dependence, and find that its likelihood is maximized for large \u03bd (i.e. \u03bd > 10 000); at this value the difference between Student's t-and Gaussian processes is negligible. Consequently, we focus on Gaussian processes. The estimated correlation structure of the Gaussian process is represented in figure 8 by a semivariogram, which compares empirical with model-based estimates of the dependence structure against distance defined on transformed space, d s = s \u2212s ; the structure itself is also shown with distance defined on the original scale. account, we see that simulations from the model are consistent with the extremal index estimates from the wind gust data, which supports the adequacy of the estimated dependence structure."}, {"section_title": "(c) Simulated windstorm events (i) Simulation specification", "text": "We base windstorm simulations on a spatio-temporal extension of the Whittle correlation function, developed as a storm model in Cox & Isham [36] and later presented for Gaussian processes in Gneiting et al. [38] and Schlather [39] . Each event, j = 1, . . . , J, is assumed to move at a random bivariate normally distributed velocity V j \u223c BVN(\u03bc j , M/2) and has corresponding correlation function\nThe large number of windstorms, J, results in too many \u03bc j s to reliably obtain maximum-likelihood estimates of M and the \u03bc j s alongside \u03c4 , \u03c6 1 , \u03c6 2 , \u03b8 and \u03ba. A simpler model with \u03bc j = \u03bc for all j fails to capture the variation that is empirically evident. Instead, the \u03bc j s and M are empirically estimated from storm tracks extracted from the ERA-Interim reanalysis [33] using the tracking algorithm of Hodges [40, 41] . Comparison against the windstorm track observations of the IBTrACS database ( [42] ; not shown) shows that, although the intensities of windstorms may tend to be underestimated, velocities are represented sufficiently well for the purposes of this simulation.\n(ii) Results on simulated events\nEvents are simulated for locations s k on a 300 \u00d7 300 grid S = \u222a n S k=1 s k , where n S = 90 000. We simulate events equivalent to 10 000 December-February winters and analyse 3-day events because a 72-h period is typically used in the insurance industry to define individual events. Only excesses corresponding to exceedances of u(s) are simulated. Under this specification, each simulated winter takes approximately 20 s, but simulations are entirely paralellizable. Figure 9 shows two randomly chosen 3-day event simulations. For both events, exceedances of the estimated 98th percentile can be seen on each of the three simulated days. Very few damagecausing gust speeds exceeding 25 m s \u22121 occur. Therefore, neither event is likely to be classed as extreme in terms of the financial loss that it might have caused.\nTo highlight potentially catastrophic events, two measures are used to quantify whether a windstorm event is extreme [2] . Each event, j = 1, . . . , J, is defined for locations s \u2208 S and a time domain T j = {t j \u2212 1, t j , t j + 1}, where t j gives the peak time of event j. Plots of the events with the highest values of each of these measures are shown in figure 9 .\nKlawa & Ulbrich [34] propose a storm severity index (SSI) given by\n, which is designed to quantify an event's kinetic energy, which relates closely to financial loss, where S SSI \u2282 S is a region for which the SSI will be calculated. Compared with S, S SSI excludes the Balkan states, where shape parameter estimates are relatively imprecise; corresponding simulated gusts would otherwise dominate SSI values. Windstorms with large financial losses seldom occur in the excluded regions. Figure 9 shows the 3-day event with highest SSI. We see that many threshold exceedances occur on day 3 in various countries, in particular, France, Switzerland, Germany, Italy, Belgium and Austria, and also, to a lesser extent, the Netherlands, UK, Denmark and Slovenia. The renowned windstorms Martin, Lothar and Kyrill affected a similar set of countries. Figure 10 . (a-f ) Simulated wind gusts for single days based on differing measurement error amounts, i.e. 1 \u2212 \u03c4 in equation (3.2) . Simulations use the same seed to aid comparison between different signal-to-noise ratios.\nWe define the exceedance area of a storm as\nThis measure captures an event's spatial extent over 3 days; that is, the proportion of Europe affected by high wind gust speeds, relative to the local 98th percentile. Figure 9 also shows the 3-day event with the largest exceedance area. The event with largest exceedance area is very similar to that with largest SSI, as it has greatest effect on day 3 and affects a similar set of countries."}, {"section_title": "(d) Alternative residual models", "text": "One of the benefits of the separation of the simulation model into marginal and residual models is the ability to combine alternative models for each.\n(i) The effect of measurement error\nThe simulations in figure 9 could prove awkward for loss estimation if the signal-to-noise ratio (the ratio of \u03c4 to 1 \u2212 \u03c4 in equation (3.2) ) is seen to be small, i.e. if the gust speeds have relatively large measurement error. Figure 10 shows single-day windstorm events simulated with different signal-to-noise ratios, based on the GPD estimates described in \u00a73b(ii). Part of the motivation for this analysis is that increasing the signal-to-noise ratio might lead to simulated events representative of unobserved actual windstorm events, as opposed to representing measurements. The effect of increasing the signal-to-noise ratio is clear, with the spatial structure Figure 11 . Events simulated using ERA-Interim residual model. Two randomly chosen 3-day windstorm event simulations (rows 1 and 2) and an extreme event (row 3) based on largest SSI or exceedance area (both happen to coincide) over 3 days.\nof events becoming smoother, eventually forming larger, more distinct patches affecting fewer parts of Europe.\n(ii) A reanalysis-based residual model\nAs a further sensitivity analysis, we swap the measurement-based residual model for one based on ERA-Interim reanalysis wind gust output, which are exempt from measurement error. We estimate the ERA-Interim 3-day dependence structure using an empirical probability transformation to convert margins to Gaussian, interpolate the resulting data onto the simulation grid S and then calculate the empirical correlation matrix. We then use this to simulate Gaussian residuals, which are then converted to the original wind gust scale using the GPD estimates of \u00a73b(ii). Figure 11 shows simulated events chosen according to the same criteria as figure 9 . Increased smoothness of the events is clear to see. We also note that the events with largest SSI and exceedance area coincide. As the ERA-Interim data are aggregated onto a 0.75 \u2022 grid, their residual structure will inevitably be smoother than in reality, which will lead to larger-than-realistic loss estimates; residuals based on the measurements will suffer the opposite. These alternative residual models could therefore be used to place upper and lower bounds on loss estimates. Based on the preceding two sensitivity analyses for the residual model, we should hope to reduce the uncertainty in loss estimates with higher-resolution wind gust output instead of ERA-Interim and/or more precise measurements.\n(iii) A max-stable process residual model\nTo fit a max-stable process residual model, we obtain unit Frech\u00e9t residuals as described in \u00a72c, i.e. e(s, t) = \u22121/ log e * (s, t) with e * (s, t) defined in equation (2.3). We fit the max-stable models by maximizing pairwise censored likelihoods [43] [44] [45] [46] and hence present the two-dimensional case. For two locationss ands , defined on the transformed space of \u00a73b(iii), we assume that\nPr(e(s, t) < e 1 , e(s , t) \u2264 e 2 ) = exp{\u2212V(e 1 , e 2 )}, where V( , ) is the exponent measure function. As asymptotic independence is indicated by figure 7 , we consider the extension of the Schlather model [27] proposed in Davison & Gholamrezaee [6] , where\nwith c( ) as in equation (3.2), where d = s \u2212s . We take \u03b1(d) = {1 \u2212 |d|/(2r)} + , which gives asymptotic independence when |d| > 2r and find that r \u2192 \u221e. The resulting estimate of c( ) has a relatively large nugget estimate (\u03c4 0.63); its \u03c1( ) component is shown on the original coordinate scale in figure 12 , which shows a very short range of dependence. Extremal index estimates for the max-stable model with distance defined on the transformed space, which are shown in figure 12 , highlight a lack of agreement between the max-stable model and the wind gust data. Simulations from the model (not shown), which take on average 90 times longer than those for Gaussian process model, have qualitatively similar characteristics to the \u03c4 = 1 case presented in figure 10 .\nWe have, however, only considered a fairly simple form for \u03b1( "}, {"section_title": "Summary", "text": "We have presented a statistical framework for simulating natural hazard events that combines extreme value theory, to accurately capture event magnitude, and geostatistics, to robustly incorporate spatial dependence. The framework can be used to quickly give high-resolution simulations that can be formally validated. The framework has been used to produce realistic simulations of European windstorm events. By virtue of its generality, speed and that it can be implemented using a standard computer, the framework could be readily used as a hazard module in a catastrophe model for other types of hazard, or as a tool for assessing other modules. Various aspects of the proposed framework that could be changed or extended to bring potential improvements are discussed in the remainder of this section. The Student's t-process residual model includes the Gaussian process as a special case. It can be fitted quickly using maximum likelihood, based on finite-dimensional counterparts of the processes, and subsequently gives fast simulations. The finite-dimensional representationas opposed to continuous-space-could better suit certain types of hazard phenomena. One example, as in studied Keef et al. [47] , is river flows, where complex dependencies between flow gauges may benefit from a finite specification. Its finite-dimensional representation loses certain computational benefits, such as those gained by circulant embedding, but remains relatively efficient. Student's t-and Gaussian processes lack max-stability. Although in \u00a73d(iii) we have looked into using max-stable processes to simulate extreme windstorm events, we note that they may better represent other types of natural hazard, or other temporal resolutions. For example, max-stable processes have been used in Huser & Davison [44] to model hourly rainfall, and for modelling annual maximum snow depths [48] and temperatures [6] . Estimates of the extremal index, as in figure 7 , may help reveal which, if any, residual model is most suitable. If the model is used as a hazard module in a catastrophe model, so that loss estimates are produced, it is important that, in addition to formal statistical validation of the residual model, validation against an appropriate loss function for the application is also performed. The anamorphosis and max-stable models can also be extended to capture dependencies between multiple processes; although a trade-off between resolution and increased dimensionality may be needed to maintain computational feasibility.\nWe have used thin plate spline forms to capture spatial smoothness in marginal distribution parameters, which appear to capture the expected parameter variation reliably, upon appropriate choice of basis dimension and cross-validation of smoothing parameters. We see spline forms as a compromise between parametric forms, which can be inflexible, and stochastic forms, such as Gaussian processes, for which inference might require computer-intensive Markov chain Monte Carlo (MCMC) techniques. We have seen little to justify this extra computational demand and also think it could hinder use of the model by end-users. The GAM forms can be extended to incorporate temporal variability, such as a model for the entire year by modelling each season separately or by allowing temporal variation in parameter estimates. For extreme gust speeds it is not valid to assume a constant distribution across the year, which is likely to be true for most hazard phenomena. Unless a fairly homogeneous period can be identified when events tend to occur, such as heatwaves in summer, a temporal model should be used. Similarly, temporal variability may benefit corresponding residual models.\nFor some hazard types, such as modelling flooding due to extreme rainfall, simulated values of non-exceedances of the threshold u(s) may be required. To model marginal distributions in this case, a GPD tail model can be coupled with a model for non-exceedances, which can be empirical [47] , non-parametric [49] or parametric [50] . To give spatial simulations, empirical estimates of spatial dependence can be used together with a suitable interpolation scheme. A geostatistical model can be used; although a different spatial dependence structure between exceedances and non-exceedances of u(s) should be considered for sufficient flexibility. Alternatively, flooding can be studied by modelling extreme river flows [47] , for which the proposed framework is better suited.\nWe have allowed for measurement error by including a nugget term in correlation functions. This is computationally convenient, but may lack interpretability when compared with the model Y(s, t) \u223c N(W(s, t), \u03c9 2 ), where W(s, t) is an actual-but unobservable-value of the hazard process at location s and time t, as it may be possible to specify \u03c9 2 based on knowledge of the measurement process. The W(s, t)s can then be modelled as the Y(s, t)s have been previously. As it is not possible to integrate out the W(s, t)s analytically within the present framework, numerical procedures, such as MCMC, will be required for inference. Integration would also need to carry over the threshold excess and residual models, negating much of the simplicity offered by GAM forms; this might make stochastic processes preferable for capturing spatial variation in parameters. This model can also be formulated as a hierarchical model and, if informative prior knowledge is available, implemented from a Bayesian perspective.\nOne aspect of the model where scope exists to add generality is the correlation function, which we have assumed is the same across the study region. This assumption can be relaxed; for example, by allowing parameters of the correlation function to vary in space. We considered replacing \u03c6 1 with \u03c6 1 (s) and using simple forms such as \u03c6 1 (s) = exp{\u03c6 1,0 + \u03c6 1,1 lon(s) + \u03c6 1,2 lat(s)}. Although such model extensions could not be reliably estimated here, they may benefit other applications. Correlation can also be defined on an alternative (potentially higher-dimensional) space, such as climate space used in Cooley et al. [8] . When modelling gust speeds over larger domains, it should be noted that storms have highs and lows, which suggests an oscillating correlation function might be more suitable, as illustrated in Lindgren et al. [51] . Windstorms are also known to behave differently over land from sea; therefore partitioning distance into overland and over-sea components might better represent dependencies. For hazard types for which events tend to affect smaller areas, the proposed model may be built based on a smaller region, which might improve the validity of using spatially constant correlation functions.\nData accessibility. The NCDC GSOD European wind gust measurements, which are provided as the electronic supplementary material, cannot be redistributed for commercial purposes."}]