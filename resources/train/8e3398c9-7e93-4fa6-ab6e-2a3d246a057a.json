[{"section_title": "Abstract", "text": "Hippocampal volume is one of the best established biomarkers for Alzheimer's disease. However, for appropriate use in clinical trials research, the evolution of hippocampal volume needs to be well understood. Recent theoretical models propose a sigmoidal pattern for its evolution. To support this theory, the use of Bayesian nonparametric * Bocconi University, Milan, Italy. PhD research funded by CONACyT \u2020 University of Cambridge, Cambridge, UK. \u2021 University of Texas at Austin, USA. \u00a7 Data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.ucla.edu). As such, the investigators within the ADNI provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.ucla.edu/wp-content/uploads/how_to_apply/ADNI_ Acknowledgement_List.pdf. 1 regression mixture models seems particularly suitable due to the flexibility that models of this type can achieve and the unsatisfactory fit of semiparametric methods. In this paper, our aim is to develop an interpretable Bayesian nonparametric regression model which allows inference with combinations of both continuous and discrete covariates, as required for a full analysis of the data set. Simple arguments regarding the interpretation of Bayesian nonparametric regression mixtures lead naturally to regression weights based on normalized sums. Difficulty in working with the intractable normalizing constant is overcome thanks to recent advances in MCMC methods and the development of a novel auxiliary variable scheme. We apply the new model and MCMC method to study the dynamics of hippocampal volume, and our results provide statistical evidence in support of the theoretical hypothesis."}, {"section_title": "Introduction", "text": "Alzheimer's disease (AD) is an irreversible, progressive brain disease that slowly destroys memory and thinking skills, and eventually even the ability to carry out the simplest tasks (ADEAR, 2011) . Due to its damaging effects and increasing prevalence, it has become a major public health concern. Thus, the development of disease-modifying drugs or therapies is of great importance.\nIn a clinical trial setting, with the purpose of assessing the effectiveness of any proposed drugs or therapies, accurate tools for monitoring disease progression are needed.\nUnfortunately, a definite measure of disease progression is unavailable, as even a definitive diagnosis requires histopathologic examination of brain tissue, an invasive procedure typically only performed at autopsy.\nNon-invasive methods can be used to produce neuroimages and biospecimens which provide evidence of the changes in the brain associated with AD. Moreover, biomarkers based on neuroimaging or biological data may present a higher sensitivity to changes due to drugs or therapies over shorter periods of time than clinical measures, making them better suited tools for monitoring disease progression in clinical trials.\nHowever, before biomarkers based on neuroimaging or biological data can be useful in clinical trials, their evolution over time needs to be well understood. Those which change earliest and fastest should be used as inclusion criteria for the trials and those which change the most in the disease stage of interest should be used for disease monitoring.\nIn this work, we focus on hippocampal volume, one of the best established neuroimaging biomarkers for AD. Jack et al. (2010) , in a recent paper, propose a theoretical model for the evolution of hippocampal volume, which is further discussed in . They hypothesize that hippocampal volume evolves sigmoidally with changes beginning early and continuing into late stages of the disease. This theoretical model needs to be validated, before the use of hippocampal volume as a measure for disease severity in clinical trials can be appropriately considered. Thus, in the present paper, we focus on the validation of Jack et al.'s proposed model. Caroli and Frisoni (2010) and Sabuncu et al. (2011) assess the fit of parametric sigmoidal curves, and Jack et al. (2012) considers a more flexible model based on cubic splines with three chosen knot points. This last approach is the most flexible among the three, but they all impose significant restrictions which favor a sigmoidal shape.\nTo provide strong statistical support for the sigmoidal shape hypothesis, a flexible nonparametric regression model is needed that would remove all restrictions on the regression curve allowing the data to choose the shape that provides the best fit.\nThere are many methods for nonparametric regression, and most standard approaches, such as splines, wavelets, or regression trees (Denison et al., 2002; Dimatteo et al., 2001 ), achieve flexibility by representing the regression function as a linear combination of basis functions. Another increasingly popular practice is to place a Gaussian process prior on the unknown regression function (Rasmussen and Williams, 2006) . While these models are able to capture a wide range of regression functions, the assumptions on the distribution of the errors about the mean is quite restrictrive; typically, independent and identically distributed additive Gaussian errors are assumed, and thus, these models are often referred to as semiparametric. In the hippocampal volume study, we not only expect a non linear behaviour for the evolution of the AD biomarker with age, but also suspect the presence of multimodality, heavy tails, and evolving variance in the error distribution due to variability in the onset of the disease and unobserved factors, such as enhanced cognitive reserve or neuroprotective genes.\nIndeed, in a semiparametric analysis of the data, we observe a non-normal behavior in the errors that depends on the covariates, which raises suspicions about the estimated regression curve.\nTo correctly model the data, a nonparametric approach for modelling the conditional density in its entirety is needed. In this way, no specific structure is imposed on the regression function or error distribution, so a fit confirming the hypothesized sigmiodal shape would provide strong statistical support for the theoretical model.\nIn this paper, we investigate the dynamics of hippocampal volume as a function of age, disease status, and gender. To do so, we construct a flexible and interpretable nonparametric mixture model for the conditional density of hippocampal volume which incorporates both continuous and discrete covariates. Simple arguments regarding the interpretation of Bayesian nonparametric regression mixtures lead naturally to regression weights based on normalized sums. To overcome the difficulties in working with the intractable normalizing constant, a novel auxiliary variable Markov chain Monte Carlo (MCMC) scheme is developed. The novel model and MCMC algorithm are applied to study the behavior of hippocampal volume, and the results provide strong support for the theoretical model.\nThe layout of the paper is as follows. In Section 2 we describe the model and provide its unique provision of interpretability. In Section 3 we introduce the associated latent variables necessary for estimating the model via MCMC methods and allowing for us to handle both continuous and categorical covariates simultaneously. Section 4, and the Appendix, details the MCMC algorithm in its entirety for estimating the model, and in Section 5 we present a comprehensive simulation study outlining exactly and precisely how the model works and what it is capable of achieving. In Section 6 we present our main work which is the study of the data for Alzheimer's disease. Finally, Section 7 concludes with a discussion."}, {"section_title": "5", "text": "For independent and identically distributed observations, a standard form of mixture model is given by\nwhere K(\u00b7|\u03b8) is a parametric family of density functions defined on Y and P is a probability measure on the parameter space \u0398.\nIn a Bayesian setting, this model is completed by a prior distribution on the mixing measure P . A common prior choice, a stick-breaking prior, makes P a discrete random measure, which can be represented as\nfor some atoms \u03b8 j \u2208 \u0398, taken i.i.d. from some probability measure P 0 , known as the base measure; and weights w j \u2265 0, such that j w j = 1 (a.s.), constructed from a sequence v j ind \u223c Beta(\u03b6 1,j , \u03b6 2,j ) with w j = v j j <j (1 \u2212 v j ). The mixture model (Lo, 1984) can then be expressed as a countable convex combination of kernels\nFor the covariate dependent density estimation problem in which we are interested, the mixture model (1) can be adapted by allowing the mixing distribution P x to depend on the covariate x and replacing the density model K(y|\u03b8) with a regression model K(y|x, \u03b8), such as a linear model. Hence, for every x \u2208 X,\nOnce again, the Bayesian model is completed by assigning a prior distribution on the family {P x } x\u2208X of covariate dependent mixing probability measures. If the prior gives probability one to the set of discrete probability measures, then\nwhere \u03b8 j (x) \u2208 \u0398, and the w j (x) \u2265 0 are such that j w j (x) = 1 (a.s.) for all x \u2208 X.\nThis general model was introduced by MacEachern (1999; , who focused on the case when the weights are constant functions of x, w j (x) = w j , defined in accordance with a Dirichlet process (DP). Such simplified versions of the model are popular, as inference can be carried out using any of the well established algorithms for DP mixture models (see e.g. Neal, 2000; Papaspiliopoulos and Roberts, 2008; Kalli et al., 2011) .\nRecent developments explore the use of covariate dependent weights. To simplify computations and ease interpretation, atoms are usually assumed not to depend on the covariates. The main constraint for prior specification, in this case, is the condition, j w j (x) = 1 for all x \u2208 X, which is non trivial for an infinite number of positive weights.\nThe only technique currently in use for directly defining the covariate dependent weights is through the stick-breaking representation, given by\nwhere the {v j (\u00b7)} are independent processes on X and independent of the atoms, {\u03b8 j }.\nThere are various proposals for the construction of the v j (x), see e.g. Griffin and Steel The stick-breaking definition poses challenges in terms of the various choices that need to be made for functional shapes and hyper-parameters when defining the {v j (x)}.\nThe difficulties are amplified by the lack of interpretation of the quantities involved.\nMoreover, combining continuous and discrete covariates in a useful way is far from straightforward.\nWe propose a different construction of the covariate dependent weights, which follows from an alternative perspective on mixture models. The idea is to realize that each weight contains information about the relative applicability of each parametric component, within the sample space Y. In a regression setting, covariate dependent weights are necessary because it is not reasonable to assume that such relative importance is equal throughout the entire covariate space X; rather, it depends on the value\nx. Since the nature of such dependence is unknown, the uncertainty about it should be incorporated through prior specification.\nIn the nonparametric mixture model\neach covariate dependent weight w j (x) represents the probability that an observation with a covariate value of x comes from the j th parametric regression model K(y|x, \u03b8 j ).\nThus, letting d be the random variable indicating the component from which an observation is generated, we have that w j (x) = p(d = j|x). A simple application of Bayes theorem implies\nwhere p(d = j) represents the probability that an observation, regardless of the value of the covariate, comes from parametric regression model j; and p(x|d = j) describes how likely it is that an observation generated from regression model j has a covariate value of x.\nTherefore, p(x|d = j) can be defined to reflect prior beliefs as to where in the covariate space the regression model j will have the largest relative applicability. A natural and simple way to achieve this is to define it through a parametric kernel function K(x|\u03c8 j ) and with some prior on the \u03c8 j . Uncertainty about the p(d = j) := w j is expressed through a prior on the infinite dimensional simplex.\nPutting things together, and incorporating the normalizing constant, we have that\nwhere 0 \u2264 w j \u2264 1 for all j and\nNote that the conditional densities p(x|j) are not related to whether the covariates are picked by an expert or sampled from some distribution, which itself could be known or unknown. They only indicate priors about where, in X, regression model j best applies. Moreover, the density p(x) = \u221e j=1 P (j) p(x|j) does not correspond to the distribution from which the covariates are sampled, if indeed they are sampled; it simply represents the likelihood that an observation has a covariate value of x.\nThe key element left to define is K(x|\u03c8 j ). If x is a continuous covariate, a natural choice is the normal density function. In this case, the interpretation would be that there is some central location \u00b5 j \u2208 X where regression model j applies best, and a parameter \u03c4 j describing the rate at which the applicability of the model decays around \u00b5 j . On the other hand, if x is discrete, then a standard distribution on discrete spaces can be used, such as the Bernoulli or its generalization, the categorical distribution.\nEven if x is a combination of both discrete and continuous covariates, it is still possible to specify a joint density by combining both discrete and continuous distributions.\nThis will be explained and demonstrated later on in the paper.\nIt is to be noted that the infinite sum in the denominator of (4) "}, {"section_title": "The latent model", "text": "The aim of this section is to re-express the model in terms of latent variables, which are essential for Bayesian inference. For a sample (y 1 , x 1 ), . . . , (y n , x n ) , the likelihood for the proposed model is given by\nwith covariate dependent weights given by expression (4). The infinite sum in the denominator constitutes an intractable normalizing constant, which makes inference infeasible. However, through a simple trick, which relies on the series expansion,\nwe can move the infinite sum from the denominator to the numerator, thus making inference possible, following the introduction of auxiliary variables.\nIn order to illustrate the ideas with a simplified notation, we start by considering the likelihood of a single data point. We assume the first q elements of x represent discrete covariates, each x h taking values in {0, . . . , G h }, for h = 1 . . . , q; the last p elements of x represent continuous covariates. In this case, we have\nwhere\nTo simplify the expression, we make \u03c4 j \u2261 \u03c4 for all j, but this restriction may be removed with some realistic assumptions on \u03c4 j .\nThe likelihood of the single data point (y, x) may be written as\nand\nNotice that we have redefined the kernel function K(x|\u03c8 j ) by cancelling the precision term \u03c4 from the normal density, which appears both in the numerator and the denominator of the normalized weights expression. In this way, we guarantee that 0 < r(x) < 1 for all x \u2208 X, so we can apply the series expansion (6) to write\nThe last equality relies on the fact that \u221e j=1 w j = 1 almost surely.\nThis trick allows us to move the infinite sum from the denominator to the numerator and equivalently express the likelihood as\nWe therefore introduce a latent variable k taking values in {0, . . . , \u221e}, where the joint density of (y, k) given the model parameters is\nso that marginalizing over k, gives us back the original likelihood (7).\nWe can now deal with the mixture in the usual way, by introducing a latent variable d to indicate the mixture component to which a given observation is associated. Thus, we obtain\nFor the remaining sum, we have the exponent k to consider. We first re-write this term as the product of k copies of the infinite sum,\nand then, introduce k latent variables, D 1 , . . . , D k , arriving at the full latent model\nIt is easy to check that the original likelihood (7) is recovered by marginalizing over\nFor a sample of size n \u2265 1 we simply need n copies of the latent variables. Therefore, the full latent model is given by\nOnce again, we note that the original likelihood (5) can be easily recovered by marginalizing over the d 1:n , k 1:n , and D 1:n . However, the introduction of these latent variables makes Bayesian inference possible, via posterior simulation of the (w j ), the (\u03b8 j ) and the (\u03c8 j ), as we show in the next section.\nA prior for P , defined by a prior specification for the weights (w j ) and the parameters, (\u03b8 j ) and (\u03c8 j ), completes the Bayesian model.\nOur focus, for the prior on the weights (w j ), is on stick-breaking priors (Ishwaran and James (2001)). Therefore, for some positive sequence (\u03b6 1,j , \u03b6 2,j ) \u221e j=1 and independent v j \u223c Beta(\u03b6 1,j , \u03b6 2,j ) variables, we have\nSome important examples of this type of prior are the Dirichlet process, when \u03b6 1,j = 1 and \u03b6 2,j = \u03b6 for all j; the Poisson-Dirichlet process, when \u03b6 1,j = 1 \u2212 \u03b6 1 and \u03b6 2,j = \u03b6 2 + j\u03b6 1 for 0 \u2264 \u03b6 1 < 1 and \u03b6 2 > \u2212\u03b6 1 ; and the two parameter stick-breaking process where \u03b6 1,j = \u03b6 1 and \u03b6 2,j = \u03b6 2 for all j.\nTo complete the prior specification, the (\u03b8 j , \u03c8 j ) are i.i.d. from some fixed distribution F 0 and independent from the (v j ). We define F 0 through its associated density f 0 , which in this case is defined by the product of the following components,\nTogether with the joint latent model, this provides a joint density for all the variables which need to be sampled for posterior estimation, i.e. the (\nHowever, there is still an issue due to the infinite choice of the (d i , D l,i ), which we overcome through the slice sampling technique of Kalli et al. (2011) . Accordingly, in order to reduce the choices represented by (d i , D l,i ) to a finite set, we introduce new latent variables, (\u03bd i , \u03bd l,i ), which interact with the model through the indicating functions 1 \u03bd i < exp(\u2212\u03bed i ) and 1 \u03bd l,i < exp(\u2212\u03beD l,i ) , for some \u03be > 0. Hence, the full conditional distributions for the index variables are given by\nwhere\nAt any given iteration, the full conditional densities for the variables involved in the MCMC algorithm do not depend on values beyond J, so\nwe only need to sample a finite number of the (\u03c8 j , \u03b8 j , w j ).\nThe (w j ) J j=1 can be updated at each iteration of the MCMC algorithm in the usual way, that is, by making w 1 = v 1 and, for j > 1, w j = v j j <j (1 \u2212 v j ), where the (v j ) are sampled independently from Beta distributions with updated parameters (specified in the Appendix).\nThe variables involved in the linear regression kernel, that is, the (\u03b2 j , \u03c3 2 j ) are updated in the standard way, well known in the context of Bayesian regression. Since the normal-inverse gamma prior is conjugate, we simply need to sample from a normalinverse gamma distribution with updated parameters, detailed in the Appendix.\nThe full conditional distribution for the (\u03c8 j ) J j=1 seems somewhat more complicated, due to the additional product term in the latent model (expression 8), involving the latent variables (k i ) and (D l,i ). However, such a product can be easily transformed into a truncation term, by the introduction of additional latent variables. Thus, posterior simulation for the (\u03c8 j ) J j=1 is achieved simply by sampling from standard truncated distributions with updated parameters, which can be easily calculated due to the conjugate prior choice. The details of this procedure, as well as the resulting updated parameters and truncations are presented in the Appendix. At this point, we only mention that the introduction of these additional latent variables does not pose a problem, since they are all conditionally independent given the (\u03c8 j ) J j=1 , and hence can be sampled in parallel, using the \"parfor\" routine in Matlab.\nFinally, for the update of each k i , we use ideas involving a version of reversible jump MCMC (see Green, 1995) introduced by Godsill (2001) , to deal with the change of dimension in the sampling space. We start by proposing a move from k i to k i + 1 with probability 1/2, and accepting it with probability\nThe evaluation of this expression requires the sampling of the additional index D i,k i +1 , and we choose D i,k i +1 = j with probability proportional to w j (1 \u2212 K(x i |\u03c8 j )), for\nSimilarly, if k i > 0, a move from k i to k i \u2212 1 is proposed with probability 1/2, and accepted with probability\nIt is therefore possible to perform posterior inference for the nonparametric regression model proposed, via an MCMC scheme applied to the latent model. We have successfully implemented the method in Matlab (R2012a), and present some results in the next section.\nIn the following examples, the aim is prediction and predictive density estimation, which under the quadratic loss are, respectively, given by\nf (y n+1 |y 1:n , x 1:\nwhere and X n+1 = (1, x n+1 ); and the expectation is taken with respect to the posterior of (w j , \u03b8 j , \u03c8 j ). MCMC estimates for this quantities are used, as specified in the Appendix.\nTo demonstrate the ability of the model to recover a complex regression function with covariate dependent errors, we simulate n = 200 data points (depicted in Figure 1a) through the following formula,\nOur model is given by\nThe prior for (w j ) and (\u03b8 j , \u03c8 j ) is described in Section 4. The prior choice for the (w j ) is a Dirichlet process, i.e. \u03b6 1,j = 1. Rather than using a hyper-prior for the precision parameter, we fix it to be 1. Due to the unidentifiability of the weights, such a practice corresponds to the standard solution of fixing the location of one of the variables for models with identifiability issues. The unidentifiability of the weights arises from the fact that they are given by w j (x) \u221d w j K(x|\u03c8 j ). We resolve this in the usual way by fixing the locations of the (w j ) rather than assigning a hyper-prior to the precision parameter. Note the model is fundamentally different from the usual DP mixture model, where the weights (w j ) are the weights, without any multiplicative factors. Hence in the DP model the use of a hyper-prior for the precision parameter is known to be important.\nFor the prior of (\u03b8 j , \u03c8 j ), we set \u03b2 0 = (0, 5/8) ; C \u22121 = diag(9, 1/4); \u03b1 1 = 1; \u03b1 2 = 1; \u00b5 0 = 0; c = 1/8; a = 1; b = 1. We are also able to produce estimates of the full conditional density f (y|x) at any value of x in the covariate space. These are also referred to as predictive densities.\nResults are shown in Figure 3b "}, {"section_title": "Alzheimer's disease study", "text": "Hippocampal volume is one of the best established and most studied biomarkers because of its known association with memory skills and relatively easy identification in sMRI. In two recent papers, Jack et al. (2010) and and University of California-San Francisco. ADNI is the result of efforts of many co-investigators from a broad range of academic institutions and private corporations, and subjects have been recruited from over 50 sites across the U.S. and Canada. The initial goal of ADNI was to recruit 800 adults, ages 55 to 90, to participate in the research, approximately 200 cognitively normal older individuals to be followed for 3 years, 400 people with MCI to be followed for 3 years and 200 people with early AD to be followed for 2 years. For up-to-date information, see www.adni-info.org. normal, we find some peculiarities in the model fit. Figures 6 and 7 display the estimated regression function and histogram of the errors within each combination of sex and disease status for the semi-parametric cubic spline and Gaussian process models, respectively, which are implemented in crs and kernlab packages in R. Notice that both of these models tend to overfit the data to overcome the rigid assumption on the errors. Furthermore, we find some abnormal behaviour in the errors that depends on sex and disease status. So, while the estimated mean function for the cubic splines looks reasonable and is similar to our own estimate (see Figure 8) , the wrong error distribution about the mean will render prediction almost meaningless.\nIn order to fully capture the dynamics of the data, a nonparametric approach which flexibly models both the regression function and the error distribution is needed. To this aim, we consider the model developed in this paper, specifically, the infinite Gaussian kernel mixture model with covariate dependent weights given by\n, where G 1 = 1 (x 1 represents gender) and G 2 = 2 (x 2 represents disease status). Note that here age (x 3 ) is a real number measuring time from birth to exam date and thus, is treated as a continuous covariate. The prior distribution for w j and (\u03b8 j , \u03c8 j ) is described in Section 4. The prior parameters for w j are \u03b6 1,j = 1 and \u03b6 2,j = 1, corresponding to a Dirichlet process prior with a precision parameter of 1. See Section 5 for an explanation of this.\nFor the prior of (\u03b8 j , \u03c8 j ), we set Inference is carried out via the algorithm discussed in Section 4 with 23,000 iterations after a burn in period of 7,000. i.e. biomarkers with small variability. In general, we observe that variance decreases with increasing age, indicating that hippocampal volume is more reliable for elderly patients. The difference is slightly more pronounced for females as opposed to males.\nIn particular, hippocampal volume is predicted to have a large variability for young females across all disease stages, with the largest for young CN females (the subgroup with no data). Instead, for older females, the variance is much smaller for all disease stages. When comparing males across disease status, we notice that young CN patients are predicted to show a large variability compared with young MCI and AD patients, "}, {"section_title": "Discussion", "text": "In this paper, we have described and implemented a fully Bayesian nonparametric approach to examine the evolution of hippocampal volume as a function of age, gender, and disease status. We find that with increasing age, hippocampal volume is predicted to display a sigmoidal decline for cognitively normal, MCI, and AD patients. We also observe the most gradual decline for CN patients, while AD patients are predicted to show the steepest decline. As the approach was nonparametric, no structure was assumed for the regression function, yet our results confirm the hypothetical dynamics of hippocampal volume proposed by Jack et al. (2010) . This provides strong statistical support for their model of hippocampal atrophy.\nFuture work in this application will involve examining the dynamics of various biomarkers jointly, which could be accomplished by replacing the normal linear regression component for y with a multivariate linear regression component. Another important future study will consist of combining the cross-sectional data with the longitudinal data for each patient.\nIn our analysis of the dynamics of hippocampal volume, we have developed a novel Bayesian nonparametric regression model based on normalized covariate dependent weights. The important contributions of this approach are a natural and interpretable structure for the weights, a novel algorithm for exact posterior inference, and the inclusion of both continuous and discrete covariates.\nWe have focused on a univariate and continuous response, but the model and algorithm can be easily extended to accommodate other types of responses by, for example, simply replacing the normal linear regression component for y with a generalized linear model. Future work will consist of examining theoretical properties of this model.\nEach of the (\u03b2 j , \u03c3 2 j ) can be sampled independently across j, from the full conditional density\nHere, X j denotes the matrix with rows given by X i = (1, x i ) for d i = j; y j is defined analogously; and I j denotes the identity matrix of size n j .\nWe now show how the introduction of an additional set of latent variables enables the update of the (\u03c8 j ) J j=1 , as explained in section 4, and specify the resulting posterior densities and truncation regions.\nObserve that, for any integer H and vector (c 1 , . . . , c H ) \u2208 (0, 1) H , the following identity holds\nwhere U = (U 1 , . . . , U H ), u = (u 1 , . . . , u H ) and U is the set of H-dimensional {0, 1}\nvectors of which at least one entry is 0. We can, therefore, introduce latent variables\n), for i = 1, . . . , n, l = 1, . . . , k i and h = 1, . . . , q + p, to deal with the terms (1 \u2212 h K(x i,h |\u03c8 j,h )) in the latent likelihood (expression 8). The full conditional density for (\u03c8 j ) J j=1 is thus extended to the latent expression f (\u03c8 1:J , {u i,l,h }, {U i,l,h }| \u00b7 \u00b7 \u00b7 )\nwhere K i,l,h = K(x i,h |\u03c8 D i,l ,h ), from which the original conditional density can be recovered by marginalizing over the (u i,l,h , U i,l,h ).\nThe latent variables (u i,l,h , U i,l,h ) can be sampled from their full conditional density by first observing that they are independent across i = l, . . . , n and l = 1, . . . , k i . For each i, l, the variable u i,l is a q + p-dimensional vector of zeros and ones with at least one zero entry. There are 2 p+q \u2212 1 such vectors, and for any u in this set, the update must be done according to the following distribution\nConditional on u i,l , the latent variables U i,l,h for h = 1, . . . , p + q are independent and uniformly distributed in the region\nTherefore, the additional variables do not pose a problem for posterior simulation.\nFurthermore, the introduction of these new variables transforms the latent term, introduced to deal with the intractable normalizing constant, into a product of truncation terms which is multiplied by the usual posterior density for the nonparametric mixture.\nWe first consider the update of the (\u03c1 j ) J j=1 , which is achieved by sampling each \u03c1 j,h\nindependently from a truncated Dirichlet distribution, f (\u03c1 j,h | \u00b7 \u00b7 \u00b7 ) \u221d Dir(\u03c1 j,h |\u03b3 j,h ) 1 (\u03c1 j,h \u2208 R j,h ) ,\nwhere, \u03b3 j,h,g = \u03b3 j,h,g + d i =j\n1 (x i,h = g) .\nThe truncation region for each of the (\u03c1 j ) J j=1 is given by R j,h = \u03c1 \u2208 (0, 1) G h : r We then consider the (\u00b5 j , \u03c4 j ) J j=1 . Recall that \u03c4 j = \u03c4 for every j, so we update this variable by sampling each \u03c4 h independently from a truncated gamma density,\nThe truncation region for each \u03c4 h is an interval T h = (\u03c4 "}, {"section_title": "= min", "text": "\u22122 log U i,l,h+q (x i,h+q \u2212 \u00b5 D i,l ,h ) 2 : u i,l,h+q = 1 .\nWe then sample each \u00b5 j,h independently from a truncated normal f (\u00b5 j,h | \u00b7 \u00b7 \u00b7 ) \u221d N(\u00b5 j,h |\u03bc j,h , (\u03c4 h\u0109j,h ) \u22121 ) 1 (\u00b5 j,h \u2208 A j,h ) , The truncation region for each of the \u00b5 j,h is an intersection of sets,\nwhere each A i,l,h is defined in terms of the intervals, I i,l,h = x i,h+q \u2212 \u22122 log U i,l,h+q \u03c4 h , x i,h+q + \u22122 log U i,l,h+q \u03c4 h , as A i,l,h = I i,l,h when u i,l,h+p = 1, and A i,l,h = I c i,j,h when u i,l,h+p = 0. Finally, in order to improve the mixing of the algorithm we applied the label switching moves introduced by Papaspiliopoulos and Roberts (2008) .\nThe Markov Chain scheme detailed here and explained in section 4, produces posterior samples (w s j , \u03b8 s j , \u03c8 s j ) for s = 1, . . . , S, which can be used to estimate the regression mean (9) and predictive density (10) ."}]