[{"section_title": "Abstract", "text": "Current models of progression in neurodegenerative diseases use neuroimaging measures that are averaged across pre-defined regions of interest (ROIs). Such models are unable to recover fine details of atrophy patterns; they tend to impose an assumption of strong spatial correlation within each ROI and no correlation among ROIs. Such assumptions may be violated by the influence of underlying brain network connectivity on pathology propagation -a strong hypothesis e.g. in Alzheimer's Disease. Here we present DIVE: Data-driven Inference of Vertexwise Evolution. DIVE is an image-based disease progression model with single-vertex resolution, designed to reconstruct long-term patterns of brain pathology from shortterm longitudinal data sets. DIVE clusters vertex-wise (i.e. point-wise) biomarker measurements on the cortical surface that have similar temporal dynamics across a patient population, and concurrently estimates an average trajectory of vertex measurements in each cluster. DIVE uniquely outputs a parcellation of the cortex into areas with common progression patterns, leading to a new signature for individual diseases. DIVE further estimates the disease stage and progression speed for every visit of every subject, potentially enhancing stratification for clinical trials or management. On simulated data, DIVE can recover ground truth clusters and their underlying trajectory, provided the average trajectories are sufficiently different between clusters. We demonstrate DIVE on data from two cohorts: the Alzheimer's Disease Neuroimaging Initiative (ADNI) and the Dementia Research Centre (DRC), UK. The DRC cohort contains patients with Posterior Cortical Atrophy (PCA) as well as typical Alzheimer's disease (tAD). DIVE finds similar spatial patterns of atrophy arXiv:1901.03553v1 [cs.CV] 11 Jan 2019 for tAD subjects in the two independent datasets (ADNI and DRC), and further reveals distinct patterns of pathology in different diseases (tAD vs PCA) and for distinct types of biomarker data -cortical thickness from Magnetic Resonance Imaging (MRI) vs amyloid load from Positron Emission Tomography (PET). We demonstrate that DIVE stages have potential clinical relevance, despite being based only on imaging data, by showing that the stages correlate with cognitive test scores. Finally, DIVE can be used to estimate a fine-grained spatial distribution of pathology in the brain using any kind of voxelwise or vertexwise measures including Jacobian compression maps, fractional anisotropy (FA) maps from diffusion tensor imaging (DTI) or other PET measures.\nDIVE availability: DIVE source code, written in Python3, is available at https://github.com/mrazvan22/dive and can be easily applied on any registered voxelwise images or images processed with the Freesurfer software. ADNI data can be downloaded from the Laboratory of NeuroImaging at the University of Southern California."}, {"section_title": "Introduction", "text": "Many biomarkers exist that can be used to track the severity of neurodegenerative diseases such as Alzheimer's disease (AD). Clinical function can be measured using cognitive assessments performed by an expert clinician and brain atrophy can be measured using Magnetic Resonance Imaging (MRI). Other measures include molecular markers such as aggregation of misfolded amyloid-beta or tau measured using Positron Emission Tomography (PET), and measures of white-matter degradation such as fractional anisotropy (FA) from Diffusion Tensor Imaging (DTI). The evolution of these biomarkers across the disease time-course creates a unique signature of the disease that can be used to stage patients, which is helpful for stratification in clinical trials.\nA hypothetical model of disease progression has been proposed by (Jack Jr et al., 2010) , describing the order of abnormality of key biomarkers along the progression of AD. The model suggests that amyloid-beta and tau biomarkers become abnormal long before symptoms appear, followed by brain atrophy measures and cognitive decline. Motivated by this hypothetical model, several data-driven disease progression models have been proposed in recent years, which aggregate information from multiple biomarkers into a single time frame representing disease progression. One such model is the Event-Based Model (EBM) (Fonteijn et al., 2012; Young et al., 2014) , which models the progression of disease as a sequence of discrete events, representing underlying biomarkers switching from a normal to abnormal state. Other types of disease progression models (Donohue et al., 2014; Jedynak et al., 2012; Li et al., 2017; Lorenzi et al., 2017; Schiratti et al., 2015) have been developed, that build continuous trajectories by \"stitching\" together short-term follow-up data from individual subjects. In contrast to the discrete disease stages that are estimated by the EBM, these models also compute a continuous measure of disease stage for every individual by estimating individual time shifts and progression speeds.\nCurrent image-based disease progression models estimate the evolution of the disease using a small set of biomarkers corresponding to pre-defined regions-of-interest (ROI). This ROI parcellation is usually coarse, doesn't allow one to find spatially dispersed patterns of atrophy. While spatiotemporal longitudinal models have already been demonstrated (Derado et al., 2010; Hyun et al., 2016; Lorenzi et al., 2015) , these models regress against pre-defined sets of covariates such as age or clinical markers. This is problematic because, in the case of age, this assumes all subjects have the same age of disease onset. Similarly, clinical markers are noisy, biased, suffer from floor/ceiling effects and are not sensitive in pre-symptomatic phases. Recently, some spatiotemporal models that estimate subject-specific time-shifts have been developed (Bilgel et al., 2016; Koval et al., 2017) . However, these models generally cannot recover dispersed and disconnected pathological patterns, because they assume correlation of voxels based on spatial distance, either through a distance function or distance from control points. However, spatially dispersed pathological patterns have been observed in AD and related dementias and are hypothesized to appear due to interactions of pathology and brain networks (Seeley et al., 2009 ). Therefore, a spatiotemporal disease progression model that allows recovery of dispersed atrophy patterns present in AD, is not currently available.\nIn this work, we present DIVE: Data-driven Inference of Vertexwise Evolution. DIVE is a novel disease progression model with single vertex resolution that makes only weak assumptions on spatial correlation. In contrast to approaches which model temporal trajectories for a small set of biomarker measures based on a priori defined ROIs, DIVE models temporal trajectories for each vertex on the cortical surface. DIVE combines unsupervised learning and disease progression modelling to identify clusters of vertices on the cortical surface that show a similar trajectory of brain pathology over a particular patient cohort. This formulation enables us to estimate a fine-grained spatial distribution of pathology and also provides a novel parcellation of the brain based on temporal change.\nWe first test DIVE on synthetic data and show that the model can recover known biomarker trajectories and disease progression scores. We then demonstrate the model on data from two cohorts: the Alzheimer's Disease Neuroimaging Initiative (ADNI) and the Dementia Research Centre (DRC), UK. We use the model to reveal spatiotemporal patterns of pathology to a much finer resolution than previous models and demonstrate the ability to assign subjects to stages that predict progression. Finally, we validate DIVE in terms of how robust are the estimated pathology patterns and how well the disease progression scores correlate with cognitive tests.\nIn this section we describe the mathematical formulation of DIVE (section 2.1), then we show how to fit the model using Expectation Maximisation (section 2.2) and we describe further implementation details of the algorithm (section 2.3). Afterwards, we outline the synthetic data-generation process (section 2.4) for testing the model in the presence of ground truth, as well as the pipeline for pre-processing the ADNI and DRC datasets (section 2.5). Figure 1 . Diagram of the proposed DIVE model. DIVE assumes that biomarkers of pathology (e.g. cortical thinning) can be measured at many vertices (i.e. locations) on the cortical surface (A), where each vertex has a distinct trajectory of change during disease progression (B). The model thus assigns to every cortical vertex one of a small set of temporal trajectories describing the change in some image-based measurement (e.g. cortical thickness, amyloid PET, DTI fractional anisotropy measures) from beginning to end of the disease progression. The estimation process simultaneously estimates the set of clusters, the trajectory defining each cluster, and the position of each subject along the trajectories, which are defined on a common time line. The process iterates assignment of each vertex to clusters (red, green and blue in this diagram) (C), estimation of the trajectory in each cluster (D) and estimation of the disease progression score (location along trajectory) for each subject (E), all within an expectation-maximisation framework, until convergence. Figure 1 illustrates the DIVE aims and implementation. DIVE input measures are vertexwise or voxelwise biomarker measures in the brain (Fig 1A) , such as cortical thickness or amyloid load. A vertex is a location on the cortical surface at which a biomarker of pathology is quantifiable (e.g. cortical thickness). For each vertex on the cortical surface (or voxel in the 3D brain volume), we estimate a unique trajectory along the disease progression timeline (Fig 1B) , while also estimating subject/visitspecific disease progression scores (i.e. disease stages). We do that by grouping vertices with similar biomarker trajectories into clusters (Fig 1C) , and we estimate a representative trajectory for every cluster (Fig 1D) . Each trajectory is a function of subject-/visit-specific disease progression scores (DPS) (Fig 1E) . The DPS depends linearly on the time since baseline visit, but with subject-specific slope and intercept."}, {"section_title": "The DIVE model", "text": ""}, {"section_title": "Modelling Subject-specific Time Shifts", "text": "The disease progression score s ij for subject i at visit j is defined as a linear transformation of time since baseline measurement t ij (in years):\nwhere \u03b1 i and \u03b2 i represent the speed of progression and time shift (i.e. disease onset) of subject i."}, {"section_title": "Modelling Biomarker Trajectory for a Single Vertex", "text": "DIVE assumes that the biomarker measure at each vertex on the cortical surface follows a sigmoidal trajectory f( . ; \u03b8) over the disease progression score s and with parameters \u03b8. We choose a parametric sigmoid function because it is a parsimonious parametric model that offers better fit compared to linear models, is monotonic, and can account for floor and ceiling effects (Caroli and Frisoni, 2010; Sabuncu et al., 2011) . We also assume that vertices are grouped into K clusters and we model a unique trajectory for each cluster k \u2208 {1, ... , K}, which will be referred to as cluster trajectories. The sigmoidal function f(s; \u03b8 k ) for cluster k is defined as:\nwhere s is the disease progression score from Eq. 1 and\nare parameters controlling the shape of the trajectory -d k and d k +a k represent the lower and upper limits of the sigmoidal function, c k represents the inflection point and a k b k /4 represents the slope at the inflection point. For a given subject i at visit j, the value V l ij of its biomarker measurement at vertex l is a random variable that has an associated discrete latent variable Z l \u2208 [1, ... , K] denoting the cluster it was generated from. The value of V l ij given that it was generated from cluster Z l can be modelled as:\nrepresents the probability density function (pdf) of the normal distribution that models the measurement noise along the sigmoidal trajectory of cluster Z l , having variance \u03c3 Zl . Next, we assume the measurements from different subjects are independent, while the measurements from the same subject i at different visits j are linked using the disease progression score from Eq. 1. Moreover, we also assume a uniform prior on cluster membership Z l . This gives the following model:\nwhere I = {(i, j)} represents the set of all the subjects i and their corresponding visits "}, {"section_title": "Modelling Biomarker Trajectories for all Vertices", "text": "So far we have a model for only one vertex on the brain surface. We therefore extend the formulation to all the vertices by assuming all these vertex measurements are spatially independent, giving the complete data likelihood: ... , Z l ] , L being the total number of vertices on the cortical surface. The formulation assumes spatial independence between measurements in different vertices, but in section 2.1.4 the model is extended to capture spatial correlations. We get the final model log likelihood for incomplete data by marginalising over the latent variables Z:\nThroughout the article, we will use the shorthand z lk = p(Z l = k)."}, {"section_title": "Modelling Spatial Correlation", "text": "The version of the model presented so far does not make any assumptions on spatial correlation. However, the regional organisation of the cortex suggests we would expect spatial correlation of the vertex measurements. More precisely, measures of cortical thickness or other modalities are often similar in neighbouring vertices on the cortical surface and likely belong to the same cluster. DIVE can be easily extended to include mild spatial constraints on the correlation of vertex measurements via a Markov Random Field (MRF), which encourages neighbouring vertices to have the same corresponding cluster. We hypothesise that incorporating such constraints should reduce the effects of noise and produce a more stable clustering. However, this does not model correlation between the actual vertex values, but only between the latent variables Z l , i.e. the cluster membership of each vertex. The MRF thus has the advantage of not requiring the use of huge covariance matrices, which are otherwise needed if we want to model correlation of vertex values directly. With the MRF, the full-data likelihood function of the model now becomes:\nwhere \u03a8 (Z l , Z l2 ) is a clique term representing the likelihood of a neighbouring vertex l 2 to have similar label with vertex l. The formula for the clique term is:\nwhere \u03bb is a parameter controlling how much to penalise neighbouring vertices that belong to distinct clusters, and g and h are positive, monotonic functions over the \u03bb>0 range. We choose g(\u03bb)=\u03bb and h(\u03bb)=\u03bb 2 , which results in a concave objective function for \u03bb, ensuring that it can later be optimised (see M-step). Therefore, the model parameters that need to be estimated are M = [\u03b1, \u03b2, \u03b8, \u03c3, \u03bb] where \u03b1 and \u03b2 are the subject specific shifting parameters, \u03b8 and \u03c3 are the cluster specific trajectory and noise parameters and \u03bb is the clique parameter denoting the penalisation of spatially non-smooth assignments of latent variables Z."}, {"section_title": "Fitting the Model using Generalised Expectation-Maximisation", "text": "We choose to fit our model using Expectation-Maximisation (EM), because it offers a fast convergence given the large number of parameters that need to be estimated and the huge dimensionality of relevant datasets (e.g. 1973 subjects x 163,842 vertices in ADNI). In the next two sections we outline the E-step and M-step. While both of these steps have no closed-form solution, we will solve them using numerical optimisation, which only results in an increase in the objective function at each iteration. However, the EM algorithm is still guaranteed to converge, and this approach is called Generalised EM (Bishop, 2006) . Algorithm 1 shows the model fitting procedure using the EM algorithm. The procedure first initialises (line 1) some parameters required to start the EM algorithm: the subject time shifts \u03b1 and \u03b2 and the latent parameters z lk which represent the assignment of vertices to clusters. In the M-step, the method updates the trajectories of each cluster (lines 4-6), the subjects-specific time shifts (line 9) and the clique penalty term \u03bb (line 17). In the E-step, the method computes z lk (line 18) using previously defined functions that compute z lk given a fixed \u03bb (line 14). Algorithm 1. The optimisation procedure for fitting DIVE using the ExpectationMaximisation algorithm."}, {"section_title": "E-step", "text": "In the Expectation step, at iteration u we seek an estimate of p (Z | V, M (u-1) ), given the current estimates of the parameters\n]. While, this does not directly factorise over the vertices l due to the MRF terms, we condition the clique terms on the values of Z from the previous iterations. This approximation gives the following factorizable likelihood:\nThe factorised form allows for tractable computation and memory storage of p(Z). M (u-1) ,Z (u-1) ). After simplifications we reach the following update rule:\nThe full derivation is given in the Supplementary material. In order to enable optimisation over \u03bb, a final modification of this step is performed, by considering z lk to be functions \u03b6 lk (\u03bb) over \u03bb. This results in the update equation from Alg. 1, line 18 which is based on pre-defined terms on lines 13-14.\nIn this step we need to estimate p(Z|V, M (u\u22121) ). For notational simplificy we will drop the (u \u2212 1) superscript\nwhere N l is the set of neighbours of vertex l. However, this doesn't directly factorise over the vertices l due to the MRF terms \u03a8(Z l , Z l 2 ). It is however necessary to find a form that factorises over the vertices, otherwise we won't be able to represent in memory the joint distribution over all Z variables. If we make the approximation\nthen we loose out all the MRF terms and the model won't account for spatial correlation. We instead do a first-degree approximation by conditioning on the values of Z (u\u22121) N l , the labels of nearby vertices from the previous iteration. The approximation is thus:\nThis form allows us to factorise over all the vertices to get p(Z l |V l , M ):\nwhere C is a normalistion constant that can be dropped. We can now further factorise p(Z l |Z\n) and apply a similar factorisation to the prior p(Z\nFactorising the summation over z N l 's we get:\nReplacing z l 2 with k 2 we get:\nWe shall also denote z lk = p(Z l |V l , M ). Further simplifications result in:\nWe further define the data-fit term D lk as follows:\nThis results in:\nFinally, we simplify the sum over k 2 to get the update equation for z lk :\nIn practice, we cannot naively compute the exponential term z lk = exp(log(z lk )) due to precision loss. However, we go around this by recomputing the exponentiation and normalisation of z lk simultaneously. Denoting x(k) = log z lk , for k \u2208 [1 . . . K], we get:"}, {"section_title": "M-step", "text": "In the Maximisation step we try to estimate the model parameters M = (\u03b1, \u03b2, \u03b8, \u03c3, \u03bb) that maximise E Z|V,M (u-1) [log p(V,Z|M) ]. We cannot simultaneously optimise all 5 sets of parameters, so we optimise them independently. In order to get the update rule for the trajectory parameters \u03b8 k corresponding to cluster k we need to maximise the expected log likelihood with respect to \u03b8 k . The key observation here is that if we assume fixed \u03b1, \u03b2 and Z, then the trajectory parameters \u03b8 k for every cluster k are conditionally independent, i.e.\nThis allows us to maximise every \u03b8 k independently using the following equation:\nA similar observation of conditional independence can also be observed for the latent variables Z. This allows us to decompose the joint distribution over Z, and after expanding the noise model we reach the optimisation problem from Alg. 1, line 4. See Supplementary material for full derivation. This does not have a closed-form solution, so we use numerical optimisation for finding \u03b8 k that maximises the equation from Alg. 1, line 4.\nA similar equation, yet in closed form, is also obtained for \u03c3 k (Alg. 1, line 6). After estimating \u03b8 and \u03c3 for every cluster, we use the new values to estimate the subject specific parameters \u03b1 and \u03b2. For every subject i, we maximise the expected log likelihood with respect to \u03b1 i , \u03b2 i independently, and after simplifications we obtain the update rule from Alg. 1, line 9, which is again solved using numerical optimisation. For the numerical optimisation of \u03b8 we used the Nelder-Mead method for its robustness, while for \u03b1 and \u03b2 we used the second-order Broyden-Fletcher-GoldfarbShanno algorithm due to fast convergence. Finally, we achieved a significant speedup in the evaluation of objective functions by computing a z lk -weighted average of vertex measurements within each cluster (see Supplementary section 4). This resulted in a convergence time of around 6 hours for the larger datasets (ADNI).\nFor optimising \u03bb, we again try to optimise in the M-step the expected full data likelihood under the Z estimates from the previous iteration:\nWe simplify the above equation by expanding the likelihood model and approximating the joint over Z with the product of the marginals z lk over all vertices l. This results in the update equation from Alg. 1 line 17 -see supplementary material for full derivation. In this final equation we also replaced z lk with a function \u03b6 lk (\u03bb) over \u03bb, which updates z lk based on the current value of \u03bb being evaluated. This is done to increase convergence, as latent variables z lk are highly coupled with the value of \u03bb being evaluated.\nThe M-step itself does not have a closed-form analytical solution. We choose to solve it by successive refinements of the cluster trajectory parameters and the subject time shifts."}, {"section_title": "Implementation Details", "text": ""}, {"section_title": "Parameter initialisation and priors", "text": "Before starting the fitting process, we need to initialise \u03b1, \u03b2 and the clustering probabilities z lk (Alg. 1, line 1). We set \u03b1 i and \u03b2 i to be 1 and 0 respectively for each subject, which sets the initial disease progression score to the age of the subject at the clinical visit. We initialise z lk using k-means clustering of the vectors V l . The DPS scores have two extra degrees of freedom (scale and shift), which we account for by setting informative gamma and Gaussian priors on parameters \u03b1 i and \u03b2 i respectively (\u03b1 i ~ \u0393(16e4, 16e4), \u03b2 i ~ N(0, 0.1)), which work well in practice as they result in realistic ranges for \u03b1 i and \u03b2 i of around [0.3, 3] and [-15,15] respectively. Such informative priors on \u03b1 i and \u03b2 i also help deal with singularities in the objective functions of \u03b1 i and \u03b2 i when the biomarker trajectories are flat. As already explained in (Jedynak et al., 2012) , the sigmoid parameters \u03b8 k are not identifiable, so we need to apply the following transformation on line 5 of Alg. 1: (u) . This ensures model identifiability and is performed at every iteration."}, {"section_title": "Estimating the Optimal Number of Clusters", "text": "The EM procedure needs to specify a-priori the number of clusters to fit on the data. We optimise the number of clusters K using Akaike Information Criterion (AIC), which we found to better agree with ground truth in simulations than other information criteria such as the Bayesian Information Criteria (BIC). The number of parameters of the fitted model is 5K+2S+1, where S is the number of subjects. Note that z lk are not included as parameters of the model because they are latent variables that are marginalised (see Eq. 6). We repeat the fitting procedure for each K from 2 to 100 clusters and select the K that minimises the AIC."}, {"section_title": "Simulation Experiments", "text": ""}, {"section_title": "Motivation", "text": "Initial assessment of DIVE performance uses synthetic data, where we know the ground truth. The aim is to explore how accurately we can recover ground truth parameters as the problem becomes harder in three different scenarios: Scenario 1: as the number of clusters increases, evaluate how well DIVE can estimate the correct number of clusters using AIC and BIC Scenario 2: as the trajectories become more similar, test how well we can recover the assignment of vertices to clusters and the DIVE parameters Scenario 3: same as Scenario 2, but for decreasing number of subjects\nWe further tested the robustness and validity of the model as follows: 1. Robustness in parameter estimation: test whether similar spatial clustering is estimated for different subsets of the data 2. Clinical validity of DPS scores: test whether the subject disease progression scores, based purely on MRI or PET data, correlate with cognitive tests such as Clinical Dementia Rating Scale -Sum of Boxes (CDRSOB), Alzheimer's Disease Assessment Scale -Cognitive (ADAS-COG), Mini-Mental State Examination (MMSE) and Rey Auditory and Verbal Learning Test (RAVLT). 3. Prediction accuracy: given initial baseline scans from different subjects, test how accurately DIVE can predict the biomarker evolution of those patients over time.\nWe were also interested to compare the performance of DIVE with other disease progression models. In particular, we were interested to test whether:\n\u2022 Modelling dynamic clusters on the brain surface improves subject staging and biomarker prediction\n\u2022 Modelling subject-specific stages with a linear transformation (the \u03b1 i and \u03b2 i terms) improves biomarker prediction"}, {"section_title": "Synthetic Data Generation", "text": "We first designed a basic simulation, which the model should be able to fit well since the trajectories were designed to be well separated and enough subject data was generated along the disease time course. The data in the basic simulation was generated as follows: 1. sample age a i1 and time shift parameters \u03b1 i , \u03b2 i for 300 subjects with 4 timepoints (each timepoint 1 year apart), with a i1 ~ U (40,80), \u03b1 i ~ \u0393(6.25, 6.25), \u03b2 i ~ N(0, 10) . Time since baseline has been obtained for every visit j of subject i as follows: t ij = a ij -a i1 2. generate three sigmoids with different (slope, centre) parameters: Fig. 2A , red lines). Upper and lower limits have been set to 1 and 0 respectively. 3. randomly assign every vertex l \u2208 {1, ..., L} where L = 1000, to a cluster a[l] \u2208 {1,2,3} 4. sample a set of L perturbed trajectories \u03b8 l from each of the original trajectories, one for each vertex ( Fig. 2A , gray lines) using covariance matrix C \u03b8 = diag ([0, 2b k /15, 11.6, 0] ). 5. sample subject data for every vertex l from its corresponding perturbed trajectory \u03b8 l with noise standard deviation \u03c3 l = 1.\nFrom the basic simulation, we generated synthetic data for each of the three scenarios by varying one parameter at a time and kept the other parameters constant, having the same values as in the basic simulation. We varied the following parameters: Scenario 1: number of clusters -2, 3, 5, 10, 15, 20, 30 and 40. The cluster centres were spread evenly across a fixed total DPS range where data was available. Scenario 2: distance between trajectory centres (as proportion of total DPS range sampled) -0.33, 0.30, 0.23, 0.17, 0.10, 0.07, 0.03 and 0.02 Scenario 3: number of subjects -300, 200, 100, 50, 35, 20, 10 and 5"}, {"section_title": "Model Fitting and Evaluation", "text": "Since there was no spatial information in the data generation procedure, we used DIVE without the MRF extension. For Scenario 1, we estimated using AIC and BIC the optimal number of clusters. For Scenarios 2 and 3, after fitting the parameters of DIVE, we calculated the agreement between the final clustering probabilities p(Z l ) and the true clustering assignments a [l] . This agreement, which we will call the clustering agreement, is defined as\n, where \u03c4 is any permutation of cluster labels. We also computed the error in the DPS estimation (sum of squared differences, SSD) and trajectory estimation (SSD between predicted trajectory and true trajectory at DPS points of every subject visit)."}, {"section_title": "Data Acquisition and Pre-processing", "text": "Data used in this work were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu) and from the Dementia Research Centre, UK. For ADNI, we downloaded all T1 MR images that have undergone gradient warping, intensity correction, and scaling for gradient drift. We included subjects that had at least 3 scans, to ensure we get a robust estimate of the subject specific parameters. This resulted in 138 healthy controls, 235 subjects with mild cognitive impairment (MCI) and 81 subjects with Alzheimer's disease.\nWe also downloaded all AV45 PET images from ADNI that were fully preprocessed, having the tag ' Co-reg, Avg, Std Img and Vox Siz, Uniform Resolution'. This meant that the images were co-registered, averaged across the 6 five-minute frames, standardised with respect to the orientation and voxel size and smoothed to produce a uniform resolution of 8mm full-width/half-max (FWHM).\nThe DRC dataset consisted of T1 MRI scans from 31 healthy controls, 32 PCA and 23 typical AD subjects with at least 3 scans each and an average of 5.26 scans per subject. All PCA patients fulfilled the (Tang-Wai et al., 2004) criteria and (Mendez et al., 2002 ) criteria based on clinical review. The typical AD patients all met the criteria for probable Alzheimer's disease (Dubois et al., 2010; Dubois et al., 2007) .\nGiven that the ADNI and DRC datasets contained subjects with different modalities or diseases, we ran DIVE independently on the following four cohorts (see Table 1 for demographics): 1) ADNI MRI: controls, MCI and tAD subjects from ADNI (cortical thickness data) 2) DRC tAD: tAD subjects and controls from the DRC dataset (cortical thickness data) 3) DRC PCA: PCA subjects and controls from the DRC dataset (cortical thickness data) 4) ADNI PET: AV45 scans from ADNI containing subjects with following diagnoses: healthy controls, subjective memory complaints, early MCI, late MCI and Alzheimer's disease. Table 1 . Demographics of the three cohorts used in our analysis. ADNI MRI and the DRC cohorts were used for the cortical thickness analysis, while ADNI PET was used for the PET AV45 analysis. MCI -mild cognitive impairment, SMCsubjective memory complaints, EMCI -early MCI, LMCI -late MCI."}, {"section_title": "Image Pre-processing", "text": ""}, {"section_title": "MR Images", "text": "On both datasets, in order to extract reliable cortical thickness measures, we ran the Freesurfer longitudinal pipeline (Reuter et al., 2012) , which first registers the MR scans to an unbiased within-subject template space using inverse-consistent registration. The longitudinally registered images were then registered to the average Freesurfer template. No further smoothing was performed on these images (FWHM level of zero mm). From these template-registered volumetric images, cortical thickness measurements were computed at each vertex (i.e. point) on an average 2D cortical surface manifold. For each vertex we averaged the thickness levels from both hemispheres. Finally, we standardised the data from each vertex with respect to the values of that vertex in the control population. Each of the final images had a resolution of 163,842 vertices on the cortical surface."}, {"section_title": "PET AV45 Images", "text": "We computed amyloid SUVR levels using the PetSurfer pipeline (Greve et al., 2016; Greve et al., 2014) , which is available with Freesurfer version 6. The PetSurfer pipeline first registers the PET image with the corresponding MRI scan, then applies Partial Volume Correction, and finally resamples the voxelwise SUVR values onto the cortical surface. While the final images also had a resolution of 163,842 vertices, the PET data we obtained from ADNI was inherently more smooth than the MRI cortical thickness data (8mm FWHM)."}, {"section_title": "The MRF Neighbourhood Graph", "text": "We estimated the MRF neighbourhood graph based on a Freesurfer triangular mesh for the fsaverage template. Each vertex was a triangle on the brain surface estimated with Freesurfer, and we connected the vertices if the corresponding triangles had a shared edge. For the MRF neighbourhood graph, we used a 3 rd degree neighbourhood structure, meaning that two vertices were considered neighbours if the shortest path between them was not higher than 3."}, {"section_title": "Results", "text": "We first present results on synthetic data (section 3.1), then on ADNI and DRC datasets (section 3.2), followed by model evaluation (section 3.3) using crossvalidation and correlation with cognitive markers.\nThe optimal number of clusters, as estimated with AIC, was three for the ADNI MRI dataset, three for the DRC tAD dataset, five for the DRC PCA dataset and eighteen for the ADNI PET dataset. Fig. 3A -left shows the results from the ADNI MRI dataset, where in the left image we coloured the vertices on the cortical surface according to the cluster they most likely belong to. We assigned a colour for each cluster (both the brain figures on the left and the trajectory figures on the right) according to the extent of pathology of its corresponding trajectory at a DPS score of 1. The cluster colours range from red (severe pathology) to blue (moderate pathology). In Fig. 3A -right, we show the resulting cluster trajectories with samples from the posterior distribution of each \u03b8 k . Similar results are shown for the other three datasets: the DRC tAD dataset (Fig. 3B) , DRC PCA dataset (Fig. 3C ) and the ADNI PET dataset (Fig. 3D) . We notice that in tAD subjects using the ADNI datasets (Fig. 3A) , there is more severe cortical thinning mainly in the inferior temporal lobe (red cluster), with disperse atrophy also in parietal and frontal regions (green cluster), with relative sparing of the inferior frontal and occipital lobes. In tAD subjects from the DRC dataset, we see a relatively similar pattern, however with more pronounced atrophy in the supramarginal cortex (red cluster) compared to ADNI. The spatial distribution of cortical thinning found with DIVE resembles results from previous longitudinal studies such as (Dickerson et al., 2008; Singh et al., 2006) . However, in contrast to these approaches, our model gives insight into the timing and rate of atrophy and is also able to stage subjects across the disease time course. We also find that the cluster trajectories in the DRC tAD dataset have similar dynamics to the ADNI MRI dataset, although they show a clearer separation between each other.\nIn the PCA subjects (Fig. 3C) , we find that atrophy is mainly focused on the posterior part of the brain, mostly the posterior parietal and supramarginal areas, with limited spread in the motor cortex, anterior temporal and frontal areas. There is a clear separation of atrophy extent between the anterior and posterior part of the brain, with the boundary running along the motor cortex. This posterior-focused pattern of atrophy is different from the one found in the tAD datasets, and agrees with previous findings in the literature (Crutch et al., 2012; Lehmann et al., 2011) .\nIn ADNI PET (Fig. 3D) we see that the regions with the highest amyloid uptake are more spatially continuous, comprising the precuneus and anterior frontal areas. On the other hand, the anterior-superior temporal gyrus shows the least uptake of amyloid. This result closely matches the result by (Bilgel et al., 2016) , which used a completely different dataset and modelling technique. The \"layers of clusters\" starting from the precuneus and frontal lobes, which range from severe to less severe atrophy, suggest a continuum of variation in vertex trajectories in the case of the PET dataset (Fig 3D-right) . These trajectories all start with a low amyloid SUVR, between 0 and 0.25, but in late stages the trajectories for some clusters such as cluster 0 can reach an SUVR of 1.5. The reason for seeing this continuum might be because the PET images have a much lower resolution than MR images and were smoothed by ADNI during the pre-processing steps.\n3 Validation of subject parameters against APOE A P O E 4 + (n = 1 7 2 ) A P O E 4 -(n = 2 3 2 ) "}, {"section_title": "Results on synthetic data", "text": "In the basic simulation, we obtained a clustering agreement \u202b\u05d0\u202c of 0.97, which suggests that almost all vertices were assigned to the correct cluster. Figure 2A shows the original trajectories and the recovered trajectories using our model, plotted against the disease progression score on the x-axis and the vertex value on the y-axis. In Fig.  2B we plotted the recovered DPS of each subject along with the true DPS. The results for the three scenarios are shown in Figs. 2C-E. In Fig 2C, we show for Scenario 1 the estimated number of clusters against the true number of clusters using both AIC and BIC criteria. In Figs 2D -E we show the distributions for \u202b\u05d0\u202c in Scenarios 2 and 3 as the problem becomes harder in each successive step. On the x-axis we show the variable that was changing within the scenario (e.g. number of clusters), while on the y-axis we show the agreement measure \u202b,\u05d0\u202c representing the percentage of vertices that were assigned to the correct cluster.\nThe results show that, in a simple experiment where the trajectories are well separated, DIVE can very accurately estimate which clusters generated each vertex. Moreover, the recovered trajectories and DPS scores are close to the true values. The results of Scenario 1 also suggest that both AIC and BIC are effective at estimating the correct number of known clusters, with AIC having slightly better performance than BIC for larger numbers of clusters. On the other hand, the results of the stress test scenarios 2 and 3 show that performance measure \u202b\u05d0\u202c drops when the trajectories become very similar with each other or when the number of subjects decreases. This happens because small differences in trajectories are hard to detect in the presence of measurement noise, while a small number of subjects doesn't provide enough data to accurately estimate the parameters. Similar decreases in performance for scenarios 2 and 3 are observed also for other measures, such as the error in recovered trajectories or DPS scores ( Supplementary Fig 1) ."}, {"section_title": "Results on ADNI and DRC Datasets", "text": ""}, {"section_title": "Initial Hypotheses", "text": "Using ADNI and DRC datasets, we aim to recover the spatial distribution of cortical atrophy and amyloid pathology, as well as the rate and timing of these pathological processes. In particular, we hypothesize that these spatial patterns of pathology and their evolution will be: 1. similar on two independent typical AD datasets: ADNI and DRC 2. different on distinct diseases: tAD vs PCA 3. different in distinct modalities: cortical thickness from MRI vs amyloid load from AV45 PET."}, {"section_title": "Model Evaluation", "text": ""}, {"section_title": "4.", "text": "Comparison with other models: we compared the performance of DIVE with a region-of-interest based method (Jedynak et al., 2012 ) and a no-staging method that doesn't estimate subject time shifts. See Supplementary Section 2 for precise specifications."}, {"section_title": "Evaluation Procedure", "text": "For all scenarios, we ran 10-fold cross-validation (CV) on the ADNI MRI dataset. At each fold we fit the model using 3 clusters, since this was the optimal number of clusters found previously on the entire dataset. The trained model was then used to estimate the DPS of the test subjects. Fig. 4 shows the brain clusters and corresponding trajectories, estimated for all the cross-validation folds after fitting the model on the training data. The clusters have been coloured using a similar colour scheme as in Fig. 3 . In Fig 5 we show scatter plots of the DPS scores with clinical measures such as CDRSOB, ADAS-COG, MMSE and RAVLT. Fig 5. Scatter plots of the DPS scores estimated from the ADNI MRI dataset, plotted against four cognitive tests: CDRSOB, ADAS-COG, MMSE and RAVLT. For each cognitive test we also report the correlation coefficient and p-value. The disease progression scores, computed only based on MRI cortical thickness data, correlate with these cognitive measures, suggesting that the DPS scores are clinically meaningful. Table 2 . Performance evaluation of DIVE and two other models on the ADNI MRI dataset using 10-fold cross-validation. In the middle four columns, we show the correlation of the DPS scores with respect to several cognitive tests: CDRSOB, ADAS-Cog13, MMSE and RAVLT. The last column shows the prediction error (RMSE) of cortical thickness values from follow-up scans. (*) Statistically significant differences between the model and DIVE, Bonferroni corrected for multiple comparisons."}, {"section_title": "Evaluation Results", "text": "The results in Fig. 4 demonstrate that DIVE is robust in cross-validation, as the estimated clusters and trajectory parameters are all similar across folds. The average Dice score overlap across the 10-folds range were 0.77, 0.76 and 0.90 for clusters 0, 1 and 2 respectively. The DIVE-derived DPS scores, which were estimated purely based on MRI data, are also clinically relevant as they correlate with cognitive tests (Fig 5) .\nThe performance of DIVE in terms of subject staging and biomarker prediction also compares favourably with simpler no-staging and ROI-based models (Table 2) . Results show that DIVE has comparable performance to the ROI-based model, both in terms of subject staging and cortical thickness prediction. The fact that DIVE has similar performance to a simpler model which has less parameters is evidence that the estimated patterns are meaningful. Moreover, DIVE offers qualitative insight into the fine-grained spatial patterns of pathology and their temporal progression. Furthermore, the No-staging model performs significantly worse than DIVE, both in terms of subject staging and for biomarker prediction. This suggests that, when modelling progression of AD, it is important to account for the fact that patients are at different stages along the disease time-course."}, {"section_title": "Discussion", "text": ""}, {"section_title": "Summary and Key Findings", "text": "We presented DIVE, a spatiotemporal model of disease progression that clusters vertex-or voxel-wise measures of pathology in the brain based on similar temporal dynamics. The model highlights, for the first time, groups of cortical vertices that exhibit a similar temporal trajectory over the population. The model also estimates the temporal shift and progression speed for every subject. We applied the model on cortical thickness vertex-wise data from three MRI datasets (ADNI, DRC tAD and DRC PCA), as well as an amyloid PET dataset (ADNI). Our model found qualitatively similar patterns of cortical thinning in tAD subjects using the two independent datasets (ADNI and DRC). Moreover, it also found different patterns of pathology dynamics on two distinct diseases (tAD and PCA) and on different types of data (PET and MRI-derived cortical thickness). Finally, DIVE also provides a new way to parcellate the brain that is specific to the temporal trajectory of a particular disease."}, {"section_title": "Limitations and future work", "text": "DIVE has some limitations that can be addressed. First, we assumed that cluster trajectories follow sigmoidal shapes, which is not the case for many types of biomarkers in ADNI which do not plateau in later stages. The assumption of sigmoidal trajectories can be avoided using non-parametric curves such as Gaussian Processes (Lorenzi et al., 2017) , which would be straightforward to incorporate into the DIVE framework. Another limitation of the model is that it assumes all subjects follow the same disease progression pattern, which might not be the case in heterogeneous datasets such as ADNI or DRC. This can be a concern, as there might be a pattern of pathology that occurs in a small set of subjects. However, DIVE can be extended to account for heterogeneity in the datasets by modelling different progression dynamics for distinct subgroups, using unsupervised learning methods like the SuStaIn model by (Young et al., 2018) . While SuStaIn, just like DIVE, estimates clusters and trajectories within the dataset, the clusters in SuStaIn are made of subjects with similar disease progression, while the clusters in DIVE are made of vertices with similar progression. Future work could combine clustering along both subjects and vertices simultaneously to estimate disease subtypes with distinct spatiotemporal dynamics at the vertexwise level.\nThere are several potential future applications of DIVE. One of the advantages of DIVE is that it can be used to study the link between disconnected patterns of brain pathology and connectomes extracted from diffusion tractography or functional MRI (fMRI). Such an analysis would enable further understanding of the exact underlying mechanisms by which the brain is affected by the disease. Our model, which can estimate fine-grained spatial patterns of pathology, is more suitable than standard ROI-based methods for studying the link between pathology and these structural or functional connectomes, because white matter or functional connections have a finegrained and spatially-varying distribution of endpoints on the cortex.\nApart from studying the link with brain connectomes, there are other potential applications for DIVE. The model can be applied to study other types of vertexwise or voxelwise data, such as FDG PET, tau PET, DTI or Jacobian compression maps from MRI. Moreover, the model can also be extended to cluster points on the brain surface according to a more complex disease signature, that can be made of two or more biomarkers. For example, using our cortical thickness and amyloid PET datasets from ADNI, we could have clustered points on the brain based on both modalities simultaneously. Such complex disease signatures can offer important insights into the relationships between different modalities and underlying disease mechanisms.\nDIVE is a spatiotemporal model that can be used for accurately predicting and staging patients across the progression timeline of neurodegenerative diseases. The spatial patterns of pathology can also be used to test mechanistic hypotheses which consider AD as a network vulnerability disorder. All these avenues can help towards disease understanding, patient prognosis, as well as clinical-trials for assessing efficacy of a putative treatment for slowing down cognitive decline. (C-D) The same error scores for Scenario 3. We notice that as the problem becomes more difficult, the errors in the DIVE estimated parameters increase. Errors were measured as sum of squared differences (SSD) between the true parameters and estimated parameters. For the trajectories, the SSD was calculated only based on the sigmoid centres, due to different scaling of the other sigmoidal parameters."}, {"section_title": "Acknowledgements", "text": "2 Comparison between DIVE and other models"}, {"section_title": "Experiment design", "text": "We compared the performance of our model to two simplified models:\n\u2022 ROI-based model: groups vertices according to an a-priori defined ROI atlas. This model is equivalent to the model by Jedynak et al., Neuroimage, 2012 and is a special case of our model, where the latent variables z lk are fixed instead of being marginalised as in equation 6.\n\u2022 No-staging model: This is a model that doesn't perform any time-shift of patients along the disease progression timeline. It fixes \u03b1 i = 1, \u03b2 i = 0 for every subject, which means that the disease progression score of every subject is age.\nWe performed this comparison using 10-fold cross-validation. For each subject in the test set, we computed their DPS score and correlated all the DPS values with the same four cognitive tests used previously. We also tested how well the models can predict the future vertex-wise measurements as follows: for every subject i in the test set, we used their first two scans to estimate \u03b1 i = 1, \u03b2 i = 0 and then used the rest of the scans to compute the prediction error. For one vertex location on the cortical surface, the prediction error was computed as the root mean squared error (RMSE) between its predicted measure and the actual measure. This was then averaged across all subjects and visits. Table 1 shows the results of the model comparison, on ADNI MRI dataset. Each row represents one model tested, while each column represents a different performance measure: correlations with four different cognitive tests and accuracy in the prediction of future vertexwise measurements. In each entry, we give the mean and standard deviation of the correlation coefficients or RMSE across the 10 cross-validation folds. Table 1 : Comparison of our model with two more simplistic models on the ADNI MRI dataset. For each of the three models, we show the correlation of the disease progression scores (DPS) with respect to several cognitive tests: CDRSOB, ADAS13, MMSE and RAVLT. The correlation numbers represent the mean correlation across the 10 cross-validation folds."}, {"section_title": "Derivation of the Generalised EM algorithm", "text": "We seek to calculate\nare the set of model parameters at iteration u of the EM algorithm, and Z = (Z 1 , . . . , Z L ) is the set of discrete latent variables, where Z l represents the cluster that voxel l was assigned to, so Z l \u2208 1, . . . , K. While Z l (with capital letter) is a random variable, we will also use the notation z l (small letter) to represent the value that the random variable Z l was instantiated to. Finally, p(M (u) ) is a prior on these parameters that is chosen by the user. Expanding the expected value, we get:\nThe E-step involves computing p Z = (z 1 , ..., z L )|V, M (u\u22121) , while the M-step comprises of solving the above equation."}, {"section_title": "Optimising trajectory parameters", "text": "Trajectory shape -\u03b8\nTaking equation 1 and fixing the subject time-shifts \u03b1, \u03b2 and measurement noise \u03c3, we can find its maximum with respect to \u03b8 only. More precisely, we want:\nWe observe that for each cluster the individual \u03b8 k 's are conditionally independent, i.e. \u03b8 k \u22a5 \u22a5 \u03b8 m |{Z, \u03b1, \u03b2, \u03c3} \u2200k, m. We also assume that the prior factorizes for each \u03b8 k : log p(\u03b8) = K k log p(\u03b8 k ). This allows us to optimise each \u03b8 k independently:\nReplacing the full data log-likelihood, we get:\nNote that we didn't include the MRF clique terms, since they are not a function of \u03b8 k . We propagate the logarithm inside the products:\nWe next assume that Z l , the hidden cluster assignment for vertex l, is conditionally independent of the other vertex assignments Z m , \u2200m = l (See E-step approximation from Eq. 3). This independence assumption induces the following factorization: \u22121) ). Propagating this product inside the sum over the vertices, we get:\nThe terms which don't contain \u03b8 k dissapear:\nWe further expand the gaussian noise model:\nConstants dissapear due to the arg max and we get the final update equation for \u03b8 k :\nMeasurement noise -\u03c3\nWe first assume a uniform prior on the \u03c3 parameters to simplify derivations. Using a similar approach as with \u03b8, after propagating the product inside the logarithm and removing the terms which don't contain \u03c3 k , we get:\nNote that, just as for \u03b8 above, the MRF clique terms were not included because they are not a function of \u03c3 k . Expanding the noise model we get:\nThe maximum of a function l(\u03c3 k ) can be computed by taking the derivative of the function l and setting it to zero. This is under the assumption that l is differentiable, which it is but we won't prove it here. This gives:\nPropagating the differential operator further inside the sums we get:\nWe next perform several small manipulations to reach a more suitable form of the derivative and then set it to be equal to zero:\nFinally, we solve for \u03c3 k and get its update equation:\n4.2.2 Estimating subject time shifts -\u03b1, \u03b2\nFor estimating \u03b1, \u03b2, we adopt a similar strategy as in the case of \u03b8, up to Eq. 18. This gives us the following problem:\nThe terms \u03b1 i , \u03b2 i for other subjects i = i dissappear:\nExpanding the gaussian noise model we get:\nAfter removing constant terms we end up with the final update equation for \u03b1 i , \u03b2 i :"}, {"section_title": "Estimating MRF clique term -\u03bb", "text": "We optimise \u03bb using the following formula:\nNote that p(Z|V, M (u\u22121) , \u03bb, Z (u\u22121) ) is a function of \u03bb, so for each lambda we estimate z lk through approximate inference. We do this because otherwise the optimisation of \u03bb will only take into account the clique terms and completely exclude the data terms. We further simplify the objective function for lambda as follows:\nWe take the logarithm:\n). Assuming independence between the latent variables Z l we get:\nHowever, we now want to make z lk a function of \u03bb as previously mentioned, so z lk = \u03b6 lk (\u03bb), for some function \u03b6 lk . More precisely, using the E-step update from Eq. 12 we define for each vertex l and cluster k a function \u03b6 lk (\u03bb) as follows:\nwhere D lk is as defined in Eq 10. We then replace z lk with \u03b6 lk (\u03bb) and introduce the chosen MRF clique model to get:\nWe separate the cliques that have matching clusters to the ones that don't:\nWe also factorise the clique terms:\nFinally, we simplify to get the objective function for \u03bb.\nFor implementation speed-up, data-fit terms D lk can be pre-computed."}, {"section_title": "Fast DIVE Implementation -Proof of Equivalence", "text": "Fitting DIVE can be computationally prohibitive, especially given that the number of vertices/voxels can be very high, e.g. more than 160,000 in our datasets. We derived a fast implementtion of DIVE, which is based on the idea that for each subject we compute a weighted mean of the vertices within a particular cluster, and then compare that mean with the corresponding trajectory value. This is in contrast with comparing the value at each vertex with the corresponding trajectory of its cluster. In the next few sections, we will present the mathematical formulation of the fast implementation for parameters [\u03b8, \u03b1, \u03b2] . Parameter \u03c3 already has a closed-form update, while parameter \u03bb has a more complex update procedure for which this fast implementation doesn't work. For each parameter, we will also provide proofs of equivalence."}, {"section_title": "Trajectory parameters -\u03b8", "text": ""}, {"section_title": "Fast implementation", "text": "The fast implementation for \u03b8 implies that, instead of optimising Eq. 29 we optimise the following problem:\nwhere < V ij >\u1e90 k is the mean value of the vertices belonging to cluster k. Mathematically, we defin\u00ea Z k = [z 1k \u03b3 k , z 2k \u03b3 k , . . . , z Lk \u03b3 k ] where \u03b3 k = ( L l=1 z lk ) \u22121 is the normalisation constant. Moreover, we have that\nWe take the derivative of the likelihood function l f ast of the fast implementation (Eq. 36) with respect to \u03b8 k and perform several simplifications:\nusing the fact that L l=1 \u03b3 k z lk = 1 we get:\nBy setting the derivative to zero, the optimal \u03b8 is thus a solution of the following equation:\nThe equivalent fast formulation for the subject-specific time shifts is similar to the one for the trajectory parameters. It should be noted however that we need to weight the sums corresponding to each cluster by \u03b3 \u22121 k . This gives the following equation for the fast formulation:\nIn order to prove that this is equivalent to the slow version, we need to take the derivative of the likelihood function (l f ast ) from the above equation with respect to \u03b1 i , \u03b2 i and set it to zero:\nWe expand the average across the vertices and slide the derivative operator inside the sums:\nSince L l=1 \u03b3 k z lk = 1 we get:\nRemoving the factor 2 and sliding \u03b3 k :\nFurther sliding L l=1 z lk to the left we get the final optimisation problem:"}, {"section_title": "Slow implementation", "text": "We will prove that if theta is a solution of the slow implementation, it is also a solution of Eq. 41, which will prove that the fast implementation is equivalent. The slow implementation is finding \u03b8 from the following equation:\nTaking the derivative of the function above (l slow ) with respect to \u03b8 k we get:\nAfter swapping terms around and using distributivity we get:\nThis is the same optimisation problem as in Eq. 41, which proves that the two formulations are equivalent with respect to \u03b8.\nIn a similar way to the trajectory parameters, we want to prove that solving the problem from Eq. 50 (fast implementation) is the same as solving the original slow implementation problem, which is defined as:\nTaking the derivative of the function above with respect to \u03b1 i , \u03b2 i , we get:\nThis is the same problem as the fast implementation one from Eq. 50, thus the fast model is equivalent to the slow model with respect to \u03b1, \u03b2."}, {"section_title": "Noise parameter -\u03c3", "text": "The noise parameter \u03c3 can actually be computed in a closed-form solution for the original slow model implementation, so there is no benefit in implementing the fast update for \u03c3. Moreover, the \u03c3 in the fast implementation computed the standard deviation in the mean value of the vertices within a certain cluster, and not the deviation withing the actual value of the vertices.\n5.3 Subjects-specific time shifts -\u03b1, \u03b2"}]