[{"section_title": "TABLE OF CONTENTS (continued)", "text": "List of Tables (continued)  9-3 ECLS-K, spring-first grade/spring-third grade panel: standard errors and design effects using C45CW0-C45CW90 and C45PW0-C45PW90, by selected child and parent variables: School years 1999-2000 and 2001-02 ... 9-15 9-4 ECLS-K, spring-kindergarten/spring-first grade/spring-third grade panel: standard errors and design effects using C245CW0-C245CW90 and C245PW0-C245PW90, by selected child and parent variables: School years 1998School years -99, 1999School years -2000School years , and 2001 9-5 ECLS-K, fall-kindergarten/spring-kindergarten/spring-first grade/ spring-third grade panel: standard errors and design effects using C1_5FC0-C1_5FC90 and C1_5FP0-C1_5FP90, by selected child and parent variables: School years 1998School years -99, 1999School years -2000School years , and 2001 9-6 ECLS-K, panel of all five rounds: standard errors and design effects for the full sample using C1_5SC0-C1_5SC40 and C1_5SP0-C1_5SP40, by selected child and parent variables: School years 1998School years -99, 1999School years -2000School years , and 2001 9-7 ECLS-K panel: median design effects for subgroups, kindergarten through third grade: School years 1998School years -99, 1999School years -2000School years , and 2001 xx  School years 1998School years -99, 1999School years -2000School years , and 2001 2-2 ECLS-K direct child assessments, by domain and round of data collection: School years 1998School years -99, 1999School years -2000School years , and 2001 2-3 ECLS-K parent interview, by major content topics and round of data collection: School years 1998School years -99, 1999School years -2000School years , and 2001 Teacher questionnaires, by major content topics and round of data collection: School years 1998School years -99, 1999School years -2000School years , and 2001 Special education teacher questionnaires, by major content topics and round of data collection: School years 1998School years -99, 1999School years -2000School years , and 2001 School administrator questionnaire, by major content topics and round of data collection: School years 1998School years -99, 1999School years -2000School years , and 2001 3-1 Reading proficiency levels, kindergarten through third grade: School years 1998School years -99, 1999School years -2000School years , and 2000 3-2 Mathematics proficiency levels, kindergarten through third grade: School years 1998School years -99, 1999School years -2000School years , and 2000      xxv"}, {"section_title": "GETTING STARTED", "text": "This document highlights key information you will need to work with the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K) data and points you to the appropriate sections of the User's Manual so that you can get started quickly. To read more about any particular topic, go to the indicated section of the User's Manual. In this document, major differences between the third grade data collection and previous rounds are summarized; cautions and caveats about using the data are provided; and basic information about using the Electronic Code Book (ECB) is summarized. You are working with the public-use file of the third grade data collection. In preparing the public-use file, the National Center for Education Statistics (NCES) takes steps to minimize the likelihood that an individual school, teacher, parent, or child participating in the study can be identified. Every effort is made to protect the identity of individual respondents. Some modifications to the data contained in the restricted-use file have been made to ensure confidentiality. The modifications that are implemented do not affect the overall data quality and most researchers should be able to find all that they need in the public-use files. Chapter 1, section 1.4.1, provides a general description of the differences between the public-use and restricted-use files. Table 7-13 in chapter 7 contains a list of the variables that have been modified. Section 7.8 contains additional information about the \"masking\" process."}, {"section_title": "Major Differences in the Third Grade Data Collection", "text": "Sample is not representative of third grade students, classrooms, or schools. The ECLS-K base year sample is a representative sample of children attending kindergarten during the 1998-99 school year, of schools with kindergartens, and of kindergarten teachers. Because the first grade sample was freshened with students who had not attended kindergarten in the United States in the previous year, the first grade sample is representative of children attending first grades in the United States during the 1999-2000 school year. However, it is not representative of schools with first grades or of first grade teachers. The third grade sample is not representative of third grade students, third grade teachers, or schools with third grades. Children who started their schooling in the U.S. in second or third grade are not represented in the sample. The data should not be used to make statements about third grade students, schools with third grades, or third grade teachers. See chapter 4, section 4.6 for more details on this point. Children rate their perceptions of social skills and interest in school subjects. In previous rounds of the ECLS-K, parents and teachers reported about children's social skills. For the first time in the ECLS-K, the children provided information about themselves by completing a short self-description questionnaire (SDQ). See sections 2.1 and 3.4 for additional information on the SDQ. Social Rating Scale (SRS) is not collected from parents. In the base year and in spring-first grade, parents and teachers completed the Social Rating Scale, which measures children's approaches to learning, self-control, interpersonal skills, and peer relations. In spring-third grade, only teachers completed this scale. Sections 2.3.2 and 3.3 provide information about the SRS. Science is a separate assessment domain. In previous years, the direct cognitive assessment included a general knowledge assessment that measured children's knowledge of the social and physical worlds. In third grade, children's knowledge of the world is more categorized into science and social studies domains. With limited time available for the direct assessment, the third grade assessment included only the science domain. Sections 2.1 and 3.1 provide information on the direct cognitive assessments."}, {"section_title": "Cautions and Caveats", "text": "Users of previous rounds of the ECLS-K data have repeatedly asked certain questions. NCES has developed a set of responses to users' most common questions. Please see the NCES web site for commonly asked questions and responses: http://nces.ed.gov/ecls. In addition to the frequently asked questions and responses, there are other aspects of working with the data that are important to know, including the following: Not all sample children are in third grade. The third grade data file includes children who were in third grade in spring 2002, and others who were either back or ahead a year or more. Users need to be aware of this fact when using the data and interpreting the findings. Most children in the sample have been in school for at least four years (K-3) and some more (those who were repeating K in the base year). A very small number may have been in school less than four years (some part of the freshened sample added in first grade)."}, {"section_title": "Student mobility and its consequences.", "text": "A random subsample of students who transferred from their base year schools was flagged to be followed in fall-first grade and in subsequent rounds of data collection. Sections 4.3.1 and 4.4 describe the subsampling of movers. There are a number of variables on the file that can be used to determine if a child moved to a different school between rounds or moved to a different school during the third grade data collection. Section 7.6 describes these variables. Student mobility has a number of consequences for the ECLS-K. It results in a reduction in sample size, fewer children per school, and more missing school and xxvii teacher questionnaire data for movers. See section 5.6.1, tables 5-12 and 5-13 for more information on the response rates for movers and nonmovers. Pay attention to missing data. Users should always be sure to recode any missing data properly before conducting analyses. If analyzing data over time, it is especially important to check that all skip patterns are the same across years because some changed between rounds of data collection. There are 5 different possible missing data codes on the file. See section 7.2 for a discussion of the different missing values codes and the circumstances when they are used. There may be no perfect weight. The third grade data file contains 3 sets of crosssectional weights and 8 longitudinal (panel) weights. Although there are a variety of weights on the file, there are scenarios for which there may not be a perfect weight. For a discussion of the weights and guidance in selecting an appropriate one, refer to sections 4.6.1 and 9.4.1. Defining special populations. The ECLS-K includes a number of analytic groups of interest that can be identified and studied separately. For example, the third grade file contains variables that identify children who have a disability diagnosed by a professional (P5DISABL), children receiving nonparental child care (P5CARNOW), and those who live in households with incomes below the poverty threshold (W3POVRTY). With variables from earlier rounds of data collection, it is possible to identify children who participated in Head Start in the year prior to kindergarten (HSATTEND from the base year and P4HSBEFK asked of new respondents in spring-first grade) and language minority children (WKLANST), as well as other subgroups. These variables are not contained on the third grade cross-sectional data file, but will be available on the K-3 longitudinal data file to be released in late 2003. Users who desire to study a specific subpopulation should search the ECB using the \"NARROW\" feature of the ECB to list variables that might help them identify their population of interest. See section 8.3.1 for a description of this feature. Examining school and classroom effects. When studying the effects of school and classrooms, it is important to restrict the analytic sample to children in the same classroom and/or same schools. Each type of respondent (child, parent, regular teacher, special education teacher, and school) has a unique ID number. These ID numbers can be used to identify children in the same classrooms and schools. Section 7.1 describes the available identification variables. Date of assessments and elapsed times between assessments are not the same for all children. The ECB contains variables that indicate the month, day, and year in which the direct assessment was administered. The ECB also contains composite variables for children's age at assessment for each sampled child. See the NCES web site http://nces.ed.gov/ecls for information on how to calculate elapsed time period between two assessments. Measuring achievement gains. One of the major strengths of the ECLS-K is the ability to measure children's achievement gains as they progress from kindergarten through the early elementary grades. There are several different approaches to"}, {"section_title": "INTRODUCTION", "text": "This manual provides guidance and documentation for users of the third grade data 1 of the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K). It begins with an overview of the ECLS-K study. Subsequent chapters provide details on the instruments and measures used, the sample design, weighting procedures, response rates, data collection and processing procedures, and the structure of the data file. The ECLS-K focuses on children's early school experiences beginning with kindergarten. It is a multisource, multimethod study that includes interviews with parents, the collection of data from principals and teachers, and student records abstracts, as well as direct child assessments. The ECLS-K has been developed under the sponsorship of the U.S. Department of Education, National Center for Education Statistics (NCES). Westat is conducting this study with assistance provided by Educational Testing Service (ETS) in Princeton, New Jersey. The ECLS-K is following a nationally representative cohort of children from kindergarten through fifth grade. The base year data were collected in the fall and spring of the 1998-99 school year when the sampled children were in kindergarten. A total of 21,260 kindergartners throughout the nation participated. Two more waves of data were collected in the fall and spring of the 1999-2000 school year when most, but not all, of the base year children were in first grade. 2 The fall-first grade data collection was limited to a 30 percent subsample of schools 3 (see exhibit . It was a design enhancement to enable researchers to measure the extent of summer learning loss and the factors that contribute to such loss and to better disentangle school and home effects on children's learning. The spring-first grade data collection, on the full sample, was part of the original study design and can be used to measure annual school progress and to describe the first grade learning environment of children in the study. All children assessed during the base year were eligible to be assessed in the spring-first grade data collection regardless of whether they repeated kindergarten, were promoted to first grade, or were promoted to second grade. In addition, children who were not in kindergarten in the United States during the 1998-99 1 The term \"third grade\" is used throughout this document to refer to the data collections that took place in the 2001-02 school year, at which time most of the sampled children-but not all of them-were in third grade. 2 Though the majority of base year children were in first grade during the 1999-2000 school year, about 5 percent of the sampled children were retained in kindergarten and a handful of others were in second grade during the 1999-2000 school year. 3 Approximately 27 percent of the base year students who were eligible to participate in year 2 attended the 30 percent subsample of schools. 1-2 school year and, therefore, did not have a chance to be selected to participate in the base year of the ECLS-K were added to the spring-first grade sample. 4 Such children include immigrants to the United States who arrived after fall 1998 sampling, children living abroad during the 1998-99 school year, children who were in first grade in 1998-99 and repeated it in 1999-2000, and children who did not attend kindergarten. Their addition allows researchers to make estimates for all first graders in the United States rather than just for those who attended kindergarten in the United States in the previous year. A fifth wave of data was collected in the spring of the 2001-02 school year when most, but not all, of the sampled children were in third grade. 5 In addition to the school, teacher, parent, and child assessment data collection components, children were asked to complete a short self-description questionnaire, which asked them how they thought and felt about themselves both socially and academically. The spring-third grade data collection can be used to measure school progress and to describe the third grade learning environment of children in the study. Exhibit 1-1. ECLS-K waves of data collection: Years 1998Years -2004 Data  1 Fall data collection consisted of a 30 percent sample of schools containing approximately 27 percent of the base year students eligible to participate in year 2. NOTE: See section 1.3 for a description of the study components. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 The sample of children in the third grade round of data collection of the ECLS-K represents the cohort of children who were in kindergarten in 1998-99 or in first grade in 1999-2000. Since the sample of children fielded in 2001-02 was not freshened with third graders who did not have a chance to be sampled in kindergarten or first grade (for example, because they were out of the country during their kindergarten and first grade year), this sample of children does not represent all third graders in 2001-02."}, {"section_title": "1-3", "text": "The vast majority of children in third grade in the 2001-02 school year are members of the cohort. However, third graders who repeated second or third grade and recent immigrants are not covered. Data were collected from teachers and schools to provide important contextual information about the environment for the sampled children. The teachers and schools are not representative of third grade teachers and schools in 2001-02. For this reason, the only weights produced from the study are for making statements about children, including statements about the teachers and schools of those children. The final wave of data collection that is currently planned is scheduled for spring 2004 when most of the study children will be in the fifth grade. The ECLS-K has several major objectives and numerous potential applications. The ECLS-K combines (1) a study of achievement in the elementary years; (2) an assessment of the developmental status of children in the United States at the start of their formal schooling and at key points during the elementary school years; 3cross-sectional studies of the nature and quality of kindergarten programs in the United States; and (4) a study of the relationship of family, preschool, and school experiences to children's developmental status at school entry and their progress during the kindergarten and early elementary school years. The ECLS-K is part of a longitudinal studies program comprising two cohorts-a kindergarten cohort and a birth cohort. The birth cohort (ECLS-B) is following a national sample of children born in the year 2001 from birth through first grade. The ECLS-B focuses on the characteristics of children and their families that influence children's first experiences with the demands of formal school, as well as children's early health care and in-and out-of-home experiences. Together these cohorts will provide the depth and breadth of data required to more fully describe and understand children's health and early learning, development, and education experiences. The ECLS-K has both descriptive and analytic purposes. It provides descriptive data on children's status at school entry, their transition into school, and their progress through fifth grade. The ECLS-K also provides a rich data set that enables researchers to analyze how a wide range of family, school, community, and individual variables affect children's early success in school; to explore school readiness and the relationship between the kindergarten experience and later elementary school performance; and to record children's cognitive and academic growth as they move through elementary school. 1-4"}, {"section_title": "Background", "text": "National policymakers and the public at large have increasingly recognized that the prosperity of the United States depends on the successful functioning of the American education system. There is also growing awareness that school reform efforts cannot focus solely on the secondary and postsecondary years but must pay attention to the elementary and preschool years as well. Increased policy interest in the early grades and the early childhood period is reflected in President Bush's No Child Left Behind Act (http://www.ed.gov.nclb) and in his Good Start, Grow Smart initiative (http://www.whitehouse.gov/infocus/earlychildhood). Efforts to expand and improve early education will benefit from insights gained through analyses of data from the large-scale, nationally representative ECLS-K data and the study's longitudinal design. The ECLS-K database contains information about the types of school programs in which children participate, the services they receive, and repeated measures of the children's cognitive skills and knowledge. The ECLS-K database also contains measures of children's physical health and growth, social development, and emotional well-being, along with information on family background and the educational quality of their home environments. As a study of early achievement, the ECLS-K allows researchers to examine how children's progress is affected by such factors as placement in high or low ability groups, receipt of special services or remedial instruction, grade retention, and frequent changes in schools attended because of family moves. Data on these early school experiences are collected as they occur, with the exception of their experiences before kindergarten, which are collected retrospectively. Collecting this information as it occurs produces a more accurate measurement of these antecedent factors and enables stronger causal inferences to be made about their relationship to later academic progress. The ECLS-K enables educational researchers and policy analysts to use a variety of perspectives on early childhood education, using techniques such as multilevel modeling to study how school and classroom factors affect the progress of individual children. The data collected will enable analysts to examine how children's status at school entry and performance in school are jointly determined by an interaction of child characteristics and school and family environments. Data collected during the kindergarten year serve as baseline measures to examine how schooling shapes later individual development and achievement. The longitudinal nature of the study 1-5 enables researchers to study children's cognitive, social, and emotional growth and to relate trajectories of change to variations in children's experiences in kindergarten and the early grades. The spring-third grade data collection can be used to describe the diversity of the study children and the classrooms and schools they attend. It can also be used to study children's academic gains in the years following kindergarten. The ECLS-K sample includes substantial numbers of children from various minority groups. Thus, the ECLS-K data present many possibilities for studying cultural and ethnic differences in the educational preferences and literacy practices of families, the developmental patterns and learning styles of children, and the educational resources and opportunities that different groups are afforded in the United States."}, {"section_title": "Conceptual Model", "text": "The design of the ECLS-K has been guided by a framework of children's development and schooling that emphasizes the interrelationships between the child and family, the child and school, the family and school, and the family, school, and community. The ECLS-K recognizes the importance of factors that represent the child's health status and socioemotional and intellectual development and incorporates factors from the child's family, community, and school-classroom environments. The conceptual model is presented in exhibit 1-2. The study has paid particular attention to the role that parents and families play in helping children adjust to formal school and in supporting their education through the primary grades. It has also gathered information on how schools prepare for and respond to the diverse backgrounds and experiences of the children and families they serve."}, {"section_title": "Study Components", "text": "The emphasis placed on measuring children's environments and development broadly has critical implications for the design of the ECLS-K. The design of the study includes the collection of data from the child, the child's parents/guardians, teachers, and schools. Children participate in various activities to measure the extent to which they exhibit those abilities and skills deemed important to success in school. They are asked to participate in activities designed to measure important cognitive (i.e., general knowledge, literacy, and quantitative) and noncognitive (i.e., fine motor and gross motor coordination and socioemotional) skills and knowledge. Most measures of a child's cognitive skills are obtained through an untimed one-on-one assessment of the child. Beginning with the third grade data collection, children report on their own perceptions of their abilities and achievement as well as their interest and enjoyment of reading, math, and other school subjects. Children are assessed in each round of data collection. Parents/guardians are an important source of information about the families of the children selected for the study and about themselves. Parents provide information about children's development at school entry and their experiences both with family members and others. Information is collected from parents each time children are assessed using computer-assisted interviews (CAIs). Information is collected from parents/guardians in each round of data collection."}, {"section_title": "1-7", "text": "Teachers, like parents, represent a valuable source of information on themselves, the children in their classrooms, and the children's learning environment (i.e., the classroom). Teachers are not only asked to provide information about their own backgrounds, teaching practices, and experience, they are also called on to provide information on the classroom setting for the sampled children they teach and to evaluate each sampled child on a number of critical cognitive and noncognitive dimensions. With the exception of the fall-first grade data collection, teachers complete self-administered questionnaires each time children are assessed. School administrators, or their designees, are asked to provide information on the physical, organizational, and fiscal characteristics of their schools, and on the schools' learning environment and programs. Special attention is paid to the instructional philosophy of the school and its expectations for students. Information is collected from school administrators via self-administered questionnaires during each spring data collection. School office staff are asked to complete a student records abstract form and a school fact sheet. The student records abstract form includes questions about an individual child's enrollment and attendance at the school, transfer to another school (if applicable), and verifies whether the child has an individualized education plan (IEP) on record. A student records abstract form is completed for each child in the study during each spring data collection. During the third grade data collection, school office staff were also asked to complete a school fact sheet. This form supplements the school administrator questionnaire with basic information about the school, including grade level, school type (public or private), length of school year, and attendance recordkeeping practices. This school fact sheet is only filled out once for each school in the study. Prior to the third grade data collection, the questions were part of the school administrator questionnaire."}, {"section_title": "ECLS-K Data Files", "text": "The ECLS-K data are released in restricted-use and public-use versions. A brief overview of the differences between the restricted-use and public-use files is provided here, followed by a description of the data files that are currently available."}, {"section_title": "Differences Between ECLS-K Restricted-Use and Public-Use Files", "text": "In preparing the public-use files, NCES takes steps to minimize the likelihood that an individual school, teacher, parent, or child participating in the study can be identified. Every effort is made to protect the identity of individual respondents. This is in compliance with the Privacy Act of 1-8 1974, as amended, the E-Government Act of 2002, the Education Sciences Reform Act of 2002, and the USA Patriot Act of 2001, which mandate the protection of confidentiality of NCES data that contain individually identifiable information. The process begins with a formal disclosure risk analysis. Variables identified as posing the greatest disclosure risk are altered, and in some instances, entirely suppressed. After modifying individual records that have the greatest risk of disclosure, the disclosure risk analysis is repeated to verify that the risk of disclosure has been reduced to acceptable levels. The following data modifications account for the differences between the public-use and restricted-use data files: Outlier values are top-or bottom-coded; 6 Individual cases for which a particular variable poses an especially high risk of disclosure have the value of that variable altered (usually by no more than 5 to 10 percent) to reduce the risk; Some continuous variables are modified into categorical variables, and categories of certain categorical variables are collapsed; 6 To understand top-and bottom-coding, consider a fictitious variable with the following frequency distribution: A small number of variables with too few cases and a sparse distribution are suppressed altogether, rather than modified; and A small number of variables are further masked to enhance confidentiality. The modifications that are implemented to avoid identification of schools, teachers, parents, and children do not affect the overall data quality and most researchers should be able to find all that they need in the public-use files. While very few of the variables are suppressed, there are a few users who might require the restricted files. Those researchers examining certain rare subpopulations such as the disabled, or children with specific non-English home languages or countries of birth, for example, will find that the restricted-use files contain a few more variables. However, in many instances even though the detailed information on the restricted-use files may be of interest, the sample sizes will be too small to support these analyses. NCES recommends that researchers who are uncertain of which data release to use first examine the public-use files to ascertain whether their specific analytic objectives can be met using those data files."}, {"section_title": "Overview of Available Data Files", "text": "A variety of ECLS-K data files are available for use by analysts. These are described below beginning with the third grade data files. ECLS-K third grade restricted-and public-use data files. The third grade data are available only as a child-level file. The file includes all data collected from or about the children and their schools including data from the child assessments and from their parents, teachers, and schools. No third grade teacher or school files are released because the sample of teachers and schools is not nationally representative of third grade teachers and schools with third grades. Analysts who wish to examine children's experiences in third grade and the influence of their classroom or school characteristics on their third grade experiences should use the third grade file. The third grade data file not only can be used to analyze data collected in the third grade but it also provides weights and variables that can be used in longitudinal data analysis of kindergarten, first grade, and third grade. In addition to the cross-sectional weights, cross-year (kindergarten-third grade) weights have been added to the third grade data file for those analysts who wish to examine children's learning across school years. Instructions on how to create a longitudinal file using the base year, first grade, and third grade data are provided in chapter 9. For more information on the third grade restricted-use data file, please see the User's Manual for the ECLS-K Third Grade Restricted-Use Data File and Electronic Code Book (NCES 2003-003). A longitudinal public-use file, however, is available that combines the base year, first 1-10 grade, and third grade data (see next bullet). Most analysts will find it more convenient to use the already created longitudinal file described below. Longitudinal kindergarten-third grade (K-third grade) public-use data file. This public-use data file combines data from the base, first grade, and third grade years. It contains cross-year weights so that analysts can examine children's growth and development between kindergarten and third grade. In order to streamline the file, the household roster that lists all household members, their relationship to the sampled child, and selected other characteristics, is not included on the file. Instead, composite variables describing the children's family structure and selected characteristics of the family members have been added to the file. Analysts who wish to study children's learning across school years, but who do not require the detailed household roster information, should use the longitudinal file. For information about this file, see the User's Manual for the ECLS-K Longitudinal Kindergarten-Third Grade Public-Use Data Files and Electronic Code Book (NCES, forthcoming). ECLS-K first grade restricted-and public-use data files. The first grade data (fall and spring) are available only as a child-level file. The file includes all data collected from or about the children and their schools including data from the child assessments and from their parents, teacher, and schools. No first grade teacher or school files are released because the sample of teachers and schools is not nationally representative of first grade teachers and schools with first grades. Analysts who wish to examine children's experiences in first grade and the influence of their classroom or school characteristics on their first grade experiences should use the first grade file. The first grade data file not only can be used to analyze data collected in the first grade but also provides weights and variables that can be used in longitudinal data analysis of both kindergarten and first grade. In addition to the cross-sectional weights, cross-year (kindergarten-first grade) weights have been added to the first grade data file for those analysts who wish to examine children's learning across school years. A longitudinal public-use file, however, is available that combines the base year and first grade data (see next bullet). Most analysts will find it more convenient to use the already created longitudinal file described below. For more information about the first grade file, see the User's Manual for the ECLS-K First Grade Public-Use Data Files and Electronic Code Book (NCES 2002-135) or the User's Manual for the ECLS-K First Grade Restricted-Use Data Files and Electronic Code Book (NCES 2002-128). Longitudinal kindergarten-first grade (K-first grade) public-use data file. This public-use data file combines data from the base and first grade years. It contains cross-year weights so that analysts can examine children's growth and development between kindergarten and first grade. In order to streamline the file, the household roster that lists all household members, their relationship to the sampled child, and selected other characteristics is not included on the file. Instead, composite variables describing the children's family structure and selected characteristics of the family members have been added to the file. Analysts who wish to study children's learning across school years or to study the extent of summer learning loss between kindergarten and the fall of the following school year, but who do not require the detailed household roster information, should use the longitudinal file. For"}, {"section_title": "1-11", "text": "information about this file, see the User's Manual for the ECLS-K Longitudinal Kindergarten-First Grade Public-Use Data Files and Electronic Code Book (NCES 2002-149). ECLS-K base year data files. There are three main and four supplementary files available for the base year. The three main files are the child-level file, the teacherlevel file, and the school-level file. The supplementary files are the teacher salary and benefits file, the special education file, the student record abstract file, and the Head Start Verification Study file. The child file contains all the data collected from or about the children, including data from the child assessments, and from their teachers, parents, and schools. Analysts who wish to obtain descriptive information about U.S. kindergarten students or their families, or who want to examine relationships involving children and families, children and teachers, or children and schools, should make use of the child file. Analysts wishing to obtain descriptive information about the population of kindergarten teachers in the United States, or to study relationships involving teachers as the principal focus of attention, should use the teacher file. Analysts who want to obtain descriptive information about public and private schools that contain kindergarten classes, or who want to examine relationships among school characteristics, should make use of the school file. These child-, teacher-, and schoollevel files are available in public-use and restricted-use versions. For more information on these files, refer to the ECLS-K Base Year Public-Use Data Files and Electronic Code Book: User's Manual (NCES 2001-029) or the ECLS-K Restricted-Use Base Year: Child File, Teacher File, and School File (NCES 2000-097). The salary and benefits file is collected at the school level and contains information on the base salary, merit pay, and benefit pay of teachers and principals. The salary and benefits data, when combined with other ECLS-K data, can be used to examine, for example, the relationship between student outcomes and school resource allocation and use. This file is only available as a restricted-use file. For more information about this file, see the ECLS-K Base Year Restricted-Use Salary and Benefits File (NCES 2001-014). The special education file is a child-based file that contains information on 784 children identified as receiving special education or related services in kindergarten. Special education teachers were asked to complete two questionnaires designed to collect information about their professional background and experience and about the nature of the special education program and special education services provided to each of the sampled children receiving services. It is only available as a restricted-use file. For more information about this file, see the ECLS-K Base Year Restricted-Use Special Education Child File (NCES 2001-015). The student record abstract file contains information from school records about children's school enrollment and attendance; Individualized Education Plan (IEP) and disability status; and home and school language. The student record abstract form was completed by school staff after the end of the school year. This file is useful in providing additional predictors and correlates of children's transitions to kindergarten and later progress in school. This file is only available as a restricted-use file. For 1-12 more information about this file, see the ECLS-K Base Year Restricted-Use Student Record Abstract File (NCES 2001-016). The Head Start Verification file contains information from Head Start program providers. The purpose of the Head Start Verification Study was twofold: (1) to identify which of the children reported by either their parents or their schools as having attended Head Start the year prior to kindergarten did indeed attend a Head Start program and (2) to evaluate the process of identifying Head Start participation through parent and school reports and provide further information on the actual process of verifying these reports. This file is a restricted-use file. For more information about this file, see the ECLS-K Base Year Restricted-Use Head Start File (NCES 2001-025). The outcomes of the verification process are also included as data items on the ECLS-K first grade and kindergarten-first grade longitudinal files."}, {"section_title": "Contents of Manual", "text": "This manual provides documentation for users of the third grade public-use data of the ECLS-K. The manual contains information about the data collection instruments (chapter 2) and the psychometric properties of these instruments (chapter 3). It describes the ECLS-K sample design and weighting procedures (chapter 4); data collection procedures and response rates (chapter 5); and data processing procedures (chapter 6). In addition, this manual shows how the public-use third grade data file is structured; provides definitions of composite variables (chapter 7); describes how to install and use the Electronic Code Book (chapter 8); and describes how to use and merge the base year, first grade, and third grade files (chapter 9). The Electronic Code Book contains unweighted frequencies for all variables. Because this manual focuses on the third grade data collection, minimal information is provided about the base year or first grade data. Users who wish to learn more about these data collections should refer to the ECLS-K Base Year Public-Use Data Files and Electronic Code Book: User's Manual (NCES 2001-029); the ECLS-K Restricted-Use Base Year: Child File, Teacher File, and School File (NCES 2000-097); the User's Manual for the ECLS-K First Grade Public-Use Data Files and Electronic Code Book (NCES 2002-135); or the User's Manual for the ECLS-K First Grade Restricted-Use Data Files and Electronic Code Book (NCES 2002-128). Additional information about the ECLS program can be found on the World Wide Web at http://nces.ed.gov/ecls."}, {"section_title": "2-1", "text": ""}, {"section_title": "DESCRIPTION OF DATA COLLECTION INSTRUMENTS", "text": "This chapter describes the survey instruments used during the third grade data collection of the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K). Exhibit 2-1 lists all the instruments used during the third grade data collection. The instrumentation for the base year and first grade data collections are also shown. Similarities and differences between the third grade instruments and those used in the previous rounds are highlighted throughout this chapter. The ECLS-K third grade data collection occurred in the spring of the 2001-02 school year. Data were collected using computer-assisted interviewing (CAI) for parent interviews and child assessments. Self-administered questionnaires were used to collect information from teachers (teacher questionnaires and special education teacher questionnaires) and school administrators or their designees (school administrator questionnaire, school fact sheet, and student records abstract). Field staff completed the school facilities check list. In addition, children completed a short self-description questionnaire on their own as part of the direct child assessments. The third grade data collection instruments, with some exceptions, are available on the CD-ROM as appendix A. The exceptions are the direct child assessment, the Social Rating Scale (SRS) 1 in the teacher questionnaire, and the Self-Description Questionnaire (SDQ). 2 These latter measures contain copyright-protected materials and agreements with the test publishers that restrict their distribution."}, {"section_title": "2-2", "text": "Exhibit 2-1. Instruments used in the ECLS-K, by round of data collection: School years 1998School years -99, 1999School years -2000School years , and 2001School years -02 1998 Statistics, 2002c) for information about the purposes and methods of the fall-first grade data collection. 2 In spring-first grade, there were two sets of teacher questionnaires\u23afone for the teachers of children who had made the transition to the first grade or any higher elementary school grade, and the second for teachers of children who were repeating or attending the second year of kindergarten. 3 In spring-first grade, there were two different school administrator questionnaires\u23afone for school administrators in schools new to the study and one for school administrators in schools that participated in the base year data collection. 4 The salary and benefits questionnaire collected information on the base salary, merit pay, and health benefit pay of teachers and principals. It was completed by the school or district business administrator or by a private school administrator or headmaster. 5 The Head Start Verification Study confirmed parent and school reports of children's Head Start participation by matching information on the name and location of the Head Start facilities the children were reported to have attended against a database of Head Start centers. For each match, the center was contacted to confirm that the child had attended the center in the year before kindergarten. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 kindergarten, first grade, and third grade data collections, school years 1998-99, 1999-2000, and 2001-02. 2-3"}, {"section_title": "Direct Child Assessments", "text": "One-on-one direct child assessments were administered using both hard-copy instruments and computer-assisted interviewing (CAI) in the spring of the 2001-02 school year. The children were assessed regardless of whether they were retained in second grade, promoted to third grade, or moved ahead to fourth. The assessments took about 90 minutes to administer. Exhibit 2-2 displays the major domains measured during the direct child assessments from all five rounds of data collection. As in the previous rounds, the third grade assessments included cognitive and physical components. In addition, the third grade assessment contained a socioemotional component completed by the children. The springthird grade cognitive assessment scores include measures that can be compared to the base year assessments conducted in the fall of 1998 and the spring of 1999 and to the first grade assessments conducted in the fall of 1999 and the spring of 2000 to study children's gains in reading and mathematics. Chapter 3 contains a detailed description of the scores and information on their use and interpretation. Exhibit 2-2. ECLS-K direct child assessments, by domain and round of data collection: School years 1998School years -99, 1999School years -2000School years , and 2001School years -02 1998  X Round that included the instrument. / OLDS was administered to language minority students who were new to the study in the spring or did not pass the cut score in the English version during the previous OLDS administration. 1 The OLDS was given to children with a non-English language background to determine if the children understood English well enough to receive the direct child assessments in English. For further information on the OLDS, please refer to the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K) Base Year Public-Use Data Files andElectronic Code Book: User's Manual (NCES 2001-029;U.S. Department of Education, National Center for Education Statistics, 2000) or the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K) Restricted-Use Base Year Child File, Teacher File, andSchool File (NCES 2000-097;U.S. Department of Education, National Center for Education Statistics, 2001). The OLDS was not used in third grade because the vast majority of children passed it by spring-first grade. 2 In spring-third grade, general knowledge assessment was replaced with a science assessment. Children received a science assessment that measured their understanding of science concepts and scientific investigation skills. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 kindergarten, first grade, and third grade data collections, school years 1998-99, 1999-2000, and 2001-02."}, {"section_title": "2-4", "text": "The third grade direct child assessment began by verifying the child's name and administering a short set of warm-up exercises similar in form to the items used in the SDQ (see below). The assessor then administered the SDQ followed by the reading, math, and science assessments, and then by the physical measurements."}, {"section_title": "Socioemotional Development", "text": "To measure children's socioemotional development, the ECLS-K assessors administered the SDQ, which is used to determine how children think and feel about themselves both socially and academically. The SDQ consists of 42 statements. Children rated their perceptions of competence and their interest in reading, mathematics, and \"all school subjects.\" They also rated their perceptions of competence and popularity with peers and reported on problem behaviors with which they might struggle. Each behavior was rated in relation to their perception of themselves on a one to four response scale: \"not at all true,\" \"a little bit true,\" \"mostly true,\" or \"very true.\" The 42 items factored into six scales: SDQ Reading scale includes eight items about reading grades, the difficulty of reading work, and their interest in and enjoyment of reading. SDQ Mathematics scale includes eight items about mathematics grades, the difficulty of mathematics work, and their interest in and enjoyment of mathematics. SDQ School scale includes seven items about how well they do in \"all school subjects\" and their enjoyment of \"all school subjects.\" SDQ Peer scale includes six items about how easily they make friends and get along with children as well as their perception of their popularity. SDQ Anger/Distractibility scale includes six items about externalizing problem behaviors such as fighting and arguing \"with other kids,\" talking and disturbing others, and problems with distractibility. SDQ Sad/Lonely/Anxious scale includes seven items about internalizing problem behaviors such as feeling \"sad a lot of the time,\" feeling lonely, feeling ashamed of mistakes, and worrying about school and friendships. The items on the first four scales were adapted with permission from the Self-Description Questionnaire I (Marsh, 1990). The items in the two problem behavior scales were developed specifically for the ECLS-K."}, {"section_title": "2-5", "text": "Because children of this age have different levels of reading ability, assessors read the SDQ questions to each child even if a child said that he or she could read them. In this way, children's responses were not affected by their reading ability. Children were given a few seconds after each statement was read to mark their response in the SDQ questionnaire. Assessors were trained to maintain a brisk pace so that the children were not tempted to move ahead. The assessors were also trained not to look at the children's answers so that the children would not be tempted to answer in a more positive way then they would have otherwise. The entire questionnaire took about 5 minutes to administer. Assessors put the SDQ away after the child had completed it and entered the answers into the computer after the child had completed the remaining assessments and had left the room."}, {"section_title": "Cognitive Components", "text": "The direct cognitive assessments were individually administered at all five time points. A two-stage cognitive assessment approach was used to maximize the accuracy of measurement and reduce administration time by using the children's responses from a brief first stage routing test to select a second stage form of the appropriate level of difficulty. 3 The kindergarten-first grade (K-1) cognitive assessment focused on three general content areas: (1) reading; (2) mathematics; and (3) knowledge of the social and physical world, referred to as \"general knowledge.\" The K-1 assessment did not ask the children to write anything or to explain their reasoning; rather, children pointed to their answers or responded orally to complete the tasks. The assessment battery was administered using small easels with the items printed on one side and administration instructions for the assessor on the other side. Assessors entered children's responses on a laptop computer. The third grade direct cognitive assessments, as in previous years, included reading and mathematics domains. By third grade, however, children's knowledge of the world is more categorized into science and social studies domains. With limited time available for direct assessment, the third grade assessment included only the science domain. The third grade assessments also utilized a two-stage design. Easels were used to administer items in mathematics and science. The students also completed workbooks with open-ended mathematics questions. The reading passages and questions were in a 3 For details on the two-stage assessment design, see the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K) "}, {"section_title": "2-6", "text": "booklet format to allow the student to refer back to the story when answering the questions. All questions were read by the assessor. In mathematics and science, all available response options were read to the child. However, the child read the response options in the reading assessment. The ECLS-K third grade direct cognitive assessment battery was designed to assess children's academic achievement in spring of third grade, and to provide a means of measuring growth since kindergarten entry. Child development and primary education experts consulted on the design and development of the assessment instruments. They recommended that the knowledge and skills assessed by the ECLS-K third grade assessments should represent the typical and important cognitive goals of elementary schools' curricula. The subject matter domains of language use and literacy skills (reading), mathematics, and science were selected. This focus on the main academic subjects of the elementary grades was made because of the central nature of these skills as antecedents of individuals' later educational outcomes. Pools of test items in each of the content domains were developed by a team of elementary education specialists. Items were chosen to extend the longitudinal scales initiated in kindergarten and first grade, but there were grade-appropriate changes in content and format. Test items were reviewed by elementary school curriculum specialists for appropriateness of content and difficulty, and for relevance to the test framework. In addition items were reviewed for sensitivity issues related to minority concerns. Items that passed these content, construct, and sensitivity screenings were field tested in spring 2000. The content validity of the ECLS-K item pools was established by comparing the results of the ECLS-K with scores on the Woodcock-McGrew-Werder Mini-Battery of Achievement (MBA;Woodcock, McGrew, and Werder, 1994) that was also administered during the field test. Additional information about the development of the third grade cognitive assessment battery can be found in the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K), Psychometric Report for the Third Grade (U.S. Department of Education, National Center for Education Statistics, forthcoming[a])."}, {"section_title": "Reading", "text": "The K-1 reading (language and literacy) assessment included questions designed to measure basic skills (print familiarity, letter recognition, beginning and ending sounds, rhyming sounds, \"sight\" word recognition), vocabulary (receptive vocabulary), and comprehension (listening comprehension, 2-7 words in context). Comprehension items were targeted to measure skills in initial understanding, developing interpretation, personal reflection, and demonstrating critical stance. The K-1 reading assessment contained five proficiency levels. These five levels reflect a progression of skills and knowledge. Children are thought to master a level if they pass the items within a level. If a child had mastered one of the higher proficiency levels, he or she was very likely to have passed the items that made up the earlier levels as well. The five levels were as follows: (1) identifyingupper-and lower-case letters of the alphabet by name; (2) associating letters with sounds at the beginning of words; (3) associating letters with sounds at the end of words; (4) recognizing common \"sight\" words; and 5reading words in context. The third grade reading assessment included items that were designed to measure phonemic awareness, single word decoding, vocabulary (reading), and passage comprehension. The comprehension items measured skills in initial understanding, developing interpretation, personal reflection, and demonstrating a critical stance. The passage reading section examined sentence, paragraph, and story comprehension and comprised a variety of literary genres including poetry, letters, informational text, and narrative text. The test items marking the highest two K-1 proficiency levels, recognizing common \"sight\" words and reading words in context, were retained in the third grade assessment. Three higher proficiency levels were added at the third grade level: literal inference, extrapolation, and evaluation. Thus the third grade reading assessment contained five proficiency levels. These five levels reflected a progression of skills and knowledge: if a child had mastered one of the higher levels, he or she was very likely to have passed the items from the earlier levels as well. The third grade proficiency levels were as follows: (1) recognizing common \"sight\" words; (2) reading words in context; (3) making inferences using cues that were directly stated with key words in text (literal inference); (4) identifying clues used to make inferences (extrapolation), and using personal background knowledge combined with cues in a sentence to understand use of homonyms; and (5) demonstrating understanding of author's craft and making connections between a problem in the narrative and similar life problems (evaluation).\nThe K-1 reading assessment contained three questions assessing children's familiarity with conventions of print. The score for these questions was obtained by counting the number of correct answers (zero to three) for the following three items, administered while the child was looking at an illustrated story: Indicating that reading goes from left to right; Going to the beginning of the next line after a line ends; and Finding the end of the story. These items were part of the reading score calculations in the direct cognitive assessment but did not necessarily fit into a hierarchical pattern of skill mastery. For example, some children scored high on print familiarity but could not recognize letters, while others had the reverse pattern. These items were not included in the third grade reading forms because nearly all children had mastered them by the end of first grade. A set of four relatively difficult decoding items was reported for the third grade assessment. These were words that were unlikely to be in most children's everyday vocabulary but could be sounded out phonetically. The print familiarity scores for the four kindergarten and first grade rounds are based on the same tasks and may be compared with each other; however, the grade three decoding score was an entirely new task, so comparisons with scores in the earlier rounds are not meaningful. See table 3-5 for variable names, descriptions, ranges, weighted means, and standard deviations for the reading cluster scores: print familiarity and decoding score. "}, {"section_title": "Mathematical Thinking", "text": "The K-1 mathematics assessment was designed to measure skills in conceptual knowledge, procedural knowledge, and problem solving. Approximately one-half of the mathematics assessment 2-8 consisted of questions on number sense and number properties and operations. The remainder of the assessment included questions in measurement; geometry and spatial sense; data analysis, statistics, and probability; and patterns, algebra, and functions. The mathematics assessment contained several items for which manipulatives were available for children to use in solving the problems. Paper and pencil were also offered to the children to use for the appropriate parts of the assessment. The items in the K-1 mathematics assessment could also be grouped into five proficiency levels, though the math clusters were less homogeneous in content than the reading clusters. The clusters of math items included the following: (1) identifying some one-digit numerals, recognizing geometric shapes, and one-to-one counting up to ten objects; (2) reading all one-digit numerals, counting beyond ten, recognizing a sequence of patterns, and using nonstandard units of length to compare the size of objects; (3) reading two-digit numerals, recognizing the next number in a sequence, identifying the ordinal position of an object, and solving a simple word problem; (4) solving simple addition and subtraction problems; and (5) solving simple multiplication and division problems and recognizing more complex number patterns. The third grade mathematics assessment addressed the following content strands: number sense, properties, and operations; measurement; geometry and spatial sense; data analysis, statistics, and probability; and pattern, algebra, and functions. The cognitive processes (conceptual, procedural, and problem solving) are assessed in each of the strands. Some of the items draw upon knowledge from more than one strand. For example, an item might require that a child apply knowledge about geometry, measurement, and number operations to answer the question correctly. Proficiency levels defined in the third grade assessment included levels 4 and 5 retained from the earlier test forms, plus two new levels: place value, and rate and measurement. Thus the items in the third grade mathematics assessment could be grouped into four proficiency levels. The clusters of third grade mathematics items included the following: (1) solving simple addition and subtraction problems; (2) solving simple multiplication and division problems and recognizing more complex number patterns; (3) demonstrating understanding of place value in integers to hundreds place; and (4) using knowledge of measurement and rate to solve word problems. 2-9"}, {"section_title": "Science", "text": "The K-1 assessment battery differed from the third grade battery. The K-1 battery included a measure of general knowledge whereas the third grade included a measure of science. The K-1 general knowledge assessment battery consisted of items that measured knowledge in the natural sciences and social studies in a single scale. The science subdomain measured two broad classes of science competencies: (1) conceptual understanding of scientific facts and (2) skills and abilities to form questions about the natural world, to answer such questions on the basis of the tools and the evidence collected, to communicate answers, and to explain how the answers were obtained. The social studies subdomain included questions that measured children's knowledge in a wide range of disciplines such as history, government, culture, geography, economics, and law. The science subdomain included questions from the fields of life, earth, space, and physical sciences. The assessment items drew on children's experiences with their environment, and many questions related to more than one of the categories. The items captured information on children's conception and understanding of the social, physical, and natural world and of their ability to draw inferences and comprehend implications. The skills children need to establish relationships between and among objects, events, or people and to make inferences and to comprehend the implications of verbal and pictorial concepts were measured. The subject matter content of the K-1 general knowledge assessment domain was too diverse and the items insufficiently ranked or graded to permit the formation of a set of proficiency levels. It was also not possible to develop separate scores for science and social studies. Instead, a single score was calculated to represent each child's breadth and depth of understanding and knowledge of the world around them. As noted previously, the third grade battery addressed the science domain. Equal emphasis was placed on life science, earth and space science, and physical science. Similar to the K-1 assessment of general knowledge, children needed to demonstrate understanding of the physical and natural world, draw inferences, and comprehend relationships. In addition, third-graders needed to interpret scientific data, formulate hypotheses, and identify the best plan to investigate a given question. As with the K-1 general knowledge assessment, no set of proficiency levels was developed.\nThe 15 routing test items of the third grade science assessment tapped a range of basic concepts, with five questions each in life science, physical science, and earth science: Life Science: a sample of concepts related to anatomy/health, animal characteristics/ behavior, and ecology; Physical Science: a sample of concepts related to states of matter, sound, physical characteristics, and the scientific method; and Earth Science: a sample of concepts related to the solar system, earth, soil, minerals, and weather.  "}, {"section_title": "2-10", "text": ""}, {"section_title": "Physical Components", "text": "In the fall of the base year there were two parts to the physical component of the child "}, {"section_title": "Parent Interview", "text": "The third grade parent interview was conducted using a computer-assisted interview (CAI). The parent interview was conducted primarily in English, but provisions were made to interview parents who spoke other languages with bilingual English-Spanish interviewers or interpreters for other languages. Most of the interviews were conducted by telephone, but a small percentage (2 percent) were conducted in person. The parent interview for the spring-third grade data collection lasted on average 62 minutes and asked approximately 500 questions covering third grade school experiences, child care, parent characteristics, and child health. Exhibit 2-3 provides an overview of the topics covered in the third grade and in the previous rounds of data collection. As can be seen in the table, key topics such as family structure, parental involvement in school, and the child's home environment and cognitive stimulation are covered in most rounds. Other topics, such as parent income, employment, and education, are measured at least once in each school year. The general content areas are similar across the questionnaires, though some topics were added and a few were dropped. For example, in spring-third grade, among the questions added were ones on reading resources in the home (e.g., regular receipt of a newspaper or magazine or the availability of a dictionary or encyclopedia), the respondent's reading practices, and if there was a place set aside for the child to do homework. Topics that were dropped included the parent's report of the Social Rating Scale and attendance at religious services."}, {"section_title": "2-11", "text": "Exhibit 2-3. ECLS-K parent interview, by major content topics and round of data collection: School years 1998School years -99, 1999School years -2000School years , and 2001School years -02 1998  before kindergarten X / / / Child care arrangements year before kindergarten X / / / Child's health and well-being X Summer activities and time use X See notes at end of exhibit."}, {"section_title": "2-12", "text": "Exhibit 2-3. ECLS-K parent interview, by major content topics and round of data collection: School years 1998-99, 1999-2000, and 2001-02-Continued 1998  Parent income X X X Welfare and other public assistance Parent health and emotional wellbeing X X Relationships and social support X X X Marital satisfaction X X Background data X X X Mother's age at first birth X Mother's age at child's birth / WIC 3 benefits during pregnancy X / / / / Whether mother worked for pay between when child was born and time child entered X Round that included the topic. / Content areas asked only of new parent respondents in each round. 1 Asked if new person added to roster or an existing person has missing information on this item. 2 Updated if changed from previous round. 3 Supplemental Food Program for Women, Infants, and Children administered by the Food and Nutrition Service, U.S. Department of Agriculture. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 kindergarten, first grade, and third grade data collections, school years 1998-99, 1999-2000, and 2001-02."}, {"section_title": "2-13", "text": "The order of preference for the respondent to the parent interview was the same as in previous rounds: (1) the respondent from the previous round (if there was one), (2) the child's mother, (3) another parent or guardian, or (4) some other adult household member. In a majority of the cases (92 percent), the grade 3 respondent was the same as the respondent from the previous round. The child's mother was the respondent in 87 percent of the cases and the child's father in 9 percent."}, {"section_title": "Teacher Questionnaires", "text": "During the spring-third grade data collection, each child's teacher received a selfadministered questionnaire consisting of three distinct parts. The first section, part A, asked about the teacher's classroom and the characteristics of the students, instructional activities and curricular focus, instructional practices in different subject areas (language arts, mathematics, science, and social studies), and student evaluation methods. The teacher was also asked about parent involvement. Only teachers of sampled children completed part A, unlike the base year when all kindergarten teachers in the school, regardless of whether they taught a sampled child, completed it. Part B asked questions on school and staff activities and the teacher's views on teaching, the school environment, and overall school climate. Background questions about the teacher were also included in this section. Teachers were asked to complete one copy of part C for each of the sampled children in their classrooms; in this part, teachers were asked to respond to 39 questions about the child's academic performance. The Academic Rating Scale (ARS) gathered data on each sampled child's skills in areas of language and literacy, mathematical thinking, science, and social studies. Part C also included questions from the Social Rating Scale (SRS) that collected data on five areas of children's social skills. The ARS and SRS are described in more detail in sections 2.3.1 and 2.3.2, respectively. The same teacher questionnaires were completed by the teacher of the sampled child regardless of the child's grade level. In addition to the teacher questionnaire described above, the ECLS-K also included special education teacher questionnaires described in section 2.4. Exhibit 2-4 shows the distribution of topics covered in the spring-third grade teacher questionnaires and previous rounds of data collection."}, {"section_title": "2-14", "text": "Exhibit 2-4. Teacher questionnaires, by major content topics and round of data collection: School years 1998School years -99, 1999School years -2000School years , and 2001School years -02 1998  Homework time in different subjects X 1 Time in reading and math achievement groups X X X X 1"}, {"section_title": "Classroom characteristics", "text": "Children with special needs See notes at end of exhibit."}, {"section_title": "2-15", "text": "Exhibit 2-4. Teacher questionnaires, by major content topics and round of data collection: School years 1998-99, 1999-2000, and 2001-02-Continued 1998 Teachers' evaluation and grading practices Perception of personal influence on policies and classroom planning X Teacher experience and education Part C Indirect child cognitive evaluation by teacher (ARS) X X X X X Language and literacy, mathematics, general knowledge (science and social studies) See notes at end of exhibit."}, {"section_title": "2-16", "text": "Exhibit 2-4. Teacher questionnaires, by major contact topics and round of data collection: School years 1998-99, 1999-2000, and 2001-02-Continued 1998 Overall academic skills and physical activity levels X Round that included the construct. / Content areas asked only of new teacher participants. 1 Topic is in teacher questionnaire part A. 2 Topic is in teacher questionnaire part B. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 kindergarten, first grade, and third grade data collections, school years 1998-99, 1999-2000, and 2001-02."}, {"section_title": "Academic Rating Scale", "text": "The kindergarten and first grade ARS contained three scales: language and literacy, mathematics, and general knowledge. There are four scales of the third grade ARS: language and literacy, mathematical thinking, science, and social studies. The areas measured in the ARS overlap and augment what is measured in the direct cognitive assessment. The items were designed to ascertain the current skill levels, knowledge, and behaviors of the child in third grade based on the teacher's past observation and experience with the child. In the third grade, the teacher most knowledgeable of each sampled child's skills and knowledge in each of the content areas was asked to complete the ratings. Thus, each sampled child's primary or homeroom classroom teacher was asked to forward the questionnaire to the appropriate content area teacher to complete. Although the topics covered in the ARS are similar across years, the skills that children exhibit for a particular topic, such as reads fluently, increase by grade. Teachers were provided with examples that helped them establish the level of difficulty of a particular item. For example, reading 2-17 fluency is covered in both first and third grade, but the third grade item sets a higher difficulty level, as seen below: Spring-first grade: Reads first grade books fluently-for example, easily reads words in meaningful phrases rather than reading word by word. Spring-third grade: Reads fluently-for example, easily reads words as part of meaningful phrases rather than word by word, including words with three or more syllables, such as rambunctious, residential, genuinely, and pneumonia. Similarly, in mathematics the item about demonstrating understanding of place value has a similar stem in both grades, but the third grade item sets a higher level of difficulty: Spring-first grade: Demonstrates an understanding of place value-for example, by explaining that fourteen is ten plus four, or using two stacks of ten and five single cubes to represent 25. Spring-third grade: Shows understanding of place value with whole numbers-for example, correctly orders the numbers 19,321, 14,999, 9,900, and 20,101 from least to greatest, or correctly regroups when adding and subtracting. Below is a description of the content of the third grade ARS: The Language and Literacy section of the ARS consists of eight items. Teachers are asked to rate each child's proficiency in expressing ideas, use of strategies to gain information, reading on grade level, and writing. In the Mathematical Thinking section, teachers rate each child on nine items that tap the following skills: number concepts (place value, fractions, and estimation), data analysis, measurement, operations (division), geometry, application of mathematical strategies, and creating and extending patterns. The Science section of the ARS consists of seven items. Teachers are asked to rate each child's ability to make predictions, form explanations and conclusions based on observation and investigation, communicate scientific information, apply scientific principles, and demonstrate understanding of life science, earth and space, and physical science. The Social Studies section of the ARS consists of six items. Teachers are asked to rate each child's knowledge and understanding of cultural differences, economics, geography (map skills and the interaction between humans and the environment), history, and government. See chapter 3, section 3.2 for scale scores, value ranges, means, and standard deviations for the ARS."}, {"section_title": "2-18", "text": ""}, {"section_title": "Teacher Social Rating Scale", "text": "Teachers rated individual students' social development on part C of the teacher questionnaire. These items are intended to measure approaches to learning, self-control, and interpersonal skills. The items were rated on a scale of one (never) to four (very often). The same five scales defined for the K-1 assessments are formed from these items. Three of the scales capture positive aspects of children's development and two represent problem behaviors. In third grade, examination of the responses suggested a different perception of student's self-control and interpersonal social abilities. The self-control scale includes items on control of attention as well as control of emotions and behavior in interactions. Third grade students who were rated higher on self-control were also rated higher on interpersonal skills that involved peers. Thus the file includes a peer relations score that combines responses on both the interpersonal items and self-control items that relate to peers, as well as these scales reported separately to facilitate comparison with earlier rounds of data collection. See chapter 3, section 3.3 for variable names, ranges, means, and standard deviations for these scales. The Approaches to Learning scale (Teacher SRS) measures behaviors that affect the ease with which children can benefit from the learning environment. It includes six items that rate the child's attentiveness, task persistence, eagerness to learn, learning independence, flexibility, and organization. In the third grade administration, an item \"child follows classroom rules\" was added to the SRS to increase variance in the selfcontrol scale. The Self-Control scale (Teacher SRS) has four items that indicate the child's ability to control behavior by respecting the property rights of others, controlling temper, accepting peer ideas for group activities, and responding appropriately to pressure from peers. The Interpersonal Skills scale (Teacher SRS) has five items that rate the child's skill in forming and maintaining friendships; getting along with people who are different; comforting or helping other children; expressing feelings, ideas, and opinions in positive ways; and showing sensitivity to the feelings of others. The Peer Relations scale (grade three Teacher SRS) has nine items. The scale is a combination of the items from the interpersonal skills and self-control scales. In the third grade, the teacher ratings indicated that self-control and interpersonal skills are so strongly related that they form a single scale that represents the child's skill in establishing and maintaining peer relationships.\nThe teacher Social Rating Scale (SRS) asked third-grade teachers to report how often students exhibited certain social skills and behaviors.  Five teacher SRS scales were developed based on responses to the scale. The scale scores on all SRS scales are the mean rating on the items included in the scale. Scores were computed only if the student was rated on at least two-thirds of the items in that scale. The five social skill teacher scales are as follows: approaches to learning, self-control, interpersonal skills, externalizing problem behaviors, and internalizing problem behaviors. Although 24 of the 26 third grade SRS items were the same as items in the K-1 instrument, teachers may place different interpretations on the meaning of the items at different time points. Therefore these scores would be most appropriately used as covariates rather than as change scores. Two items were added to the third grade scales due to a high number of maximum scores on the field assessment of these items. One item was added to the externalizing problem behavior scale (\"child talks during quiet study time\"). The other additional item \"child follows classroom rules\" was added to the SRS in an attempt to increase variance in the self-control scale. Analysis of the item responses indicated that it contributed strongly to the approaches to learning scale, increasing the variance and reliability of that scale. Thus, this item is included in the approaches to learning scale."}, {"section_title": "2-19", "text": "The two problem behavior scales reflect behaviors that may interfere with the learning process and the child's ability to interact positively in the classroom. Externalizing Problem Behaviors scale (Teacher SRS) includes acting out behaviors. The kindergarten and first grade forms have five items on this scale that rate the frequency with which a child argues, fights, gets angry, acts impulsively, and disturbs ongoing activities. To increase the variance on this scale, an item was added in third grade asking about the frequency with which a child talks during quiet study time. The Internalizing Problem Behavior scale (Teacher SRS) asks about the apparent presence of anxiety, loneliness, low self-esteem, and sadness. This scale comprises four items. These measures are adapted with permission from the instrument Social Skills Rating Scale: Elementary Scale A (\"How Often?\") (Gresham and Elliott, 1990)."}, {"section_title": "Special Education Teacher Questionnaires", "text": "In the spring-third grade data collection, ECLS-K supervisors reviewed accommodation and inclusion information for children who received special education services. During the preassessment phone call with the school coordinator, the field supervisors asked for the names of sampled children receiving special education services, and the names of the teachers providing these services. The supervisor then listed special education staff working with each child (e.g., speech pathologists, reading instructors, and audiologists). Questionnaires were given to these special education teachers and related services providers. If a child received special education services from more than one special education teacher/provider, a field supervisor determined the child's primary special education teacher/service provider. The primary special education teacher/service provider was defined as The teacher who managed the child's individualized education plan (IEP); The teacher who spent the most amount of time providing special education services to the child; or The teacher who was most knowledgeable about the child's special needs and use of assistive technologies."}, {"section_title": "2-20", "text": "The spring-third grade special education teacher questionnaires were very similar to the ones used in previous rounds. The only differences were that questions on transition to school were not asked and a few new questions were added. Exhibit 2-5 provides a summary of the content areas addressed in the special education teacher questionnaires in spring-third grade and in the previous rounds. The questionnaires addressed topics such as the child's disability, IEP goals, the amount and type of services used by sampled students, and communication with parents and general education teachers. Exhibit 2-5. Special education teacher questionnaires, by major content topics and round of data collection: School years 1998School years -99, 1999School years -2000School years , and 2001School years -02 1998  Disability category X X X IEP goals for the school year X X X Extent of services X X X Types of services provided for the year X X X Primary placement X X X Teaching practices, methods, and materials X X X Assistive technologies used by child X X X General education goals, expectations, and assessments X X X Collaboration/communication with child's general education teacher X X X Frequency of communicating with child's parents X X X See notes at end of exhibit."}, {"section_title": "2-21", "text": "Exhibit 2-5. Special education teacher questionnaires, by major content topics and round of data collection: School years 1998-99, 1999-2000, and 2001-02-Continued 1998 1998-99, 1999-2000, and 2001-02. Part A of the special education teacher questionnaire was designed to collect information about the special education teacher's professional background and experience. Part B asked about the special education services provided to the child and the nature of the child's special education curriculum. The special education teacher of a sampled child or children was asked to complete a copy of part B for each sampled child she or he was responsible for overseeing."}, {"section_title": "School Administrator Questionnaire", "text": "The principal, administrator, or headmaster at the school attended by the sampled child was asked to complete the school administrator questionnaire in the spring of 2002. This self-administered questionnaire was intended to gather information about the school, student body, teachers, school policies, and administrator characteristics. The questionnaire was divided into nine sections. The first seven sections of the school administrator questionnaire requested mainly factual information about each school and the programs offered at the school. Either a principal or a designee who was able to provide the requested information could complete these sections. The school's principal was asked to complete the remaining two sections concerning his or her background and evaluations of the school climate. Exhibit 2-6 summarizes the content areas addressed in this questionnaire in spring-third grade and previous rounds."}, {"section_title": "2-22", "text": "Exhibit 2-6. School administrator questionnaire, by major content topics and round of data collection: School years 1998School years -99, 1999School years -2000School years , and 2001School years -02 1998 -99  school year   1999-2000  school year   2001-02  school year  Spring-first grade  Content topic Springkindergarten"}, {"section_title": "Returning schools", "text": "New schools Community characteristics and school safety X X X X Teaching and other school staff characteristics X X X X Range of salary paid to teachers X X Race/ethnicity of staff X X X X Full-and part-time staff in different specialties X School policies and programs X / X X Assessments, testing, and retention X X X X School-family-community connections X / X X Programs and activities for families X X X Parent involvement and participation X X X Programs for special populations X X X X ESL 1 and bilingual education X X X X Special education X / X X Gifted and talented X X X Principal characteristics X X X X Sex, race/ethnicity, age of principal X X X X Experience and education X X X X School governance and climate X X X X Goals and objectives for teachers X X X X School functioning and decisionmaking X X X X / Fewer details on the topic were collected than for new schools. 1 English as a second language. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 kindergarten, first grade, and third grade data collections, school years 1998-99, 1999-2000, and 2001-02."}, {"section_title": "2-23", "text": "For nonresponding and late-responding schools, interviewers were trained to visit the school and encourage the school administrators to complete the questionnaire. If necessary, the interviewers were to sit down with the administrators to help them fill out the questionnaire. However, if the school administrators were still reluctant to complete the full questionnaire, the interviewers were instructed to obtain key information. This key information covered such topics as the school environment, particularly the safety of the school; school policies and practices; school programs for special populations; staffing and teacher characteristics; and principal characteristics."}, {"section_title": "School Fact Sheet", "text": "The school fact sheet collects basic information about the school including the grades taught in the school, school sector and focus, the length of the school year, and whether the school keeps student attendance records. Some of this information had been included in the school administrator questionnaire and student record abstract in previous rounds. A separate school fact sheet was developed for the springthird grade round for ease of administration."}, {"section_title": "School Facilities Checklist", "text": "ECLS-K supervisors completed the facilities checklist during their visits to the school in the spring of third grade. The facilities checklist collects information about the (1) number of portable classrooms on school grounds, (2) presence of security measures, (3) presence of environmental factors that may affect the learning environment, and (4) overall learning climate of the school."}, {"section_title": "Student Records Abstract Form", "text": "School staff completed the student records abstract form for each sampled child in the spring of kindergarten, first grade, and third grade. This instrument was used to obtain information about the child's attendance record, presence of and details on a child's IEP, and the type of language or English proficiency screening that the school used. A copy of each child's report card was also obtained. The spring-third grade version of the student records abstract form differed from the spring-kindergarten version in two ways: First, no data were collected on the pre-kindergarten Head Start status of children in 3-1"}, {"section_title": "ASSESSMENT AND RATING SCALE SCORES USED IN THE ECLS-K", "text": "Several types of scores were used in the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K) to describe children's cognitive and social development during kindergarten through third grade. These scores were for the direct cognitive assessment, the Academic Rating Scale (ARS), the Social Rating Scale (SRS), and the Self-Description Questionnaire (SDQ). Descriptions of the scores for each assessment or scale follow, along with variable names, variable descriptions, and descriptive statistics from the ECLS-K data files. 1 Guidelines for when and how to use each cognitive assessment score are also provided in this chapter."}, {"section_title": "Direct Cognitive Assessment", "text": "The third grade direct cognitive assessment contained items in reading, mathematics, and science. In each subject area, children received a 15-to 17-item routing test. Performance on the routing items guided the selection and administration of one of three second-stage forms. The second-stage form contained items of appropriate difficulty for the level of ability indicated by the routing items. 2 The third grade direct cognitive assessment built on the framework established in the kindergarten and first grade rounds of data collection, but differed in several important respects: No English language screening: In kindergarten and first grade, children who were identified as coming from a language minority background were administered a language-screening assessment, the Oral Language Development Scale (OLDS), prior to administration of the direct cognitive assessments. Once they achieved a score sufficient for assessment in English, the OLDS was not administered in subsequent rounds of data collection. At kindergarten entry, about 15 percent of the ECLS-K participants were found to need screening for English proficiency. By spring of first grade, less than 6 percent of the sample was screened, and nearly two-thirds of them achieved the score required to go on to the rest of the assessment. The number of sampled children who might still lack English proficiency two years later, in third grade, was assumed to be so small that the language screening assessment was unnecessary. Therefore, the OLDS was not administered in the third grade data collection."}, {"section_title": "3-2", "text": "New assessment instruments: The four rounds of data collection in kindergarten and first grade used the same set of assessment instruments in reading, mathematics, and general knowledge. Children were routed to different levels of difficulty within each assessment domain depending on their performance on a short routing test in each subject area. Because children's academic skills in third grade could be expected to have advanced beyond the levels covered by the kindergarten/first grade (K-1) assessments, a new set of assessment instruments was developed for the third grade. Some of the K-1 assessment items were retained in the third grade forms to support development of a longitudinal score scale. Science assessment: The K-1 general knowledge assessment included basic natural science concepts as well as concepts in social studies. For third grade, a science assessment replaced the general knowledge assessment. There was no longitudinal scale for measuring gains in science through third grade, because the third grade science assessment was not comparable to the K-1 general knowledge assessment."}, {"section_title": "Assessment format:", "text": "The format of the third grade assessment was similar to that of prior rounds, with some changes to accommodate the more advanced level of the questions. As before, a survey administrator presented the questions to the child and entered responses into a computer for each individually administered assessment. A workbook of one to seven questions that required computations or written responses was added to the third grade mathematics assessment. The reading assessment in third grade was administered in booklet format instead of on an easel to accommodate the length of the reading passages used in the assessment."}, {"section_title": "Item cluster scores:", "text": "The K-1 assessment scores included a count of the number right on three questions related to familiarity with conventions of print. Additional cluster scores, based on small numbers of reading and science items, are reported for the third grade assessment and are described in detail below.   "}, {"section_title": "Item Response Theory (IRT) Scale Scores; Standardized Scores (T-Scores)", "text": "Broad-based scores using the full set of assessment items in reading, mathematics and science were calculated using IRT procedures. The IRT scale scores estimated children's performance on the whole set of assessment questions, while standardized scores (T-scores) reported children's performance relative to their peers on the content domains. IRT makes it possible to calculate scores that can be compared regardless of which second-stage form a child takes. IRT uses the pattern of right, wrong, and omitted responses to the items actually administered in an assessment and the difficulty, discriminating ability, and \"guess-ability\" of each item to place each child on a continuous ability scale. The items in the routing tests, plus a core set of items shared among the different second-stage forms and   "}, {"section_title": "Item Cluster Scores", "text": "Several item cluster scores were reported for the reading and science assessments. These are simple counts of the number right on small subsets of items linked to particular skills. These clusters of items are also included in the broad-range scores described above. Because they are based on very few assessment items, their reliabilities are relatively low. See section 3.1.7 for reliability statistics. 3-9"}, {"section_title": "3-10", "text": ""}, {"section_title": "Proficiency Levels", "text": "Proficiency levels provide a means of distinguishing status or gain in specific skills within a content area from the overall achievement measured by the IRT scale scores and T-scores. Clusters of four assessment questions having similar content and difficulty were included at several points along the score scale of the reading and mathematics assessments. Clusters of four items provided a more reliable assessment of proficiency than did single items because of the possibility of guessing; it is very unlikely that a student who had not mastered a particular skill would be able to guess enough answers correctly to pass a four-item cluster. The following 8 reading and 7 mathematics proficiency levels (exhibits 3-1 and 3-2) were identified in the reading and mathematics assessments for kindergarten through third grade. No proficiency scores were computed for the science assessment because the questions did not follow a hierarchical pattern. The proficiency levels were assumed to follow a Guttman model, that is, a student passing a particular skill level was expected to have mastered all lower levels; a failure should be consistent with nonmastery at higher levels. Only a very small percentage of students in kindergarten through third grade had response patterns that did not follow the Guttman model, that is, a failing score at a lower level followed by a pass on a more difficult item cluster. Overall, including all five rounds of data collection, less than 7 percent of reading response patterns, and less than 5 percent of math assessment results, failed to follow the expected hierarchical pattern. This does not necessarily indicate a different order of learning for these children; since most of the proficiency-level items were multiple choice, many of these reversals may be due to children guessing."}, {"section_title": "3-12", "text": "Exhibit 3-1. Reading proficiency levels, kindergarten through third grade: School years 1998School years -99, 1999School years -2000School years , and 2001 "}, {"section_title": "3-13", "text": "Two types of scores are reported with respect to the proficiency levels: a single indicator of highest level mastered, and a set of IRT-based probability scores, one for each proficiency level. More information on each of these types of scores is provided below."}, {"section_title": "Highest Proficiency Level Mastered", "text": "Mastery of a proficiency level was defined as answering correctly at least 3 of the 4 questions in a cluster. This definition results in a very low probability of guessing enough right answers by chance, generally less than 2 percent. At least two incorrect or \"don't know\" responses indicated lack of mastery. Questions that were answered with an explicit \"I don't know\" were treated as wrong, while omitted items were not counted. Since the ECLS-K direct cognitive child assessment was a two-stage design (where not all children were administered all items), and since more advanced assessment instruments were administered in third grade, children's data did not include all of the assessment items necessary to determine pass/fail for every proficiency level at each round of data collection. The missing information was not missing at random; it depended in part on children being routed to second stage assessment forms of varying difficulty. In order to avoid bias due to the non-randomness of the missing proficiency level scores, imputation procedures were undertaken to fill in the missing information. Pass or fail for each proficiency level was based on actual counts of correct or incorrect responses, if they were present. If too few items were administered or answered to determine mastery of a level, a pass/fail score was assigned based on the remaining proficiency scores only if they indicated a pattern that was unambiguous. That is, a \"fail\" was inferred for a missing level if there were easier cluster(s) that had been failed and no higher cluster passed; or a \"pass\" was assumed if harder cluster(s) were passed and no easier one failed. In the case of ambiguous patterns (e.g., pass, missing, fail, where the missing level could legitimately be either a pass or a fail), an additional imputation step was undertaken that relied on information from the child's performance on all of the domain items answered in that round of data collection. IRT-based estimates of the probability of a correct answer were computed for each missing assessment item and used to assign an imputed right or wrong answer. These imputed responses were then aggregated in the same manner as actual responses to determine mastery at each of the missing levels. More than 80 percent of the \"highest level\" scores in both reading and mathematics were determined on the basis of item response data alone; the rest utilized IRT-based probabilities for some or all of the missing items. Scores were not imputed for missing levels that included a reversal (e.g., fail, blank, pass) because no resolution of the missing data could result in a consistent hierarchical pattern."}, {"section_title": "3-14", "text": "Scores in the data file represented the highest level of proficiency mastered by each child at each round of data collection, whether this determination was made by actual item responses, by imputation, or by a combination of methods. The highest proficiency level mastered implies that children demonstrated mastery of all lower levels and non-mastery of all higher levels. A zero score indicates nonmastery of the lowest proficiency level. Scores were excluded only if the actual or imputed mastery level data resulted in a reversal pattern as defined above. The highest proficiency level-mastered scores did not necessarily correspond to an interval scale, so in analyzing the data, they should be treated as ordinal. See table 3-7 for variable names, descriptions, and weighted percentages for the highest proficiency level-mastered scores. "}, {"section_title": "3-15", "text": ""}, {"section_title": "Proficiency Probability Scores", "text": "Proficiency probability scores were reported for each of the proficiency levels described above, at each round of data collection. The scores estimate the probability of mastery of each level, and can take on any value from zero to one. An IRT model was employed to calculate the proficiency probability scores, which indicated the probability that a child would have passed a proficiency level, based on the child's whole set of item responses in the content domain. The item clusters were treated as single items for the purpose of IRT calibration, in order to estimate students' probabilities of mastery of each set of skills. The hierarchical nature of the skill sets justified the use of the IRT model in this way. The proficiency probability scores differed from the highest-level scores in that they could be used to measure gains over time, and from the IRT scale scores in that they targeted specific sets of skills. The proficiency probability scores can be averaged to produce estimates of mastery rates within population subgroups. These continuous measures can provide a close look at individuals' status and change over time. Gains in probability of mastery at each proficiency level allow researchers to study not only the amount of gain in total scale score points but also where along the score scale different children made their largest gains in achievement during a particular time interval. For example, subtracting the level 1 probability at time 1 from the level 1 probability at time 2 indicates whether a student advanced in mastery of the particular set of level 1 skills during this time interval. Thus, students' school experiences can be related to improvements in specific skills. See tables 3-8 and 3-9 for variable names, descriptions, ranges, weighted means, and standard deviations for the proficiency probability scores in reading and mathematics.  "}, {"section_title": "3-16", "text": ""}, {"section_title": "3-17", "text": ""}, {"section_title": "3-18", "text": "Some examples of interpretation and use of the proficiency probability scores whose means appear in table 3-8 are the following: At entry to kindergarten about 70 percent (mean probability = .70) of children were proficient at letter recognition (C1R2RPB1). The largest gains between spring-kindergarten and spring-first grade were made in reading simple sight words, with 15 percent of children having mastered this skill at the end of kindergarten (C2R2RPB4) compared to 74 percent a year later (C4R2RPB4). There were only small gains in letter recognition after spring-kindergarten, because most children, 94 percent, knew their letters by this time (C2R2RPB1). Children's skills in making inferences based on cues directly stated in text (literal inference) increased dramatically between first and third grade, from 15 percent (C4R2RPB6) to 74 percent (C5R2RPB6). In spring-third grade, most children had not yet demonstrated understanding of the author's craft or making connections between a problem in the narrative and similar life problems. Only 26 percent mastered the evaluation level (C5R2RPB8). Comparisons of subgroups may be made by computing the mean probability for each group at a single point in time, or the mean gain for each group from one time to another. See section 3.1.6 for further discussion of measurement of gain."}, {"section_title": "Choosing the Appropriate Score for Analysis", "text": "Each of the types of scores described earlier measures children's achievement from a slightly different perspective. The choice of the most appropriate score for analysis purposes should be driven by the context in which it is to be used: A measure of overall achievement versus achievement in specific skills; An indicator of status at a single point in time versus growth over time; and A criterion-referenced versus norm-referenced interpretation. 3-19"}, {"section_title": "Item Response Theory-Based Scores", "text": "The scores derived from the IRT model (IRT scale scores, T-scores, proficiency probabilities) were based on all of the child's responses to a subject area assessment. That is, the pattern of right and wrong answers, as well as the characteristics of the assessment items themselves, were used to estimate a point on an ability continuum, and this ability estimate, theta, then provided the basis for criterion-referenced and norm-referenced scores. The IRT scale scores are overall, criterion-referenced measures of status at a point in time. They are useful in identifying cross-sectional differences among subgroups in overall achievement level and provide a summary measure of achievement useful for correlational analysis with status variables, such as demographics, school type, or behavioral measures. The IRT scale scores were used as longitudinal measures of overall growth. Gains made at different points on the scale have qualitatively different interpretations. For example, children who made gains in recognizing letters and letter sounds are learning very different lessons from those who are making the jump from reading words to reading sentences, although the gains in number of scale score points may be the same. Comparison of gain in scale score points is most meaningful for groups that started with similar initial status. The standardized scores (T-scores) are also overall measures of status at a point in time, but they are norm-referenced rather than criterion-referenced. They do not answer the question, \"What skills do children have?\" but rather \"How do they compare with their peers?\" The transformation to a familiar metric with a mean of 50 and standard deviation of 10 facilitates comparisons in standard deviation units. Tscore means may be used longitudinally to illustrate the increase or decrease in gaps in achievement among subgroups over time. T-scores are not recommended for measuring individual gains over time. The IRT scale scores or proficiency probability scores are used for that purpose. Proficiency probability scores, derived from the overall IRT model, are criterionreferenced measures of proficiency in specific skills. Because each proficiency score targets a particular set of skills, they are ideal for studying the details of achievement, rather than the single summary measure provided by the IRT scale scores and Tscores. They are useful as longitudinal measures of change because they show not only the extent of gains but also where on the achievement scale the gains are taking place. Thus, they can provide information on differences in skills being learned by different groups, as well as the relationships with processes, both in and out of school, that correlate with learning specific skills. For example, high socioeconomic status (SES) kindergarten children showed very little gain in the lowest reading proficiency level, letter recognition, because they were already proficient in this skill at kindergarten entry. At the same time, low SES children made big gains in basic skills, but most had not yet made major gains in reading words and sentences by the end of 3-20 kindergarten. Similarly, the best readers in third grade may be working on learning to make evaluative judgments based on reading material, which would show up as large gains in reading level 8. Less skilled readers may show their largest gains between first and third grade at levels 5 or 6, comprehension of words in context and literal inference. The proficiency level at which the largest change is taking place is likely to be different for children with different initial status, background, and school setting. Changes in proficiency probabilities over time may be used to identify the process variables that are effective in promoting achievement gains in specific skills."}, {"section_title": "Scores Based on Number Right for Subsets of Items (Non-IRT Based Scores)", "text": "The routing test number-right and item cluster scores do not depend on the assumptions of the IRT model. They were derived from item responses on specific subsets of assessment items, rather than estimates based on patterns of overall performance. Highest proficient level mastered also, in theory, was derived from item responses, although a relatively small number of IRT-based estimates were substituted for missing data. Routing test number-right scores for the third-grade reading, math, and science assessments are based on 15, 17, and 15 items respectively (20, 16, and 12 items for the K-1 reading, math and general knowledge assessments, respectively). They target specific sets of skills and cover a broad range of difficulty. These scores may be of interest to researchers because they are based on a specific set of assessment items, which was the same for all children who took the assessment. Item cluster scores in reading (e.g., Decoding Score Gr 3) and science (e.g., Life Science Gr 3) are based on a count of the number correct for a particular set of items. Users may wish to relate these scores to process variables to get a perspective that is somewhat different from that of the hierarchical levels of skills. However, with only three to five items in each of these item cluster scores, reliabilities tend to be relatively low. Highest proficiency level mastered is based on the same sets of items as the proficiency probability scores but consist of a set of dichotomous pass/fail scores, reported as a single highest mastery level. Pass/fail on each of the individual levels in the set is based on whether children were able to answer correctly at least three out of four actual items in each cluster. For about 20 percent of these scores, the item data was supplemented with IRT-based estimates to avoid complications associated with missing data that was not missing at random. The highest proficiency level mastered should be treated as an ordinal variable. 3-21"}, {"section_title": "Measuring Gains", "text": "This section outlines approaches to measuring gains that rely on multiple criterionreferenced points to identify different patterns of student growth. It describes how analysts might use the proficiency probability scores to address policy questions dealing with subgroup differences in achievement growth over time. Traditional approaches using a total scale score to measure change may yield uninformative if not misleading results. For example, analysis of the gain in total scale score points in reading between fall-and spring-kindergarten shows an average increase of about 10 points. Subgroup analysis shows nearly identical average gains of about the same magnitude for groups broken down by sex, race/ethnicity, SES, and school type, even though the mean scores for the subgroups are quite different. Similarly, each of these groups gained about 7 points, on average, on the mathematics scale during the same time, again starting from very different initial status. It would be incorrect to conclude that because different subgroups of children are gaining quantitatively the same number of scale score points, they are learning the same things, or that these gains are qualitatively comparable in any sense. The problem is non-equivalence of scale units: children who gain 10 points at the low end of the scale, for example, by mastering letter recognition and letter sounds, are not learning the same things as more advanced children, who are achieving their 10 point gains by learning to read words and sentences. The use of adaptive assessments increases the reliability of individual assessment scores by removing the sources of floor and ceiling effects. When assessment forms are matched to children's ability levels, all students have an equal chance to gain on the vertical scale. Depending on how adaptive the measure is, how the scale is constructed, and how even-handed the educational treatment, one may not observe large differences in individual children's amounts of gain in total scale score points. Individual and group differences in the amount of gain given a fairly standard treatment (e.g., a year of schooling) can be relatively trivial compared to individual and group differences in where the gains take place. It is more likely that one will see substantial subgroup differences in initial status than in gains, suggesting that the gains being made by individuals at different points on the score scale are qualitatively different."}, {"section_title": "3-22", "text": "Thus analysis of the total IRT scale score without explicitly taking into consideration where the gain takes place tells only part of the story. The ECLS-K design utilized adaptive assessments to maximize the accuracy of measurement and minimize floor and ceiling effects, and then to develop an IRT-based vertical scale with multiple criterion-referenced points along that scale. These points, the 8 reading and 7 mathematics proficiency levels described in section 3.1.4, model critical stages in the development of skills. Criterion-referenced points serve two purposes at the individual level: (1) they provide information about changes in each child's mastery or proficiency at each level, and 2they provide information about where on the scale the child's gain is taking place. This provides analysts with two options for analyzing achievement gains and relating them to background and process variables. First, gains in probability of proficiency at any level may be aggregated by subgroup, and/or correlated with other variables. Second, the location of maximum gain may be identified for each child by comparing the gains in probability for all of the levels, and focusing on the skills the child is acquiring during a particular time interval. The probabilities of proficiency at any level may be averaged to estimate the proportion of children mastering the skills marked by that level. For example, the spring-first grade mean for mathematics level 5, \"Multiply/Divide,\" was .23, analogous to 22 percent of the first-grade population demonstrating mastery of this set of items. The mean probability at the end of third grade, .75, is equivalent to a population mastery rate of 75 percent. While most children were making their largest gains at level 5, a small number of children were advancing their skills in solving word problems based on rate and measurement, level 7. The mastery rate for level 7 advanced from near zero at the end of first grade to about 14 percent at the end of third grade. These proportions, and the average gains in the proportions for this particular skill, would very likely be quite different for subgroups of children defined by various demographic and school-process categories. Similarly, gains at each level between time 1 and time 2 may be computed for individual children and treated as outcome variables in multivariate models that include background and process measures. Another approach entails computing differences in probabilities of proficiency between time 1 and time 2 for all of the proficiency levels. The largest difference marks the mastery level where the largest gain for a given child is taking place: the \"locus of maximum gain.\" The locus of maximum gain is likely to vary for different subgroups of children categorized according to variables of interest. Once having identified mutually exclusive groups of children according to the proximity of their gains to each 3-23 of the critical points on the developmental scale, one can treat the different types of gains as qualitatively different outcome measures to be explained by background and process variables. Each different analytical approach provides a different perspective with respect to understanding student growth. While comparisons of scale score means may be used to capture information about children at a single point in time, analysis of gains in probability of proficiency is more likely to provide useful information about the contribution of background and process variables to gains in achievement over time. Examples of these approaches can be found in Rock and Pollack (2002). Another important issue to be considered in analyzing achievement scores and gains is assessment timing: children's age at first assessment, assessment dates, and the time interval between successive assessments. Assessment dates ranged from September to November for fall data collections, and from March to June for spring rounds. At kindergarten entry, boys, on average, tend to be older than girls. Children assessed in November of their kindergarten year may be expected to have an advantage over children assessed in the first days or weeks of school. Substantial differences in intervals between assessments may also affect analysis of gain scores. Children assessed in September and June of kindergarten or first grade have more time to learn skills than children assessed in November and March. These differences in intervals may have a relatively small impact on analysis results for long time intervals, such as measuring gains from spring-first grade to spring-third grade, but may be more important within grade, especially fall-to spring-kindergarten. In designing an analysis plan, it is important to consider whether and how differences in ages, assessment dates and intervals may affect the results, to look at relationships between these factors and other variables of interest, and to compensate for differences if necessary."}, {"section_title": "Reliability", "text": "Reliability statistics appropriate for each type of score were computed for each subject area for each round of data collection. For the IRT-based scores, the reliability of the overall ability estimate, theta, is based on the variance of repeated estimates of theta. These reliabilities, ranging from .88 to .96, apply to all of the scores derived from the theta estimate, namely, the IRT scale scores, T-scores, and proficiency probabilities (see table [3][4][5][6][7][8][9][10]. Alpha coefficients for the routing test number correct ranged from .75 to .86 for the third grade assessment forms (see table [3][4][5][6][7][8][9][10][11]. The third grade reading alpha is somewhat lower than in earlier rounds, at least in part due to the third grade form having fewer items 153-24 than the 20 items in the K-1 routing test. Conversely, the alpha coefficient for the math routing test was slightly higher in third grade. The increase in the number of mathematics routing items, from 16 in the K-1 form to 17 in third grade, probably accounts for a small part of this difference. Split-half reliabilities were computed for the item cluster scores in reading and science (see table 3-12). These reliabilities were higher for the reading clusters (.60 to .67) than for the science scores (.46 to .59). The difference in internal consistency statistics is due to the reading items being essentially replications of the same or similar tasks, while the science items had a greater diversity of content. The score indicating the highest proficiency level mastered is based on a combination of raw item scores and IRT-based imputations. For a majority of students, highest level mastered could be determined on the basis of actual item responses alone, while for others imputations were required. Thus, the reported score represents a hybrid of different methodologies, as well as a collapsing of separate measurements into one. Standard measures of reliability are not suitable for assessing the reliability of the highest proficiency level score, for several reasons. Split-half reliabilities at each individual proficiency level could be calculated for children with complete sets of item responses, but would not apply to levels or students whose item response data was supplemented with IRT-based estimates. Similarly, the reliability of the IRT theta would be relevant only to the small percentage of proficiency level scores for which IRTbased estimates alone provided the pass/fail determinations. Furthermore, the score denoting the highest level mastered reduces the series of pass/fail level scores to a single composite, so any reliability estimates obtained for individual levels would not necessarily represent the reliability of the single reported score. As a result, the statistics reported in table 3-13 are not traditional reliability measures. Another approach was employed, based on the idea that reliability statistics are estimates of consistency of measurement under different circumstances, such as different sets of assessment items, or in this case, different methodologies. The two scoring methods, actual item responses and imputations, were compared, and statistics on the agreement between the methods presented in table 3-13 serve as reliability estimates for the highest level mastered scores. First, highest proficiency level mastered was determined on the basis of item responses alone for all of the children (about 80 percent in each round of data collection) who could be categorized without requiring imputations. Then, for the same group of children, highest level was obtained using only imputations, and ignoring actual item responses. The percent of agreement of the scores resulting from independent application of the two methods can be considered a 3-25 proxy for the reliability of the hybrid scoring procedure. The numbers in table 3-13 are the percentages of exact agreement between these two extremes, and the percentages of scores that were within one level of each other. When discrepancies were examined grade-by-grade and level-by-level, there did not appear to be a pattern of either method consistently overestimating or underestimating highest proficiency level when compared with the alternate method. The vast majority of the highest proficiency level scores in the data file were determined on the basis of item responses alone, or item responses supplemented by IRT estimates. The high level of exact-plus-adjacent agreement between the methods indicates that the IRT approach supports the use of the highest level score sufficiently well for use in aggregate statistics. Tables 3-10 through 3-13 present the reliability statistics for all of the assessment scores. .88 \u2020 Not applicable. There was no science assessment prior to third grade. NOTE: The IRT-based scores consist of the IRT scale scores, T-scores, and proficiency probabilities. See sections 3.1.2 and 3.1.4 for a discussion of these scores. Approximately 89 percent of the children interviewed were in third grade during the 2001-02 school year, 9 percent were in second grade, and less than 1 percent were in fourth grade. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 kindergarten, first grade, and third grade data collections, school years 1998-99, 1999-2000, and 2001-02. .75 \u2020 Not applicable. There was no science assessment prior to third grade. NOTE: Approximately 89 percent of the children interviewed were in third grade during the 2001-02 school year, 9 percent were in second grade, and less than 1 percent were in fourth grade. See section 3.1 for a discussion of the routing tests and section 3.1.1 for a discussion of number-right scores. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 kindergarten, first grade, and third grade data collections, school years 1998-99, 1999-2000, and 2001-02. Cluster not collected. NOTE: Approximately 89 percent of the children interviewed were in third grade during the 2001-02 school year, 9 percent were in second grade, and less than 1 percent were in fourth grade. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 kindergarten, first grade, and third grade data collections, school years 1998-99, 1999-2000, and 2001-02. "}, {"section_title": "3-26", "text": ""}, {"section_title": "Validity", "text": "Evidence for the validity of the direct cognitive assessments was derived from several sources. A review of national and state performance standards, comparison with state and commercial assessments, the judgments of curriculum experts and teachers all provided input to test specifications. In addition, comparing the reading and mathematics field-test item pool scores with those obtained from an established instrument provided validity information."}, {"section_title": "3-27", "text": "The ECLS-K test specifications were derived from a variety of sources. For the third through fifth grade assessments, national and state performance standards in each of the domains were examined. The scope and sequence of materials from state assessments, as well as from major publishers, were also considered. The resulting ECLS-K fourth grade frameworks are similar to the NAEP fourth grade frameworks, with some differences due to ECLS-K formatting and administration constraints. The fourth grade frameworks were modified for third grade (and for the earlier K-1 forms). An expert panel of early elementary school educators, including curriculum specialists in the subject areas and teachers at the targeted grade levels from different regions of the country, examined the pool of items and the recommended allocations. The assessment specifications indicated target percentages for content strands within each of the subject areas. These percentages were matched as closely as possible in developing the field-test assessment item pool as well as in selecting items for the third-grade assessment forms. Some compromises in matching target percentages were necessary to satisfy constraints related to other issues, including linking to K-1 scales, avoiding floor and ceiling effects, and field-test item performance. This was especially true for the reading assessment, whose structure, i.e., several questions based on each reading passage, placed an additional constraint on the selection of items to match content strands. Experts in each of the subject areas then reviewed the proposed third-grade forms for appropriateness of content and relevance to the assessment framework. Reliabilities for the MBA were computed both with not-administered and omitted items treated as missing, and with these items treated as incorrect. The correlations of MBA with ECLS-K measures were quite close to the square roots of the reliabilities, indicating that the two assessments were measuring closely related skills. The correlations are presented in table 3-14. "}, {"section_title": "3-28", "text": ""}, {"section_title": "Indirect Cognitive Assessment", "text": "The Academic Rating Scale (ARS) was developed for the ECLS-K to measure teachers' evaluations of students' academic achievement in four domains: language and literacy (reading and writing), science, social studies, and mathematical thinking. Teachers rated the child's skills, knowledge, and behaviors on a scale from \"Not Yet\" to \"Proficient\" (see table [3][4][5][6][7][8][9][10][11][12][13][14][15]. If a skill, knowledge, or behavior had not been introduced into the classroom yet, the teacher coded that item as N/A (not applicable). In third grade, the classroom teacher most knowledgeable of the child's academic achievement in the four domains may not be the primary or homeroom teacher. The primary teacher was asked to forward the rating form to the teacher most knowledgeable of the particular domain to complete the ratings. The differences between the direct and indirect cognitive assessments, and the scores available, are described here. For a discussion of the content areas of the ARS, see chapter 2, section 2.3.1. 3-29"}, {"section_title": "Comparison to Direct Cognitive Assessment", "text": "The ARS was designed both to overlap and to augment the information gathered through the direct cognitive assessment battery. Although the direct and indirect instruments measure children's skills and behaviors within the same broad curricular domains with some intended overlap, several of the constructs they were designed to measure differ in significant ways. Most importantly, the ARS includes items designed to measure both the process and products of children's learning in school, whereas the direct cognitive battery measures only the products of children's achievement. Because of time and space limitations, the direct cognitive battery is less able to measure the process of children's thinking, including the strategies they use to read, solve math problems, or investigate a scientific phenomenon. Due to time constraints, the direct cognitive battery does not include a scale of children's knowledge in social studies. On the ARS teachers reported the children's knowledge and understanding of civics, geography, history, culture, and economics. The criterion-referenced indirect measures on the ARS are targeted to the specific grade level of the student and draw upon the daily observations made by teachers of the students in their class."}, {"section_title": "Rasch Scores Available for the Academic Rating Scale", "text": "A Rasch analysis was used to create measures of the reported performance of students on a hierarchy of skills, knowledge, and behavior. The Rasch Rating Scale model uses the pattern of ratings on items to determine an estimate of the difficulty of each item and to place each student on an interval scale set with a minimum score of one and a maximum score of five. The Rasch analysis showed that the reliability of the estimates of child ability was very high for all domains of the ARS (see table 3-16). As mentioned, the ARS scores are scaled to have a low value of one and a high value of five to correspond to the 5-point rating scale that teachers used in rating children on these items. The item difficulties and student scores are placed on a common scale. Students have a high probability of receiving a high rating on items whose difficulty is below their scale score, and a lower probability of receiving a high rating on items above their scale score. Therefore, the scores children receive on the ARS subscales should not be interpreted as mean scores, but as the child's relative probability of success with the items. Students who received maximum ratings on all the items or minimum ratings on all the items are assigned an estimated score."}, {"section_title": "3-30", "text": "The variable names, descriptions, value ranges, weighted means, and standard deviations for the third grade (T5) ARS scores are shown in table 3-17. The description for each variable in the tables begins with a \"T,\" indicating that it is a teacher questionnaire child-level variable. The items and the metric for the third grade ARS are different from the ARS ratings in earlier rounds of data collection, so the scores are not directly comparable to those for kindergarten and first grade. The students' scores are calculated relative to the item difficulty. With different items used across the grades and separate calibrations performed, the size of the metric differs from one grade to another. On the ARS, teachers indicated \"not applicable\" when the knowledge, skill, or behavior had not been introduced to the classroom. Because some children might have already had this skill (from home or other opportunities for learning), the \"not applicable\" ratings were treated as missing data and the child's score was estimated based on the items on which the child was rated. Although the Rasch"}, {"section_title": "3-31", "text": "program estimates scores for all children based on the information provided, the file includes only the scores of children who had more than 60 percent of the items in a scale rated. In other words, if 40 percent or more of the items in a scale were not rated, then the score was set to missing. Fewer than 1 percent of literacy and mathematics scores, and fewer than 5 percent of science and social studies scores, failed to meet the completeness criterion.  Tables 3-18 to 3-21 provide the estimates of difficulty for each of the items. Higher values mean that teachers rated fewer students as proficient on those items. Students would have a greater than 50 percent probability of receiving ratings of \"5\" on items below their ability level. Tables are provided for third grade items.    The ARS scale was designed to provide information on children's abilities at a given point in time, not necessarily over time. In addition, although some item stems are similar to those used in the kindergarten and first grade teacher questionnaires, the actual items include performance criteria that increase in difficulty from one time to the next. Moreover, the ARS scores are placed on different metrics relative to the item difficulty in a given grade. Therefore, change scores should not be calculated between time points. However, covariance models may be used to compare teacher's ratings of performance in different grades. Before using these variables in such analyses, the distribution of the samples should be assessed to determine if the assumption of normal distribution is met. Tables 3-22 to 3-25 provide standard errors (SE) for each of the Rasch scores for third grade."}, {"section_title": "3-32", "text": ""}, {"section_title": "3-33", "text": "The \"Score\" column is the sum of the raw score ratings. \"Measure\" is the Rasch-based score. The column labeled \"SE\" is the corresponding standard error of measurement for those scores. These standard errors can be used in analytic models to correct for the heteroskedasticity of scores. The \"Score\" column is the sum of the raw score ratings. \"Measure\" is the Rasch-based score. The column labeled \"Standard error (SE)\" is the corresponding standard error of measurement for those scores. E=estimated extreme score. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 third grade data collection, school year 2001-02. The \"Score\" column is the sum of the raw score ratings. \"Measure\" is the Rasch-based score. The column labeled \"Standard error (SE)\" is the corresponding standard error of measurement for those scores. E=estimated extreme score. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 third grade data collection, school year 2001-02. The \"Score\" column is the sum of the raw score ratings. \"Measure\" is the Rasch-based score. The column labeled \"Standard error (SE)\" is the corresponding standard error of measurement for those scores. E=estimated extreme score. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 third grade data collection, school year 2001-02. NOTE: The \"Score\" column is the sum of the raw score ratings. \"Measure\" is the Rasch-based score. The column labeled \"Standard error (SE)\" is the corresponding standard error of measurement for those scores. E=estimated extreme score. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 third grade data collection, school year 2001-02."}, {"section_title": "3-34", "text": ""}, {"section_title": "3-35", "text": "Classroom teachers were asked to forward the ARS to the child's teacher most knowledgeable about the child's performance in each of the subject areas to complete. The majority of teachers rated more than one student on the ARS. The number of students rated by each teacher ranged from one to more than 20. The teacher ratings do not represent a systematic national sample of teachers. Each set of teacher ratings is linked to a sampled child, and teachers were asked to rate as many ECLS-K sample children as they had in class. 3-36"}, {"section_title": "3-37", "text": "In third grade, examination of the responses suggested a different perception of a student's self-control and interpersonal social abilities. The self-control scale includes items on control of attention as well as control of emotions and behavior in interactions. Third-grade students who were rated higher on self-control were also rated higher on interpersonal skills that involved peers. Thus, in addition to the self-control and interpersonal social abilities scale scores, a peer relations scale score was included. This additional scale combines responses on both the interpersonal and self-control scale items that relate to peers. Variable names for the teacher scores, descriptions, ranges, weighted means, and standard deviations for these scales are shown in table 3-27. About 90 percent of the children whose teachers provided social ratings data were in third grade during the round 5 data collection, and about 9 percent were in first or second grade. Numbers in the table are for third graders, with scores for children who at round 5 were still in first or second grade shown in parentheses. The number of children who had advanced to fourth or fifth grade by round 5 was too small to be analyzed separately.  Care should be taken when entering these scales into the same analysis due to problems of multicollinearity. The intercorrelations among the five SRS factors (excluding the combined peer relations scale) are high. The factor intercorrelations with the internalizing problem behaviors are the lowest. The absolute values of correlations among the teacher SRS factors range from 0.32 to 0.81, with nearly identical patterns for third graders and for children who were still in first or second grade."}, {"section_title": "Self-Description Questionnaire", "text": "For the first time in the ECLS-K, third grade students rated their perceived competence and interest in reading, mathematics, and all school subjects. They also rated their perceived competence and popularity with peers and reported on problem behaviors with which they might struggle. The \"Externalizing Problems\" scale included questions about anger and distractability, while \"Internalizing Problems\" scale included items on sadness, loneliness, and anxiety. For further description of the Self-Description Questionnaire (SDQ) see chapter 2. Students rated whether each item was \"not at all true,\" \"a little bit true,\" \"mostly true,\" or \"very true.\" Five scales were produced from the SDQ items. The scale scores on all SDQ scales represent the mean rating of the items included in the scale. Students who responded to the SDQ answered virtually of the questions, so treatment of missing data was not an issue. As with most measures of social-emotional behaviors, the distributions on these scales are skewed (negatively skewed for the positive social behavior scales, and positively skewed for the problem behavior scales). The reliability is lower for scales with only six items (see table 3-29).  "}, {"section_title": "3-39", "text": ""}, {"section_title": "SAMPLE DESIGN AND IMPLEMENTATION", "text": "The Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K) employed a multistage probability sample design to select a nationally representative sample of children attending kindergarten in 1998-99. In the base year the primary sampling units (PSUs) were geographic areas consisting of counties or groups of counties. The second-stage units were schools within sampled PSUs. The third and final stage units were students within schools. The first grade data collection targeted base year respondents, where a case was considered responding if there was a completed child assessment or parent interview in fall-or spring-kindergarten. While all base-year respondents were eligible for the spring-first grade data collection, the effort for fallfirst grade was limited to a 30 percent subsample. The spring student sample was freshened to include current first graders who had not been enrolled in kindergarten in 1998-99 and, therefore, had no chance of being included in the ECLS-K base year kindergarten sample. For both fall-and spring-first grade, only a subsample of students who had transferred from their kindergarten school was followed. The third grade data collection targeted base year respondents and children sampled in first grade through the freshening operation. As in the first grade data collection, only a subsample of students who had transferred from their kindergarten school was followed. In third grade, however, the subsampling rate applied to transferred children was slightly higher: children whose home language is non-English (also known as children belonging to the language minority group) and who moved for the first time in third grade were followed at 100 percent. In other words, children belonging to the language minority group who did not move in first grade but moved in third grade were all followed into their new third grade schools. The higher subsampling rate allows for the preservation of this group in the sample for analytic reasons. Children not in the language minority group continued to be subsampled for followup if they moved in third grade."}, {"section_title": "Base Year Sample", "text": "In the base year, children were selected for the ECLS-K using a multistage probability design. The PSUs were counties or groups of counties selected with probability proportional to size (PPS). The basic PSU measure of size was the number of 5-year-olds, but this was modified to facilitate 4-2 the oversampling of Asian and Pacific Islanders (APIs) required to meet precision goals. In all, there were 100 PSUs selected for the ECLS-K. The 24 PSUs with the largest measure of size were designated selfrepresenting (SR) and were included in the sample with certainty. The remaining non-SR PSUs were partitioned into 38 strata of roughly equal size. An initial cross-classification of census region with metropolitan statistical area (MSA) status created eight superstrata. These were further subdivided by percent minority, PSU measure of size (a composite count of five-year-old children), and 1988 per capita income. From each non-SR stratum, two PSUs were selected PPS without replacement using Durbin's Method (Durbin, 1967). Table 4-1 summarizes the characteristics of the ECLS-K PSU sample. In the second stage, public and private schools offering kindergarten programs were selected. For each PSU, a frame of public and private schools offering kindergarten programs was constructed using existing school universe files: the 1995-96 Common Core of Data (CCD; U.S. , 1995-96) and the 1995-96 Private School Universe Survey (PSS; U.S."}, {"section_title": "Department of Education", "text": ""}, {"section_title": "Department of Education, 1998). The 1995-96 Office of Indian Education Programs Education Directory", "text": "was consulted in order to complete the list of Bureau of Indian Affairs (BIA) schools in the CCD file. For Department of Defense (DOD) domestic schools, a 1996 list of schools was obtained directly from the DOD. A procedure was implemented to identify kindergarten programs that would be operational at the time of ECLS-K's base year data collection, but that were not included in the frame just described. These were newly opened schools that were not listed in the CCD and the PSS, and schools that were in the CCD and the PSS but did not appear to offer kindergarten programs according to those sources. The selection of schools was systematic, with probability proportional to a weighted measure of size based on the number of kindergartners enrolled. As with the PSU sample, the measure of size was constructed 4-3 taking into account the desired oversampling of APIs. Public and private schools constituted distinct sampling strata. Within each stratum, schools were sorted to ensure good sample representation across other characteristics. In total, 1,280 schools were sampled from the original frame, and 133 from the freshened frame. Of these, 953 were public schools and 460 were private schools.  The third stage sampling units were children of kindergarten age, selected within each sampled school. The goal of the student sample design was to obtain an approximately self-weighting 4-4 sample of students and at the same time to achieve a minimum required sample size for APIs who were the only subgroup that needed to be oversampled to meet the study's precision goals. For each sampled school, the field staff obtained a complete list of kindergartners enrolled. Two independent sampling strata were formed within each school, one containing API students and the second, all other students. Within each stratum, students were selected using equal probability systematic sampling, using a higher rate for the API stratum. 1 In general, the target number of children sampled at any one school was 24. Once the sampled children were identified, parent contact information was obtained from the school. The information was used to locate a parent or guardian and gain parental consent for the child assessment and for the parent interview. Table 4-3 presents characteristics of children sampled for the base year. During the fall-kindergarten data collection, a census of kindergarten teachers was taken at each school. Each sampled child was linked to his or her kindergarten teacher. In spring-kindergarten, teacher-child linkages were reviewed and updated. If new kindergarten teachers had joined the school, they were added to the census of kindergarten teachers. Special education teachers who taught one or more sampled children were included in the spring-kindergarten data collection. If a sampled child received special education services from such a teacher, the teacher was linked to that child."}, {"section_title": "Fall-First Grade Subsample", "text": "A subsample of ECLS-K PSUs was selected for fall-first grade data collection.    Fall-first grade data collection consisted of the direct child assessment and the parent interview. Data collection was attempted for every eligible child found still attending the school in which he or she had been sampled during kindergarten. \"Eligible\" is defined as a base year respondent (i.e., a child who had either a fall-or spring-kindergarten child assessment or parent interview). Base year nonrepondents would be adjusted for during weighting. Because of the additional burden of school recruiting, the cost of collecting data for a child who transferred from the school in which he or she was originally sampled greatly exceeds that for a child who stayed enrolled. To contain these costs, a random 50 percent of children were flagged to be followed for fall-first grade data collection in the event that they had transferred."}, {"section_title": "4-6", "text": ""}, {"section_title": "4-7", "text": "Except for children who were repeating kindergarten, all base year children sampled in schools with a high grade of kindergarten are de facto movers. Since many of these movers may move en masse to the same first grade school, steps were taken to follow these children at a higher rate. Using the information collected during spring-kindergarten, a list of destination schools was compiled for each such school. The destination school having the most movers was designated as primary, unless no such school had more than three movers. Children who moved en masse into a primary destination school in fall-first grade were treated as \"nonmovers\" and were not subsampled. Prior to subsampling with equal probability, children were stratified into groups of nonmovers, movers with information identifying their new schools, and movers without such identifying information. A flag was created for each child indicating whether the child had been sampled to be followed. Table 4-5 shows the characteristics of the children subsampled for fall-first grade. Region, locale, religious affiliation, and school type describe the school the child attended in kindergarten."}, {"section_title": "Spring-First Grade Sample", "text": "The ECLS-K spring-first grade data collection targeted all base year respondents. In addition the spring student sample was freshened to include current first graders who had not been enrolled in kindergarten in 1998-99 and, therefore, had no chance of being included in the ECLS-K base year kindergarten sample. While all students still enrolled in their base year schools were recontacted, only a 50 percent subsample of base year sampled students who had transferred from their kindergarten school was followed for data collection."}, {"section_title": "Subsampling Movers", "text": "In spring-first grade all children in a random 50 percent subsample of base year schools were flagged to be followed for data collection if they transferred from their base year school. (This is in contrast to fall-first grade where a random 50 percent of children in each of the 30 percent of schools 4-8 4-9 subsampled were flagged.) In order to maximize the amount of longitudinal data, care was taken during spring-first grade sampling to ensure that any child who had been flagged to be followed in fall-first grade would continue to be so. In selecting the spring-first grade 50 percent subsample of schools where movers would be flagged for followup, the three primary strata were SR PSUs, NSR PSUs that had been selected for fallfirst grade, and NSR PSUs that had not been selected for fall-first grade. Within these major strata, schools were grouped by frame source (original public, original private, new from Catholic dioceses, new from local governments, etc.). Finally within each frame source, schools were stratified by response status, and arranged in original selection order. Schools that had been part of the 30 percent fall-first grade sample were automatically retained. Then equal probability sampling methods were employed to augment the sample to the desired 50 percent. The net result of these procedures was that every base year selected school had a 50 percent chance of having its ECLS-K transfer students followed during springfirst grade, and any transfer student who had been followed in fall-first grade would still be followed in spring-first grade. Table 4-6 shows the characteristics of the children in the spring-first grade sample, excluding freshened students. Region, locale, religious affiliation, and school type describe the school at which the child attended kindergarten.  \nIn spring-first grade all children in a random 50 percent subsample of base year schools were flagged to be followed for data collection if they transferred from their base year school. In order to maximize the amount of longitudinal data, care was taken during spring-first grade sampling to ensure that any child who had been flagged to be followed in fall-first grade would continue to be followed. The spring-first grade sampling procedure for movers is described in section 4.3.1. In spring-third grade, 4-13 children who were followed in spring-first grade were retained in the sample (i.e., the mover followup still targeted the same 50 percent subsample of children in the base year schools). In addition, language minority children who moved between first and third grade were followed with certainty as described below."}, {"section_title": "4-10", "text": ""}, {"section_title": "Student Freshening", "text": "The spring-first grade student freshening used a half-open interval sampling procedure (Kish, 1965). The procedure was implemented in the same 50 percent subsample of ECLS-K base year schools where transfer students were flagged for followup. Each of these schools was asked to prepare an alphabetic roster of students enrolled in first grade and the names of ECLS-K kindergarten-sampled students were identified on this list. Beginning with the name of the first kindergarten-sampled child, school records were checked to see whether the student directly below in the sorted list attended kindergarten in the United States in fall 1998. If not, (1) that child was considered to be part of the freshened sample and was linked to the base year sampled student (i.e., was assigned that student's probability of selection) and (2) the record search procedure was repeated for the next listed child, and so forth. When the record search revealed that a child had been enrolled in kindergarten the previous year, that child was not considered part of the freshened sample and the procedure was begun all over again with the second base year sampled student name, and so on. Note: the student roster was \"circularized\" (i.e., the first name on the roster was considered to follow the last name on the roster in the implementation of the procedure). Student freshening brought 165 first graders into the ECLS-K sample, which increased the weighted survey estimate of the number of first graders in the United States by about 2.6 percent."}, {"section_title": "4-12", "text": "The student freshening procedure was not entirely free of bias. A first grader would have no chance of being in the ECLS-K first grade sample if he or she was enrolled in a school where neither the child nor any of his or her classmates had attended kindergarten in the United States in fall 1998. This would be a rare circumstance and is not thought to be an important source of bias. A more significant source of potential bias is nonresponse. One source of nonresponse inherent to the freshening plan was that the procedure only involved students who had not transferred from the school in which they had been sampled during the base year. A more detailed discussion of freshened student nonresponse can be found in section 5. "}, {"section_title": "Spring-Third Grade Sample", "text": "The sample of children for spring-third grade consists of all children who were base year respondents and children who were brought into the sample in spring-first grade through the sample freshening procedure described in section 4.3.2. Sample freshening was not implemented in third grade; hence no new students entered the sample. While all students still enrolled in their base year schools were recontacted, slightly more than 50 percent of the base year sampled students who had transferred from their kindergarten school were followed for data collection. This subsample of students was the same 50 percent subsample of base year movers flagged for following in spring-first grade, with the addition of movers whose home language was not English (language minority students). The two special sampling procedures implemented in spring-third grade are described below."}, {"section_title": "Language Minority Children", "text": "In addition to the subsample of movers to be followed described above, children whose home language was not English and who moved between spring-first grade and spring-third grade were all retained rather than being subsampled at the 50 percent rate. Operationally, this means that children whose home language was not English who were not flagged for followup in the previous round had their flags switched. This only affects children who had not moved out of the original sample schools before third grade. If they had moved before third grade, than their flags were not switched and they continued not to be followed. This modification to the mover followup procedure provides a larger sample of children whose home language is not English for analytic purposes. The mover followup activities that originally targeted a 50 percent subsample of children in base year schools resulted in a 54 percent subsample with the addition of language minority children. Table 4-7 shows the characteristics of children in the spring-third grade sample, excluding freshened students. Region, locale, religious affiliations, and school type describe the school at which the child attended kindergarten."}, {"section_title": "Sample Attrition", "text": "In a longitudinal study, sample attrition due to nonresponse and change in eligibility status is expected. The sample of respondents decreases with each round of data collection. In the case of the ECLS-K, a combination of field and sampling procedures was applied that caused the sample to increase after the fall-kindergarten data collection, but then decrease in spring-first grade and again in spring-third grade.  The first procedure was refusal conversion in spring-kindergarten, resulting in a number of schools that agreed to participate in the study after having refused to do so in the previous round. From these schools, 1,426 children were sampled and added to the initial sample. The second procedure was sample freshening in spring-first grade as described in section 4.3.2. This brought in 165 eligible children to add to the sample of 21,192 base year respondents who remained eligible after the base year. A base year responding child was defined as one with at least one direct cognitive test score in fall-or springkindergarten or whose parent responded to the family structure section of the parent instrument in fall-or spring-kindergarten. The third and last procedure, applied in first and third grades, required that a subsample of children who moved out of their original sample schools not be followed into their new schools, as described in sections 4.3.1 and 4.4.1, resulting in a decrease in the sample. Table 4-8 shows the sample size for each round of data collection of the ECLS-K, and the response status of the children in each round. Fall-first grade is not included in this table, as it pertains only to a subsample of the ECLS-K children. Tables 4-9 and 4-10 show the same children separately by the original sample school type (public/private).  1 1,426 children were sampled from refusal-converted schools. 2 21,192 children remained eligible after the base year. In addition, 165 children were sampled via the sample freshening procedure. NOTE: Response status is defined in terms of completed child assessment OR completed family structure data of the parent interview. Children who died or moved out of the country are classified as ineligible. Children who moved and were subsampled for followup but could not be located were treated as belonging to the unknown eligibility category. A portion of children who moved was subsampled out and not followed into their new schools. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 kindergarten, first, and third grade data collections, school years 1998-99, 1999-2000, and 2001-02. 12,070 \u2020 Not applicable. 1 891 public school children were sampled from refusal-converted schools. 2 16,638 public school children remained eligible after the base year. In addition, 146 public school children were sampled via the sample freshening procedure. NOTE: Response status is defined in terms of completed child assessment OR completed family structure data of the parent interview. Children who died or moved out of the country were classified as ineligible. Children who moved and were subsampled for followup but could not be located were treated as belonging to the unknown eligibility category. A portion of children who moved was subsampled out and not followed into their new schools. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 kindergarten, first, and third grade data collections, school years 1998-1999, 1999-2000, and 2001-02. 2 4,554 private school children remained eligible after the base year. In addition, 19 private school children were sampled via the sample freshening procedure. NOTE: Response status is defined in terms of completed child assessment OR completed family structure data of the parent interview. Children who died or moved out of the country were classified as ineligible. Children who moved and were subsampled for followup but could not be located were treated as belonging to the unknown eligibility category. A portion of children who moved was subsampled out and not followed into their new schools. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 kindergarten, first, and third grade data collections, school years 1998-99, 1999-2000, and 2001-02. The number of children who participated in the base year and first grade and third grade data collections is 13,698 (10,900 in original public schools and 2,798 in original private schools). This represents 64 percent of the base year respondents or 60 percent of children sampled for the base year."}, {"section_title": "4-14", "text": ""}, {"section_title": "4-16", "text": ""}, {"section_title": "4-17", "text": ""}, {"section_title": "Calculation and Use of Sample Weights", "text": "As in previous years, the ECLS-K data were weighted to compensate for differential probabilities of selection at each sampling stage and to adjust for the effects of nonresponse. As in the first grade year, only child-level weights were computed for third grade. The use of these weights is essential to produce estimates that are representative of the cohort of children who were in kindergarten in 1998-99 or in first grade in 1999-2000. Since the third grade sample was not freshened with third graders who did not have a chance to be sampled in kindergarten or first grade (as was done in first grade), grade teachers and schools in 2001-02. For this reason, the only weights produced from the study are for making statements about children, including statements about the teachers and schools of those children. Several sets of weights were computed for third grade. As in previous years, there are several survey instruments administered to sampled children and their parents, teachers and schools: cognitive and physical assessments for children; parent instruments; several types of teacher instruments; and school instruments. The stages of base year sampling in conjunction with differential nonresponse at each stage and the diversity of survey instruments require that multiple sampling weights be computed for use in analyzing the ECLS-K data. Several combinations of kindergarten through third grade longitudinal weights were also computed. Details on these longitudinal weights are available in chapter 9. This section describes the different types of third grade cross-sectional weights, how they were calculated, how they should be used, and their statistical characteristics."}, {"section_title": "Types of Sample Weights", "text": "Three sets of cross-sectional weights were computed for children in the third grade sample. These weights are defined as follows: C5CW0 is nonzero if the child has completed assessment data or the child was excluded from direct assessment due to a disability. C5PW0 is nonzero if the child has completed parent interview. C5CPTW0 is nonzero if the child has completed assessment data and parent interview data and teacher data from questionnaire part B. Careful consideration should be given to the choice of a weight for a specific analysis since it depends on the type of data analyzed. Each set of weights is appropriate for a different set of data or combination of sets of data. Exhibit 4-1 summarizes how the different types of cross-sectional weights should be used. Cross-sectional weights are used to provide estimates for the third grade data collection. Details under \"to be used for analysis of . . .\" provide guidance based on whether the data to be used with the weights were collected through the child assessments, parent interviews, or teacher questionnaires."}, {"section_title": "4-19", "text": "Exhibit 4-1. ECLS-K third grade cross-sectional weights: School year 2001-02 Weight to be used for analysis of ..."}, {"section_title": "C5CW0", "text": "third grade direct child assessment data, alone or in conjunction with any combination of (a) a limited set of child characteristics (e.g., age, sex, race/ethnicity), (b) any third grade teacher questionnaire A, B or C data, and (c) data from the school administrator questionnaire or school fact sheet."}, {"section_title": "C5PW0", "text": "third grade parent interview data alone or in combination with (a) third grade child assessment data, (b) third grade teacher questionnaire A, B, or C data, and (c) data from the school administrator questionnaire or school fact sheet. Exception: If data from the parent AND child assessment AND teacher questionnaire A or B (not C) are used then C5CPTW0 should be used."}, {"section_title": "C5CPTW0", "text": "third grade direct child assessment data combined with third grade parent interview data AND third grade teacher data alone or in conjunction with data from the school administrator or school fact sheet or facilities checklist. Weight C5CW0 is used to estimate child-level characteristics or assessment scores for third grade. Examples of such estimates are the percent of third grade children who are male, the percent of children who are API, the percent of children who are 9 years old when they begin third grade, and the mean reading score of children in third grade. These weights exist not only for children who were administered a child assessment but also for children who could not be assessed due to a disability. 2 These children were not administered the ECLS-K direct cognitive battery, but their background characteristics such as age, sex, race/ethnicity, and characteristics of their parents, teachers, classrooms, and schools are available from the parent interviews, the teacher questionnaires, the school administrator questionnaire, and the school fact sheet. The academic and social rating scores (see chapter 3) from teachers are also available for children with disabilities, regardless of whether they completed the direct child assessment. When analyzing child assessment data in conjunction with teacher data collected in third grade, weight C5CW0 should be used. An example of the use of C5CW0 is in the analysis of the relationship between children's approaches to learning as rated by their teachers, the teacher's type of teaching certification, and the children's cognitive skills and knowledge. Some data may be missing because some teachers did not complete the questionnaire, but these are the most appropriate weights for 4-20 this type of analysis. However, different weights should be used for analysis of child data in conjunction with both parent and teacher data (C5CPTW0). C5PW0 is used for child-level estimates associated with data collected through the parent interview. Examples are the percent of children whose mothers are currently employed, the percent of children who are in a particular type of child care, and the percent of children who are read to every day. These weights should not be used for estimates solely using direct child assessment data but should be used when analyzing parent and child assessment data together, for example, when exploring the relationship between home literacy behaviors and children's reading skills. C5CPTW0 is used when child direct assessment and teacher and parent data are combined in an analysis; for example, in the analysis of the relationship between parent education, teacher education, and children's reading knowledge and skills. These weights should be not be used for estimates using only direct child assessment data or only parent interview data. Careful consideration should be given to which set of weights is appropriate for the desired analysis. Using the wrong weights will result in more biased or inefficient estimates. For example, if C5CPTW0 were used in an analysis of child and teacher/classroom data only, then the resulting estimates will be inefficient compared to estimates using C5CW0. The lower parent response causes C5CPTW0 to result in lower sample size with positive weights. There may be combinations of data from a different source for which no weights were developed, but most analyses are possible from the weights provided. The distribution of schools by number of sampled students with nonzero third grade weights and the mean number of sampled students with nonzero weights per school are useful in analysis using hierarchical linear modeling. These are given in  "}, {"section_title": "Weighting Procedures", "text": "The third grade sample included all base year respondents as defined earlier and a supplemental sample of first graders brought in through a sample freshening procedure implemented in spring-first grade. Only a subsample of children who moved from the schools they were attending when they were sampled originally was followed into their new schools. However, children who belong to the language minority group and who had not moved out of the original sample schools at anytime during the first grade year were all followed into their new third grade schools if they moved from the original sample school during their third grade year. The weighting procedures were divided into three main stages. The first stage of weighting was to compute an initial child weight that reflects the following: Adjustment of the school base weight for base year school-level nonresponse; Adjustment of the child weights for base year child-level nonresponse; and Adjustment of the base year child weight for subsampling of schools for freshening in first grade (for children sampled in first grade only). The procedures used in this first stage are the same as for the first grade year. They are described again for completeness.\nIn this section we discuss the statistical procedures used to produce the K-3 longitudinal weights. These procedures are nearly identical to the procedures used for the cross-sectional weights (see chapter 4). The differences are primarily in how mover status and eligible respondents are defined, and in how adjustment cells are created. For example, in computing weight C45CW0, a child was identified as a mover if the child moved in either spring-first grade or spring-third grade; a respondent was defined as a child for whom both cross-sectional weights, C4CW0 and C5CW0, are nonzero. A child with a nonzero C45CW0 had both spring-first grade and spring-third grade scorable cognitive assessment data, or was excluded from the cognitive assessments because he or she was a child with disabilities. Longitudinal 9-8 weights involving the fall-first grade collections were computed differently to adjust for the fact that only a subsample of children were included in fall-first grade."}, {"section_title": "4-22", "text": "The second stage of weighting was to adjust the initial child weight computed in the first stage for the following: Subsampling of movers; and Child-level nonresponse. The third and last stage was to rake the weights adjusted in the second stage to sample-based control totals. The computation of the initial child weights is described in section 4.6.3. The subsequent weight adjustments are described in section 4.6.4. Section 4.6.5 describes the different types of weights computed for spring-third grade. In general, in each adjustment to the weight, the adjustment factor is multiplied by the weight in the prior step to get the adjusted weight. This fact is not repeated in the discussions of the weight adjustments in the following sections, only the computation of the adjustment factor is discussed."}, {"section_title": "Computation of Spring-First Grade Initial Child Weights", "text": "As mentioned earlier, the first stage of weighting was to compute an initial child weight that reflects: (1) the adjustment of the school base weight for base year school-level nonresponse (school-level weights), (2) the adjustment of the child weights for base year child-level nonresponse (child-level weights), and (3) the adjustment of the base year child weight for subsampling of schools for freshening in first grade (child-level weights, for children sampled in first grade only). These weights were already computed for spring-first grade. For completeness, they are described below, in section 4.6.3.1 for the school-level weights, and in section 4.6.3.2 for the child-level weights."}, {"section_title": "Base Year Nonresponse-Adjusted School Weights", "text": "This weight is the same as that computed for the first grade data collection. It was computed as the school base weight adjusted for base year school-level nonresponse. The base weight for each school was the inverse of the probability of selecting the PSU (county or group of counties) multiplied by 4-23 the inverse of the probability of selecting the school within the PSU. For schools selected in the base year through the frame freshening procedure, an additional factor equal to the inverse of the selection probability of the district or diocese was included in the base weight. A base year responding school was an original sample school with at least one child with a positive C1CW0, C2CW0, C1PW0, or C2PW0 weight. C1CW0 is positive for LM/not Spanish children, children with disabilities and children with at least one direct cognitive test score in fall-kindergarten. C1PW0 is positive for children whose parents completed the family structure questions of the parent interview in fall-kindergarten. C2CW0 and C2PW0 weights are positive under similar circumstances but for spring-kindergarten. Schools that did not meet this condition are nonrespondents and their weights distributed across responding units (at the school level) in this stage. The base year school weight was adjusted within nonresponse weighting classes created in the base year using the Chi-squared Automatic Interaction Detector (CHAID) and variables with known values for both respondents and nonrespondents. School characteristics used for constructing nonresponse cells were the school type (public, Catholic private, non-Catholic private, or nonsectarian private), the school locale (large city, midsize city, suburb of large city, suburb of midsize city, large town, small town, or rural area), the region where the school is located (Northeast, Midwest, South, or West), and the size classification of the school in terms of school enrollment. Once the weighted nonresponse cells were determined, the nonresponse adjustment factors are the reciprocals of the response rates within the selected nonresponse cells."}, {"section_title": "Base Year Child Weights", "text": "As mentioned earlier, two groups of children were fielded in spring-third grade: base year respondents, and eligible children who were sampled in first grade as part of the sampling freshening procedure. The base year child weights for the two groups were the same as those computed for the first grade year. A description of them follows."}, {"section_title": "Base Year Child Weights for Base Year Respondents", "text": "As previously described, a base year respondent was defined as one with at least one direct cognitive test score in fall-or spring-kindergarten or whose parent responded to the family structure section of the parent instrument in fall-or spring-kindergarten. In terms of weights, a base year 4-24 respondent is a sampled child with a positive fall-or spring-kindergarten weight (i.e., C1CW0, C2CW0, C1PW0 or C2PW0 weights). The base year child weight is the product of the base year nonresponseadjusted school weight and the inverse of the within school selection probability of the child, adjusted for child-level nonresponse. The nonresponse weighting classes included school characteristics from the school nonresponse adjustments such as school type, locale, region, school enrollment class, and child characteristics such as age group, sex, and race/ethnicity. These weighting classes are similar to those used for the child weights in the base year. For a description of the computation of child weights in the base year, see chapter 4, section 4. "}, {"section_title": "Base Year Child Weights for Eligible Children Sampled in First Grade", "text": "Since each child sampled in first grade was directly linked to a child sampled in kindergarten, the first step was to compute a weight for the children who were sampled in kindergarten that reflected the school freshening subsampling and the school freshening nonresponse (some schools refused to provide information needed for freshening). This weight was then linked back to the child sampled in first grade and further adjusted for nonresponse due to not obtaining the data from the sample of freshened children. First the school base year weight adjusted for school nonresponse (as described in section 4.6.3.1) was adjusted for the subsampling of schools for freshening. Student freshening was done in the same 50 percent subsample of schools that were flagged for following movers in spring-first grade. The school freshening subsampling adjustment factor was computed as: 0 if the school was not in the set of schools subsampled for freshening 3 and The sum of base year nonresponse-adjusted school weights for all schools over the sum of base year nonresponse-adjusted school weights for schools subsampled for freshening, if the school was in the set of schools subsampled for freshening. This adjustment was done within cells defined by school type and census region. 3 These weights, used only to link children sampled in first grade to children sampled in kindergarten, sum up to zero in schools not subsampled for freshening, meaning that there are no children sampled in those schools through freshening."}, {"section_title": "4-25", "text": "The freshening procedure could not be applied in all designated schools because some schools did not provide the information needed for freshening. These schools are considered freshening nonrespondents. The school weight adjusted for freshening subsampling was then adjusted for this type of nonresponse. The school freshening nonresponse adjustment factor was calculated as the sum of weights of the freshening-adjusted school weights for all schools designated for freshening over the sum of weights of the freshening-adjusted school weights for schools who responded to freshening. In both the numerator and denominator of this factor, the school measure of size was incorporated; the school measure of size is relevant because the weights will be used for child-level estimates, not school-level estimates. The nonresponse cells for this adjustment were created using school type and urbanicity. Next, the school-adjusted weight was multiplied by the inverse of the within school selection probability of the child in the base year to obtain a base year child weight. The base year child weight was then adjusted for base year child nonresponse because children who did not respond in the base year could not be linked to children in first grade in spring 2000. The adjustment factor was computed as the sum of the base year child weights of all base year children over the sum of the base year child weights of base year respondents within each nonresponse cell. The nonresponse cells were created using school characteristics such as school type, locale, region, school enrollment class, and child characteristics such as age group, sex, and race/ethnicity. Only children who did not move from their original schools were designated as links to children in the freshening procedure. The children who moved and were followed into their new schools were not identified to participate in the freshening process in their new schools. As a result, all the children who moved were considered nonrespondents to the freshening process. Additionally, nonmovers and movers who were not in first grade were not eligible for freshening (e.g., if a child was in kindergarten in spring 2000, he or she would be linked only to other kindergarten children and thus was not eligible for the freshening of first graders). Adjustment was necessary to account for these two groups of children and was done in two steps. In the first step, adjustment was done for movers whose grade was unknown. A portion of the movers was assumed to be in first grade. In the second step, the weights were adjusted for children who were in first grade but who were not identified to participate in the freshening process because they moved into a new school. For this two-step adjustment, each child was classified as a (a) mover in first grade, (b) mover in another grade, (c) mover with unknown grade, (d) nonmover in first grade, and (e) nonmover in another grade."}, {"section_title": "4-26", "text": "The first step adjustment for movers whose grade was unknown was computed as 0, if the child was a mover with unknown-grade (group c); 1, if the child was a nonmover, in first grade or another grade (group d or e); and The sum of the nonresponse-adjusted base year child weights (computed in the step before) of all movers (group a, b, or c) over the sum of the nonresponse-adjusted base year child weights of movers with known grade (group a or b), if the child was a mover with known grade (group a or b). The second step adjustment for movers who could not be used as links for freshening was computed as 0, if the child was a first grade mover (group a); 1 if the child was in a grade other than first grade (group b or e); and The sum of the weights adjusted in step 1 of all first graders (group a or d) over the sum of the weights adjusted in step 1 of nonmovers in first grade (group d), if the child was a nonmover in first grade (group d). This two-step adjustment was done within cells defined by school type and census region. The weights thus created for children sampled in kindergarten were then linked to the children that they brought into the sample in first grade through sample freshening. In other words, the weight of the child sampled in first grade was defined at this point to be the weight computed for the child sampled in kindergarten that was responsible for bringing the first grader into the sample. For the next step in the computation of the spring-first grade child weights, the two groups of children-base year respondents and children sampled in first grade through sample freshening-were put together, and a common variable and label were used to designate the initial child weight. This is the base year child weight as computed above for each group of children."}, {"section_title": "4-27", "text": ""}, {"section_title": "Computation of Spring-Third Grade Child Weights", "text": "The initial child weights described in section 4.6.3 were adjusted for movers between the base year and third grade and nonresponse in third grade, and raked to sampled-based control totals to obtain the final spring-third grade child weights."}, {"section_title": "Adjustment for Movers", "text": "First, the initial child weights were adjusted to reflect the subsampling of movers. In the ECLS-K, a child could move more than once and at different times. For example, a child could move out of his original sample school because the school did not have grades higher than kindergarten. Then he could move again between first and third grade. Once a child was identified as a mover, he stayed a mover (unless he moved back to the original sample school). The spring-first grade follow flags were maintained for all children in the spring-third grade sample except for children whose home language was not English. For these language minority children, their spring-first grade flags were switched to 1 if they were not already equal to 1 and if they had not already been subsampled out because they moved in spring-first grade. Thus, children who moved out of their original sample school were followed in the random 50 percent of schools where the follow flag was set to 1, and language minority children were followed at 100 percent if they had not moved previously. The adjustment factor for subsampling movers was computed as follows: 1, if the child was not a mover; 0, if the child was a mover and the value of the follow flag was 0; and The sum of initial child weights of children who were movers over the sum of initial child weights of children who were movers and whose follow flags have value 1, if the child was a mover whose follow flag has value 1. For the third category, the adjustment factor was computed within cells created using the following characteristics: whether children were sampled in kindergarten or first grade, whether they were movers in spring-first grade, whether they were language minority children, the school type of their original sample school, and the region where their original sample school was located. Seven children with large weights had their weights trimmed by half. However, the weights were not redistributed because the total sum of weights was re-established in the raking procedure that came later."}, {"section_title": "4-28", "text": ""}, {"section_title": "Adjustment for Nonresponse", "text": "After the adjustment for subsampling movers, the child weights were adjusted for nonresponse. As in spring-first grade, the nonresponse adjustment was done in two steps. In the first step, the adjustment was for children whose eligibility was not determined (unknown eligibility). A portion of children of unknown eligibility was assumed to be ineligible. In the second step, the adjustment was for eligible nonrespondents. To carry out these adjustments, each child was classified as (a) an eligible respondent, (b) an eligible nonrespondent, (c) ineligible (out of the country or deceased) or (d) of unknown eligibility (mover who could not be located). The first adjustment factor (for children of unknown eligibility) was computed as 0, if the child was of unknown eligibility (group d) and The sum of the mover adjusted weights of all children (any group) over the sum of the mover adjusted weights of children who were eligible respondents, eligible nonrespondents or ineligible (group a, b or c), if the child was not of unknown eligibility. The second adjustment factor (for eligible nonrespondents) was computed as 0, if the child was an eligible nonrespondent (group b) and The sum of the weights adjusted in the first step of eligible children (group a or b) over the sum of the weights adjusted in the first step of eligible responding children (group a), if the child was an eligible respondent. In both steps of the adjustment, separate nonresponse classes were created for movers and nonmovers using various combinations of response status of child assessments and parent interviews in the base year as well as whether children belong to the language minority group, the type of household collected from the parent interviews (C5PW0 only), and the school type including whether the child was homeschooled (C5CPTW0 only)."}, {"section_title": "Raking to Sample-Based Control Totals", "text": "To reduce the variability due to the subsampling of schools and movers, the child weights were then raked to sample-based control totals computed using the initial child weights computed in Before raking the C5CPTW0 weights, 11 responding movers had their nonresponse-adjusted weights trimmed and the excess weight redistributed among the remaining responding movers so that the sum of weights before trimming was equal to the sum of weights after trimming. The raking factor was computed separately within raking cells as the sample-based control total for the raking cell over the sum of the nonresponse-adjusted weights for children in the same cell. Raking cells (also known as raking dimensions) were created using school and child characteristics collected in the base year or first grade year: school type, region, urbanicity, sex, age, race/ethnicity, SES, language minority status, whether sampled in kindergarten or first grade, and if sampled in kindergarten, mover status."}, {"section_title": "Types of Weights and Their Use", "text": "The different types of cross-sectional weights are described in section 4.6.1 and their use was summarized in exhibit 4-1. They were all created as described in sections 4.6.4.2 and 4.6.4.3, but the definition of which children were eligible respondents varied. The adjustment for movers was done once, then the resulting weights were adjusted for nonresponse separately for C5CW0, C5PW0 and C5CPTW0."}, {"section_title": "Weights To Be Used With Direct Child Assessment Data (C5CW0)", "text": "In spring-third grade, responding children for this type of weight were eligible children who had spring-third grade scorable direct child cognitive assessment data, or children with disabilities who according to specifications in their IEP could not participate in the assessments. A child was eligible if he 4-30 or she was a base year respondent or freshened in first grade. Children who transferred to schools and were not flagged to be followed, who moved out of the country or were deceased were ineligible. In spring-third grade, responding children were classified using rules similar to those used in spring-first grade. Table 4-12 shows the number of children who were not assessed due to the following special situations: children with disabilities, children who moved out of their original sample schools and were not flagged to be followed, children who moved and were flagged to be followed but could not be located or moved into a school in a nonsampled county, and children who moved outside of the country or who were deceased. Of these, only children with disabilities had weights. "}, {"section_title": "Weights To Be Used With Parent Data (C5PW0)", "text": "The weight C5PW0 is to be used with parent interview data. In spring-third grade, a respondent was defined as a child for whom the family structure section (FSQ) in that child's parent interview for the corresponding round was completed. Note that this weight is at the child level even though the data were collected from the parents; they sum to third grade children, not to the parents of third grade children."}, {"section_title": "4-31", "text": ""}, {"section_title": "Weights To Be Used With a Combination of Child Direct Assessment Data and Parent", "text": ""}, {"section_title": "Interview Data and Teacher Data (C5CPTW0)", "text": "The weight C5CPTW0 is to be used for analysis involving child, parent, and teacher data. A respondent for this type of weight was defined as a child who had scorable cognitive assessment data for spring-third grade (or children with disabilities), whose parent completed the FSQ section of the parent interview for spring-third grade, and whose teacher completed part B of the teacher questionnaire."}, {"section_title": "Replicate Weights", "text": "For each weight included in the data file, a set of replicate weights was calculated. Replicate weights are used in the jackknife replication method to estimate the standard errors of survey estimates. All adjustments to the full sample weights were repeated for the replicate weights. For spring-third grade, there are 90 replicate weights. Each set of replicate weights has the same prefix in the variable name as the full sample weight. For example, the replicate weights for C5CW0 are C5CW1 through C5CW90. The method used to compute the replicate weights and how they are used to compute the sampling errors of the estimates are described in section 4.7."}, {"section_title": "Characteristics of Sample Weights", "text": "The statistical characteristics of the sample weights are presented in table 4-13. For each type of weight, the number of cases with nonzero weights is presented together with the mean weight, the standard deviation, the coefficient of variation (i.e., the standard deviation as a percentage of the mean weight), the minimum weight, the maximum weight, the skewness, the kurtosis, and the sum of weights. The difference in the estimate of the population of students (sum of weights) between rounds of data collection and types of weight is due a combination of factors, among them: (1) the number of first graders who became ineligible in third grade (due to death, leaving the country, or being a nonsampled mover), and (2) the adjustment of the weights for the children of unknown eligibility. "}, {"section_title": "4-32", "text": ""}, {"section_title": "Variance Estimation", "text": "The precision of the sample estimates derived from a survey can be evaluated by estimating the variances of these estimates.   The variance estimates of selected survey items presented in section 4.8 were produced using WesVar and JK2. Replicate weights were created to be used in the calculation of variance estimates. Each replicate weight was calculated using the same adjustment steps as the full sample weight but using only the subsample of cases that constitute each replicate. For the original ECLS-K design in the base year, replicate weights were created taking into account the Durbin method of PSU selection. The Durbin method selects two first-stage units per stratum without replacement, with probability proportional to size and a known joint probability of inclusion. In the ECLS-K PSU sample design, there were 24 SR strata and 38 NSR strata. Among the 38 NSR strata, 11 strata were identified as Durbin strata and were treated as SR strata for variance estimation. The purpose of the Durbin strata is to allow variances to be estimated as if the first-stage units were selected with replacement. This brings the number of SR PSUs to 46 (24 original SR PSUs and 22 Durbin PSUs from the 11 Durbin strata). The remaining 54 NSR PSUs are in 27 NSR strata; thus 27 replicates were formed, each corresponding to one NSR stratum. For the SR strata, 63 replicates were formed. The 90 replicates will yield about 76 degrees of freedom for calculating confidence intervals for many survey estimates. As stated earlier, the sample of PSUs was divided into 90 replicates or variance strata. The 27 NSR strata formed 27 variance strata of two PSUs each; each PSU formed a variance unit within a variance stratum. All schools within an NSR PSU were assigned to the same variance unit and variance stratum. Sampled schools in the 46 SR PSUs were grouped into 63 variance strata. In the SR PSUs, schools were directly sampled and constituted PSUs. Public schools were sampled from within PSU while private schools were pooled into one sampling stratum and selected systematically (except in the SR PSUs identified through the Durbin method where private schools were treated as if they were sampled from within PSU). Schools were sorted by sampling stratum, school type (from the original sample or newly selected as part of freshening), type of frame (for new schools only), and their original order of selection (within stratum). From this sorted list, they were grouped into pairs within each sampling 4-34 stratum; the last pair in the stratum may be a triplet if the number of schools in the stratum is odd. This operation resulted in a number of ordered preliminary variance strata of two or three units each. The first ordered 63 strata were then numbered sequentially from 1 to 63; the next ordered 63 strata were also numbered sequentially from 1 to 63, and so on until the list was exhausted, thus forming the desired 63 variance strata. In strata with two units, a unit being a PSU in the case of NSR PSUs and a school in the case of SR PSUs, the base weight of the first unit was doubled to form the replicate weight, while the base weight of the second unit was multiplied by zero. In strata with three units, two variance strata were created: in the first variance stratum, the base weight of two of the three units was multiplied by 1.5 to form the replicate weight and the base weight of the last unit was multiplied by zero; in the second variance stratum, the base weight of a different group of two units was multiplied by 1.5, and the base weight of the third unit was multiplied by zero. Multiplying the base weight in a unit by zero is equivalent to dropping one unit as required by the jackknife method. All adjustments to the full sample weights were repeated for the replicate weights. For each full sample weight, there are 90 replicate weights with the same weight prefix. A child sampled in first grade through the freshening process was assigned to the same replicate as the originally sampled child to whom the child was linked. When the child sampled in first grade was assigned a full sample weight (see section 4.6.3.2), he or she was assigned the replicate weights in the same manner. To reflect the variability of the control totals in the sample-based raking, a set of replicate control totals was created. Each replicate was then raked to the corresponding replicate-based control totals. This resulted in each replicate retaining the variability associated with the original sample estimates of the control totals. The replicate weights can be used with software such as WesVar, SUDAAN and AM.\nFor each K-3 full sample weight listed in exhibit 9-1, a set of replicate weights was calculated. Replicate weights are used in the jackknife replication method to estimate the standard errors of survey estimates. Any adjustments done to the full sample weights were repeated for the replicate weights. For longitudinal weights not involving the fall-first grade data, there are 90 replicate weights. For a description of how the replicates were formed, see chapter 4, section 4.7. For the two longitudinal weights involving fall-first grade (C1_5SC0 and C1_5SP0), there are 40 replicate weights. The smaller number of replicates was due to the fact that only a subsample of schools was included in the fall-first grade sample. The weights associated with the fall-first grade data do not account for the Durbin method of selecting primary sampling units (PSUs), since it no longer applied. Rather, they reflect the fact that only one of the two sampled PSUs in the non-self-representing (NSR) strata was kept in the subsample. To account for this feature, pairs of similar NSR PSUs were collapsed into 19 variance strata. The self-representing (SR) PSUs account for the remaining 21 variance strata. Each replicate weight variable name has the same weight prefix as for the full sample weight variable name. For example, the replicate weights for C1_5FC0 are C1_5FC1 through C1_5FC90; the replicate weights for C1_5SC0 are C1_5SC1 through C1_5SC40. Stratum and first-stage unit identifiers used with the Taylor Series method are provided for each of the K-3 longitudinal weights in the file. They are described in exhibit 9-2. For a description of the Taylor Series method, see chapter 4, section 4.7.2. Specifications for computing standard errors are given in table 9-2. For each type of analysis described in table 9-2, users can choose between the replication method and the Taylor Series method for computing standard errors."}, {"section_title": "Taylor Series Method", "text": "The Taylor Series method produces a linear approximation of the survey estimate of interest; then the variance of the linear approximation can be estimated by standard variance formulas. The stratum  "}, {"section_title": "4-37", "text": "For the replication method, the full sample weight, the replicate weights, and the method of replication are required parameters. All analyses of the ECLS-K data should be done using JK2. As an example, to compute spring-third grade child-level estimates (e.g., mean reading scores) and their standard errors, users need to specify CHILDID in the ID box of the WesVar data file screen, C5CW0 as the full sample weight, C5CW1 to C5CW90 as the replicate weights, and JK2 as the method of replication. For the Taylor Series method using SUDAAN, STATA, SAS or AM, the full sample weight, the sample design, the nesting stratum and PSU variables are required. For the same example above, the full sample weight (C5CW0), the stratum variable (C5TCWSTR), and the PSU variable (C5TCWPSU) must be specified. The \"with replacement\" sample design option, WR, must also be specified if using"}, {"section_title": "SUDAAN.", "text": "The last column in table 4-15 gives the average root design effect that can be used to approximate the standard errors for each type of analysis. For a discussion of the use of design effects, see section 4.8.1."}, {"section_title": "Design Effects", "text": "An \nAn important analytic device is to compare the statistical efficiency of survey estimates with what would have been obtained in a hypothetical and usually impractical simple random sample (SRS) of the same size. For a discussion of design effects and their use, see chapter 4, section 4.8. In this section, design effects are presented for selected illustrative estimates produced using kindergarten-first grade longitudinal weights. The tables that follow show estimates, standard errors, and design effects for selected means and proportions based on the ECLS-K child and parent data. For each survey item, the tables present the number of cases, the estimate, the standard error taking into account the actual sample design (Design SE), the standard error assuming SRS (SRS SE), the root design effect (DEFT), and the design effect (DEFF). Standard errors (Design SE) were produced using JK2. Standard errors and design effects are presented in tables 9-3 to 9-6. Data items are from the direct child assessment, the parent interview, and the teacher child-level questionnaire. Full sample weights were used to compute the estimates; then the corresponding replicate weights were used to compute standard errors and design effects.  9-17 Table 9-4. ECLS-K, spring-kindergarten/spring-first grade/spring-third grade panel: standard errors and design effects using C245CW0-C245CW90 and C245PW0-C245PW90, by selected child and parent variables: School years 1998School years -99, 1999School years -2000School years , and 2001 Survey 9-18 Table 9-4. ECLS-K, spring-kindergarten/spring-first grade/spring-third grade panel: standard errors and design effects using C245CW0-C245CW90 and C245PW0-C245PW90, by selected child and parent variables: School years 1998School years -99, 1999School years -2000School years , and 2001 Table 9-5. ECLS-K, fall-kindergarten/spring-kindergarten/spring-first grade/spring-third grade panel: standard errors and design effects using C1_5FC0-C1_5FC90 and C1_5FP0-C1_5FP90, by selected child and parent variables: School years 1998School years -99, 1999School years -2000School years , and 2001 Survey 9-20 Table 9-5. ECLS-K, fall-kindergarten/spring-kindergarten/spring-first grade/spring-third grade panel: standard errors and design effects using C1_5FC0-C1_5FC90 and C1_5FP0-C1_5FP90, by selected child and parent variables: School years 1998School years -99, 1999School years -2000School years , and 2001 1 Design SE is the standard error under the ECLS-K sample design. For an explanation of this statistic, see chapter 4, section 4.8."}, {"section_title": "4-38", "text": "The root design effect, DEFT, is defined as: where SE is the standard error of the estimate."}, {"section_title": "Use of Design Effects", "text": "Methods of computing standard errors for the ECLS-K are replication and Taylor  and SE SRS can be computed using the formulas below for means and proportions. Means: where w i are the sampling weights, n is the number of respondents in the sample, and the sample mean x w is calculated as follows:"}, {"section_title": "4-39", "text": "Proportions: where p is the weighted estimate of proportion for the characteristic of interest and n is the number of cases in the sample. In both cases of means and proportions, the standard error assuming SRS should be multiplied by DEFT to get the approximate standard error of the estimate under the actual design."}, {"section_title": "Average Design Effects for the ECLS-K", "text": "In the ECLS-K, a large number of data items were collected from students, parents, teachers, and schools. Each item has its own design effect that can be estimated from the survey data. One way to produce design effects for analysts' use is to calculate them for a number of variables and average them. The averaging can be done overall and for selected subgroups. The tables that follow show estimates, standard errors, and design effects for selected means and proportions based on the ECLS-K third grade child, parent, teacher, and school data. For each survey item, the tables present the number of cases for which data are nonmissing, the estimate, the standard error taking into account the actual sample design (Design SE), the standard error assuming SRS (SRS SE), the root design effect (DEFT), and the design effect (DEFF). Standard errors (Design SE) were produced in WesVar using JK2 based on the actual ECLS-K complex design. For each survey estimate, the variable name as it appears in the ECLS-K first grade Electronic Code Book is also provided in the table. For more information on the variables used in this section, refer to chapter 3, which describes the assessment and rating scale scores used in the ECLS-K, and chapter 7, which has a detailed discussion of the other variables. Standard errors and design effects for the child-level items are presented in tables 4-16 and 4-17. The survey items were selected so that there was a mix of items from the direct child assessment, the parent interview, and the teacher child-level questionnaire They include the different scores from the direct child assessment, the social rating scores as provided by teachers, characteristics of the parents, and 4-40 characteristics of the students as reported by the parents and teachers. For a small number of estimates, the data were subset to cases where the estimate is applicable; for example, the proportion of children who have access to the internet is only for children in households with a computer.     "}, {"section_title": "4-42", "text": ""}, {"section_title": "Overview of Data Collection Methods", "text": "The Computer-assisted personal interviewing (CAPI) was the mode of data collection for the child assessments, and telephone and in-person computer-assisted interviewing (CAI) was the mode of data collection for the parent interview; self-administered questionnaires were used to gather information from teachers, school administrators, and student records. The facilities checklist was completed by field staff."}, {"section_title": "Field Staff Training", "text": "Several in-person training sessions were conducted to prepare staff for the third grade data collection. In the fall of 2001, supervisors were trained to contact original schools and recruit transfer schools. In the spring of 2002, three trainings were held: one for staff who only conducted parent interviews, one for field supervisors, and one for assessors. Field supervisors managed all the data collection activities within their assignment, supervising the assessors and interviewers and conducting child assessments and parent interviews. Assessors conducted the child assessments and parent interviews. Interviewers only conducted parent interviews."}, {"section_title": "Advance Contact and Recruitment Training", "text": "In September 2001, field supervisors were trained for 3 days to contact original sampled schools and transfer schools to set up the data collection in the spring. A total of 50 field supervisors and 5-3 2 field managers completed training. Topics included an overview of study activities to date, verifying parent consent procedures, identifying and locating children who moved from the schools they attended in the first grade, identifying the teachers of ECLS-K children and linking them to those children, and exercises on scheduling schools efficiently within a work area. As in the first grade training, advance contact and recruitment training was conducted using the automated Field Management System (FMS). The FMS was used throughout the data collection period to enter information about the sampled children, parents, teachers, and schools and to monitor production on all data collection activities. The field supervisors entered information into the FMS during training presentations, thus acquiring hands-on experience with the FMS and all field procedures prior to beginning data collection. The field supervisors completed role plays and exercises that involved entering information into the FMS."}, {"section_title": "Spring-Third Grade Training", "text": "Field supervisors, interviewers, and assessors were trained for the spring-third grade data collection in three sessions in February and March 2002. Prior to the March in-person training session, supervisors and assessors completed 8 hours of home study training on the study design, field procedures, and computer keyboard skills."}, {"section_title": "Parent Interviewer-Only Training", "text": "Supervisors and staff assigned to complete only parent interviews during the spring data collection attended a 2-day training in February 2002. Trainers presented the content of the parent interview and discussed protocols for interviewing. The interviewers practiced using the CAI system on laptops during interactive lectures and role plays. Supervisors had an additional day of training to practice using the FMS to organize and track production and to discuss management techniques for overseeing their teams of interviewers. Eight (8) supervisors and 66 interviewers completed training. 5-4"}, {"section_title": "Field Supervisor Training", "text": "Field supervisor training preceded the assessor training and lasted for 3 days. The topics covered in the field supervisor training session included reviewing materials from the fall school recruitment, role plays to practice contacting school coordinators, identifying and locating children who moved from their first grade schools, identifying the regular and special education teachers of ECLS-K children and linking them to those children, distributing and following up on teacher questionnaires and school administrator questionnaires, completing the facilities checklist, and conducting quality control observations. Field supervisors were also trained to use the FMS, and the field supervisors entered information into the FMS during training presentations. Seventy-seven (77) field supervisors completed training."}, {"section_title": "Assessor Training", "text": "The assessor training sessions included an overview of study activities to date, interactive lectures based on the direct child assessments and the parent interview, practice parent interviews in pairs using role-play scripts, practice direct child assessments using role-play scripts, direct child assessment precertification exercises on each form of the direct child assessments, techniques for parent refusal avoidance, and strategies for building rapport with children. A major goal of the assessor training was to train field staff in the proper procedures to conduct the direct child assessments. This included following standardized procedures for administration of all assessment items as well as giving children neutral praise with the sampled children. The sessions provided trainees with hands-on experience with all the direct child assessment materials and procedures and the CAI programs prior to data collection. Interactive lectures and role plays were also used to train field staff in administering the parent interviews. Trainees practiced entering information into the CAI system on laptop computers during training presentations on conducting the direct child assessments and parent interview. Assessor training lasted for 5 days; field supervisors were also trained to perform all assessor activities. Two hundred sixty-six 266assessors and 77 field supervisors completed training. 5-5"}, {"section_title": "Certification of the Child Assessors", "text": "In order to ensure that the supervisors and assessors who completed training administered the direct child assessments in a standardized manner, all field staff completed certification exercises. Certification was composed of written exercises on each level form of each of the assessment domains (e.g., the red form of reading, which corresponds to a low difficulty level) and an observation of each trainee administering the assessment to children specifically recruited for the training sessions. Each level form of an assessment domain was reviewed in detail during an interactive lecture. Time was then given to each trainee to review and practice administering it individually. After the individual practice, written exercises were distributed. The written exercises were used to ensure that each trainee understood the coding rules for selected open-ended questions with particularly complex scoring rubrics. Each exercise included certain assessment items from the level form that was just discussed, with an assortment of possible responses. The trainees were instructed to score each response as either correct or incorrect. The exercises were then scored by the co-trainer during the next training session. Trainees who did not achieve a passing score were asked to attend a training session in the evening to review the items. These trainees then re-took the same exercises that they had previously failed to pass.  and 84 percent were required to complete remedial training and an additional certification in the field before beginning assessments. The majority of the trainees (98.2 percent) scored at or above 85 percent on the certification form, with only 1.8 percent scoring between 70 and 84 percent. None of the trainees failed to meet the 70 percent threshold on the assessment certification form. All trainees who needed remedial training were certified qualified to administer the child assessments after they conducted a second assessment on a third grade child who was not part of the ECLS-K sample."}, {"section_title": "Fall Preassessment School Contact", "text": "Beginning in September 2001, all participating ECLS-K schools, i.e., schools that participated in fall or spring of kindergarten or first grade, were contacted by telephone to prepare for the 5-7 spring data collection. When children were identified as transferring to another school, the child's new school (and district, if necessary) was recruited."}, {"section_title": "Advance Mailings", "text": "In September 2001, an advance package was mailed via Federal Express to all participating ECLS-K schools asking them to prepare for the preassessment contact telephone call. The schools were asked to identify a school staff coordinator to serve as a liaison with the study (in returning schools, this person was usually the coordinator from previous rounds of data collection). The advance package contained study findings from first grade and an overview of third grade data collection activities. The school coordinators were asked to complete an information form about the ECLS-K sampled children prior to the telephone call."}, {"section_title": "Preassessment Contact", "text": "The preassessment contact was made by telephone between September and November 2001. The preassessment school contact was successful in meeting two important goals: (1) contacting original sampled schools to set up the spring assessment and (2) identifying children who withdrew from their spring-first grade school. Schools were determined to be ineligible for third grade data collection if no sampled children were currently enrolled. Original sampled schools became ineligible if second grade was the highest grade in the school or if the school had closed, that is, was no longer operational. More transfer schools were determined to be ineligible as children transferred out of them into other schools. During the preassessment contact, the field supervisor contacted the school coordinator to schedule the dates of the assessment visit for original sampled schools, identified ECLS-K sampled children who were no longer enrolled at the school, collected locating information for those children, identified each enrolled child's regular and special education teacher, reviewed parental consent status, obtained information on special accommodations 1 during assessment for the enrolled sampled children, and answered any questions the school coordinator may have had. 5-8"}, {"section_title": "Identifying ECLS-K Sampled Children Who Withdrew From the School", "text": "Field supervisors asked the school coordinators to identify ECLS-K children who transferred out of the school. If the school records indicated where the children had transferred, then the field supervisors asked the school coordinator to provide the names, addresses, and telephone numbers of these transfer schools. Of those children who transferred, only a subset were followed to their new school (see section 4.4.1 in chapter 4 for more detail on how mover children were subsampled). If the new school belonged to a district that was new to the study, the district was contacted and recruited before any contact was made with the school. If the district was already cooperating, the new school was contacted and recruited directly."}, {"section_title": "Reviewing Information About ECLS-K Sampled Children", "text": "Field supervisors collected information from the school coordinators about the ECLS-K sample children still enrolled in the school, including the child's current grade, the name and classroom of the child's regular teacher, and whether or not the child had an Individualized Education Plan (IEP). If the child had an IEP, then the name and classroom of the child's special education teacher were collected, along with whether the child required any accommodations to participate in the direct cognitive assessment. The accommodations to the third grade direct cognitive assessment were the same as those for the kindergarten and first grade direct cognitive assessments. Field supervisors contacted the teachers of the ECLS-K children as necessary for any of this information."}, {"section_title": "Reviewing Parent Consent", "text": "Although parental consent was obtained in the base year (and, in some schools, in the first grade year), field supervisors reviewed the parental consent with the school coordinator to determine if the base year or first grade consent was acceptable for third grade. If the schools required consent to be re-obtained or changed the type of consent that was required (e.g., from implicit to explicit), parent letters and consent forms were mailed either to the school for distribution to parents or directly to parents from Westat, based on the schools' preference. Parents were requested to return signed consent forms to the school coordinator. 5-9"}, {"section_title": "Contacting Families of Home-Schooled Children", "text": "As part of the advance school contact, children who were home schooled in previous rounds were identified. The status of home-schooled children who were identified in round 1 through 4 was verified with their parents and updated as necessary. In addition, some home-schooled children were identified by the schools during the preassessment contact. Their status was also verified with their parents during data collection. Parents of these children were contacted in September through November 2001 to determine if the child was still home schooled or had enrolled in a school. If the child had enrolled in a school, the new school was contacted and recruited into the study. Parents of children who were still schooled at home were notified about the next round of data collection in the spring."}, {"section_title": "Preparing for Spring-Third Grade Data Collection", "text": "In order to ensure that as many of the sampled children as possible were contacted in the spring, locating efforts were undertaken in the winter of 2001. Staff in Westat's Telephone Research Center (TRC) traced children who could not be located during the preassessment school contact phase. TRC staff also used the Internet, telephone directories, and other means to locate these children and their households. When children and/or households were found, the new school and contacting information was entered into the computer database, for fielding in the spring. A mailing to the post office requesting change of address information for sampled households was also conducted in the winter of 2001."}, {"section_title": "Spring-Third Grade Data Collection", "text": "All children who were assessed during the base year or for whom a parent interview was completed in the base year were eligible to be assessed in the spring-third grade data collection. Eligibility for the study was not dependent on the child's current grade, that is, children were eligible whether they were promoted to third grade or were retained in second grade. \"Unlocatable\" means that the children and their households could not be found using the available tracing and locating strategies; \"out of scope\" means that a child was ineligible to participate; \"final refusal\" means that the child's family indicated that they did not want to participate; \"partially located\" means that the tracing and locating effort yielded some information about the child, but not enough to definitively locate the child; \"unable to locate due to language barriers\" means that the household language was not English and no staff were available who were bilingual in that language. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 third grade data collection, school year 2001-02."}, {"section_title": "5-10", "text": "As in previous rounds of data collection, the field staff were organized into work areas, each with a data collection team consisting of one field supervisor and three or more assessors. The data collection teams were responsible for all data collection activities in their work areas; they conducted the direct child assessments and the parent interviews, distributed and collected all school and teacher questionnaires, and completed school facilities checklists. The majority of field staff members in third grade were continuing from previous rounds of data collection; a few new staff were hired in areas where no experienced ECLS-K staff lived."}, {"section_title": "Preassessment School Contact", "text": "Based on the information collected in the fall of 2001, packets of hard-copy teacher and school administrator questionnaires were assembled and mailed to schools in February 2002, along with letters confirming the scheduled visits to the school. Teachers and school administrators were asked to either complete the questionnaires for pickup on assessment day, or to return the questionnaires in a Federal Express mailer that was provided in the packet. Letters were also mailed to parents reminding them of the spring-third grade data collection activities."}, {"section_title": "5-11", "text": "Field supervisors conducted most preassessment activities by telephone starting in March 2002. The preassessment activities for these schools were similar to those conducted in previous rounds of data collection and included confirming the assessment date and the receipt of the hard-copy questionnaires and arranging for space to conduct the assessments."}, {"section_title": "Timeline of the Direct Child Assessments", "text": "The direct child assessments were conducted from March through June 2002, the same time of year as in prior spring data collections. Conducting the child assessments began in March with 91 percent of the assessments completed between April and May and a small percentage (9 percent) completed in June. In year-round schools, assessment teams made multiple visits to the school, visiting when each track was in session to assess the sampled children."}, {"section_title": "Conducting the Direct Child Assessments", "text": "The direct child assessments were usually conducted in a school classroom or library. Before conducting the assessments, field supervisors and assessors set up the room for the assessments. They followed procedures for meeting children that were agreed upon during the preassessment contact with the school. Each child was signed out of his or her classroom prior to the assessments and signed back into the classroom upon the conclusion of the assessments. When scheduling schools in the fall, an attempt was made to conduct the direct child assessments at about the same point in time from the beginning of school year and at the end of the year to increase the chances that exposure to instruction was about the same for all children. The third grade direct child assessments averaged 94 minutes. Table 5-3 displays the number of completed child assessments for each round of data collection, including spring-third grade. All of the assessments in spring-third grade were completed in English. Most (74.6 percent) of these assessments were completed in original schools, although the number of assessments in transfer schools has grown at each data collection point. In spring-third grade, a quarter of the sample was assessed in transfer schools. 25.4 \u2020(a) Not applicable. The assessment was conducted only in English in third grade. \u2020(b) Not applicable. There were no transfer schools in fall-kindergarten. 1 The term accommodation in this table is the field operational definition of accommodation, which includes the wearing of glasses and hearing aids. These types of aids were systematically tracked to ensure that every child had the same chance at a successful assessment. With this information, assessors could prompt a child, for example, to get her glasses before being assessed. NOTE: This table reflects final production numbers prior to statistical adjustment. This table does not include children who were subsampled out in fall-and spring-first grade and spring-third grade (see section 5.4.4.) These numbers should not be used to estimate student mobility. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 kindergarten, first, and third grade data collections, school years 1998-99, 1999-2000, and 2001-02."}, {"section_title": "5-12", "text": ""}, {"section_title": "5-13", "text": ""}, {"section_title": "Accommodations and Exclusions", "text": "Approximately 1 percent of participating children in third grade required accommodations or were excluded from the direct child assessments. Children were excluded from the direct assessments because of a disability, e.g., blindness or deafness, that was not accommodated by the ECLS-K direct assessments or their Individualized Education Plan prevented their participation in assessments or required an accommodation not offered in the assessments. Accommodations offered in the assessments were as follows: alternative setting, scheduling, or timing; health care aide present; or the use of an assistive device. Table 5-4 presents the number of children excluded from and requiring an accommodation to the direct child assessment procedures in the spring of third grade. "}, {"section_title": "Conducting the Parent Interview", "text": "Parent interview procedures mirrored those of the base year and first grade. The parent interview was administered, primarily by telephone interview using CAI, between March and July 2002."}, {"section_title": "5-14", "text": "Sixteen percent of the parent interviews were completed in March, 54 percent were completed in April and May, and 30 percent were completed in June or later. The parent interview averaged 62 minutes. As in previous rounds of data collection, the parent interview was conducted in person if the respondent did not have a telephone. "}, {"section_title": "Conducting Data Collection on Children Who Withdrew From Their Previous Round", "text": ""}, {"section_title": "School", "text": "While contacting schools, field supervisors asked school coordinators to identify children who had withdrawn from the school since the spring of first grade. School staff were asked whether they knew the name and address of the school to which the child transferred, as well as any new information about the child's household address. For the children who had moved from their spring-first grade school and were not part of the sample to be followed, information was collected only from the school personnel and not parents. For children who had withdrawn from their spring-first grade school and were identified to be followed (i.e., were part of the sample of movers), supervisors also consulted parents and other contacts for information on the children's new school. This information was entered into the FMS and processed at Westat for data collection. Table 5-6 presents the status of the children who were identified as movers in third grade; 9,889 children were identified as having transferred from the school in which they were enrolled during the spring of first grade. Of the 9,889 children who moved in spring-third grade, 5,668 were in scope, i.e., children selected to be followed, and followed (57.3 percent of total movers). The remaining 4,221 mover children were out of scope and were not followed; no child assessments or parent interview was conducted for these children.  2 <1 In-scope and followed 1 5,668 57.3 Completed assessment 3 3,682 65. Different data collection strategies were followed for children who moved, depending on where they moved to and the status of their new school. Data collection was attempted for children who moved and were flagged as \"follow\" in spring-third grade in the following ways:"}, {"section_title": "5-15", "text": ""}, {"section_title": "5-16", "text": "Data collected for children moving into cooperating base year sampled schools included the child assessments in the school, school administrator questionnaire, regular and/or special education teacher questionnaires, facilities checklist, and student record abstract forms. Data collected for children moving into nonsampled schools in base year cooperating districts included the child assessments in the school, school administrator questionnaires, regular and/or special education teacher questionnaires, and student record abstract forms, if school permission was obtained. If school permission was not obtained, the assessments were conducted in the home and no school or teacher data were collected. Parent interviews were attempted for all children. For children moving into transfer schools that refused, schools in sampled districts that refused, or originally sampled schools that were ineligible when sampled because they did not have kindergarten classes, the direct child assessments were conducted in the home. No school or teacher data were collected. Parent interviews were attempted for all children."}, {"section_title": "5-17", "text": "For children moving into schools in nonsampled districts or dioceses: -If the school was within the primary sampling unit (PSU), data collected included the child assessments in the school, school administrator questionnaire, regular and/or special education teacher questionnaires, facilities checklist, and student record abstract forms, if school permission was obtained. If school permission was not obtained, the assessments were conducted in the home and no school or teacher data were collected. Parent interviews were attempted for all children. -If the school was outside the PSU, no child, school, or teacher data were collected. The parent interview was still attempted. For children who were not enrolled in school in the spring (including children who were home schooled), data collected included the child assessments in the home if the child was in the sampled PSU. If the child was outside the sampled PSU, no child assessment or school or teacher data were collected. Parent interviews were attempted for all children. Of the children who were identified as movers in third grade and who were selected to be followed, 15.4 percent moved into a school outside the PSU, and 10.7 percent could not be located. Assessments were completed for 65 percent of the movers who were followed in the spring-third grade data collection, and parent interviews were completed for 68 percent of these children."}, {"section_title": "Teacher and School Data Collection", "text": "Data were collected from school administrators, regular classroom teachers, and special education teachers from March through June 2002. The school and teacher questionnaires were mailed to the school coordinators in February 2002. Using the child-teacher linkage information collected in the fall, a packet of questionnaires was assembled for each regular and special education teacher. The regular teacher packet included a cover letter, a sheet explaining the study and its goals, and teacher questionnaire part A, teacher questionnaire part B, and teacher questionnaire part C for each student who had been linked to the teacher in the fall. The special education teacher packet contained a cover letter and summary sheet, special education teacher questionnaire part A, and special education teacher questionnaire part B for each sampled student linked to the teacher. Packets were bundled together by school and mailed to the school coordinator for distribution. If the school and/or teacher and school administrator were not identified in the fall advance"}, {"section_title": "5-18", "text": "contact, then the supervisor gathered the relevant information during the preassessment call in the spring and mailed the packets at that time. Teachers were asked to complete individual ratings for the sampled children in their classrooms, and they were paid $7 for each child rating (teacher questionnaire part C) they completed. In addition, school staff were asked to complete a student record abstract after the school year closed and were paid $7 for every student record abstract completed. Field supervisors also completed a facilities checklist for each sampled school. During the field period, field supervisors followed up with school administrators and teachers by telephone and visits to the schools to ensure that questionnaires were not missing critical information and that completed questionnaires were mailed to Westat. To improve response rates, in early September 2002 a package was mailed to all schools with outstanding school fact sheets or student record abstracts with a request to complete and return questionnaires. School staff were prompted by telephone for the return of the questionnaires and abstracts through October 2002. The hard-copy followup increased child-level responses rates for the school fact sheet by 10 percent and the student record abstract by 12 percent."}, {"section_title": "Data Collection Quality Control", "text": "Continuous quality assurance procedures were employed during all data collection activities, but with a particular focus on the assessments. The procedures were incorporated throughout all stages of the study (e.g., during instrument development, in the staff training program, through assessment certification, and as part of the ongoing staff observations and evaluation activities). Data collection quality control efforts began with the additional development and testing of redesigned sections of the CAI/CAPI applications and the FMS. As sections of these applications were reprogrammed, extensive testing of the entire system was conducted to verify that the systems were working properly from all perspectives. This testing included review by project design staff, statistical staff, and the programmers themselves. Quality control processes continued with the development of field procedures that maximized cooperation and thereby reduced the potential for nonresponse bias."}, {"section_title": "5-19", "text": "Quality control activities continued during training and data collection. During assessor training, field staff practiced conducting the parent interview in pairs and practiced the direct child assessments with third grade-aged children brought to the training site for this purpose. The supervisors and assessors were certified on the child assessments using the Training Certification Form. When the fieldwork began, field supervisors observed each assessor conducting child assessments and made telephone calls to parents to validate the interview. Field managers made telephone calls to the schools to collect information on the school activities for validation purposes."}, {"section_title": "Child Assessments Observations", "text": "Field supervisors conducted on-site observations of the child assessments and completed the child observation form. Our quality control plan called for conducting two observations for each of the 266 assessors who completed training. The first observation was to be within 2 weeks of the start of the assessments, and the second observation within 3 weeks of the first observation. These procedures were followed for the majority of assessors (over 80 percent), but some assessors were observed only once due to the school year ending or to the travel distance involved. A standardized observation form was used to evaluate the assessor's performance in conducting the child assessments. The assessor was rated in three areas: 1. Rapport building and working with the child-use of neutral praise and the assessor's response to various child behaviors."}, {"section_title": "2.", "text": "Cognitive assessment activities-reading questions verbatim, the use of acceptable probes, the use of appropriate hand motions, and the absence of coaching.\nSelect the Settings menu and then the Control Panel folder icon.\nClick on a command from the dropdown list. The Menu Bar may also be activated and its options selected using the shortcut keys described in section 8.2.7.\nClick on the Go button.\nIn the Search Text Box (shown in exhibit 8-21), type in \"edu\" and then click on the Search button. \nRun the program generated after extraction to create a base year data set (DATA1)."}, {"section_title": "3.", "text": "Specific assessment activities-correctly coding answers to open-ended questions in the assessments and following administration procedures. The field supervisors recorded their observations on the form and then reviewed the form with the assessor. The most frequent problems observed were not reading the items verbatim and inappropriate gesturing. Feedback was provided to the assessors on the strengths and weaknesses of their performance and, when necessary, remedial training was provided in areas of weakness. Table 5-7 presents the result of the observations. \nIn the Control Panel window, click on the Display icon.\nThe Variable List will then scroll down automatically to show the selected variable.\nWhat family background characteristics (e.g., family poverty, parent education, maternal employment) affect children's later school outcomes?\nUsing the child catalog from the First Grade Restricted-Use ECB, select the variables to be analyzed and the variable CHILDID."}, {"section_title": "5-20", "text": ""}, {"section_title": "Validation of Parent Interviews", "text": "Approximately 10 percent of the completed parent interviews were called back by a field supervisor (i.e., validated). The first parent interview completed by an assessor was always validated. Over the course of the field period, a running count of an assessor's completed parent interviews was maintained, and each tenth completed parent interview was selected for validation, thus ensuring that 10 percent of each assessor's cases were selected for validation. The parent validation was approximately 5 minutes long and was conducted by telephone. Field supervisors used a standardized parent validation script to make validation calls to parents. The script covered the following topics: Verification of the child's name, date of birth, and sex; and Eight to ten questions from the parent interview that were re-asked of the parent."}, {"section_title": "Validations of School Visits", "text": "To ensure that assessments proceeded smoothly, a validation call was completed with the school principal in at least two of each supervisor's assigned schools in the spring-third grade data collection."}, {"section_title": "5-21", "text": "Field managers conducted the school validations by telephone. The first school that each team completed was called to ascertain how well the preassessment and assessment activities went. If the feedback from the school was positive, the fifth school that each team completed was called. If any problems were indicated in the first validation call, immediate action was taken with the field supervisor. The validation feedback was discussed with the supervisor and remedial action was taken, including inperson observation of the supervisor's next school, if necessary. In spring-third grade, a total of 155 school visits were validated with no negative reports of the assessment team or study made by school staff. Field managers used a standardized script to call the school principals. The script covered the following topics: An overall rating of how the assessments went; Feedback about the study from the children and teachers; Suggestions for improving procedures and making it easier for a school to participate and; General comments and suggestions."}, {"section_title": "Assessor Interrater Reliability", "text": "As part of the child assessments observation described in section 5.5.1, field supervisors completed an assessment certification form for each observation they conducted. An important element of this form was the \"validation items.\" With the exception of the reading routing test, all of the assessments included at least one item that both the observer and the assessor scored. These items had open-ended responses that called for interpretation on the part of the assessor to determine whether a child's response was correct. By comparing the extent to which assessors and observers agreed on scoring these validation items, a measure of interrater reliability was obtained. Interrater reliability provided a measure of the accuracy of the assessor's scoring compared with the standard, the observer's.  the lowest percent agreement (87 percent). The reading yellow level path received a relatively large number of observations (262) and also contained a relatively large number of validation items (5) compared with some of the other paths. Thus, there was greater opportunity for disagreement on this path compared with the others. The science yellow level (the medium science level) also had a relatively higher opportunity for disagreement (228 observations and 4 validation codes) and it, too, exhibited a somewhat lower interrater reliability (89 percent) compared with some of the other paths. The reliability, however, even on these more difficult paths, was high and demonstrated that the assessors accurately coded open-ended items. More details on the instruments can be found in the Early Childhood "}, {"section_title": "5-23", "text": ""}, {"section_title": "Spring-Third Grade Completion Rates", "text": "In the sections that follow, spring-third grade completion rates are presented for three groups of students: (1) students sampled in kindergarten, (2) students sampled in first grade through the freshening procedure, and (3) both groups combined. Completion rates were computed with the same procedures used for spring-first grade to allow for comparisons of completion rates between the two years of data collection following the base year. For spring-first grade and spring-third grade, the sample of children is the same: base year respondents (i.e., children who had either a fall-or spring-kindergarten child assessments or parent interview) and children sampled in spring-first grade as part of sample freshening as described in section 4.3.2."}, {"section_title": "Students Sampled in Kindergarten", "text": "Tables 5-9 to 5-11 present weighted and unweighted child-level completion rates for springthird grade data collection, broken out by school characteristics. These rates pertain to children who were sampled as part of the kindergarten cohort in the base year. (Rates for students sampled in first grade through the student sample freshening procedure can be found in table 5-19.) For the ECLS-K, a completion rate is a response rate conditioned on the results of an earlier stage of data collection. For the group of children sampled in kindergarten, all completion rates are conditioned on the case having been a base year respondent. Relative to spring-first grade, the overall completion rates for the child assessments (80.8 percent) and the parent interview (77.8 percent) both decreased 7 percentage points in spring-third grade. The decrease is almost completely due to the increase in the number of children who moved outside of the sampled PSUs or moved within the sampled PSUs but could not be located (the numbers slightly more than doubled in both cases). These children are included in the category labeled \"Unknown\" for each of the different school characteristics (tables 5-9 and 5-11). The category includes children who were unlocatable as their whereabouts were unknown, and those children who had moved into a nonsampled county. If no information concerning the child's school was available, they were included in the \"Unknown\" category. The completion rates for the child assessments are quite high and uniform across school characteristics (ranging from 97.1 percent in schools with 750 or more enrolled to 100 percent in 5-24 1 Based on ECLS-K survey data and not data from the sampling frame. 2 Reading, math, or science assessment was scorable or child was disabled. 3 Family structure portion of parent interview was completed. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 third grade data collection, school year 2001-02.  "}, {"section_title": "5-26", "text": ""}, {"section_title": "5-28", "text": ""}, {"section_title": "5-30", "text": "schools in large towns), except for the \"unknown\" category. Similarly, the completion rates for the parent interviews were uniform across school characteristics (ranging from 80.9 percent for children in large cities to 93.4 percent for children in large towns), except for the \"unknown\" category. The \"unknown\" category aside, both the child assessments and the parent interview completion rates increased between spring-first grade and spring-third grade for all school characteristics. The completion rates by mover status are discussed later, but the rates of completing all the instruments are much lower for children who moved than for those who did not move. Table 5-10 shows that the overall weighted completion rate is 66.1 percent for the school administrator questionnaire and 76.5 percent for the school fact sheet. The completion rate for the school fact sheet is about 10 percent higher than that for the school administrator questionnaire due to the continued data collection in fall 2002 that affected the school fact sheet and the student record abstract. The completion rate for the school administrator questionnaire is about 10 percentage points lower than the corresponding rate in spring-first grade. Note that there was no school fact sheet in spring-first grade. Once again, the increase in the movers is largely responsible for the lower rates, as discussed below. The completion rates for the school administrator questionnaire range from 69.0 percent for children in large cities to 99.4 percent for those in large towns (ignoring the \"unknown\" category). Rates for the school fact sheet follow the same pattern. Excluding the movers, the completion rate for the school administrator questionnaire is much higher, with an overall rate of 86.7 percent, only slightly lower than the spring-first grade rate of 88.7 percent. In the case of the school fact sheet, the rate for nonmovers is 95.8 percent. It is worth noting that the completion rates for the school administrator questionnaire are lower for schools with higher percentages of minorities, a phenomenon also observed in previous rounds for the school administrator questionnaire. However, this disparity decreased considerably in the first-grade year and in spring-third grade compared to the base year, reflecting the success of increased data collection efforts targeted toward these schools. All three of the teacher questionnaires were completed at an overall rate of 62 to 63 percent. The completion rates have substantial variation when broken out by school characteristics, even when the \"unknown\" category is ignored. The completion rates are 90 percent or more for Catholic schools and for schools in large and small towns. Schools in large cities, schools with 750 students or more, and schools with 90 percent or more minority enrollment have completion rates in the 60s. The \"unknown\" categories have, by far, the lowest completion rates."}, {"section_title": "5-31", "text": "As noted above, the rate at which the survey instruments were completed varies markedly by mover status and within movers, by whether or not the child was located and followed. As shown in table 5-12 the completion rate for the child assessments was 94.5 percent for children still enrolled in their base year school. For movers it dropped by close to 9 points to 85.6 percent for those who were located and followed, and for those not located or followed due to a move to a non-ECLS-K PSU, it was zero. The parent interview completion rates varied from 84.5 percent for nonmovers to 74.8 percent for movers who were located and followed for the purposes of the child assessments, to 51.2 percent for movers who could either not be located or were not followed for the purposes of the child assessments. Even though children who had moved to a non-ECLS-K PSU were not administered the child assessments, a parent interview was conducted by telephone wherever possible, leading to the 51.2 percent response rate for this category. Table 5-13 shows that the school administrator questionnaire completion rate is about 30 points lower for movers compared to nonmovers, even when the children who had moved were located and followed. For the school fact sheet, it is about 20 points lower for movers than for nonmovers. There are several reasons for this difference: located movers were not always assessed in schools; new schools in which movers enrolled had a lower level of commitment to the ECLS-K and often refused to complete the school administrator questionnaire; and some of these schools were contacted too late in the school year for them to consider completing it. The completion rate for nonmovers was 86.7 percent for the school administrator questionnaire and 95.8 percent for the school fact sheet. For located and followed movers it was 56.0 and 74.5 percent for the school administrator questionnaire and for the school fact sheet, respectively. For all three teacher questionnaires the completion rates were approximately 82 percent if the child had not moved; about 54 percent if the child moved, was located, and followed; and just about 0 percent if not located or followed (table [5][6][7][8][9][10][11][12][13][14]. A handful of children who could not be located but had teacher data was due to the fact that, if they move during the term, and teachers in their old schools already filled out the questionnaires but teachers in the new schools did not, the teacher data from the old schools were attached to these children. The reasons for lower completion rates from teachers if the child moved are similar to the reasons that affected the school administrator questionnaire and school fact sheet completion rates for movers.  Tables 5-15 to 5-17 present child-level completion rates for the spring-third grade data collection, this time broken out by child characteristics for children who were sampled as part of the kindergarten cohort in the base year. When the \"unknown\" categories are not included, the differences in completion rates by sex and by year of birth are very small, but for race and ethnicity they are more substantial. For the child assessments the completion rate was highest for Asians and Pacific Islanders (84.1 percent and 84.9 percent, respectively) and lowest for American Indians or Alaska Natives (75.5 percent). For the parent interview it was highest for Whites (82.9 percent), and lowest for Blacks (67.0 percent) and Asian students (68.6 percent). The ECLS-K sample of Pacific Islanders is very clustered and has unusually high completion rates for the instruments filled out by school personnel, 80.9 percent for the school administrator questionnaire, 84.6 percent for the school fact sheet, and about 74 percent for each of the teacher questionnaires. The lowest completion rate for the school administrator questionnaire is for Blacks (57.1 percent); for the school fact sheet it is for American Indians or Alaska Natives (66.0 percent). For the teacher questionnaires the lowest rates are in the 53 to 55 percent range and are associated with Blacks, Hispanics, and American Indians or Alaska Natives. Since 60 percent of the Black and Hispanic students fielded in spring-third grade are enrolled in high minority schools (50 percent or higher), this may be associated with lower levels of response for the school administrator questionnaire from high minority schools. Of the 32 percent of Black and Hispanic students with no school administrator questionnaire data, roughly half are enrolled in high minority schools."}, {"section_title": "5-32", "text": "In addition to the child assessments, parent interview, teacher questionnaires, school administrator questionnaires and school fact sheets whose completion rates have been summarized in the preceding tables, various other types of data were collected during spring-third grade as well. Table 5-18 presents counts of completes and weighted and unweighted completion rates at the overall student level for these other data collection efforts. The facilities checklist has a 77.5 percent completion rate, which is about 11 points higher than that for the school administrator questionnaire but only 1 point higher than the rate for the school fact sheet, the two other school-level survey instruments. The student record abstract, which was to have been completed for all students except for those who moved and could not be located, has a 67.0 percent completion rate. Data were also collected during spring-third grade from the special education teachers for children in special education programs (fewer than 1,200). The completion rates for these instruments are higher than for the regular teacher questionnaires, 73.0 percent for part A, which captures teacher information, and 72.8 percent for part B, which relates to individual students who receive special education services. 1 Based on ECLS-K survey data and not on data from the sampling frame. 2 Reading, math, or science assessment was scorable or child was disabled. 3 Family structure portion of parent interview was completed. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 third grade data collection, school year 2001-02.   "}, {"section_title": "5-36", "text": ""}, {"section_title": "5-37", "text": ""}, {"section_title": "5-38", "text": ""}, {"section_title": "Students Sampled in First Grade", "text": "In spring-first grade, the student sample was freshened to include first graders who had no chance of selection in the base year because they did not attend kindergarten in the United States or were in first grade in the fall of 1998. (For a detailed description of the freshening procedure see section 4.3.2.) This same group of students was followed into spring-third grade. Nonresponse in the freshened student sample could occur at two stages: during the procedure for sampling schools for freshening and identifying children to be used as freshening links in spring-first grade (first component) and then during data collection from the freshened students in spring-third grade (second component). The first component alone can further be decomposed into two sources: attrition due to entire schools refusing to implement the freshening procedure (the school term), and attrition due to ECLS-K sampled children moving to other schools (the child term). To contain costs, students who transferred from schools targeted for freshening were not used as links to identify freshened students, even when they were otherwise followed for data collection. These movers were considered freshening nonrespondents in the child term.  The second component varies by survey instrument. The rates for the paper-and-pencil instruments range from 50.9 percent for the child-level teacher questionnaire to 80.3 percent for the facilities checklist and are uniformly lower than for the kindergarten sample. The child assessments at 78.3 percent are 3 points lower than for the kindergarten sample and the parent interview, at 63.7 percent, is 14 points lower. The final completion rate for each instrument is the product of the two components. Because of the low rates at the first stage, these range from a high of 51.7 percent for the facilities checklist to a low of 32.7 percent for the child-level teacher questionnaire. Table 5-20 presents final spring-third grade completion rates for children sampled in kindergarten, children sampled in first grade, and all children combined. Because children sampled in first grade represent such a small fraction of the total population of children, their inclusion in the computation of the completion rate brings down the rates for all children by less than one percent relative to the children sampled in kindergarten rates, even though the completion rates for children sampled in first grade are lower than the kindergarten rates. 1 Reading, math, or science assessment was scorable or child was disabled. 2 Family structure portion of parent interview was completed. 3 A completed questionnaire was defined as one that was not completely left blank. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 third grade data collection, school year 2001-02."}, {"section_title": "Spring-Third Grade Completion Rates-All Children", "text": ""}, {"section_title": "5-42", "text": "5-43 Table 5-21 shows the completion rates for the child assessments, the parent interviews, and the school and teacher instruments for children who have nonzero child weights (C5CW0>0). These are children whose spring-third grade reading, math, or science assessments were scorable, or children who could not be assessed because of disabilities. For these children, the completion rate for the child assessments should be 100 percent. The less than 100 percent rate shown when children sampled in kindergarten are combined with children sampled in first grade is due to the school freshening nonresponse for children sampled in first grade."}, {"section_title": "Spring-Third Grade Completion Rates Conditioned on Child Assessments", "text": "When the completion rates are conditioned on the presence of the child weight (i.e., completion rates for children with C5CW0>0), they are at least 9 points higher than the unconditioned completion rates for all instruments but the special education questionnaires. For these last two instruments, the difference between the number of completes for the conditioned and unconditioned rates is very small; hence the conditioned rates are not affected as much as for the other instruments. For all the other instruments, the conditioned completion rates are higher by 9 points for the parent interviews and as high as 17 points for the student record abstract. These numbers are the differences between the unconditioned rates (table [5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20] and conditioned rates (table 5-21), hence not shown in any table. Since data were collected from schools, parents, teachers, and children, there were many opportunities for sources to contribute differentially to nonresponse, and this is reflected in the varying completion rates in the tables in this section. These completion rates differ not only by survey instruments, but within each survey instrument they are also different by school and child characters. A separate report examines the potential for bias in estimates produced from the ECLS-K third grade data. Since analysis of the third grade data is conditioned on the base year-only base year respondents were included in the collection of first grade and third grade data-the analysis of nonresponse bias is built on the base year nonresponse bias analysis (see  "}, {"section_title": "5-44", "text": ""}, {"section_title": "DATA PREPARATION", "text": "As described in chapter 5, two types of data collection instruments were used for the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K) data collection in the springthird grade: computer-assisted and self-administered paper forms (hard copy). The data preparation approach differed with the mode of data collection. The direct child assessments and parent interview were conducted using computer-assisted interviewing (CAI) techniques. Editing specifications were built into the computer programs used by assessors to collect these data. The teacher and school administrator forms were self-administered. When the field supervisors returned these forms, coders recorded the receipt of these forms into a project-specific forms tracking system. Coders reviewed the questionnaires to ensure data readability for transfer into an electronic format. The visual review included changing (upcoding) any \"Other, specify\" responses that actually fit within the available response categories of the question. There were some items for which upcoding was conducted after the data were keyed due to the large volume of \"Other\" responses. Once they finished this review, the coders sent the instruments to data entry to be manually transferred to an electronic format and reviewed for range and logic consistency. The following sections describe the data preparation activities for both modes of data collection in more detail."}, {"section_title": "Coding and Editing Specifications for Computer-Assisted Interviews (CAI)", "text": "The very nature of designing a computer-assisted interview forces decisions about edit specifications to be made up front. Both acceptable ranges and logic consistency checks were preprogrammed into the electronic questionnaire. The next few sections describe the coding and editing of the data collected using CAI. Though the child assessments and the parent interviews were both collected using CAI, the child assessments did not contain some of the additional range and edit checks contained in the parent interview. The following sections describe the coding and editing that were conducted on the CAI parent interview. 6-2"}, {"section_title": "Range Specifications", "text": "Within the CAI parent interview instruments, respondent answers were subjected to both \"hard\" and \"soft\" range edits during the interviewing process. \"Hard ranges\" are those that have a finite set of parameters for the values that can be entered into the computer, for example, \"0-5 times\" for the number of times the child, in the previous 5 days, ate a breakfast that was not school provided. Out-of-range values for closed-ended questions were not accepted. If the respondent insisted that a response outside the hard range was correct, the assessor could enter the information in a comments data file. Data preparation and project staff reviewed these comments. Out-of-range values were accepted and entered into the data file if the comments supported the response.\nHard-copy range specifications set the parameters for high and low acceptable values for a question. Where values were printed on the forms, these were used as the range parameters. For openended questions, such as, \"Counting this school year, how many years have you taught in your current school including part-time teaching?,\" high and low ranges were established as acceptable values. Data frequencies were run on the range of values to identify any errors. Values outside the range were identified as errors and were printed on hard copy for a data editor to review. Cases identified with range 6-9 errors were identified, and the original response was updated. In some cases, range violations were retained in the data because the value was checked and found to be the value reported by the teacher or school. These were marked as \"keep as is\" cases. Data frequencies were then rerun and reviewed. This iterative process was repeated until no further range errors were found."}, {"section_title": "Consistency Checks (Logical Edits)", "text": "Consistency checks, or logical edits, examine the relationship between responses to ensure that they do not conflict with one another or that the response to one item does not make the response to another item unlikely. For example, in the household roster, one could not be recorded as a mother and male. When a logical error such as this occurred during a session, the interviewer saw a message requesting verification of the last response and a resolution of the discrepancy. In some instances, if the verified response still resulted in a logical error, the assessor recorded the problem either in a comment or on a problem report. Consistency checks were not applicable to the child assessments.\nBy programming logical edits between variables, consistency between variables not involved in a skip pattern was confirmed. For example, in the school administrator questionnaire, the number of children eligible for free breakfast could not exceed the total number of children enrolled in the school. These logical edits were run on the whole database after all data entry and range edits were complete. The logical edits were run separately for each form. All batches of data were combined into one large data file, and data frequencies were produced. The frequencies were reviewed to ensure the data remained logically consistent within the form. When an inconsistency was found, the case was identified and the inconsistency was printed on paper for an editor to review. The original value was corrected (or checked and \"kept as is\" if the date had been reported, and the case was then rerun through the consistency edits. Once the case passed the consistency edits, it was appended back into the main data set. The frequencies were then rerun and reviewed. This was an iterative process; it was repeated until no further inconsistencies were found."}, {"section_title": "Coding", "text": "Additional coding was required for some of the items collected in the CAI instruments. These items included \"Other, specify\" text responses, occupation, and race/ethnicity. Interviewers keyed 6-3 verbatim responses to these items. Once the data were keyed, staff were trained to code these data using coding manuals designed by Westat and the National Center for Education Statistics (NCES) to support the coding process. In this section, we describe the coding activities for the CAI instruments.\nThe hard-copy questionnaires required coding of race/ethnicity for teachers, review of \"Other, specify\" text responses, and a quick visual review of particular questions in each questionnaire. The quick visual review was to ensure that the questionnaire values were accurate, complete, and consistent across variables and that the numbers were converted to the appropriate unit of measurement prior to converting data to an electronic format. The coding staff were trained on the coding procedures and had coding manuals to support the coding process. This staff also did the data editing after data entry was complete. Senior coders verified coding. The verification rate was set at 100 percent for each coder until accuracy of less than 1 percent error rate was established. After that point, work was reviewed at a rate of 10 percent."}, {"section_title": "Review of \"Other, specify\" Items", "text": "The \"Other, specify\" open-ended parent interview responses were reviewed to determine if they should be coded into one of the existing response categories. During data collection, when a respondent selected an \"other\" response in the parent interview, the interviewer entered the text into a \"specify\" overlay that appeared on the screen. The data preparation staff reviewed these text \"specify\" responses and, where appropriate, coded them into one of the existing response categories. There were no \"Other, specify\" items in the child assessments.\nThe \"Other, specify\" text responses were reviewed by the data editing staff and, where appropriate, upcoded into one of the existing response categories. The small number of specify responses that remained after upcoding did not fit into any preexisting category."}, {"section_title": "Parent Occupation Coding", "text": "As in the base year and first grade data collections, occupations were coded using the categories revealed that some categories had very small numbers of cases and some categories that are similar had similar participation rates, suggesting that the separate codes could be collapsed without significant loss of information. The NHES industry and occupation code categories use a two-digit code, the highest level of aggregation, to have sufficient numbers of cases to support analysis without collapsing categories. There are 13 industry codes and 22 occupation codes in the NHES coding scheme. If an industry or occupation could not be coded using this manual, the Index of Industries andOccupations-1980 (U.S. Department of Commerce, 1982) and Standard Occupational Classification Manual-1980(U.S. Department of Commerce, 1980 were used. Both of these manuals use an expanded coding system and at the same time are directly related to the much more condensed NHES coding scheme. These manuals were used for reference in cases where the NHES coding scheme did not 6-4 adequately cover a particular situation. (See chapter 7, section 7.4.7 for an expanded description of the industry and occupation codes.) Occupation coding began with an autocoding procedure using a computer string match program developed for the NHES. The program searched the responses for strings of text for each record/case and assigned an appropriate code. A little over half the cases were autocoded (50.4 percent). Cases that could not be coded using the autocoding system were coded manually using a customized coding utility program designed for coding occupations. The customized coding utility program brought up each case for coders to assign the most appropriate codes. In addition to the text strings, other information, such as main duties, highest level of education, and name of the employer, was available for the coders. The coders used this information to ensure that the occupation code assigned to each case was appropriate. Almost half the cases (49.6 percent) were manually coded. All of the cases were then verified. Verification of coding is an important tool for ensuring quality control and extending coder training. As a verification step, two coders independently assigned codes (i.e., double-blind coding) to industry and occupation cases. A coding supervisor arbitrated disagreements between the initial code and the verification code. In the early stages, 100 percent of each coder's work was reviewed. Once the coder's error rate had dropped to 1 percent or less, 10 percent of the coder's work was reviewed. Almost 20 percent (19.9 percent) of the cases that were autocoded required adjudication because the verifier disagreed with the autocoding. About the same percent (21.2 percent) of the cases that were manually coded required adjudication because the manual coder and the verifier disagreed."}, {"section_title": "Race/Ethnicity Coding", "text": "The same coding rules used in the base year were used to code all race/ethnicity variables for children, resident parents, and nonresident parents. See chapter 7, section 7.4 for details on how the race variables were coded and how the race/ethnicity composite was created. 6-5"}, {"section_title": "Partially Complete Parent Interviews", "text": "A \"completed\" parent instrument was defined by whether the section on family structure (FSQ) was completed by the respondent. Only completed interviews were retained in the final data file. A small number of interviews in each wave, approximately 103 (less than 1 percent) in third grade terminated the parent interview after the FSQ section but before the end of the instrument. These interviews were considered as \"partially complete\" cases and were included in the data file. All instrument items after the interview termination point were set to -9 for \"not ascertained.\""}, {"section_title": "Household Roster in the Parent Interview", "text": "Several tests were run on the household roster to look for missing or inaccurate information. These tests are the same tests run on the first grade files. One flag was used to identify cases that were edited for any of the reasons described below. The flag is P5EDIT; the flag was set to 1 if the case was edited in the given wave. There were 644 cases requiring edits in wave 5. There were essentially three general types of roster tests performed to determine which cases required editing. First, the relationship of an individual to the focal child was compared to the individual's listed age and sex. Problems found were corrected on the basis of data from prior data collections wherever possible. Second, households with more than one mother or more than one father were scrutinized for errors. While it is possible to have more than one mother in a household-for example, a household could contain one biological and one foster mother of the focal child-such cases warranted closer inspection. Corrections were made whenever clear errors and a clear resolution existed. Lastly, the relationship of an individual to both the focal child and the reference person was examined, as there were cases in which the relationship of an individual to the focal child conflicted with his status as the spouse/partner of the reference person. For example, in a household containing a child's grandparents but not his or her parents, the grandmother may be designated the \"mother\" figure, and the grandfather thus becomes the \"father\" (for the purposes of some questions in the interview) by virtue of his marriage to the grandmother. These cases were examined but left unchanged. Both the original-and correct (grandfather)-relationship data and the new \"parent-figure\" designation (father) that had been constructed were kept. In the third grade data, there are 164 cases with these types of errors after the roster tests were run; the cases can be identified by the flag \"P5ERRFLG.\" In order to monitor the more than 50,000 documents that were to be received in the third grade year, the project-specific receipt and document control system developed in the base year was used, with some modifications. The receipt and document control system was initially loaded with the identifying information, such as identification numbers for schools, teachers, and children; the links between teachers and children; and the questionnaires that were expected from each school and teacher for each cooperating school in the sample. As data were collected in the field, field supervisors completed transmittal forms for each school to indicate which questionnaires were being mailed to the home office. Once data collection started, receipt control clerks reviewed the questionnaires sent in from the field for accuracy and completeness. The identification number on each form was matched against the identification numbers in the tracking system to verify that the appropriate number of forms for each school was returned. When the clerks verified that the correct questionnaires were returned, the questionnaires were scanned for missing critical items. Critical items were identified for each hard-copy questionnaire, except for the facilities checklist. Questionnaires with incomplete or missing data for critical items were not considered complete, and were processed for return to the field. Clerks completed scan edit sheets listing the missing critical items. The questionnaires were receipted in the system as \"needs data retrieval\" and were forwarded to the data preparation department for coding, data entry, and editing. Using the scan edit sheets, clerks identified the missing or incomplete items on data retrieval forms that were then sent to the appropriate field supervisor. The supervisor was instructed to contact the school to try to obtain the missing information. Questionnaires that were scanned and deemed complete were logged into the receipt and document control system as \"complete, no data retrieval.\" Once forms were logged in, if they had any data (some forms had no data due to refusal by the respondent to complete them), they were then coded. The data were then keypunched into electronic format, after which the data were edited."}, {"section_title": "6-7", "text": "The following sections describe the coding, data entry, and editing processes for hard-copy questionnaires."}, {"section_title": "Coding Teacher Race/Ethnicity", "text": "\"Other, specify\" text responses for race/ethnicity in the teacher questionnaire part B were coded using the base year and first grade procedures. Many of these \"others\" included more than one response (e.g., African American/Asian or American Indian/White "}, {"section_title": "Data Entry", "text": "Westat data entry staff keyed the forms in each batch. The data were rekeyed by more senior data entry operators at a rate of 100 percent to verify the data entry. The results of the two data entry passes were compared and differences identified. The hard-copy form was pulled and examined to determine what corrections had to be made to the keyed data. These corrections were rekeyed, resulting in an accuracy rate exceeding 99 percent. The verified batches were then transmitted electronically to Westat's computer system for data editing."}, {"section_title": "Data Editing", "text": "The data editing process consisted of running range edits for soft and hard ranges, running consistency edits, and reviewing frequencies of the results."}, {"section_title": "Frequency and Cross-Tabulation Review", "text": "Frequencies and cross-tabulations were run to determine consistency and accuracy across the various forms and matched against the data in the field management system. If discrepancies could not be explained, no changes were made to the data. For example, in teacher questionnaire part A, an item asking about languages other than English spoken in the classroom included a response option of \"No language other than English.\" If a respondent circled that response, but also answered (in subsequent items) that other languages besides English were spoken in the classroom, then the response was left as recorded by the respondent because the discrepancy could not be resolved. sample.) There is also a 9000 series of S_ID numbers that refers to children who do not attend regular school because they are schooled at home (S_ID numbers 9101 through 9499). There are also several specific 9000 series codes for children who were not located or not followed at the end of a round. The school ID numbers start with 999 for these cases. These are described in section 7.6."}, {"section_title": "DATA FILE CONTENT AND COMPOSITE VARIABLES", "text": "The child ID number (CHILDID) is a concatenation of the school ID where the child was sampled, a three-digit student number, and the letter \"C.\" For example, 0001010C is the ID number of the tenth child sampled in school 0001. The teacher ID number (T5_ID) is a concatenation of the school ID where the teacher was sampled, the letter \"T,\" and a two-digit teacher number. In previous rounds of the study, the numbering for the two-digit teacher number started with 01, such that 0001T01 was the ID number for the first teacher sampled in school 0001. In spring-third grade, the two-digit teacher numbers started numbering with T41 so that the teachers from this round of the study could be identified easily. Thus, in spring-third grade 0002T41 is the ID number for the first teacher sampled in school 0002. The parent ID number (PARENTID) is linked to the child ID number and is a concatenation of the four-digit school ID, the three-digit student number, and the letter \"P.\" It is the same number as the child ID with a letter \"P\" instead of a letter \"C\" at the end. For example, 0001010P is the ID number of the parent of the tenth child sampled in school 0001. If twins are sampled, the ID of the first child sampled is used to 7-3 generate the parent ID. For twins, there are two child-level records with the same parent ID. Children with the same teacher can be identified by finding all children on the child file with the same teacher ID. It should be noted that there is a difference in the variable names between the base year and both the first and third grade special education teacher IDs. In the base year of the study information from special education teachers was included in a separate file and was not part of the child or teacher catalogs. The ID number for special education teachers in the base year special education file was T_ID. In the third grade data file (and the first grade data file), the special education teacher information is included with the rest of the data; thus ID numbers were needed to distinguish special education teachers from regular education teachers. In the third grade file, T5_ID is used to identify regular education teachers and D5T_ID is used to identify special education teachers."}, {"section_title": "Missing Values", "text": "All variables in the ECLS-K data use a standard scheme for missing values.  1998-99, 1999-2000, and 2001-02. The \"Not Applicable\" code (-1) has two purposes. Its primary purpose is to indicate that a respondent did not answer the question due to skip instructions within the instrument or external reasons that led a respondent not to participate. In the parent interview, where the parent or guardian was a respondent, a \"Not Applicable\" is coded for questions that were not asked of the respondent because of a 7-4 previous answer given. For example, a question about a sibling's age is not asked when the respondent has indicated that the child has no siblings. A \"Not Applicable\" code is also used in the direct child assessment if a child did not participate in any section due to a disability. For the teacher and school data where the instruments are self-administered, a \"Not Applicable\" is coded for questions that the respondent left blank because the written directions instructed him or her to skip the question due to a certain response on a previous question. Another use of the \"Not Applicable\" code is the circumstance in which it is not known The \"Refused\" code (-7) indicates that the respondent specifically told the interviewer that he or she would not answer the question. This, along with the \"Don't Know\" code and the \"Not Ascertained\" code, indicates item nonresponse. The \"Refused\" code rarely appears in the school and teacher data because it indicates the respondent specifically wrote something on the questionnaire indicating an unwillingness to answer the question. The \"Don't Know\" code (-8) indicates that the respondent specifically told the interviewer that he or she did not know the answer to the question (or in rare cases on the self-administered questionnaires, \"I don't know\" was written in for the question). For questions where \"Don't Know\" is one of the options explicitly provided, a \"-8\" will not be coded for those that choose this option; instead the \"Don't Know\" response will be coded as indicated in the value label information for that question. The \"Not Ascertained\" code (-9) indicates that the respondent left a question blank that he or she should have answered. For the school and teacher self-administered questionnaires, this is the primary code for item nonresponse. For data outside the self-administered questionnaires (e.g., direct assessment scores), a \"-9\" means that a value was not ascertained or could not be calculated due to nonresponse. \"System Missing\" appears as a blank when viewing code book frequencies and in the ASCII data file. System missing codes (blanks) in the third grade data file indicate that an entire instrument or 7-5 assessment is missing due to unit nonresponse. (Note that in the first grade, \"system missing\" also indicated that some questions were not asked in the school administrator questionnaire for returning schools but were asked in another form of a questionnaire for new schools. This issue does not apply to the third grade file because only one form of the school administrator questionnaire was used.) An example of system missing is a child's parent not participating in the parent interview. In this case, all questions from the parent interview will be blank (system missing). These may be translated to another value when the data are extracted into specific processing packages. For instance, SAS will translate these blanks into periods (\".\") for numeric variables. Depending on the research question being addressed, cases with missing values may need to be recoded. It is advised that users cross-tabulate all lead questions (e.g., whether the child received child care from a relative) and followup questions (e.g., hours of child care from a relative) before proceeding with any recodes or use of the data. Missing values for composite variables were coded using the same general coding rules as those used for other variables. If a particular composite was inappropriate for a given household-as the variable P5MOMID was for a household with no resident mother-that variable was given a value of \"-1\" (Not Applicable). In instances where a variable was appropriate, but complete information to construct the composite was not available, the composite was given a value of -9 (Not Ascertained). The \"Refused\" and \"Don't Know\" codes were not used for the composites, except in the calculations of the height, weight, and body mass index (BMI) composites for spring-third grade. 1 The ECLS-K Third Grade Public-Use Data File is provided on a CD-ROM and is accessible through an ECB that allows data users to view variable frequencies, tag variables for extraction, and create the SAS, SPSS for Windows, or Stata code needed to create an extract file for analysis. The child data file on the ECB is referred to as a \"catalog.\" Instructions for using the CD-ROM and ECB are provided in chapter 8. 1 Children's height and weight measurements were each taken twice to prevent error and provide an accurate reading. Children's BMI was calculated based on height and weight. The rules for using \"Don't Know\" and \"Not Ascertained\" codes for these values was as follows. If both the first and second measurement of height in the child assessment were coded as -8 (Don't Know), then the height composite was coded as -8 (Don't Know). If both the first and second measurements of weight were coded as -8 (Don't Know), the weight composite was coded as -8 (Don't Know). If either the height or weight composites were coded as not ascertained (-9), the BMI composite was coded as not ascertained (-9). If neither the height nor weight composites were coded as not ascertained, and either the height or weight composite was coded as -8 (Don't Know), then the BMI composite was coded as -8 (Don't Know). 7-6"}, {"section_title": "Variable Naming Conventions", "text": "Variables were named according to the data source (e.g., parent interview, teacher questionnaire) and the data collection point. (A number is used to indicate in which round of data collection the variable was obtained, as follows: 5 for spring-third grade, 4 for spring-first grade, 3 for fall-first grade, 2 for spring-kindergarten, and 1 for fall-kindergarten. This numbering system is used for all variables except those beginning with \"W.\" For those variables, 3 indicates third grade, 1 first grade, and K kindergarten.) These variable prefixes are used throughout the catalog, with a few exceptions, and are presented in two categories, (1) third grade variables and (2) cross-sectional and cross-round panel weights, in exhibit 7-2. A few exceptions that do not follow the prefix convention are as follows: 2 The identifiers CHILDID, PARENTID, T5_ID, and S5_ID. The composite variable R5R4SCHG. This variable indicates change in school between spring-first grade and spring-third grade. Source variables and other details for this and all other composite variables can be found in table 7-12."}, {"section_title": "Composite Variables", "text": "To facilitate analysis of the survey data, composite variables were created and added to the child data file. Most composite variables were created using two or more variables, each of which is 2 It should be noted that in past rounds derived child demographic variables for gender, race/ethnicity, and date of birth (GENDER, RACE, DOBMM, DOBDD, and DOBYY) in the kindergarten and first grade files did not follow the prefix conventions noted above because they combined information across data collection points and/or several sources. In spring-third grade these same demographic variables begin with the prefix R5 (e.g., R5RACE). This was done because reports of these variables from parent data were prioritized over other sources in round 5 and a prefix change was used to indicate the difference to users. "}, {"section_title": "Child Composite Variables", "text": "There are many child-level composite variables on the child catalog. Table 7-12 describes all of the composites. Some of these variables are described in further detail here."}, {"section_title": "Child's Age at Assessment (R5AGE)", "text": "The child's age was calculated by determining the number of days between the date when the child completed the ECLS-K direct child assessment and the child's date of birth (R5DOBMO, R5DOBDA, R5DOBYR). The total number of days was then divided by 30 to calculate the age in months. The child assessment date was tested for the appropriate range (March to July 2002). If the assessment date fell outside these ranges, the modal assessment date for the child's school was used. It should be noted that the date of assessment used for R5AGE may be different from the set of assessment dates and times incorporated into methodological variables that are described further in section 7.5. These variables are not edited like those for R5AGE and are text variables that note both date and time."}, {"section_title": "Gender (R5GENDER)", "text": "The third grade gender composite was derived using the gender indicated in the parent interview (INQ.016), child report (AIQ.050), and the FMS. (The composite variable is on the file for R5GENDER, although the source variables are not.) For most of the cases the data were collected in the 7-9 base year. Gender was asked in the third grade parent interview only if the information was missing from previous parent data, and asked in the child assessment only if the information was missing from previous FMS data. In the kindergarten and first grade files, the variable GENDER was derived from the parent data and, if it were missing, the FMS. However, in examining the third grade data it was noted that there were some discrepancies in reports of a child's sex from different sources. Using the parent report, the child report, and the FMS, the most frequently reported sex was used for the child. If there were an equal number of reports for male and female from these sources, the following hierarchy of rules was used: if the data were from the parent interview in previous rounds, then R5GENDER was equal to gender from that parent data. Otherwise, gender was updated from the third grade parent interview question. If the parent interview data were missing, gender was updated from child report. Otherwise, R5GENDER was equal to the composite GENDER from a previous round (because GENDER in previous rounds incorporated the FMS, this last step meant that the FMS was used as the final source of data). After R5GENDER was created, all cases for which reports of gender differed by variable source were printed with the child's name and checked against the composite. This check showed that the rules for assigning gender were successful. In three cases in which the name was clearly male or female, the gender was changed."}, {"section_title": "Child's Date of Birth (R5DOBYY, R5DOBMM, and R5DOBDD)", "text": "In the third grade, the child's date of birth was derived from one of three sources: the parent report (CHILDDOB), the child report (AIQ.040), or the FMS. If the child's date of birth had been reported in a parent interview from a previous round, that value was used. Otherwise, the value from the third grade parent interview was used. If those data were not available or were outside the criteria for inclusion (June 1, 19901, to March 31, 1995, the date of birth from the child interview was used. Finally, if the child report was not available or was outside the criteria for inclusion, the FMS value was used. If the date of birth given was before June 1, 1990, or after March 31, 1995, the data were excluded. It should be noted that in the kindergarten and first grade files, the child date of birth composites (DOBYY, DOBMM, and DOBDD) were created using two rather than three sources of data. The two sources were parent interview data and, in cases in which the parent interview data did not exist 7-10 or were outside reasonable boundaries, FMS data. In spring-third grade, a third source-the child-was added. (W3AMERIN, W3ASIAN, W3PACISL, W3BLACK, W3WHITE,   W3HISP, W3MT1RAC, W3RACETH, and R5RACE) The composites for the child's race/ethnicity are presented in the ECLS-K files in three ways: (1) as dichotomous variables for each race/ethnicity category (W3AMERIN, W3ASIAN, W3PACISL W3BLACK, W3WHITE, W3HISP, W3MT1RAC) from the parent interview data;"}, {"section_title": "Race/Ethnicity", "text": "(2) as a single race/ethnicity composite taken from the parent interview data (W3RACETH); and 3as a race/ethnicity composite taken from either the parent data or the FMS, with FMS data used only if parent data were missing (R5RACE). Respondents were allowed to indicate that they belonged to more than one of the five race categories (White, Black or African American, American Indian or Alaska Native, Asian, Native Hawaiian or other Pacific Islander). From these responses, a series of five dichotomous race variables were created that indicated separately whether the respondent belonged to each of the five specified race groups. In addition, one more dichotomous variable was created for those who had simply indicated that they were multiracial without specifying a race (e.g., biracial). The retention of the dichotomous variables on the file allows users to create different composites as needed. Data were collected on ethnicity as well. Specifically, respondents were asked whether or not they were Hispanic. Using the six race dichotomous variables and the Hispanic ethnicity variable (P4HSP_1 to P4HSP_25, depending on household size), the race/ethnicity composite variables (W3RACETH and R5RACE) were created. The categories were: White, non-Hispanic; Black or African American, non-Hispanic; Hispanic, race specified; Hispanic, no race specified; Asian; Native Hawaiian or other Pacific Islander; American Indian or Alaska Native, and more than one race specified, non-Hispanic. The child composites W3RACETH (race/ethnicity) and R5RACE (race/ethnicity) both share these categories; however, FMS data were used to fill in missing parent report data for the variable R5RACE and only parent report data were used for the variable W3RACETH. A child was classified as Hispanic if a respondent indicated the child's ethnicity was Hispanic regardless of whether a race was identified and what that race was."}, {"section_title": "7-11", "text": "For W3RACETH, if the child's race/ethnicity information was available from a parent interview in a prior data collection, this value was used and copied forward. 3 If the data were missing for a child in a previous parent interview, they were collected in third grade (FSQ.190,FSQ.195) and those data were used. For R5RACE, responses from previous parent interviews were prioritized over the FMS. This is different from the method used to derive the variable RACE in the first grade. In the first grade, the composite RACE was copied forward from previous rounds and FMS data were used if parent reports were not available. Because parent reports were expected to be more accurate than school records, if new information about race was obtained in the third grade parent interview, it was used rather than previous information obtained from the FMS. Therefore, the third grade variable R5RACE is different from RACE in previous rounds for a minority of cases."}, {"section_title": "Child's Height (C5HEIGHT)", "text": "To obtain good measurements, each child's height was measured twice. For the height composite C5HEIGHT, if the two height values from the instrument (i.e., C5HGT1 and C5HGT2 for spring-third grade) were less than two inches apart, the average of the two height values was computed and used as the composite value. Otherwise, the value that was closest to 52.5 inches (the median height for 9-year-olds as developed by the National Center for Health Statistics (NCHS) in collaboration with the National Center for Chronic Disease Prevention and Health Prevention (NCCDPHP)) was used as the composite value."}, {"section_title": "Child's Weight (C5WEIGHT)", "text": "Each child's weight was also measured twice. For the weight composite (C5WEIGHT), if the two weight values from the instrument (i.e., C5WGT1 and C5WGT2 for spring-third grade) were less than 5 pounds apart, the average of the two values was computed and used as the composite value. Otherwise, the value that was closest to 64.0 pounds (the median weight for 9-year-olds as developed by NCHS in collaboration with the NCCDPHP) was used as the composite value."}, {"section_title": "Child's Body Mass Index (C5BMI)", "text": "Composite Body Mass Index (BMI; variable name C5BMI), which is a calculation of the child's body weight adjusted for height, was calculated by multiplying the composite weight in pounds by 703.0696261393 and dividing by the square of the child's composite height in inches."}, {"section_title": "Child's Disability Status (P5DISABL)", "text": "A composite variable was created to indicate whether a child had a disability that was diagnosed by a professional. Questions in the parent interview about disabilities in spring-third grade asked about the child's ability to pay attention and learn, overall activity level, overall behavior and relations to adults, overall emotional behavior such as anxiety or depression, ability to communicate, difficulty hearing and understanding speech, and eyesight. For each disability or behavior, a question was asked about whether a diagnosis of a problem was obtained by a professional (CHQ.050,CHQ.110,CHQ.170,CHQ.210,CHQ.300,CHQ.335,CHQ.360). A question was also asked about receipt of therapy services or participation in a program for children with disabilities (CHQ.520). The composite variable P5DISABL was coded 1 (Yes) if any of the source variables (CHQ.050, CHQ.110, CHQ.170, CHQ.210, CHQ.300, CHQ.335, CHQ.360, CHQ.520) about diagnosis or therapy services were coded 1 (Yes). This was done even if data for some of the source variables were missing. If data for all the source variables were missing, the composite was coded -9 (Not Ascertained). Otherwise, P5DISABL was coded 2 to indicate no reported disability. It should be noted that the spring-third grade composite is somewhat different from the composite in previous rounds of the study because questions were added about both overall behavior and relations to adults and overall emotional behavior such as anxiety or depression. In addition, (like the spring-first grade composite P4DISABL) the spring-third grade composite does not include a question used in the fall-kindergarten questionnaire and composite that asked parents about their children's coordination in using their arms or legs. 7-13 7.4.1.9 Nonparental Care (P5CARNOW) There are several composite variables on the file that can be used to describe child care arrangements based on information from the parent interview. One of these (P5CARNOW) describes whether the child had any type of nonparental care at the time of the interview. The creation of P5CARNOW was as follows. If the child was receiving care from a relative (CCQ.010), a nonrelative (CCQ.150), or a day care center or before or after school program at a school or in a center (CCQ.260), P5CARNOW was equal to 1 (Yes). Otherwise, if any of the three variables was unknown, P5CARNOW was coded as -9 (Not Ascertained). If the respondent indicated that the child was not currently receiving any of the three types of care (CCQ.010,CCQ.150,and CCQ.260 all equaled 2 [No]), P5CARNOW was coded as 2 (No). It should be noted that the nonparental care as defined by P5CARNOW does not have to be received on a regular basis. However, for the composite P5HRSNOW (hours per week in child care) described below, if the nonparental care is not regular, the number of hours in care is coded as zero. This is because the child must have a regular arrangement in order for hours per week in care to be reported. Users should be aware of the differences in definitions when comparing P5CARNOW with P5HRSNOW. variables were refused or unknown, then the number of hours for that type of care was coded as -9 (Not Ascertained). Also, if the regular receipt variable was coded as 1 (Yes), but the hours given was refused or unknown, then the number of hours for that type of care was coded as -9 (Not Ascertained). Otherwise, if the indicator for regular receipt of care was equal to 1 (Yes), and the hours given were greater than or equal to 0, then the number of hours for that type of care was coded as the number of hours given."}, {"section_title": "7-14", "text": "The composite also includes hours spent with additional regularly scheduled providers of care of the same type. This was done to include child care arrangements such as those in which two different relatives cared for the child on a regular basis or two different child care programs were attended. For each type of care, if the care receipt variables indicated no care of that type, or if the number of providers of that type of care (questions CCQ.060, CCQ.165, and CCQ.325 indicated number of regular providers of each type) was equal to 1, then additional hours were coded to 0. Otherwise, if the number of providers or the number of additional hours (questions CCQ.140,CCQ.250,and CCQ.403 indicated number of hours spent with additional providers) was refused or unknown, then the number of additional hours was coded as -9 (Not Ascertained). Otherwise the number of additional hours was coded to equal the appropriate number of additional hours variables in the instrument (CCQ.140,CCQ.250,or CCQ.403). This process was followed three times, once each for relative care, nonrelative care, and center-based care. If any of the three primary caregiver hour variables or the three additional hours variables was missing then the total number of hours was coded as -9 (Not Ascertained). Otherwise the total number of hours in regularly scheduled child care was coded as the sum of the six hour variables. It should be noted that in earlier rounds, if the primary care arrangement hours were not missing and the additional hours were missing, the primary caregiver hours were used for the composite. In spring-third grade, if any of the primary or additional hours variables were missing, the composite was missing. This change makes the variable represent all types of regular care rather than prioritizing primary arrangements. Because there are slightly more missing data for the composite in spring-third grade than in the previous rounds (the percentage of \"not ascertained\" answers was 0.8 percent in fall-kindergarten, 0.6 percent in spring-first grade, and 1.5 percent in spring-third grade), users who want to prioritize primary care hours over additional hours may want to calculate their own composite. Although P5HRSNOW was created almost identically to the same composite variable in kindergarten (P1HRSNOW), with the exception noted above, there was one other difference. In kindergarten, questions were asked about whether the child was ever in a particular type of care. If not, P1HRSNOW was set to 0. Because questions about the child having ever been in a particular type of care were not included after the kindergarten year, they were not part of the composite variable definition for either the third or first grade variables. 7-15"}, {"section_title": "Number of Child Care Arrangements (P5NUMNOW)", "text": "Another composite variable (P5NUMNOW) was used to indicate the total number of all types of care arrangements the focal child had at the time of the spring-third grade parent interview. The variable was created as follows. If any of the child care receipt variables for relative, nonrelative, or center-based care (CCQ.010, CCQ.150, or CCQ.260) was refused, unknown, or missing, then P5NUMNOW was coded as -9 (Not Ascertained). If any of the care receipt variables was equal to 1 (Yes), but its corresponding number of arrangements variable (CCQ.060, CCQ.165, and CCQ.325) was refused, unknown, or missing, then P5NUMNOW was again coded as -9 (Not Ascertained). Otherwise, the number of arrangements indicated in CCQ.060, CCQ.165, and CCQ.325 were summed to obtain the total number of current child care arrangements. The differences in how missing data are handled for each of the child care composites are important to note when combining variables. For example, because P5NUMNOW requires that the number of child care arrangements be known, it is possible for a child to have P5CARNOW =1 (child was in nonparental care) and have P5NUMNOW be -9 (Not Ascertained). To obtain the composite, hours were compared for relative care in the child's home (CCQ.090) or in other home (CCQ.070); nonrelative care in child's home (CCQ.190) or in other home (CCQ.170); and center/program care (CCQ.355). First, the composite P5HRSNOW, described earlier, was used to code individuals missing current hours of care (P5HRSNOW=-9) or with no hours of nonparental care (P5HRSNOW=0). Those with missing hours of care were coded as -9 (Not Ascertained); those with no hours of care or no regularly schedule care were coded as 0. For the remaining cases, if the number of hours of either relative or nonrelative care (given in CCQ.090 and CCQ.190) were higher than all other hours of care, the variable indicating location of care for that type was examined using instrument items CCQ.070 and CCQ.170. If location of care was missing, then P5PRIMNW was coded as -9 (Not Ascertained); if P5PRIMNW was not missing, then P5PRIMNW was coded 1, 2, 3, or 4, depending on the type (relative/nonrelative) and location (child's home/other home) of care. Otherwise, if the number of hours of care in center-based programs (CCQ.355) was higher than for relative or nonrelative care, then P5PRIMNW was coded as 5. If the number of hours of care was equal for two or more types of care, P5PRIMNW was coded as 6. P5PRIMNW was coded as 7 if the location of care varied between two homes. It should be noted that it is possible to have missing data for the primary child care arrangement (P5PRIMNW), but still have information on the number of hours of child care a child has (P5HRSNOW). This is because there must be information about the location of care in order to have a valid value for P5PRIMNW."}, {"section_title": "Family and Household Composite Variables", "text": "Many composites were created to capture information about the sampled children's family and household characteristics. Several of these are described below. All of the family and household composites are listed and described in table 7-12."}, {"section_title": "Number of Siblings (P5NUMSIB)", "text": "The composite P5NUMSIB indicates the total number of siblings (full, step, adoptive, or foster) with whom the child lived in the household (FSQ.160 and FSQ.170). Siblings were identified 7-17 through the respondents' stated relation of the sibling to the focal child. In addition, any child that was reported to be a child of the focal child's parent/guardian was considered a sibling of the focal child. Members' Age (P5LESS18, P5OVER18, P5HDAGE, and"}, {"section_title": "Parent and Household", "text": ""}, {"section_title": "P5HMAGE)", "text": "There are several composite variables on the file that refer to the ages of adults and children in the household. These are P5LESS18 (total number of people in the household under age 18, including focal child, siblings, and other children), P5OVER18 (total number of people in the household age 18 or older, siblings, and other children), P5HDAGE (age of resident father), and P5HMAGE (age of resident mother). The ages of these persons in the household were collected during the fall of kindergarten in the household matrix. However, in subsequent years of the study, questions about age were not asked for household members who were previously in the household. This was done to save interviewing time. In the third grade, ages were collected only for new household members. Otherwise, ages were incremented by adding years based on the round in which the person joined the study. Age changes were made to increase the ages of all household members other than the focal child and twin (the ages of the focal child (and twin, if applicable) were updated based on birthdate). The ages of all household members who were not new to the study in spring-third grade (other than the focal child and twin) were increased by the numbers shown in table 7-1. The guidelines for creating these were as follows: (1) half years could not be included, and (2) the same number of years was added for those who entered the study during the same school year. The numbers were made to err on the side of making persons older rather than younger because this would cause fewer problems with range checks and displays in the parent interview if there was a discrepancy between actual age and imputed age. In order to save interviewing time, questions about age were not asked about household members whose ages were reported in previous rounds of the study. Instead, years were added to the originally reported age. The number of years added was based on when the household member joined the study. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 third grade data collection, school year 2001-02."}, {"section_title": "7-18", "text": ""}, {"section_title": "Food Security Status", "text": "Food security status of the children's families was assessed based on responses to the 18 food security questions (P5WORRFD through P5NOMONY) in the spring-third grade parent interview. The questions measured a wide range of food insecurity and reduced food intake issues. They were P5FSSCAL is the scale score presentation of the Household Food Security items. It is a continuous, interval-level measure of food insecurity and is appropriate for linear models. This scale score is a Rasch transformation of the raw score (P5FSRAW). Valid values range from 1.4 to 13.0, with higher values indicating more severe food insecurity. Under Rasch-model assumptions, the scale score for families that affirm no items (in other words, they did not provide \"yes\" answers to the questions and the raw score = 0) is indeterminate. It is less than the lowest measured value (1.4), but its precise value is unknown and may vary substantially among families. P5FSSCAL for such cases is assigned a value of -6. If these cases (a substantial majority of all cases) are included in linear models, appropriate methods must be used to take into account this indeterminacy. P5FSCHSC is similar to P5FSSCAL but is the Children's Food Security scale score. This is a measure of the severity of food insecurity or hunger experienced by children in the household in the previous 12 months. Valid values range from 4.1 to 12.2, with higher values indicating more severe food deprivation. The scale score is undefined for households that affirmed no child-referenced items (see discussion of P5FSSCAL above)."}, {"section_title": "Food Security Status: Categorical Measures (P5FSSTAT and P5FSCHST)", "text": "P5FSSTAT is a categorical measure of Household Food Security status formed by dividing P5FSSCAL into four ordered categories: food secure, food insecure without hunger, food insecure with hunger (moderate), and food insecure with hunger (severe). P5FSSTAT is appropriate for comparing prevalence rates of food insecurity and hunger across subpopulations and can be used as a categorical variable in associative models. There are few cases in the most severe category, so for most prevalence reporting purposes, the two categories of food insecure with hunger (moderate and severe) should be collapsed and reported as a single category. When interpreting food security statistics, users should remember that food security status is a household-level characteristic. In most households classified as food insecure with hunger, the children in the household were not hungry. P5FSCHST is a categorical measure of Children's Food Security status that identifies households with hunger among children at some time during the 12 months prior to the survey. This variable is appropriate for comparing prevalence rates of hunger among children across subpopulations."}, {"section_title": "7-20", "text": "There were few households (n=27, 0.2 percent) that reported hunger among children, so the analytic utility of this variable is limited. However, for analytic purposes, other categories of children's food insecurity delineated by less severe thresholds (based on children's food security raw scores or scale scores) may be useful. For example, Nord and Bickel (2001)  The Household Food Security raw score, P5FSRAW, is a count of affirmative responses to the 18 items. This is an ordinal-level measure of food insecurity and is not recommended for use in analysis. The Children's Food Security raw score, P5FSCHRA, is a count of affirmative responses to child-referenced items. Responses to items skipped because of screening are assumed to be negative. Families with no valid responses are coded as missing (-9). Missing item responses of families with one or more valid responses are imputed as negative responses (only 0.13 percent of the sample)."}, {"section_title": "Socioeconomic Status (SES) and Poverty (W3DADSCR, W3MOMSCR, W3SESL, W3SESQ5, W3INCCAT, W3POVRTY)", "text": "Socioeconomic status (SES) was computed at the household level using data for the set of parents who completed the parent interview in spring-third grade. The SES variable reflects the socioeconomic status of the household at the time of data collection for spring-third grade (spring 2002). The components used to create the SES variable were as follows: Father/male guardian's education; Mother/female guardian's education; Father/male guardian's occupation; Mother/female guardian's occupation; and Household income."}, {"section_title": "7-21", "text": "Occupation was recoded to reflect the average of the 1989 General Social Survey (GSS) prestige score. This was computed as the average of the corresponding prestige scores for the 1980 Census occupational categories covered by the ECLS-K occupation. Table 7-12 provides details on the prestige score values (W3DADSCR, W3MOMSCR). The variables were collected as follows: Income. The information about income was collected in spring-third grade. Broadrange and detailed-range income questions were asked of all participants. The broad range classifies household income as $25,000 and less per year, or as greater than $25,000. The detailed range classifies household income as shown in table 7-2. Households that were determined to meet the size and income criteria related to poverty shown in table 7-3 were asked to report income to the nearest $1,000. (We call this exact income for simplicity.) Because not all households were asked to report exact income, the midpoint of the detailed income range was used to compute the SES composite variable. Parent's education. The information about parent's education was collected or updated in spring-third grade. Parent's occupation. The information about parent's occupation was collected or updated in spring-third grade.  Not all parents completed the parent interview; among those who did, not all responded to every question. Therefore, there were missing values for some of the components of the SES composite variable. Only a small percentage of values for the education and occupation variables were missing; a larger proportion of households had missing values for the detailed income range (see table 7-4). A two-stage procedure was used to impute missing values for each component of the SES composite variable. First, if a parent had completed an interview in the kindergarten or first grade year, missing values for the spring-third grade education, occupation, and detailed income range were filled in with values from the previous years. The rationale for this approach was that the best source of data for an individual or a household was the data from a previous year."}, {"section_title": "7-23", "text": "This first imputation stage was implemented as follows: Education level was brought forward from the most recent previous round. This was done only if the same person was the parent figure both in spring-third grade and in the earlier round. Occupation was brought forward only if the individual was in the labor force (i.e., was working at a paid job, on vacation from a paid job, or looking for a job). It was also required that the same person be the parent figure both in spring-third grade and in the earlier round. NOTE: Prestige scores were not assigned to individuals unless they were in the labor force, regardless of whether they reported an occupation. Detailed income category was brought forward from the most recent previous round. Second, data still missing after this initial step were imputed using a hot deck methodology. In hot deck imputation, the value reported by a respondent for a particular item is assigned or \"donated\" to a \"similar\" person who failed to respond to that question. Auxiliary information known for both donors and nonrespondents is used to form groups of persons having similar characteristics. These groups of similar respondents and nonrespondents are called \"imputation cells.\" The imputed value for a case with a missing value is taken from a randomly selected donor among the respondents within the cell. Imputation cells were defined by respondent characteristics that were the best predictors of the variables to be imputed. These relationships had been determined previously by CHAID (Chi-squared Automatic Interaction Detector) analyses of the base year data. Missing values for the education, occupation, and detailed income range variables were imputed by the hot deck method for all households. Hot deck imputation was done in a sequential order, separately, by type of household (female single parent, male single parent, and both parents present). For households with both parents present, the mother's and father's variables were imputed separately. Imputed as well as reported values were used to define imputation cells; missing values for donor characteristics were treated as a separate category. No imputed value was used as a donor. No donor was used more than once. The order of hot deck imputation for all the variables was from the lowest percent missing to the highest. Occupation imputation involved two steps. First, the labor force status of the parent was imputed (i.e., whether the parent was employed). Then the parent's occupation was imputed only for those parents whose status was identified as employed either through the parent interview or the first imputation step. The detailed income range was imputed in two steps: first for cases where the broad income range was known, and second for cases where it was unknown."}, {"section_title": "7-24", "text": "For households where both parents were present, the order of hot deck imputation was as follows: Mother's education; Father's education; Mother's labor force status; Mother's occupation; Father's labor force status; Father's occupation; Detailed income range, where the broad income range was known; and Detailed income range, where the broad income range was unknown. At this point, all of the missing values had been imputed. However an exact income value was still required to construct the SES composite. The midpoint of the detailed income range was assigned for this purpose to all households. The log of the detailed income range midpoint was then used to compute the SES composite. This value does not vary widely within the levels of the detailed income range, so the midpoint was a reasonable choice. It was used only for the purpose of computing the SES composite and was not retained in the data file. All missing values of the SES components were imputed by the process described above. Tables 7-5 through 7-8 summarize the results.    "}, {"section_title": "7-26", "text": "Once the components of the SES variable were imputed, their corresponding z-scores or normalized values were computed. The expression of z-score z hi for the h-th component in the i-th is the value of the h-th SES component for the i-th household; w i is the base weight for the i-th record; where i m is the number of nonmissing SES components for the i-th household. W3SESL is the continuous variable for the SES composite that ranges from -2.49 to 2.58. As described, the SES composite is the average of up to five measures, each of which was standardized to have a mean of 0 and a standard deviation of 1, hence the negative values. For analyses that require a continuous SES measure, such as multivariate regressions, W3SESL is the variable to use. A categorical SES variable (W3SESQ5) was created that contains the quintile for the value of the composite SES for the child. Quintile 1 represents the lowest SES category and quintile 5 represents the highest SES category. The quintiles were computed at the child level using the spring-third grade parent weights. For categorical analyses, use W3SESQ5 and the parent weight. Note that for households with only one parent present, not all the components were defined. In these cases, the SES was computed by averaging the available components."}, {"section_title": "7-27", "text": "The imputed detailed income range variable (W3INCCAT) was also used to create a household-level poverty variable (W3POVRTY). Income was compared to Census poverty thresholds for 2001, which vary by household size. Table 7-9 shows the detailed income categories used in the ECLS-K parent interview for determining whether to ask a more detailed question about income to the nearest 1,000. For comparison, the table also shows weighted poverty thresholds from Census data. 4 Households whose income fell below the appropriate threshold were classified as poor. For example, if a household contained two members, and the household income was lower than $11,569, then the household was considered to be below the poverty threshold. "}, {"section_title": "Parent Education (W3PARED, W3DADED, and W3MOMED)", "text": "There are three parent education composites on the file. These are W3PARED (the highest level of education for the child's parents or nonparent guardians who reside in the household), W3DADED (father's highest level of education), and W3MOMED (mother's highest level of education). The variables include both parent (birth, adoptive, step, and foster) and nonparent guardians. For example, if the child had no parents but had a guardian, the education of the guardian and his or her spouse were used in the creation of the composites if the guardian was specified as such in the relationship variable or if the guardian was the respondent/respondent's spouse and there were no other parent figures in the household. In spring-third grade, parent education level was updated from spring-first grade if it was a household that had been part of that round of the study. Respondents were asked if they or their corresponding parent figures, if applicable, completed any additional grades of school or received any diplomas or degrees (PEQ.010). If so, PEQ.020 asked what grade the parent completed or what degree was received. If there was no education information to update from spring-first grade, respondents were asked for their highest education level in PEQ.020. If this education level was less than the education level reported in a previous round, the higher education level was kept for the composite. 5 If both parents/guardians resided in the household, W3PARED was the highest value for education level from either the mother/guardian in W3MOMED or the father/guardian in W3DADED. If the household only had one parent or guardian, then W3PARED was equal to either W3MOMED or W3DADED depending on which parent or guardian resided with the child. If the education data for either of the parents were missing 6 it was imputed, and the composite W3PARED was created based on both the reported and imputed data."}, {"section_title": "Parent Race/Ethnicity (P5HDRACE and P5HMRACE)", "text": "The composites for race/ethnicity for the parents were calculated in the same way as those for the child, except that there is not a variable that supplements parent reported race/ethnicity with FMS data similar to the variable R5RACE for children. All data on parent race/ethnicity come from the parent interview. Race/ethnicity for parents is presented in the spring-third grade data file as a categorical race/ethnicity composite (for the father/male guardian it is P5HDRACE, and for the mother/female guardian it is P5HMRACE). 5 Because of a programming issue, many respondents were asked the education question in PEQ.020 rather than asked to update education information obtained in a previous round. For 1,385 mothers and 1,124 fathers, the spring-third grade education levels were lower than the education levels provided in the base year of the study. One source of the discrepancy may be that the question structures were different in the base year of the study and spring-third grade. In the base year, if a respondent answered any grade less than 12, he or she was then asked if the person had received a high school diploma or its equivalency, such as a GED. For example, if he or she answered that the highest grade completed was 9th grade, but that he or she had completed a GED, then the highest education level would not be 9th grade, it would be high school diploma/equivalent. However, in spring-third grade, the follow up question on high school equivalency was not asked. Thus, a base year respondent (who was a nonrespondent in spring-first grade) who had answered 9th grade in the base year, would answer 9th grade again in the spring-third grade data collection, but this time due to the absence of the followup question, the highest level of education completed would be 9th grade. Based on a review of the cases, the higher of the two education levels was used. This solution took into consideration that the base year cases had a followup item that collected the information in a more informative way. 6 Missing data were due to \"refused\" or \"don't know\" answers from respondents, in addition to program issues that caused a few cases to have missing data."}, {"section_title": "7-29", "text": "Respondents were allowed to indicate that they belonged to more than one of the five race categories (White, Black or African American, American Indian or Alaska Native, Asian, Native Hawaiian or other Pacific Islander). From these responses, a series of five dichotomous race variables were coded that indicated separately whether the respondent belonged to each of the five specified race groups. In addition, one more variable was coded for those who had simply indicated that they were multiracial without specifying the race (e.g., biracial). 7 The dichotomous codes for each of the race variables are not provided on the spring-third grade file, but the composite derived from the responses is provided. Parent race/ethnicity was obtained for all parents and spouses of respondent parents, but may or may not have been collected for a parent's boyfriend or girlfriend. For example, in a family with a birth mother and stepfather the race/ethnicity of both parents was obtained. However, in a family with a birth mother and the mother's boyfriend, if the mother's boyfriend was not identified as a spouse or partner of the mother, the race/ethnicity of the mother was obtained but that of the boyfriend was not."}, {"section_title": "Teacher Composite Variables", "text": "Details about how two of the teacher composites, child grade level and class type, were created are provided here. All of the teacher composites are listed and described in table 7-12."}, {"section_title": "Grade-Level Composite (T5GLVL)", "text": "To create the grade-level composite (T5GLVL), five possible sources of information were used: (1) teacher questionnaire part C (Q1 T5GRADE for grade level); (2) special education teacher part B (Q2 E5GRADE for grade level); (3) child assessment introductory section (AIQ.030 C5INGRAD); 4child assessment closing section (ACQ.005 C5THIRD and ACQ.010 C5GRADE, completed by interviewer), and (5) FMS information about grade level. If conflicts existed among these five sources, the grade level indicated by the majority of the nonmissing sources was used for T5GLVL. When there were five, four, or three sources of information 7-30 and three were in agreement, the grade level indicated by the three sources was taken. When there were four sources of information and only two were in agreement, the grade level indicated by the two sources in agreement was taken. When there were three sources of information and two were in agreement, the grade level indicated by the two sources in agreement was taken. When there were four or five sources of data, and two sources indicated one option and the other two indicated another option, the grade indicated in a particular source was selected, according to the hierarchy presented below."}, {"section_title": "Classroom teacher, T5GRADE", "text": "Special education teacher, E5GRADE Assessment introduction, C5INGRAD Assessment closing, C5THIRD and C5GRADE"}, {"section_title": "FMS", "text": "In establishing this hierarchy, it was assumed that teachers had the best knowledge and that school records (on which the FMS are based) were more apt to be in error. It was also assumed that children were reliable reporters of their own grade level, so their reports were prioritized over the FMS. When equal numbers of sources were in conflict (1 vs. 1) or (2 vs. 2) or (1 vs. 1 vs. 1), the decision was made by using the information from the source highest on the list above. One exception to this hierarchy was made. Because the FMS and AIQ grade-level information did not allow for ungraded classrooms, the FMS and AIQ information were not considered in any case in which at least one source indicated an \"ungraded\" classroom. It should be noted that in spring-first grade, there was information about grade level from the student record abstract; however, there were no grade-level questions in the child assessment at that time. In spring-third grade, grade level was not asked in the student record abstract, but was included as part of the child assessment instead."}, {"section_title": "Class Size (A5CLSZ)", "text": "The composite for class size was created from class totals provided in three different questions in the teacher questionnaire, part A. The totals for race/ethnicity (Q4, A5TOTRA), age (Q3, 7-31 A5TOTAG), and sex (Q5, A5BOYS+A5GIRLS) were compared. If one of the totals differed, but two totals matched, the total shown by the two matching sources was used. If there were no matches among the totals, the total for the composite was set using, in order of priority, the sex, age, or race/ethnicity total. Otherwise, A5CLSZ was coded as -9 (Not Ascertained). It should be noted that the class size composite A5CLSZ was used in spring-third grade as the denominator for the composite variables A5PMIN, A5PHIS, A5PBLK, and A5PLEP. In previous years, the total class size used in the calculation of these variables was based on the total number of children in the question about numbers of children by race/ethnicity."}, {"section_title": "School and Class Composite Variables", "text": "Variables on school and class characteristics were constructed from the teacher and school data and the sample frame. Details on how some of the variables were created follow."}, {"section_title": "School Type (S5SCTYP)", "text": "In spring-third grade, S5SCTYP was created as follows. Questions 2 (L5PUBLIC) (whether school is public) and 4 (L5CATHOL, L5OTHREL) (type of private school) from the school fact sheet, along with school sample frame data, were used to create the school type composite variable. If the response to question 2 (Is this a public school?) was \"Yes,\" then S5SCTYP was coded \"public.\" If the response to question 4.a. (L5CATHOL) (Is your school a Catholic school) was \"Yes,\" then the school was coded as \"Catholic.\" Otherwise, if the response to question 4.b. (L5OTHREL) (Is your school private with another religious affiliation?) was \"Yes,\" then S5SCTYP was coded as \"private, other religious.\" Otherwise, because the skip pattern to question 4 was used only if the school was private, if the response to question 4.a. (L5NAISKL, private school accredited by NAIS), question 4.d. (L5OTHPRI, other private), question 4.e. (L5PVTSPD, special education school-primarily serves children with disabilities), or question 4.f. (L5PVTEAR, an early childhood center-school or center includes preschool and/or early elementary grades) was \"Yes,\" then S5SCTYP was coded as \"private, non-religious.\" If S5SCTYP could not be coded from the school fact sheet, reports of school type from the same school in previous rounds were used (in previous rounds, school type was asked in the school 7-32 administrator questionnaire and the variable names were S4SCTYP, S3SCTYP, S2KSCTYP, and CS_TYPE2). If those sources were unavailable, a variable from the school master file (taken from the 1999-2000 PSS/2000-01 CCD frame) was used to code S5SCTYP. If S5SCTYP could not be coded, S5SCTYP was coded as -9 (Not Ascertained). If the child was schooled at home, the composite was coded as -1 (Not Applicable)."}, {"section_title": "Public or Private School (S5PUPRI)", "text": "This variable is a less detailed version of school type (with only two categories-public and private) and is derived from the school type composite S5SCTYP described above. In spring-third grade, it was created as follows. If S5SCTYP was 4 (public), then S5PUPRI was coded as public (1). If S5SCTYP was 1-3 (Catholic, other religious, other private) then S5PUPRI was coded as private (2). If S5SCTYP was coded as Not Ascertained (-9), then S5PUPRI was -9 (Not Ascertained). If S5SCTYP was coded \"Not Applicable,\" then S5PUPRI was coded \"Not Applicable.\""}, {"section_title": "School and Grade-Level Enrollment (S5ENRLS, S5ENRLT)", "text": "There are two composite enrollment variables on the third grade file: total school enrollment (S5ENRLS) and third grade enrollment (S5ENRLT). Total school enrollment was created using the school enrollment variable from the school administrator questionnaire (S5ANUMCH). If this variable was missing, data for private schools were taken from the 1999-2000 Private School Survey (PSS) and data for public schools were taken from the 2000-01 Common Core of Data (CCD) public school universe. If these were also missing, the variable was coded -9 (Not Ascertained). If the child was schooled at home, the composites were coded -1 (Not Applicable). Third grade enrollment was not obtained during data collection. The third grade enrollment data for private schools came from the 1999-2000 PSS data. The enrollment data for public schools came from the 2000-01 CCD public school universe data. 7-33"}, {"section_title": "Percent Minority Students in the School (S5MINOR)", "text": "The composite variable S5MINOR indicates the percentage of minority students in a school in spring-third grade. The composite is based on a question in the school administrator questionnaire Q3that was used to ask about the number or percentage of students in the following categories: Hispanic, regardless of race; Black, not of Hispanic origin; White, not of Hispanic origin; Asian or Pacific Islander; American Indian or Native Alaskan; and other. The composite was based on the sum of percentages for all categories except White, not of Hispanic origin. In some cases, the composite could not be obtained from the data because of missing data or errors. If the composite could not be derived from the data, percent minority was obtained from the CCD (for public schools) or the PSS (for private schools). If these data were missing, the composite was coded -9 (Not Ascertained). If the child was schooled at home, the composite was coded as -1 (Not Applicable). In spring-third grade, school administrators were allowed to report their answers to the student racial composition questions as either numbers or percents, whereas in spring-kindergarten they were asked to report those answers as percents. All answers recorded as numbers in spring-third grade were converted to percentages for the composite variable. The sum of the answers across all categories was allowed to add within +/-5 percent of the reported total. In a few cases, this produced answers slightly over 100 percent. These were topcoded to 100 percent. A flag for each race/ethnicity variable indicates whether the answer was reported as a number or a percent. 8 Because the composite is calculated as a percent, these flags will not be needed by users unless the analyst is interested in examining how answers were reported. If the flags (S5ASNFL, S5HSPFL, S5BLKFL, S5WHTFL, S5INDFL, and S5OTHFL) were equal to 1 for each of the race variables S5ASNPCT, S5HISPPCT, S5BLKPCT, S5WHTPCT, S5INDPCT, S5OTHPCT, these 6 race/ethnicity variables were reported by the respondent as percentages. It should be noted that the spring-third grade composite was created in the same way as the composite for spring-first grade. However, both the spring-third grade and first grade composites are 8 There were also other questions in the school administrator questionnaire that allowed for answers to be recorded as either a number or percent. The flags for these variables are S5ADAFLG (average daily attendance reported as number/percent), S5ASNFLG (question about Asian or Pacific Islander teachers reported as number or percent), S5HSPFLG (question about Hispanic teachers reported as number or percent), S5BLKFLG (question about black teachers reported as number or percent), S5WHTFLG (question about white teachers reported as number or percent), S5INDFLG (question about American Indian or Native Alaskan teachers reported as number or percent), and S5OTHFLG (question about teachers of other races reported as number or percent). In all cases, the final variables related to these flags are reported as percentages, but the flags indicate how the answers were originally recorded by respondents."}, {"section_title": "7-34", "text": "slightly different from the one used in spring-kindergarten (S2MINOR) because the school administrator questionnaire item that asked about the percent of minority students in the school had different response options. In spring-kindergarten, the percent of minority students was derived from answers to the school administrator questionnaire by determining the percentage of children who were of either Hispanic or Latino origin (question 14) and the percentage of children who were American Indian or Alaska Native, Asian, black or African American, or Native Hawaiian or Other Pacific Islander (question 15) to create the percent minority composite. In spring-third grade, Hispanic or Latino origin and race were included in the same question."}, {"section_title": "School Instructional Level (S5SCLVL)", "text": "The purpose of this composite is to classify schools based on the highest grade taught in the school. This composite is taken in spring-third grade from the school fact sheet (Q1, L5PRKNDR, L5KINDER, L5GRADE1, L5SECOND, L5THIRD, L5FOURTH, L5FIFTH, L5SIXTH, L57TH, L58TH, L5NINTH, L5TENTH, L511TH, L512TH). The highest grade level circled on the form was determined, and the grade level was classified accordingly. If data were missing, data were used from the school master file (based on the 1999-2000 PSS and the 2000-01 CCD) to fill in instructional level. If school master file data were unavailable for a particular school, data from previous school administrator questionnaires from spring-first grade or spring-kindergarten schools (S4SCLVL and S2KSCLVL) were used to determine instructional level. If those sources were also not available, S5SCLVL was coded as -9 (Not Ascertained). If the child was schooled at home, the composite was coded as -1 (Not Applicable). In previous rounds of the study, this composite was taken from the school administrator questionnaire rather than the school fact sheet. Also, in previous rounds, if the question about grade levels in the school was left blank, another question from the school administrator questionnaire about grade levels that participated in special programs was used. If the respondent did not answer either of these questions, then school sample frame data were used to determine the value for the composite in previous rounds. 7-35"}, {"section_title": "School Year Start and End Dates (L5SCHBDD, L5SCHBMM, L5SCHBYY, L5SCHEDD, L5SCHEMM, L5SCHEYY)", "text": "The composite for school year start and end dates was taken from the school fact sheet (Q5, L5SYRSMM, L5SYRSDD, L5SYRSYY, L5SYREMM, L5SYREDD, L5SYREYY). If those data were missing, the values were taken from the FMS. It should be noted that in past rounds, the composites for school year start and end dates were created differently because they were based on different questions. In past rounds, the question was in the student record abstract rather than the school fact sheet and was based on responses to multiple questions about start and end dates for school terms (e.g., semesters, trimesters). Composite variable names in past rounds started with a \"U\" prefix (rather than an \"L\" prefix as in spring-third grade) because they were taken from the student record abstract (variables for spring-first grade were U4SCHBDD, U4SCHBMM, U4SCHBYY, U4SCHEDD, U4SCHEMM, U4SCHEYY). If the start and end dates varied for children in the same school, the composite was created by using the school start and end dates reported for the majority of children in a school. Because school start and end dates were collected only once in the spring-third grade school fact sheet, discrepancies in questionnaire reports for children in the same school were not an issue."}, {"section_title": "L5SCHBDD", "text": ""}, {"section_title": "Student Record Abstract and Field Management System Composite Variables", "text": "The composite variables created from the student record abstract and FMS data follow. 7-36"}, {"section_title": "Year-Round Schools (F5YRRND)", "text": "This composite was created using data from the FMS. The FMS flag was \"1\" if the child was in year-round school. The values for the year-round school composite variable are 1 (Yes) and 2 (No). If the child was schooled at home, the composite was coded as -1 (Not Applicable)."}, {"section_title": "Indicator of Whether Child Received Special Education Services (F5SPECS)", "text": "The composite variable F5SPECS indicates whether or not the child received special education services in the spring of third grade, based on the presence or absence of a link to a special education teacher in the FMS in spring-kindergarten. The values are 1 if the child received special education services, 2 if the child did not receive special education services, and -9 if the link was missing between the child and his or her teacher in the FMS."}, {"section_title": "Indicator of Whether Child Had an Individualized Education Plan (IEP) on Record at", "text": ""}, {"section_title": "School (U5RIEP)", "text": "The variable U5RIEP indicates whether or not the child had an IEP or Individualized Family Service Plan (IFSP) on record at his/her school or another school in the spring of third grade. The values for the variable are 1 (child has an IEP/IFSP on record at his or her school, or at another school) and 2 (child does not have an IEP/IFSP on record at his or her school). If the information was missing, U5RIEP was coded as -9 (Not Ascertained)."}, {"section_title": "Parent Identifiers and Household Composition (P5DADID, P5MOMID, P5HPARNT, P5HDAD, P5HMOM, P5HFAMIL, P5MOMTYP, P5DADTYP)", "text": "The construction of parent identifiers and the household composition variables from the parent interview data was a two-step process. First, individuals identifying themselves as the child's mother/father were located within the household roster, and the type of their relationship to the child (biological, adoptive, foster, step, partner of parent, or unknown) was established. For households containing more than one father or mother, a hierarchy was used to designate the \"current,\" or residential, 7-37 parent of each gender. The biological parent, if present, was always the current mother or father. In the absence of a biological parent, the current mother/father designation was assigned to the adoptive, step, foster/guardian, partner, or \"unknown-type\" parent. If there were more than one father or mother of the same type, the one with the lower person number on the household roster was selected. Person number refers to the number each household member has on the roster list. Household members are listed in the It should be noted that because the composite construction identifies only one resident mother or one resident father, same-sex parents are not readily identified in the composites themselves. Two approaches can be used to identify these couples. First, the user should search the relationship variables (P5REL_1, etc.) to identify households in which more than one person is identified as a father/mother to the focal child. Second, since not all same-sex partners identify themselves as \"mother\" or \"father\" to the focal child, the user should also search for households in which the respondent (identified by P5PER_1, etc.) is the child's parent and the respondent's spouse/partner (identified from P5SPOUSE) is the same sex as the respondent. There are two sections in the parent interview that asked parent-figure-specific questions: Each of these sections was completed during the parent interview for up to two parents or parent figures. To indicate which household member or members were the subject of each section, \"pointer\" variables that hold the original number of the household member on the household roster were used. To illustrate how the pointer variables work, suppose there is a household with both a mother and a"}, {"section_title": "Service Occupations", "text": "The category includes occupations providing personal and protective services to individuals, and current maintenance and cleaning for building and residences. Some examples include food service, health service (e.g., aides or assistants), cleaning services other than household, and personal services."}, {"section_title": "Agricultural, Forestry, and Fishing Occupations", "text": "This category is concerned with the production, propagation (breeding/growing), gathering, and catching of animals, animal products, and plant products (timber, crop, and ornamental); the provision of services associated with agricultural production; and game farms, fisheries, and wildlife conservation. \"Other agricultural and related occupations\" include occupations concerned with the production and propagation of animals, animal products, plants, and products (crops and ornamental)."}, {"section_title": "Mechanics and Repairers", "text": "Mechanics and repairers are persons who do adjustment, maintenance, part replacement, and repair of tools, equipment, and machines. Installation may be included if installation is usually done in conjunction with other duties of the repairers."}, {"section_title": "Construction and Extractive Occupations", "text": "This category includes occupations that normally are performed at a specific site, which will change over time, in contrast to production workers, where the work is usually at a fixed location. Construction workers include those in overall construction, 7-43 brick masons, stonemasons, carpenters, electricians, drywall installers, paperhangers and painters, etc. Extractive occupations include oil well drillers, mining machine operators, and so on."}, {"section_title": "Precision Production Occupations", "text": "Precision production includes occupations concerned with performing production tasks that require a high degree of precision or attainment of rigid specification and operating plants or large systems. Examples are tool and die makers, pattern and model makers, machinists, jewelers, engravers, and so on. Also included are some food-related occupations including butchers and bakers. Plant and system operators include water and sewage, gas, power, chemical, petroleum, and other plant or system operators."}, {"section_title": "Production Working Occupations", "text": "This category includes occupations concerned with setting up, operating, and tending of machines and hand production work usually in a factory or other fixed place of business."}, {"section_title": "Transportation and Material Moving Occupations", "text": "This category includes occupations concerned with operating and controlling equipment used to facilitate the movement of people or materials and the supervising of those workers."}, {"section_title": "Handlers, Equipment Cleaners, Helpers, and Laborers", "text": "This category includes occupations that involve helping other workers and performing routine nonmachine tasks. A wide variety of helpers, handlers, etc., are included in this category. Examples include construction laborers, freight, stock, and material movers, garage and service station related occupations, parking lot attendants, and vehicle washers and equipment cleaners."}, {"section_title": "Unemployed, Retired, Disabled, or Unclassified Workers", "text": "This category includes persons who are unemployed, have retired from the work force, or are disabled. It also includes unclassified occupations that do not fit into the categories above (e.g., occupations that are strictly military, such as \"tank crew member\" and \"infantryman\")."}, {"section_title": "Methodological Variables", "text": "To facilitate methodological research, eleven new variables were added to the third grade file. The identifiers for parent interview work area (F5PWKARE), parent interviewer (F5PINTVR), child 7-44 assessment work area (F5CWKARE), and child assessor (F5CASSOR) were extracted from the field management system. Start and end times for both the child assessment (C5ASMSTM, C5ASMETM) and the parent interview (P5INTSTM, P5INTETM) were created from keystroke-by-keystroke records of each parent interview and child assessment. All four are text variables in the form MM/DD/YY hour:minute:second AM/PM. It should be noted that there may be more than one attempt to complete an interview or assessment, and those attempts could span several days. For example, an interviewer could begin a parent interview on one evening and complete the remainder of the interview several days later. For this reason, variables to indicate the number of attempts necessary to complete the parent interview (P5ATTMPT) and the number of attempts necessary to complete the child assessment (C5APPMPT) have also been included on the file. Finally, an indicator variable (F5PREFCV, Parent Interview Refusal Conversion) was created to flag cases that had, at any time, refused to respond to the parent interview but then agreed to participate. The values for F5PREFCV are 1=YES (refused but were converted to be a participant) and 2=NO (did not refuse)."}, {"section_title": "Children Who Changed Schools", "text": "There are several variables in the file that can be used to determine if a child moved to a different school between rounds or moved to a different school during the third grade data collection period."}, {"section_title": "Children Who Changed Schools During Third Grade Data Collection", "text": "The variable S5_ID is a school identification number that indicates which school the child was in at the end of the third grade data collection. There is another school ID variable, S5_ST_ID, that indicates where the child was at the beginning of the round. By comparing school ID variables, users can determine whether the child physically moved from one school to another during round 5. For the vast majority of the children these two variables will be identical, but for those who moved during the data collection period they will be different. If it was not known where the child was at the beginning or the end of the round, the scheme shown in table 7-11 for assigning ID numbers was used. 9 In previous rounds of the study, the code \"9994\" was used to indicate that the student was deceased. This code was not needed in third grade and thus does not appear on the spring-third grade file. 9995 End of field period. Information on the transfer student's new school was identified too late in the field period for the case to be re-fielded for the assessment. 9996 Moved to nonsampled PSU. The transfer student enrolled in a school that was outside of ECLS-K's sampled PSUs-field staff did not attempt to collect the assessment but did attempt to collect the parent interview. 9997 Do not follow. The transfer student was flagged by the statisticians as \"do not follow\" because of subsampling of transfer students due to cost constraints. If the child moved from his/her original school, field staff did not \"follow\" him or her to the new school and did not collect a child assessment or parent interview. In addition, there are other variables on the file that identify the origin of data for children who moved within the current round. These are helpful, for example, if a child changed schools within the 2001-02 school year and there are data from the first school the child attended (the starting school for the round, S5_ST_ID) but not the second school (the ending school for the round, S5_ID). The procedures for locating children within schools were as follows: Children's schools were identified at the beginning of the 2001-02 school year. If data collectors went to a school in the spring and found that a child was no longer there, the child was followed to his/her transfer school and data collection was attempted at the new school. If data were obtained from the new school, they were used. If data were not obtained and teacher or school data from the first school were available, those data were used."}, {"section_title": "7-46", "text": "The names and labels for the variables that can be used to determine whether data came from starting or ending schools are as follows: identification number from which the child or school data were obtained. For users interested in the source of the data for children who moved within the round, they should match the original school or teacher IDs with the first 9 instrument-specific IDs above to determine if the data for a given instrument were collected from the starting school or the ending school. The flag F5NOTEND can also be used to identify children for whom some data were from a school or teacher other than the ending school or ending teacher. There are 147 children with \"F5NOTEND=TRUE\" flags. The following scenarios illustrate how these variables may appear in the files: Child has same starting and ending school. In this case, the data from the school and teachers are all from the same location and the 9 instrument-specific IDs above will match the appropriate ending school and teacher IDs. The \"overall\" IDs containing the correct end-of-round data are S5_ID (school), T5_ID (regular teacher), and D5T_ID (special education teacher). Thus, if the child had the same starting and ending school, S5_S_ID would match S5_ID; A5_T_ID, B5_T_ID, and T5_T_ID would match T5_ID, etc. Child has same starting and ending school, no data from either source. In this case, the school/teacher IDs will be from the ending school. The 9 instrument-specific IDs above will be system missing, as will the rest of the data from those instruments. For"}, {"section_title": "7-47", "text": "instance, most children do not have a special education teacher. The special education data are all missing, including the Special Education A ID (D5_T_ID ) and the Special Education B ID (E5_T_ID). The same is true if there are no data from other sources, such as the teacher questionnaire A. If there were no hard-copy data (but there was a child assessment or parent interview), then all the hard-copy instrument data including the instrument IDs would be system missing. Child has different starting and ending schools, all data from ending school. The 9 instrument-specific school/teacher IDs above will be from the ending school. Child has different starting and ending schools, some data from ending school, some data from starting school. The school ID (S5_ID) or teacher ID will be from the ending school, and each of the 9 instrument-specific flags will indicate the ending school or the starting school depending on which school was the source of the instrument. Child has different starting and ending schools, ending school is unknown, child has a few instruments from starting school. If there is nonresponse from the ending school, S5_ID will be coded with a \"999n\" number described in table 7-11 to appropriately reflect the type of nonresponse. If there are instrument-specific data, the instrument IDs will be filled. If not, the instrument-specific IDs will be blank. Child has same starting and ending school. User is interested in whether there are special education data for the child. If there is no special education teacher, D5T_ID will be missing. If there is a special education teacher, D5T_ID will be filled. In either case, it should be noted that there could be missing data for special education data in the part B questionnaire. It is left to users to determine how they would like to set \"Not Applicable\" versus \"Not Ascertained\" codes for such combinations. For users interested in links to special education services, regardless of whether the source of the information was the starting or ending school, the composite variable F5SPECS that is based on information from the FMS system rather than the receipt of particular special education questionnaires can be used."}, {"section_title": "Children Who Changed Schools Between Rounds (R5DEST, R5R4SCHG)", "text": "Children moved between schools for a variety of reasons, but one factor was that a school terminated before the third grade and most of the students went to third grade at another particular school. This is known as a \"destination school\" and the move is known as a \"destination move.\" Destination schools were schools for which it was determined (during a fall data collection conducted to locate children prior to the spring data collection) that at least four ECLS-K children would move into them from a school that ended before the third grade or a school that had closed. The variable on the file that indicates destination moves is R5DEST (moved to a spring-third grade destination school)."}, {"section_title": "7-48", "text": "It should be noted that the destination school may also have been an originally sampled school; in this case, the school was a destination school only for the new students, not for the originally sampled students. The variable R5DEST was set to 1 (True) if a child made a school change and destination move to a spring-third grade destination school. If a child did not move to a spring-third grade destination school or did not move between schools at all, the composite is coded 0 (False). If the data are missing about whether the move was a destination move, the composite was coded -9 (Not Ascertained). If the child was schooled at home, the composite was coded as -1 (Not Applicable). Another variable on the file that will be of interest to users examining school change is R5R4SCHG (school type change between spring-first grade and spring-third grade). It is used in the creation of R5DEST (R5R4SCHG must indicate a school change for R5DEST to be set to \"1\"). It indicates whether the child changed schools and, if so, what the school type was in the previous and new school (e.g., whether the change was from public to private school, private to private school, etc.). R5R4SCHG is created by comparing the school IDs from spring-first grade and spring-third grade for children who were in the spring-first grade data collection. A difference in IDs indicated a change. If there was no difference in ID's, R5R4SCHG was coded 1 (child did not change schools). For children who changed schools, the spring-first grade school type variable S4SCTYP was compared to the springthird grade school type variable S5SCTYP. Categories were assigned as appropriate (2 = child transferred from public to public; 3 = child transferred from private to private; 4 = child transferred from public to private; 5 = child transferred from private to public; and 6 = child transferred, other). Category six was used for those children who transferred schools, but school type was unknown. Children who were not in the spring-first grade data collection were coded \"Not Ascertained\" on R5R4SCHG. Children who were home-schooled in spring-first grade or spring-third grade were coded \"Not Applicable\" for R5R4SCHG. Table   Table 7-12 describes the composite and derived variables that are on the ECLS-K child catalog. Please note that a few of the variables specified in the \"derived from\" column are intermediary variables that were not included in the final data sets. The \"derived from\" column also contains the item numbers from the questionnaire, which help in identifying the items that were used in the creation of these composites. This information allows a user to decide if he or she would like to use the composite based on how it was defined. "}, {"section_title": "Composite", "text": ""}, {"section_title": "Masked Variables", "text": "All the variables from the ECLS-K restricted-use file are included in the same order on the ECLS-K public-use file. For some of the variables, certain categories were modified. The value labels for those masked variables were updated from the restricted-use variables to reflect the new categories that were created during the masking process. Variables on the restricted-use files were modified in different ways based on the disclosure analysis NCES conducted in order to protect the identity of the respondents and children. There are several types of modifications on the public-use files. Outliers are top-or bottom-coded to prevent identification of unique schools, teachers, parents, and children without affecting overall data quality. Certain schools identified as at risk for disclosure have a 5 to 10 percent noise introduced in those variables that pose a risk for disclosure. Variables with too few cases and a sparse distribution are suppressed in the public-use files. The values for these variables were set to -2 and labeled \"suppressed\" in the ECB. For one group of variables, values were modified by \"data swapping.\" This process removes a reported value and replaces it with a reported value from a different respondent for a subset of the records.       Recoded from values of 0-100 to the following: 1=Less than 1% 2=1% to less than 5% 3=5% to less than 10% 4=10% to less than 25% 5=25% or more A5PBLK Teacher Percent of Blacks in classchild-level data A5BLACK (TQA item Q4c), A5CLSZ Recoded from values of 0-100 to the following: 1=Less than 1% 2=1% to less than 5% 3=5% to less than 10% 4=10% to less than 25% 5=25% or more A5PHIS Teacher Percent of Hispanics in classchild-level data A5HISP (TQA item Q4b), A5CLSZ (composite) Recoded from values of 0-100 to the following: 1=Less than 1% 2=1% to less than 5% 3=5% to less than 10% 4=10% to less than 25% 5=25% or more Recoded from values of 0-100 to the following: 1=Less than 10% 2=10% to less than 25% 3=25% to less than 50% 4=50% to less than 75% 5=75% or more See note at end of table. Recoded from values of 0-100 to the following: 1=Less than 10% 2=10% to less than 25% 3=25% to less than 50% 4=50% to less than 75% 5=75% or more See note at end of table. Recoded from values of 0-100 to the following: 1=Less than 1% 2=1% to less than 5% 3=5% to less than 10% 4=10% to less than 25% 5=25% or more "}, {"section_title": "7-67", "text": ""}, {"section_title": "7-71", "text": ""}, {"section_title": "7-73", "text": "Certain continuous variables are modified into categorical variables, and certain categorical variables have their categories collapsed in the public-use file. While this protects the cases from a disclosure risk, these variables can still be used in all different kinds of analysis such as regression analysis. There is a comment field in the variable frequency distribution view screen of the electronic code book that displays a comment for each masked variable indicating whether the variable from the restricted-use file has been recoded or suppressed in the public-use file. Variables that were recoded in any way during the data masking process display the comment, \"These data recoded for respondent confidentiality.\" Variables that were suppressed on the public-use file for protection of the respondent or child from identification display the comment, \"These data suppressed for respondent confidentiality\" and all values for the variable are set to equal -2 for that variable. All variables from the special education teacher questionnaire part A (i.e., all variables with the prefix D5), from the special education teacher questionnaire part B (i.e., all variables with the prefix E5), and from the student record abstract (i.e., all variables with the prefix U5) have been suppressed in the third grade public-use file. Included in this group of suppressed variables are all teacher and school identifiers, which have last two characters \"ID\" and prefix D5, E5, or U5. For brevity, these variables are not included in table 7-13. These functions allow users to access the accompanying catalog and \"view\" the data in various ways by performing customized searches, queries, and extractions. The organization of this document provides a \"start to finish\" approach through the system, beginning with the installation of the ECB, utilizing the ECB's functions, navigating through the catalog, and performing user-specified data extractions. "}, {"section_title": "7-77", "text": ""}, {"section_title": "4.", "text": "Select the Settings tab.\nThe selected variable is highlighted.\nDo children who easily adapted to a school setting in kindergarten do better in third grade than their peers who experienced more difficulty settling into school or are there any lingering effects of a slow adjustment to kindergarten? 5. Are there particular school or classroom characteristics that enhance growth rates in reading and math skills between first and third grade? To study these and similar questions, researchers would combine information from two or more rounds of data collection, across the kindergarten, first, and third grade years. For the first question, the researcher would need to examine differences between fall-kindergarten and spring-third grade assessment scores. To do this, one would combine fall-kindergarten data with spring-third grade data. Similarly, questions 2 and 3 (regarding the relationship between readiness at kindergarten entry-or maternal employment in that time frame-and third grade outcomes) would be examined by combining data from the same two time points. Note that for question 3 one would need to include data from the parent interview in the base year. Researchers who want to examine the influence of children's kindergarten adjustment on their later grade performance, as in question 4, might use data from several rounds (i.e., fall-kindergarten, spring-kindergarten, spring-first grade, and spring-third grade). For example, one could create variables from fall-kindergarten and spring-kindergarten to measure adjustment during kindergarten and then relate those variables to outcomes in the spring of the first and third grades. To be assigned a longitudinal weight for the K-3 data, a case must have participated in at least one of the base year rounds, and in both spring-first grade and spring-third grade. Thus, the K-3 longitudinal weights should not be used to examine questions that only use data from the base year and the first grade years. For such analyses, it is advisable to use the K-1 longitudinal weights.\nRun the program generated after extraction to create a first grade data set (DATA2)."}, {"section_title": "5.", "text": "Set the Desktop Area to 800 x 600 pixels with the Desktop Area slidebar. The ECB requires approximately 20 MB of available disk space on your hard drive. If 20 MB of space is not available, you may wish to delete unnecessary files from the drive to make space for the ECB.\nFollow any prompts. You will be prompted by the InstallShield Wizard to confirm the uninstallation and finish the process.\nThe field ID of the current variable selected is shown on the right of the Go button (exhibit 8-18).\nClick on the Search button to initiate the search.\nUsing the child catalog from the Third Grade Restricted-Use ECB, select the variables to be analyzed and the variable CHILDID."}, {"section_title": "ECB Features", "text": "The ECB allows a user to do the following: Search the names and labels of variables in the database (called the catalog) to select variables for analysis (see section 8.3, Variable List). Examine the question wording, response categories, and response frequencies for variables the user selects (see section 8.4.9, Viewing Code Book and Variable Information). Create a list of variables to be extracted from the catalog, save the list for later use, print the list as a code book, or use a predefined list on the ECB (see section 8.4, Working Taglist). Automatically generate SAS, SPSS for Windows, or Stata programs to extract selected variables from the whole data set or for a subset of the cases that are defined by the user (see section 8.5, Extracting Data from the ECB)."}, {"section_title": "8-3", "text": "The ECB does not create a SAS, SPSS for Windows, or Stata data file. It will prepare the statements that you can use with your own SAS, SPSS for Windows, or Stata software to create your file. As noted earlier, the CD-ROM contains an ASCII data set that the ECB uses to extract specific subdata files. The CD-ROM must be in the drive for the data to be extracted."}, {"section_title": "Installing, Starting, and Exiting the ECB", "text": "The ECB is provided on a CD-ROM and is intended to be installed and run from within the "}, {"section_title": "Installing the ECB Program on Your Personal Computer", "text": "Program installation is initiated by running the Setup.exe file found within the CD-ROM's root directory."}, {"section_title": "6.", "text": "The program is designed so that the uninstallation will keep the taglists when the ECB program is uninstalled in order that all the saved taglists will be retained when the ECB is reinstalled. As a result, the uninstallation will not remove the directory where the ECB was located.\nClick the Reset button to return to the top of the original Variable List (Field ID 1) or enter another field ID to scroll to another variable. For field IDs that identify different groups of variables, please refer to exhibit 8-60 for the catalog-specific topical variable groupings. The Go button will not be available in a narrowed or expanded list. After a Narrow Search or an Expand Search, you must reset the Variable List (see section 8.3.1.4) before you can use the Go button.\nThe variables meeting the specified criteria will be displayed in the Variables List column.\nRun the program generated after extraction to create a third grade data set (DATA3)."}, {"section_title": "Title Bar", "text": "The Title Bar, shown below in exhibit 8-14, is the horizontal bar located at the top of the main screen. It will list the name of the program and the catalog that you have opened, and it will indicate that you are in the \"Create Taglist\" mode. "}, {"section_title": "8-13", "text": ""}, {"section_title": "Using Shortcut Keys to Navigate", "text": "The shortcut keys provide a means for selecting menu options and screen buttons without the use of a mouse. These shortcut keys are identified by an underscore under the shortcut letter within the option or button label. The menus that appear on the windows are activated by simultaneously selecting the <ALT> key and the underscored letter. An example of this is the activation of the Taglist Menu by selecting the key combination of <ALT>-<T>. Once the menu is activated and all options are displayed, the options can be selected by then pressing the underscored letter for the desired option or by pressing the arrow keys to move between the options. Not all screens have shortcut keys. They may, however, be used without mouse capability by pressing the <TAB> key. The <TAB> key moves the cursor or highlight through the options and buttons within the windows. When the desired option or button is highlighted, it can be selected by pressing the <ENTER> key."}, {"section_title": "Variable List", "text": "The ECB main screen, shown in exhibit 8-16, comprises two primary lists that each provide functions for reviewing, grouping, and extracting variable data from the opened catalog. These lists  How To Use the Go Button: 1. Type the field ID in the input box on the left of the Go button."}, {"section_title": "8-17", "text": "The \"Field ID\" remains active in a narrowed or expanded list. However, the field IDs indicate the order of the variables in the catalog rather than that in the Variable List. As a result, the field IDs would not change in a narrowed or expanded list. Enter a key character string, word, or phrase in the Enter Narrow Text field. Character strings can include a single alphanumeric character or a sequence of several characters. The search is not case sensitive. The results returned will be all entries that contain that exact sequence of letters, numbers, spaces, and words."}, {"section_title": "Click in the Variable Name, Variable Description, or Both Variable Name and", "text": "Description radio button to specify where to search."}, {"section_title": "8-19", "text": "If no variable names or descriptions in the catalog contain the specified search text, then the message shown in exhibit 8-20 will appear. Exhibit [8][9][10][11][12][13][14][15][16][17][18][19][20]. No Matches Found message"}, {"section_title": "7.", "text": "Repeat the Narrow Search procedure if necessary. Please note that the field ID at the upper right corner of the Variable List reflects the order of the variables in the catalog rather than that in the narrowed Variable List.\nTo print selected pages of the code book, select Pages from the Printer Dialog box. Enter the pages you want to print and the number of copies you want. Click on the OK button.\nSort DATA1 and DATA2 and DATA3 by CHILDID."}, {"section_title": "Example of Narrowing a Search", "text": "The following example shows you how to narrow the Variable List. In this example, you want to include all the variables from the catalog that measure education. Do the following: 1. In the Variable List, click on the Narrow button."}, {"section_title": "Working Taglist", "text": "The Working Taglist, shown in exhibit 8-24, displays a list of variables that are currently selected or tagged for extraction. All Working Taglists contain a set of variables, called required variables that will be automatically included in all data files that the user creates. The required variables provide a foundational data set upon which other variables rely. These required variables cannot be untagged or deleted from the Working Taglist by the user. When a catalog is first opened, the default Working Taglist consists of only the required variables for that catalog. (See exhibit 8-58 for the catalog-specific required variables.) To create a taglist, add the variables you have selected to the required variables."}, {"section_title": "8-23", "text": "Exhibit 8-24. ECB Working Taglist"}, {"section_title": "Opening a Taglist", "text": "The ECB allows you to open a predefined or previously saved taglist and display it in the Working Taglist column. Taglists, however, are saved as part of a particular catalog and can only be opened as part of the associated catalog."}, {"section_title": "8.", "text": "When you are done viewing the entire code book, close the window by clicking on the Windows control \"X\" in the upper right corner of the screen. You will return to the main screen.\nMerge DATA1 and DATA2 and DATA3 by CHILDID. This merged file will contain 21,409 cases, some of which will not have K-3 longitudinal weights. For example, base year respondents who did not participate in either fall or spring of first grade or spring of third grade, and movers who were not included in the first grade and third grade sample, will not have any K-3 longitudinal weights. To select cases with K-3 longitudinal data, a user can use a K-3 longitudinal weight appropriate to the analysis. C45CW0 is nonzero if assessment data are present for both spring-first grade and spring-third grade, or if the child was excluded from direct assessment in both of these rounds of data collection due to a disability;"}, {"section_title": "Extracting Data from the ECB", "text": "Once the variables have been selected (tagged) for extraction and reside in the Working Taglist, the next step is to generate the code through which the statistical analysis software can retrieve and display the results. The ECB provides options for generating the code for analyzing data with the SAS, SPSS for Windows, or Stata statistical analysis programs. To run these programs, you will need the appropriate statistical software and the ECB CD-ROM from which the program can extract data. SPSS users should note that an entire catalog can produce a Frequencies command statement with more than 500 variables. This may produce a warning of \"too many variables,\" and the Frequencies command will not execute. Users may work around this limitation by dividing the Variable List into two or more Frequencies commands. When extracting data to be used with either the SAS, SPSS for Windows, or Stata programs, a dialog box will be presented that allows the user to define the extract population through the Limiting Fields. See exhibit 8-46. The Limiting Fields include various subgroups of respondents that are typically of interest to analysts. These subgroups can be selected or deselected to narrow the data field that is extracted. Also, please note that the ECB extract function allows the user to specify the drive letter of the CD-ROM drive. If you attempt to run the resulting SAS, SPSS, and Stata programs on a workstation with a different CD-ROM drive letter, you must alter the program code accordingly or regenerate the program code using the ECB."}, {"section_title": "8-42", "text": "The SAS, SPSS, or Stata source code generated by the ECB to read in the data may contain code statements that are \"commented\" out (e.g., with * in SAS). These code statements either run descriptive statistics (e.g., frequencies, means, etc.), or associate formats with variables. They are commented out because not all analysts will want them included in the source code. SAS users (prior to SAS, Version 8) should note that, although the ECB will allow data set names larger than eight characters, the SAS system will reject these names at run-time. "}, {"section_title": "Third Grade Child Catalog Topical Variable Groupings", "text": "The variables within the third grade child catalog are organized into topical categories to help locate and identify desired variables in the ECB. These categories are identified in exhibit 8-60. The first column of the exhibit describes the topic, the second column lists the variable identifiers, which generally indicate the source of the data (e.g., the parent interview, the teacher questionnaires). The third column is a description of the topic. The last column is the Field ID, which is used to search the data file for the topics. Use the keywords in the Variable Identifier column to search for variables while using the 8-61"}, {"section_title": "Child Catalog Predefined Taglists", "text": "There is one predefined taglist provided with the child catalog that can be used for extracting data. This taglist, third grade, is defined in exhibit 8-61. The required variables, identified with \"Yes\" appearing in the Required field column, are the same as those listed in exhibit 8-58. The limiting variables, identified as \"Yes\" in the Limiting field column, are the same as those described in section 8.7.3."}, {"section_title": "Child Catalog Limiting Fields", "text": "The limiting fields for the child catalog include (a) whether the child attended kindergarten for the first time in the base year (base year variable), (b) child's grade level in spring 2002, and (c) child's school type (public vs. private) in spring 2002. These limiting fields are specific to the child catalog and allow codes within each variable to be included or excluded from the extraction depending on the selection indicator. For example, the user can select \"No\" for private schools and for cases with \"not ascertained\" or \"not applicable\" data on school type if he or she would like to create a data file that only includes public school third-graders. The selection indicator will be either a \"Yes\" or \"No\" to specify whether the variable code should be included or excluded, respectively. The limiting fields feature for the catalog allows the user to create a subset of cases based on the settings of the Select column in the Extract Specifications (exhibit 8-62) window. The default setting is all \"Yes\" in the Select column meaning that all records will be present in the extract file. To exclude records in a particular category of a variable, change the \"Yes\" associated with that code in the Select column to \"No\" by double-clicking on it. At least one of the codes for each limiting variable must be selected as \"Yes\" or no records will be extracted for analysis. For example, an extract using the default specifications above will include children from all grades. To restrict the extract to records for thirdgraders only, double-click the \"Yes\" next to all the classes other than third grade to change the other codes to \"No.\" 9-1"}, {"section_title": "CREATING A LONGITUDINAL FILE", "text": "Longitudinal analyses with the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K) can be conducted both \"within school year\" and \"across school years.\" Examples of within-year analyses are those that look at children's growth in cognitive scores between fall and spring of kindergarten or between fall and spring of first grade. Such analyses do not require the combined use of kindergarten and first grade data. They can be conducted using just the kindergarten base year files or just the first grade files. Therefore, within-school year analyses are not discussed in this chapter. Since data were only collected once for third grade, no within-third grade analyses are possible. Cross-year analyses, on the other hand, are those that combine information from two or more of the kindergarten, first grade, or third grade years and are the focus of this chapter. This chapter describes how to combine (or merge) the kindergarten, first grade, and third grade files to create cross-year files for K-3 longitudinal analyses. The information contained in this chapter applies to users of the base year, first grade, and third grade files. Users of the public-use files can consider using the public-use longitudinal file briefly described in chapter 1, which combines data from the base year, first, and third grades. It contains longitudinal weights so that analysts can examine children's growth and development between kindergarten and third grade. Although it is somewhat streamlined, it contains most of the variables in the restricted-use files. This chapter begins with a discussion of K-3 longitudinal analyses and the types of research questions that can be addressed with cross-year files. It then describes the merging procedures and the K-3 longitudinal weights."}, {"section_title": "Conducting Longitudinal Analyses", "text": "As described in chapter 1, one of the primary goals of the ECLS-K is to understand how children's early experiences influence their transition into kindergarten and their progression through the early elementary school years. A major strength of the ECLS-K design is that it captures important aspects of children's experiences as they occur. Thus, information about children's transition into kindergarten is measured in the fall of their kindergarten year and again in the spring. Capturing this information as it occurs means that the information is not distorted by faulty memory or by revisions to 9-2 memory based on subsequent experiences. In addition, information from earlier points in time can be used as predictors of later events and experiences, thereby strengthening the ability of researchers to make causal inferences. In conducting K-3 longitudinal analyses with the ECLS-K data, it is important to keep in mind the sample design described in chapter 4. Certain features of the design must be considered. "}, {"section_title": "Examples of Research Questions", "text": "A variety of research questions can be examined using the K-3 longitudinal files. The following are some examples: 1. How much do children's reading and math skills increase between the fall of kindergarten and the spring of third grade? 2. Do measures of school readiness at the beginning of kindergarten predict children's skill and knowledge levels at the end of third grade?"}, {"section_title": "Merging Base Year Child-Level Data With the First Grade and Third Grade Child-", "text": ""}, {"section_title": "Level Data", "text": "To create a K-3 data file, which combines data from the base year, first grade, and third grade data collections, an analyst should use the ECLS-K Base Year Restricted-Use or Public-Use Electronic Code Book (NCES 2000-097 or NCES 2001; the ECLS-K First Grade Restricted-Use or Public-Use Electronic Code Book (NCES 2002-127 or NCES 2002; and the ECLS-K Third Grade Restricted-Use or Public-Use Electronic Code Book (NCES 2003-002 or NCES 2004. The same procedures can be followed by users who wish to create a K-3 longitudinal file themselves using publicuse data files. To create a longitudinal file, perform the following steps to merge the base year child-level variables needed for analysis with the first grade and third grade child-level variables needed: 9-4"}, {"section_title": "1.", "text": "Select the variables to be analyzed from the base year ECB child catalog and the variable CHILDID. This creates a \"working taglist\" (see section 8.4 in chapter 8 for more detail on how to create a working taglist)."}, {"section_title": "K-3 Longitudinal", "text": "C45PW0 is nonzero if parent interview data are present for both spring-first grade and spring-third grade; C245CW0 is nonzero if assessment data are present for spring-kindergarten and spring-first grade and spring-third grade, or if the child was excluded from direct assessment in all of these three rounds of data collection due to a disability; C245PW0 is nonzero if parent interview data are present for spring-kindergarten and spring-first grade and spring-third grade; C1_5FC0 is nonzero if assessment data are present for four rounds of data collections involving the full sample of children (fall-kindergarten, spring-kindergarten, springfirst grade, and spring-third grade), or if the child was excluded from direct assessment in all four of these rounds of data collection due to a disability; C1_5FP0 is nonzero if parent interview data are present for four rounds of data collections involving the full sample of children (fall-kindergarten, springkindergarten, spring-first grade, and spring-third grade); C1_5SC0 is nonzero if assessment data are present for all five rounds of data collection (fall-kindergarten, spring-kindergarten, fall-first grade, spring-first grade, and spring-third grade), or if the child was excluded from direct assessment in all five rounds of data collection due to a disability; and C1_5SP0 is nonzero if parent interview data are present for all five rounds of data collection (fall-kindergarten, spring-kindergarten, fall-first grade, spring-first grade, and spring-third grade). The use of the K-3 longitudinal weights, available on the third grade restricted-use ECB, is described in exhibit 9-1. This exhibit is designed to help users choose appropriate weights for their analysis. First, decide which two or more points in time are the focus of the analysis. The analysis could pertain to two points in time (spring-first grade and spring-third grade), three points in time (springkindergarten, spring-first grade, and spring-third grade), four points in time (fall-kindergarten, springkindergarten, spring-first grade, and spring-third grade), or five points in time (all five rounds of data collection). For example, if the analysis uses spring-first grade and spring-third grade data, then the appropriate weights would be those beginning with C45 (denoting child-level data from round 4, springfirst grade AND round 5, spring-third grade). Second, consider the source of the data, which also affects the choice of the weight. In exhibit 9-1, details under \"to be used for analysis of \u2026\" provide guidance based on whether the data were collected through the child assessments, parent interviews, or teacher questionnaires A or B. For the same example noted earlier, the two weights available are C45CW0 and 9-6 C45PW0. If parent data from spring-first grade and spring-third grade are needed for the analysis, then C45PW0 should be used. Base year longitudinal weights for the analysis of the base year data (within the kindergarten year) alone are described in the base year user's manuals. First grade longitudinal weights for the analysis of the first grade data (within the first grade year) alone, and of the combined kindergarten/first grade data are described in the first grade user's manuals. K-3 longitudinal weights are used to produce estimates of differences between two or more rounds of data collection spanning across kindergarten, first grade, and third grade. Simple examples involving two rounds of data collection are the differences in children's mean assessment scores between spring-first grade and spring-third grade using the C45CW0 weight and the difference in the total number of persons in the household size using C45PW0. K-3 longitudinal weights are also used to study the characteristics of children who were assessed in two or more rounds of data collection. For example, one can study how family background characteristics of children in kindergarten affect assessment scores in spring-third grade for children who were assessed in spring-kindergarten, spring-first grade, and springthird grade. In this case, C245PW0 is used to study the characteristics of the children as reported by their parents, and C245CW0 is used to estimate the difference in assessment scores between springkindergarten and spring-third grade. As noted earlier, any longitudinal analysis that uses data from fallfirst grade will be limited to a 27 percent subsample of children."}, {"section_title": "9-7", "text": "Exhibit 9-1. ECLS-K: K-3 longitudinal weights, spring-third grade: School year 2001-02 Weight to be used for analysis of ..."}, {"section_title": "C45CW0", "text": "child direct assessment data from BOTH spring-first grade and spring-third grade, alone or in conjunction with any combination of a limited set of child characteristics (e.g., age, sex, race/ethnicity)."}, {"section_title": "C45PW0", "text": "parent interview data from BOTH spring-first grade and spring-third grade."}, {"section_title": "C245CW0", "text": "child direct assessment data from spring-kindergarten AND spring-first grade AND spring-third grade, alone or in conjunction with any combination of a limited set of child characteristics (e.g., age, sex, race/ethnicity)."}, {"section_title": "C245PW0", "text": "parent interview data from spring-kindergarten AND spring-first grade AND spring-third grade."}, {"section_title": "C1_5FC0", "text": "child direct assessment data from FOUR rounds of data collections involving the FULL sample of children (fall-kindergarten, spring-kindergarten, spring-first grade, spring-third grade), alone or in conjunction with any combination of a limited set of child characteristics (e.g., age, sex, race/ethnicity)."}, {"section_title": "C1_5FP0", "text": "parent interview data from FOUR rounds of data collections involving the FULL sample of children (fall-kindergarten, spring-kindergarten, spring-first grade, spring-third grade) C1_5SC0 child direct assessment data from ALL FIVE rounds of data collection, alone or in conjunction with any combination of a limited set of child characteristics (e.g., age, sex, and race/ethnicity)."}, {"section_title": "C1_5SP0", "text": "parent interview data from ALL FIVE rounds of data collection. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 third grade data collection, school year 2001-02. There may be combinations of data for which no weights were developed. For further advice on which weights to use when analyzing a complex combination of data, contact NCES at ECLS@ed.gov."}, {"section_title": "Longitudinal Weights Not Involving the Fall-First Grade Data", "text": "The first stage of weighting was to compute an initial child weight that reflects the following: Adjustment of the school base weight for base year school-level nonresponse; Adjustment of the child weights for base year child-level nonresponse; and Adjustment of the base year child weight for subsampling of schools for freshening in first grade (for children sampled in first grade only). The second stage of weighting was to adjust the initial child weight computed in the first stage for the following: Subsampling of movers and Child-level nonresponse. In the adjustment for subsampling of movers, mover status was created so that it was specific to each panel. For example, for the spring-first grade/spring-third grade panel (longitudinal weights C45CW0 and C45PW0), a child was a mover if he or she was a mover in spring-first grade or spring-third grade, i.e., in either round he attended a school that was not the school where he was sampled in kindergarten. The adjustment factor for subsampling movers was computed within cells created using the following characteristics: whether children were sampled in kindergarten or first grade, whether they were movers in spring-first grade, whether they were language minority children, the school type of their original sample school, and the region where their original sample school was located. Eight children with large weights had their weights trimmed by half. However, the weights were not redistributed because the total sum of weights was re-established in the raking procedure that came later. In both steps of the nonresponse adjustment, separate nonresponse classes were created for movers and nonmovers using various combinations of response status of child assessments and parent interviews in the base year, as well as whether children belonged to the language minority group (all weights), and the type of household collected from the parent interviews (C45PW0, C245PW0, C1_5FP0 only)."}, {"section_title": "9-9", "text": "The third and last stage was to rake the weights adjusted in the second stage to sample-based control totals. The raking factor was computed separately within raking cells as the sample-based control total for the raking cell over the sum of the nonresponse adjusted weights for children in the same cell. Raking cells (also known as raking dimensions) were created using school and child characteristics collected in the base year or first grade year: school type, region, urbanicity, sex, age, race/ethnicity, socioeconomic status (SES), language minority status, whether sampled in kindergarten or first grade, and if sampled in kindergarten, mover status."}, {"section_title": "Longitudinal Weights Involving the Fall-First Grade Data", "text": "For the longitudinal weights involving the fall-first grade data collection where children were part of a subsample of the ECLS-K full sample, the initial weights were from fall-first grade. These were the base year child adjusted weights (as described in section 4.6.3.2 for base year respondents), incorporating the school subsampling factor appropriate for fall-first grade. These weights were also trimmed to reduce the weight of all the children in one private school that had a large school weight. The adjustments for subsampling movers and for child nonresponse are identical to those for the other longitudinal weights. The adjustment factor for subsampling movers was computed within cells created using the following characteristics: whether children moved in fall-first grade or spring-first grade and whether they belonged in the language minority group. One child with large weights had his weight trimmed by half. However, the weights were not redistributed because the total sum of weights was reestablished in the raking procedure that came later. In both steps of the nonresponse adjustment for the C1_5SC0 weight, separate nonresponse classes were created for movers and nonmovers using the parent interview response status from the base year as well as whether children belonged to the language minority group. For the C1_5SP0 weight, nonresponse classes were created using the type of household collected from the parent interviews and whether children belonged to the language minority group. The raking dimensions are the same as those for the other longitudinal weights. After the first raking for the C1_5SC0 weight, four children had their weights trimmed, then all the weights were raked again. Only one raking was necessary for the C1_5SP0 weight. 9-10"}, {"section_title": "Characteristics of Longitudinal Weights", "text": "The statistical characteristics of the longitudinal weights are presented in table 9-1. For each weight, the number of cases with nonzero values is presented together with the mean weight, the standard deviation, the coefficient of variation (i.e., the standard deviation as a percentage of the mean weight), the minimum value of the weight, the maximum value of the weight, the skewness, the kurtosis, and the sum of weights. The difference in the estimate of the population of students (sum of weights) between the different panels of students and types of weights is due to a combination of factors, among them: (1) the number of base year respondents who became ineligible (due to death, leaving the country, or being a nonsampled mover) after the base year, (2) the adjustment of the weights for the children of unknown eligibility, and (3) the difference in the number of records used to construct sample-based control totals. Of the eight longitudinal weights computed, only the first two (C45CW0 and C45PW0) involve children sampled in first grade. For these two weights, the child records included in the file used for computing the control totals are records of base year respondents and records of eligible children sampled in first grade. For all other longitudinal weights, records of children sampled in first grade were not included in the file, causing the sum of weights to be smaller. 9-11"}, {"section_title": "9-12", "text": "Exhibit 9-2. ECLS-K Taylor Series stratum and first-stage unit identifiers, spring-third grade: School year 2001-02 Variable name Description"}, {"section_title": "C45CSTR", "text": "Sampling stratum-spring-first grade/spring-third grade longitudinal C-weights C45CPSU First-stage primary sampling unit within stratum-spring-first grade/spring-third grade longitudinal C-weights"}, {"section_title": "C45PSTR", "text": "Sampling stratum-spring-first grade/spring-third grade longitudinal P-weights C45PPSU First-stage primary sampling unit within stratum-spring-first grade/spring-third grade longitudinal P-weights"}, {"section_title": "C245CSTR", "text": "Sampling stratum-spring-kindergarten/spring-first grade/spring-third grade longitudinal C-weights"}, {"section_title": "C245CPSU", "text": "First-stage primary sampling unit within stratum-spring-kindergarten/spring-first grade/spring-third grade longitudinal C-weights"}, {"section_title": "C245PSTR", "text": "Sampling stratum-spring-kindergarten/spring-first grade/spring-third grade longitudinal P-weights C245PPSU First-stage primary sampling unit within stratum-spring-kindergarten/spring-first grade/spring-third grade longitudinal P-weights"}, {"section_title": "C15FCSTR", "text": "Sampling stratum-fall-kindergarten/spring-kindergarten/spring-first grade/spring-third grade longitudinal C-weights C15FCPSU First-stage primary sampling unit within stratum-fall-kindergarten/springkindergarten/spring-first grade/spring-third grade longitudinal C-weights"}, {"section_title": "C15FPSTR", "text": "Sampling stratum-fall-kindergarten/spring-kindergarten/spring-first grade/spring-third grade longitudinal P-weights C15FPPSU First-stage primary sampling unit within stratum-fall-kindergarten/springkindergarten/spring-first grade/spring-third grade longitudinal P-weights"}, {"section_title": "C15SCSTR", "text": "Sampling stratum-longitudinal C-weights covering all five rounds of data collection C15SCPSU First-stage primary sampling unit within stratum-longitudinal C-weights covering all five rounds of data collection C15SPSTR Sampling stratum-longitudinal P-weights covering all five rounds of data collection C15SPPSU First-stage primary sampling unit within stratum-longitudinal P-weights covering all five rounds of data collection SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 third grade data collection, school year 2001-02. 1 WR = with replacement, specified only if using SUDAAN. WR is the only option available if using SAS, Stata, or AM. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 third grade data collection, school year 2001-02."}, {"section_title": "9-13", "text": ""}, {"section_title": "9-14", "text": "For the replication method using WesVar, the full sample weight, the replicate weights, and the method of replication are required parameters. Variance estimation using the ECLS-K data should be done using the paired jackknife method (JK2). As an example, to compute the mean difference in reading scores between spring-kindergarten and spring-first grade and their standard errors, users need to specify C45CW0 as the full sample weight, C45CW1 to C45CW90 as the replicate weights, and JK2 as the method of replication. For the Taylor Series method using SUDAAN, SAS, Stata, or AM the full sample weight, the sample design, the nesting stratum, and PSU variables are required. For the same example earlier, the full sample weight (C45CW0), the stratum variable (C45CSTR), and the PSU variable (C45CPSU) must be specified. The \"with replacement\" sample design option, WR, must also be specified if using SUDAAN."}, {"section_title": "9-15", "text": "2 SRS SE is the standard error assuming simple random sample. For an explanation of this statistic, see chapter 4, section 4.8. 3 DEFT is the root design effect. For an explanation of this statistic, see chapter 4, section 4.8. 4 DEFF is the design effect. For an explanation of this statistic, see chapter 4, section 4.8. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 kindergarten, first grade, and third grade data collections, school years 1998-99, 1999-2000, and 2001-02. 9-21 Table 9-6. ECLS-K, panel of all five rounds: standard errors and design effects for the full sample using C1_5SC0-C1_5SC40 and C1_5SP0-C1_5SP40, by selected child and parent variables: School years 1998School years -99, 1999School years -2000School years , and 2001 Survey 9-22 Table 9-6. ECLS-K, panel of all five rounds: standard errors and design effects for the full sample using C1_5SC0-C1_5SC40 and C1_5SP0-C1_5SP40, by selected child and parent variables: School years 1998School years -99, 1999School years -2000School years , and 2001 1 Design SE is the standard error under the ECLS-K sample design. For an explanation of this statistic, see chapter 4, section 4.8. 2 SRS SE is the standard error assuming simple random sample. For an explanation of this statistic, see chapter 4, section 4.8. 3 DEFT is the root design effect. For an explanation of this statistic, see chapter 4, section 4.8. 4 DEFF is the design effect. For an explanation of this statistic, see chapter 4, section 4.8. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 kindergarten, first grade, and third grade data collections, school years 1998-99, 1999-2000, and 2001-02. The median design effect is 3.2 for the spring-first grade/spring-third grade panel, 3.0 for spring-kindergarten/spring-first grade/spring-third grade panel, 2.9 for the panel of children in all four rounds of data collection involving the full sample of children, and 2.6 for the panel of children in all five rounds of data collection that included children sampled for fall-first grade only. Table 9-7 presents the median design effects for subgroups based on school type, child's sex and race/ethnicity, geographic region, level of urbanicity, and the socioeconomic status scales of the parents. For the panels that include the full sample of children, the median design effect is lowest for Pacific Islanders (hovering around 1.2 or 1.3) and highest for children in Catholic schools (around 3.4 or 3.5). For the panel involving all five rounds, the range of variability of the median design effects is very different from all other panels. The all-five-round panel has a much reduced sample size, as 9-23 Table 9-7. ECLS-K panel: median design effects for subgroups, kindergarten through third grade: School years 1998School years -99, 1999School years -2000School years , and 2001 Spring-first grade/  1998-99, 1999-2000, and 2001-02. 9-24 it includes the fall-first grade subsample from the full base year sample. For this panel, the median design effects range from 1.5 for children of lowest socioeconomic level to a high of 4.1 for Pacific Islanders and 4.5 for American Indians. In this reduced panel, the samples of Pacific Islanders and American Indians are highly clustered, resulting in the higher design effects. Standard errors and design effects were not computed for items from the teacher and school administrator questionnaires since there are no teacher or school weights computed for spring-third grade year. Although standard errors and design effects may also be calculated for the teacher and school administrator questionnaires at the child level, they are quite large compared to those typically found for the ECLS-K data. Design effects for teacher and school items are large because the intraclass correlation is 100 percent for children in the same school and very high for children in the same class; children attending the same school have the same school data, and children in the same class have the same teacher data. The correlation is not 100 percent for children in the same class because teacher data include not only items about the teacher and the class but also items about the individual students as completed by their teachers."}]