[{"section_title": "", "text": "School quality, broadly defined, is an important predictor of educational attainment and labor-market success (e.g., Altonji & Mansfield, 2011). School size is one potential measure of school quality over which policymakers have some control. For example, the school consolidation movement in the United States in the middle of the 20th century was predicated on the notion that larger schools could offer more specialized instruction, increase administrative efficiency, and reduce per-student costs by exploiting economies of scale (e.g., Berry, 2006;Conant, 1959;Duncombe & Yinger, 2007). The movement successfully eliminated about 70% of schools and increased the average school enrollment from less than 100 to about 440 between 1930 and 1970 (Berry & West, 2010). More recently, primary school enrollments have been affected by desegregation policies such as Boston's Metco program (Angrist & Lang, 2004) and the increasing prevalence of school-closure and school-choice programs (Brummet, 2014; Common Core of Data [CCD], 2011; Ravitch, 2011). 1 Today, the average U.S. primary school enrolls about 480 students and experiences annual enrollment fluctuations of about 30 students. 2 However, the benefits of larger schools described in the preceding paragraph come at a cost, as larger schools have higher rates of student absences and social disorder that may hinder cognitive and social development (Gottfredson & DiPietro, 2011). Perhaps unsurprisingly, empirical evidence on the relationship between school size and academic performance is mixed (see Andrews, Duncombe, and Yinger, 2002;Cotton, 1996;and Leithwood and Jantzi, 2009, for thorough reviews of the literature). Kuziemko (2006) speculates that the \"current confusion in the literature\" is at least partly driven by the cross-sectional, correlational nature of many previous studies of school size. Similarly, Andrews et al. (2002) lament the literature's 136S general failure to utilize student-level data. Exceptions to these critiques include value-added style analyses that use instrumental-variables procedures to identify the impact of attending a small high school on educational achievement in Chicago (Barrow, Claessens, & Schanzenbach, 2013) and New York City . Furthermore, Schwartz et al. (2013) find evidence of heterogeneous effects by schools' vintage, suggesting that the earlier literature's focus on parsimonious models that assumed homogeneous effects of school size may have further contributed to the mixed results. A second shortcoming of the existing schoolsize literature is the underrepresentation of primary schools (Cotton, 1996;Leithwood & Jantzi, 2009;Ready & Lee, 2006), as the mechanisms through which school size influences academic performance likely differ between primary and secondary schools. This may be partially attributable to the recent \"small schools\" movement's focus on high schools. However, understanding the relationship between primary school size and achievement is no less important, as the recent wave of school closures, school choice, and desegregation programs in the United States have altered enrollment patterns, and children undergo substantial developmental changes during these ages. For example, problems of chronic absences and school disengagement begin to manifest as early as first grade (Alexander, Entwisle, & Kabbani, 2001;Schoeneberger, 2012). Existing studies of the relationship between primary school size and academic performance are typically correlational and were conducted at the school or district level. For example, Lamdin (1995) and Chen (2007) examine school-level data from Baltimore and New York City, respectively, and find contradictory results. In a study notable for applying an instrumental-variables procedure to school-level panel data, Kuziemko (2006) finds a sizable negative relationship between primary school size and academic performance in Indiana during the 1990s. The present study contributes to the school-size literature by exploiting within-school variation in enrollments in North Carolina's public primary schools between 2003 and 2010 to identify the effect of such changes on the academic performance of individual students. Our preferred identification strategy is similar to that used by Hanushek, Kain, and Rivkin (2009) to investigate the effect of school diversity on educational achievement in that we include school size as a contemporaneous input in value-added models (VAMs) of the education production function that condition on various combinations of teacher-byschool fixed effects (FE), school-by-year FE, student FE, school-specific linear time trends, and a rich set of time-varying school characteristics that jointly predict enrollment and academic performance. The various FE and time trends are important and novel contributions of the present study as they account for the nonrandom sorting of teachers and students across schools and the unobserved trends and year-specific shocks that jointly influence enrollments and student achievement. The empirical analysis utilizes linked student-, teacher-, and school-level administrative records from North Carolina's longitudinal data system. North Carolina's Department of Public Instruction (DPI) began collecting these data in the mid-1990s, which are now made available to researchers through the North Carolina Education Research Data Center (NCERDC). The NCERDC was formed in 2000 in cooperation with DPI, along with financial support from the Spencer Foundation, by researchers at Duke University and the University of North Carolina (Muschkin, Bonneau, & Dodge, 2011). Since then, the NCERDC has cleaned, coded, and standardized numerous longitudinal data files; created encrypted student and teacher identifiers that allow students, teachers, and schools to be linked while retaining individuals' anonymity; and created extensive codebooks for each data set along with instructions for linking students, teachers, and classrooms. The NCERDC is currently housed in the Social Science Research Institute at Duke University and continues to process each new wave of data collected by DPI. The authors obtained the data used in the present study by submitting a formal research proposal, which included a description of the project, evidence of institutional review board (IRB) approval from the authors' home institution, and a data security plan, to the NCERDC. 3 Of course, we are not the first researchers to address policyrelevant research questions using these data. For example, NCERDC data have been used to study differences in the observed teacher characteristics in charter relative to traditional public schools 137S (Carruthers, 2012), the relationship between observable teacher characteristics and student test scores (Clotfelter, Ladd, & Vigdor, 2007), stability of value-added measures of teacher effectiveness across time (Goldhaber & Hansen, 2013) and subjects (Goldhaber, Cowan, & Walch, 2013), the persistence of estimated teacher effects over time (Jacob, Lefgren, & Sims, 2010), teacher peer effects (Jackson & Bruegmann, 2009), the effect of statewide early childhood programs on third-grade academic achievement (Ladd, Muschkin, & Dodge, 2014), the effect of NCLB's subgroup-specific accountability requirements on student achievement (Lauen & Gaddis, 2012), and the presence of nonrandom student-teacher assignments (Rothstein, 2010). This is not an exhaustive list; see the NCERDC website (provided in Note 3) for a full list of projects to which the NCERDC contributed data. The sheer volume of high-quality policy-relevant education research generated by the NCERDC-DPI partnership suggests that this is a successful model that other states and universities might emulate."}, {"section_title": "Theoretical Background and Literature Review", "text": "Previous scholarship has considered the potential for school size to influence students' behavior and academic performance. Regarding the former, theoretical and empirical evidence suggests that larger schools experience higher rates of student indiscipline (e.g., Haller, 1992;Johnston, 2009;Leung & Ferris, 2008). Disorderly incidents may decrease academic achievement through some combination of diverting student attention, creating a fearful or disruptive environment, changing schools' social norms, and decreasing student attendance (Akerlof & Kranton, 2002;Gottfredson & DiPietro, 2011). Through what mechanisms, then, might a change in school size affect achievement net of its effects on school disorder and average daily attendance? We interpret such effects as the result of changes in schools' climates (Akerlof & Kranton, 2002;Anderson, 1982;Welsh, Stokes, & Greene, 2000). Specifically, the National School Climate Center states that \"School climate is based on patterns of students', parents' and school personnel's experience of school life and reflects norms, goals, values, interpersonal relationships, teaching and learning practices, and organizational structures.\" 4 School size affects school climate by changing the school's stock of social capital (Coleman, 1988) and \"sense of community\" (Wynne & Ryan, 1997). For example, there is less frequent and less direct communication between teachers, administrators, and students in larger schools (Gottfredson & DiPietro, 2011). Akerlof and Kranton (2002) argue that students in small schools benefit by being better able to identify with the school and with each other. Boccardo, Schwartz, Stiefel, and Wiswall (2013) provide empirical support for these claims by showing that students have better interpersonal relationships in New York City's old small schools. Furthermore, the benefits of small schools may spill over into the community, as Dee, Ha, and Jacob (2006) provide suggestive evidence that small rural schools promote parental involvement in the form of volunteering at school and participation in Parent Teacher Associations. Numerous studies, reviewed by Andrews et al. (2002), Cotton (1996), and Leithwood and Jantzi (2009), have investigated the relationship between school size and academic achievement. However, much of the existing literature provides mixed evidence and cannot be given a causal interpretation (Kuziemko, 2006). The recent studies that do provide arguably causal estimates (e.g., Barrow et al., 2013;Bloom, Levy Thompson, & Unterman, 2010;Schwartz et al., 2013) are not particularly relevant to the present study, both because they focus on high schools and because they examine the effect of variation in school size generated by the \"small high schools\" movement. In addition to the likelihood that the mechanisms through which secondary school size affects academic achievement are different from those in primary schools, these studies exploit a fundamentally different type of variation in school size from that investigated in the present article. For example, Bloom et al. (2010) and Bloom and Unterman (2014) investigate the impact of attending a \"Small School of Choice\" (SSC) on high school graduation and college readiness. New York City built 123 SSCs in the early to middle 2000s that were designed to educate traditionally disadvantaged students. The SSCs were oversubscribed so admissions were determined by random lottery, which 138S allowed MDRC researchers to tease out the causal effect of attending an SSC. However, although the SSCs did have positive impacts on students' academic progress and achievement, it is impossible to disentangle the effect of school size from the other innovations simultaneously implemented in SSCs (e.g., curricular changes; Bloom & Unterman, 2014). Only two studies have used student-level data to attempt to identify the causal effect of transitory variation in school size on academic achievement in the context of primary schools. We review each in turn both to contextualize the results and identify the novel contributions of the present study. First, Lee and Loeb (2000) estimated the effect of school size using studentlevel achievement data in a value-added context. The authors estimated hierarchical linear models of the 1997 math achievement of sixth-and eighth-grade students in Chicago Public Schools that controlled for lagged math achievement. The authors categorized schools as small (<400 students), medium, or large (>750 students) and found that math-achievement gains were significantly greater in small schools than in mediumor large-sized schools, but the authors found no significant difference between medium and large schools. However, the lack of panel data prohibited the authors from using school FE to control for unobserved school-level heterogeneity that may be correlated with school size. Second, McMillen (2004) conducted a similar analysis of third-and fifth-grade end-of-grade test scores for North Carolina's 1997 third-grade cohort. Because 2 years transpired between the current and lagged test scores, the author measured school size as the 2-year (fourth and fifth grade) average school size experienced by each student. Again, the lack of repeated observations of schools over time prevented the use of school FE. The author found no direct effect of school size on academic achievement. The present study expands on these early works in three important ways. First, by using school-level panel data we are able to control for both the nonrandom sorting of teachers and students across schools and unobserved school heterogeneity by conditioning on various combinations of teacher-by-school, student, and school-by-year FE as well as linear school time trends. Second, rich administrative data from a state as diverse as North Carolina enables the present study to provide results that are more generalizable to the U.S. student population than the results of a district-level analysis, test for differential effects of school size by schools' and students' observable characteristics, and control for observed school-level attributes that jointly predict achievement gains and school size. Finally, by updating these early analyses using data from the modern era of school choice, accountability, charter schools, and school closings, we are able to estimate the effect of school size in the context of current education policy and test for differential effects of school size in charter schools."}, {"section_title": "Data", "text": "We use student-level administrative records on the population of third-through fifth-grade students in North Carolina's public schools between 2003 and 2010 to estimate VAMs of the education production function. The data contain end-of-grade math and reading scores, student demographics, classroom identifiers, and a set of potentially time-varying school-level characteristics. 5 Although the generalizability of North Carolina's educational context is potentially limited, two redeeming features make the data well suited for the present analysis. First, repeated observations of both schools and students over time enable FE estimators that exploit withinschool and within-student changes in school size over time. Second, detailed annual school-level administrative data on total enrollment; average daily attendance; suspensions, expulsions, and crimes per 1,000 students; charter status; grade span; geographic locale; and the demographic composition of the student body facilitate tests for heterogeneous effects of school size and provide time-varying information on the attributes that likely covary with school size and predict student achievement. Furthermore, North Carolina is home to a geographically, socioeconomically, and demographically diverse student population: The analytic sample contains 691,450 unique students who attended 1,417 unique schools in North Carolina between 2004 and 2010. 6 The analytic sample is restricted to students who attended the same school in third through fifth grades to avoid conflating the effect of changing schools with that of changes in enrollment; however, including the approximately 50,000 students who changed schools during this time does not change the qualitative results. Importantly, because school size is taken from administrative enrollment data, these sample restrictions do not create systematic measurement error in the school-size variables. The independent variable of interest is school size, which can be operationalized as either total student enrollment or total grade-level enrollment (Ready & Lee, 2006). We consider both measures, as Manski (1993) and Epple and Romano (2011) stress the importance of appropriately identifying peer \"reference groups.\" Enrollment counts in the administrative data reflect schools' average daily membership during the course of each academic school-year. However, there are several theoretical and methodological reasons to prefer grade-level enrollment as a measure of school size. First, students and teachers within each grade level primarily interact with one another (Ready & Lee, 2006). Second, schools' enrollments may vary across grades due to demographic trends in the district. Third, schools' total enrollments depend on grade spans (Lee & Loeb, 2000). Finally, the school-by-year FE identification strategy of Hanushek et al. (2009), which we describe in the following section, can only be implemented using grade-level enrollments. When possible, each model is estimated twice: once using total school enrollment as the measure of school size and once using total enrollment in the student's current grade as the measure of school size. Figure 1 depicts histogram and kernel density estimates of the school-size distribution in which size is measured as total school enrollment and school-years are the unit of observation. The school-size distribution is approximately symmetric about the mean of 512 students but has a long right tail that contains schools with 1,000 or more students. Figure 2 does the same for the distribution of total grade enrollments, and again the distribution is approximately symmetric with a long right tail. Summary statistics of the schoolsize distribution are reported in Table 1. North Carolina's primary schools moderately increased in size between 2004 and 2008, before shrinking in 2009 and 2010. Table 1 also reports school-size summary statistics conditional on school type, as previous research suggests potential heterogeneities in the effect of school size by vintage (Schwartz et al., 2013) and geographic locale (Crispin, 2012). Charter schools are substantially smaller than traditional public schools and the 87 new schools introduced between 2004 and 2008 tend to be slightly larger than existing schools. Urban and suburban schools are similar in size and both tend to be larger than rural   Table A1 similarly describes the population of U.S. primary schools for the same time period using data from the National Center for Educational Statistics' CCD. 7 North Carolina's primary schools are about 25 students larger than the national average, although average within-school annual changes in enrollment are similar to corresponding U.S. averages. Because the identification strategy described in the following section exploits within-school variation in enrollments over time, it is also useful to describe the annual enrollment changes experienced by North Carolina's primary schools between 2004 and 2010. The lower panel of Table 1 shows that the average annual change in school enrollment was 33 students and that average increases and decreases were of roughly the same magnitude. Similarly, the average annual change in grade-specific enrollment was 11 students, and again, increases and decreases were equally likely and of the same size, on average. In percentage terms, the average within-school change in total enrollment was about 7%, and the average within-school change in grade-level enrollment was about 16%. Figures 3 and 4 plot the distributions of annual within-school changes in total and grade-level enrollments, respectively. Both are approximately normally distributed and centered on small changes in enrollment. Although changes are generally small, there are occasional changes in school enrollments of 100 to 200 students and changes in grade-level enrollments as large as 40 students. The following section investigates the possible sources of such changes and how schools accommodate changes in enrollment. Online Appendix Table B1 (see the online appendix at http:/epa.sagepub.com/supplemental) reports summary statistics for all variables included in the empirical models at both the student-year and school-year levels. 8 The analytic sample is approximately evenly split between the fourth and fifth grades, poor and nonpoor, male and female, and White and non-White students. The majority of North Carolina's primary schools are K-5, and only 2% are classified as charter schools. Disorderly incidents are rare in the estimation sample, which is unsurprising given the ages of primary school students; still, they do occur."}, {"section_title": "How and Why Does School Size Change?", "text": "This section presents some descriptive regressions that investigate why school enrollments change over time and how schools accommodate such changes. Changes in a school's enrollment must be associated with one or more of the following: a change in grade span, a change in the number of classrooms, or a change in average class size. About 10% of the 1,417 schools in the analytic sample experienced a change in grade span between 2004 and 2010. Column 1 of Table  2 reports within-school (school FE) estimates of the effect of grade span on total enrollment. 9 The point estimate of 37.7 indicates that, on average, a one-grade increase in grade span from one academic year to the next increases a school's total enrollment by about 38 students. However, the regression's R 2 indicates that grade-span changes account for less than 5% of the total withinschool variation in enrollments observed between 2004 and 2010. Still, given that 10% of schools did experience a change in grade span during the time period analyzed in the present study, as a sensitivity analysis, we will estimate the baseline model on the restricted sample of schools that did not change grade spans between 2004 and 2010. Columns 2 through 10 of Table 2 further investigate three dimensions of the relationship between school size and school structure. 10 The first panel reports estimated effects of total school enrollment on grade-specific enrollments. Again, these estimates were generated by bivariate school-FE regressions. The coefficients on total enrollment suggest that at least between kindergarten and sixth grade, enrollment changes are evenly distributed across grades. Specifically, the point estimates suggest that on average, a one-student increase in school enrollment is associated with a 0.15 student increase in grade size. The seventh-and eighth-grade point estimates are slightly smaller, although not significantly so. They are also less precisely estimated, which is likely due to the significantly smaller number of K-8 schools in the analytic sample. Overall, the results reported in the first panel of Table 2 suggest that variation in school size is driven by students changing schools and idiosyncratic variation across districts in cohort size as opposed to systematic increases in the size of incoming public school cohorts. The sources of variation in school enrollments are further investigated in Table 3. Having shown that the within-school variation in school size is approximately evenly distributed across grade levels, the second and third panels of Table 2 examine the relationship between changes in grade size and the average size and number of that grade's classrooms, respectively. The effect of grade-specific enrollment on average class size is smaller in the lower grades, suggesting that schools are more likely to accommodate increased enrollments in lower grades, particularly kindergarten through third grade, by adding additional classrooms. Still, even in the higher elementary grades, the effects of enrollment increases on average class sizes are modest: For example, a 10-student (1 SD) increase in fifth-grade enrollment is associated    (3) "}, {"section_title": "144S", "text": "with an average increase in fifth-grade class size of about 0.4 students. The \"classroom per grade\" results in the last panel of Table 2 are consistent with the average class size results, again suggesting that enrollment increases in the lower primary grades are more likely to result in new classrooms than are enrollment increases in the higher primary grades, although the effects are fairly similar across grade levels. Generally, a 25-student (2.5 SD) increase in any grade's enrollment is associated with about one new classroom in that grade. Together, the descriptive results presented in Table 2 provide two general insights into how within-school variation in total enrollment relates to schools' average grade and class sizes. First, increases in total school enrollment are approximately evenly distributed across grade levels. Second, increases in grade size are more likely to be accommodated by increasing the number of classrooms than by increasing average class size, particularly in the lower primary grades (i.e., K-5). These findings provide context for interpreting the main analyses of the relationship between school size and student achievement reported below. The first finding, that enrollment changes are approximately evenly distributed across grade levels, seems to rule out one potential source of within-school variation in total enrollments: systematic trends in the size of incoming kindergarten cohorts during the sample time period. By definition, changes over time in a school's total enrollment are driven by students entering and exiting the school. One source of variation in total enrollment, alluded to above, is that at the start of each new school-year the cohort in the school's highest grade level that graduated the previous spring is \"replaced\" by the new cohort entering the school's lowest grade level. The other source of within-school variation in total enrollments is student mobility. Generally, "}, {"section_title": "145S", "text": "student mobility includes students who enter or leave the state of North Carolina, enter or leave particular school districts, and make intradistrict school changes. The latter may include moves induced by school closures and school openings. Table 3 investigates how school closures, school openings, and total enrollments in schools' catchment areas affect school enrollments. Three geographic catchment area definitions are considered. From smallest to largest, they are the school's 5-digit ZIP code, Local Education Agency (LEA; i.e., school district), and county. Each model conditions on school FE, thus exploiting within-school variation in catchment area characteristics and school size, and is estimated both with and without academic year FE. Importantly, the academic year FE control for statewide trends in public primary school enrollments. The effect of school closure(s) in a given school's catchment area on the school's total enrollment is positive and statistically significant only when the catchment area is defined as the school's 5-digit ZIP code; school closures in a school's district or county have no significant effect on remaining schools' enrollments. The point estimate of approximately 11 suggests that schools within ZIP codes that experienced at least one school closure had an average enrollment increase of 11 students (about 25% of the within-school enrollment increase SD shown in Table 1). Similarly, school openings in a school's catchment area have negative, statistically significant effects on the enrollments of remaining schools, regardless of how the catchment area is defined. The effects of school openings are generally larger in magnitude than the corresponding increases in school size associated with school closures, perhaps because openings are significantly more frequent than closures during the sample's time period. Such effects are largest when schools' catchment areas are defined as schools' ZIP codes, which is intuitive given that these schools provide the most direct competition to existing schools. The total number of students enrolled in public primary schools in the catchment area is also significantly related to individual schools' enrollments, although the effects are modestly sized. For example, the within-ZIP-code standard deviation (SD) in total primary enrollment is 164, so a 1-SD increase in ZIP code enrollment is associated with increases of about eight students-or 25% of the average within-school enrollment increase in schools' total enrollments. Once again, the effects are strongest when the catchment area is defined by ZIP code. Together, the results presented in Table 3 suggest that school openings, school closures, and geographical variation in the growth of primaryschool-age populations play modest roles in the within-school variation in total school enrollments observed in the analytic sample. Moreover, statewide trends in public primary school enrollments do not appear to be driving changes in school-level variation in total enrollments. The remaining within-school variation in enrollments seems to be driven by student mobility and idiosyncratic variation in the sizes of schools' entering and graduating cohorts."}, {"section_title": "Method", "text": "The identification strategy includes school size as a contemporaneous input in VAMs of the education production function. We estimate student-level models so that we can test for heterogeneity in the effect of school size by observable student characteristics and control for teacher FE. We take a basic lag-score VAM specification as a point of departure, as Guarino, Reckase, and Wooldridge (2015) find this approach the most robust to a variety of potential nonrandom student-teacher assignment scenarios, and similar specifications are commonly used in analyses of the effects of educational inputs. 11 Formally, we model student i's end-of-grade test score (y) in year t as  where j indexes teachers and s indexes schools; X is a vector of student characteristics including race, gender, grade level, English proficiency, the presence of a documented learning disability, and poverty status; f(size) is a general function of school size; Z is a vector of potentially timevarying school characteristics including the demographic and socioeconomic composition of the student body, the size and credentials of the instructional staff, geographic locale (i.e., rural, suburban, or urban), grade span, charter status,"}, {"section_title": "146S", "text": "average daily attendance, and the numbers of crimes, suspensions, and expulsions per 1,000 students; c is student i's class size; p is the average of student i's year-t classmates' lagged test scores, which is a commonly used control for classroom peer effects; \u03c9 is a teacher-by-school FE; \u03c4 is a year FE; and u is an idiosyncratic error term that contains student i's potentially timevarying unobserved ability and test-score measurement error. Standard errors are made robust to clustering at the school level, which makes statistical inference robust to serial correlation within schools and students because the estimation sample is restricted to students who did not change schools between third and fifth grades (Angrist & Pischke, 2009). Five aspects of Equation 1 warrant further discussion. First, we consider several specifications of f(size): linear, quadratic, and logarithmic functions of school size as well as sets of categorical indicators for quartiles of the school-size distribution. Similarly, we interact school size with a variety of student-and school-level observed characteristics to test for heterogeneity in the relationship between school size and student achievement. Second, note that the specifications of f(size) discussed above restrict the effects of increases and decreases in school size to be symmetric. To test whether the effects of increases and decreases in school size are symmetric, we also consider a linear switching specification of f(size) in which d equals 1 if size increased between years t \u22121 and t, and 0 otherwise:  It is then straightforward to test the hypothesis that \u03b3 1 equals \u03b3 2 . Third, two potential concerns are that schools close endogenously and new-school sites are endogenously determined. To verify that the results are not driven by school openings or closings, we estimate the baseline model Equation 1 on the balanced panel of schools that were consistently open between 2004 and 2010 (Wooldridge, 2010). Fourth, the teacher-by-school FE are crucial to the identification strategy as they control for the quality of individual students' teachers, the sorting of teachers across schools, and unobserved time-invariant school effects during teachers' school-specific spells. Because the teacher-byschool FE control for unobserved school effects that remain constant during each teacher's spell in a given school, time-invariant teacher and school FE are redundant in Equation 1. However, the time-varying school-level characteristics in Z perform the equally important role of controlling for school characteristics that potentially vary with school size and predict gains in student performance. Similarly, there may be unobserved schoolspecific trends that jointly predict enrollments and achievement. Thus, we augment Equation 1 in two ways to ensure that the estimated effect of school size is not biased by the presence of time-varying unobserved school effects: We add either schoolspecific linear time trends (Wooldridge, 2010) or school-by-year FE (Hanushek et al., 2009) to Equation 1. The latter specification can only identify the effect of school size when size is measured as total grade enrollment, as total school enrollment is perfectly collinear with the school-by-year FE. The school-by-year FE estimates are identified by same-year, same-school differences between fourth-and fifth-grade enrollments. Finally, a potential concern with the lagged test-score specification of Equation 1 is that unobserved student ability not captured by the lagged test score is left in the error term, which may be correlated with the model's covariates. Guarino et al. (2015) find that ordinary least squares (OLS) on Equation 1 is the most robust estimator in the presence of nonrandom student-teacher assignments, which is likely the case in North Carolina (Rothstein, 2010). However, when estimating the effect of a school-level characteristic rather than teacher effects, student sorting into schools is a potentially greater source of bias than student sorting into classrooms, and the former may be driven by relatively time-invariant student characteristics (Brummet, 2014). Accordingly, we examine the robustness of Equation 1 to conditioning on timeinvariant unobserved student heterogeneity (i.e., student FE) in a gain-score VAM (Guarino et al., 2015). Specifically, we modify Equation 1 by decomposing the error term, restricting \u03b1 to equal 1, and subtracting the lagged test score from both sides of the equation, which yields: ( The presence of student FE (or the school-byyear FE mentioned above) in addition to the teacher-by-school FE creates computational difficulties, as traditional FE estimators are infeasible due to the combination of unbalanced panels and the high dimensionality of the problem (Abowd, Kramarz, & Margolis, 1999). Thus, we estimate Equation 3 using the two-way-FE estimator proposed by Mittag (2012)."}, {"section_title": "Results", "text": ""}, {"section_title": "Main Results", "text": "Each row of Table 4 corresponds to a different regression and reports the estimated coefficient on school size, measured as either total school enrollment or total grade-level enrollment, for a variety of specifications and samples that assume a linear relationship between school Note. Each column reports estimates from four distinct regressions: school size and grade size models for both math and reading achievement. School size is measured in hundreds of students and grade size is measured in tens of students. Parentheses contain standard errors that are clustered at the school level. All specifications include the full set of student-and school-level controls described in the text and summarized in Online Appendix Table B1 (see the online appendix at http:/epa.sagepub.com/ supplemental), in addition to teacher-by-school FE. All estimated coefficients for the math and reading specifications in column 1 are reported in Online Appendix Table B2. For each row in column 2, the first estimate is the estimated size coefficient for school-years in which size increased over the previous year, and the second estimate (\u03b3 2 ) is the size coefficient for school-years in which size either decreased or did not change since the previous year. None of the asymmetric-pair estimates in column 2 are statistically significantly different from each other at the 95% confidence level. FE = fixed effects. *p < .10. **p < .05. ***p < .01."}, {"section_title": "148S", "text": "size and student achievement. The top panel of Table 4 reports results for math achievement and the bottom panel reports results for reading achievement. Column 1 reports baseline estimates of the lag-score model given in Equation 1. 12 Column 2 estimates the asymmetric switching specification defined by Equation 2. Column 3 estimates the baseline specification of Equation 1 using a balanced panel of schools that were consistently in operation between 2004 and 2010. Column 4 estimates the baseline model on the restricted sample of schools that did not experience a change in grade span between 2004 and 2010. The estimated coefficients in the first four columns of Table 4 are quite similar, which suggests two things. First, the effects of increases and decreases in school size are approximately symmetric, as none of the pairs of coefficients in column 2 are significantly different from one another other at the 95% confidence level. Second, the results are not driven by endogenous school closings, the performance of newly opened schools, or schools that changed grade span. Both the school-and grade-level measures are statistically significant in the math-achievement VAMs and tend to be about twice as large as the corresponding estimates in the reading achievement VAMs. Only the school-enrollment estimates are even marginally statistically significant in the reading achievement VAMs. The finding that school size has less, if any, effect on reading than on math achievement is consistent with studies of the relationship between high school size and student achievement (e.g., Crispin, 2012) and with the general finding that educational interventions have relatively stronger impacts on math achievement (e.g., Hanushek & Rivkin, 2010). This may be due to the structure of reading tests (Measures of Effective Teaching, 2010) or the fact that schools do in fact have larger impacts on the development of math skills as children learn and develop reading skills at home (Currie & Thomas, 2001). The estimated coefficient on total school enrollment in the top panel of column 1 suggests that a 100-student increase in school size lowers math scores by 0.01 of a test-score standard deviation (SD). Although this effect appears small, recall that the SD of school size is about 200 students. Thus, a 1-SD increase in school size is associated with a 0.02-SD decrease in math achievement. This effect is arguably practically significant as it is approximately 20% of the effect of a 1-SD increase in teacher effectiveness (e.g., Hanushek & Rivkin, 2010). However, as discussed in the methodology section, the estimates in column 1 might be biased by time-varying unobserved school characteristics that jointly predict enrollment and achievement. To control for this possibility, columns 5 and 6 of Table 4 augment Equation 1 to condition on school-specific linear time trends and school-by-year FE, respectively. Controlling for linear time trends in column 5 reduces the estimated effect of school size on math achievement by 30% to 50%, and even more so for reading. None of the estimates remain even marginally statistically significant. The school-by-year FE estimates in column 6 actually turn positive but are quite small and indistinguishable from zero. These results suggest that the negative coefficients on school size observed in column 1 were at least partially driven by time-varying unobserved school characteristics. Column 7 of Table 4 reports the estimated coefficients on school size in gain-score specifications that are otherwise identical to Equation 1, to provide context for comparisons between estimates of Equations 1 and 3. The point estimates in column 7 are generally similar to those in column 1, indicating that the results are robust to moving from the lag-score to gain-score specification. Column 8 of Table 4 reports two-way FE estimates of Equation 3, which conditions on student FE. The math results remain negative and similar in magnitude to those of the other specifications considered in Table 4, but are less precisely estimated. The imprecision of the twoway FE estimates is unsurprising, as only 2 years of data are available per student, the student FE are jointly insignificant, and the adjusted R 2 actually turns negative. Once again, none of the two-way FE reading estimates are statistically significant. Taken as a whole, the results reported in Table 4 provide no evidence of a causal relationship between transitory changes in school size and academic achievement, regardless of whether school size is measured at the school or grade level. Specifically, the modestly sized, marginally significant negative effects of school size reported in column 1 vanish when either linear time trends, school-by-year FE, or student FE are controlled for. However, the nonfinding in Table 4 could be due to the linear specification of school size. For example, the true relationship between school size and student achievement might be nonlinear or might vary by student or school characteristics. We subsequently investigate these hypotheses using the school-by-year FE specification, which is arguably the most robust to the presence of unobserved school-year specific shocks (Hanushek et al., 2009), and hence our preferred specification. Similar analyses using the linear time trend specification yield qualitatively similar results. Table 5 tests for nonlinearities in the relationship between school size and student achievement using three nonlinear specifications of f(size) in the preferred school-by-year FE specification: the natural log, a quadratic function, and categorical indicators for the first and fourth quartiles of the size distribution (omitting the second and third quartiles as the reference group). Columns 1 to 3 of Table 5 estimate these three specifications for math achievement, and columns 4 to 6 do so for reading achievement. None of the estimated coefficients on the nonlinear size terms are individually significantly different from zero. For the quadratic specifications reported in columns 2 and 5 of Table 5, the average partial effects are statistically insignificant and similar in magnitude to those reported in column 6 of Table 4. Moreover, the quadratic terms in Table 5 are jointly Note. N = 1,034,490. Grade size is measured in tens of students. Parentheses contain standard errors that are robust to clustering at the school level. All specifications include the full set of student-and school-level controls summarized in Online Appendix Table B1 (see the online appendix at http:/epa.sagepub.com/supplemental), as well as the teacher-by-school and school-by-year FE included in column 4 of Table 3. In columns 3 and 6, the first and third quartiles of grade size are 45 and 72 students. In columns 2 and 5, p values are reported for F tests of the joint significance of the grade size variables. APE = average partial effect; FE = fixed effects. *p < .10. **p < .05. ***p < .01."}, {"section_title": "Nonlinear Effects of School Size", "text": ""}, {"section_title": "150S", "text": "insignificant as well. Taken as a whole, the results reported in Table 5 provide no evidence of a nonlinear relationship between school size and academic achievement and confirm that the nonfindings in Table 4 are not driven by an assumed linear relationship. Table 6 tests for heterogeneity in the relationship between school size and student achievement by both student and school observed characteristics. This is accomplished by interacting school size with four student and five school characteristics in the preferred school-by-year FE specification. The observed student characteristics are binary indicators for administratively classified Limited English Proficiency (LEP), subject-specific learning disabilities (i.e., math and reading), poverty status, and gender. The observed school characteristics are binary indicators for charter school, new school, kindergarten through eighthgrade school (as opposed to Kindergarten through fifth grade), and geographic locale (urban and rural, with suburban serving as the omitted reference category). Because the interaction terms are not jointly significant, either as a group or separately by the student-and school-level interactions, there is no evidence that the results of this analysis are distorted by collinearity among the interaction terms. For this reason, as well as for presentational ease, we include the nine interaction terms simultaneously in one model; however, it is worth noting that adding each interaction term to the baseline specification individually yields qualitatively similar estimates-and patterns of statistical significance-of the interaction effects."}, {"section_title": "Heterogeneous Effects of School Size", "text": "There is reason to believe that students may differentially respond to school size, as Krueger (1999) finds that the effect of class size varies by students' socioeconomic status (SES), and Baydar and Brooks-Gunn (1991) find differential effects of child care on cognitive development by SES and child's gender. The marginal benefit of individual attention provided by smaller schools may be greater for disadvantaged students who Note. N = 1,034,490. Grade size is measured in tens of students. Parentheses contain standard errors that are robust to clustering at the school level. All specifications include the full set of student-and school-level controls summarized in Online Appendix Table B1 (see the online appendix at http:/epa.sagepub.com/supplemental), as well as teacher-by-school FE and either linear school time trends or school-by-year FE. The p values are reported for F tests of the joint significance of the student-level interactions, school-level interactions, and all interactions. FE = fixed effects. *p < .10. **p < .05. ***p < .01."}, {"section_title": "151S", "text": "receive less attention at home (Gershenson, 2013;Guryan, Hurst, & Kearney, 2008). Similarly, the effect of school size would differ by students' LEP and learning disability status if exceptional students either encounter difficulties navigating the social and administrative environments of large schools or benefit from larger schools' economies of scale in the provision of services to exceptional students. The most striking result in Table 6 is that students in larger schools who have administratively classified learning disabilities perform significantly worse on math and reading standardized tests. The interaction effects are similar in magnitude for both math and reading, indicating that a 10-student increase in total grade enrollment reduces the achievement of learningdisabled students by about 0.015 test-score SD. This is an arguably practically significant difference, as the sample SD of grade enrollments is about 40 students and the average annual change in grade enrollments was 11 students. The poverty interaction is also strongly statistically significant for reading achievement, perhaps because reading skills are primarily developed at home and high-SES households are able to provide more or higher quality resources to the development of children's reading skills (Currie & Thomas, 2001). However, the interaction term is an order of magnitude smaller than that of learning disabilities and is unlikely to be practically significant. Interestingly, none of the school-level interaction terms are statistically significant at traditional confidence levels, although the point estimates of the charterschool and new-school interactions are negative and large in size."}, {"section_title": "Discussion", "text": "The present study uses rich student-level administrative records from North Carolina's public school system that include repeated observations of both students and schools over time to estimate the effect of school size on academic achievement in a value-added framework. Estimates of naive VAMs suggest that a 1-SD increase in school size reduces math achievement by about 0.02 test-score SD while the effect on reading achievement is even smaller and only marginally statistically significant. However, when the likely endogeneity of school size is accounted for either by linear school time trends or school-by-year FEs, the estimated effect of school size on math achievement decreases in magnitude and loses its statistical significance. Therefore, the primary analysis provides no evidence of a causal relationship between school size and student achievement, regardless of whether school size is measured at the school or grade level, nor do we find any evidence of a nonlinear relationship between school size and student achievement, at least within the range of school sizes observed in North Carolina between 2004 and 2010. However, there do appear to be important heterogeneities in the relationship between school size and student achievement. Specifically, the math and reading achievement of students with learning disabilities, and the reading achievement of socioeconomically disadvantaged students, are disproportionately harmed by increases in school size. Interestingly, the effect of school size does not vary by geographic locale or in charter schools. Students with learning disabilities may be particularly sensitive to increases in school size either because larger schools are less able to match exceptional students to the relevant support programs or because exceptional students are more sensitive to the weaker social bonds likely inherent in larger schools. Specifically, a 10-student increase in grade size is found to reduce the math and reading achievement of exceptional students by about 0.015 test-score SD, which is an arguably practically significant reduction in learning. This modest effect is most easily interpreted relative to the effects of other potential educational interventions that are considered to be practically significant. For example, it is approximately 15% of the effect of a 1-SD increase in teacher effectiveness (e.g., Hanushek & Rivkin, 2010). Arguably, these are nontrivial impacts on student achievement as schools in the analytic sample frequently experience annual changes in grade-level enrollments as large as 20, 30, and even 40 students. The general finding of no overall, causal effect of school size on standardized test scores is also policy relevant and merits discussion, as educators,"}, {"section_title": "152S", "text": "reformers, and policymakers have debated the costs and benefits of school size for more than 100 years. This nonfinding is consistent with studies of the small high school movements in Chicago and New York by Barrow et al. (2013) and , respectively, whose instrumental-variables estimates find no significant effect of school size on high school students' standardized test scores. These results suggest that the successes of New York City's SSCs (e.g., Bloom et al., 2010;Bloom & Unterman, 2014) were at least partly due to the curricular and organizational changes enacted at the same time as the enrollment reductions. It is also worth noting that Barrow et al. (2013) and Bloom and Unterman (2014) find significant effects of school size on graduation rates, suggesting that improved school climates in smaller schools strengthen noncognitive skills such as motivation and grit that are valued in the workplace and associated with long-run socioeconomic outcomes. It would thus be useful for future research to investigate the effects of school size on character skills in the primary school context. Finally, the finding in the present study that school size influences the academic achievement of exceptional and disadvantaged students over and above its influence on class size, student indiscipline, and attendance highlights the potentially nuanced importance of school climate in the educational process, which encompasses a number of behaviors, attributes, and cultural and social norms. This raises deeper questions of how and why school climate is a function of school size and why certain subsets of the student population are particularly influenced by school climate. It would be useful for future research to attempt to get inside the \"black box\" of school size's role in the educational process and further unpack the effects of school size and school climate. For example, do instructional practices or rates of teacher attrition systematically vary by school size? Similarly, additional surveys and qualitative analyses of student, parent, teacher, and administrator perceptions of school climate similar to those analyzed by Lee and Loeb (2000) and Boccardo et al. (2013) may prove useful as policy implications and recommendations for best practice ultimately depend on the channels through which school size and school climate influence students' development and long-run socioeconomic outcomes."}, {"section_title": "153S", "text": ""}, {"section_title": "Notes", "text": "1. There was a well-organized and well-funded \"small high schools\" movement in several urban school districts, most notably in New York City and Chicago, during the 1990s and 2000s . The movement was funded by the Gates, Carnegie, and Annenberg Foundations and the U.S. Department of Education (Barrow et al., 2013). However, because the movement focused entirely on reducing the size of high schools, it is only tangentially related to the present study. See U.S. Department of Education (2006), Ravitch (2011), and Shear and Smerdon (2003) for further discussion of the modern \"small high schools\" initiative. 2. Source: Authors' calculations using the Common Core of Data. See Appendix Table A1. 3. Formal instructions for requesting the data are publicly available on NCERDC's website. See https://ssri.duke.edu/data-it-services/north-carolinaeducation-research-data-center-ncerdc. 4. See http://www.schoolclimate.org/ for additional information. 5. The end-of-grade tests are state mandated, criterion referenced, and vertically aligned. We standardize test scores by grade, year, and subject to have mean 0 and SD 1 (Ballou, 2009). 6. Third graders and academic-year 2003 observations do not appear in the estimation sample, as these data are used to create lag scores and gain scores for use in the value-added models. 7. The CCD is publicly available here: http://nces. ed.gov/ccd/. 8. Test-score means and SD are not precisely 0 and 1, respectively, because they were standardized using all available test scores. 9. This estimate was generated by regressing total enrollment on grade span and a full set of school indicators. Adding year indicators to the regression does not significantly change the point estimate on grade span. 10. The estimates reported in columns 2 to 10 of Table 2 are robust to conditioning on year fixed effects and to dropping schools that changed grade spans from the analytic sample. 11. Rothstein (2010) finds evidence of nonrandom student-teacher assignments in North Carolina. 12. Estimated coefficients for the full set of covariates included in the lag-score specification of column 1 are generally of the expected sign and are reported in Online Appendix Table B2 (see the online appendix at http:/epa.sagepub.com/supplemental)."}]