[{"section_title": "Abstract", "text": "Missing values present a prevalent problem in the analysis of establishment survey data. Multivariate imputation algorithms (which are used to fill in missing observations) tend to have the common limitation that imputations for continuous variables are sampled from Gaussian distributions. This limitation is addressed here through the use of robust marginal transformations. Specifically, kernel-density and empirical distribution-type transformations are discussed and are shown to have favorable properties when used for imputation of complex survey data. Although such techniques have wide applicability (i.e., they may be easily applied in conjunction with a wide array of imputation techniques), the proposed methodology is applied here with an algorithm for imputation in the USDA's Agricultural Resource Management Survey. Data analysis and simulation results are used to illustrate the specific advantages of the robust methods when compared to the fully parametric techniques and to other relevant techniques such as predictive mean matching. To summarize, transformations based upon parametric densities are shown to distort several data characteristics in circumstances where the parametric model is ill fit; however, no circumstances are found in which the transformations based upon parametric models outperform the nonparametric transformations. As a result, the transformation based upon the empirical distribution (which is the most computationally efficient) is recommended over the other transformation procedures in practice."}, {"section_title": "Introduction", "text": "Missing data are a particularly common and particularly troublesome problem in establishment surveys. A large portion of the statistical literature has been devoted to the analysis of data that contain missing values, and as a result a myriad of approaches exist. Pertinent techniques include calibration weighting (Kott and Chang 2010) and the EM algorithm (Dempster et al. 1977) ; however, imputation (for a summary, see Rubin 1987 ) is often the preferred method for handling missing data since it yields a completed dataset on which classical tools for analysis may be applied. Additionally, multiple (or repeated) imputation (Rubin 1996) may be used to quantify imputation error. Despite the ubiquity of missing data problems and methodology designed to address them, existing imputation algorithms have many drawbacks, largely with respect to robustness and computational efficiency.\nMultivariate imputation techniques tend to be fairly restrictive with respect to the types of model assumptions. Techniques that impute via a multivariate normal model (Schafer 1997; Robbins et al. 2013 ) are popular and theoretically justified. Techniques that use fully conditional specification (a.k.a. SRMI, as outlined in Raghunathan et al. 2001) , which is implemented in several software packages including IVEware (Raghunathan et al. 2002) , MICE (Van Buuren and Oudshoorn 1999) , and mi (Su et al. 2011) , can be used to create imputations in data that contain categorical and discrete variates but lack theoretical justification due to the use of a potentially incompatible Gibbs sampler. However, each of the aforementioned procedures is best suited to sample (i.e., draw) imputations for continuous variables from a normal distribution.\nMultivariate techniques that do not sample imputations for continuous items under Gaussian assumptions are relatively sparse. Algorithms which employ fully conditional specification can be modified so that imputations are generated via a conditional modeling/sampling technique known as predictive mean matching (PMM, Little 1988) . PMM is a nearest-neighbor procedure; imputations are sampled from observed data values. However, PMM is computationally burdensome in comparison to its Gaussian counterparts and thus can have little utility in high dimensional settings. The IRMI algorithm (Templ et al. 2011 ) is similar in structure to SRMI-type procedures with the added functionality of estimating conditional models through robust regression; however, steps are not taken to ensure that imputations are sampled from the true conditional distribution, which implies that IRMI imputations will likely distort complex distributional characteristics (further justification for this claim is provided in Section 5). To increase the robustness of traditional normality-based methods, many authors recommend the use of marginal transformations of continuous variates prior to the application of imputation methodology. For example, Raghunathan et al. (2001) suggest a power transformation, whereas Robbins et al. (2013) suggest a density-based transformation (specifically, a skew-normal density is used).\nThe practicality of the aforementioned procedures is muddled by their computational complexity. The growing ubiquity of multiple imputation, the prevalence of iterative sampling techniques (e.g., Markov chain Monte Carlo) for imputation, and the high dimensional nature of modern statistical analyses result in algorithms that mandate a substantial computational burden. Such issues become increasingly problematic under the guise of the benefits provided by the use of a wide-ranging imputation model (Robbins and White, Forthcoming) .\nHere, the transformation-based schemes of Robbins et al. (2013) are extended, resulting in the introduction of robust techniques for transformation. In particular, a transformation based on the kernel density is suggested. Woodcock and Benedetto (2009) use a kernel density to generate data values for the purpose of creating a public use dataset from confidential data. Additionally, a fully empirical transformation (which uses a modified empirical distribution) is presented here. The empirical transformation yields a hot-deck (or nearest-neighbor) technique that may be applied jointly with commonly used multivariate imputation algorithms (such as IVEware, MICE or mi) in a very computationally efficient manner. The proposed methodologies yield simple tools which uphold the ability to preserve complex distributional structures provided by PMM while maintaining the computational efficiency of techniques which mandate Gaussian assumptions.\nIn this article, imputations for a widely-used data product are generated via the aforementioned transformation techniques. The marginal and multivariate efficacy of the resulting imputations, as well as the inadequacies of imputations generated using a fully parametric model, are illustrated. Specifically, in Section 2, the dataset that will be used throughout, and the technique that will be used to generate imputations (following transformation), are introduced. The robust methods of transformation are presented in Section 3, and data analysis is provided in Section 4. Further, Section 5 presents a simulation study (performed using real and synthetic data) that illustrates the effectiveness of the proposed transformation schemes. The article concludes by providing comments and practical advice in Section 6."}, {"section_title": "The ARMS and Associated Imputation Technique", "text": "In June 2009, a research project commenced with the goal of creating a new imputation method for the US Department of Agriculture's (USDA) Agricultural Resource Management Survey (ARMS). Partial findings of the research project are outlined in , Robbins et al. (2013) and Robbins and White (Forthcoming) ; this article relates additional findings of the project. Although the methodologies presented here are widely applicable, the problem of interest is motivated here through a discussion of the ARMS and its recently developed imputation technique.\nARMS data are a key source of information for congressional decisions that allocate billions of dollars in farm subsidies (Robbins et al. 2013) . The survey provides the USDA's most comprehensive view of the American farm household; ARMS data contain 30,000 -40,000 units (observations) with 1,000 -2,000 items (variables). The ARMS has a multiphase, dual-frame, stratified, probability-weighted sampling design. Design weights are calibrated, and the calibrated weights are used to calculate key survey indications (U.S. Department of Agriculture 2011). Calibration of design weights also accounts for unit nonresponse; the rate of unit nonresponse tends to hover around 30% (National Research Council 2008) . Analyses presented herein use data from the 2010 ARMS.\nAside from being high dimensional, ARMS data have a complex distributional structure -the majority of ARMS variables have semicontinuous distributions. To elaborate, a portion of units will report a zero for a given variable, whereas the responses for the remaining observations for that variable are sampled from some strictly positive and (theoretically) continuous distribution.\nThe new ARMS imputation procedure handles semicontinuous variables via a commonly used mixture model (see Javaras and van Dyk 2003, for example Su et al. (2011) for an example of an extant procedure that utilizes similar approaches for handling semicontinuous data. Another key characteristic of the missingness in ARMS data is that all missing values are assumed to be positive. Thus, B is fully observed for all variables.\nThe positive portions of ARMS variables (i.e., the Y * s) tend to be highly skewed. Since all imputation procedures that are practical in high-dimensional settings link variables through a multivariate normal model, each Y * is transformed in order to achieve normality. Letting X (which is theoretically Gaussian) represent a transformed version of Y * , Robbins et al. (2013) provide the following procedural outline of the algorithm for imputation in ARMS data:\n1. Break each semicontinous variable Y into B and Y * (observed 0s are treated as missing). 2. Transform: Y * ) X for each variable. 3. Impute: FindX (the imputed version of X) for each variable. 4. Untransform:X )\u0176 (the imputed version of Y) for each variable (values that are originally observed as 0 are reset to 0).\nThe imputed data also undergo an editing process to ensure that imputations satisfy all data constraints prior to release. Most variables are not subject to such constraints, and the editing process does not damage the quality of the imputations with regards to analytic properties. Robbins et al. (2013) focus on Step 3 above. For that purpose, they introduce a dynamic imputation procedure, the so-called iterative sequential regression (ISR) method, that builds a multivariate (normal) model for the Xs (and respective covariates) through a sequence of conditional linear models while allowing flexibility in the form of each conditional model. For the purpose of transformation, they apply a skew-normal model (Azzalini 1985) to the logged versions of the Y * s. It had been established that such a transformation is sufficient for the majority of ARMS variables (Miller et al. 2010) . However, for certain ARMS variables (and surely data from most any other survey) such a model is insufficient.\nAs a result, the focus here turns to Steps 2 and 4 above: the mechanisms for transformation. We present robust nonparametric methods for transformation that will retain the applicability of the ISR procedure while ensuring that imputations preserve the marginal structure of complex survey variables (as will be illustrated in the sections that follow). It is emphasized that the methods presented in the following are widely applicable; these techniques may be applied to any data that contain theoretically continuous (or semicontinuous) variables and may be applied in conjunction with a wide array of imputation procedures.\nTo help illustrate the applicability of the methodology presented here to general imputation problems, statistical analyses that require the specific ARMS design are not the focus here. Regardless, the survey design is not expected to have a substantial influence on the choice of transformation scheme.\nIn this article, the standard errors of estimators derived using imputed data are adjusted for imputation error via multiple imputation (MI, Rubin 1987; . MI involves the generation of multiple datasets which have been imputed independently of one another; imputations are presumed to have been randomly sampled from the posterior distribution of the missing data given the observed data. Rubin's rules for combining information across datasets have been provided in a number of references (including the two given above). The validity of MI inferences in settings where complex survey data are used has been called into question frequently (Kott 1995; Fay 1996; Kim et al. 2006) . Although MI has demonstrated utility for analysis of ARMS data (Robbins and White, Forthcoming) , MI is used here primarily due to its simplicity and effectiveness in comparing imputation error across datasets imputed via differing methods."}, {"section_title": "Transformation Techniques", "text": "Let the length-n vector Y \u00bc {Y 1 ; : : : ; Y n } 0 denote a survey variable, where n is the sample size (i.e., number of experimental units). To develop a transformation scheme that attains normality, consider the fact that any continuous random variable with a known cumulative distribution function (CDF) can be transformed into a standard normal variate. Specifically, let X be any scalar random variable with known CDF F(x), and let\nrepresent the transformation function, where F\u00f0\u00c1\u00de denotes the standard normal CDF, then\nT\u00f0X\u00de , N\u00f00; 1\u00de\nIt is noted that when variables are transformed via (1) and then linked through a multivariate normal distribution (which is the model used for imputation here), the resulting model may be considered a Gaussian copula (Nelsen 2009 ). The impasse with respect to application of the above transformation scheme is the fact that in practical circumstances, the CDF F(x) tends to be unknown. Thus, in order to apply the above transformation to the positive portions survey, it is necessary to first develop a manner for determining (or approximating) the CDF of these positive portions. As mentioned above, a log-skew-normal model suffices for the majority of ARMS variables. That is, in accordance with (1); Robbins et al. (2013) suggest that if\nthen T 1 \u00f0logY i \u00de should have (or approximately have) a standard normal distribution for all relevant i. In the above, {\u0135;v;\u00e2} represent consistent estimators of the skew-normal parameters. Clearly, such marginal transformations provide no general implication that joint normality will be obtained; however, Robbins et al. (2013) illustrate rigorously that for ARMS data multivariate normality is (adequately) achieved through marginal transformation to normality. It is noted that these conclusions also hold when the nonparametric transformations proposed herein are used.\nAs was also mentioned above, the transformation in (2) is inadequate for certain ARMS variables. For instance, labor variables, where the response indicates the number of weekly hours worked, tend to observe onerous marginal distributions. Names and descriptions of ARMS variables that will be discussed in this study are given in Table 1 . Names of ARMS variables are formed by placing a \"P\" in front of the numeric item code seen on the survey questionnaire.\nAs an example, Figure 1 provides a histogram of log(P829) with the best-fitting skewnormal density curve. Only positive responses for this variable are included in this graph (and similar plots that follow). A scatter plot of log(P829) and log(P830) is also provided in the figure to illustrate the bivariate dispersion of the data points. Likewise, only units that report positive values for both variables are plotted in this graph (and similar ones that follow). These labor variables are analyzed on the log scale because logged values are closer to being Gaussian than the untransformed values. To illustrate further the specific deficiencies of the skew-normal (SN) transformation for the labor variables, Figure 2 provides a histogram log(P829) and a scatter plot of log(P829) versus log(P830), all following skew-normal transformation. As the transformed data should observe a standard normal distribution, the standard normal density is plotted over the histogram of the transformed data. Additionally, a Kolmogorov-Smirnov (KS) test under the assumption of a standard normal distribution is applied to the transformed values shown in the left graph in Figure 2 , and the distance statistic (d-stat) is given in the upperleft corner of the plot. Labor variables such as P829 tend to have repeating values, which makes the KS test theoretically inappropriate, but such results are given here and in further plots for a comparison of goodness of fit.\nThe power (or Box-Cox) transformation is often applied within imputation procedures (e.g., Raghunathan et al. 2001) . However, the Box-Cox transformation show no increase in utility over the log-skew-normal transformation described above; therefore it is not discussed further. A more robust transformation scheme is clearly warranted. Accordingly, nonparametric models for F(x) are considered."}, {"section_title": "Transformation Via the Kernel Density", "text": "Next, consider the Gaussian kernel, which is used to estimate the probability density function (PDF). Similarly, Woodcock and Benedetto (2009) use kernel densities for marginal transformation to normality. The kernel density (using a Gaussian kernel) of\nwhere h . 0 is a bandwidth parameter, and f\u00f0\u00c1\u00de represents the standard normal PDF. The CDF of Y may be approximated wit\u0125 Therefore, the kernel-density transformation for Y is\nand T 2 \u00f0Y i \u00de should appear to have been sampled from a standard normal distribution. Figure 3 provides a histogram log(P829) and a scatter plot of log(P829) versus log(P830), all following the kernel-density (KERN) transformation. Clearly, the figure provides an instance where the kernel density offers a transformation to normality that is superior to that of the skew-normal family -the plots indicate that normality assumptions appear reasonable (in both the univariate and multivariate sense).\nSelection of the bandwidth parameter, h, in kernel-density functions is a well-studied issue (Silverman 1986; Sheather and Jones 1991; Scott 2009 ). Selection algorithms often return small values of h for ARMS variables; such choices of h fail to adequately differentiate the KERN transformation from the EMP transformation described below. To avoid this issue, a bandwidth parameter of h \u00bc 0.2 is used whenever the KERN transformation is applied to ARMS data herein; this value offers adequate smoothing for the ARMS variables used."}, {"section_title": "Transformation Via the Empirical Distribution", "text": "The empirical distribution function of Y \u00bc {Y 1 ; : : : ; Y n } 0 is now considered:\nwhere 1{A} is the indicator of event A. We, however, focus on F\u00f0 y\u00de \u00bc 1 n Figure 4 provides a histogram of P829 and a scatter plot of P829 versus P830, all following the EMP transformation. Repeating values of P829 prevent the EMP transformation from achieving exact normality. Regardless, the figure indicates that the EMP transformation is also clearly superior to the SN transformation in the circumstances illustrated here.\nSince F\u00f0 y\u00de a:s \u00ff ! F\u00f0 y\u00de, the transformation in (4) is preferable when there is enough observed data to ensure that the empirical data provide a sufficient scope of the full distribution (including, most importantly, the tails)."}, {"section_title": "Untransformation", "text": "Let X represent a transformed version of Y following application of one of the aforementioned schemes. Imputations will then be created for X, resulting inX, an imputed version of the transformed data. However, the imputations must be \"untransformed\" (i.e., returned to their original scale). If a transformation of the type in (1) has been applied to Y, the following inverse transformation may be applied to the imputed values:\nwhere F 21 \u00f0u\u00de, for u [ \u00f00; 1\u00de, represents the inverse of the F\u00f0 y\u00de, for y [ \u00f021; 1\u00de. The CDF found using skew-normal assumptions, F\u00f0 yjj; v; a\u00de, and the CDF found using (5) is not directly applicable. Hence, inversion of the empirical distribution transformation works as follows. Let U \u00bc {U 1 ; : : : ; U n }, where\nNote that the U i should resemble uniform variates. Forx [ \u00f021; 1\u00de, let u x \u00bc F\u00f0x\u00de, and after setting\nuntransform imputations in variables requiring the empirical transformation by calculatin\u011d\nInverting the empirical distribution in this manner ensures that any imputation of values in variables transformed using (4) will be sampled directly from observed values. Accordingly, an imputation method that utilizes the empirical method can be considered a \"hot-deck\" technique (Little 1988; Little and Rubin 2002) . The EMP transformation is also advantageous due to its computational simplicity. However, the KERN transformation scheme is very demanding computationally (as it requires numeric integration)."}, {"section_title": "Analysis of Imputed Data", "text": "The ISR algorithm of Robbins et al. (2013) is applied to the complete 2010 ARMS dataset using the imputation model described therein, where only the transformation technique is varied. For instance, five completed datasets were independently created (in the vein of multiple imputation) where the skew-normal (SN) transformation in (2) is used for all variables requiring transformation. This process is then repeated using the kernel-density (KERN) transformation in (3) and the empirical distribution (EMP) transformation in (4). Discussion is limited to the ARMS variables described in .\nTo begin, analysis of marginal data characteristics of the variables (which contain missingness) in Table 1 is considered. Table 2 provides the unweighted sample mean ( x) and sample standard deviation (s) of the nonzero values, in addition to the between imputation variance (B) and upper bound (U MI ) and lower bound (L MI ) for the 95% confidence interval (as found using Rubin's combining rules for multiple imputation) for the population mean of each variable. The reported values of x and s represent the mean of their respective values when calculated in each of the five imputed datasets. Table 2 presents the results in \"cells\", where the top, middle, and bottom value in each cell is the respective estimate found using the SN, KERN and EMP transformations, respectively. Table 2 indicates that the choice of transformation method may result in differing values of means and variances. The discrepancies do not appear to be substantial, although it is noted that they are not explained by randomness in the imputations alone. Further, the lack of influence of the transformation type is likely due to relatively small missingness rates. It is also noted that other quantities (e.g., a 90% quantile) may be more heavily influenced by the transformation technique; however, the objective here is to present statistics that are of practical relevance.\nTo further examine marginal characteristics of imputations, discussion is now restricted to the variable P884. This variable is of particular interest because a large portion of positive and observed responses take on a single value (the specific value may not be disclosed here). This phenomenon is illustrated by the histogram of the positive and observed values of P884 which is provided in the left plot in Figure 5 . The middle plot shows the positive and observed values of P884 following the EMP transformation. The plot provides visual evidence that the EMP transformation imposes \"separation\" between values that are frequently repeated and neighboring values. This separation ensures that there is a relatively high probability that an imputed value will equal the repeating value. For instance, 16.6% of positive and observed responses for P884 take on the frequently occurring value, and 9.3% of all EMP imputations take on that value (whereas 0% of SN and KERN imputations take on the value). The right plot in Figure 5 provides kernel-density plots of observed and imputed values (for each of the three transformation schemes), which further illustrates the need for a nonparametric transformation procedure.\nThere are alternative approaches for imputing P884. For instance, a three-level mixture model which includes two indicator variables (the first one indicating the occurrence of an observation equaling zero and the second indicating the observation taking on the frequently occurring value) may be more appropriate. However, such a procedure would have to enable the second indicator variable to have missing values (since it is not known whether or not the missing values of P884 take on the frequently occurring value). Therefore, the use of the marginal transformations (as opposed to higher-level mixture models) permits the convenience of a multivariate normal imputation model while producing high-quality results. To monitor the multivariate influence of imputations sampled using the various transformation schemes, consider scatter plots. Figure 6 provides scatters plots of log(P829) versus log(P830) for each of the three transformations where pairwise positive and observed pairs are marked with an ' \u00a3 ' and imputed pairs are marked with a '\u00fe'. Lines of best fit for observed and imputed pairs are also included. Plots are given on the log scale in order to emphasize the differences between methods. The plots appear to indicate that bivariate extremes are underimputed, which may (partially) be a result of imputed values tending to be smaller than observed values for both variables in the plots. This phenomenon is to be expected for the labor variables; data indicate that \"hobby\" farmers, who are less likely to work on-farm full time, are more likely to refuse response for labor items. Regardless, the EMP transformation is clearly the most likely to preserve the underlying bivariate structure.\nTo further gauge the multivariate quality of the imputations, consider an econometric model motivated by the following. Farm operators often pursue off-farm sources of income; the on-and off-farm labor decisions of farmers have been well scrutinized in the economic literature. Economic theory suggests that the amount of time a farm operator (and the operator's spouse) choose to work on the farm is heavily influenced by factors \nwhere Z represents a set of additional categorical covariates and 1 is a mean zero error term. In the above, OPHR is the number of on-farm hours worked weekly by the farm operator (calculated as the average of P828, P829, P830 and P831). Likewise, OPOFFHR is the number of hours worked off-farm by the farm operator. OFFRATE is calculated as P952/(OPOFFHR \u00fe SPOFFHR) where SPOFFHR is the number of hours worked off farm by the operator's spouse. That is, OFFRATE represents the combined off-farm wage rate for the operator and spouse. Further, P1242 is the operator's age. Estimated values of coefficients are found using least squares while isolating to units that report nonzero values for all pertinent variables. A model similar to (6) which involves the hours worked on farm by the spouse was also considered throughout this study, but the findings are redundant and thereby omitted. Table 3 provides results for these two models. The format of this table is similar to that of Table 2 , as are the findings: The choice of transformation method may have a noticeable (but in this case not substantial) impact on the estimations found using econometric modeling."}, {"section_title": "A Simulation Study", "text": "This section presents simulation analyses which evaluate the efficacy of the proposed transformation techniques. Ideally, all assessments would be performed using real data, since synthetic data are not guaranteed to adequately mimic the complex structures encountered in practice -the motivation behind the proposed techniques is to capture such structures. Accordingly, when possible, evaluations are performed with observed ARMS data; in circumstances where such analyses do not offer sufficiently clear conclusions; a small-scale study using entirely synthetic data is used to inform the discussion."}, {"section_title": "Simulations Involving ARMS Data", "text": "A preferable technique for simulation involving real data would be to draw a sample of respondents from the observed units while treating the full dataset as a population from which population parameters can be ascertained; implementations of this scheme are seen As setup, a completed ARMS dataset is created using the imputation scheme outlined in . Specifically, the full-scale ISR algorithm and model are used in conjunction with various transformation schemes. It is not feasible to use complete cases only since there are an insufficient number of complete cases. This single completed dataset is used to create all of the benchmark values required within the simulation study. Next, missingness is randomly imposed in eight of the ARMS variables according to a probabilistic model. Imputations are then created for these newly missing values and the values of desired metrics as found using the imputed data are compared to values found using the original benchmark dataset. It is worth noting that the rate of missingness that is imposed will vastly exceed the original rate of missingness in ARMS data. The eight variables in which holes are poked are marked in Table 1 with an asterisk; some of these variables originally contained missingness, whereas others did not.\nIn addition to the eight variables in which missingness is imposed, there are 18 additional variables used as covariates within the imputation model for ISR. The imposed missingness is completely at random (MCAR, in the terminology of Little and Rubin 2002) . Specifically, any positive value is imposed as missing with a probability of 0.5, and the occurrence of imposed missingness is independent across all values. Since the imposed rate of missingness is far higher than the missingness rate in the original dataset, the influence of imputations within the benchmark study should be filtered out. The performance of ISR with density transformations has been analyzed in great detail under other missingness mechanisms (e.g., MAR and NMAR -for details, see the supplemental material of Robbins et al. 2013) . Analyses under MAR and NMAR are not expected to yield information regarding the influence of the transformation type beyond what is learned under MCAR missingness; for brevity, only MCAR is considered in these ARMS-based simulations. Since ISR is iterative (as it is a form of Markov chain Monte Carlo), each completed dataset is sampled using a burn-in period of 200 iterations.\nThe goal is to assess the potential for bias (in any point and interval estimates calculated from the ARMS data) caused by the choice of transformation method. The performance of the methodology is measured in terms of the relative change of a metric post imputation. Missingness is randomly imposed in the completed benchmark dataset 100 different times. Each time missingness is imposed, imputations are independently created five times (in the vein of multiple imputation) for each method. The methods used are as follows.\n1. SN -The skew-normal transformation of (2) is used for all variables. 2. KERN -The kernel-density transformation of (3) is used for all variables. 3. EMP -The empirical distribution transformation of (4) is used for all variables. 4. EMPABB -EMP with an approximate Bayesian bootstrap.\nThe transformation schemes discussed in Section 3 will result in imputations that understate variability due to the fact each transformation scheme requires that any variable's CDF, F(x), be treated as known despite the fact that F(x) is, in fact, estimated.\nTo address this issue, Woodcock and Benedetto (2009) suggest an approximate Bayesian bootstrap (ABB), where F(x) is estimated using a bootstrapped pool of observations as opposed to the actual pool of observations. Here, ABB is used together with the EMP method, resulting in EMPABB as above.\nLet x denote the benchmark dataset, and let x \u00bdd k denote the d th completed dataset (d \u00bc 1; : : : ; 5) as imputed for the k th artificially incomplete dataset (where k \u00bc 1; : : : ; 100). Finally, let u\u00f0\u00c1\u00de denote a metric of interest (where the argument represents the dataset used to compute the metric). The percent change in the metric is computed via Du\u00f0k\u00de \u00bc 100 u k 2 u\u00f0x\u00de u\u00f0x\u00de ; where\nResults are presented in the form of box plots of the 100 values of Du\u00f0k\u00de.\nMetrics tracked in this simulation study include the sample mean and standard error of the sample mean as calculated over the nonzero values of each variable in which missingness is imposed in addition to the regression coefficients in (6) and their respective standard errors. Note that the standard error of a sample mean equals the sample standard deviation times a constant (i.e., n 21=2 obs ). Covariances were also monitored but yielded results that mimic those of the regression coefficients (accordingly, those results are omitted from the discussion). Confidence intervals for the sample means and regression coefficients can be calculated using Rubin's combining rules for MI, although the details are omitted here.\nFindings are shown in Figure 7 for P784, P829, b 1 and b 2 . The results indicate that for certain variables (e.g., P829) whose marginal distributions cannot be modeled with an appropriate parametric density, biases in basic marginal characteristics may be induced if one does not utilize a nonparametric transformation. Further, the nonparametric transformations result in imputations that appear to adequately preserve the quantities studied here (though there may be evidence of a moderate decrease in the variance of P784 caused by the nonparametric methodology). Likewise, there does not appear to be an advantage to using the EMPABB method in place of the EMP method.\nFinally, since the empirical distribution transformation is designed to handle repeating values, it has the potential to be applied to variables that are binary or ordinal (though not strictly categorical with more than two categories). However, such efficacy of the transformation for such a purpose has not been thoroughly investigated.\nOf interest is P784; this variable was included in this study since it has a particularly low number of positive and observed values (151 in the true dataset and thereby approximately 75 prior to imputation within the simulation study -see Table 1 ). Parametric and nonparametric transformations (when the former are well fit) are expected to perform equivalently on large samples (wherein sufficient data are available to adequately approximate the CDF under all transformation types); discrepancies between transformations are anticipated to be most visible when there are few observations available. To that end, it is noted that the SN transformation results in a substantially wider confidence interval for the mean of the nonzero observations of P784 (approximately three to four times wider on average than the KERN and EMP transformations) within the simulations used to generated Figure 7 . Since the SN model seems appropriate for this variable (the KS test yields a p-value of 0.801 when a skew-normal distribution is assumed), since it seems unreasonable to assume that 75 observations can sufficiently quantify a CDF, and since Figure 7 implies that the nonparametric transformations may decrease the variance of this variable, it is suggested that the SN transformation is more appropriate than the nonparametric transformations for P784. Ideally, comparisons to predictive mean matching (PMM, Little 1988) could have been presented in this study. PMM is a popular technique that builds a predictive model for imputations through regression, and then samples imputations from observed datamaking it similar to (and useful in the same settings as) the methods presented here. However, direct comparisons to PMM within the simulations above (wherein such comparisons would be most useful due to the unknown distributional structure of ARMSdata) cannot be made here due to computational constraints. For instance, one iteration of ISR takes 1.15 seconds, and one iteration of MICE with PMM takes 15 minutes when run on the group of variables used above. These computations are executed on a 64-bit Windows machine with a 3.3 GHz processor and 8.0 GB of RAM.\nTo summarize, the above study helps to verify the efficacy of the proposed methodology on real data, but it has some notable shortcomings. For instance, it is desirable to investigate the comparative performance of the proposed techniques against other methods such as PMM, and to present results for a variety of missingness structures. Many of these shortcomings are the consequence of computational issues. Furthermore, the above simulations leave unanswered the question as to whether or not a parametric transformation is preferable in settings involving small samples. A small-scale study involving fully synthetic data is thus presented below."}, {"section_title": "Simulations Involving Synthetic Data", "text": "The small scale of the following simulation study (only two variables are used for various sample sizes) makes it computationally feasible to consider a variety of methods and missingness mechanisms. Specifically, the four transformation techniques mentioned above (SN, KERN, EMP, and EMPABB) are used in conjunction with ISR. As needed, skew-normal MLEs are used, and the kernel bandwidth parameter is estimated via the method of Sheather and Jones (1991) . Further, PMM is considered (while used in conjunction with mice) as well as IRMI (Templ et al. 2011) ; no transformation is used when these methods are applied.\nData are generated as follows. Let X \u00bc {X 1 ; : : : ; X n } represent a random sample from a skew-normal distribution with parameters j \u00bc 4, v \u00bc 2 and a \u00bc 22. Additionally, let X \u00bc {X 1 ; : : : ;X n } represent the version of X that has been transformed in accordance with (2) while using the true parameter values, and define Y \u00bc {Y 1 ; : : : ; Y n }, where Y i \u00bc 1 \u00fe 0:5X i \u00fe 1 i for i \u00bc 1; : : : ; n, and where 1 \u00bc {1 1 ; : : : ; 1 n } is a random sample of length n from a standard normal distribution.\nMissingness is imposed in the values of X through the following mechanisms. Under MCAR missingness, each observation of X is missing with probability 0.5. For MAR missingness, X i is missing with a probability equal to 1=\u00f01 \u00fe exp\u00f02\u1ef8 i \u00de\u00de, where\u1ef8 i represents a standardized version of Y i . NMAR missingness was also considered, but the results are excluded for brevity since they provided no additional information regarding the choice of transformation scheme beyond what is learned from the other mechanisms. Imputations in X are generated via the techniques mentioned above; the elements of Y are not transformed at any point. Further, m \u00bc 5 imputed datasets are created, and no burn-in period is necessary since missingness is restricted to one variable. MI point and interval estimates are generated for a handful of parameters, and the entire process is replicated independently 1,000 times for various values of n.\nFor a given imputation method, missingness mechanism, and value of n, let\u00fb k denote the MI point estimate of a generic parameter u calculated following the k th replication (k \u00bc 1; : : : ; 1; 000). The percent bias in the multiple imputation estimate of u is approximated by calculating Du \u00bc 100 P 1;000 k\u00bc1 \u00bd\u00f0\u00fb k =u\u00de 2 1=1;000. Similarly, the sequence of 1,000 values of\u00fb k can be tested to see if the percent bias is statistically nonzero. Further, the empirical coverage of the MI interval estimate of u is calculated via the portion of the 1,000 replications in which the true value of u is contained within its 95% confidence interval.\nFirst, we consider the basic univariate parameters m \u00bc E\u00bdX 1 and s 2 \u00bc Var\u00f0X 1 \u00de; results are given in Table 4 . All transformation methods offer strong performance in terms of bias and coverage for these parameters, as does the PMM procedure. However, the IRMI procedure shows some evidence of bias and observes poor coverage for these simple quantities. It appears that all methods induce a small amount of bias (which mostly disappears with increasing n) under MAR missingness; the fact that this bias tends to be negative is a consequence of the form of the function that generates the MAR missingness. Moreover, the results imply that the use of the approximate Bayesian bootstrap does not improve the results. Finally (and most importantly), all transformation schemes appear to offer equivalent performance.\nIn order to provide parallels to the log-skew-normal distributions that positive portions of ARMS data observe, we also study summary statistics of the transformed variable U i \u00bc exp\u00f0X i \u00de. Specifically, we use multiple imputation to develop point and interval estimates of g \u00bc E\u00bdU 1 and n 2 \u00bc Var\u00f0U 1 \u00de by applying Rubin's combining rules to the sequence {\u00db 1 ; : : : ;\u00db n }, where\u00db i represents a version of U i that contains imputations of missingvalues. The ability of an imputation algorithm to preserve such quantities is a strong indication that the distribution of the imputed data matches that of the actual data had they been fully observed (since g and n 2 follow from the specificform of the MGF of X 1 ). Results for these two quantities are shown in Table 5 . The table indicates that IRMI imputations provide biased estimates of g and n 2 under all missingness mechanisms. This observation is not surprising, since IRMI does not take steps to ensure that the full distributional structure is captured in the imputation process. Although all methods are more imprecise in their estimation of g and n 2 than of m and s 2 , Tables 4 and 5 both yield the same conclusions regarding the comparative performance of the techniques.\nIn summary, the key findings of the simulation studies presented in this subsection are that all methods involving transformation are comparable to PMM and that the choice of transformation technique does not have a significant influence on bias or coverage probabilities. The latter finding is noteworthy because the SN method, which is ideally suited to this setting, shows no gains over the nonparametric methods, whereas the Table 4 . Empirical bias and coverage probabilities (the latter are in parentheses) of the point estimates and 95% confidence intervals (found using MI) of two parameters involving the synthetic random variable X. An asterisk indicates that the bias is statistically nonzero at the 0.01 significance level nonparametric methods will certainly provide higher efficacy in settings where the skewnormal assumption is violated. Figure 7 shows that the nonparametric methods yield a decrease in the variance of P784, and Tables 4 and 5 implicate that all methods may have decreased variability in items with small sample sizes. This decrease is not seen by the SN method in Figure 7 , perhaps because the skew-normal distribution does not adequately capture the tails of the distribution of P784 (which also helps to explain the outlying values for this variable and transformation method in Figure 7 )."}, {"section_title": "Comments", "text": "Nonparametric transformation of survey data prior to imputation provides a straightforward manner through which unique marginal data characteristics can be preserved throughout the imputation process -such transformations are also shown to maintain multivariate aspects. The empirical transformation described above has the added advantage that imputations are drawn from observed data, which makes a method that utilizes it a nearest neighbor-type technique, and which also increases the probability that complex underlying data structures (that are common in establishment surveys) are maintained. Further, the empirical transformation is advantageous due to its computational simplicity.\nThe evaluations presented in this article did not unveil circumstances in which a transformation based upon a parametric model (i.e., the skew-normal distribution) is clearly preferable to the nonparametric methods. Further, no settings were found in which a transformation based upon a kernel density outperformed the transformation based upon an empirical distribution -the latter is more computationally efficient. In light of the above, the recommendation is that in practical circumstances the empirical distribution transformation be used when possible (however, further evaluations beyond those presented here may be needed to support this conclusion). With any transformation method, the practitioner should always investigate the validity of the posttransformation multivariate model (a joint normal distribution was used here) prior to generating imputations.\nAs an additional comment, it is noted that the nonparametric methods are applied here while exclusively using ISR (Robbins et al. 2013) . ISR has the restriction that variables with missing values be sampled from continuous distributions. However, the nonparametric transformations are applicable in conjunction with any imputation technique which applies normality assumptions to continuous variables. For instance, these transformations could be employed with IVEware (Raghunathan et al. 2002) or MICE (Van Buuren and Oudshoorn 1999), which include capabilities for imputation of categorical variables. Furthermore, it is also possible to use the methods discussed here for simulation of fully or partially synthetic datasets for the purposes of data confidentiality (Rubin 1993; Reiter 2002; Raghunathan et al. 2003) . Woodcock and Benedetto (2009) use a kerneldensity transformationfor this purpose, and it is noted that the empirical transformation has such utility if it is acceptable for synthetic values to be sampled from the observed data."}]