[{"section_title": "Introduction", "text": "Model averaging is receiving growing attention in statistics and econometrics. Averaging is a smoothed extension of model selection, and substantially reduces risk relative to selection. The key issue is weight selection. A traditional approach to model selection is to minimize an information criterion that is an estimate of the risk of the selected estimator (e.g., Akaike (1973), Mallows (1973)). Similarly, averaging weights can be selected by minimizing an information criterion that is an estimate of the risk of the averaging estimator, as proposed in Hansen (2007). While the asymptotic nonparametric optimality of such estimators has been established, our understanding of the sampling distribution remains incomplete. Following Hjort and Claeskens (2003), Schorfheide (2005), Saleh (2006), and Hansen (2013), this paper explores the asymptotic distribution and risk of nested averaging estimators in a local asymptotic framework where the coefficients are in a root-n neighborhood of 0. We derive the asymptotic distribution of a general class of averaging estimators that minimize a penalized least-squares criterion. We show that the asymptotic distribution can be written as a (nonlinear) function of the normal random vector that characterizes the unrestricted estimator. We then derive a representation for the asymptotic risk that is similar in form to those of shrinkage estimators (as presented, for example, in Lehmann and Casella (1998)). Using this representation, we derive sufficient conditions for the asymptotic risk of the averaging estimator to be globally smaller than the risk of the unrestricted estimator. We find that this condition is a simple generalization of the classic condition for shrinkage estimators. In particular, the Mallows averaging estimator of Hansen (2007) satisfies this condition under homoskedasticity if the regressors are grouped in sets of four or greater. This means that if we restrict attention to submodels that are differentiated by four or more regressors, we can guarantee that the Mallows averaging estimator will have reduced mean-squared error (MSE) relative to the least-squares estimator, regardless of the values of the coefficients or the distributions of the regressors or regression errors. We find in a simple simulation experiment that this modified averaging estimator has substantially reduced risk relative to the standard averaging estimator as well as the least-squares estimator, and has much better risk performance than alternative methods such as the least absolute shrinkage and selection operator (Lasso) (Tibshirani (1996)), smoothed Akaike information criterion (AIC) (Burnham and Anderson (2002)), and approximate Bayesian model averaging (BMA). The message from this analysis is simultaneously subtle yet profound. First, it reinforces our view that selection and averaging methods should be derived from rigorous theory, not from intuition or analogy. Second, it points to the need for careful examination of the submodels used for estimation. Rather than simply estimating every possible submodel, we should limit the number of submodels and enforce the constraint that the separation between each submodel be four coefficients or greater. Nested model selection and averaging rests on the implicit assumption that the regressors are individually ordered, from \"most relevant\" to \"least relevant.\" Similarly, our method requires that the regressors are groupwise ordered. In practice, it may be much easier to order regressors by groups rather than individually. For example, the difference between specifications may be whether or not all state dummy variables are included, which is a 50-member grouping. In this sense, our focus on groupwise ordering is somewhat attractive. A limitation of our analysis is that it is critically confined to nested models. Nesting permits the application of Stein's lemma (Stein (1981)), which lies at the heart of our risk calculations. It would be greatly desirable to extend our results to the case of nonnested models, but it unclear how to do so. This paper builds on an extensive literature. Stein (1956) first showed that a Gaussian estimator is inadmissible when the number of coefficients exceeds two. A feasible estimator with smaller risk than the Gaussian estimator was introduced by James and Stein (1961). Baranchick (1964) showed that a positive-part James-Stein estimator has even smaller risk. Efron and Morris (1973b) showed the close connection between Stein and empirical Bayes estimators. Akaike (1973), Mallows (1973), and Schwarz (1978) introduced information criteria suitable for model selection. Judge and Bock (1978) provided an extensive evaluation of the Stein-rule estimator in linear regression. Leamer (1978) proposed the method of Bayesian model averaging. Akaike (1979) proposed the expo-nential AIC as an analog of Bayesian probability weights. Lehmann and Casella (1998) provided an excellent introduction to the theory of shrinkage estimation. Saleh (2006) is a recent review of statistical shrinkage methods. The idea of grouping regressors for shrinkage has been investigated previously in the statistics literature, including Efron and Morris (1973a), Berger and Dey (1983), Dey and Berger (1983), and George (1986aGeorge ( , 1986b. Model averaging is an extension of the idea of forecast combination, which was introduced by Bates and Granger (1969) and spawned a large literature. The idea of using Bayesian model averaging for forecast combination was pioneered by Min and Zellner (1993). Some excellent reviews include Clemen (1989), Diebold and Lopez (1996), Hendry and Clements (2004), Timmermann (2006), and Stock and Watson (2006). Related ideas are the empirical Bayes regressions of Knox, Stock, and Watson (2004), and the bagging method of Inoue and Kilian (2008). Model averaging methods are receiving an explosion of interest in econometrics and statistics. Averaging methods for linear regression have been proposed by Buckland, Burnham, and Augustin (1997), Hjort and Claeskens (2003), Danilov and Magnus (2004), Hansen (2007), Hansen and Racine (2012), Liu (2012), and Liu and Okui (forthcoming). The theory has been further studied in Magnus (2002), Magnus, Powell, and Pr\u00fcfer (2010), Wan, Zhang, and Zou (2010), Liang, Zou, Wan, andZhang (2011), andMcCloskey (2012). Averaging for instrumental variable and generalized method of moments estimation has been proposed by Kuersteiner and Okui (2010), Liao (2012), Lee and Zhou (2011), and DiTraglia (2013). The remainder of the paper is organized as follows. Section 2 introduces the regression model and submodels. Section 3 introduces the submodel estimators. Section 4 presents the class of penalized weight criteria, and Section 5 rewrites the criteria using cumulative weights. Section 6 demonstrates the connection between the averaging estimator and James-Stein shrinkage. Section 7 presents the asymptotic distribution of the averaging estimator in the local asymptotic framework, and Section 8 calculates the asymptotic risk. Section 9 simplifies the conditions under bounded heteroskedasticity. Section 10 discusses weight selection under heteroskedasticity. Section 11 presents the results of simulation experiments. Section 12 is an empirical application to a regression example from Fryer and Levitt (2013). Section 13 presents a conclusion. Mathematical proofs are presented in the Appendix. Further simulation results are presented in a supplemental appendix, available on the journal website, http://qeconomics.org/supp/332/supplement.pdf. The replication codes for the simulation experiment and empirical application are also posted on the journal website, http://qeconomics.org/supp/332/code_and_data.zip."}, {"section_title": "Regression model", "text": "We have observations {y i x i : i = 1 n}, where y i is real-valued and x i is K \u00d7 1. The observations are assumed to satisfy the linear projection equation In matrix notation, we write the equation as We assume that the regressors can be partitioned into ordered groups as , where x ji is k j \u00d7 1 and the total number of regressors is We then consider M nested submodels, where the mth can be written as That is, the mth submodel includes the regressors x 1i through x mi and excludes the remaining regressors. In matrix notation, Note that the mth submodel has regressors and that the regressors x 1i are included in all models. Notationally, we allow k 1 = 0, in which case there is no x 1i and model 1 is the zero vector. Note that the error in equation 2is not a projection error, as the coefficients are defined in the full model (1) and thus have common meaning across models. Another way of thinking about this is that equation 2has omitted variables. The ordering of the groups is important, as x 1i is included in all submodels, x 2i is included in all submodels except for model 1, and so on. Thus it is prudent for the user to construct the ordering so that the variables expected to be most relevant are included in the first groups, and those expected to be least relevant are included in the final groups. If the regressors have been standardized to have zero mean and common variance, then it would be ideal if the regressors are ordered so that their coefficients are descending in absolute value. The performance of our averaging estimator will depend on this ordering, in the sense that the efficiency gains will be greatest when the regressors have been so ordered. However, for all of our theoretical results, we do not impose any assumption on the ordering; it is not required to be \"correct\" in any sense."}, {"section_title": "Estimation", "text": "The unconstrained least-squares estimator of \u03b2 in the full model is with residual e i = y i \u2212 x i \u03b2 LS or in vector notation as e = y \u2212 X \u03b2 LS . The standard estimator of the coefficient vector \u03b2 in the mth submodel is the leastsquares estimate of y on the included regressors X m . Notationally, let be the K \u00d7 K m matrix that selects the included regressors; thus X m = XS m . The leastsquares estimate of \u03b2 m in the mth submodel is and that of \u03b2 is The corresponding residual is w M ) be a weight vector. We require that w m \u2265 0 and M m=1 w m = 1, and thus is an element of the unit simplex in R M : An averaging estimator of \u03b2 is The residual from the averaging estimator is w m e mi or in vector notation"}, {"section_title": "Penalized weight criteria", "text": "A general class of penalized least-squares criteria take the form C n (w) = e(w) e(w) The leading term e(w) e(w) = e mi e i is the sum of squared residuals from the averaging estimator. The constants T m are (possibly data-dependent) penalties satisfying For example, the Mallows averaging criterion (Hansen (2007) where K m is the number of coefficients in model m as defined in (3), and is the standard estimator of the unconditional error variance \u03c3 2 = E(e 2 i ). Thus the Mallows averaging criteria is Given the criterion (7), the selected weight vector w is the element of the unit simplex that minimizes 7: The averaging estimator (6) computed with the weights (10) is then For the weight vector w MMA which minimizes the Mallows averaging criteria (9), Hansen's (2007) Mallows model averaging (MMA) estimator is"}, {"section_title": "Cumulative weight criteria", "text": "It turns out that there is a convenient alternative representation of the averaging estimator (11) in terms of the cumulative weights: . Notice that w \u2208 H is equivalent to w * \u2208 H * , where Similarly, define the selected cumulative weights and set We can equivalently discuss averaging in terms of the weights w \u2208 H or cumulative weights w * \u2208 H * . Notice that (6) is equivalent to and 11is equivalent to The representation in terms of the cumulative weights w * is convenient, as the penalized least-squares criterion (7) can be written as a simple function of w * . Define the marginal penalty for model m + 1 as Note, for example, that for the Mallows criterion, we have t m+1 = s 2 k m+1 . Also, let L m = e m e m denote the sum of squared residuals in the mth model. Lemma 1. For the penalized criterion (7), Hence Lemma 1 shows that C n (w) and C * n (w * ) are equivalent up to the term L M + 2 T M , which does not depend on the weight vector and thus the cumulative weights (13) are the minimizers of (16). We call C * n (w * ) the cumulative criterion. It is a simple function of w * , as it is quadratic with no cross-terms. The representation (15)-(17) turns out to be useful because it facilitates an asymptotic distribution theory for the averaging estimator, as we show in Section 7."}, {"section_title": "James-Stein shrinkage", "text": "Consider the case of two submodels so M = 2 and for simplicity suppose k 1 = 0. In this case, write w = w 1 = w * 1 and t = t 2 so that (16) equals where the second line uses the substitution L 1 \u2212 L 2 = y y \u2212 e e = \u03b2 X X \u03b2. The solution (17) minimizes this function subject to the constraint 0 \u2264 w \u2264 1, and equals otherwise. It follows that the averaging estimator (15) equals where (a) + = a1 a\u22650 is the positive part operator. This averaging estimator (18) is the classic James-Stein estimator with shrinkage parameter t. That is, when there are two models, averaging estimators whose weights minimize penalized least-squares criteria of the form 7are numerically identical to the James-Stein estimator. This is fascinating as it shows that averaging estimators are in the class of shrinkage estimators. Furthermore, note that the classic James-Stein recommendation was to set t = s 2 (K n \u2212 2), while the Mallows criterion sets t = s 2 K n . This is a modest difference for small K n , and is quite minor when K n is large."}, {"section_title": "Asymptotic distribution", "text": "We require that the least-squares estimator is asymptotically normal. The following condition is sufficient for our needs. Assumption 1. 1. Either (a) {y i x i } is independent and identically distributed (i.i.d.) with finite fourth moments or (b) {y i x i } is a strictly stationary and ergodic time series with finite q > 4 moments and E(e i |F i\u22121 ) = 0, where ; e i\u22121 e i\u22122 ). The following conditions are required to obtain an asymptotic distribution for the penalized weight criterion. Assumption 1 states that the penalties in (7) converge in probability to constants. For example, in the case of Mallows averaging with i. Assumption 2 is a local asymptotic framework and it specifies the coefficients \u03b2 m to be in a local n \u22121/2 neighborhood of 0. The coefficients \u03b2 1 are not included in Assumption 2 since these variables are included in all models. The local asymptotic framework is a technical device commonly seen in model selection and averaging asymptotic theory, for example, Hjort and Claeskens (2003) and Schorfheide (2005). It allows the application of asymptotic theory, for in a constant parameter model, the largest model will always dominate and the asymptotic analysis will not produce a useful approximation. Alternatively, Assumption 2 could be omitted and replaced by the assumption that the errors e i are i.i.d. N(0 \u03c3 2 ) with known \u03c3 2 , in which case the results described below are exact and finite sample, rather than asymptotic. The virtue of the local asymptotic framework of Assumption 2 is that it does not require i.i.d. normality and thus allows application to the wide variety of practical econometric applications. It is not a practical restriction. where where and The function \u03b7(x) is absolutely continuous, and w * (x) satisfies the following characterization. For all x, there exists an integer J(x) \u2264 M and index set {m and The main contribution of Theorem 1 is 21, which is a representation of the asymptotic distribution of the averaging estimator as a (nonlinear) function of the limiting normal random vector Z. The characterization of this function in (22)-(28) will allow us to apply Stein's lemma to calculate the estimator's asymptotic risk. In addition, the asymptotic distribution (21) may be useful for alternative purposes such as inference. The representation (21)-(22) shows that the weights are asymptotically random functions of the limiting distribution (19) plus the localizing parameters \u03b4. The characterization of the weights in (26)-(28) shows that given the random variable x = Z + \u03b4, there is a set of models {m 1 (x) m J (x)} that receive positive weight and the remaining models receive zero weight. The set of models that receive positive weight is random (depends on Z), but largely influenced by the localizing parameters \u03b4."}, {"section_title": "Asymptotic risk", "text": "The asymptotic trimmed risk or weighted mean-squared error (MSE) of an estimator \u03b2 for \u03b2 is While we use the matrix Q to weight the estimates, in principle other weight matrices could be used. The choice Q is particularly convenient for two reasons. One, it induces invariance to parameter scaling and rotation. Two, with this choice, the MSE function (29) plus \u03c3 2 corresponds to out-of-sample mean-squared forecast error (under stationarity), which is a natural risk measure in time-series applications. The trimming in (29) conveniently avoids the need to establish uniform integrability. When \u03b2 has an asymptotic distribution, that is, \u2212\u2192 \u03c8, then the asymptotic trimmed risk equals E(\u03c8 Q\u03c8) and is thus straightforward to calculate. For example, the unrestricted least-squares estimator \u03b2 LS from (19) has the asymptotic distribution \u221a n( We are now in a position to calculate the asymptotic risk of the averaging estimator. It will be convenient to define the matrices and the constants where Q m = E(x mi x mi ). As we discuss in the next section, under conditional homoskedasticity, we have the simplifications \u03a9 = Q\u03c3 2 and D m = \u03c3 2 K m , so D m is a measure of the number of coefficients adjusting for heteroskedasticity. Theorem 2. Under Assumptions 1 and 2, Equations (31)-(32) give an expression for the asymptotic risk of the averaging estimator. We now use this expression to show that its risk is smaller than the unrestricted least-squares estimator under a mild condition. Let \u03bb max (A) denotes the largest eigenvalue of a symmetric matrix A and define and Assumption 3. For all m \u2265 2, Theorem 3. Under Assumptions 1, 2, and 3, Theorem 3 states that the averaging estimator has smaller asymptotic trimmed risk than the unrestricted estimator. This holds regardless of the values of the coefficients \u03b2 or other characteristics of the data distribution. Notice in particular that the result does not depend on the ordering of the regression groups. That is, (35) holds even if the groups have been improperly ordered. This shows that in a quite generic sense, the averaging estimator \u03b2 A dominates the least-squares estimator \u03b2 LS . The key condition is Assumption 3. Condition (a) states that the number of coefficients in each regression group, adjusted for heteroskedasticity, is sufficiently large. Condition (b) states that the marginal penalties t m are all positive, but not too large. Notice that condition (a) is necessary for condition (b) to be feasible. Notice that Assumption 3 does not impose any conditions on the regressors x 1i that are included in all models. The set of such regressors can have any dimension, including 0, and still satisfy Assumption 3."}, {"section_title": "Bounded heteroskedasticity", "text": "We can simplify Assumption 3 in a leading case of interest. Define the conditional variance function E e 2 i |x i = x = \u03c3 2 (x) its maximal and minimal values and the \"variance ratio\" In the leading case of conditional homoskedasticity, E(e 2 i |x i ) = \u03c3 2 , and then \u03c3 2 = \u03c3 2 and r = 1. The deviation of r from 1 measures the degree of heteroskedasticity. We now give a sufficient condition for Assumption 3. Assumption 4. r < \u221e and for all m \u2265 2, Lemma 2. Assumption 4 implies Assumption 3. Corollary 1. Under Assumptions 1, 2, and 4, the averaging estimator (11) satisfies Assumption 4(a) states that the number of coefficients in each regression group is larger than twice the variance ratio r. In the context of two models (M = 2) and homoskedasticity, Assumption 4 is identical to the conditions required for a Stein-type estimator to be minimax-to have globally smaller risk than the unrestricted estimator. Assumption 4 extends these conditions to the case of multiple models, that each regressor \"group\" x ji has three or more regressors and the marginal penalties satisfy 0 < t m \u2264 2(k m \u2212 2)\u03c3 2 . In the important special case of the Mallows averaging estimator (12), a sufficient condition for the asymptotic marginal penalties t m = \u03c3 2 k m to satisfy Assumption 4 is k m \u2265 4r/(2 \u2212 r). Assumption 5. r < 2 and for all m \u2265 2, k m \u2265 4r/(2 \u2212 r). Corollary 2. Under Assumptions 1, 2, and 5, the Mallows averaging estimator Corollary 2 is a powerful and important result. It shows that for regression models, the Mallows averaging estimator \u03b2 MMA of Hansen (2007) globally dominates the leastsquares estimator \u03b2 LS as long as the degree of heteroskedasticity is not too large and the regressor groupings each have a sufficiently large number of regressors. Corollary 2 shows that this modification guarantees that the estimator is a strict improvement on least squares. A particularly important case of interest is conditional homoskedasticity E(e 2 i |x i ) = \u03c3 2 . Corollary 3. Suppose Assumptions 1 and 2 hold, E(e 2 i |x i ) = \u03c3 2 , and k m \u2265 4 for all m \u2265 2. Then the Mallows averaging estimator Corollary 3 is our clearest statement of the gains from regressor grouping. It shows that for homoskedastic regressions, a sufficient condition for the global dominance of the Mallows averaging estimator over the least-squares estimator is that each regressor grouping has four or more regressors. This is a simple modification of the averaging estimator. In Section 11, we show that this modification results in significant improvements in finite sample mean-squared error. As a final remark, we note that for the case M = 2 under homoskedasticity, the condition k m \u2265 4 is both necessary and sufficient for R( \u03b2 MMA \u03b2) < R( \u03b2 LS \u03b2), as the inequality 0 < t \u2264 2(k \u2212 2)\u03c3 2 is both necessary and sufficient for the James-Stein estimator to be minimax, and this condition is violated when k = 3 and t = 3\u03c3 2 . However, when M > 2, the necessity of k m \u2265 4 is unclear."}, {"section_title": "Heteroskedasticity", "text": "When heteroskedasticity is present, it may be desirable to use alternative penalties. We describe two possible approaches and their properties. First, the upper bound in Assumption 4(b) suggests that the penalties could be based on estimates of the smallest conditional variance \u03c3 2 rather than the unconditional variance \u03c3 2 . Let \u03c3 2 be an estimate of \u03c3 2 . For example, \u03c3 2 = min 1\u2264i\u2264n \u03c3 2 (x i ), where \u03c3 2 (x) is a Nadaraya-Watson estimator of \u03c3 2 (x) = E(e 2 i |x i = x) from a standard kernel regression of the squared residuals e 2 i . Setting Alternatively, we can set the penalty T m to be a consistent estimate of D m defined in (30), so that the marginal penalty t m is a consistent estimator of d m defined in (34). An example is the heteroskedasticity-robust Mallows criterion of Liu and Okui (forthcoming), which sets the penalties to equal where The penalties T LO m are moment estimators of D m with a degree-of-freedom adjustment n/(n \u2212 K), which Liu and Okui suggest is useful in finite samples. It follows that  Corollary 4. Under Assumptions 1, 2, and 6, the averaging estimator The condition k m \u2265 4r in Assumption 6 is considerably less restrictive than the condition k m \u2265 4r/(2 \u2212 r) in Assumption 5. This suggests that averaging estimators using these modified penalties should have broader robustness properties than the Mallows averaging estimator. Closely related to the Liu-Okui estimator is the jackknife model averaging (JMA) estimator of Hansen and Racine (2012). This is an averaging estimator (11), where the weights are selected to minimize the cross-validation criterion is the leave-one-out prediction residual, which is easily computed using the algebraic The jackknife criterion JMA n (w) is close to the Liu-Okui heteroskedasticity-robust Mallows criterion and thus has similar MSE properties under conditional heteroskedasticity."}, {"section_title": "Finite sample simulations", "text": "We now use simulation 1 to investigate the finite sample performance of the averaging estimators. We explore both cross-section and time-series settings."}, {"section_title": "Cross-section regression", "text": "The cross-section model is similar to that used in Hansen (2007). The data are generated by the linear regression with E(e i |x i ) = 0 and E(e 2 i ) = 1. We set x ji \u223c N(0 1). For the results reported here, we set e i \u223c N(0 1) and M = 12, though we discuss sensitivity to these assumptions below. We vary the sample size n among {50 150 500 1000}. The coefficients are set as \u03b2 0 = 0 and \u03b2 j = cj \u2212\u03b1 for j \u2265 1 with \u03b1 \u2265 0. Higher values of \u03b1 mean that the coefficients \u03b2 j decline more quickly to zero as j increases. Lower values of \u03b1 mean that the coefficients \u03b2 j are of relatively similar magnitude. Thus \u03b1 controls the trade-off between bias and parsimony, a key issue in model selection. We vary \u03b1 among {0 1 2 3}. Notice that in contrast to the asymptotic theory, we will treat the coefficients as fixed when we vary the sample size, so that the experiments reported here correspond to actual econometric practice. The coefficient c is selected to vary the population The estimators \u03b2 of \u03b2 are assessed by finite sample mean-squared error We calculate MSE( \u03b2 \u03b2) by simulation, averaging across 10,000 independent replications. We also normalize the MSE by the MSE of the unconstrained ordinary leastsquares estimator \u03b2 LS . Thus a reported MSE below 1 indicates that the estimator has smaller MSE than unconstrained ordinary least squares (OLS), and a reported MSE above 1 indicates that the estimator has larger MSE than unconstrained OLS. The estimates were constructed from M + 1 separate regressions of the form for m = 0 M and then \u03b2(m) = ( \u03b2 0 (m) \u03b2 1 (m) \u03b2 2 (m) \u03b2 m (m) 0 0) was set. These are nested regression models ranging from intercept only to unconstrained. From these M + 1 regressions, the following estimates were compared. 1. OLS: Ordinary least squares \u03b2 LS = \u03b2(M). 2. MMA: The Mallows model averaging estimator using all M + 1 models. 3. MMA 4 : The Mallows averaging estimator, grouping regressors in sets of four. For M = 12, this includes models m = {0 4 8 12}. 4. Stein: A James-Stein estimator, shrinking the unconstrained least-squares estimator \u03b2(M) toward the intercept-only model \u03b2(0). (Tibshirani (1996)) with penalty \u03bb selected to minimize fivefold cross-validation. 6. BMA: Approximate Bayesian model averaging or smoothed Bayesian information criterion. (Akaike (1979), Buckland, Burnham, and Augustin (1997), and Burnham and Anderson (2002))."}, {"section_title": "Lasso: Least-angle regression", "text": ""}, {"section_title": "SAIC: Smoothed Akaike information criterion", "text": "Methods 4-7 are alternative averaging and shrinkage methods that are included for comparison. As discussed in Section 6, the Stein estimator is equivalent to the MMA estimator restricted to two submodels. Thus by comparing MMA 4 with the Stein estimator, we illustrate the gains by averaging over more than two models. The Lasso is a popular method for regression shrinkage that does not require regressor ordering. We use the glmnet function in R with all default settings. BMA and SAIC are two popular model averaging methods. The estimators take the form (11) with the weights w m proportional to exp(\u2212 1 2 BIC m ) (where BIC denotes the Bayesian information criterion) and exp(\u2212 1 2 AIC m ), respectively, with The results are reported graphically in Figures 1-4. Each figure corresponds to a single value of \u03b1, and each figure has four panels, for n = 50, 150, 500, and 1000. Each panel plots the normalized MSE of the estimators as a function of the population R 2 . To reduce the clutter in the figures, the SAIC method is not displayed here, but is displayed in the plots in the supplemental appendix available on the journal website. (In most cases, SAIC performs quite similarly to MMA.) From the results, we can see some clear trends. First, both the MMA 4 and the Stein estimators globally have reduced risk relative to OLS (their normalized MSEs are everywhere less than 1), but the MMA estimator has risk that is greater than OLS for some parameter values. This confirms a strong prediction of the asymptotic theory. Second, for most parameter values, the MMA 4 estimator dominates the Stein estimator. This is especially the case for large values of \u03b1 and for small sample sizes. This is because the MMA 4 estimator is able to exploit the ordering of the regressors, while the Stein estimator treats all symmetrically. For larger \u03b1, the differences in MSE are quite substantial. Third, for most parameter values, the MMA 4 estimator dominates the MMA estimator. This is especially the case for large sample sizes. In summary, by grouping the regressors in sets of four before applying Mallows averaging, MSE is reduced for most parameter settings, and the averaging estimator globally dominates OLS. The results also show that the MMA 4 method compares quite well relative to the alternative averaging and shrinkage methods. In particular, the BMA method has er- ratic performance, with some parameterizations leading to extremely high MSE, and this poor performance gets worse with larger sample sizes. MMA 4 also generally has lower MSE than the Lasso and SAIC methods for most parameterizations and sample sizes. To explore the sensitivity of the simulation results to the design, we varied some of the assumptions. We summarize the results here for brevity: graphs of the results can be found in the supplemental appendix available on the journal website. First, we sampled the error e i from a skewed nonnormal distribution, and there was no change in the results. Second, we sampled the error from the heteroskedastic distribution e i \u223c N(0 (1 + x 2 2i )/2), and there was no change in the results. Third, we introduced correlation between the regressors. The performance of several of the shrinkage and averaging estimators greatly improved (relative to OLS), especially for small \u03b1, but otherwise the qualitative results were unchanged. Fourth, we increased the number of regressors to M = 24. Again, the performance of the shrinkage and averaging estimators greatly improved relative to OLS, but otherwise the qualitative results were unchanged. As a final important robustness check, we investigated the sensitivity of the results to the ordering of the regressors. The MMA, MMA 4 , BMA, and SAIC methods all depend on the ordering of the regressors, and the above results are constructed using the correct ordering (ordering the regressors by the magnitude of the true coefficients). To investigate the sensitivity of the MMA 4 method to this knowledge, we reverse the order of the regressors and then implement the MMA 4 method. This should be most unfavorable to nested model averaging, as the regressors are ordered from smallest to largest coeffi- cients. The results are quite interesting. In nearly all cases (except \u03b1 = 0, where reversing the order has no effect), the reversed-order MMA 4 method has nearly identical performance to the Stein estimator. Thus its MSE is better than OLS, but not as good as the MMA 4 method with the correct order. This confirms the asymptotic prediction that regardless of the ordering, MMA 4 will have lower MSE than OLS. The reason MMA 4 has nearly identical MSE with the Stein estimator is because in this context the MMA 4 criterion typically puts all weight on only two models-the intercept-only model and the full model. The other models are irrelevant and thus receive zero weight. What this shows is that the cost of misordering regressors is not too severe, and the MMA 4 estimator has improved efficient relative to OLS. Overall, the simulation results confirm the predictions from the asymptotic theory. Averaging can greatly reduce estimation error relative to unconstrained estimation if the averaging weights are selected by minimizing a penalized least-squares criterion and the regressors are grouped in sets of four or larger."}, {"section_title": "Time-series regression", "text": "Our second simulation experiment explores the performance of the method in a simple time-series autoregression. The model is an AR(M), with \u03b2 0 = 0, e t i.i.d. N(0 \u03c3 2 ), \u03c3 2 = 1, and the coefficients set to be monotonically decreasing as The parameter \u03b8 controls the magnitude of the coefficients and is varied from 0 to 0 95 in steps of 0 05. As in the previous experiment, we set M = 12 and n = {50 150 400 1000}. We apply the same set of estimators and again calculate the MSE of the coefficient estimates normalized by the MSE of the OLS estimator. The results are displayed in Figure 5. The findings are quite similar to the crosssection results, with the exception that the MMA 4 method does not uniformly dominate OLS, as for small samples and large \u03b8 (high persistence) the ordering is reversed. We suspect that this is a consequence of the fact that as \u03b8 approaches 1, the autoregression approaches nonstationarity and the asymptotic approximations become unreliable."}, {"section_title": "Testing for racial differences in the mental ability of young children", "text": "To illustrate the method, we apply the grouped MMA estimator to the regression analysis of Fryer and Levitt (2013). Their goal was to assess whether there are measurable differences across races in the mental ability of very young children, especially after controlling for birth, demographic, and socioeconomic factors. The answer to this question helps shed light on the extent to which ability is genetic versus environmental. Fryer and Levitt use two data sets, but primarily focus on the Early Childhood Longitudinal Study Birth Cohort (ECLS-B), which is a sample of over 10,000 children born in 2001 and includes two waves of mental functioning tests, the first when most of the children were between 8 and 12 months of age, and the second when the children were close to 2 years old. For each test wave, Fryer and Levitt estimated six nested regressions of children's test scores on racial categories plus varying sets of controls. These control groups are as follows:   Model 5. Adds home environment variables (number of siblings as dummy categories, family configuration, region, mother's age as fifth order polynomial, parent-asteacher measure as fifth order polynomial). Model 6. Adds prenatal variables (child's birth wight in four categories, number of days premature in twelve categories, singleton birth, twin birth).  Note: Data are from Fryer and Levitt (2013). The dependent variable is normalized to have a mean of 0 and a standard deviation of 1. Non-Hispanic whites are in the omitted race category. Estimation is by weighted least squares. The number of observations is 8871. As in Fryer and Levitt, observations with missing test scores, race, interviewer identification, or sampling weight are excluded, and for other covariates an indicator variable for missing values is included. Table 1 lists the OLS estimates of the coefficients on the racial groups from these six models. The top section of the table displays the estimates for the first test wave (children approximately 9 months of age) and the bottom section displays the estimates for the second test wave (children approximately 2 years of age). Fryer and Levitt use these estimates to make the point that for infants, the mean differences across racial groups is small and diminishes after controlling for covariates, yet the differences are meaningful for toddlers. Tables of this form are commonly seen in empirical economics. Yet there is an inherent ambiguity about how to concisely summarize the findings from these six regression estimates. Which is the best estimate? What number is the best summary? The richer models have more controls (so less omitted variable bias) yet have higher variances due to larger number of estimated parameters. The conventional estimates do not give one concise summary estimate. The method of model averaging (MMA) gives a single estimate, averaging across the six regression estimates, and our theory shows this estimator has reduced risk (meansquared error) relative to ordinary least-squares estimation of the full model. Thus MMA is a concrete way to concisely summarize the point estimates. We computed the MMA estimates for the Fryer-Levitt estimates and include these estimates in the seventh column of the table. We also report the MMA weights w m at the bottom of each column. The MMA estimates are the weighted average of the individual model estimates using these weights, and these weights are selected to minimize the Mallows averaging criterion. We can see that the MMA criterion puts a weight of 97% on the full estimates for the first wave and a weight of 91% on the full model for the second wave, so that for both regressions, the MMA estimates are close to the full regression estimates. The fact that MMA puts most of the weight on the full model is largely a consequence of the way that Fryer and Levitt ordered their regressors. Some of the most important regressors (birth weight and days premature) are only included in the full model, and thus it is not surprising that MMA wants to give a high weight to this model. To rectify this situation, we redefined and reordered the regressors. We treated age, birth weight, number of days premature, and the number of siblings as continuous variables (rather than grouping them into categories) and included powers up to order 5 (as done for the parent-as-teacher measure and mother's age), but only included the higher powers in the largest models. We estimated eight nested models. The first two are the same as Fryer and Levitt; the remaining six are as follows:  Table 2 reports the OLS estimates from these eight nested model, plus the MMA estimates in the final column and the MMA weights w m at the bottom. What is noticeable is that the MMA method distributes weights differently across the models. For the first wave data set (children aged 9 months), 53% of the weight is put on the full model, 21% on the fifth model (the one with quadratic terms), 14% on the seventh model, and 11% on the third model. For the second wave data set (2-year-old children), MMA puts no weight on the full model, 51% on the sixth model, and the remainder is split between models 1, 4, 5, and 7. The MMA estimate of the controlled 9-month test score difference between non-Hispanic whites and blacks is 0 02 standard deviation units (blacks outscore whites, but by an infinitesimal amount). For 2-year-olds, the MMA estimate is that, on average, whites outscore blacks by 0 22 standard deviation units. Our theory suggests that these are the best estimates of the key parameters of interest."}, {"section_title": "Conclusion", "text": "This paper has extended our understanding of model selection and combination. We examine averaging weights selected by minimizing a penalized criteria and find that such averaging estimators have reduced risk relative to unconstrained estimation if the regressors are grouped in sets of four or larger so that the Stein shrinkage effect holds. The simulation shows that the gains are substantial and hold in finite samples. While the theory of this paper has focused on the context of least-squares regression, we believe that the concepts can be extended to other contexts including panel data and the generalized method of moments. The theory also is confined to the context of nested models. While it would be greatly desirable to extend the analysis to include nonnested models, it is not clear how this could be accomplished. Another unexplored issue is inference. The asymptotic distributions of selection and averaging estimators are nonstandard (at least in the local asymptotic framework used here). This is routinely ignored in applications involving postselection estimators, but is difficult to avoid when using averaging estimators. This is a challenging topic and quite important for future investigation."}, {"section_title": "Appendix", "text": "Proof of Lemma 1. Note that L j \u2265 L j+1 and e j e m = L max(j m) by the properties of the least-squares residuals. The penalized criterion is then The first term in (38) can be rewritten as The second term in (38) is Summing 39and 40, we find C * n (w * ) + L M + 2 T M with C * n (w * ) as defined in (16). Proof of Theorem 1. Since x 1i is included in all models, all the centered submodel estimates \u03b2 m \u2212 \u03b2 are invariant to \u03b2 1 , the coefficient on x 1i , and thus so are both the least-squares and averaging estimators. Hence, without loss of generality, we set \u03b2 1 = 0. Combined with Assumption 2, this yields n 1/2 \u03b2 \u2192 \u03b4 as n \u2192 \u221e. The organization of the argument is as follows. We first derive the joint asymptotic distribution of the least-squares estimate \u03b2 LS , the differenced submodel estimates \u03b2 m+1 \u2212 \u03b2 m , and the differenced sum of squared errors L m \u2212 L m+1 . We then derive the asymptotic distribution of the cumulative criterion C * n (w * ), its minimizer w * , and the averaging estimator. After that, we characterize the minimization problem (24). Assumption 1 is sufficient to imply that and with \u03a9 defined in (20). Combining (41), (42), the assumption Q > 0 and the continuous mapping theorem, \u2261 Z which is (19). The condition n 1/2 \u03b2 \u2192 \u03b4 allows us also to deduce that Since Since e m+1 e m = e m+1 e m+1 and e m \u2212 e m+1 = X( \u03b2 m+1 \u2212 \u03b2 m ), we calculate that L m \u2212 L m+1 = e m e m \u2212 e m+1 e m+1 = ( e m \u2212 e m+1 ) ( e m \u2212 e m+1 ) Applying (41) and (45) this converges in distribution to the equality by Lemma 3. Using equations (16) and (46), and using Assumption 1, we find that Since 17is a convex minimization problem (C * n (w * ) is quadratic and H * is convex), we can apply the argument of Kim and Pollard (1990) and deduce that w * d \u2212\u2192 w * (Z + \u03b4), where w * (x) is defined in (24). Combined with (45), it follows that which is (21). We now consider the minimization problem (24). Note that this is a deterministic problem and the solution is a function of the argument x. We now fix x and, to simplify notation, we omit dependence (of the weights and selected models) on x. Since C * (w * x) is quadratic in w * , the unconstrained minimum is simply If w * \u2208 H * , then w * = w * , which satisfies (26)-(27) with J = M. If w * / \u2208 H * , then w * lies on the boundary of H * . The latter is the union of sets of binding constraints, each constraint corresponding to excluding a specific model m. Equivalently, each section of the boundary of H * consists of the set of models {m 1 m J } with positive weight. If a model m is not in this set, it receives a weight of zero and w * m = w * m\u22121 . Thus if {m 1 m J } are the models with positive weight, then for j = 1 M \u2212 1, Thus (25) can be written as which is minimized by This establishes (26)-(27). It remains to show (28). Assume that m J < M, which means that w * m J = 1. Consider minimizing (25), allowing the models {m 1 m J M} to have positive weight (and setting the remaining models to have zero weight). We know (by assumption) that the solution puts positive weight on the models {m 1 m J } and zero weight on model M. The constrained optimization problem can be written as where \u03bb j \u2265 0 are Kuhn-Tucker multipliers enforcing the constraints w * 1 \u2265 0, w * m j \u2265 w * m j \u22121 , and w * m J \u2264 1, respectively. The first-order condition for w * m J can be solved to find Since m J is a model with positive weight, then w * J > w * J\u22121 and \u03bb J = 0 (as the constraint is not binding). Also, since model M receives zero weight, then w * m J = 1. Thus which implies (28) as desired. The proof of Theorem 2 will require the application of a famous result known as Stein's lemma. We use a version from Hansen (2013, Lemma 1). (See Stein (1981).) Lemma 4. If Z \u223c N(0 V) \u2208 R K , V > 0, and \u03b7(x) : R K \u2192 R K is absolutely continuous, then E \u03b7(Z + \u03b4) QZ = E tr \u2202 \u2202x \u03b7(Z + \u03b4) QV Stein's lemma allows a simple calculation of the asymptotic risk for estimators with asymptotic distributions that take the form Z \u2212 \u03b7(Z + \u03b4). Proof of Theorem 2. From (21) in Theorem 1, the averaging estimator has the asymptotic distribution As discussed in the text, E(Z QZ) = tr(Q \u22121 \u03a9), and by Lemma 4 and QV = \u03a9Q \u22121 , E \u03b7(Z + \u03b4) QZ = E tr \u2202 \u2202x \u03b7(Z + \u03b4)\u03a9Q \u22121 Hence (31) holds with We now show that (49) equals (32). From (22) and (26)-(27), we see that (T m j+1 \u2212 T m j ) x Q(P m j+1 \u2212 P m j )Qx (P m j+1 \u2212 P m j )Qx + (P M \u2212 P m J )Qx1 (m J <M) where, for simplicity, we do not write J and m j explicitly as functions of x. Using Lemma 3, we calculate that This is (32). Proof of Theorem 3. The inequality b \u03a9b \u2264 b b\u03bb max (\u03a9) with b = Q 1/2 c implies c \u03a9c = c Q 1/2 Q \u22121/2 \u03a9Q \u22121/2 Q 1/2 c \u2264 c Qc\u03bb max Q \u22121/2 \u03a9Q \u22121/2 = \u03c3 2 c Qc\u03bb where \u03bb is defined in (33). Setting c = (P m j+1 \u2212 P m j )Qx, we find x Q(P m j+1 \u2212 P m j )\u03a9(P m j+1 \u2212 P m j )Qx \u2264 \u03c3 2 x Q(P m j+1 \u2212 P m j )Q(P m j+1 \u2212 P m j )Qx\u03bb = \u03c3 2 x Q(P m j+1 \u2212 P m j )Qx\u03bb where the final equality uses Lemma 3. Equation 28shows that Together, x Q(P m j+1 \u2212 P m j )Qx (T m j+1 \u2212 T m j ) x Q(P m j+1 \u2212 P m j )Qx The final inequality holds since Assumption 3 implies 2d +1 \u2212 t +1 > 4\u03bb for all . For any x such that m J (x) < M, we have the strict inequality Thus q(x) \u2265 0 for all x and q(x) > 0 for some x. Since Z has a continuous distribution, we deduce that E(q(Z + \u03b4)) > 0. Thus and 35 We now show (50) and (51). Using the property of the maximum eigenvalue and the bound E(e 2 i |x i ) = \u03c3 This establishes (50). Similarly, This establishes (51)."}]