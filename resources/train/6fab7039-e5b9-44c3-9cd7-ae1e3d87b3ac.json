[{"section_title": "Abstract", "text": "Purpose: MR image reconstruction exploits regularization to compensate for missing k-space data. In this work, we propose to learn the probability distribution of MR image patches with neural networks and use this distribution as prior information constraining images during reconstruction, effectively employing it as regularization.\nMethods: We use variational autoencoders (VAE) to learn the distribution of MR image patches, which models the high-dimensional distribution by a latent parameter model of lower dimensions in a non-linear fashion. The proposed algorithm uses the learned prior in a Maximum-A-Posteriori estimation formulation. We evaluate the proposed reconstruction method with T1 weighted images and also apply our method on images with white matter lesions.\nResults: Visual evaluation of the samples showed that the VAE algorithm can approximate the distribution of MR patches well. The proposed reconstruction algorithm using the VAE prior produced high quality reconstructions. The algorithm achieved normalized RMSE, CNR and CN values of 2.77%, 0.43, 0.11; 4.29%, 0.43, 0.11, 6.36%, 0.47, 0.11 and 10.00%, 0.42, 0.10 for undersampling ratios of 2, 3, 4 and 5, respectively, where it outperformed most of the alternative methods. In the experiments on images with white matter lesions, the method faithfully reconstructed the lesions.\nConclusion: We introduced a novel method for MR reconstruction, which takes a new perspective on regularization by using priors learned by neural networks. Results suggest the method compares favorably against the other evaluated methods and can reconstruct lesions as well."}, {"section_title": "Introduction", "text": "Acquisition time in magnetic resonance (MR) imaging is directly related to the number of samples acquired in k-space. For high quality images, a large number of samples, and therefore long acquisition times are necessary. Reducing acquisition time in a reliable manner is an important question in MR imaging and many methods exploiting different properties of the k-space [1] and hardware design [2] have found wide use in clinical practice.\nOn the other hand, a substantial amount of effort went into the investigation of reconstruction methods from randomly or regularly undersampled k-space acquisitions. The random undersampling * Two of the images used in preparation of this article were obtained from the Alzheimers Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report.\nA complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wpcontent/uploads/how to apply/ADNI Acknowledgement List.pdf approach was primarily motivated by the compressed sensing framework where the incoherence between k-space sampling and some sparsifying transform was exploited to achieve theoretically exact reconstruction [3, 4, 5, 6, 7] . Similarly, regular undersampling schemes and corresponding reconstruction algorithms were extensively investigated [8, 9, 10, 11] . The common aspect of these two approaches is that they both are interested in inverting an underdetermined system of equations by finding good regularizers to overcome the ill-posedness. A key observation is that the use of regularizers in both approaches can essentially be viewed as introduction of prior information to the reconstruction problem.\nRecently, researchers started to employ deep neural networks (DNN) [12] for MR reconstruction inspired by its success in computer vision and medical image analysis [13] . In the related literature there are two main approaches for applying DNNs to the reconstruction problem. The first is to teach the networks a mapping going from the undersampled images to their fully sampled versions [14] . After training, an undersampled image is fed into the network to obtain its reconstruction at the output. This approach does not have any explicit data consistency term and cannot guarantee this property at test time. To overcome this limitation, in [15] the authors introduce a deep cascaded network. Here a block of convolutional neural network (CNN) is followed by a data consistency term and this structure is repeated multiple times. This guarantees consistency with measured data also at test time while the CNNs perform de-aliasing.\nThe second approach is to use the efficient trainability of the networks to improve existing algorithms. In [16] the authors show that the iterations of the alternating direction method of multipliers (ADMM) algorithm solving a dictionary based -1 regularized reconstruction problem can be expressed as a multi-layer network. Kernels and non-linear functions are parameterized and learned by the network which improve on the fixed version in the original method. Both deep learning approaches suffer from specificity of training. Learned mappings, kernels and non-linearities are specific to a certain undersampling pattern and possibly field-of-view (FOV). Hence, the networks need to be retrained for each different configuration of FOV, undersampling factor and pattern, which limits wide use of these methods.\nIn computer vision, DNNs have also been used for approximating high dimensional probability distributions from samples through unsupervised learning. Such approximations allow to estimate the likelihood of a previously unseen data point. One such approach is the variational auto encoder (VAE) algorithm [17, 18] . Using VAEs, it is possible to approximate the distribution of patches of MR images and compute likelihood of a previously unseen image. Furthermore, the likelihood function is differentiable since it is modelled as a neural network.\nIn this work we develop a probabilistic reconstruction model that uses priors learned via VAEs, and deploys them as regularizers, which we term deep density prior (DDP) based reconstruction. The approach can be used to learn the prior distribution using fully sampled examples once and reconstruct for different sampling parameters without retraining. To achieve this we first formulate a Bayesian model of the imaging process and express the reconstruction problem as a Maximum-A-Posteriori (MAP) estimation problem. We show how some of the previous methods fit into this formulation and then present the proposed model using a VAE-learned prior. In the experiments, we first demonstrate that VAEs can approximate distribution of patches of MR images by showing examples of patches sampled from the prior. We then show reconstruction results from the proposed model and compare with both more conventional approaches as well as recent DNN based methods."}, {"section_title": "Methods", "text": "In the first two parts of this section, we provide a brief background on Bayesian formulation of the MR reconstruction problem and the VAE algorithm. Then, starting from Section 2.3, we present our main technical contribution, that is learning a prior for MR image patches and integrating it in the reconstruction problem."}, {"section_title": "Bayesian formulation of the MR reconstruction problem", "text": "The MR image is denoted as m \u2208 C N , where N is number of voxels 1 . The imaging operation is given by the undersampling encoding operation E = U F S, where S : C N \u2192 C N \u00d7\u03b3 is the sensitivity encoding operator. Here, \u03b3 is number of coils, F : C N \u00d7\u03b3 \u2192 C N \u00d7\u03b3 is the Fourier operator and U : C N \u00d7\u03b3 \u2192 C M \u00d7\u03b3 is the undersampling operator, with M < N .\nAssuming complex valued zero mean normal distributed noise, denoted as \u03b7, the acquired data y \u2208 C M \u00d7\u03b3 can be written as y = Em + \u03b7. Under this noise model the data likelihood becomes\nwhere H denotes the Hermitian transpose and \u03c3 \u03b7 is the standard deviation of the noise. In reconstruction, we are interested in the posterior distribution p(m|y), i.e. the probability of the image being m given the k-space measurements, which can be written using the Bayes' theorem as\nThe common approach to model the reconstruction problem, which we will also use, is to use the MAP estimation arg max\nwhere the equality is due to p(y) not depending on m. p(m) is called the prior term and represents the information one has about the fully sampled image before the data acquisition. Taking the log of both sides and plugging in the definition of the likelihood term yields arg max\nRewriting the likelihood and ignoring terms that do not vary with m yields arg max m log p(m|y) = arg max\nwhere we define \u03bb 2\u03c3 \u03b7 . Taking the maximum, or equivalently taking the minimum of the negative of the expression and multiplying all sides with \u03bb recovers the conventional formulation of a reconstruction problem with a data and a regularization term\nFrom the point of view of Equation 7, regularization-based reconstruction algorithms differ in the log prior term they use. A generic prior expression can be written as log p(m) = \u03a8(m \u2212 \u00b5) p , where \u03a8 is a sparsifying operator, \u00b5 is the expected image and \u00b7 p is the -p norm.\nFor instance, forcing the image to be sparse in some domain can be expressed as assuming the data is i.i.d. according to the Laplace distribution in the domain [19, 20] . Mathematically, this is\nwhere we set \u00b5 = 0, denote the matrix determinant with det 2 , use | \u00b7 | as the magnitude and i indexes voxels in the image. Now, taking \u2212 log of the expression and ignoring the terms that do not vary with m we get\nSetting b = 1/\u03bb yields the prior term in Equation 7 . Furthermore, setting \u03a8 as the gradient operator with first order finite differences, denoted by \u2207, corresponds to enforcing sparsity in the gradient domain recovering the basic total variation compressed sensing reconstruction\nAlternatively, both \u03a8 and \u00b5 can be estimated from data. One example for this approach is to assume a Normal prior on m, p(x) = N (\u00b5 m , \u03a3 m ) and estimate the mean \u00b5 m and the covariance \u03a3 m from a lowresolution image of the same object [10, 22] . The posterior p(y|m) is then also a Normal distribution due to conjugacy relations and the MAP estimate is given with the posterior mean [23] \nA similar idea has been applied in k-t SENSE and k-t PCA methods in the dynamic imaging setting to exploit spatio-temporal correlations [22, 10] . In this work, we follow the same approach of estimating the prior term from example images and propose to approximate \u2212 log p(m) with a neural network model. We train a VAE on patches of fully sampled MR images to capture the distribution of patches and use this prior for reconstruction."}, {"section_title": "Learning the data distribution with VAEs", "text": "VAE is an unsupervised learning algorithm used to approximate high-dimensional data distributions [17, 18] . We introduce VAEs very briefly and refer the reader to [17] for further discussion.\nThe main goal of the VAE algorithm is to approximate the data distribution using a latent model given as\nwhere z \u2208 R L denote the latent variables, x \u2208 R P the data and p(z) the prior over the z's. Direct computation of p(x), log(p(x)) or p(z|x) requires integrating over z, which is not feasible even with relatively small L. Variational approximation uses an approximate distribution for the posterior q(z|x) \u2248 p(z|x) to address this problem. Using the approximation, log p(x) can be decomposed into two terms [24] log p(\nThe first term is referred to as the evidence lower bound (ELBO) and the second term is the KullbackLeibler divergence (KLD) between the approximate and true posteriors. VAE parameterizes the data likelihood and the approximate posterior using networks, i.e. q(z|x) = q \u03b8 (z|x) and p(x|z) = p \u03c6 (x|z), and during training maximizes ELBO for a given set of data samples with respect to the parameters of the networks \u03b8 and \u03c6. Maximizing ELBO will minimize the KLD and maximize log p(x). The optimization during training is written as\nwhere x n is the n th training sample. The networks q \u03b8 (z|x) and p \u03c6 (x|z) are called the encoder and the decoder, respectively. The former takes the data sample x and encodes it into a posterior distribution in the latent space with network parameters \u03b8. If the posterior distribution is modelled as a Gaussian, then the encoder outputs a mean and a covariance matrix for z depending on x. The decoder network on the other hand, takes a latent vector z and maps it to a conditional distribution of the data given z.\nIn this work, we use the vanilla VAE design [17] except for the data likelihood, where we use a multi-modal Gaussian p \u03c6 (x|z) = N (x|\u00b5 \u03c6 (z), \u03a3 \u03c6 (z)), similar to [25] .\nIn our model, we use the VAE model to approximate the prior distribution of magnitude of 2D MR patches of fully sampled images. In other words, we approximate p(|x|), where x is a patch extracted from a complex valued MR image of size \u221a P x \u221a P . We provide the exact parameters of the network design in Section 2.5."}, {"section_title": "Deep density prior reconstruction model", "text": "Once the network is trained and optimal values \u03b8 * and \u03c6 * are found, we can integrate the prior within a Bayesian formulation of the reconstruction problem. We make two key observations to achieve this. First, an approximate log likelihood of an image patch x can be obtained by evaluating ELBO(|x|) with the optimal parameters.\nwhere we use the magnitude of the patch |x| within the ELBO. This allows us to formulate the proposed model as the following MAP estimation problem arg min\nwhere \u2126(m) denotes a set of (overlapping) patches covering the image m and |x r | is the magnitude of the r th image patch. Note that this approach assumes independence between different patches and therefore, ignores statistical dependencies between them. However, it also makes the computations much easier.\nSince an exact computation of the ELBO term requires evaluating the expectation with respect to q(z||x|), which is computationally not feasible, we use a Monte Carlo sampling approach to calculate the ELBO as follows\n, with z j \u223c q(z||x|).\nHere J represents the number of Monte-Carlo samples.\nOur second key observation is that the approximation in Equation 15 is differentiable since each term is defined through networks that are themselves differentiable. This is the critical aspect that allows integrating the trained VAE as a prior into an optimization based reconstruction algorithm.\nPlugging the ELBO approximation into Equation 14 , we obtain the formulation of the DDP reconstruction problem arg min\nwhere the first term is the usual data term and the second term within the summation is the regularization term that arises from the learned prior. We can compute the total derivative of the prior term with respect to each image patch as follows\nwhere we defined R(x, z j ) for notational simplicity. The second term in the last line is due to the dependency of the samples z j to x. The term x/|x| is due to taking the derivative of the magnitude with respect to the image patch. for t = 0 : T \u2212 1 do main loop: POCS iterations 7:\nfor k = 0 : K \u2212 1 do inner loop: iterations for the prior projection P prior 9:\ncreates a set of patches covering the image 10: for r = 1 : no of patches do loop over all the patches in {x t r } 11: g \u2190 patches2image({g r }) P prior m t 14:\nend for 16:\nend for 19: return m T Resulting reconstruction 20: end procedure"}, {"section_title": "Optimization using projection onto convex sets", "text": "We solve the DDP optimization problem given in Equation 16 using the projection onto convex sets (POCS) algorithm [26] , specifically using the formulation in [27] . POCS is an iterative minimization process, where the solution variable is projected sequentially onto different convex sets, each defined by one of the constraints in the problem.\nThe projection for the data consistency term is given simply as P DC m = m \u2212 E H (Em \u2212 y) [26] . Since we do not have a projection operator for the prior term, we approximate it by several gradient ascent steps with a small step size. We use the final image at the end of the ascent steps as the projected image patch. We define the operation as follows: i) we create a set of patches {x t r } = \u2126(m t ) from the image m t at iteration t, ii) obtain the derivatives for each of these patches using Equation 19 , which have the same size as the patches themselves, iii) create a derivative image from these derivative patches by averaging the values where the patches overlap, iv) update the image using the derivative image, v) repeat this K times. To reduce edge effects resulting from patchwise projections, we use four sets of overlapping patches.\nAs the likelihood term is only trained on magnitude patches and cannot convey any information regarding the phase, we also use a phase projection following the prior projection, where we set the phase of the image to zero, given as P abs m = |m|. Notice we do this because we expect the images to have zero phase in this specific application. It is however possible to change this to any other constraint on the phase of the image, such as a low resolution estimation of the phase [3] or zerodivergence constraint for phase contrast flow imaging reconstruction [28] .\nWith the data consistency, prior and phase projections defined as above, one step of reconstruction within the POCS framework becomes\nWe apply T POCS steps to complete the reconstruction. Algorithm 1 provides a summary of the reconstruction procedure."}, {"section_title": "Details on VAE network architecture and training", "text": "We use batches of 50 patches of size 28x28 and a 60 dimensional latent space. Both networks are depicted in Figure 1 . For the encoding network, the input is an image patch and the output is the mean and the covariance of the corresponding posterior. The decoding network takes a vector z as input and outputs the mean and the covariance of the corresponding likelihood for the patch. Both networks are mostly convolutional with single fully connected layers. All convolutional layers for both networks use 3x3 kernels and all layers also have additive bias terms throughout the network. Due to stability issues we use the log of the variance values throughout the network. We initialize the network weights with a truncated normal initializer with standard deviation 0.05. We use Adam [29] for optimization (learning rate of 5e-4, default momentum values in Tensorflow)."}, {"section_title": "Experimental setup 3.1 MR image data", "text": "We evaluated our method using structural images from the Human Connectome Project (HCP) 3 data set [30] . High quality and large number of HCP images are ideal for learning priors with the VAE model.\nWe took 2D slices from the T1 weighted 3D MPRAGE images acquired with 2400 ms, 2.14 ms and 1000 ms for TR, TE and TI, respectively, flip angle of 8 degrees, a standard field of view for all subjects of 224x224x224 mm 3 with 0.7 mm isotropic resolution using a bandwidth of 210 Hz/voxel. Images were Figure 1 : Architecture of the encoding (top) and decoding (bottom) networks of our VAE. Arrows indicated with C# are convolutional layers followed by ReLU non-linear activation except C4 of decoding network, which is not followed by non-linear activation. Arrows indicated by FC are fully connected layers. FC of the decoding network is followed by a ReLU activation but not of the encoding network.\nacquired with a 3T Siemens device. Fat suppression was used during acquisition. In our experiments, we used the minimally preprocessed images in order to also have the corresponding FreeSurfer [31] segmentations, which we use in our evaluation. Preprocessing steps were resampling to 1mm isotropic resolution with FOV matrix 260x311x260 and rigid alignment. Despite the rigid alignment, there were substantial local orientation differences between the images from different subjects. We used five central slices with skipping four slices between each. We cropped the images to a size of 252x308, removing only background, to reduce computational load. We normalized the images by mapping their 99 th percentile to 1.\nWe used images from 158 subjects (790 images in total) to train the prior VAE model. The VAE model trained for 200k iterations. As test data, we took central slices from images of 17 separate subjects.\nTo test if the proposed reconstruction method can be used on a domain that is different from the one the prior is trained on, we experimented with two slices from the Alzheimer's Disease Neuroimaging Initiative (ADNI) data set 4 . The images were selected from subjects with Alzheimer's disease and who 4 Two of the images used in the preparation of this article were obtained from the Alzheimers Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). The ADNI was launched in 2003 as a public-private partnership, led by Principal Investigator Michael W. Weiner, MD. The primary goal of ADNI has been to test whether serial magnetic resonance imaging (MRI), positron emission tomography (PET), other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment (MCI) and early Alzheimers have visible white matter lesions to also test whether the proposed method will be able to faithfully reconstruct lesions. The acquisition parameters of the ADNI images were different than HCP: TR, TE, TI values were 7.34 ms, 3.03 ms, 400 ms, flip angle was 11 degrees, FOV matrix was 196x256x256 with resolution 1.2 mm x 1 mm x 1 mm. The images were acquired with a 3T GE scanner with no fat suppression and were bias-field corrected with the N3 algorithm [32] . We extracted the central slices that showed the largest lesions from these images and further cropped the FOV to 168x224 to get rid of the empty regions in the images to accelerate computations. We normalized the image intensities the same way as the HCP data."}, {"section_title": "Setup and evaluation", "text": "In our assessment, we artificially undersampled the test images in k-space, reconstructed them back and compared the results with the original images. We experimented with varying undersampling ratios, which we denote with R when presenting results.\nWe used Cartesian undersampling in one dimension while fully sampling in the other dimension, corresponding to phase-encoding and readout directions, respectively. We present some examples of undersampling patterns in Figures 3 and 4 . We generated the patterns by randomly sampling a 1D Gaussian distribution along the phase-encoding dimension. We randomly drew many sampling patterns from the Gaussian distribution and selected the ones with the best peak-to-side ratio. In addition, we added the central 15 profiles to these selected patterns to fully sample the low-frequency components. We used 2, 3, 4 and 5 for net undersampling ratios (including the fully sampled center). We assumed a single coil imaging with uniform sensitivity throughout the experiments for proof of concept, as the proposed method works independent of the number of coils.\nWe tried different configurations of latent space size and patch size to see how sensitive the method is to changes in these parameters. We used patch sizes of 12x12, 20x20, 28x28, and 36x36 and latent space dimensions of 40, 60 and 80. For each configuration we retrained the prior model. We also analyzed the proposed method's robustness to noise. To this end, we added synthetic noise to the fully sampled images before undersampling. The original images had a SNR A between 50 to 70, depending on the subject. We added 4 different levels of noise with SNR values 40, 30, 20 and 10. Notice the actual noise on the images is higher (SNR nearly half) due to the base noise on the original images. We studied the different configurations and robustness to noise using 5 of the test images.\nWhile assessing the DDP method, we generated a new undersampling pattern for each test to make sure the reconstruction results covered the variability of the undersampling patterns. We reconstructed the test images using 30 POCS iterations (T=30) for R=2 and 3 and 60 for R=4 and 5, 10 iterations for the prior projection (K=10) and the step size was set to \u03b1=1e-4.\nWe compared the reconstruction with the fully sampled \"ground truth\" images using Root-MeanSquared-Error (RMSE), Contrast-to-Noise-Ratio (CNR) and Contrast difference (CN) computed at the gray and white matter boundary in the brain. In order to obtain white matter boundaries we applied binary erosion (with a square structuring element of size 7x7) to the white matter segmentations, as computed by FreeSurfer, and took the difference of the original and eroded segmentations."}, {"section_title": "Compared methods", "text": "As a basic benchmark, we evaluated zero-filling reconstructions. As the first baseline, we used total variation (TV) reconstruction as described in [3] . We used the BART toolbox implementation that is publicly available [33] . We used the \"pics\" tool with TV regularization (regularization strength 0.075) and the ADMM parameter \u03c1 as 1. We used 20 conjugate gradient steps and 4500 total iterations (\"bart pics -R T:3:0:0.0075 -u1 -C20 -i4500\"). We ran a grid search for the best parameter setting in the RMSE sense, which we chose as above."}, {"section_title": "disease (AD). For up-to-date information, see www.adni-info.org", "text": "As the second method, we used reconstruction using dictionary learning (DL) [34] 5 . We used 200 iterations, a patch size of 36 voxels and 36 dictionary atoms. Furthermore, we set number of signals used for training to 7200 and the overlap stride to 2. We used K-SVD learning with both sparsity and error threshold. The sparsity level was set to 7. The error threshold was set to 0.046 for the first four iterations, then to 0.0322 for the rest of the iterations. We used 15 K-SVD iterations. We choe the parameters as sugested by the authors in code, but increased the number of iterations.\nWe employed the recently proposed ADMM-Net [16] 6 . We modified the code very slightly to work with non-square images and Cartesian undersampling patterns. To train the model, we used the same 790 images, which we used to train the VAE model. To train and test the ADMM-Net, we fixed the undersampling pattern for all undersampling ratios, as required by the method. We used 15 stages with 8 filters (filter size 3x3), padding as 1. We did not use weight decay during training. We set the maximum iteration number to 25 for the L-BFGS algorithm. It trained to maximum iterations for R=2 (45 hours), 13 iterations before convergence for R=3 (42 hours), 18 iterations before running out of time (120 hours) for R=4 and maximum iterations for R=5 on a GPU (GeForce GTX TITAN X). The normalized mean squared errors were 0.078 and 0.035 for R=2, 0.11 and 0.071 for R=3, 0.15 and 0.11 for R=4, and 0.19 and 0.13 for R=5, before and after training, respectively. We took the parameter setting for which the best results were reported in the paper.\nWe also compared with the methods shift-invariant discrete wavelet reconstruction (SIDWT) [35] , fast multiclass dictionary learning (FDLCP) [36] and patch based directional wavelets (PBDW) [37] . For these methods we cropped the images to 256x256 and generated new undersampling patterns for this size since the authors implementations worked only with that size 7 . We did not modify the code for these methods and took the parameters them as set by the authors in the code. For comparison, we ran our proposed method on these images as well.\nFinally we compared to an algorithm based on BM3D denoising, namely BM3D-MRI [38] 8 and used the parameters as set by the author in the code."}, {"section_title": "Results", "text": "We start by showing patches sampled from the prior model trained for patch-size of 28x28 and latent dimension of 60 in Figure 2 . These patches were generated by simply feeding 16 random z vectors drawn from unit Gaussian to the decoder network. The decoder network outputs the mean images, i.e. \u00b5 \u03c6 (z), and the corresponding variance images, i.e. \u03a3 \u03c6 (z). We would like to note that we have not cherry-picked these images. The sampled mean patches, shown on the left, look realistic where gray matter, white matter and gyri/sulci structures are clearly defined and properly positioned. These generated samples suggest that the VAE algorithm is able to approximate the underlying distribution of MR patches. Variance images images on the right show that VAE places high variance in the CSF areas as well as boundaries between structures. We observe that isolated gray matter or CSF islands receive high variance.\nNext, we show visual reconstruction results. Figure 3 shows results for one of the test images from the HCP data for R=2. The sampling pattern is also shown in the figure. All the methods achieve good reconstruction quality in terms of reproducing structural and texture information, except the TV images which are cartoon-like. The error images show higher error in regions where gray matter and white matter structures are intertwined.\nAliasing artifacts become more prominent at R=3 as seen in Figure 4 . SIDWT cannot complete dealiasing, TV and PBDW are have structural and texture problems. ADMM-Net cannot reconstruct We show four more randomly selected images from the test set in supplementary Figures S4 and S5 . We also show results for higher undersampling ratios in supplementary Figures S6 and S7 .\nWe present the quantitative results for reconstruction accuracy in Table 1 . In the first part, i.e. for the full FOV reconstructions BM3D-MRI performs the best for R=2 and 3, followed by the DDP. After R=4 the DDP method works best. For the cropped FOV part, The FDLCP reconstruction outperforms all the other methods in all cases in terms of RMSE with the proposed method following as the 2nd. In terms of CNR, the proposed method performs better than all the others.\nA plot showing convergence of the POCS algorithm can be seen in Figure S1 . Results for patch size and latent dimension can be seen in supplementary Figure S2 .\nLastly, we show DDP reconstructions in Figure 5 for the ADNI images for R=2. We used the VAE model was trained on HCP images, which is composed of healthy subjects, to reconstruct the images here. The reconstructed images recover gray matter-white matter structures and edges faithfully. The white matter lesions are also well reconstructed. The error maps do not indicate a specific increase of error in the lesion regions."}, {"section_title": "Discussion", "text": "MR patches sampled from the learned distribution shown in Figure 2 support our hypothesis that the VAE model can learn to approximate the distribution of MR patches. The variance maps show that the sulci-like generated structures filled with CSF have higher variance, which is in accordance with the literature [39] .\nThe reconstruction examples in Figure 3 and Figure 4 show that our proposed VAE based deep density prior reconstruction method produces promising results. This suggests that the learned distribution of MR patches can be used as prior for reconstruction. Reconstructions with R=3 shows limitations of the methods. In the full FOV case, the proposed method performs better than most other methods in terms of quality metrics except for the BM3D-MRI and better or equally in visual quality assessment. The BM3D based method performs worse than the DDP in higher R's since denoising becomes more difficult.\nFor the cropped FOV FDLCP performs consistently better, which is a highly optimized method. The results we report here are for proof of concept and are open to improvement. Compared to most other methods learning the data distribution and using this as a prior works better than enforcing a specific distribution in a specific domain, such as the Laplace distribution in the gradient domain as in TV or more generalized distributions in a kernel domain as in DL. These methods are inherently limited by their formulations compared to the more general formulation of the DDP approach presented here. Hence the DDP approach can be expected to improve over these methods further.\nTwo of the limitations of our method are the variational assumption and the unit Gaussian prior for the latent space. A lot of work went into investigating different latent space prior structures such as Gaussian mixtures or graphical models which could in principal improve the encoding and representative capacity of the latent variables to address the latter limitation [40, 41] . Similarly a lot of research is going into different density estimation methods, for instance using generative adversarial networks [42] . These have the potential to improve on the approximation of the true likelihood and hence to improve reconstruction quality as well.\nAnother limitation of the proposed method is the independence assumption over patches. This assumption can be removed by taking into account the statistical dependence between different patches in an image. However, this is likely to increase the computational complexity of the reconstruction method.\nExperiments with different configurations show that the DDP reconstruction is not sensitive to patch size and latent space dimensions for a reasonable range. Furthermore, as expected the performance degrades with decreasing SNR.\nWe emphasize the generality of our method in contrast to most DNN based methods in the literature. Most methods require a specific mapping to be learned for every undersampling pattern and/or lack an explicit data consistency term, whereas our method does not suffer from these limitations. Our method learns the prior directly on images and can be used to reconstruct any sampling scheme faithful to the measured data without need of retraining as long as the images are from the same domain.\nThe ADNI reconstruction results are also encouraging in two terms. First, they show that the learned model does not blur out the lesion regions during the prior projection, which could be expected since the training images do not have any examples of lesions. However, for a proper treatment of images with lesions, we believe the training data set should include images with lesions.\nSecondly, the proposed method performs reasonably despite the domain difference between the training and test sets. Although the two data sets are acquired at the same field strength, they still differ in the acquisition protocol and imaging parameters. Furthermore, our method is invariant to FOV changes but not to scale changes. HCP and ADNI voxel sizes are different (1mm x 1mm vs. 1.2 mm x 1 mm), however the reconstructions still look plausible. Notice that the HCP has fat suppression which reduces the signal from the skull significantly, however ADNI images do not have the suppression, making the dealiasing more challenging. The results indicate that the learned model can generalize to different scales and imaging protocols without retraining. More challenging problems which might be attacked by domain adaptation methods such as training and testing on different imaging modalities still require further research. R -net undersampling ratio, RMSE -root mean squared error in percentage, CNR -contrast to noise ratio, CN -contrast. "}, {"section_title": "Conclusion", "text": "In this paper we proposed a novel method, termed deep density prior for MR reconstruction from undersampled k-space acquisitions. The method uses the VAE algorithm to learn the distribution of MR patches from a training set composed of fully sampled images. The model then uses this learned distribution as a probabilistic prior in a Bayesian reconstruction framework. We have shown that the VAE can approximate the distribution and generate realistic looking MR patches. Furthermore, reconstruction with the DDP approach yielded promising results for HCP and ADNI data sets in terms of visual quality and quantitative measures. "}]