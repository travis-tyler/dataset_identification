[{"section_title": "", "text": "This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460)."}, {"section_title": "NAEP-TIMSS Linking Study", "text": "The 2011 NAEP-TIMSS linking study conducted by the National Center for Education Statistics (NCES) was designed to predict Trends in International Mathematics and Science Study (TIMSS) scores for the U.S. states that participated in 2011 National Assessment of Educational Progress (NAEP) mathematics and science assessment of eighth-grade students. The study design involved four samples of students: 1. Students assessed in NAEP mathematics or science during the winter (January-March) 2011 NAEP administration (NAEP operational/national sample); 2. Students in the United States assessed in TIMSS (mathematics and science) during the spring (April-June) 2011 TIMSS administration (TIMSS U.S. operational/national sample); 3. Students assessed during the 2011 NAEP testing window with booklets, referred to as braided booklets, containing one block of NAEP and one block of TIMSS items (which followed NAEP administration procedures); and 4. Students assessed during the spring 2011 TIMSS testing window with booklets, also referred to as braided booklets, containing one block of NAEP and three blocks of TIMSS items (which followed TIMSS administration procedures). The braided-booklet sample under the NAEP administration window (i.e., sample 3) was given the NAEP-like booklets, which were designed to appear as similar as possible to a regular NAEP assessment booklet and were administered under the same conditions as NAEP. Similarly, the braided-booklet sample under the TIMSS administration window (i.e., sample 4) was given the TIMSS-like booklets. Those booklets were designed to appear as similar as possible to a regular TIMSS assessment booklet and were administered under nearly the same conditions as TIMSS. In addition, the braided booklets in the 2011 TIMSS window were administered in the same schools in which TIMSS was administered, with one intact classroom randomly assigned to the U.S. TIMSS national sample and another to the braided-booklet sample. In addition to these linking study samples, nine states-Alabama, California, Colorado, Connecticut, Indiana, Florida, Massachusetts, Minnesota, and North Carolina-participated in 2011 TIMSS directly as separate jurisdictions and, therefore, received actual TIMSS scores. These nine states provided a \"validation sample\" upon which the NAEP-TIMSS link was evaluated. The validation states were selected based on their state enrollment and willingness to participate, and also on whether they as a whole represented a substantial range of performances relative to the national NAEP average, had previous experience as benchmarking participants in TIMSS, and were geographically diverse. See Figure 1 for details on sample sizes. Based on the evaluation of the linking results, NCES has adopted the statistical moderation technique to report predicted TIMSS scores for the 43 U.S. states/jurisdictions that did not participate in the 2011 TIMSS grade 8 assessments at the state level. This decision was made because the evaluation of results showed that all three methods of linking yielded essentially the same predicted TIMSS results. In addition, among the three methods, the statistical moderation technique is the simplest method requiring the estimation of the fewest parameters (i.e., the means and standard deviations of the U.S. national public school samples for NAEP and TIMSS). The method also could be applied to the extant national samples of NAEP and TIMSS and did not require the use of separate braided-booklet samples that were required for the calibration and projection methods of linking. This implies that NCES has the option of conducting future NAEP-TIMSS linking studies using statistical moderation without the additional resources needed for the braided-booklet samples. Selecting a relatively simple and efficient methodology allows NCES to conduct additional linking studies in the future. Multiple NCES contractors were involved in carrying out the linking study. One NCES contractor, Educational Testing Service (ETS), applied the calibration and the statistical projection methods, while another, American Institutes for Research (AIR), applied the statistical moderation method. In the next section of this paper, descriptions of the methods applied in the This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). 2011 linking study are presented. A third contractor, the Human Resources Research Organization (HumRRO), evaluated the results obtained by the three linking methods and made a set of recommendations based on their evaluation. The linking results and the recommendations were discussed with various expert panels, namely, the NAEP Design and Analysis Committee and the National Assessment Governing Board. HumRRO's evaluation of the linking results and their recommendations are presented in the final section of this paper. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460)."}, {"section_title": "Linking Methodology: Calibration", "text": "In the literature, the term calibration has several different meanings and connotations. We use it here to refer to a procedure of putting all the NAEP and TIMSS items in a given domain (mathematics or science) on a common item response theory (IRT) scale. As discussed in Kolen and Brennan (2004, page 430), calibration linking is a type of linking used when the two assessments are based on 1. the same framework but different test specifications and different statistical characteristics, or 2. different frameworks and different test specifications, but the frameworks are viewed as sharing common features and/or uses. Calibration linking is typically used in a nonequivalent groups anchor test (NEAT) design in which a set of \"common items\" or common test questions is administered to all groups. For instance, student sample 1 is administered item sets A and B, while student sample 2 is administered item sets B and C. Items in set B are the common items. Although NAEP and TIMSS are based on different frameworks and have different test specifications, the two assessments do share a number of common features (Neidorf, Binkley, Gattis, & Nohara, 2006, Nohara, 2001, Provasnik et al., 2012. Therefore, calibration linking is used based on the second type of linking condition listed above. As shown in Figure 1, the 2011 NAEP-TIMSS linking study design included braided-booklet samples that took items from both NAEP and TIMSS at the same time and under the same testing conditions. Consequently, NAEP items were common among the 2011 operational NAEP sample and the two braided-booklet samples (one in the NAEP administration window, and the other in the TIMSS administration window), and TIMSS items were common among the 2011 operational TIMSS U.S. sample and the two braided-booklet samples. Figure 2 illustrates how the study design provided common items in linking NAEP and TIMSS. The study thus supports the use of calibration linking, the goal of which was to express the IRT item parameters for the 2011 NAEP items on the TIMSS scale. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460).  The objective of the NAEP-TIMSS linking study was to use states' 2011 NAEP scores to predict their mean TIMSS scores and percentages of students reaching each of the TIMSS international benchmark levels. Therefore, we wanted the predicted TIMSS scores to be placed on the existing TIMSS scale, which was established based on countries that participated in TIMSS (Foy, Brossman, & Galia, 2012). Consequently, for the calibration linking analysis, we employed the fixed parameter calibration method. That is, we first fixed the IRT item parameters for the TIMSS items at their values from the TIMSS 2011 operational analysis. Next, the items from the NAEP assessment were placed onto the established TIMSS scale by calibrating the items from the NAEP and TIMSS assessments together but keeping TIMSS item parameters fixed. Three major steps were involved in the fixed parameter calibration linking: (1) calibrating the NAEP items onto the TIMSS scale; (2) estimating population proficiency scores in TIMSS for the 2011 NAEP samples in mathematics and science; and (3) placing the predicted proficiency scores on the metrics used to report TIMSS results. In the following sections, we describe each step of the calibration linking analysis. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). Step 1: Calibrating the NAEP items onto the TIMSS scale For this first step, we used the item parameters for the eighth-grade TIMSS mathematics and science items from the TIMSS 2011 operational analysis. In the TIMSS operational analysis, the two IRT scales, one for mathematics and the other for science, were constructed separately. In linking assessments between administrations, TIMSS uses concurrent calibration, which calibrates item parameters for the items in the current assessment through a concurrent calibration of the data from the current assessment and from the previous assessment (See, for example, Foy, Brossman, & Galia, 2012, for details). In line with TIMSS operational practice, we conducted two separate fixed parameter calibrations, one for mathematics and the other for science. The item parameters of the TIMSS items were fixed at the values obtained from the TIMSS 2011 operational analysis, and the NAEP item parameters were calibrated ) onto the TIMSS IRT scale. The item responses from three groups of students-the 2011 NAEP national sample, 1 1 The 2011 NAEP national sample included students from both public and private schools. the NAEP window braided-booklet sample, and the TIMSS window braided-booklet sample-were used in the calibration, and the proficiency distributions for the three groups were not constrained to be equal. Note that the 2011 TIMSS sample was not included. This is because only the NAEP item parameters need to be estimated in the fixed parameter calibration; no NAEP items were administered to the 2011 TIMSS sample. For dichotomously scored items, two-and three-parameter logistic models (Lord & Novick, 1968) were used, while for polytomously scored items the generalized partial-credit model (Muraki, 1992) was used. Details about the IRT model fit evaluation and the estimated item parameters for all 2011 NAEP mathematics and science items from fixed parameter calibration will be provided in the forthcoming NAEP-TIMSS linking study technical report. Step 2: Estimating population proficiencies in TIMSS for the 2011 NAEP national sample In the second step, we took the IRT item parameters for the NAEP items estimated in the first step and employed a procedure called \"conditioning\" to estimate mathematics and science proficiency distributions for the 2011 NAEP national sample. 2 The item parameters estimated in step 1 served the purpose of setting the TIMSS IRT scale on which these proficiencies were estimated. Plausible values-random draws from the predictive scale score distribution for each respondent on the TIMSS IRT scale (see von Davier, Gonzalez, & Mislevy, 2009) were generated for all students in the 2011 NAEP national sample. The plausible values were used to estimate student subgroup proficiencies and associated variances. We drew 20 plausible values per respondent in the 2011 NAEP national sample. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). Step 3: Transform the predicted proficiency distributions for the 2011 NAEP national samples to the TIMSS reporting metrics The third step was to transform the proficiency distributions obtained in step 2 from the TIMSS IRT scales to the TIMSS scale score reporting metrics. A mean-sigma linear transformation procedure was applied that transformed the distribution of the 2011 NAEP national sample from the TIMSS IRT scale to match the mean and standard deviation of the proficiency distribution of the 2011 TIMSS U.S. national sample that was available on the TIMSS reporting metric. The transformation was carried out separately for mathematics and science. Student plausible values were used in computing the means and standard deviations of the score distribution. The transformation equation was as follows: \u2022 PV Calibrated was the plausible value on TIMSS IRT scale from fixed parameter calibration; \u2022 PV Target was the plausible value on the TIMSS reporting metric, obtained using linear transformation parameter estimates and \u00c2B = the estimated standard deviation of the proficiency distribution for the 2011 TIMSS U.S. national sample on the TIMSS reporting metric; \u2022 Calibrated SD = the estimated standard deviation of the proficiency distribution for the 2011 NAEP national sample on the TIMSS IRT metric; \u2022 M Target = the estimated mean of the proficiency distribution for the 2011 TIMSS U.S. national sample on the TIMSS reporting metric; and = the estimated mean of the proficiency distribution for the 2011 NAEP national sample on the TIMSS IRT metric. The estimated transformation parameters are listed in Table 1. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). Conceptually, projection is a type of statistical machinery that estimates a relationship between scores on two tests, and then derives predictions (\"projections\") of scores on one test from scores on the other test (Mislevy, 1992). Projection linking can be applied without the assumption or expectation that the same constructs are being measured by the two tests (Feuer et al., 1999). Projection linking is directional. That is, projecting NAEP scores onto the TIMSS scale is different from projecting TIMSS scores onto the NAEP scale. In addition, this approach requires a linking sample where (groups of) students take items from both tests. The projection linking analysis uses the linking sample to model the relationships between scores on the two assessments. In this linking study, the students in the braided-booklet samples provided answers to test questions or items from both NAEP and TIMSS at the same time and under the same conditions, without knowing whether they were given an operational test booklet, or a braided booklet with items from two different assessments. In addition to responding to cognitive test items, the braided-booklet samples assessed during the NAEP administration window were given the NAEP survey questionnaires. Likewise, the braided-booklet sample under the TIMSS administration window took the TIMSS survey questionnaires. Therefore, the current design allowed us to directly estimate the joint NAEP-TIMSS population-structure model by using survey questionnaires and students' responses to the cognitive test questions and taking into account the relationship between the two assessments. The conditional proficiency distribution of TIMSS given the NAEP proficiency distribution can subsequently be derived from the braided-booklet sample and serve as the projection linking function. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). Given the availability of the braided-booklet samples under both NAEP and TIMSS administration windows as shown in Figure 1, we were able to derive two projection functions for each subject domain and compare them for consistency. Note that in theory, the braidedbooklet samples from both administration windows can be combined to estimate a single projection function for each subject. However, as will be more evident from the description of the projection linking procedure that follows, forming a single projection function would not have been a straightforward replication of deriving a projection function for an individual braided-booklet sample, as the students in the NAEP window took only either mathematics or science, while those in the TIMSS window took test items from both subjects. For this study, the braided-booklet samples across assessment windows were not combined in deriving projection functions. Next, a six-step procedure, which was applied to carry out the projection linking, is described. Step 1: Apply the NAEP and TIMSS latent proficiency scale item IRT parameters to the linking sample item responses The NAEP and TIMSS latent proficiency scales are both estimated based on a combination of IRT models (see, for example, Allen, Donoghue, & Schoeps, 2001, Foy, Galia & Li, 2008. For dichotomously scored items two-and three-parameter logistic models (Lord & Novick, 1968) were used while for polytomously scored items the generalized partial-credit model (Muraki, 1992) was used. The braided instrument that was administered to the braided-booklet samples included the complete pool of items administered in the 2011 NAEP and TIMSS mathematics and science assessments. We used the operational 2011 NAEP item parameter estimates 3 to calculate NAEP proficiency estimates for the braided-booklet samples. Likewise, we applied the operational 2011 TIMSS item parameter estimates from the overall mathematics and science scales in the calculation of TIMSS proficiency estimates. Details about the IRT model fit evaluation will be provided in the forthcoming NAEP-TIMSS linking study technical report. Step 2: Estimate the projection function for the braided-booklet samples In the second step, the \"conditioning\" procedure was employed to estimate the joint NAEP and TIMSS proficiency distribution through a latent regression model, based on the IRT parameters from step 1, student responses to the subset of items they received, as well as other relevant and available background information. 4 This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). administration window, a bivariate latent regression population-structure model was used to estimate this joint distribution of NAEP and TIMSS mathematics scores. Plausible values were generated for all students in the braided-booklet sample. These plausible values can subsequently be used to represent probabilities in joint and conditional proficiency distributions, and allow unbiased group-level estimates. Similar to the calibration method, 20 plausible values were drawn for individual students in the braided-booklet sample. The same conditioning procedures were used to estimate the joint distribution of NAEP and TIMSS science proficiencies from the science linking sample in the NAEP administration window. Students in the TIMSS window linking sample were administered items from both subjects (mathematics and science) and assessments (NAEP and TIMSS), and so a four-variate latent regression was conducted where each combination of subject and assessment comprised a dimension-NAEP mathematics, NAEP science, TIMSS mathematics, and TIMSS science. Step 3: Transform the proficiency distributions for the braided-booklet samples from the IRT metrics to the reporting metrics The NAEP and TIMSS proficiency distributions for the braided-booklet samples obtained from step 2 were estimated on the NAEP and TIMSS IRT scales, respectively. The third step is to place the proficiency distributions on the NAEP and TIMSS reporting metrics. Both NAEP and TIMSS apply linear transformation to transform results from IRT metrics to the appropriate reporting metrics. In operational TIMSS analysis, based on concurrent IRT calibration approaches, linear transformation parameters are estimated that transform the distribution of the previous assessment data under the concurrent calibration to match means and standard deviations of the distribution of these data that are available on the reporting metric. Those transformation parameter estimates are then used to place the current assessment data on the TIMSS reporting scale. Student plausible values are used in computing the means and standard deviations of the score distribution. There exist five plausible values for individual students. A total of five sets of transformation parameter estimates ( 's and '\u015d i A\u02c6i B ) are available, one for each plausible value. The transformation equation is as follows:"}, {"section_title": "PV", "text": "was the plausible value i on the transformed TIMSS reporting scale;\nwas the plausible value i on the original IRT scale on the TIMSS IRT scale; and \u2022 and \u02c6i A\u02c6i B were the estimates of the linear transformation parameters. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). Instead of obtaining and applying five sets of transformation parameter estimates, NAEP estimates only one set of transformation parameters and \u00c2B , which is computed by first averaging the means and standard deviations of the score distribution obtained from both the IRT and NAEP reporting metrics. For the braided-booklet samples in the NAEP-TIMSS linking study, given that the original 2011 NAEP item parameter estimates were used in estimating the plausible values on the calibration scale, we applied the transformation parameter estimates and \u00c2B from the operational 2011 NAEP analysis 5 5 For 2011 NAEP science, an overall univariate scale was established in the operational analysis. Therefore the transformation constants A and B from the operational 2011 NAEP science analysis were directly applied. For 2011 NAEP mathematics, five separate scales were constructed in the operational analysis, one for each content domain. For the purpose of this linking study, an overall univariate scale was first established for 2011 NAEP mathematics and linked to the NAEP mathematics reporting scale. The transformation constants A and B obtained from the overall NAEP mathematics scale were applied to the braided-booklet samples. to place the NAEP plausible values on the NAEP reporting metric. Likewise, the transformation parameter estimates from the operational 2011 TIMSS analysis were used to place the TIMSS plausible values from the IRT scale on the TIMSS reporting metric. To transform 20 plausible values drawn in step 2 to the TIMSS reporting metrics, each of the five sets of transformation parameter estimates from the operational 2011 TIMSS analysis was applied to four different plausible values. Step 4: Smooth the projection functions from the braided-booklet samples Taking the NAEP and TIMSS plausible values obtained in step 3, the joint NAEP-TIMSS proficiency distribution for each subject estimated from the plausible values was smoothed using a continuous bivariate exponential family distribution (Haberman, 2011). With the NAEP and TIMSS latent proficiencies presented as a joint continuous distribution, the projection function was smoothed by deriving the conditional distribution of TIMSS proficiency given NAEP proficiency. Step 5: Predict TIMSS scores for all the states The prediction functions derived in step 4 were used to predict TIMSS plausible scores for students in the 2011 NAEP national sample. For each subject, mathematics and science, there were five NAEP plausible values available for each student in the 2011 NAEP national sample. Four plausible values were drawn from the conditional TIMSS proficiency distribution for each given NAEP plausible value. Then, for each student, a total of 20 new sets of predicted TIMSS plausible values were drawn. The predicted TIMSS plausible values were used to estimate individual state average TIMSS scores and the percentage of students reaching each of the TIMSS international benchmarks. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). Step 6: Additional linear adjustment to the predicted overall TIMSS mathematics and science distributions The predicted TIMSS plausible values obtained from step 5 of the projection linking procedure are estimates of how students in the 2011 NAEP sample would have performed if they had taken TIMSS, to the extent that differences between NAEP and TIMSS are accounted for in the projection functions. To better facilitate comparisons to other countries and subnational education systems that participated in TIMSS2011 during the TIMSS window and under TIMSS administration conditions, the distributions of predicted TIMSS plausible values from the 2011 NAEP national sample were then aligned (through a linear transformation adjustment) to the distribution of TIMSS plausible values from the 2011 TIMSS U.S. national sample, separately for mathematics and science.  Table 2 contains the estimates of the adjustment function parameters, separately for mathematics and science, and for the different projection functions obtained from the NAEP and TIMSS window braided-booklet samples. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). "}, {"section_title": "Findings from calibration and statistical projection", "text": "The key findings from the calibration and projection linking methods are presented next. For projection linking, as discussed above, two separate projection functions were developed for each subject-one using the braided-booklet sample data from the NAEP testing window and one using that from the TIMSS testing window. Besides the main goal of providing predicted TIMSS results for the states that took NAEP, another question of interest in the study is whether the braided-booklet samples and instruments that were developed for the two assessment windows were necessary for carrying out a projection type of linkage. Differences were found between the projection functions obtained from the two linking samples. For the nine validation states that had their actual TIMSS scores, before applying the linear adjustment as described in step 6 of the projection linking, the projected state TIMSS means were closer to their actual results when using the projection function derived from the braided-booklet sample from the NAEP testing window. The linear adjustment applied to the projection-based TIMSS proficiency distribution generally reduced differences between the predicted and actual state TIMSS results. In addition, after incorporating the linear adjustment, the projection-based results with projection functions derived from the two testing windows were comparable. Details on the projection-This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). based results with and without the linear adjustments will be provided in the forthcoming NAEP-TIMSS linking study technical report. For the purpose of comparing the predicted TIMSS results obtained from different linking approaches, we use the projection-based results, incorporating the linear adjustment, derived from the NAEP window braided-booklet sample. Tables 3a and 3b contain the state-level predicted TIMSS mean scores from both calibration and projection linking approaches and differences thereof for Mathematics (Table 3a) and Science (Table 3b). In addition, the actual TIMSS mean scores for the validation states obtained by directly participating in TIMSS are presented along with differences between those and the predicted scores. The last column shows that the two linking approaches result in largely comparable predicted results. Based on the nine validation states, it shows that the calibration has a very slight edge over projection when compared to the actual TIMSS results. That being said, it is also observed from columns 4 and 6 that there were sizeable discrepancies between predicted and actual state results for more than half of the validation states. The complete set of predicted TIMSS results, including predicted state-level means and percentages of students at or above the four TIMSS international benchmarks are listed in Tables 4a and 4b. These four benchmarks are: 625 (Advanced), 550 (High), 475 (Intermediate), and 400 (Low). These benchmarks provide a way to interpret the average scores and understand how students' proficiency in mathematics and science varies along the TIMSS scale. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). 3.3 95 0.6 80 1.4 43 2.1 10 1.5 532 3.4 95 0.7 79 1.6 1.7 9 1.3 North Carolina 515 3.5 93 1.5 70 1.9 33 1.9 7 1.3 514 3.4 93 1.1 70 2.0 1.8 7 1.0 This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460)."}, {"section_title": "Standard Error Estimation For Calibration and Projection", "text": "The 2011 TIMSS eighth-grade achievement results for the participating countries, subnational education systems, and the nine states in the United States, were released in December 2012. The standard errors of the actual TIMSS mean scores and benchmark percentages include sampling and measurement components As a result of the linking study, we predicted TIMSS state results for the states that participated in NAEP. For all the states (validation plus non validation states), the error variance associated with predicted TIMSS results can be expressed as In both the NAEP and TIMSS assessments, a jackknife procedure is used to calculate sampling error for any reporting statistic directly. As discussed before, for calibration linking, the predicted TIMSS proficiency distribution for the NAEP national sample was transformed from IRT scale to the TIMSS reporting metric. For projection linking, the predicted TIMSS proficiency distribution for the NAEP national sample obtained was adjusted to have the same mean and standard deviation as the reported TIMSS U.S. national sample. It can be conjectured that the transformation/adjustment function parameter estimates from calibration and projection linking are subject to non-negligible error. Therefore, a jackknife procedure was employed at the transformation/adjustment stage as well as the summary statistics estimation stage to estimate both sampling and linking errors. The standard errors for the predicted state-level TIMSS results are provided in Tables 4a and 4b. The linking study can be thought of as a linking and prediction question where state-level TIMSS results are to be predicted. The variance estimated in equation (E5) captures the uncertainty of the linking function. However, there is also uncertainty associated with predicting a new point based on the linking function, which is referred to as prediction residual error variance. How to estimate prediction residual error variance could be challenging, given that a number of factors are involved that (a) may not be separable and (b) may represent not only random variance, but also bias. The mean squared error (MSE) can be used to quantify the discrepancies between actual and predicted values (see equations (E6) and (E7) for the formula of MSE). The MSE of prediction includes both variance and bias squared. For example, in sampling, the variance would be random error due to drawing different samples. Bias would be the result of using different sampling rules (such as eligibility requirements) where the populations used to draw the samples are no longer the same. The square of this (systematic) bias plus the (random) variance is the MSE. In comparing the predicted TIMSS scores with the actual scores, the bias portion can be expected to be considerable due to the many differences in administration policies and procedures. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). Subsequently, treating MSE as the prediction residual error variance in standard error calculation and hypothesis testing might result in misleading statements, indicating no significant differences when there are real differences if results from equivalent samples and under equivalent conditions would have been compared. Yet, the MSE may give an indication of how large this combined error is relative to the three random error components discussed above. Effort has been made to adjust the predicted state results with the intention to (partially) remove bias and to review the impact of factors related to differential accommodations and exclusions. The idea is that if all or most of the bias can be accounted for, the remaining term is a prediction random error term that can be used in hypothesis testing. The section below on Selection Bias provides an account of this effort so far and the following conclusion is drawn from this (preliminary) work. While some impact was detected, these corrections are ad hoc and experimental in nature, do not fully account for many other sources of bias, and still need to be further studied in terms of removing bias components appropriately. But such analyses are insightful to assess what level of bias reduction could be obtained by applying some initial approaches."}, {"section_title": "Selection Bias and Predicted TIMSS Score Adjustments", "text": "To further evaluate the predicted state results, we define prediction residual error as the difference between predicted and actual state results on TIMSS, then predicted residual sum of squares, or PRESS, across the nine validation states can be used as a summary measure of the prediction model where i t is the actual observed state result for the i th validation state, and \u00ee t is the predicted value. We further define MSE as Var t i ( ) s the variance of the predicted result for the ith validation state, and Var t is the variance of the actual result for the ith validation state. Taking calibration linking results as an example, the PRESS and MSE for the predicted mean scores from calibration linking were computed across the nine validation states. The results are listed in the first row of Tables 6a and 6b. To the extent that these discrepancies showed a consistent pattern, several possible factors were considered, including construct differences, administration differences, and sample/target This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). population differences. Among those, a significant factor is the difference in exclusion rate/accommodation policy. As shown in Figure 3, TIMSS exclusion rates are in general higher than in NAEP, at the national level, and for individual validation states, largely because accommodations are not offered in TIMSS. Such difference in the selection of assessment samples is referred to as sample selection bias. Two types of ad hoc adjustments were considered to assess and quantify the impact of selection bias due to differences in exclusion rates and accommodation policies. The first is to adjust the state exclusion rates in NAEP to be the same as in TIMSS. Note that we only know the exclusion rate for TIMSS at the state level for the validation states and, therefore, the following analyses are based on that subset only. With no information on which and how student groups are excluded in TIMSS but included in NAEP, this procedure presumed that those students that would most likely be excluded from TIMSS are the lowest performing accommodated (i.e., Students with Disabilities (SD) and/or English Language Learners (ELL)) students in NAEP. From each state sample, the exact number of accommodated students were identified and excluded such that NAEP state-specific \"inclusion\" rates matched TIMSS state-specific inclusion rates. The predicted state results were then computed based on the reduced NAEP state samples. A second possible ad hoc adjustment would be to account for as many as possible bias factors and using a \"residual\" MSE as a fourth variance component (in addition to measurement, sampling, and linking variances) in standard error estimation. As described in the section, Evaluation of Methodologies, for the nine validation states the prediction residual error is negatively correlated with the state percentage of accommodation rates in NAEP. Subsequently, a simple linear regression was built and estimated to minimize the variance of prediction residual error for the nine states. The scores were adjusted before calculating the MSE. This approach is reasonable in principle. However, MSE contains estimation bias as well as variability. Given the limited state-level data about TIMSS' exclusion rates of SD and ELL students, it cannot be tested whether a sufficient amount of bias has been accounted for. In other words, it is not determinable whether a mostly random variance component is obtained or major sources of biases still are left unaccounted for MSE. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460).  Figure 3. Percentage of students excluded from NAEP and TIMSS assessments at grade 8: 2011. Table 5a provides the actual TIMSS means and rankings of the nine validation states in mathematics. Also provided are rankings of the validation states and the prediction residual errors based on a. Predicted TIMSS from the calibration linking (i.e., baseline); and b. Predicted TIMSS from the calibration linking adjusted for exclusion rate differences between NAEP and TIMSS (i.e., reduced NAEP samples)."}, {"section_title": "0%", "text": "For reference, the ranking of the nine states based on the reported NAEP mathematics scores are listed as well. The PRESS and MSE computed from equations (E6) and (E7) are presented in Table 6a. Comparing to the predicted state means from calibration linking in the top row, the adjustment yields smaller prediction residual errors for most of the validation states and commensurate reduced PRESS and MSE values. Science results show similar patterns and are presented in Tables 5b and 6b. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460)."}, {"section_title": "Linking Methodology: Statistical Moderation", "text": "The following describes the statistical moderation technique applied to establish a link between the 2011 NAEP and the 2011 TIMSS in grade 8 in mathematics and science. In this approach, NAEP results are expressed in the metric of TIMSS. By expressing both assessments in the same metric, statistical moderation estimated the state TIMSS means and percentages of students by TIMSS benchmarks that each state might have obtained had that state actually taken TIMSS. The 2011 NAEP-TIMSS linking using statistical moderation was accomplished in five steps. (Please Note: Steps 1 and 2 correspond to the first stage adjustment, and step 3 corresponds to the second stage adjustment referred to in the highlights report, U.S. States in a Global Context: Results From the 2011 NAEP-TIMSS Linking Study, NCES 2013-460.) Step 1: Estimating State TIMSS-Equivalent Means from State NAEP Means In the discussion below x = NAEP and y = TIMSS are used in the formulas. In the study by Johnson, Cohen, Chen, Jiang, & Zhang (2003), NAEP was linked to TIMSS using statistical moderation. The same methodology is used in the 2011 NAEP/2011 TIMSS linking study. This means the estimated scores are actually NAEP scores adjusted to have the same mean and standard deviation as TIMSS. That is what it means in statistical moderation to say \"NAEP is linked to TIMSS.\" The state mean TIMSS-equivalent 1 j z associated with a NAEP state mean In equations (A1) and (A2), \u2022 \u00c2 is an estimate of the intercept of a straight line, and B is an estimate of the slope; \u2022 x and y are the national public school means of the U.S. NAEP and U.S. TIMSS results; \u2022 \u02c6x \u03c3 and\u02c6y \u03c3 are the public school standard deviations NAEP and TIMSS respectively; and z is the mean TIMSS-equivalent of the NAEP mean j x in state j. The error variances in the mean TIMSS-equivalents are This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). The square root of equation A3is the standard error of linking and forms the basis for the standard errors reported in Tables 15, 16, 19, and 20. According to Johnson et al. (2003), the error variances of the parameters of the linear transformation, , and \u03c3 \u03c3 2 \u03c3 can be approximated by Taylor-series linearization (Wolter, 1985). Estimates of the Means and Standard Deviations. The process begins with the analysis of plausible values for both NAEP and TIMSS. In this study, only public school students were included in the analysis of plausible values for both NAEP and TIMSS. In both NAEP and TIMSS, five plausible values were used to represent the student's posterior distribution. Let us label the parameter we are estimating as P, the number of plausible values as \"N,\" and the estimates of P as \u2211 . This formula was used to estimate the means and standard deviations in Table 7 and the linking parameter estimates in Table 11. Table 7 shows the calculations for the parameter estimates of the means and standard deviations. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). Error variance (sampling). Let us label the error variance due to sampling as S. For example, the error variances for the parameter estimates of the means and standard deviations are shown in Table 8. The sampling error in the estimates of the means and standard deviations were obtained using a jackknife error variance approach for complex samples. The jackknife procedure was carried out for each plausible value and then averaged across all five plausible values. In the jackknife procedure, one primary sampling unit (PSU) is excluded; the sampling weights are redistributed across the other units within the stratum in which the PSU was excluded; the mean and standard deviations are calculated on the remaining PSUs; and the process is repeated until all PSUs have been excluded. After the jackknife procedure is carried out on each plausible value, the average across plausible values is This process results in the variance estimates reported in Table 8, which are estimates of error variance due to sampling for the mean and standard deviations. This same process was carried out for error variances due to sampling for the linking parameters estimates in Table 12. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). Error variance (measurement). Let us label the error variance due to measurement as M. For example, the error variance for the parameter estimates of the means and standard deviations due to measurement are shown in Table 9. This is estimated by . This same process was carried out for error variances due to measurement for the linking parameters estimates in Table 13. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). Error variance (total) of the mean and standard deviation. The total error variance is T = + S M and is shown in Table 10.  Tables 12 and 14 were obtained from equation (A4). Each component of equation A4was calculated using procedures described above in the error variance (sampling) and error variance (measurement) section. Estimates presented in Table 14 were obtained as sums of values from Tables 12 and 13. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460).   Error variance (measurement) of the linking parameters A and B. The quantities needed to estimate the error variance in the linking parameters due to measurement error are shown in Tables 13 and 14. Tables 15 and 16 show standard error estimates for the nine validation states.  Covariance between A and B for Science, ( ) This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). Step 2: Adjusting the State TIMSS-Equivalent Means to Account for Differences in Accommodation Rates between NAEP and TIMSS HumRRO conducted an investigation of the relationships between state-level accommodation rates and mean scores. They recommended that the state TIMSS-equivalent means be adjusted to account for differences in the accommodation rates among states that predict differences between NAEP and TIMSS exclusion rates. The derivations of specific adjustments are described in a later section of this report, Adjustments to Predicted State Mean Estimates, on pages 82 and 83. The following adjustments were used following the HumRRO recommendation: \u2022 For mathematics: where % Acc j is the percentage of students in state j receiving NAEP accommodations; and 9.7 is the national NAEP accommodation rate for mathematics. \u2022 For science: where % Acc j is the percentage of students in state j receiving NAEP accommodations; and 10.6 is the national NAEP accommodation rate for science. The estimated state accommodation rates are shown in Tables 17 and 18. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460).  This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). The TIMSS-equivalents of the nine validation state NAEP means with adjustments for accommodations are contained in the Tables 19 and 20. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). Step 3: Predicting State TIMSS Means from Adjusted TIMSS-Equivalents of State NAEP Means In the sections above, the goal was to link or rescale NAEP to have the same scale as TIMSS. This allows us to find the NAEP score on the NAEP scale that is the TIMSS-equivalent of the TIMSS international benchmarks Low, Intermediate, High, and Advanced. A second goal of the study is the estimated state performance on TIMSS based on NAEP performance in the 43 states in which TIMSS was not administered at the state level. We can do that in this study by taking advantage of the correlation between NAEP and TIMSS estimated from the nine validation states. The prediction of the state TIMSS means from the state NAEP means can be accomplished through statistical projection. With intercept and slope regression parameters The quantities in equation A5are defined as follows: z is the predicted state mean TIMSS-equivalent for a given 1 j z ; \u2022 1 z is the weighted mean of the adjusted TIMSS-equivalent means (from step 2) among the nine validation states (weighted by the effective sample sizes in each state); \u2022 1 j z is the adjusted state mean TIMSS-equivalent (from step 2) obtained for each of the validation states; \u2022 1 z \u03c3 is the weighted standard deviation of the adjusted state means of TIMSS-equivalents in the nine validation states; \u2022 y is the weighted mean of the actual TIMSS means among the nine validation states; This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). \u2022 \u02c6y \u03c3 is the weighted standard deviation of the state means of actual TIMSS among the nine validation states; and \u2022 \u03c1 is the weighted correlation between the state's mean TIMSS-equivalents 1 j z and actual TIMSS state means j y in the nine validation states. The error variance in the projection is found by The square root of equation A7is the standard error of projection that is presented in Tables 22  and 24. In equation (A7) the projection error variance components are as follows: \u2022 2 \u03b2 times the linking error variance 1 2 j z \u03c3 in the TIMSS-equivalents, and \u2022 the prediction error variance (how accurate the \u03b1 and \u03b2 were estimated) The variances and co-variances of and \u03b1 \u03b2 in equation A7  This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460).  2  2  2  1  1  1  2  The components of equations (A8) to (A10) can be estimated as follows:  This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). Weighted correlations between the TIMSS-equivalent means and the actual TIMSS means for the nine validation states were calculated with and without accommodation adjustments. Without accommodation adjustments, the weighted correlations were .92 and .93 for mathematics and science, respectively. After the accommodation adjustments were applied to the nine states, the weighted correlations were .94 and .97 for mathematics and science, respectively. In both cases the weighted correlation between TIMSS-equivalent means and actual TIMSS means were improved by the adjustment for accommodations. Therefore, the accommodation adjustments in both mathematics and science were used. Below are projections that were conducted for the nine validation states with the accommodation adjustments. Tables 21 and 23 show the projection  parameter estimates and Tables 22 and 24 show the resulting state mean estimates and standard errors for mathematics and science respectively. The standard errors in Table 22 contained two components, standard error of linking and standard error of prediction. The error variance of linking was given in equation A3, and the error variance of prediction was given in equation A7, which estimates the degree of uncertainty in the prediction equation. Note that in the section Evaluations of the Methodologies another source of error-model error-was discussed. Model error is a valuable criterion in quantifying the discrepancies between actual and predicted values. However, in reporting the predicted TIMSS scores for the 43 states that did not participate in TIMSS at the state level, the model error in standard error calculation and hypothesis testing was not included. This is because model error variance reflected estimation bias as well as variability/standard error, and the data at the state level necessary to evaluate and account for bias was limited for the validation states.   This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). PrP We can define .   In the above equation Var \u03c3 is the error variance in the standard deviation of 1 j z . This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). Step The parameter estimates needed to conduct the projections for each of the international benchmarks (step 5) and the resulting distributions for the nine validation states are contained in Tables 25 through 40 below.        This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460)."}, {"section_title": "Evaluations of the Methodologies", "text": "The following section describes the key findings from HumRRO's evaluation and summarizes the evidence underlying those findings. In the following descriptions, the methodologies are referred to CAL for the joint calibration method, PRO for the statistical projection method, and MOD for statistical moderation."}, {"section_title": "Evaluation Design", "text": "Two stages were included in the plan for evaluating the results of the linkage study. The first stage involved applying each of the linkages to state NAEP samples for the nine validation states participating in TIMSS and comparing the resulting estimates to corresponding estimates generated from the operational TIMSS state samples. HumRRO reviewed NAEP and TIMSS reports and concluded that the statistics most likely to be used in reporting results from the linkage were scale score means and the percentage of students at or above each of the TIMSS benchmarks. HumRRO also examined differences in the estimated TIMSS scale score standard deviations for each validation state, providing a general comparison of differences in the estimated scale score distributions throughout the score range. In addition to comparing statistics for each state sample as a whole, they also examined differences in linkage estimates for subgroups defined by gender and, where sample size permitted, race/ethnicity. The second stage of the evaluation involved investigation of the extent to which key differences between the two assessments affected the linkages or threatened the validity of key interpretations. Prior to analyzing any data, HumRRO conducted discussions with key members of the Quality Assurance Technical Panel (QATP) 6 6 The QATP comprises nine nationally and internationally recognized experts in various aspects of assessment who work with HumRRO to design and implement special quality assurance studies. Four panelists, in particular, provided ongoing advice on the NAEP-TIMSS linkage: Kadriye Ercikan, Mark Reckase, William Schafer, and Richard Wolfe. to identify differences between the two assessments that might plausibly affect the scale score linkages. Figure 4 lists key differences identified in these discussions. Some differences, such as differences in accommodation and exclusion rates, could be readily quantified so that state-level differences could be related to state-level differences in the linkages. Others, such as the impact of the difference in testing windows, could not be investigated directly from the available data. Note that the braided samples did provide estimates from each of the two assessments during each testing window, but the braided samples were too small to support separate analyses by state and also testing window differences were confounded with other differences in test administration procedures. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460).  "}, {"section_title": "Primary Findings and Conclusions -Stage 1", "text": "Tables 41 and 42 show differences between estimates of mean TIMSS scale scores from the operational TIMSS samples and from the NAEP state samples using each of the three linkage methods. (See also Tables 1 and 2 from the ETS section and Tables 7 and 8 from the AIR section above.) The root mean square error (RMSE) provides an overall indicator of the accuracy of each linkage method in estimating state means. Confidence bounds for both the empirical TIMSS estimates and estimates using the NAEP linkages include estimates of sampling and measurement error. In addition, the estimates generated from the NAEP samples include error variance associated with error in estimating the linkage functions. Figures 5 and 6 show confidence bounds estimated for each of the empirical and linkage-based estimates of state means. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). NOTE: MOD=Moderation; PRO=Projection; CAL=Calibration. The U.S. national samples for NAEP and TIMSS include students from both public and private schools. The \"nation\" results presented in the table were estimated using the students from public schools only, for comparison to the states which are restricted to public school students. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). NOTE: MOD=Moderation; PRO=Projection; CAL=Calibration. The U.S. national samples for NAEP and TIMSS include students from both public and private schools. The \"nation\" results presented in the table were estimated using the students from public schools only, for comparison to the states which are restricted to public school students. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460).  This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). As shown in Figures 5 and 6 the confidence bounds for the empirical and linkage-based estimates of state means did not overlap for several states. Note that the confidence bounds for the empirical TIMSS means are larger than for the linkage-based projections because the TIMSS state samples are considerably smaller than the NAEP state samples used in generating the linkage-based projections. Tables 43 and 44 show statistical significance of the difference between the empirical and linkage-based estimates for mathematics and science respectively. As shown, the differences were statistically significant for nearly half of the validation states in mathematics and for at least two validation states in science. Tables 45 through 48 show differences in estimates of the percentage above each of the TIMSS benchmark cutoffs along with statistical tests of these differences. As with the state means estimates, differences between empirical and linkage-based estimates were larger than would be expected based on estimates provided by AIR and ETS of the standard error of each estimate. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). Moderation results were based on moderation linking before the two-stage adjustment. B: The standard error includes sampling and measurement errors only. NOTE: Bold font indicates predicted means are statistically significant from the actual means. The U.S. national samples for NAEP and TIMSS include students from both public and private schools. The \"nation\" results presented in the table were estimated using the students from public schools only, for comparison to the states which are restricted to public school students. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). Moderation results were based on moderation linking before the two-stage adjustment. B: The standard error includes sampling and measurement errors only. NOTE: Bold font indicates predicted means are statistically significant from the actual means. The U.S. national samples for NAEP and TIMSS include students from both public and private schools. The \"nation\" results presented in the table were estimated using the students from public schools only, for comparison to the states which are restricted to public school students. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). Moderation results were based on moderation linking before the two-stage adjustment. B: The standard error includes sampling and measurement errors only. NOTE: P-A=Predicted minus Actual; Bold font indicates predicted estimates are statistically significant from the actual estimates. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). Moderation results were based on moderation linking before the two-stage adjustment. B: The standard error includes sampling and measurement errors only. NOTE: P-A=Predicted minus Actual; Bold font indicates predicted estimates are statistically significant from the actual estimates. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). Moderation results were based on moderation linking before the two-stage adjustment. B: The standard error includes sampling and measurement errors only. NOTE: P-A=Predicted minus Actual; Bold font indicates predicted estimates are statistically significant from the actual estimates. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). : The standard error includes sampling and measurement errors only. NOTE: P-A=Predicted minus Actual Bold font indicates predicted estimates are statistically significant from the actual estimates. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). Tables 49 and 50 show differences for each gender between estimates of mean TIMSS scale scores from the operational TIMSS and predicted TIMSS means from each of the three linkage methods. At the national level, the errors for each gender were small and not statistically significant, although the PRO method yielded slightly larger errors (greater than half a scale score) in the estimates for males compared to the other two methods. The pattern of statistically significant differences at the state-level was similar for males and females, both following the pattern of overall errors in state level mean estimates. Figures 7 and 8 display the confidence bounds for the empirical linkage-based estimates of state means for both males and females. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). : The standard error includes sampling and measurement errors only. NOTE: Bold font indicates predicted means are statistically significant from the actual means. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). : The standard error includes sampling and measurement errors only. NOTE: Bold font indicates predicted means are statistically significant from the actual means. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460).  This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). Results from the comparisons of empirical and linkage-based estimates led to the following general conclusions: Finding 1: The three different linkage methods yield similar linkage functions. In all cases, differences in the estimates produced by the three different linkage methods are quite small in comparison to differences between each of the linkage-based estimates and the empirical TIMSS results. Estimates of sampling and measurement error for both the NAEP and TIMSS samples are well established. Linking function error for the statistical moderation approach is based on wellestablished estimates of variation in the national NAEP and TIMSS means and standard deviations. Observed differences between the empirical and linkage-based estimates are larger than predicted by these sources of variation, so other differences between the two assessments must be contributing significant amounts of variation in the estimates. The difference between predicted and actual TIMSS results was not statistically significant for any national gender or racial/ethnic group across all linking methods. The pattern of statistically significant differences at the state-level was similar for males and females, both following the pattern of overall errors in state level mean estimates. Again, the pattern of differences for each racial/ethnic group at the state level was similar to the pattern of errors in the overall state mean estimates."}, {"section_title": "Primary Conclusions -Stage 2", "text": "HumRRO investigated the impact of two key differences between the NAEP and TIMSS assessments: (1) differences in exclusion and accommodation policies, and (2) differences in the distribution of test item difficulty and item formats. Other differences, such as the difference in testing window, could not be investigated within the scope of the current study."}, {"section_title": "Differences in Accommodation and Exclusion Rates", "text": "Tables 59 and 60 show the percentage of students in each of the validation states excluded from the NAEP and TIMSS assessments and the percentage receiving one or more testing This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). accommodations in the NAEP assessment for mathematics and science respectively. 7 7 Note that TIMSS combines the mathematics and science assessments, so exclusion rates are the same for these two subjects. The NAEP program has worked assiduously in recent years to maximize inclusion rates by offering a menu of accommodations and ensuring states and schools correctly include students who can be accommodated. Over time, in general NAEP accommodation rates have grown while exclusion rates declined. However, NAEP exclusion and accommodation rates varied considerably across the nine validation states. TIMSS allows few, if any, accommodations and data on TIMSS accommodation rates were not available. As shown, TIMSS exclusion rates are considerably higher than NAEP exclusion rates. The difference between the percentage excluded in the NAEP and TIMSS assessments also varies considerably from state to state. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460).  Table 61 shows the correlation of errors in estimating TIMSS state scale score means with NAEP and TIMSS exclusion and NAEP accommodation rates. As shown, state differences in the percentage of students accommodated were highly correlated (about .8) with errors in the state mean estimates. Differences in NAEP accommodation rates are significant for two reasons. First, the additional students excluded from the TIMSS assessment are most likely students requiring accommodations in the NAEP assessment that are not provided in TIMSS. Roughly 10 percent of students taking NAEP received accommodations. The percentage of students included in NAEP but not TIMSS was about half of this number. This means that at least half of the students receiving accommodations in NAEP did participate in TIMSS, most likely without these accommodations. Differences in the use of accommodations may also have led to mean score differences for these students. NAEP collects questionnaire data for SD and ELL that provide information about specific student disabilities and characteristics. TIMSS does not collect comparable background information on the students. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). HumRRO investigated several methods for adjusting the NAEP samples to reduce the impact of differences in exclusion and accommodation rates. The first approach (Accommodations Reweighted, or AccRW) involved proportionally reducing the weight assigned to each student receiving accommodations by an amount related to the difference between the NAEP and TIMSS exclusion rates for each state. The ratio of sum of weights for the reweighted and original NAEP sample was equal to the ratio of the TIMSS and NAEP inclusion rates. We also examined options for reweighting accommodated students differentially based on type of accommodation or primary disability code, but found that these options provided essentially the same results as the proportional reweighting. (See full technical report, forthcoming, for more details.) A second approach (Accommodations Adjusted, or AccADJ) involved an empirically derived adjustment based on the percentage of students accommodated in NAEP. We do not have actual TIMSS exclusion rates for states not participating in TIMSS. The percentage accommodated in NAEP is the best available predictor of the difference in NAEP and TIMSS exclusion rates. The correlations shown in Table 61 led to an adjustment that added approximately two TIMSS scale score points for every percentage point that a state's NAEP accommodation rate exceeded the national average. Thus, the initial AccADJ model used a coefficient of 2 to compute adjusted predicted means. In reviewing initial results with our technical panel, it was noted that the race/ethnicity distributions differed for the NAEP and TIMSS samples in several of the validation states. This difference may have resulted from differences in exclusion rates by race/ethnicity or may have resulted from differences in school and class participation rates by race/ethnicity that were not fully accounted for in nonresponse adjustments. A third approach (Race Adjusted, or RaceADJ) involved reweighting the NAEP samples for each state to yield the race/ethnicity distribution of the TIMSS state sample. We also examined an adjustment (Accommodations and Race Adjusted, or RaceAccADJ) that combined the race/ethnicity adjustment and the adjustment based on accommodation rates. Figures 9 and 10 display the adjusted means using the RaceAccADJ. While the RaceAccADJ did improve prediction, it was not feasible to use this approach for states not participating in TIMSS, since TIMSS race/ethnicity distributions would not be available. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460)."}, {"section_title": "NOTE:", "text": "The adjustment coefficients and observed percentage accommodated were treated as constants so that the standard errors and confidence bounds for the adjusted estimates were the same as for the original estimated TIMSS means. NOTE: The adjustment coefficients and observed percentage accommodated were treated as constants so that the standard errors and confidence bounds for the adjusted estimates were the same as for the original estimated TIMSS means. Figure 10. Adjusted projected TIMSS means using the race and accommodation adjustment (RaceAccADJ) -Science. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). Figure 11 shows the RMSE for estimates of state means using each of the three linkage methods and each of the four adjustments. The race/ethnicity adjustment, by itself, yielded only a small reduction in error. The accommodation adjustment and the combination of race/ethnicity and accommodation adjustments led to the largest reduction in errors. Figure 11. Comparison of error rates resulting from each of the four adjustments for exclusion and accommodation differences. \nWe did not create a separate AccADJ equation for the CAL and PRO methods, but we did compute \"residual error\" separately for each method and created confidence bounds that reflected the revised total error estimates. Figure 12. Adjusted projected TIMSS means using the accommodation adjustment (AccADJ) and incorporating model error in the confidence bands -Mathematics.\nWe did not create a separate AccADJ equation for the CAL and PRO methods, but we did compute \"residual error\" separately for each method and created confidence bounds that reflected the revised total error estimates. Figure 13. Adjusted projected TIMSS means using the accommodation adjustment (AccADJ) and incorporating model error in the confidence bands -Science. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460)."}, {"section_title": "Test item differences", "text": "Examination of NAEP and TIMSS differences in item difficulty and format did not lead to any plausible corrections to the NAEP-TIMSS linkages. There were no significant differences in the distribution of item difficulties for multiple choice and constructed response items. There were differences in the degree to which the assessments used short and extended constructed response items, but there were no significant differences among states in students' relative performance on This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460).  "}, {"section_title": "Adjustments to Standard Error Estimates", "text": "Additional analyses performed as part of this evaluation involved developing an estimate of the additional variance in linkage-based estimates. The approach used will be described in detail in the forthcoming technical report for this linking study. Briefly, it involved examining the variance of differences between the empirical and linkage-based estimates of state means and This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). subtracting out known estimates of variance due to NAEP and TIMSS sampling and measurement error and linkage error. Tables 64 and 65 show estimates of the different NAEP and TIMSS variance components for each state and the squared difference between the linkage-based and empirical TIMSS mean estimates for the state. These analyses used the linkage derived from statistical moderation, because the assumptions of this model are fewer and the linkage error variance is well estimated for this method. Also, we used the predicted TIMSS mean estimates that included the AccADJ described above. We averaged the variance component estimates across the nine validation states and then subtracted these variance components from an unbiased estimate of the mean squared error using eight degrees of freedom to get an unbiased estimate of residual error. This residual error is a consequence of the various differences between the two assessments, although we cannot attribute specific amounts of variance to specific differences. We have labeled this residual variation as \"model error\" to indicate that the variance results from differences in the two assessment models. Tables 66 and 67 show the impact of adding model error into standard error estimates for the linkage-based state means. Further analyses indicated that none of the differences between linkage-based and empirical estimates of TIMSS state means were statistically significant when the expanded standard error estimates were used. Figures 12 and 13 display the AccADJ for math and science with model error. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460)."}, {"section_title": "Adjustments to Percentage Above Cut Estimates", "text": "HumRRO investigated two approaches for adjusting percentage above (Benchmark Level) cut estimates using empirical adjustment based on the percentage of students accommodated in NAEP. The first approach is based on a normal approximation to the projected TIMSS score distribution (AccADJ_Normal). In this approach, we first converted the original percentage estimate into TIMSS scale score metric using the inverse normal cumulative distribution with mean equal to the unadjusted TIMSS mean estimate. This gives a \"normalized\" cut score that may differ from the original cut score depending on how the predicted TIMSS score distribution differs from a normal distribution. The adjusted percentage above cut estimate was then obtained by evaluating the cumulative normal distribution with mean equal to the TIMSS mean estimate that included the empirically derived accommodation adjustment at the normalized cut score. To estimate the standard error for the adjusted percentage above cut estimate, we added and subtracted the adjusted estimate of the standard error of the TIMSS mean estimate that included model error (see Table 64). The standard error estimate is half of the difference between their corresponding percentiles based on the normal distribution with adjusted mean. The second approach applied the accommodation adjustment method directly using the percentile metric by regressing the percentage above cut prediction error on NAEP accommodation rates (AccADJ_Direct). In this approach the adjustment coefficient for the percentage of students receiving NAEP accommodations was estimated separately by benchmark level. The adjusted predicted percentage above cut estimates are obtained by adding the adjustment to the original predicted percentage above cut estimate. Corresponding adjusted standard errors were obtained in the same fashion as in accommodation adjustment for the mean. We obtained an unbiased estimate of the mean squared error by dividing the sum of the squared difference between adjusted NAEP predicted and empirical TIMSS percentage above cut estimates by eight degrees of freedom. We then averaged the NAEP and TIMSS variance components for percentage above cut scores across the nine validation states and subtracted these from unbiased estimates of the mean squared error to get an estimate model error. The adjusted standard error for NAEP predicted percentage above cut estimate is the square root of the sum of the model error and the original variance. Table 68 shows the mean squared errors (MSEs) for the unadjusted NAEP projected percentage above cut estimates and the two adjusted estimates. Tables 69 through 72 compare the two percentage above cut adjustments (AccADJ_Normal and AccADJ_Direct) with model error with the unadjusted estimates without model error. As seen in Tables 69 through 72, the AccADJ_Direct results in one negative estimate (Table 72) and negative model errors for three of the benchmark levels (Tables 69-71). These results, combined with comparisons of the MSEs suggest that normal approximation adjustment is as good as or better than the direct adjustment across all benchmark levels. The normal approximation is also the more parsimonious method because it only requires one adjustment equation as opposed to four separate adjustment equations by benchmark level used in the direct approach. For these reasons, we recommend the This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). normal approximation method to adjust the percentage above cut estimates for differences in NAEP accommodation rates. -No adjustment for % ACC; AccADJ_Normal -Using adjustment to the mean and the normal approximation; AccADJ_Direct -Direct adjustment using a separate regression equation for each cutoff. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). NOTE: Bold font indicates predicted estimates are statistically significant from the actual estimates. Bold underlined font indicates that the model error was negative; thus, the SE estimates were set to equal the unadjusted SEs. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). NOTE: Bold indicates predicted estimates are statistically significant from the actual estimates. Bold underlined font indicates that the model error was negative; thus, the SE estimates were set to equal the unadjusted SEs. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). NOTE: Bold font indicates predicted estimates are statistically significant from the actual estimates. Bold underlined font indicates that the model error was negative; thus, the SE estimates were set to equal the unadjusted SEs. This NCES document was prepared to supplement the \"Linking Methodologies\" section presented in the Global Context report (NCES 2013-460). NOTE: Bold font indicates predicted estimates are statistically significant from the actual estimates. Bold underlined font indicates that the model error was negative; thus, the SE estimates were set to equal the unadjusted SEs."}]