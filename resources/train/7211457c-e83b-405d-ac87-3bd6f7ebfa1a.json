[{"section_title": "I. INTRODUCTION", "text": "S INCE the outbreak of the novel coranavirus disease 2019 , which is caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), there have been more than 30 million confirmed infected cases and more than 1 million deaths has been reported worldwide (as on September 1st). This threat has encouraged joint efforts to obtain accurate early detection of COVID-19 to try and limit the spread of the pandemic.\nWhilst real-time reverse transcription polymerase chain reaction (RT-PCR) COVID-19 test is the current gold standard for diagnostis, this type of test has demonstrated several limits and burdens. Firstly, it is prone to false negatives that heavily AI Aviles-Rivero is with the Department of Pure Mathematics and Mathematical Statistics, University of Cambridge, UK. e-mail: ai323@cam.ac.uk. P. Sellars rely on the sample acquisition characteristics including insufficient quantities and location (nasal, throat or sputum) [1] , [2] . Secondly, the limitation of several world regions in obtaining fast accessibility to the test. The use of imaging techniques, including computerised tomography and chest x-rays, has been suggested as a parallel option to RT-PCR test. Clinical manifestation of COVID-19 is that of a respiratory infection, which is associated with viral pneumonia. To distinguish viral pneumonia from bacterial pneumonia is a challenging task. This task becomes even harder when a large number of suspected patients need to be screened. This time consuming task strains already limited medical resources, thus reducing the efficiency of the diagnostic.\nComputerised tomography (CT) has been a focus of attention in the literature for COVID-19 e.g. [3] , [4] . However, the burden imposed in terms of infection control using CT scan suites and the inefficiencies relating to room decontamination and access restriction from several world regions make CT challenging to be used as a routinely basis -despite its high sensitivity [5] . Due to its wide availability and inexpensive screening, a great focus has been placed on Chest X-rays (CXR) for both clinical and AI areas e.g. [6] - [9] . These advantages make CXR a perfect alternative and complement to the RT-PCR. Despite the CXRs advantages, accurate interpretation still remains a challenge [10] . This is because the accuracy of the interpretation relies on the radiologist's expertise level and there is still a substantial clinical error on the outcome [11] . Therefore, there is an urge for fast automated evaluations of CXRs, to quickly explore the vast amount of data which will save time in evaluating the diseases.\nFor the task of classifying COVID-19 using CXRs data, there has been a fast development of deep learning techniques e.g. [12] - [16] , in which supervised learning is the goto paradigm. However, the performance of these techniques strongly rely on a large and representative corpus of labelled data. In the medical domain and in particular with a new disease, this might be a strong assumption in the design of a solution, as it involves scarce annotations that might contain strong human bias. The current leading supervised model for COVID, COVID-Net [9] , has reported promising results with a sensitivity of 91% for COVID-19. Hence there is still plenty of room for improvements, namely on how to use the vast amount of available unlabelled data to prevent labelling errors and uncertainties from affecting the classification output.\nMotivated by the above limitations in current techniques, we address the following problem -Can one get a robust classifier with performance higher or comparable to the current leading supervised technique for COVID-19 using far less labels? To answer to this question we propose a deep semi-supervised framework to go beyond human bias and the limited amount of labelled data. We remark that many deep SSL techniques e.g. [17] - [20] , have only been considered for natural images. No work has evaluated the performance outside this domain, by considering the fundamental differences between natural and medical images [21] . This paper extends our work in [22] with noticeable differences. Firstly, we construct the graph based on the initial embeddings coming from a deep net, making for an accurate first construction. Secondly, we use our optimisation diffusion model as means for generating pseudo-labels that can be updated iteratively in a trained deep net. Moreover, unlike the pseudo-label perspective of [23] , our technique is a graph based approach and the pseudo-labels are computed by our diffusion model rather than the network. Our contributions are as follows:\n\u2022 We propose a deep semi-supervised framework, in which we highlight: -An optimisation model with strong class priors for multi-class graph diffusion, which is based on normalised and non-smooth p = 1 Dirichlet energy. Our method offers theoretical guarantees and efficient solving. -The connection of our diffusion model to the generation of meaningful pseudo-labels, which avoids the current SSL trend on the use of consistency regularisation. We show that our framework reinforces the natural relation among the tiny labelled set and the vast unlabelled data.\n\u2022 We evaluate our technique with several numerical, statistical and visual results using an unified dataset that contains highly diverse samples from different sources.\nTo the best of our knowledge, this is the first graph based deep semi-supervised technique proposed for identifying COVID-19. Moreover, we also report explainable results from our prediction scores to assist and accelerate the radiologist diagnosis. \u2022 We demonstrate that our technique reports higher sensitivity in COVID-19 and global performance than the current leading deep supervised technique for such application whilst requiring far less labelled data."}, {"section_title": "II. RELATED WORK", "text": "The recent problem of classifying CXRs for COVID-19 has observed a fast growing in the literature. Existing techniques are reviewed in this section.\nA. Chest X-ray Classification for COVID-19\nThe task of classifying CXRs has been widely investigated in the community. The go-to paradigm to address this problem is deep learning [24] , [25] , [25] , [26] . The fast development of these techniques has been motivated by the release of several benchmarking datasets including ChestXray14 [24] and CheXpert [27] , in which a large number of annotated samples from different pathologies (classes) are contained in each dataset. With new diseases such as COVID-19, where the annotated data is limited, one needs to rethink the whole design of the techniques. In this work, we motivate the power of semi-supervised learning for COVID- 19 . In what follows, we review related COVID-19 and SSL techniques.\nThe bulk of literature addressing the task of classifying CXRs for COVID-19 is largely based on deep supervised learning e.g. [9] , [12] - [16] , [28] , and several techniques arise every single day. Most of approaches apply pre-trained off-theshelf networks; in which diverse generic architectures, including ResNet [29] , DenseNet [30] and VGG [31] . Existing works thus leverage on fine tuned networks and networks trained from scratch on CXRs data. However, there are fundamental differences between natural and medical image classification including features and data size. Hence, as shown in [21] , transfer learning might offer little benefit to performance due to the over-parameterisation of standard models. Moreover, the samples available for COVID-19 are scarse in comparison to other type of pneumonia, and one needs to deal with highly imbalanced datasets.\nThe key assumption of supervised techniques is a wellrepresentative labelled dataset, and in new diseases, such as COVID-19, this core assumption is a strong one. Moreover, the available annotations might be far from being a definite expression of ground truth [32] . Therefore, by using deep supervised learning techniques are prone to labelling error and uncertainty that adversely affect the classification output. Although transfer learning [33] and Generative Adversarial Networks [34] mitigate, at some level, the lack of a large and representative dataset, they weakly account for the mismatch between expert annotation and ground truth annotation, which is generated by human bias and uncertainty, and the performance can be limited due to the differences among datasets as demonstrated in [21] .\nMotivated by the above-mentioned drawbacks in deep supervised learning and with the goal of generating a high sensitivity technique that largely decrease the need for large annotation set, we introduce a deep semi-supervised technique for the application to COVID-19 identification, which to our knowledge it is the first in its type. In the next subsection we discuss recent techniques in deep SSL for other domains and how they differ to ours."}, {"section_title": "B. Semi-Supervised Classification for Medical Images", "text": "Semi-supervised learning has been applied in the medical domain since its early developments, in which model based techniques have been the main focus of attention e.g. [35] - [39] . These approaches have demonstrated the potential of SSL-but the generalisation of the feature space along with the computational requirements have raised limitations. Recently, with the advent of deep learning, deep semi-supervised learning has been investigated.\nIn the past few years, there has been a silent revolution in deep SSL techniques, e.g. [18] - [20] , that have sought to combine the theoretical underpinning of SSL [40] with Fig. 1 . Workflow of our proposed technique. We use a tiny labelled set and large unlabelled set. First our technique optimise over the labelled set using (2), this seek to extract meaningful features to construct a strong graph. We then use our proposed diffusion model (5), which results in generated pseudo-labels than then are iteratively optimised using (7). The output is the confidence score per class and the attention map.\nthe generalisation and feature extraction of deep neural networks. The largest trend for new deep SSL methods involves consistency regularisation [17] - [20] . The core idea of this perspective is to, for both labelled and unlabelled examples, induce perturbations \u03b4 and then add a regularisation term to the loss function such that the prediction of the model is invariant to the perturbation, i.e. f (x + \u03b4) = f (x). The main variety between approaches stem from how to generate perturbations \u03b4. The definition of perturbations is indeed a complex problem, and no work has ever evaluated the performance of consistency based approaches outside the natural image domain.\nTo the best of our knowledge, there has been no deep SSL approach proposed for COVID-19 analysis. The potential performance of SSL has nevertheless been shown in our prior work [22] on CXRs analysis. We readily competed with SOTA supervised techniques for identification of several pathologies in CXRs, using a small fraction of the available labels. In this work, we extend this framework to build upon the concept of pseudo-labels, first introduced in [23] . However, compared to that initial work, there are now several key differences since a graph based approach is considered and the pseudo-labels are predicted from our diffusion model rather than the deep net."}, {"section_title": "III. GRAPHX-COVID FRAMEWORK", "text": "This section presents the three parts of our proposed technique: i) data representation and robust graph construction, ii) optimisation model for graph diffusion and iii) driving optimisation that connects our diffusion model with deep nets. The overview of our GraphXCOVID is illustrated in Fig. 1 .\nProblem Definition. Given a small amount of labelled data\n., L} and y h \u2208 L, and a large amount of unlabelled data X u = {x k } n m=l+1 . The whole set of data is thus X = X L \u222a X U , where X L = {x 1 , ..., x l }. We seek to infer a function f : X n \u2192 Y n such that f gets a good estimate for {x k } n m=l+1 with minimum generalisation error.\nIn particular, in a deep semi-supervised setting, one seeks to minimise a functional of the form:\nwhere L S is the per example loss for the labelled set (e.g. standard cross-entropy) and L U denotes a loss defined on the unlabelled set (e.g. consistency loss). Moreover, \u03b3 \u2208 R + is a weighting parameter to balance the two terms, and \u03b8 is the network parameters to estimate. The current go-to perspective in deep SSL is to apply consistency regularisation for L U in (1), which enforces invariant network predictions with respect to perturbations on the unlabelled data X u , e.g. [17] - [20] However, the definition of such \u03b4-pertubations, e.g. flip-and-shift, rotate, posterise and sharpness, is not trivial. In this work, we avoid the explicit definition of such \u03b4 by taking a proxy-based perspective. In particular, we rely on the concept of pseudo-labels [23] \u0177 i for images of the unlabelled set x i \u2208 X u . In this work, the pseudolabels are generated by optimising our graph-based model. Our framework is an iterative two-part technique, the first part concerns pseudo-label generation, including graph representation (see Subsection A) and label diffusion (Subsection B). The second step deals with the update of the generated pseudolabels (Subsection C)."}, {"section_title": "A. Feature Extraction & Graph Construction", "text": "The most common data representation is a Euclidean or grid-like structure. We rather define our data in a non-Euclidean domain with a graph. This framework offers different benefits including mathematical properties such as sparseness which allows for fast computation, and the ability to correct initially mislabelled data by smoothing the embeddings. We represent the dataset X as a graph, where each node is an image, to produce pseudo-labels. Then, unlike pure model-based approaches or pure deep learning techniques, we introduce a hybrid model -that is, a combination between a model-based (energy model) and a deep learning framework.\nA deep network f \u03b8 is considered for updating the pseudolabels generated by our optimisation model. It is initialised from the tiny labelled set (x, y) \u223c D L by minimising:\nwhere the loss function H is cross entropy, which is the most common choice for classification tasks. This optimisation process only involves the provided small labelled set to construct the initial graph (i.e. it is run once as initialisation). More precisely, a given set of data (or features) can be represented as an undirected weighted graph by G = (V, E, W) composed of n nodes V = {v 1 , ..., v n }, which are connected by edges E = {{v i , v j } : v i , v j \u2208 V} with weights w ij = S(i, j) \u2265 0 that correspond to some similarity measure S between the features of nodes i \u2208 V and j \u2208 V, and w ij = 0 if (i, j) / \u2208 E. In our setting, a node v i represents an image of the set x i \u2208 X. A fundamental question when using graph based approaches is how to effectively extract representative features to construct a robust graph. In order to avoid the sensitive task of hand-crafted feature selection, we rely on embeddings automatically produced by (2) . Hence we reduce the potentially large generalisation error that appear when extracting features on a small training set X L .\nWe consider as feature extractor the function \u03d5 \u03b8 : X \u2192 R P , given by the bottleneck of the current network f \u03b8 , that maps the input to some feature space of dimension P . To construct our graph, we compute, for each sample, the set of descriptors by c i = \u03d5 \u03b8 (x i ) \u2208 R P for x i \u2208 X, and connections are created through the k nearest neighbours (k-NN) approach in R P . An illustration of extracted features is given in Fig. 2 .\nNotice that the model f \u03b8 and the feature extractor \u03d5 \u03b8 are updated (subsection C) through the exposition to pseudo-labels of unlabelled data. The graph thus evolves along our process. We now detail how pseudo-labels are obtained on the graph by following a transductive strategy."}, {"section_title": "B. Label Diffusion as Pseudo-Labelling", "text": "Pseudo-labels are estimated through a diffusion process on the whole graph containing both labeled and unlabelled examples. For each image, our method also assigns a score reflecting the uncertainty of the produced pseudo-label (see subsection C). To do this, we first generate the pseudo-labels through an optimisation model based on the normalised graph p-Dirichlet energy expressed as:\nwhere the degree of the node i is denoted by d i , and the weights are computed from the descriptors described in previous section. The minimisation of this energy allows the diffusion of a labelling variable u. Whilst techniques in this line have been reported in the medical domain e.g. [36] , [37] , [41] and in the pure machine learning community e.g. [42] , [43] , they only seek to use eigenfunctions of a normalised Dirichlet energy based on the graph Laplacian for p = 2 or only approximate p \u2192 1. However, latter machine learning works [44] has demonstrated that using the non smooth p = 1 Dirichlet energy (related to total variation) achieves better performance for label propagation.\nWith this motivation in mind, we introduce an optimisation model based on the normalised and non smooth p = 1 Dirichlet energy. Model (3) is thus defined as \u2206 1 (u) = |W D \u22121 u|, where D is the diagonal matrix containing the degrees d i and the m \u00d7 n matrix W encodes the m edges in the graph. Each of these edges is represented on a different line i of the matrix W , with the value w ij (resp. \u2212w ij ) on the column i (resp. j).\nWe now detail our multi-class model that will be applied to L = 3 classes: 0: Healthy, 1: Pneumonia and 2: COVID-19. For each class k = 1 \u00b7 \u00b7 \u00b7 L, we set a variable u k that contains the node values for class k and denote u = [u 1 , \u00b7 \u00b7 \u00b7 u L ]. For unlabelled nodes i > l, we couple the L variables with the constraint: L k=1 u k i = 0, \u2200i > l. We make the standard assumption that there exists a non empty set of labelled nodes I k \u2282 {1 \u00b7 \u00b7 \u00b7 l} for each class k. For these nodes, we set u k i \u2265 if i \u2208 I k (positive response for the class), and u k i \u2264 \u2212 if i \u2208 I k and k = k (negative output for the other classes).\nUnder such constraints, we seek to minimise the multi-class functional [45] that contains the sum of normalised ratios :\nWe consider an iterative scheme to optimise this problem:\nwhere t is a time index associated to the step \u2206t > 0. This process diffuses information from labelled nodes to unlabelled ones. To avoid trivial solutions [46] , [47] , we apply shifting u k,(t+1) = u k,(t+1) \u2212 median(u k,(t+1) ) and normalisation u (t+1) = u (t+1) /||u (t+1) || steps at the end of each iteration. From (5), one can see that the solution u k,(t+1) satisfies:\nso that we get a reduction of the normalised ratio \u2206 1 (u k,(t) )/|u k,(t) | along iterations k. When L = 1, the scheme u k,(t) converges to a bivalued function that naturally segments the graph [47] . Note that the same behaviour is observed when L > 1, but we can only guarantee the decrease of (6), without considering more computationally expensive schemes [48] . Once u k has converged to some u * = [u * ,1 , \u00b7 \u00b7 \u00b7 , u * ,L ], he label of each node is finally given by y i = argmax j u * ,j i . In practice, our model (5) is solved using an accelerated primal dual algorithm [49] . Our generated pseudo-labels are denoted Y U = { y k } n k=l+1 and used to update the classification network f \u03b8 as explained in the next section. Notice that, unlike [23] where the pseudo-labels are inferred from a given deep net, we compute them from our optimisation model through label diffusion."}, {"section_title": "C. Deep Graph Pseudo-Labelling Update", "text": "Although our graph diffusion model (5) generates relevant pseudo-labels [22] , one can further decrease the uncertainty over time. This can be achieved if we consider the two major bottlenecks in real-world problems. A first prevalent issue in the medical domain is to face highly imbalanced class samples. As illustrated in Fig. 3 , this is particularly true in the COVIDx dataset. The second one is to deal with inferred pseudo-labels with different levels of uncertainty. As it has been shown in several works e.g. [50] - [53] , these problems can be mitigated by weighting the importance over the inferred pseudo-labels and the classes.\nThe problem of classifying with a highly imbalanced dataset has been widely studied in the literature, e.g. [50] , [54] . We apply a common strategy for imbalanced class population [50] , [55] and add a weighting factor inversely proportional to the effective number of samples for class k : \u03c9 k \u221d 1/E n , n \u2208 Z >0 where E n is the total number of samples. For the second problem, we associate an uncertainty weighing factor, \u03c5 i , to each u\u0177 i generated in the diffusion process. We use entropy as the measure for uncertainty, given by \u03c5 i = 1 \u2212 (H(u\u0177 i )/ log(L)), where H refers to the entropy and u\u0177 i is normalised beforehand with respect to the values in u * i . We finally define the main driving optimisation (i.e. estimation of network parameters) as:\nthe loss in (7) is connected with model (1) but unlike the typical consistency loss, for L U , we are using the philosophy of pseudo-labels which are generated by our diffusion model. Let us summarise the overall process. We first optimise (2) for a set of epochs, this serves for extracting the embeddings from the deep net to construct a graph. We then perform (5) to diffuse the small labelled set to the unlabelled data. The output of this process is the generation of pseudo-labels for the unlabelled set. These pseudo-labels are then used to optimise (7) , which in turn updates the model parameters. The whole process (feature extraction, graph update, pseudo label diffusion, network update) is then iterated."}, {"section_title": "IV. EXPERIMENTAL RESULTS", "text": "In this section, we detail the set of experiments conducted to validate our technique."}, {"section_title": "A. Dataset Description", "text": "We evaluate our approach on the COVIDx Dataset which was introduced in [9] . The dataset is composed of a total of 15,254 CXR images. The official partition only considers 13,975 CXR images across 13,870 patients, with a training set composed of 13,675 images and a test test of 300 (1579 for the test set on the full dataset). This dataset is, up to our knowledge, the largest and most diverse one for COVID-19. COVIDx indeed merges five different datasets repositories: COVID-19 image data collection [8] , Actualmed COVID-19 Chest X-ray Dataset Initiative [56] , COVID-19 Chest X-ray Dataset Initiative [57] , RSNA Pneumonia Challenge dataset [24] , [58] and COVID-19 Radiography Database [59] .\nThe COVIDx dataset conatin thress classes: Healthy, Pneumonia and COVID-19. The class breakdown, for the full and official partition CXR images, is illustrated in Fig. 3 . As it can be observed from these plots, it is a highly imbalanced dataset, in which COVID-19 samples are much smaller than for the other other two classes. "}, {"section_title": "B. Evaluation Methodology & Implementation Details", "text": "We validate our proposed technique as follows. First we evaluate the performance of our approach compared to supervised techniques including the leading fully supervised paper in the field COVID-Net [9] . These comparisons include VGG-16 [31] , ResNet-18 and ResNet-50 [29] , InceptionV3 [60] and DenseNet-121 [30] . The selection of these architectures follows the same line of motivation as in [9] : they offer a clear advantage for dealing with the unique traits of COVID. Our experiments are then conducted using: i) the official partition, in which the test set is 300 samples split evenly across the classes; ii) the full COVIDx dataset, in which the main differences with respect i) is that the test set is composed of 1,579 samples (see Fig. 3 ); and iii) an additional random partition. We ran all the experiments under the same conditions, and followed standard pre-processing protocol to normalise the images to have zero mean and unit variance. The images we resized to the resolution 480\u00d7480.\nThe evaluation is addressed from both qualitative and quantitative points of view. The former is based on visual outputs of our classification. The latter present the per class computation of sensitivity, positive predictive value and F1-scores. The overall performance is computed in terms of accuracy and error rate. Furthermore, for sake of completeness and guided by the field of estimation statistics, we report along with the error rate the confidence intervals (95%) of all techniques. Finally, we performed a data ablation study of our SSL method by using 10%, 20% and 30% of labels.\nWe now give implementation details. For the COVID-Net [9] technique, we used the implementation and parameters provided by the authors. In particular, we considered the For the compared techniques, we used weight decay= 5e-4, momentum= 0.9 and learning rate 1e-2 (1e-3 for [23] , and ResNet-18 for a fair comparison). For our technique, the k-NN neighborhood graph has been built with k = 50. A ResNet-18 architecture has been used for the deep network f \u03b8 . In practice, we used a total number of epochs of 210 with and a weight decay of 2 \u00d7 10 \u22124 and learning rate was set to 5e-2 decreasing with cosine annealing. Furthermore, we follow standard protocol in semi-supervised learning to report our results, we randomly select the labelled samples over five repeated times, that isone has five different splits. We then report the mean error over the splits. All techniques were implemented in PyTorch and using Stochastic Gradient Descent (SGD) as optimiser."}, {"section_title": "C. Results & Discussion", "text": "We begin by evaluating the different methods on the official COVIDx partition. As a baseline comparison, we [29] , InceptionV3 [60] , DenseNet-121 [30] and COVID-Net [9] . To the best of our knowledge, there exists no semi-supervised technique dedicated to COVID-19 identification that we can compare. Hence, for the sake of fairness, we also adapted one semi-supervised technique, Pseudo-Labelling [23] , that has a philosophy close to ours. The supervised methods use the full training set, whereas the SSL techniques only consider 30% of the labelled set. We provide a detailed quantitative analysis to understand the performance of the different techniques. The per class metrics across the official data partition are thus reported in Table I . Concerning positive predictive values, we observe that our GraphXCOVID approach performs the best for healthy and pneumonia classes and it readily competes with COVID-Net in the COVID-19 class. Due to design limitations, VGG-16 performed the worst, whereas ResNet-50 presents performances closer to GraphXCOVID thanks to its residual architecture. We also observe that excepting for the COVID-19 class, InceptionV3 and DenseNet-121 both sightly outperform ResNet-50. In order to show the robustness of our technique, we also considered the full COVIDx dataset, a scenario closer to a real medical setting. We compared our method with the supervised approach of COVID-Net, the second better method on the official partition. As reported in Table II , GraphXCOVID here performs better for all classes and all considered metrics, while only using 30% of the labelled set.\nThe second evaluation is done in terms of sensitivity. As shown in tables I and II, GraphXCOVID reports the highest values for pneumonia and COVID-19. For COVID-19, the true positive proportion is significantly higher for our method (0.94) and COVID-Net (0.92) than for other ones (\u22640.88). This observation is confirmed with the sensitivity results on the highly imbalanced full dataset (see Table II ).\nTo give a view of the relative performance of all techniques, the F1-scores are respectively reported in Tables I and II for the official and full partitions. It still underlines that VGG-16, ResNet-50, DenseNet-121 and InceptionV3 are not sufficiently competitive, whereas COVID-Net and our GraphXCOVID technique are readily performing in a similar level. However, looking deeper into the performance of these two last techniques, we observe that within a highly imbalanced scenario (see Table II ), our approach outperforms COVID-Net.\nWe also compare our technique to a SSL technique, with similar philosophy than ours but different in design, Pseudo-Labelling [23] . This technique generates the pseudo-labels directly from the network whilst our approach considers labels coming from the diffusion model. From Table I , one can observe that GRAPHXCOVID offers a substantial improvement over Pseudo-Labelling for all metrics. Overall, we achieve better accuracy with an improvement of 8%, and reduce the error rate (\u00b11.20CI) by more than half as displayed in Fig  4. The improvement comes from two parts. The first major benefit is related to pseudo-labels generation. The work of [23] provides naive pseudo-labels with the network itself, while our technique generates more certain ones that are iteratively updated using both our diffusion models and the network, along with a uncertainty weight. Secondly, our technique also accounts for imbalanced class distribution.\nTo further support the previous results and give a global performance view, we compute the error rate and the confidence intervals for each model. The results are reported in Fig. 4 for the official and a random partition of this set. For both experiments, VGG-16 reported the worst performance followed by ResNet-18. Our model performed the best among all the compared models, reporting an error of 5.4 \u00b1 1.1 at the 95% confidence level. As for the other criteria, COVID-Net ranked second, reporting an error of the model of 6.7\u00b11.23 at the 95% confidence level. One can also observe from the top of Fig. 4 Ours Fig. 6 . Visualisation of results: the probability score for each class and compared with the human diagnostic (GT). We see from the confidence measures, that the model has clearly separated normal x-rays from injected x-rays as well as distinguished between pneumonia and COVID-19.\nchange of partition. Such approaches are indeed heavily reliant on the training set being well-representative and balance. In comparison, the variation is negligible with our SSL approach.\nIn order to analyse the robustness of our model, we run an ablation study for different label counts. In Fig. 5 (top) , we present the error rates and confidence intervals obtained by GraphXCOVID with 10%, 20% and 30% of the available labels. Why do not increase the percentage of labels? First, we want to use the lowest possible number of labels. Secondly, we seek to keep the advantage of transductive inference. Indeed, as pointed out in early works e.g. [61] , the benefit of a transductive model is decreased when a large number of labels is considered. This effect was observed in our experiments. We finally illustrate in Fig. 5 (bottom) , the behaviour of our model along iterations. Around 200 epochs were required to reach a stable error rate when considering 30% of labels.\nA visual illustration of the result is next provided in Fig. 6 , where the probability scores of our technique are reported and compared with the human prediction (GT). One can see that the obtained classifier f \u03b8 easily differentiate between classes. However, the probability scores (from Fig. 6) are not enough to assist the radiologist in making the decision. To accommodate with this issue, we use a Gradient-weighted Class Activation Mapping [62] type solution to highlight abnormal and normal areas in the lungs, in which Pneumonia and COVID-19 are linked to abnormally regions. Samples outputs of the attention maps are displayed in Fig. 7 and compared with the human prediction (GT). The attention maps aims to accommodate with the mental model on how the radiologists work in a clinical scenario. Therefore, we project the attention only in the lungs areas. This tool is designed to help, in a friendly user-interface, the radiologist judging whether the diagnostic is correct or not, and in consequence to accelerate the decision.\nFrom the aforementioned findings, we emphasise a central message: the strong performance when using far less labelled data than the compared techniques is a core strength of our SSL technique. We highlight the value of the vast available unlabelled data in medical domain, and in particular the potentials and benefits for diagnostic COVID-19 disease.\nV. CONCLUSION In this work, we propose a graph-based deep semisupervised framework for classifying COVID-19 Chext X-ray images based on an optimisation model for label diffusion. Through the minimisation of a normalised and non-smooth p = 1 Dirichlet energy, the model generates meaningful pseudo-labels that are iteratively used to update a deep net. To our knowledge, this is the first graph based deep semi-supervised technique for COVID-19 analysis. From our results, we demonstrated that our technique reports higher sensitivity in COVID-19 and better global performances than the current leading deep supervised technique, while requiring a very reduced set of labels. We also provide attention maps as means to visualise the output of our technique. These visualisation aims to assist the radiologist in judging whether the diagnosis is correct. With this work, we encourage a new line of research on deep semi-supervised learning for novel disease prediction, as for COVID-19. Such approaches alleviate the need for a large labelled dataset, which is costly and time consuming to produce especially for emerging pan-demic diseases where both human and monetary resources are stretched thin. ACKNOWLEDGMENTS AIAR gratefully acknowledges support from CMIH and CCIMI, University of Cambridge. PS is supported by EPSRC and NPL. CBS acknowledges support from the Leverhulme Trust project on 'Breaking the non-convexity barrier', the Philip Leverhulme Prize, the Royal Society Wolfson Fellowship, the EPSRC grants EP/S026045/1 and EP/T003553/1, the EPSRC Centre EP/N014588/1, the Wellcome Innovator Award RG98755, European Union Horizon 2020 research and innovation programmes under the Marie Skodowska-Curie grant agreement 777826 NoMADS and 691070 CHiPS, the CCIMI and the Alan Turing Institute. AIAR and CBS also thank the team of the project 'AI assisted diagnosis and prognostication in Covid-19' for very helpful discussions. NP acknowledges H2020 RISE project NoMADS."}]