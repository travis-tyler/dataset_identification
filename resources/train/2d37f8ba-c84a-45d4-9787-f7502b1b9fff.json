[{"section_title": "Abstract", "text": "Graph-based transductive learning (GTL) is a powerful machine learning technique that is used when sufficient training data is not available. In particular, conventional GTL approaches first construct a fixed inter-subject relation graph that is based on similarities in voxel intensity values in the feature domain, which can then be used to propagate the known phenotype data ( i.e. , clinical scores and labels) from the training data to the testing data in the label domain. However, this type of graph is exclusively learned in the feature domain, and primarily due to outliers in the observed features, may not be optimal for label propagation in the label domain. To address this limitation, a progressive GTL (pGTL) method is proposed that gradually finds an intrinsic data representation that more accurately aligns imaging features with the phenotype data. In general, optimal feature-to-phenotype alignment is achieved using an iterative approach that: (1) refines inter-subject relationships observed in the feature domain by using the learned intrinsic data representation in the label domain, (2) updates the intrinsic data representation from the refined inter-subject relationships, and (3) verifies the intrinsic data representation on the training data to guarantee an optimal classification when applied to testing data. Additionally, the iterative approach is extended to multi-modal imaging data to further improve pGTL classification accuracy. Using Alzheimer's disease and Parkinson's disease study data, the classification accuracy of the proposed pGTL method is compared to several state-of-the-art classification methods, and the results show pGTL can more accurately identify subjects, even at different progression stages, in these two study data sets."}, {"section_title": "Introduction", "text": "In the elderly population, neurodegenerative diseases, such as Alzheimer's diseases (AD) and Parkinson's disease (PD), are the most common types of neurological disorders. Because of the progressive nature of these disorders, memory and other mental functions gradually worsen over time, which eventually affects the patients' quality of life ( Group, 2004; Reisberg et al., 2008; Thompson et al., 2007 ) . Unfortunately, there is no cure for these neurodegenerative diseases, although treatments include medications and management strategies may improve the quality of life. Therefore, timely and accurate diagnosis of neurodegenerative diseases and its prodromal stage, i.e. , mild cognitive impairment (MCI) for AD, is highly desired in practice. MCI stage can be further categorized into progressive MCI (pMCI) and stable MCI (sMCI). Since an overwhelming amount of literature exits ( Mueller et al., 2005; Ohtsuka et al., 2013 ) that relate neurodegenerative impairments to morphological abnormalities in the brain, MRI studies that reveal the structural abnormalities of the brain, or PET and SPECT studies that reveal the functional abnormalities of the brain have been widely used. Furthermore, methods that combine structural and functional neuroimaging data have been used to guide computer aided diagnosis techniques ( Long et al., 2012; Ohtsuka et al., 2013; Prashanth et al., 2014; Rana et al., 2014; Salvatore et al., 2014; Weiner et al., 2013 ) . More specifically, a technique called OPLS (orthogonal partial least squares to latent structures) is used to distinguish subjects with AD and MCI from healthy controls by combing MRI and CSF data ( Westman et al., 2012 ) . Joint feature and sample selection http://dx.doi.org/10.1016/j.media.2017.05.003 1361-8415/\u00a9 2017 Elsevier B.V. All rights reserved. methods based on SVM classification model for classification of AD and PD related diseases are also proposed in An et al., 2016 ) . Other machine learning methods, such as kernel learning methods Peng et al., 2016 ) , subspace learning methods ( Hu et al., 2016; Zhu et al., 2016 ) , random forest ( Gray et al., 2013b ) , deep learning ( Liu et al., 2015a ) and graph fusion ( Tong et al., 2015; Wang et al., 2014a ) , have also been used to guide the classification of neurodegenerative diseases. However, morphological abnormalities are often subtle when compared to the high inter-subject variations ( Zhu et al., 2013 ) . Hence, sophisticated pattern recognition methods are of high demand to accurately identify individuals at different stages of neurodegenerative disease.\nOn the other hand, medical imaging applications also have various challenges that are related to high feature dimensionality, large data heterogeneity, and the small number of samples with groundtruth labels ( e.g. , diagnosis scores). Furthermore, even if a large number of labeled samples exist, it is very difficult to identify a computational model that will work well with the entire set of data due to large inter-subject variations across individuals. Transductive learning is a semi-supervised learning (SSL) method, which is recently emerged in the machine learning domain, introducing a strategy halfway between supervised and unsupervised learning schemes to improve classification performance by exploring the relationship between both labeled and unlabeled samples ( Adeli-Mosabbeb and Fathy, 2015; Joachims, 2003; Zhou and Burges, 2007; Zhu et al., 2005 ) . Here, the labeled samples are used to guide the transductive learning, while the unlabeled samples are used to maintain the intrinsic geometric structure of the observed samples. In particular, the graph-based SSL takes advantage of computational efficiency and representational ease for the medical imaging data. Because of the graph structures, it is more efficient to integrate different types of data for better explanations of the clinical outcomes ( Kim et al., 2013 ) . Since graph is usually used to describe the data manifold, most of the proposed transductive learning methods fall to the category of graph-based transductive learning ( Blum and Chawla, 2001; Zhou et al., 2004; Zhu et al., 2005 ) .\nGraph-based transductive learning is widely used in image retrieval, image segmentation, data clustering and classification Liu and Chang, 2009; Wang et al., 2014a; Zhang et al., 2015 ) . For example, a fast and robust graph-based transductive learning method was proposed in ( Zhang et al., 2015 ) by using a minimum tree cut, which was designed for largescale web-spam detection and interactive image segmentation. Also, graph-based transductive learning methods have been investigated with great success in medical imaging area Kim et al., 2013; Tong et al., 2015 ) , since it can overcome the above difficulties by taking advantage of the data representation on unlabeled testing subjects. In the current state-of-the-art methods, each subject, regardless of being labeled or unlabeled, is often treated as a graph node. Then two subjects are connected by an edge in the graph if they both show similar morphological patterns. Using these connections, the labels can be propagated throughout the graph until all latent labels are determined. Typically, there are two separate steps in graph-based transductive learning methods: ( 1 ) construct the graph, where the vertices represent the labeled and unlabeled samples and the edges reflect the similarity degree between two connected samples ( Zhu et al., 2005 ) ; and ( 2 ) propagate labels from labeled samples to unlabeled samples. Many current label propagation strategies have been proposed to determine the latent labels of testing subjects based on the inter-subject relationships encoded in the graph ( Wang and Tsotsos, 2016; Zhang et al., 2015 ) .\nThe basic assumption of current methods is that the graph constructed in the observed feature domain represents the real data distribution and can be transferred to guide label propagation. However, this assumption usually does not hold, since the distribution of examples in the feature space does not necessarily cluster into groups as defined by the clinical scores and labels ( Braak and Braak, 1995 ) . Although the clinical scores and labels are different, they are highly correlated since the diagnosis is drawn upon the clinical score. Meanwhile, we believe the intrinsic data representation should be close or reflect the characteristic of the clinical score. Due to lack of ground truth, the underlying clinical score distribution used to validate the learned intrinsic data representation. As an example, Fig. 1 (a) shows the affinity matrix of 51 AD and 52 NC subjects using the ROI-based features extracted from each MR image, where red dots and blue dots denote the high and low inter-subject similarities, respectively. Since the clinical data ( e.g. , MMSE and CDR scores ( Thompson et al., 2007 ) ) are more relevant with clinical labels, we use these clinical scores to construct another affinity matrix, as shown in Fig. 1 (c) . It is apparent that the data representations using imaging features and clinical scores are completely different. Thus, it is not guaranteed that the learned graph from the affinity matrix in Fig. 1 (a) can effectively guide the classification of AD and NC subjects. More critically, the affinity matrix using observed image features is not even necessarily optimal in the feature domain, due to possible imaging noises and outlier subjects. In the literature, many studies have taken advantage of multi-modal information to improve discrimination power of transductive learning. However, the graphs from different modalities might also be different, as shown in the affinity matrices using structural image features from MR images ( Fig. 1 (a) ) and functional image features from PET images ( Fig. 1 (b) ). Although recent graph diffusion technique ( Wang et al., 2014a ) is effective in finding a common graph from multiple graphs, as shown in Fig. 1 , it is hard to find a combination for the graphs in Figs. 1 (a) and (b) that can be similar to the graph in Fig. 1 (c) , which is more related with the final classification task.\nTo solve these issues, we propose a progressive graph-based transductive learning method to learn the intrinsic data representation for optimal label propagation. Specifically, the intrinsic data representation should be (a) in consensus with inter-subject relationships constructed by imaging features extracted from different modalities, (b) aligned with the clinical labels or scores, and (c) verified on the training data for label propagation. To that end, we simultaneously ( 1 ) refine the data representation (inter-subject graph) in the feature domain, ( 2 ) find the intrinsic data representation based on the constructed graphs on both multi-modal imaging data and the clinical labels of entire subject set (including known labels on training subjects and the tentatively-determined labels on testing subjects), and ( 3 ) propagate the clinical labels from training subjects to testing subjects, following the latest learned intrinsic data representation. Promising results have been achieved in identify subjects with neurodegenerative disease on two neurodegenerative databases ( i.e. , Alzheimer's disease (AD) and Parkinson's disease (PD)), each with two modality images (such as MR and PET/SPECT).\nThe rest of this paper is organized as follows. Section 2 presents our proposed progressive graph-based transductive learning method. After that, we apply the method to the two real brain neurodegenerative imaging databases (ADNI and PPMI datasets 1 ), and present the comparison results to validate the advantages of our method in Section 3 . Finally, we conclude our method in Section 4 . "}, {"section_title": "Method", "text": "Suppose we have N subjects { I 1 , . . . , I P , I P+1 , . . . , I N } , which sequentially consist of P training subjects and Q ( Q = N \u2212 P ) testing subjects. For P training subjects, the clinical labels\n.. ,P are known, where each f p \u2208 [0, 1] C is a binary coding vector indicating the clinical label from C classes. Our goal is to jointly determine the latent labels for Q testing subjects based on a set of their continuous likelihood vectors\n.. ,N , where each element in the vector f q indicates the likelihood of the q -th subject belonging to one of C classes. For convenience, we concatenate F P and F Q into a single label matrix"}, {"section_title": "Graph-based transductive learning on single-modal imaging data", "text": "Graph-based transductive learning learns over both labeled and unlabeled samples, aiming to harness the structure of entire data representation to improve the prediction of the latent labels. For clarity, we first extract single-modality image features from each subject I i ( i = 1 , . . . , N), denoted as x i . Using measurement from each modality, a graph G = ( V, E ) can be constructed to model the relations among the N subjects, where the nodes V correspond to N subjects and the edges E are weighted by the similarities between linked subjects. In the conventional graph-based transductive learning methods, the inter-subject relationships are computed based on feature similarity, which is encoded in an N \u00d7 N feature affinity matrix A . Each element a ij ( a ij \u2265 0, i, j = 1 , . . . , N) in A represents the feature affinity degree between x i and x j . Therefore, the graph construction can be divided into two steps: graph topology definition and edge weight computation.\nFor the graph topology definition, current methods can be classified into two categories ( de Sousa et al., 2013; Zhu et al., 2005 ) : 1) Using the fully-connected graph. A fully-connected graph is created with edges between all pairs of nodes. Similar nodes have larger edge weights between them. In these methods, usually the weights of a fully-connected graph can be simply learned, but the computational cost is relatively high. 2) Using sparse graph. The k -nearest neighbor ( k NN) graph and -neighborhood ( NN) graph are both the sparse graphs, in which each node connects to only a few nodes. Sparse graphs are computationally fast and can often provide good empirical performance. However, the neighborhood relationship changes with the change of hyperparameters. Hence, for the sake of generalizability, the k NN graphs are constructed for each modality in this paper.\nThe most direct way to compute the weight matrix A (by defining the edge weight between each pair of nodes) is based on a given similarity measure; in practice, it generally redefines the weight matrix A by using different measures for better interpretability. Binary weighting is the simplest method for assigning edge weights, which is to set A = E directly (where E is a binary matrix, indicating if there is an edge between each pair of the nodes). Obviously, such a scheme cannot provide any extra information beyond the graph topology. RBF (Gaussian) kernel is one of the most common methods to assign edge weights for a graph. RBF kernel computes the similarity between x i and x j by\nis a pair-wise similarity measure. For instance, pair-wise Euclidean distance can be used\nhere. In addition, \u03c3 is a scale parameter ( de Sousa et al., 2013; Zhu et al., 2005 ) . In practice, one can employ any meaningful measure for defining edge weights, such as mutual information that has been successfully applied to brain and gene network modeling or detecting non-linear relationships ( Plis et al., 2014 ) . Additionally, some post-processing and optimization processes can also be used to efficiently weight the edges. These processes are often referred to as graph learning methods ( Nie et al., 2014; Wang et al., 2014a ) . Without loss of generality, we select RBF kernel as the similarity measure to define the pair-wise affinity degree a ij as:\nwhere \u03c3 is the scale controlling the exponential penalty strength of Euclidian distance between x i and x j . Based on the affinity matrix A , conventional methods determine the latent label for each testing subject I q by solving a classic graph learning problem ( Golub and Van Loan, 2012; Nocedal and Wright, 2006 ) :\nAs shown in Fig. 1 , the affinity matrix A might not be closely related with the intrinsic data representation in the label domain. Therefore, it is necessary to find a hidden data representation which aligns with the clinical labels, rather than solely using the affinity matrix constructed based on imaging features. However, initially, labels on the testing subjects are not determined yet. In order to solve this chicken-and-egg dilemma, we propose to iteratively optimize the data representation of each observed imaging data and align the refined imaging data representations to a common space for reflecting the intrinsic data representation of phenotype data ."}, {"section_title": "Progressive graph-based transductive learning", "text": "Instead of relying on the affinity matrix A , we propose to find an intrinsic data representation T = [ t i j ] i, j=1 , ... ,N which is more relevant than using affinity matrix A to guide the label propagation in Eq. (2) . Therefore, the problem of determining the latent label for each testing subject I q becomes: arg min\nwhere t i j ( t i j \u2265 0 , i, j = 1 , . . . , N ) denotes the latent intrinsic intersubject relationship between subject I i and I j . Since the clinical labels on the testing subjects are unknown, joint optimization of latent clinical label F q and hidden intrinsic data representation T in Eq. 3 is an ill-posed problem. In order to turn the energy function to a well-posed problem, we require that the latent intrinsic data representation T should respect the affinity matrix A as follows:\narg min\nwhere a ij is computed by Eq. 1 . \u03bb is the parameter controlling the influence of affinity matrix A on the estimation of T (intrinsic data representation). Since the affinity degree a ij is computed based on the observed imaging data x i and x j , possible noisy/outlier features could bring a series of unrealistic feature similarities. In order to suppress the influence of noisy/outlier imaging features, we propose to estimate the optimal imaging data representation S = [ s i j ] N\u00d7N based on the observed imaging features, where the regularization term is enforced on s ij : arg min\nwhere \u03b7 is the scalar controlling the strength of regularization term. Although the optimization of inter-subject relationship s ij (in Eq. 5 ) and the calculation of affinity value a ij (in Eq. 1 ) are both driven by the imaging features, the optimized inter-subject relationship s ij is more robust than a ij to the deteriorated imaging features. Hence, the edge weights are learned in the optimization process. By replacing the affinity degree a ij with the optimal intersubject relationship s ij , we jointly optimize the intrinsic data representation T , imaging data representation S , and the latent clinical label F Q in the following energy function: arg min\nwhere \u03bc is the scalar balancing the data fitting terms from two different domains ( i.e. , the first and second terms in Eq. (6) ). Note, s ii and t ii are required to be 0. The sum of inter-subject similarity degree of subject I i to all other subjects equals to 1, i.e. , s i 1 = 1 and t i 1 = 1 , where s i and t i denote for the i -th column vectors of matrices S and T , respectively."}, {"section_title": "Progressive graph-based transductive learning on multi-modal imaging data", "text": "Recently, multi-modal neuroimaging data become more and more popular. For example, ADNI dataset provides a wide spectrum of neuroimaging data, which includes MR images and PET images. In order to improve the classification accuracy, we go one step further to extend our progressive graph-based transductive learning by fully using the complementary information in multimodal data.\nSuppose we have M modalities. For each subject I i , we can extract multi-modal image features\nFor the m -th modality, we can optimize the imaging data representation S m of\nAs shown in Figs. 1 (a) and (b), the data representations across different modalities could be different. Thus, we require the intrinsic data representation T to be close to all S m . To that end, we extend our above pGTL method from the singlemodal to the multi-modal scenario:\nThe intuition behind Eq. 7 is that the label propagation is steered by the hidden intrinsic data representation T. The criteria for obtaining reasonable estimation of T are: (1) T should be close to all imaging data representations S m estimated from the observed imaging features { x m i } (as shown in the last term in Eq. (7) ), which eventually makes T act as a common space for S 1 , . . . , S M ; and (2) the label propagation results should be in consensus with the labels on the known subjects (the first term in Eq. (7) ) such that the intrinsic data representation is essentially aligned with the phenotype data. It is apparent that our energy function describes a highly dynamic system since the variables are all correlated to each other. In the following, we give the optimization solution to Eq. 7 , which falls into a divide-and-conquer scenario."}, {"section_title": "Optimization", "text": "Fortunately, our proposed energy function in Eq. (7) is convex with respect to each of the variables S m , T , and F . Thus, we can alternatively optimize one set of variables at a time by fixing other sets of variables. The optimization for each sub-problem is detailed below."}, {"section_title": "Estimation of imaging data representation S m for each modality", "text": "Removing the unrelated terms w.r.t . S m in Eq. (7) , the optimization of S m falls to the following objective function: arg min\nSince Eq. (8) is independent of variables i and j , we further reformulate Eq. (8) in the vector form as below:\nwhere , we can obtain the imaging data representation matrix S m ."}, {"section_title": "Estimation of intrinsic data representation T", "text": "Fixing S m and F , the objective function w.r.t. T reduces to:\narg min\nSimilarly, we can reformulate Eq. (10) by solving each t i at a time: arg min\nwhere\n, and r 2 = M \u03bb is a scalar. Similar to the solution for Eq. (9) , the problem in Eq. (11) can also be solved using a closed-form solution. After solving each t i , we can obtain the affinity matrix T . "}, {"section_title": "Updating of latent labels F Q on testing subjects", "text": "Given both S w and T , the objective function for the latent label F Q can be derived from Eq. (3) as below:\nEq. (12) is equal to the following problem:\nwhere trace (.) denotes the matrix trace operator. L = diag(T ) \u2212 ( T + T ) / 2 is the Laplacian matrix of T ( diag ( T ) denotes for the diagnial matrix of T ). F P is with known clinical labels. By differentiating Eq. (13) w.r.t . F and letting the gradient equal to zero such as LF = 0 , we can obtain the following equation:\nwhere L PP , L PQ , L QP , and L QQ denote the top-left, top-right, bottomleft, and bottom-right blocks of L . The solution for F Q can be obtained by \u02c6\nThe solution to the optimization problem in Eq. (7) is briefly summarized as follows.\nAlgorithm 1: Progressive transductive learning on multi-modal imaging data.\nOutput: Predicted labels of unlabeled data F Q \u2208 R Q \u00d7 C . Compute the Euclidean distance between samples in each modality; Initialize S m using the affinity matrix A m , and initialize T by letting (2) Estimate the intrinsic data representation T , which requires the estimations of both S 1 and S 2 and also the known clinical labels in the label domain (purple arrows); (3) Update the latent labels F Q on the testing subjects which needs guidance from the learned intrinsic data representation T (blue arrows). It is apparent that the intrinsic data representation T links the feature domain and label domain, which eventually leads to the dynamic graph learning model."}, {"section_title": "Experiments", "text": "\u2022 SVM: Support Vector Machine is a parametrically kernel-based supervised learning method, which maps the data into a higher dimensional input space and constructs an optimal separating hyperplane in this space. In our experiment, we use linear kernel.\n\u2022 S4VM: Safe Semi-Supervised Support Vector Machine is a semisupervised learning approach that does not significantly reduce learning performance when unlabeled data are used. This method uses multiple low-density separators to approximate the ground-truth decision boundary and maximizes the improvement in performance of inductive SVMs for any candidate separator. S4VM is semi-supervised learning method that guar- antees the performance improvement using unlabeled data will be maximized ( Li and Zhou, 2015 ) .\n\u2022 wellSVM: wellSVM is a semi-supervised method via a novel label generation strategy ( Li et al., 2013 ) . It is focused on the problem of learning from weakly labeled data, where labels of the training examples are incomplete. This method assumes different weakly labeled scenarios; including (i) semi-supervised learning, where labels are partially known; (ii) multi-instance learning, where labels are implicitly known; and (iii) clustering, in which labels are completely unknown. In this paper we use the first case, i.e. semi-supervised learning, to compare with our proposed method.\n\u2022 JCR: This sparse joint classification and regression method utilizes the sparse regularization to perform imaging biomarker selection and learn a sparse matrix under a unified framework that integrates both heterogeneous and homogenous tasks . In this paper we obtain the classification results using this unified framework that integrate both label and clinical score information.\n\u2022 CCA-SVM: Canonical correlation analysis is used to find the mappings for aligning two distributions of sets of multivariate variables (vectors), which makes the correlation between the projected variables to be mutually maximized ( Thompson, 2005 ) after mapping. Then, we train the SVM classifier based on the projected features.\n\u2022 MK-SVM: Multi-Kernel SVM method adequately utilizes the particular characteristic of each source and provides more possibility to choose suitable kernels or their weighted combination especially for the data from multiple heterogeneous sources ( G\u00f6nen and Alpayd\u0131n, 2011 ) . Each input has a kernel, and in this work a combination kernel ( i.e. , weighted sum of all kernels) is used to classify.\n\u2022 GTL: Graph-based transductive learning method is a semisupervised learning method. The affinity matrix is constructed only in the feature domain but fixed in label propagation. In this experiment, we use the code of classic graph-based learning method in ( Zhu et al., 2003 ) .\nFor single-modality case, we only compare our proposed pGTL method with SVM method and GTL methods. For multiplemodality case, SVM, S4VM, wellSVM, JCR, CCA-SVM, MK-SVM, and GTL methods are used to compare with our pGTL method. Specifically, we apply SVM, S4VM, wellSVM, JCR, GTL methods to multimodal imaging data by concatenating the feature vectors from all modalities into a single feature vector. Specifically, we follow a 10-fold crossvalidation strategy, in which for each testing fold, the nine other folds are used to train the models. This is repeated for all ten existing folds and the performance score are averaged over these ten runs to illustrate reliable non-over-fitted results. In order to narrow down the factors affecting the classification performance, no feature selection step is included. Parameter settings. For all competing methods, the best parameters are selected through an inner 5-fold cross-validation on the training data using a grid-search strategy. The important parameters (along with their explanations and the respective ranges) used in each classification method are summarized in Table 1 ."}, {"section_title": "Experiments setting", "text": ""}, {"section_title": "Experimental results on Alzheimer's disease", "text": ""}, {"section_title": "Subjects and image preprocessing", "text": "In this study, we consider subjects with both MRI and PET modalities available in the ANDI database. As a result, we have 93 AD subjects, 202 MCI subjects, and 101 NC subjects. Specially, 55 pMCI subjects (who converted from MCI to AD in last 36 months) and63 sMCI subjects (who didn't not convert to AD in both 24 months and 36 months) are included in pMCI vs sMCI classification. Each subject has both MR and 18-Fluoro-DeoxyGlucose PET (FDG-PET) images. The demographics of the subjects are detailed in Table 2 .\nFor each subject, we first align the PET image to the MR image space. Then we remove both the skull and cerebellum from MR image, and segment MR image into white matter, gray matter and cerebrospinal fluid ( Wang et al., 2014b; Zhang et al., 2011 ) . Next, we parcellate each subject image into 93 ROIs (Regions of Interest) by registering the template (with manual annotation of 93 ROIs) to the subject image domain. Of note, these 93 ROIs cover important cortical and sub-cortical regions in human brain. Finally, the gray matter volume and the mean PET intensity in each ROI are used and form a 186-dimensional feature vector."}, {"section_title": "Experimental results of classification performance", "text": "The classification performance by SVM, S4VM, wellSVM, JCR, CCA-SVM, MK-SVM, GTL and our proposed method are evaluated in three classification tasks (AD vs NC, MCI vs NC, and pMCI vs sMCI), respectively. Each task is conducted in both single-modal (MRI or PET) and multi-modal (MRI and PET) scenarios separately. Our proposed pGTL method achieves better classification performance compared to the other counterpart methods. Specifically, Table 3 shows the classification performance of the competing methods in the classification AD and NC. Our proposed pGTL method shows the best classification accuracies of 88.6%, 87.3% and 92.6% by using MRI, PET and (MRI + PET), respectively. Moreover, the performance improvements of classification accuracy over the second best counterpart method are 1.8% when using MRI only, 0.3% when using PET only, and 2.1% when using MRI + PET, respectively. Similarly, our proposed method achieves the best classification accuracy in MCI vs NC and pMCI vs sMCI tasks, as shown in Table 4 and Table 5 , respectively.\nThe comparisons with recently published state-of-the-art methods are reported in Table 6 . It summarizes the subject informa- tion, imaging modality, and average classification accuracy by using state-of-the-art methods. These comparison methods represent various machine learning techniques. Since the classification are not reported between pMCI and sMCI groups in ( Gray et al., 2013b; Liu et al., 2015c; Peng et al., 2016; Tong et al., 2015 ) , between MCI and NC groups in ( Trzepacz et al., 2014 ) , we do not include the classification results and use '--' instead. Our method achieves higher classification accuracy than both random forest and graph fusion methods, even though those two methods use additional CSF and genetic information. Deep learning approach in ( Liu et al., 2015b ) learns feature representation in a layer-by-layer manner. Thus, it is time consuming to re-train the deep neural-network from scratch. Instead, our proposed method only uses handcrafted features for classification. It is noteworthy that we can complete the classification on a new dataset (including grid-search for parameter tuning) within three hours on a regular PC (8 CPU cores and 16GB memory), which is much more economic than massive training cost in ( Liu et al., 2015b ) . Complementary information in multi-modal data can help improve the classification performance; therefore, in order to find the intrinsic data representation, we combine our proposed pGTL with multi-modal information.\nBesides, we also evaluate the classification performance w.r.t . the number of training samples using AD vs . NC classification as example, as shown in Fig. 3 . It is clear that (1) our proposed method always has higher classification accuracy than MK-SVM methods; and (2) all methods can improve the classification accuracy as the number of training samples increases. It is worth noting that our proposed method achieves large improvement against MK-SVM, when only 33% of data is used as training samples. The ( Trzepacz et al., 2014 ) 20pMCI + 30sMCI MRI + PET ----76 HMFSS 165AD + 342MCI + 195NC MRI + SNP 90 .8 77 .6 78 .3 Kernel learning ( Peng et al., 2016 ) 49AD + 93MCI + 47NC MRI + PET 92 .3 76 .4 --Feature-trans ( Zhu et al., 2015 ) 198AD + 403MCI + 229NC MRI-HOG + MRI-ROI 89 .9 75 .2 72 .1 Random forest ( Gray et al., 2013a ) 37AD + 75MCI + 35NC MRI + PET + CSF + Genetic 89 .0 74 .6 --Graph fusion ( Tong et al., 2015 ) 35AD + 75MCI + 77NC MRI + PET + CSF + Genetic 91 .8 79 .5 --Deep learning ( Liu et al., 2015b ) 85AD + 169MCI + 77NC MRI + PET 91 .4 82 .1 --Our method 99AD + 202MCI + 101NC MRI + PET 92 .6 78 .6 76 .7 reason is that the supervised methods require a sufficiently large number of samples (with labels) for training a robust classifier. Otherwise, the classification performance decreases rapidly. On the contrary, our proposed p GTL method can alleviate this issue by leveraging the data distribution of both labeled and unlabeled data. Since the training samples with known labels are expensive to collect in medical imaging area, this experiment indicates the potential of our method in current neuroimaging studies.\nTo illustrate the representation of our method, confusion matrix is also introduced. Confusion matrix, also known as error matrix, is a specific table layout that allows visualization of the performance of an algorithm ( Hay, 1988 ) . In confusion matrix, each column of the matrix represents the instances in a predicted class while each row represents the instances in an actual class. The use of confusion matrix makes it easy to see if the system is confusing two classes.\nWe randomly select 165 subjects out of 369 PD subjects to evaluate the classification performance with another 165 NC subjects. This is used to make the data balanced. Moreover, to prevent any unintended bias in the results, the process of random selection is repeated 5 times, and the average value of the 5 times of reputation is used as the final result as shown in Fig. 5 .\nIn the single-modal MR image based classification of PD and NC subjects, the proposed method achieves the accuracy of 68.0%. Compared to other competing methods (S4VM, JCR, wellSVM, SVM and GTL) that achieve the accuracies of 58.0%, 58.8%, 58.4%, 58.5% and 62.2%, respectively, our proposed method improved by 10% over S4VM. For the case of using only SPECT images, the improvements of classification accuracy achieved by our pGTL method are less significant over other two methods (such as 95.4% by S4VM,94.2% by JCR, 95.3% by wellSVM, 94.9% by SVM, 95.9% by GTL, and 96.6% by our pGTL), due to the high sensitivity of features from SPECT images. In multi-modal (MRI + SPECT) classification scenario, the overall classification accuracies are 92.9% by S4VM, 82.2% by JCR, 87.2% by wellSVM, 88.5% by SVM, 90.3% by CCA-SVM, 94.2% by MK-SVM, 85.1% by GTL, and 97.4% by our proposed p GTL method. It is apparent that our proposed p GTL method has achieved the highest classification performance in both singleand multi-modal classification scenarios. Confusion matrix about classification performance for PD vs NC is showed in Fig. 4 (d) .\nSince the SPECT image provides only four features, high-sensitivity morphological patterns are nominated by the overwhelming lessdiscriminative imaging features from MRI. Thus, the overall classification accuracy of the competing methods (except pGTL) using both MRI and SPECT data are lower than only using SPECT data, indicating high importance of using the state-of-the-art multi-modal classification method to combine the powers of different modalities. It is noteworthy that, although our proposed method does not learn the weights for different modalities, the learning process of finding the intrinsic data representation can adaptively adjust the effect of different modalities. On the other hand, CCA-SVM and MK-SVM methods can find either maximum correlation or suitable weighted kernel between different imaging modalities, thus improving the classification accuracies up to 90.3% by CCA-SVM and 94.2% by MK-SVM, respectively. Compared to CCA-SVM and MK-SVM, our proposed pGTL method uses data representation of unlabeled samples to guide the classification in a semisupervised manner, which is very effective in alleviating the issue of small sample size. Thus, our proposed pGTL method can achieve the highest classification accuracy in classifying PD and NC by using both MRI and SPECT data."}, {"section_title": "Experimental results on Parkinson's disease", "text": ""}, {"section_title": "Subject information and image preprocessing", "text": "Recently, a major initiative, the Parkinson Progression Marker Initiative (PPMI) ( PPMI, 2011 ), was developed to identify and validate PD progression markers. Abundant imaging data from the enrolled PD subjects at the earliest detectable stage of disease significantly enhances the potential to both identify PD imaging markers and develop computer-assisted diagnosis system for neuroprotective interventions ( Beitz, 2014; Jankovic, 2008; Stern and Siderowf, 2010 ) . PD subjects in the PPMI study are just diagnosed and unmediated. The healthy/normal control subjects are both age-and gender-matched with the PD patients. In this research, we use 369 PD and 165 NC subjects, each with both MRI and SPECT modalities.\nFor MR images, a T1-weighted, 3D sequence ( e.g. , MPRAGE or SPGR) is acquired for each subject using 3T SIEMENS MAGNETON Trio Tim syngo scanners. The T1-weighted images were acquired for 176 sagittal slices with the following parameters: repetition time = 2300 ms, echo time = 2.98 ms, flip angle = 9 \u00b0, and voxel size = 1 \u00d7 1 \u00d7 1 mm 3 . All the MR images were preprocessed by skull stripping ( Wang et al., 2014b ), cerebellum removal, and then segmented into white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF) tissues ( Lim and Pfefferbaum, 1989 ) . The AAL atlas ( Tzourio-Mazoyer et al., 2002 ) , parcellated with 90 predefined regions of interest (ROI), was registered using HAMMER ( Shen and Davatzikos, 2002 ) to each subject's native space. We further added 8 more ROIs to the atlas in the basal ganglia and brainstem regions, which are clinically important ROIs for PD. These 8 ROIs are 'superior cerebellar peduncle', 'midbrain', 'pons' and 'medulla oblongata' in the brainstem, along with 'substantia nigra' (left and right) and 'red nucleus' (left and right). We then computed WM, GM and CSF tissue volumes in each of these 98 ROIs as features. To acquire SPECT images, the 123 I-ilflupane neuroimaging radiopharmaceutical biomarker was injected, which binds to the dopamine transporters in the striatum. Brain images were then acquired. To process these images, the PPMI study has performed attenuation correction on the SPECT images, along with a standard 3D 6.0 mm Gaussian filter. Then, the images were normalized to standard Montreal Neurological Institute (MNI) space. Next, the transaxial slice with the highest striatal uptake was identified and the 8 hottest striatal slices around this slice were averaged, to generate a single slice image. On the averaged slice, the four caudate and putamen (left and right) ROIs, which are in the striatum brain region, were labeled and considered as target ROIs. The occipital cortex region was also segmented and used as a reference ROI. Count densities for the regions were used to calculate the striatal blinding ratios (SBRs), which were used as morphological signatures for SPECT images."}, {"section_title": "Discussion", "text": "Feature extraction and data representation are always the very important steps in many classification tasks. Specifically, in medical imaging applications, deficiency in the imaging devices will be reflected as noisy or redundant features for the latter processes, which will reduce the overall learning performance of the classification system. Feature selection aims to choose a small subset of the relevant features from the original ones according to certain relevance evaluation criteria. This usually leads to better performance, lower computational cost and better model interpretability ( Tang et al., 2014 ) . One possible strategy is to integrate the classic feature selection and our graph-based transductive classification, where the input to our method will be the optimized features, instead of features extracted from the whole brain. To verify the effectiveness of our proposed method, we use the selected features from MRI reported in , and then combine them with the features from SPECT for obtaining even better performance (ACC: 97.5%). Furthermore , we can simultaneously select the best features and also learn the data representation by intro- ducing an additional variable for measuring the importance of each observed feature. However, in this paper, we focused mainly on the graph-learning strategy, since feature selection schemes have been widely explored in the literature. It is important to note that our proposed method can learn the importance of each feature, through looking into the graph weights and regularizing the optimization objective to enforce the selection of a compact set of features. This is a direction for our future work.\nLastly, biomarkers from different modalities provide complementary information, which is very useful for neurodegenerative disease diagnosis. However, it is clear that different modalities should be weighted differently. For example, the imaging features from SPECT in Section 3.3 have high-sensitivity morphological patterns; when SPECT features are weighted equally with lessdiscriminative imaging features from MRI, the classification performance of multi-modalities will be reduced, which can be seen in Fig. 4 . In our method, we can adaptively learn a weight for each graph during the optimization. However, this will lead to some additional parameters to optimize in our proposed method. Hence, in the current implementation, we treat each imaging modality equally. In the future, we will try to adopt a strategy similar to Auto-weighted Multiple Graph Learning (AMGL) framework in our method to learn a set of weights automatically for all the graphs. This process will not need any additional parameters ( Nie et al., 2016 ) ."}, {"section_title": "Conclusion", "text": "Here we presented a novel pGTL method that can accurately identify the different neurodegenerative stages for wide range of subjects, when applied to multi-modal imaging data. Compared to the conventional methods, the proposed method seeks to identify an intrinsic data representation that is simultaneously learned from the observed imaging features while also being validated on the training data with known phenotype labels. Since the learned intrinsic data presentation is more relevant to phenotype label propagation, the pGTL approach has shown promising results when performing AD vs NC, MCI vs NC, pMCI vs sMCI, and PD vs NC classification tasks when compared to several the state-of-the-art supervised and semi-supervised machine learning methods."}, {"section_title": "Acknowledgments", "text": "Parts of the data used in preparation of this article were obtained from the Alzheimer's disease Neuroimaging Initiative (ADNI) database ( http://adni.loni.ucla.edu ) and Parkinson's Progressive Markers initiative (PPMI) database ( http://www. ppmi-info.org ). The investigators within the ADNI and PPMI contributed to the design and implementation of ADNI and PPMI and/or provided data but did not participate in analysis or writing this paper. A complete listing of ADNI investigators can be found at: http://adni.loni.ucla.edu/wp-content/uploads/ howtoapply/ADNIAcknowledgementList.pdf .\nThis work was supported in part by National Institutes of Health (NIH) grants ( HD081467 , EB006733 , EB008374 , EB009634 , MH100217 , AG041721 , AG049371 , AG042599 , CA140413 ). Zhengxia Wang was supported in part by the National Natural Science Foundation of China ( 61273021 ) For each i , the objective function in problem (8) is equal to the one in problem (9) . The Lagrangian function of problem (9) is as follows ( Duchi et al., 2008 \nwhere \u03b7 and \u03b2 i \u2265 0 are the Lagrange multipliersto be determined.\nDifferentiating with respect to s ij and comparing to zero gives the optimality condition. And, according to KKT conditions ( Boyd and Vandenberghe, 2004 ) , we have the following equations:\nThe complementary slackness KKT condition implies that, whenever s ij > 0, we must have \u03b2 i j = 0 , so\ncan be verified that the optimal solution s ij should be\nwhere (a ) + = max ( 0 , a ) is the positive part of the variable a . Therefore, the remaining problem is the estimation of \u03b7 in Eq. 18 . From Lemma 1 in ( Duchi et al., 2008 ) , suppose that \nAccording to Eq. (18) and the constraint s i 1 = 1 , we have\nAfter we solve each s m i , we can obtain the affinity matrix S m . The convergence of our algorithm is O ( nlog n ) ( Duchi et al., 2008 ) . Table 7 . Table 8 ."}, {"section_title": "Appendix B", "text": ""}, {"section_title": "Table 7", "text": "IDs of the ADNI subjects."}, {"section_title": "Categories", "text": "ID of subjects AD(93) 1257 , 221, 929, 1341 , 547, 653, 316, 1339 , 1354 , 786, 3, 10, 53, 183, 712, 720, 699, 1161 , 1205 , 991, 1263 , 286, 682, 213, 343, 642, 1109 , 219, 543, 1171 , 1307 , 850, 1254 , 836, 1056 , 321, 554, 147, 400, 1037 , 889, 1281 , 1283 , 1285 , 341, 577, 760, 1001 , 627, 1368 , 1391 , 1044 , 474, 1371 , 1373 , 1379 , 535, 690, 730, 565, 1090 , 1164 , 1397 , 1402 , 149, 470, 492, 1144 , 743, 747, 1062 , 777, 1157 , 374, 979, 370, 891, 1221 , 431, 754, 1382 , 167, 216, 266, 740, 1409 , 1430 , 1201 , 1290 , 497, 438, 841, 1041 NC(101) 610, 484, 498, 731, 751, 842, 862, 67, 419, 420, 2, 5, 8, 16, 21, 23, 637, 1133 , 502, 575, 359, 43, 55, 97, 883, 647, 14, 96, 130, 985, 1063 , 74, 120, 843, 845, 866, 618, 95, 734, 741, 48, 555, 576, 672, 813, 1023 , 327, 454, 467, 262, 898, 1002 , 779, 818, 934, 768, 1099 , 315, 311, 312, 386, 363, 489, 526, 171, 90, 352, 533, 534, 47, 967, 1013 , 173, 416, 360, 648, 657, 506, 680, 259, 230, 245, 272, 500, 522, 863, 778, 232, 1200 , 123, 319, 283, 301, 459, 686, 972, 1194 , 1195 , 1197 , 1202 , 1203 MCI(202) 1074 , 1122 , 222, 546, 1224 , 675, 1130 , 101, 128, 293, 344, 414, 698, 1030 , 1199 , 161, 422, 904, 326, 362, 861, 1282 , 634, 917, 932, 1033 , 1165 , 1175 , 240, 325, 860, 1120 , 1186 , 1275 , 354, 590, 1028 , 1092 , 57, 80, 142, 155, 141, 178, 424, 626, 544, 924, 961, 1351 , 1394 , 1393 , 1400 , 256, 408, 461, 485, 914, 1038 , 1073 , 1215 , 1218 , 1318 , 1384 , 294, 214, 718, 978, 511, 513, 567, 723, 906, 33, 204, 292, 997, 656, 673, 748, 945, 976, 1135 , 1240 , 150, 377, 552, 566, 1078 , 1421 , 282, 314, 407, 446, 549, 598, 679, 721, 1010 , 1260 , 1411 , 1412 , 1418 , 1420 , 1423 , 1425 , 1346 , 389, 621, 919, 464, 941, 957, 1007 , 1217 , 1265 , 1294 , 1299 , 1211 , 1380 , 746, 909, 1357 , 641, 531, 1188 , 1314 , 1398 , 1417 , 160, 51, 54, 291, 551, 880, 958, 1034 , 892, 930, 995, 1154 , 950, 1114 , 1343 , 378, 410, 1103 , 1106 , 1118 , 361, 1243 , 1315 , 1322 , 708, 709, 865, 1077 , 112, 394, 925, 1032 , 1210 , 1419 , 1427 , 135, 138, 188, 200, 205, 225, 227, 258, 608, 715, 770, 947, 1043 , 1406 , 1407 , 1408 , 1204 , 1246 , 285, 289, 783, 409, 987, 695, 158, 443, 481, 669, 722, 800, 825, 994, 1414 , 1426 , 1245 , 1378 , 1295 , 1311 pMCI(56) 54, 57, 101, 128, 141, 161, 204, 214, 222, 240, 256, 258, 289, 294, 325, 344, 394, 461, 511, 549, 567, 675, 695, 708, 723, 860, 861, 892, 904, 906, 930, 941, 947, 978, 987, 997, 1007 , 1010 , 1033 , 1077 , 1130 , 1135 , 1217 , 1240 , 1243 , 1282 , 1295 , 1299 , 1311 , 1393 , 1394 , 1398 , 1412 , 1423 , 1427 sMCI(63) 33, 142, 150, 158, 178, 188, 200, 225, 285, 291, 414, 464, 481, 544, 546, 598, 608, 621, 626, 634, 656, 673, 679, 698, 709, 715, 718, 746, 748, 770, 783, 800, 914, 919, 925, 932, 950, 961, 1028 , 1032 , 1034 , 1103 , 1114 , 1118 , 1120 , 1122 , 1165 , 1175 , 1186 , 1211 , 1215 , 1218 , 1246 , 1260 , 1314 , 1378 , 1380 , 1384 , 1414 , 1417 , 1418 , 1419 , 1421 3010, 3012, 3014, 3018, 3020, 3021, 3023, 3024, 3026, 3027, 3028, 3051, 3052, 3054, 3056, 3059, 3060, 3061, 3062, 3066, 3067, 3068, 3076, 3077, 3078, 3080, 3081, 3083, 3086, 3088, 3089, 3102, 3105, 3107, 3108, 3111, 3113, 3116, 3118, 3119, 3120, 3122, 3123, 3124, 3125, 3126, 3127, 3128, 3129, 3130, 3131, 3132, 3134, 3150, 3154, 3166, 3167, 3168, 3173, 3174, 3175, 3176, 3178, 3179, 3181, 3182, 3184, 3185, 3190, 3251, 3252, 3253, 3254, 3267, 3268, 3269, 3272, 3275, 3278, 3279, 3280, 3281, 3282, 3284, 3285, 3288, 3290, 3305, 3307, 3308, 3309, 3311, 3314, 3321, 3322, 3323, 3325, 3327, 3328, 3332, 3352, 3354, 3359, 3360, 3364, 3365, 3366, 3367, 3371, 3372, 3373, 3374, 3375, 3376, 3377, 3378, 3380, 3383, 3385, 3386, 3387, 3392, 3406, 3407, 3409, 3413, 3415, 3417, 3418, 3419, 3420, 3421, 3422, 3423, 3429, 3430, 3432, 3433, 3434, 3435, 3436, 3440, 3443, 34 4 4, 34 45, 34 46, 34 48, 3451, 3454, 3455, 3459, 3461, 3462, 3467, 3469, 3470, 3471, 3472, 3473, 3475, 3476, 3482, 3500, 3501, 3502, 3504, 3505, 3506, 3507, 3514, 3516, 3522, 3528, 3530, 3532, 3536, 3540, 3542, 3552, 3556, 3557, 3558, 3559, 3564, 3567, 3574, 3575, 3577, 3584, 3585, 3586, 3587, 3588, 3589, 3591, 3592, 3593, 3601, 3603, 3604, 3605, 3606, 3607, 3608, 3609, 3612, 3616, 3617, 3621, 3622, 3625, 3628, 3629, 3630, 3631, 3632, 3633, 3634, 3638, 3650, 3653, 3654, 3657, 3659, 3660, 3661, 3664, 3665, 3666, 3700, 3702, 3710, 3752, 3753, 3757, 3758, 3760, 3762, 3763, 3764, 3770, 3771, 3775, 3776, 3777, 3778, 3780, 3781, 3787, 3788, 3789, 3800, 3802, 3808, 3814, 3815, 3818, 3819, 3822, 3823, 3824, 3825, 3826, 3827, 3828, 3829, 3830, 3831, 3832, 3833, 3834, 3835, 3837, 3838, 3863, 3866, 3868, 3869, 3870, 3900, 3903, 3904, 3905, 3910, 3911, 3914, 3916, 3951, 3953, 3954, 3957, 3958, 3960, 3961, 3962, 3963, 3964, 3970, 3972, 40 01, 40 05, 40 06, 4012, 4013, 4019, 4020, 4021, 4022, 4024, 4025, 4026, 4027, 4029, 4030, 4033, 4034, 4035, 4037, 4038, 4051, 4052, 4054, 4055, 4056, 4057, 4058, 4059, 4061, 4065, 4069, 4070, 4071, 4072, 4073, 4074, 4075, 4076, 4077, 4078, 4091, 4092, 4093, 4094, 4096, 4098, 4099, 4101, 4102, 4103, 4106, 4107, 4108, 4109, 4110, 4111, 4112, 4113, 4114, 4115, 4117, 4121, 4122, 4123, 4126, 4135, 4136 NC(165) 30 0 0, 30 04, 30 08, 3011, 3013, 3016, 3029, 3053, 3055, 3057, 3064, 3069, 3071, 3072, 3073, 3074, 3075, 3104, 3106, 3112, 3114, 3115, 3151, 3156, 3157, 3160, 3161, 3165, 3169, 3171, 3172, 3188, 3191, 3257, 3260, 3264, 3270, 3271, 3274, 3276, 3277, 3300, 3301, 3310, 3316, 3318, 3320, 3350, 3353, 3355, 3357, 3358, 3361, 3362, 3368, 3369, 3370, 3389, 3390, 3405, 3410, 3411, 3414, 3424, 3428, 3450, 3452, 3453, 3457, 3464, 3466, 3468, 3478, 3479, 3480, 3481, 3503, 3515, 3517, 3518, 3519, 3521, 3523, 3525, 3526, 3527, 3541, 3544, 3551, 3554, 3555, 3563, 3565, 3569, 3570, 3571, 3572, 3600, 3611, 3614, 3615, 3619, 3620, 3624, 3627, 3635, 3636, 3637, 3651, 3656, 3658, 3662, 3668, 3750, 3756, 3759, 3765, 3767, 3768, 3769, 3779, 3803, 3804, 3805, 3806, 3807, 3811, 3812, 3813, 3816, 3817, 3850, 3851, 3852, 3853, 3854, 3855, 3857, 3859, 3901, 3907, 3908, 3917, 3950, 3952, 3955, 3959, 3965, 3966, 3967, 3968, 3969, 4004, 4010, 4018, 4032, 4067, 4079, 4090, 4100, 4104, 4105, 4116, 4118, 4139 "}]