[{"section_title": "Abstract", "text": "Easy access to large collections of historical survey and census data, and the associated metadata that describes it, has long been the goal of researchers and analysts. Solutions to problems such as understanding the behavior of current survey data, respondent burden, improved statistical techniques, and data quality are often found in the careful analysis of historical data. Many questions have gone unanswered, because the data was not readily available, access was limited, metadata was not well defined, or query performance was intolerably slow. This paper describes the database modeling techniques that permit end users fast and easy access to large amounts of micro-level data contained in different data systems and from different time frames. Also, techniques for tracking metadata changes and standardization are also discussed. A generalized dimensional model is presented that can be used for any census or survey to track the full history of the data series."}, {"section_title": "", "text": "data quality. In 1996 NASS began work on an easy-to-understand and easy-to-use high performance historical Data Warehouse. At a minimum, the Data Warehouse would track previous survey and census data, including changes in specifications and metadata, and be readily accessible by all NASS employees, not just a few power users. It would also answer the Agency's strategic need to continually improve statistical analysis, survey and census procedures. Examples include: linking census, survey, and administrative data, improving sampling efficiency, enhancing survey management and administration, streamlining survey data collection, improving data quality, expanding analysis capabilities for all employees, and broadening statistical estimation methodology. Considerable research and evaluation was conducted during 1996 and 1997 to find the best Data Warehouse solution to satisfy our ambitious strategic objective. In April 1998 that research along with a great deal of support from senior management, and input from end users produced an easy-to-understand and easy-to-use Data Warehouse. Three months later, close to 700 NASS employees were accessing the Data Warehouse and reviewing, in one integrated database, survey responses from 35 different surveys extracted from over 1300 data files, and the complete set of census responses from the 1997 Census of Agriculture conducted during 1998. Currently, the database has grown to over 2 billion records covering over 602 surveys beginning with the January 1997 National Cattle Survey up to the most recent addition, the March 2004 Florida Citrus Survey."}, {"section_title": "A Brief Statement of the Problem", "text": "Internal reports articulating the strategic need for historical data, while making a powerful case, did nothing to show how such an endeavor might be accomplished. Indeed, when members of the original working groups that published these reports were interviewed, they said, in effect, the whole idea was a \"pie in the sky\". The problem was one of understanding how the original data sets could be organized into a robust data model that would not only store historical data, but track all changes made to survey and census programs over time. Thinking up to this point had been rectangular. Each single data set had N number of observations by P number of variables. To combine these data sets into a rectangular model with over 1.5 million farms on the NASS list frame, and over 10,000 discrete survey items being surveyed every year was not possible. Tracking history using the NxP model, besides being very difficult to administrator and slow to query, quickly developed a severe sparsity problem. Not every farm produces the same commodity. Other database models were investigated including the standard entity/relationship (E/R) model. This model performed well for transaction processing, but could not support ad-hoc decision support queries. Ad-hoc queries were essential to understand current farm trends against historical farm trends for many different commodities. The E/R model also failed the database understandability test. People using the system could not navigate the hundreds of tables required in this model, and applications written to support analysis did not work for the ever-changing ad-hoc query."}, {"section_title": "The Star Join Schema", "text": "Enter the star join schema or the dimensional database model. The star join schema represents to the end user a simple and query-centric view of the data by partitioning the data into two groups of data base tables: facts and dimensions. Facts are the data items being tracked, and dimensions contain the metadata describing the facts. In the NASS model shown in Figure 1 , facts are stored in the Survey Responses table in the two columns labeled Cell Value and Weight. The cell value is the individual data response for a particular question from a particular survey or census, and the weight is the value used to adjust the cell value for such things as non-response, and the sample weights. The dimensionality of each fact is described by the words or metadata stored in the columns of the dimension tables. The keys make the link between the metadata (dimensions) and the facts. Notice the join lines connecting the keys in Figure 1 . The dimension tables were built from the business rules that define a survey or census. The Var Name on the mode of data collection, the respondent, usability information, and the type of agricultural operation. The dimensions were chosen because they describe the business rules that govern the NASS survey and census programs, and are the \"by\" statements (slicing and dicing variables) for counts, sums, and ratios.\nUsing the dimensional attributes contained in the tables, data can be summed, counted, and analyzed by any of these attributes. For example, during the 1997 Census of Agriculture, data was being loaded into the Survey Responses table on a weekly basis. Reports were then produced that gave counts, sums, and ratios for the major agricultural items at the State and National level. Including other attributes, such as County from the Location table and Census ID from the Reporter table also produced reports by county and/or reporter. Direct comparisons of survey and census data at the individual reporter level were also possible, because the individual reports from both survey and census data sets from 1997 were stored in the Data Warehouse. Another example of using the Data Warehouse occurred during the 1999 June Agricultural Survey. Some of the questionnaires were returned with missing data. In a matter of seconds, all historical information on a respondent was retrieved and used to impute the missing values. Currently there are over 2 billion rows (cell values) covering state and national surveys from 1997 to the present.\nIn summary the E/R model is designed to model data relationships, and the star schema or dimensional model is designed to model the business rules. The fact table row with its unique set of keys on every row serves as a large business rule cross-reference table of metadata events that intersect at the cell value. When disparate data sets are combined in such a model, comparisons across and within surveys and censuses are quickly possible. Many different data comparisons using any of the rich dimensional attributes can be formulated and presented. Reporter classification and profiling becomes multidimensional. Many different aggregations can be calculated and displayed by summing to different dimensional levels.\nThe implications of this model are compelling:\n\u2022 The dimension tables store the metadata about the cell value in terms familiar and understandable to the end user. Codes and their descriptions can be placed together, as well as comments and documentation about the data item.\n\u2022 The dimension tables are attribute rich and hierarchical. Analysis can shift from a high vantage point with a broad set of attributes, to a very specific and narrow range of attributes depending on the study requirements.\n\u2022 The dimension tables track additions and changes over time in all aspects of the survey program.\nNew program content and questions, as well as small attribute changes, are tracked easily by adding additional rows to the appropriate dimension tables, rather than adding new columns to the fact table.\n\u2022 The dimension tables are conforming. Dimension tables created for other roles or uses in new applications can retain the keys assigned in the original or master table. This way data from many sources can be combined together by any combination of key relationships.\n\u2022 The cell value column in the fact table contains heterogeneous data. By transposing the column variables in the old rectangular data structure into individual rows in a dimension table, the fact table row now contains data on the complete range of attributes from all surveys. This effectively removes all sparsity from the fact table.\n\u2022 The fact table stores the data at the lowest level of granularity -the cell value. This allows drilling down into any level of data.\n\u2022 New fact tables can be created for any level of granularity or aggregation.\n\u2022 New fact tables can reuse any dimension table keys that span the facts being tracked, and use new dimensions created for those facts.\nThe star join schema, therefore, represents the model of choice for on-line integrated data access organized by dimensions that end users can understand, remember, and easily navigate."}, {"section_title": "Metadata and the Dimensional Model", "text": "The dimensional model is an elegant relational database model for organizing and accessing survey metadata. These tables serve the needs of end users by providing, among other things, on-line access to survey and questionnaire specifications, reporter profiling, data classification, and interviewing practices. The metadata is rich and organized visually and in tables that reflect the way the business of the Agency is actually conducted. No attempt is made to remove redundancy within the table, as is done with a modeling process known as normalization. In the E/R model, the need to normalize is critical. This speeds transactional updates and saves disk storage. An E/R modeler would look at the Location In this way minor changes to the name, address, certain demographics, and other non-major changes can be tracked for the particular ID. We have uncovered occurrences of a reporter's name changing completely from one classify year to the next, due to major updates being made to the name, and the ID remaining unchanged."}, {"section_title": "Issues with the Star Schema Model", "text": "As stated above, by converting all of the columns in the old flat file databases into rows in a dimension table, the result was a narrow two data column table with very little sparcity. Every row in the fact table has a Cell Value, and for more recent data, a weight. The model is also completely generalized. If a new variable, or a new reporter, or a new survey is needed, all that is required is the addition of a new row to the appropriate dimension table.\nThere are potential problems with this approach because of several issues associated with using a star schema, such as:\n\u2022 How to handle comparisons among data in the same column. This is not a new problem with the star schema, because of the nature of the tables and their intended use. This tends to create fact tables that are long and narrow as the double column design demonstrates.\n\u2022 Dimensional ad hoc analysis against a star schema requires multi-table joins between the dimensions and the fact table. Most query optimizers designed for transaction processing will default to a pairwise join strategy, or the joining of two tables at a time, on all related tables being queried for analysis. Of course, this will probably involve the very large fact table, because it is related to all the other dimensions. This can have a very limiting influence on the analysis if the intermediate result set\nis too large or must be sampled.\n\u2022 Referential integrity of the data being loaded into the fact There are other important issues in using a star join schema such as indexing strategies, load times, and disk drive segmentation by time periods. There are currently database engines on the market that solve many of these problems. NASS purchased one of these engines when they acquired Red Brick Data Warehouse System. Informix Data Systems recently purchased this company. Oracle worked from 1996 to 1998 developing an optimizer that would recognize the star schema. Other companies have small niches in the Data Warehouse market space such as Sybase and Microsoft, but are not considered up to the task of handling the extreme issues of very large databases."}, {"section_title": "Conclusion", "text": "Since the first official Crop Production Report, NASS statisticians have grappled with the need to understand their data. There are many influences on the data used to set official agricultural estimates and opportunities for error, both sampling and non-sampling errors. It is the tracking of these influences and the potential for modeling them against the estimates that give the data warehouse its true appeal. Every aspect of the business of creating official estimates, from planning and conducting surveys to statistical methodologies, and data analysis, will be influenced by this new technology. Productive and efficient analysis requires knowledge of the inputs that produce a given output. Data alone does not fulfill this requirement, because it does not carry along the information or metadata about the inputs and how they interrelate. This information and knowledge, in the past, have been separated from the data. It may have been available, but only in other disparate data sources, or in manuals and E-mail, or in programs, or in the hip pocket of an analyst. The star join schema represents a relational database model that gathers a great deal of this information and knowledge about the data, stores it, organizes it, and then relates it directly to the factual data being analyzed.\nThe richness of this information was not available in the transaction models. The emphasis there was on data, not on information. The end user or analyst was dependent on the Information Technology professional or power user to get at the data and report it in such a way that analysis could be performed. If further analysis was required, the process was repeated. The relational star join schema, on the other hand, simplifies the transaction model greatly and is designed for information gathering by the end user. It is an elegant software solution that presents data to the end user in the familiar and understandable terms of the business.\nAs information from the data warehouse is used in analysis and decision making, there will be a strong influence on all the processes that create our end product, the official estimates of U.S. agriculture. Operational systems that are choked with both operational and historical data will be freed up to operate more efficiently. As these systems are freed of excess data, re-engineering for efficiencies and quality will be less of a challenge. This re-engineering of tasks and procedures will occur, not because the warehouse needs it that way, but because the warehouse will help uncover data errors and inconsistencies resulting from these tasks and procedures. Perhaps, and most importantly, the information in the warehouse will be used strategically to help carry out the long range goals of the Agency, which is the real reason the data warehouse is a key element in the NASS Strategic Plan."}]