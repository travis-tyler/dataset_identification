[{"section_title": "", "text": "system where student record data already abstracted through the CADE were preloaded into CATI to minimize the length of the telephone interview. The purpose of the student interview was to collect information on additional sources used by students in the financing of their education, expenses and aid obtained at institutions other than the sampled institutions. Students sampled for the Baccalaureate and Beyond (B&B) cohort--those who graduated in 1992-93--were administered a slightly longer questionnaire that included items on future plans related to education, occupation and family formation."}, {"section_title": "Response Rates", "text": "Response rates for NPSAS:93 have been calculated for two levels of institutional participation --those institutions providing student enrollment lists as frames for student sample selection and those providing the financial aid and other data abstracted from administrative records. In addition, response rates have been calculated for student and parent participation in the telephone interview component of the study. Weighted response rates were calculated based on the institutional probabilities of selection. The weighted response rates can be interpreted as the estimated percentages of institutions in the population that would have participated, if selected. The overall weighted response rate for providing student enrollment lists was about 88 percent, ranging from 80 percent of the private for-profit schools to about 96 percent of the public institutions. About 98 percent of institutions agreeing to participate provided some information needed for locating sampled students. Students were considered CATI respondents if they completed at least Section A of the CAT! interview. Of the 77,000 CATI-eligible sample students, about 53,000 or nearly 70 percent of the CAT! eligibles, were interviewed. The overall parent response rate was about 62 percent. More detailed information on response rates is presented in Chapters 4 and 5."}, {"section_title": "Data Access", "text": "Data from the NPSAS:93 and other NCES data programs are made available through the Data Analysis System (DAS) and the Electronic Code Book (ECB). NPSAS:93 studentlevel data are derived from record abstracts and student and parent telephone interviews. In analysis, data may be drawn from any of seven separate data sets for undergraduate students and graduate students (including first professionals). The institutional data (CADE) and telephone interview (CATO files contain data either abstracted directly from institutional administrative records or entered during telephone interviews with students and parents. Data from all parent interviews are included in a single data set. Derived variableg are constructed from either the CADE or CATI or both sources. For each of the derived variables, the DAS includes an indicator for the source of the information. The verbatim files induct:: responses from \"Other, specify\" items and verbatim response to items concerning student's majors, and the industry and occupation of jobs held by the student. Student majors and industry and occupations were coded during the telephone interviews using software developed by NCES for this purpose and the codes for these items are in the derived variable files."}, {"section_title": "ACKNOWLEDGMENTS", "text": "Abt Associates Inc., with Research Triangle Institute (RTI), and MPR Associates, Inc. conducted the 1993 National Postsecondary Student Aid Study (NPSAS) under contract with the National Center for Education Statistics (NCES). John D. Loft of Abt Associates provided project direction and management with John A. Riccobono of RTI and Robert A. Fitzgerald of MPR Associates. A cadre of other staff--including statisticians, analysts, survey managers, programmers, and data collectors and interviewers--too numerous to list here worked long hours to produce the data files and reports of the 1993 NPSAS. The project is also indebted to the staff of over 1,000 postsecondary education institutions who assisted in the institution records collection and to the over 70,000 students and parents who generously participated in the telephone survey. Without their willingness to share information, the 1993 NPSAS would not exist. The following Office of Educational Research and Improvement/National Center for Education Statistics reviewers provided helpful comments on this report: Nabeel Alsalam, Bob Burton, and Peggy Quinn. Rick Apling (Congressional Research Service), Sonia Con ly (Treasury Department), Dan Goldenberg (ED, Office of Policy and Planning), Dan Madzelan (ED, Office of Postsecondary Education), and Jay Noe 11 (Congressional Budget Office). In addition, Carol Fuller and Ken Redd (National Association of Independent Colleges and Universities), and Fred Galloway (Amefican Council on Education) also provided helpful comments. Andrew G. Malizio served as the NCES project officer who managed the study under the overall supervision of C. Dennis Carroll, Longitudinal Studies Branch. Paul D. Planchon, Associate Commissioner at NCES, provided management and direction. Throughout the design, implementation and analysis phases of the study, the NPSAS Technical Review Panel members provided helpful suggestions for improving the quality of data collected. The National Center for Education Statistics is indebted to all these individuals who assisted NCES and Abt in the planning, design and implementation of the study.                          .77ec.flor 7"}, {"section_title": "vii", "text": "..-unVel...1..1.-71-1P---,771.7.en-                               CHAPTER 1 STUDY OBJECTIVES AND DESIGN The National Postsecondary Student Aid Study (NPSAS) is a comprehensive nationwide study conducted by the Department of Education's National Center for Education Statistics (NCES) to determine how students and their families pay for postsecondary education, and to describe some demographic and other characteristics of those enrolled. The study is based on a nationally representative sample of all students in postsecondary education institutions, including undergraduate, graduate and first-professional students. Students attending all types and levels of institutions are represented in the sample, including public and private institutions and less-than-2-year institutions, 2-year institutions, and 4-year colleges and universities. The study is designed to address the policy questions resulting from the rapid growth of financial aid programs, and the succession of changes in financial aid program policies since 1986. The first NPSAS study was conducted in 1986-1987, then again in 1989-90. Abt Associates, and its subcontractors, Research Triangle Institute (RTI), and MPR, Inc. designed and completed the 1992-93 study (NPSAS:93) under contract with the NCES. 1.1. Student Aid Study: 1993 1.1.2 Research, Policy and Programmatic Issues Addressed by NPSAS A main objective of the study is to produce reliable national estimates of characteristics related to financial aid for postsecondary students. The data is part of the National Center for Education Statistics' (NCES) comprehensive information on student financial aid and other characteristics of those enrolled in postsecondary education. The study focuses on three topics that have important policy implications for financial aid programs:"}, {"section_title": "Objectives of the National Postsecondary", "text": "How students and their families finance postsecondary education; The process of financial aid, i.e., characteristics of the students who apply, those who actually receive it, and examining the different types of aid received; and Effects of the receipt of financial aid on the students and their families. The first topic addresses the sources of financial aid and measures whether different need analysis systems used to determine the need for financial aid are sensitive to changing costs. The second topic describes various strategies used to finance postsecondary education, and how they might be predictive of changes in financial aid programs. What are the differences between Federal financial aid and aid from other sources, and the distribution mong students at different types of postsecondary institutions? The third topic addresses the concerns about the effects of the actual receipt of financial aid, for example, the level of debt due to education and the student/family's ability to repay it; the effect of financial aid on student persistence/completion of postsecondary education. In order to meet this challenge, NPSAS:93 relied on a highly integrated system of computer assisted data capture instruments. The NPSAS Integrated Control System (ICS) provided the framework for articulating modules developed to abstracc data from financial aid and other administrative records maintained by institutions and gather data from telephone interviews with students and parents. Additional modules of the ICS provided editing of these data, preloading data from one module to another (as, for example from the record abstract system to the student telephone interview), and preparing routine production and management reports. Communication modules of the ICS provided the capability for transfer of data from the field to a central office and also for routine communication via electronic mail between all members of the project team. In addition to this general methodological strategy, the NPSAS:93 field test provided an opportunity to evaluate particular features of the survey design. The general objectives of the NPSAS:93 field test were to (1) evaluate the timing of key data collection activities; (2) evaluate data collection systems; (3) test methods for increasing participation in NPSAS; and (4) determine whether certain students could be induced to take the Graduate Record Examination (GRE) in order to measure student ability and other factors that may affect student achievement."}, {"section_title": "Sample Design", "text": "\nThe NPSAS:93 project staff compared a three-stage and two-stage sample design to determine whether the potential statistical efficiencies of a two-stage design would be cost effective. As summarized in Chapter 2, the cost savings due to geographic clustering in a three-stage design are significant if a great deal of travel is anticipated. In the NPSAS:93, field data collectors were required to travel to about half of the institutions in order to complete the record abstraction tasks. For this reason, the issue of travel costs and geographic clustering remained salient. However, an important result of NPSAS:93 was the demonstration that many institutions could complete the record abstraction task themselves using the project-developed software. As the usage of personal computers continues to expand, the number of institutions willing to undertake this task may well increase. If this happens, a selfadministered NPSAS (at the institution level) could minimize travel costs to a degree that the two-stage sample design should be reconsidered."}, {"section_title": "Target Population", "text": "The target population of NPSAS:93 consisted of all students (including those who did and those who did not receive financial aid) enrolled in postsecondary institutions in the United States, the District of Columbia and Puerto Rico, during the 1992-93 financial aid award year (terms beginning from July 1, 1992 through June 30, 1993), excluding students who were enrolled solely in a GED program or were concurrently enrolled in high school. The survey population was defined as those students who were enrolled in any term or course of instruction that began between May 1, 1992 andApril 30, 1993. In this way student sampling could be obtained during the Spring, 1993. An important feature of the NPSAS:93 study design was the selection of a subsample of students representing the cohort that received a baccalaureate degree during the NPSAS year. A longitudinal study of baccalaureate recipients, Baccalaureate and Beyond (B&B), began with NPSAS:93 as the base year. These students will be interviewed annually, beginning in the NPSAS year, and during five subsequent years, to determine the impact of financial aid arrangements on their future educational attainment, labor force participation, and family formation. The sample design is fully described in Chapters 2 and 3."}, {"section_title": "Survey Frame", "text": "The survey frame for NPSAS:93 was based on postsecondary institutions, the primary source of information for NPSAS. Institutions provided enrollment files and graduation lists that constitute the frame for the student sample, in addition to critical locating, enrollment and financial aid data about the students selected for the study. The institutional sampling frame for NPSAS:93 was built from the 1990-91 Integrated Postsecondary Education Data System Institutional Characteristics file (IPEDS-IC). The IPEDS-IC file was supplemented with the Office of Postsecondary Education Data System (OPE-IDS) file of institutions participating in the Stafford and Pell student aid programs as of April 15, 1992. Records added to IPEDS-IC were carefully examined to assure that the added records were for-eligible institutions and non-duplicative. This list of institutions formed the universe for sample selection of NPSAS:93, were designed to include separate federal, state, institutional, student, and parent data components, in order to obtain a complete record of financial aid. The educational institutions are the best source for information about how a student's eligibility for aid and the amount of aid awarded is determined. The institutions also provide the most accurate records of the amount of financial aid received and the details of the financial aid package, including the source of funding. Students are the best source of information pertaining to the actual costs of their education, their financial resources, and personal characteristics and attitudes. As both students and institutions often lack complete information about parent finances and financial obligations, the parents are the best source of a family's financial information when a student is dependent and unaided. Although NPSAS:93 included separate data collection components from institutions, students, and parents, some overlap of data elements were built into the data collection instruments as measures of accuracy and reliability. For example, although the institutional records are regarded as the best source of data on financial aid awards, financial award data was also collected from students. The institutional information and student self-report data were compared in order to corroborate the financial aid data. In addition, student data was used to complete missing information, in cases where the institutional information were not collected, or if the student attended other schools and institutional records had not been examined, or if the student happened to obtain financial aid from another source (i.e, an employer, family, private organization), and the institution had not been informed."}, {"section_title": "Description of Instruments and Data Collection Procedures", "text": "Institutional Records Data Collection Software The student record abstraction software was used to abstract comprehensive information about the student's involvement with the institution, the amount(s) of financial awarded and the student/family's income and assets. Data were abstracted from the student financial aid and other administrative records maintained by the institution. A menu-driven computer assisted data entry (CADE) software was designed for use in abstraction of student data. Seven modules were created within the Records Abstract Software for NPSAS:93. The first module was designed for data about the students at the institution, e.g., participation in federal student aid programs. terms of enrollment, credit or clock hours, and other data pertinent to all students in that institution. Other modules were designed for specific student information: student and parent locating information gathered for follow-up purposes, periods of student enrollment, student characteristics, actual financial aid awarded, the student's need analysis and budget; financial aid eligibility information contained in output documents, and financial aid formulae used to determine a student's need."}, {"section_title": "Student CATI Interview", "text": "The students selected for NPSAS:93 were contacted for a telephone interview The student interviews were conducted using a computer-assisted telephone interview (CATO system where student record data already abstracted through the CADE were preloaded into CATI to minimize the length of the telephont. interview. The purpose of the student interview was to collect information on additional sources used by students in the financing of their education, expenses and aid obtained at institutions other than the sampled institutions."}, {"section_title": "1-7", "text": "Students sampled for the B&B cohort were administered a slightly longer questionnaire that included items on future plans related to education, occupation and family formation."}, {"section_title": "Parent CATI Interview", "text": "Three types of information were collected during the parent interview. Parents were asked to describe the financial support that they had given to the student, i.e., dollar amounts, source of the funds and whether the support was a contribution or loan. They were also asked about other dependents to whom they had provided support, total number of dependents and the total tuition paid for college, elementary and secondary schools. They were asked to describe their personal finances, sources of income, and any money that they had borrowed to provide financial aid to the sampled student. There were six separate modules in the parent CATI interview: Parental Support, Dependents, Employment and Financial Condition, Parent Demographics, Sample Student Education, and Attitudes."}, {"section_title": "Data Collection Procedures", "text": "The NPSAS:93 data collection methods were specifically designed to maximize response rates of institutions, parents and students. Serious attempts were also made to minimize efforts required during data collection and to fully gain cooperation of all respondents. Contacts with institutions began in February, 1993. Advance mailings were sent to the Chief Administrators of the 1,386 institutions selected for participation. If a school had previously participated in a NPSAS survey, the letter to the Chief Administrator distinguished between a NPSAS:90 school and those new to the sample. Participating sampled institutions wcre requested to provide enrollment files containing all eligible students enrolled during the study period. Once the student sample was selected, institutions were contacted again to arrange for the data abstraction from student financial aid and other administrative records maintained by the institutions. The institutions could choose to complete the record abstraction tasks themselves, (i.e., \"self-administered\"), or receive the assistance of an Abt/RTI field representative to abstract the student records. Student Institutional Records Data Collection (CADE). The CADE software insured uniformity, comparability and quality of the data collected from diverse institutions. Every effort was made to encourage school representatives most familiar with the institutional student records to utilize the menu-driven CADE method for abstraction of institutional data. If the school required assistance, a field interviewer was used to collect data. \"Schoolspecific\" information was electronically transmitted to the Field Interviewer prior to the institutional visit. The information was \"pre-loaded\" into the CADE program used for each institution to minimize data collection time, and maximize accuracy. The Abt/RTI field staff were specially trained to abstract the necessary data from administrative records at the institutions. Downloading directly from the institution's computerized system was considered and was discussed with the data processing staff of several institutions, both in the field test anu in the full-scale study. However, costs of the programming effort required for the download exceeded the -ost of CADE data in each instance where downloading was considered."}, {"section_title": "\" 24", "text": "Comprehensive information was obtained for the students who would be selected for the B&B cohort sample. Information for the entire undergraduate period of students earning a baccalaureate degree between July 1, 1992 and June 30, 1993, institutions was gathered. Because the data requested in each module could exist in several locations on school campuses, each was designed so that it could be completed for all sampled students at once. If a complete set of student records did happen to be present in one location, the entire CADE questionnaire could be completed for each student. Institution-level student data from self-administered institutions were collected from July through August 1993. Field interviewers who assisted in data collection conducted institution visits from June through December 1993. Student and Parent Telephone Interviews. Overlapping record abstract data were preloaded hto the telephone interview to minimize its length. Both the student and parent questionnaires were designed so that either one could be administered first. Therefore, if similar data elements were already provided by one respondent, those questions were not repeated during that family's second interview. The student and parent telephone data collection began Septembt. 6, 1993, and was conducted until March 21, 1994."}, {"section_title": "Quality Control Methods", "text": ""}, {"section_title": "CADE System", "text": "To insure the completeness of the record abstraction, answers to certain questions were essential in order to fulfill the record abstraction task. Questions were designated as Hard Critical and Soft Critical questions. Nine hard critical questions required an answer before data entry could be continued. If an attempt was made to leave a hard critical question blank, the data collector could not proceed. Ten soft critical questions also required an answer. If an attempt was made to leave a soft critical question blank, the option was to enter either an answer or a reserve code, before continuing to the next question. Entry of a reserve code indicated that attempts were made to locate the necessary information, but it was \"U\"--\"unavailable\" or \"unspecified\". Reserve codes became separate categories for analysis purposes. Range checks were established and coded into the CADE system. Range checks were established as a check for data entry errors. If an out-of-range number was entered into the program, a re-check of the data entry was required. A corrected entry could be made, or if the out-of-range number was correct, data entry could continue after the re-check. Skip patterns were also programmed into the CADE system to maximize data entry efficiency and to safeguard against incorrect entry of information. During the field test, a small-scale verification of record abstract data with institutions was conducted. A CADE validation form to verify a limited number of data elements was requested for nine student records from each of 11 institutions. Responses for 96 of the 99 students were returned. A high level of agreement was found between the initial reports, and the validation reports for Pell Grants, Federal College Work-Study Program and Stafford Loans. The percentage of updates ranged from 1 percent to 2.1 percent. In about 6 percent of the cases, the date of first enrollment was updated. The largest differences were found in Need Analysis Tuition reports, where 21 of 96, or 22 percent, of student records were updated, mostly attributable to missing data in the initial collection. In both the field test and the full-scale study, an additional edit step occurred in the central office prior to preloading data into the CATI system. An ICS module, CADE-Operations, was developed to keep track of data files returned from institutions on diskette or from field data collectors via telephone and modem. This module also included a feature to monitor the completeness of each institution's data file. Institutions with a large amount of missing data were identified for follow-up efforts."}, {"section_title": "CATI System", "text": "Telephone interviewing personnel were required to adhere to high performance standards, to meet the expected quality and production levels. The performance standard was four completed cases per interviewer for each six hour shift, and each interviewer was monitored at least once during each shift. Performance was monitored for the application of proper interviewing techniques, interview production rates, refusals, and breakoffs. Interviewers were selected for monitoring using the Monitoring Log, a part of the software program used to help prioritize the monitoring schedule during each shift, and the Daily Seating Chart, used to develop the monitoring schedule for each shift. Supervisors had the responsibility to insure the high quality of the data collected. Procedures were developed and used for this purpose."}, {"section_title": "Follow Up on Call-Backs and Appointments", "text": "Telephone Interview Supervisor had primary responsibility to review the appointments for daily reports at the beginning of every shift. The review was conducted to ensure that call-backs and appointments made were not missed. The supervisor followed up with interviewers, or assigned specific cases for interviewers to complete."}, {"section_title": "Status of Cases Review", "text": "Status of cases were reviewed by Telephone Interview Supervisors. The review was conducted with the aid of reports that delineated the status of cases according to specific requirements: locating, refusal conversion, bilingual interviewer. After status review, the supervisor classified cases to the appropriate queue and/or moved them if status had changed. Each week, the Case Status by Number of Attempts Report was reviewed. When a case had more that 10 attempts, a critical review was made by the supervisor to determine exactly why contact had not been made. Cases were reviewed using these criteria: missing locating information; calls made at the same time of day each attempt, case coded correctly, special notation in case comments to explain problem.  (DAS) and Electronic Codebook (ECB). Analysis files have beim created for the data obtained directly from the record abstract system (CADE) and the student and parent telephone interviews (CATI). In addition, a series of about 800 variables have been derived from either the CADE or CATI data. Finally, verbatim descriptions of certain \"other specify\" responses and of responses to queries about student major and industry and occupation will be available to researchers. A listing of the data elements from CADE and CATI and the Derived Variables is provided in Appendix A. bVariables from the parent questionnaire are included in the counts of student CATI variables 1.4.2 Relationship of variables and files to prior NPSAS Surveys For comparability purposes, many variables in NPSAS:93 based on institution and/or telephone interview data were created similarly to variables in prior NPSAS studies, (for example, total loans and total grants). The NPSAS:93 analysis file also contains a variable that allows researchers to included only those students from NPSAS:93 sampled in terms similar to those in the NPSAS:87 sample, (i.e., fall only and not enrolled in Puerto Rico). As explained in a recent NPSAS:93 tabulation (see National Postsecondary Student Aid Study: Estimates of Student Financial Aid 1992-93, NCES 95-746, June 1995, those estimates will not reflect total expenditures as reported by the Department's specific Title IV program offices. Those interested in the methodology for NPSAS:87 should refer to the Methodology Report for the National Postsecondary Student Aid Study, 1987(NCES 90-309, March 1990; the NPSAS:90 procedures are descibed more fully in the Methodology Report for the 1990 National Postsecondary Student Aid Study, NCES 92-080, May 1992). Further, researchers are encouraged to read the descriptions of variables contained in the electronic codebook and the Data Analysis Systems to determine comparability across years. For example the total income variable in NPSAS:90 refers to the total adjusted gross income. In NPSAS:93, several income variables are included on the analysis file, including total income from all sources, adjusted gross income (for federal financial aid applicants) and income from all jobs."}, {"section_title": "CHAPTER 2 INSTITUTION SAMPLING AND ENLISTMENT", "text": ""}, {"section_title": "Investigating Two-Stage Versus Three-Stage Sample Selection", "text": "A three-stage sampling design in which geographical areas were selected at the first stage of sampling was used for NPSAS:87 partly because it was necessary to use local sources at that time to construct sufficiently complete institutional sampling frames. The first-stage sample areas selected for NPSAS:87 were retained for NPSAS:90. However, the 1990-91 IPEDS institutional Characteristics (IC) file was believed to provide essentially complete coverage of the NPSAS:93 target population. Therefore, the feasibility of eliminating one stage of sampling by selecting institutions at the first stage was investigated. Eliminating one stage of sampling would reduce sample clustering and thereby improve the precision of survey statistics for a given sample size. However, it could also increase the cost of data collection by virtue of increased travel costs to abstract student data at sample institutions. Therefore, the evaluation of two-stage versus three-stage sampling for NPSAS:93 focused on cost effectiveness. Conducting this evaluation required first constructing a comprehensive institutional sampling frame from the IPEDS IC file, from which a first-stage sample of institutions could be selected."}, {"section_title": "Constructing the Institutional Sampling Frame", "text": "Nearly all postsecondary institutions in the 50 States, the District of Columbia, and Puerto Rico belong to the target population for NPSAS:93. However, to be eligible for NPSAS:93 an institution was required to satisfy all the conditions listed in Figure 2.1. Institutions serving postsecondary students that were not eligible for NPSAS:93 included those that: Provided only avocational, recreational, or remedial courses; Offered only in-house courses for their own employees; Offered only correspondence courses; or Offered only courses requiring less than 3 months or 300 clock hours of instruction, such as some driver training schools, real estate schools, and tax preparation schools. In addition, U.S. Service Academies were classified as ineligible because of their unique funding/tuition base, as had been done for both NPSAS:87 and NPSAS:90. To be eligible for NPSAS:93 an institution was required to satisfy all the following conditions during the 1992-93 academic year: Offered an education program designed for persons who have completed secondary education; Offered an academically, occupationally, or vocationally oriented program of study; Offered courses to students not employed by the institution; Offered more than just correspondence courses; Offered at least one program requiring at least 3 months or 300 clock hours of instruction; and W as located in one of the 50 States, the District of Columbia, or Puerto Rico. Since the IPEDS IC file was used to create the institutional sampling frame, each record cn the IPEDS file was considered to define a separate institution. Hence, each campus in a multi-campus state university system was generally considered to be a separate institution. Likewise, if a law or medical college on a university campus had its own separate IPEDS identificution number, the law or medical college was treated as a separate institution. The 1990-91 IPEDS Institutional Characteristics (IC) file contained 10,287 records. Records that were identified on the IC file as not representing eligible institutions were deleted: 123 central offices, 10 U.S. Service Academies, and 9 institutions outside the geographic target area. Five other institutions were deleted as ineligible based on telephone calls to the schools regarding discrepancies in the IPEDS enrollment data. After deleting these 147 records, the NPSAS institution-level sampling frame contained 10,140 records. The 10,140 institutions on the NPSAS:93 frame were first stratified as 4-year, 2-year, or less-than-2-year institutions based primarily on the LEVEL variable from the IC file. However, three institutions were re-classified as 4-year institutions. The IC file showed that these institutions had graduate students enrolled. Morecver, a telephone call to the third school regarding discrepant enrollment data confirmed that this school enrolls graduate students. The SECTOR variable was used to determine if these schools were public or private institutions, and the highest level of offering w:Is assumed to be Master's."}, {"section_title": "2-2", "text": "The 4-year institutions were stratified into the following four categories based primarily on the IC variables \"first-professional offering\" and \"highest level of offering.\" bachelor's. When the data for highest level of offering were missing on the IC file, professional judgement was used to make the stratum assignment based on the unduplicated enrollment data and the institution name. Institutions were assigned to these strata in a hierarchical manncr. Thus, all institutions that awarded first-professional degrees were placed in the firstprofessional stratum; all remaining institutions that awarded doctoral degrees were placed in the doctoral stratum; etc. The eight strata formed for 4-year institutions by crossing institutional control with the above four levels of offering were further subdivided into high and low proportions of baccalaureate degrees awarded in education based on the 1989-90 IPEDS Completions file. The \"high education\" substrata were designed to contain approximately 20 percent of the institutions in each stratum. Operationally, which the proportion of baccalaureate following thresholds. they were defined to be those institutions for degrees that were awarded in education exceeded the Thus, for example, public, first-professional institutions were classified into the high education substratum if over 15 percent of the baccalaureate degrees awarded were in education. However, private, first-professional institutions were classified into the high 2-3 education substratum if any baccalaureate degrt.,..--s were awarded in education. Institutions for which the 1989-90 Completions file contained no data for the number of degrees awarded in education, including institutions missing from the Completions file, were treated as if they had no degrees awarded in education. The absolute number of degrees awarded in education was not a criterion for forming the strata because the sample yield from a fixed number of sample students per institution depends only on the proportion of baccalaureate degrees in education, not on the absolute number of education degrees. Having completed this stratification, seven of the strata for 4-year institutions contained mostly large institutions and nine contained mostly small institutions. To achieve a more efficient sampling frame, eight small institutions were moved from large institution strata to small institution strata. In particular, the following changes in stratification were implemented: (1) one small institution was moved from \"public, 4-year, first-professional, high education\" to \"private, 4-year, first-professional, low education;\" (2) two small institutions were moved from \"public, 4-year, first-professional, low education\" to \"private, 4-year, first-professional, low education;\" and 3five small institutions were moved from \"public, 4-year, master's, low education\" to \"private, 4-year, master's, low education.\" Knowing that the stratum assignments are all imperfect and that analysis domains must be based on data collected in the survey, not on the sampling strata, these few reclassifications to achieve more homogeneous institution sizes within strata was preferable to creating additional strata for small institutions. The resulting strata are summarized in Table 2.1 for the final institutional sampling frame constructed to test the cost-effectiveness of selecting institutions at the first stage of sampling."}, {"section_title": "Comparing Cost Effectiveness", "text": "After creating the institutional sampling frame, ten hypothetical NPSAS:93 samples of institutions were selected. The institutions were selected with probabilities proportional to the following measure of the size' for the i-th institution: 1This measure of size is not identical to that used for the final sample of institutions, but the effect is negligible. 2-4 0 0 0 As S1(i) = GRCNT + 1.7 UNCNT + 3.7 BACNT + 4.5 FPCNT , (1) where GRCNT = number of graduate students, UGCNT = number of undergraduate students, excluding baccalaureate recipients, BACNT = number of baccalaureate degree recipients, and FPCNT = number of first-professional students based on the WEDS IC and Completions files. A sample of 1,520 institutions was allocated to the 22 institutional sampling strata as shown in Table 2.2. This allocation was designed to facilitate approximately equal overall probabilities of selection for students within institutional level: 4-year, 2-year, or less-than-2year. Multiple selections of institutions were not allowed because doubling or tripling the sample size at an institution to compensate for multiple selections at the first stage was considered undesirable. Therefore, all institutions with an expected frequency of selection greater than one (determined iteratively) were designed as certainty selections, as shown in Table 2.2. The institutions in the ten hypothetical samples were located in from 340 to 345 of the 362 area frame primary sampling units (PSUs) defined for NPSAS:90. Thus, sample institutions were widely dispersed across the entire target area (the 50 States, D.C., and Puerto Rico). In contrast, NPSAS:90 had been restricted to 173 of these PSUs. Therefore, the three-stage sampling procedure would produce i. jor cost savings by greatly reducing the number of areas to which field staff would have to travel to abstract student records, and a three-stage design in which geographic areas were selected at the first stage was implemented for NPSAS:93 in much the same way that three-stage samples were implemented for NPSAS:87 and NPSAS:90.  "}, {"section_title": "Area Frame Construction", "text": "Three-digit postal ZIP code areas were used as the basis for creating primary sampling units (PSUs) for NPSAS:93. Initially, PSUs were defined for probability sampling as geographically compact areas that did not cross State boundaries and were as nearly equal in size (student enrollment) as possible. Ultimately, some PSUs containing large institutions were defined to be certainty selections and were expanded in geographic extent without regard to the total measure of size. Defining the geographic areas or PSUs to be of nearly equal sizes was an important goal to ensure statistical efficiency. This was especially important for NPSAS:93 because the design for selecting sample institutions was technically a two-phase sampling procedure, rather than a two-stage sampling procedure (i.e., a clustered sample of institutions was selected, but these institutions were not sampled independently within the selected geographic areas). The process was two-phase because after geographic areas (PSUs) had been selected, the set of all institutions in the sample PSUs were combined into a single frame for sf:lecting a second-phase sample of institutions. A two-stage sampling procedure would have tquired selecting an independent sample of institutions within each sample PSU or geographic area. The two-phase sampling procedure was adopted for NPSAS:93 (as it had been for the previous NPSAS studies) because it facilitates using the 22 institutional strata shown in Table  2.1. However, two-phase sampling has some disadvantages. First, variance estimation problems arise if some sample PSUs contain no responding institutions. However, this situation did not occur for NPSAS:93. A second disadvantage is additional variability in the probabilities of selection for institutions because the probability of selecting an institution is the product of the probability of selecting the area in the first-phase sample and the probability of selecting the institution in the second-phase sample. In order to minimize the potential loss of precision because of unequal probabilities of selection, PSUs were constructed to have approximately equal measures of size. Hence, the sample of PSUs, selected with probabilities proportional to size, was an approximately equal probability sample of PSU areas. Postal ZIP-code maps were used to combine adjacent three-digit ZIPs within states, as necessary, to create PSUs that were geographically compact and had measures of size that were generally in the range from 60,000 to 100,000. The measure of size for each PSU was the sum of the institution measures of size given by (1) for all the institutions located in the PSU on the IPEDS IC file. Three-digit ZIPs that had large measures of size (e.g., over 100,000) were generally subdivided into smaller PSUs, occasionally allowing a single large institution to be a PSU, so that approximately 80 percent of the PSUs had measures of size from 60,000 to 100,000. Subdividing large three-digit ZIPs helped to achieve the goal of creating PSUs with nearly equal measures of size without compromising the geographical compactness of the PSUs. At the conclusion of this process of creating PSUs of nearly equal sizes, 398 area frame PSUs covering the 50 States, D.C., and Puerto Rico were defined. Because the PSUs were defined with approximately equal measures of size, selecting PSUs with probabilities proportional to size did not result in any certainty selections. However, the desired sample sizes for institutional strata, shown in Table 2.2, could be achieved within the sample PSUs only if something on the order of 300 of the 398 PSUs were selected. The travel costs that would result from data collection in such a large number of PSUs was considered to be prohibitive. Several strata that contained mostly large institutions yielded few sample institutions. Therefore, the PSUs containing the largest institutions were defined to be certainty PSUs and increased in geographical extent. By stratum, the size measure thresholds used to define certainty PSUs were as follows. The geographical boundaries of all certainty PSUs were reviewed. Because having equal measures of size was not important for certainty PSUs, they were combined with neighboring PSUs whenever that was possible without greatly expanding the geographical size of the PSU. The final area sampling frame contained 291 PSUs, of which 86 were certainty PSUs and the remaining 205 were non-certaint, PSUs. Technically, the set of all certainty PSUs was a stratum from which a two-stage sample of students was selected. That is, selection of sample institutions was the first stage of probability sampling within the certainty PSUs. A first-phase sample of 90 PSUs was selected from the 205 non-certainty PSUs, and sample students were selected within the second-phase sample institutions. The latter design for the non-certainty institutions will be referred to as a three-stage design hereafter to simplify the terminology."}, {"section_title": "Selecting Sample Areas", "text": "The final NPSAS:93 sampling design was based on the 86 certainty PSUs and a sample of 90 of the 205 non-certainty PSUs. Thus, data were collected within 176 of the 291 area frame PSUs. The 90 sample PSUs were selected from the 205 non-certainty PSUs with probabilities proportional to size (pps) using a sequential, probability minimum replacement (pmr) sampling algorithm (Chromy, 1979). The sample was implicitly stratified by OBE Region, state within Region, and measure of size within state by sorting the frame units. PSUs in Alaska and Hawaii were placed in Region 9 (outside the coterminous states), and Puerto Rico was placed in Region 5 (South). Sequential selection from an ordered frame was used to facilitate variance estimation using either replication methods or Taylor series methods."}, {"section_title": "2.3", "text": ""}, {"section_title": "Primary Sample of Iastitutions", "text": "The IPEDS-based sampling frame, developed as described in Section 2.1.1, was subset to those institutions located in the 86 certainty PSUs and the 90 sample PSUs. As a result of the editing performed for the supplemental sampling frame, described in Section 2.4, some additional frame cleaning was performed on the IPEDS frame among the 176 survey PSUs. One entry was deleted because it matched an entry on the OPE-IDS file that was flagged as a closed institution and because the telephone number listed in both files was non-working. Three other entries identified as representing only administrative offices were deleted. In addition, some duplicate entries in the IPEDS IC file were identified by printing sets of records that had the same institutional telephone number. Thirteen pairs of institutions having the same name, address, and telephone number were identified, and one member of each pair was deleted from the frame. Allocation of the institutional sample to the strata shown in Table 2.1 was developed to achieve approximately equal overall student-level sampling rates within level of institution (4-year, 2-year, and less-than-2-year) while achieving NCES' student sample size requirements for institutional strata and achieving average cluster sizes ranging from about 30 responding students in the institutional strata with the smallest institutions (e.g., less-than-2year institutions) to about 150 responding students within the institutional strata with the largest institutions (e.g., public, 4-year institutions). The resulting allocation of the institutional sample to the 22 institutional strata is shown in Table 2.3 for both I he 86 certainty PSUs and the 90 sample PSUs. Tto.:, table also presents the partition of the sample between the primary sample selected from the IPEDS-based frame and the supplemental sample of 22 institutions selected from the Office of Postsecondary Education's Institutional Data System (OPE-IDS) file. Sample institutions were selected from the IPEDS-based frame with probabilities proportional to size. The measure of size used for each institution was proportional to tht expected sample allocation for the institution, i.e.,"}, {"section_title": "s.(;", "text": "where fk is the overall population sampling rate for student stratum \"k\" and Njk is the number of students in institution \"j\" that belong to stratum \"k.\" The desired sample sizes for the four types of students being selected from 4-year institutions were used to set the overall population sampling rates, fk, as follows. Scaling up by multiplying by the lowest sampling rate, that for other undergraduate students, the measure of size for each 4-year institution was calculated as: S2 (j) = UGCNT + 1.1 GRCNT + 3.9 BACNT + 4.7 FPCNT . The measure of size for each less-than-4-year institution was simply its total unduplicated annual (undergraduate) enrollment. An independent sample of institutions was selected from the institutions located in the 86 certainty PSUs and from those located in the 90 sample PSUs using the sample sizes shown for the 22 institutional strata in Table 2.3. In each case, the sample institutions were selected with probabilities proportional to size (pps) using the same sequential, probability minimum replacement (pmr) sampling algorithm used to select the first-stage sample (Chromy, 1979). The samples were implicitly stratified by OBE Region, state, PSU, and measure of size by sorting the frame units within the 22 institutional strata. Institutions in Alaska and Hawaii were placed in Region 9, and Puerto Rico was placed in Region 5 (South). Within the set of certainty PSUs, sequential selection from an ordered fi me was necessary to facilitate replication-based and Taylor series variance approximations because institutions were the first stage of probability sampling in the certainty PSUs. Institutions for which the expected frequency of selection exceeded one (determined iteratively) were designated as certainty selections. The resulting partition into certainty and non-certainty sample institutions is shown in Table 2.4 for both the 86 certainty PSUs and the 90 sample PSUs. ore than 15 percent of baccalaureate degrees awarded in education. bAny baccalaureate degrees awarded in education. More than 25 percent of baccalaureate degrees awarded in education."}, {"section_title": "2-11", "text": ""}, {"section_title": "2.4", "text": "Supplemental Sample of Institutions"}, {"section_title": "Frame Construction", "text": "Although the IPEDS frame provided good coverage of the population of postsecondary institutions, NCES felt that the coverage could be improved by selecting a supplemental sample from the Office of Postsecondary Education's Institutional Data System (OPE-IDS) file of institutions participating in the Pell and Stafford student aid programs as of April 15, 1992. Each institution in the OPE-IDS file was identified as either a main campus or a branch campus (RECTYPE = M or B) and had a unique identification number (OPEID). In addition, if the NCES staff could identify the institution in the April 1992 IPEDS Institutional Characteristics (IC) file, the institution was assigned the matching institution's IPEDS ID number (although some matches were flagged as uncertain). In some cases, multiple OPE-IDS records (e.g., multiple branches) were assigned the same IPEDS ID number. NCES assigned all other institutions \"dummy\" IPEDS ID numbers beginning with double-zero (00). The first step in processing the OPE-IDS file was to subset to those institutions located in the 176 survey PSUs (86 certainty and 90 sample PSUs), based on ZIP codes. Institutions that had been assigned IPEDS ID numbers tha t matched those on the primary IPEDS-based sampling frame for NPSAS:93 were then ddeted. Telephone calls were placed to some of the larger branch campuses with no match in the IPEDS file to determine if they had their own registrar's office. Institutions that reported having their own registrar's office from which a separate list of students could be obtained were re-classified as main campuses. In the process, six closed or ineligible institutions were identified and deleted from the sampling frame. The remaining branch campuses (those not re-classified as main campuses) that did not match the current IPEDS IC file (had IPEDS IDs beginning with 00) were deleted. When a main campus was selected into the supplemental sample, the associated branch campuses that had been deleted from the frame were included in the sample with the main campus. Therefore, these deletions had no effect on the completeness of the frame. The branch campuses that had been assigned real IPEDS ID numbers were retained on the sampling frame. The fact that a campus was assigned a real IPEDS ID number was interpreted as meaning that it had its own separate registrar's office. In retrospect, deleting all the branch campuses may have been a better strategy. Sets of branch campuses were sometimes all assigned the same IPEDS ID number, suggesting that ex:), were covered by a single IPEDS record, possibly a main campus record. It might have been simpler to always include the branches with the main campuses for samples selected from the OPE-IDS file. Public, 4-year, first-prof, high eda  Because the purpose of the supplemental frame was to provide coverage for institutions not listed on the primary IPEDS-based frame, pairs of records from the two frames that matched on state and telephone number were examined. This resulted in deleting 39 institutions from the supplemental frame that matched on name, address, and telephone number. The OPE-IDS file contained three variables that provided enrollment data as of the time that the institution became eligible for Title IV student aid: number of students enrolled (a) full-time, (b) at least half-time but less than full-time, and (c) less than half-time. All three variables were missing or zero for approximately half of the institutions on the sampling frame. Nevertheless, using these data to generate measures of size for sample selection was preferable to selecting supplemental institutions with equal probabilities. Because most institutions on the supplemental frame were small institutions, the list of institutions with missing or zero enrollment was reviewed to identify any that appeared to be major institutions that should not be imputed to be small institutions. Then, the IPEDS-based sampling frame was searched for these \"major\" institutions; two lists were printed to manually search for matches: (1) all institutions listed as being in the same city, and (2) all institutions listed as being in the same state and having a name beginning with the same first three letters. As a result, seven records were deleted from the supplemental frame. Missing measures of size (enrollment) were imputed as the first quartiles of the known measures of size within strata defined by institutional level and control, analogous to the strata defined for the IPEDS-based frame. The control variable in the OPE-IDS file (CONT) was missing for only two main campuses. The level variable (INST) was missing for 27 main campuses. Control and level were logically imputed from the names of these institutions. Branch campuses with control or level missing were imputed to have the same control or level as their associated main campus. At this point, the supplemental OPE-IDS frame contained 34 4-year institutions. Because the primary IPEDS frame was expected to provide nearly complete coverage of the 4-year institutions, the IPEDS frame was searched for matches on these 34 institutions. Two lists were printed to manually search for matches for each institution: (1) all institutions listed as being in the same city, and (2) all institutions listed as being in the same state and having a name beginning with the same first three letters. As a result, thirteen institutions from the supplemental frame were deleted either because they had a direct match to the primary frame or because they were a \"branch\" (not necessarily flagged as such) for which the registration records were available from the main campus listed on the primary frame. These 13 deletions left 21 4-year institutions on the supplemental frame that appeared to not be covered by the IPEDS IC frame. Because the supplemental frame contained only 21 4-year institutions, institutional level was collapsed to two levels --(a) less than 2 years and (b) 2 years or more --for imputing measures of size. The numbers of institutions with zero or missing enrollment data versus those with positive enrollment data are summarized by level and control below. Using the first quai ule as the imputed measure of size for institutions with zero or missing enrollment data in the OPE-IDS file resulted in imputed sizes ranging from 9.5 to 28 students, depending on institutional level and control."}, {"section_title": "Sample Selection", "text": "The supplemental sampling frame was explicitly stratified by whether the institution was located in one of the 86 certainty PSUs or in one of the 90 sample PSUs because selecting institutions was the first stage of probability sampiing for institutions located in certainty PSUs. The supplemental sample was selected in \"waves\" until the requisite number of institutions had been selected. A sample of 22 eligible supplemental institutions was deemed to be sufficient. Only about 11 percent (9 out of 81) of the institutions selected from the supplemental frame for NPSAS:90 were eligible, but the frame cleaning for NPSAS:93 resulted in a much higher proportion of eligible institutions in the supplemental sample for NPSAS:93. Once measures of size had been defined for all institutions on the supplemental frame, institutions were selected with probabilities proportional to size (pps) using essentially the same procedures described in Section 2.3 for the WEDS-based frame. In order to allow sampling in waves and preserve overall probabilities proportional to the institutional measures of size, a relatively large initial sample was selected using pps sampling. Equal probability subsamples were then selected for the waves."}, {"section_title": "2-16", "text": "Each institution selected for the supplemental sample was checked for a match in the IPEDS frame. This was accomplished by manually inspecting the following two lists for each sample institution: (1) all institutions listed as being in the same city, and (2) all institutions listed as being in the same state and having a name beginning with the same three letters. Matches to the IPEDS frame were ineligible for selection from the OPE-IDS frame and were deleted from the sample. An initial sample size of 70 institutions was allocated to the certainty and noncertainty PSUs proportional to the size measure totals for these strata. After eliminating seven certainty selections because of matching IPEDS frame records, 16 certainty sample selections were identified. After identifying the 16 certainty selections, 70 sample institutions were selected: 38 from 488 institutions in certainty PSUs and 32 from 260 institutions in noncertainty PSUs, as shown in Table 2.5. The samples were selected with pps sampling and were stratified implicitly by using a sequential sampling procedure and sorting on level, control, and OPEID. The latter sorting variable was included simply to produce a unique frame ordering. Wavespecific subsamples were selected as simple random samples within the two explicit strata. For the first wave, three institutions were randomly selected from each explicit stratum (certainty and noncertainty PSUs) to complete an initial sample of 22 institutions (together with the 16 certainty selections). Matching IPEDS records were not found for any of these six institutions. Telephone calls were made to administrative officials (primarily registrars) at the 22 sample institutions to determine if they were eligible for participation in NPSAS:93. All 22 schools were determined to be eligible."}, {"section_title": "2.5", "text": "Probabilities of Selection Let S1(h,i,j) represent the measure of size for institution \"j\" in institutional stratum \"i\" within PSU \"h\" that was accumulated to define PSU-level measures of size, where h = 1, 2, ..., 291, i = 1, 2, ..., 22, and j = 1, 2, Moreover, let h = 1,2, ..., 86 denote the certainty PSUs. Then, S1(h,i,j) is given by2 where g, u, b, and f represent the unduplicated graduate, other undergraduate, baccalaureate, and first-professional student counts, respectively, from the IPEDS-based sampling frame. The measure of size for the h-th PSU was then Because sample PSUs were selected with probabilities proportional to size (pps) with probability minimum replacement (pmr) and none of the PSUs had an expected frequency of selection exceeding one (1.00), the probability of selecting the h-th PSU was n1S1 (h,+,+) / Si.F(+,+.+) if PSU \"h\" was not a certainty PSU (h) = 1 if PSU \"h\" was a certainty PSU, This measure of size is not identical to that used for the final sample of institutions, but the effect is negligible."}, {"section_title": "2-18", "text": "4 7 (6) where n1 is the number of non-certainty PSUs selected into the sample (n1=90) and 291   h=87Among the set of 86 certainty PSUs, institutions were selected with probabilities proportional to size (pps), using the following measure of size, S2(h,if) = u(h,i,j) + 1.1 g(h,i,j) + 3.9 b(ki,j) + 4.7 f(h,i2j) . (7) 8Institutions for which the expected frequency of selection exceeded one (1.00) were defined to be certainty selections, rather than allowing the possibility of multiple selections, because selecting multiple samples of students within an institution was considerable undesirable. Hence, the probability of selecting the j-th institution in stratum \"i\" among the set of certainty PSUs was where n2,c(i)S2(h,i,j) I Szc(+,i,+) if institution \"j\" was not a certainty selection for stratum \"i\" 7c2( h, ij) = 1 if institution \"j\" was a certainty selection for stratum \"i\" is the number of noncertainty institutions selected from stratum \"i\" among the 86 certainty PSUs, as shown in Table 2.4, and if institution \"j\" was a certainty selection for stratum \"i\" 12(h,i,j) = (11) 0 if institution \"j\" was not a certainty selection for stratum \"i\" . Within the set of 90 noncertainty PSUs selected for NPSAS:93, institutions were selected with probabilities proportional to the size measure, S2(h,i,j) / n1(h). As shown below, dividing the size measure, S2(h,i,j), by the probability of selecting the PSU, it1(h), resulted in overall institution-level probabilities of selection that were proportional to S2(h,i,j), comparable to two-stage sampling, even though a two-phase sampling process was implemented."}, {"section_title": "2-19", "text": "Institutions for which the expected frequency of selection exceeded one (1.00) were defined to be certainty institutions within the sample PSUs, as they were among the certainty PSUs. Thus, the conditional probability of selecting the j-th institution in stratum \"i,\" given that it was located in one of the 90 sample PSUs, was if institution \"j\" was a certainty selection for stratum \"i\" 12is the number of noncertainty institutions selected from stratum \"i\" among the 90 noncertainty PSUs as shown in Table 2.4, and"}, {"section_title": "h=87", "text": ".i=1 1 if the h-th PSU was a sample PSU Therefore, the overall, unconditional probability of selecting the j-th institution from stratum \"i\" of the IPEDS-based sampling frame was n1 Si(h,+,+) 1 S1,(+,,) 1 if institution \"j\" was a noncertainty selection within a noncertainty PSU if institution \"j\" was a noncertainty selection within a certainty PSU if institution \"j\" was a certainty selection within a noncertainty PSU if institution \"j\" was a certainty selection within a certainty PSU. 15Thus, if an institution was a noncertainty selection within either a certainty or a noncertainty PSU, the overall, unconditional probability of selection was proportional to the institution's measure of size, S2(h,i,j), within each institution-level sampling stratum \"i.\" Sample institutions were also selected from the supplemental OPE-IDS sampling frame with probabilities proportional to size (pps). The formulae for the probabilities of selection are essentially the same as for the selections from the IPEDS-based frame with the following exceptions. First, only two strata were defined: (1) the institutions within the 86 certainty PSUs and (2) the institutions within the 90 sample PSUs. Second, the size measures were computed differently, as discussed in Section 2.4.1. After identifying the 16 certainty institutions, the number of pps selections, n2,, from the 488 institutions in the 86 certainty PSUs was 38, and the number, selected from the 260 institutions in the 90 sample PSUs was 32. Finally, a subsample of three institutions was selected from each of the two strata, resulting in an additional subsampling factor in the formulae for the probabilities of selection."}, {"section_title": "2.6", "text": "Institutional Response Rates Eligible sample institutions were asked to participate in NPSAS:93 by: (1) providing lists of students for sample selection and (2) abstracting data from student records for sample students. Hence, the potential for institutional nonresponse existed at these two points in the survey process. The subsections that follow examine the occurrence of nonresponse at these two points in the study. The initial contact with the sampled institutions was a packet of materials sent to the Chief Administrator of each sampled school. Four types of packets were assembled based on 2-2 1 whether the institution had participated in earlier rounds of NPSAS and whether the institution granted the baccalaureate degree. An example of a packet for a new, baccalaureate-granting institution is displayed in Appendix B. The materials asked the Chief Administrator to designate an Institutional Coordinator for further contact. A diagram of the data collection steps appears in Figure 4.2. 2.6.1"}, {"section_title": "Response Rates for Student Sampling Lists", "text": "About 100 sample institutions agreed to provide lists of students for sample selection, and continued to say that they would do so each time that they were contacted, but never provided those lists. Hence, the tabulation of the numbers of institutions that agreed to provide student lists for sample selection. Table 2.6 shows that 1,243 of the 1,386 sample institutions were determined to be eligible for NPSAS:93 and that 1,197, or 96.3 percent, of them agreed to provide a list for sample selection. The rate of refusal was greatest among private, for-profit institutions (about 10 percent) and among less-than-2-year institutions (about eight percent), a theme repeated at each stage of data collection. Table 2.7 shows that 1,098 of the 1,243 eligible sample institutions provided a student list or data base that could be used for sample selection, although another nine institutions provided electronic files that could not be processed. Hence, 88.3 percent of the eligible sample institutions provided lists that could be used for sample selection. The percentage providing student sampling lists ranged from 73.8 percent for private, for-profit, less-than-2year institutions to 95.3 percent for public institutions with a Masters degree as the highest level of offering. Weighted response rates were calculated based on the institutional probabilities of selection. The weighted response rates can be interpreted as the estimated percentages of institutions in the population that would have provided a student sampling list, if asked. The overall weighted response rate is 88.2 percent, almost identical to the unweighted response rate (88.3 percent). For some of the institution categories in Table 2.7, there is a considerable difference between the weighted and unweighted response rates. This probably occurs because institutions were selected with probabilities proportional to their measures of size, leading to considerable variation in the institution-level sampling weights. 'Unreadable electronic files were obtained from nine additional institutions. 2-24 5;,-3 CHAPTER 3 STUDENT AND PARENT SAMPLING"}, {"section_title": "Student Eligibility", "text": "The students eligible for NPSAS:93 were those who were enrolled in, or were receiving a baccalaureate degree from, an institution eligible for NPSAS:93 during the 1992-93 academic year. The specific eligibility conditions are delineated in Figure 3.1. However, students enrolled in high school or solely in a GED program were ineligible for NPSAS:93, even if they Oso satisfied the conditions listed in Figure 3.1. About the only other types of students enrolled in institutions eligible for NPSAS:93 who were not themselves eligible were those enrolled only in avocational or recreational courses or enrolled only in courses of short duration not leading to any degree or other formal award."}, {"section_title": "Figure 3.1 Students Eligible for NPSAS:93", "text": "Students attending an institution eligible for NPSAS:93 who: were enrolled in at least one of the following at any time between July 1, 1992 and June 30, 1993: course(s) for credit toward a degree or formal award; degree or formal award program of at least 3 months duration; or an academically, occupationally, or vocationally specific program requiring at least 3 months or 300 clock hours of instruction; Plus all students who: received a baccalaureate degree between July 1, 1992 and June 30, 1993 ] Students who completed baccalaureate degree requirements prior to July 1, 1992 but may not have attended classes after July 1, 1992 were eligible]. Note: To facilitate the data collection schedule, enrollment lists included students who were enrolled in any term or course that started on or after May 1, 1992 and started no later than April 30. 1993. From the standpoint of including all students receiving financial aid funded during the 1992-93 federal financial aid award year, the ideal target population would include all students enrolled in an eligible course of instruction that began between July 1, 1992 and June 30, 1993. However, the survey population was restricted to students enrolled in courses that began between May 1, 1992 and April 30, 1993 to facilitate receiving lists of students for sample selection in the Spring of 1993."}, {"section_title": "3-1", "text": "This definition of the survey population provides reasonable comparability with the survey populations for NPSAS:87 and NPSAS:90. Only students enrolled in fall 1986 were sampled for NPSAS:87. Students enrolled on August 1, 1989;October 15, 1989;February 15, 1990;or June 15, 1990 were sampled for NPSAS:90, except that the June 15 enrollees were not sampled for 4-year institutions because of budgetary limitations."}, {"section_title": "3.2", "text": ""}, {"section_title": "Student Frame Construction", "text": "Each eligible sample institution was asked for a list of all enrolled students who satisfied the eligibility conditions listed in Figure 3.1, excluding students enrolled in high school or solely in a GED program. The institutions were asked to provide, if possible, an unduplicated, machine-readable list of all eligible students in alphabetical order. The institutions were asked to provide for each student: full name; student identification number; most recent educational level (undergraduate, graduate, or first-professional); indicator if the student was a candidate to receive a baccalaureate degree between July 1, 1992 and June 30, 1993; and major or field of study for baccalaureate candidates. When institutions were not able to provide unduplicated lists, separate lists of students for each term or course of instruction plus lists of baccalaureate candidates were accepted. When institutions were not able to provide machine-readable files, hard-copy lists were accepted. Significant deviations from the numbers of students expected, based on IPEDS counts, were verified by the schools to ensure the quality of the lists used as student sampling frames."}, {"section_title": "3.3", "text": ""}, {"section_title": "Student Sample Selection", "text": "The basic student sampling procedure was to select a systematic sample of students at fixed stratum sampling rates from either hard-copy or machine-readable lists of students arranged in alphabetical order within strata. Systematic sampling was used primarily because of its ease of implementation with hard-copy lists. The student sampling rates, rather than the sample sizes, were fixed for each sample institution for three reasons: (1) to facilitate selecting student samples on a flow basis as lists were received, to facilitate the procedures used to \"unduplicate\" the samples selected from hard-copy lists, and 3because sampling at a fixed rate based on the overall stratum sampling rate and the institutional probabilities of selection results in approximately equal overall probabilities of selection within the ultimate student strata. Whenever an institution provided a separate hard-copy list for each term of enrollment or for each course of instruction, the sample was selected in such a manner that each student had a positive probability of selection from only one of the lists provided. The lists were first ordered for processing. If there were separate lists of baccalaureate recipients, those lists were processed first. Otherwise, the generally preferred ordering was: Fall 1992, First"}, {"section_title": "3-2", "text": "Summer Session 1992, Second Summer Session 1992, and Spring 1993 unique order satisfied the requirement of giving each student only one chance of selection from the institution's lists. A sample was selected at the fixed stratum sampling rate(s) from the first and second lists. The sample selected from the second list was checked against the complete first list, and any members of the sample from the second list that were on the first list were deleted from the sample selected from the second list, thereby \"unduplicating\" the sample. In the same manner, the sample from each subsequent list was unduplicated against all previous lists. This unduplication procedure guaranteed that any student found on multiple lists could only be selected from one list. The target numbers of eligible sample students that were to be selected for the NPSAS:93 full-scale study are presented below by type of student. The estimated total number of students of each type in the survey population, based on the 1990-91 IPEDS IC file, and the resulting overall student sampling rates are also presented. The numbers of eligible sample students actually selected are presented for comparison. The observed or actual number of eligible students exceeds the target number for all types of students except first-professional students. This happened because sampling rates were based on conservative estimates of eligibility rates and because the total enrollment in postsecondary institutions increased between the 1990-91 and the 1992-93 academic years. The relationship between target and actual counts is not entirely consistent because of sampling variability.  Table 3.1 presents these target numbers of eligible sample students by the 22 institutional sampling strata for each of the five types of students: (1) business baccalaureate recipients; (2) other baccalaureate recipients; (3) other undergraduates, including enrollees at less-than-4-yr institutions; (4) graduate students; and (5) first-professional students. The student sample sizes needed to achieve this sample allocation are presented in Table 3.2 for 29 student sampling strata defined by institutional stratum and the above five student levels. Includes business baccalaureate recipients."}, {"section_title": "3-3", "text": "'1 0  More than 25 percent of baccalaureate degrees awarded in education. Table 3.2 also presents the resulting overall student sampling rates. The allocation to strata was determined to minimize the differences in overall student sampling rates, subject to the constraint of achieving the sample sizes shown in Table 3.1. Because of unresolved inconsistencies in the IPEDS-based sampling frame, Tables 3.1 and 3.2 show that some firstprofessional and graduate students were projected to be selected from institutions classified as not offering those leveis of instruction. When determining the student sampling rates, some of the students on the graduation lists received from the sample institutions would not actually receive their baccalaureate degrees during the NPSAS academic year (degrees awarded between July 1, 1992 and June 30, 1993). Based on the NPSAS:93 field test data, we estimated that 93 percent and 2.5 percent of the students selected from the baccalaureate recipient strata and from the other undergraduate stratum, respectively, among 4-year institutions would actually receive their baccalaureate degrees during the NPSAS academic year. Assuming these rates, the numbers of additional baccalaureate recipients from the other undergraduate stratum would more than compensate for losses from the baccalaureate recipient strata because of the much larger sample size for other undergraduates. Therefore, the student sampling rates shown in Table  3.2 were used to select the student samples for the NPSAS:93 full-scale study. However, in the full-scale study the losses due to baccalaureate candidates not receiving their degrees were not completely offset by students sampled as other undergraduate students who received baccalaureate degrees. The numbers of sample students actually selected are presented in Table 3.3 by the 22 institutional sampling strata for each of the five types of students. The total number of students selected, 82,016, is somewhat greater than the targeted total number of eligible sample students, 77,875, shown in Table 3.1 to compensate for the expected rates of student ineligibility based on the NPSAS:90 experience. Because the stratification information for the 1990-91 IPEDS IC file was not perfect, some baccalaureate recipients were selected from institutions stratified as 2-year or less-than-2-year institutions and that graduate and firstprofessional students were occasionally selected from institutions classified as not offering those levels of instruction (see Table 3.3). These misclassifications have minor effects on statistical efficiency, but have no effect on the validity of the study. Institutional analysis domains are based on the data collected in the NPSAS:93 study, not on the sample selection strata. 'More than 25 percent of baccalaureate degrees awarded in education. 'One institution sampled as a 2-year institution (based on the (PEDS IC file) was determined to bc a 4-year institution. It is classified as such in all NPSAS:93 analysis tables. 'One institution sampled as a less-than-2-year institution (based on the IPEDS IC file) was determined to be a 4-year institution. It is classified as such in all NPSAS:93 analysis tables."}, {"section_title": "BEST COPY AVAILARU", "text": "3-7 3.4"}, {"section_title": "Probabilities of Selection", "text": "To define the student sampling rates, let = the ovci all probability of selecting for the j-th institution from the i-th institutional stratum (ignoring the area PSU \"h\"), nk = the desired number of eligible sample students to be selected from student stratum \"k\" (k = 1, 2, ..., 29, as shown in Table 3.2), Nk = the total number of eligible students in the population for student stratum \"k,\" and njk = the number of students selected from the j-th institution for the k-th student sampling stratum. The overall population sampling rate among eligible students in student stratum \"k\" is then 1k= nk/ Nk For the unconditional probability of selection to bc a constant, rk, for all eligible students in stratum k, or equivalently, where Nik is the number of eligible students in stratum \"k\" at institution \"j.\" Thus, the conditional sampling rate for stratum \"k,\" given selection of the j-th institution, becomes However, in this case, the desired overall student sample size, nk , is achieved only in expectation over all possible samples."}, {"section_title": "3-8", "text": "To achieve the desired sample sizes with equal probabilities within strata in the particular sample that has been selected and simultaneously adjust for institutional nonresponse and ineligibility, then njk = nk jeR where \"R\" denotes the set of eligible, responding institutions. If the conditional student sampling rate for the k-th stratum in institution \"j\" is Because it was necessary to set the student sampling rates before complete information on eligibility and response status was obtained, &k was calculated as follows: where \"S\" denotes the set of all 1,386 sample institutions, E, = R, = Eik = the institutional eligibility factor for institutional stratum \"i,\" the institutional response factor for institutional stratum \"i,\" the student eligibility factor for student stratum \"k\" within institutional stratum \"i.\" (25) Using the known institutional probabilities of selection, ir, and the student sample sizes, nk, shown for each of the 29 student sampling strata shown in Table 3.2, the sampling rate for student stratum \"k\" in institution \"j\" was calculated using eligibility and response rate factors E R and Elk, based on the NPSAS:90 experience, except when an institution's eligibility or response status was already known for NPSAS:93. The sample was initially allocated as described above. This allocation achieved the desired sample sizes for all student strata with equal weighing allocations to institutions within student strata. However, at least 30 responding students were desired, whenever possible, at each sample institution so that they could be sent a report regarding their students. Such reports are a benefit to the institutions and encourage their participation. Based on NPSAS:90 student eligibility and response rates, the cluster sizes (within institution sample sizes) needed to achieve 30 respondents were derived by type of institution. The initial sampling rates were then revised to achieve, whenever possible, an expected total sample allocation of at least 40 students for 4-year institutions, 45 students for 2-year institutions, and 50 students for less-than-2-year institutions. When a minimum was imposed for an institution, that was done by multiplying the sampling rates, i'jk, for all five types of students by a fixed constant so that the sampling rates were proportionately increased for all types of students. When the sampling rate for one type of student reached 100 percent without achieving the required minimum expected sample size, the stratum sampling rates were arbitrarily increased, as needed, to achieve the minimum (e.g., setting the rates to 100 percent for all types of students). After the student sampling rates had been set for the institutions with fixed minimum allocations, the allocations for the remaining institutions were recomputed using the original algorithm (achieving equal weighing within strata) based on the reduced sample sizes remaining to be allocated for each of the 29 student sampling strata. Finally, the overall population sampling rates were used to set non-zero sampling rates for all five types of students for 2-year and less-than-2-year institutions so that positive sampling rates would be available whenever those institutions had been misclassified. Thus, the sampling rates, i-jk, were computed from (18) and (20) using the following sample sizes as nk for those institutions: (1) 1,620 business baccalaureate recipients; (2) 14,571 other baccalaureate recipients; 3-10 (3) 9,000 graduate students; (4) 5,500 first-professional students; and computing f1k by summation over all sample institutions. As a check on the effect of constraining the sampling rates to produce the above expected minimum student sample sizes, we computed the survey design effects resulting from unequal probabilities of selection for both the initial (unconstrained) and final (constrained) sample allocations for the following analysis domains: (1) the total sample (2) baccalaureate recipients at 4-year institutions (3) all undergraduates (including baccalaureate recipients) at 4-year institutions (4) graduate students (5) first-professional students (6) students at 2-year institutions (7) students at less-than-2-year institutions. As shown in Table 3.4, the minimum sample size constraints resulted in very little variance inflation, as measured by the unequal weighting design effect, except among the less-than-2year institutions."}, {"section_title": "Student Sample Quality Control", "text": "To help ensure the overall quality of the samples selected, the numbers of students on the lists or files provided by the sample institutions were compared to counts based on the IPEDS files.2 In addition, lists were checked to make sure that the following information needed to process the sample was received: student name, ID number, level (undergraduate, graduate, first-professional, or baccalaureate candidate), and major for baccalaureate candidates. When major discrepancies were detected, we called the institutions to determine if they had provided lists for all the proper terms of enrollment and for all the different types of students. Figure 3.2 provides an overview of the quality assurance (QA) procedures that we used to determine when a telephone call to a sample institution was necessary. The tolerance range for the count for each type of student depended on whether or not the corresponding count from the IPEDS files was considered imputed or actual data. Less stringent tolerances for imputed counts were used because they were considered less reliable than reported counts. (Imputation procedures are usually designed to produce correct results only on the average over all possible imputations.) Hard-copy lists were checked prior to sample selection using tolerance ranges that allowed for potentially duplicated counts (e.g., persons appearing on both the Fall and Spring enrollment lists). To help ensure adherence to our sampling procedures, Research Triangle Institute staff checked the sample sizes from hard-copy lists prior to being sent to data entry. These post-sampling checks are summarized in Part II of Figure 3.2. RTI staff checked machine-readable lists only after they had been unduplicated, and a sample had been selected. If the sample size was outside of the tolerance range and the files provided were determined to be incorrect, the sample was discarded and not used. Otherwise, if the sampling files were determined to be correct, the sample was retained. All samples (hard-copy and machine-readable) with fewer than 10 students or more than 100 students greater than expected were rejected. RTI staff usually reset the sampling rates for these institutions, unless RTI staff had already selected all eligible students, even when the institutions verified that the lists they had provided were correct. RTI staff evaluated the QA procedures in early May after about 20 percent of the lists were received. At that time, about 70 percent of the lists received were outside the initial tolerance ranges for at least one type of student and required telephone follow-up with the institutions. However, only about six percent of these institutions reported that the lists they provided were incorrect. Because most of the incorrect lists had student counts which varied dramatically from the IPEDS counts, the QA tolerances were relaxed on May 11, 1993, as shown in Figure 3.2. About two-thirds of the sample was processed using these relaxed QA tolerances. The QA procedures were evaluated again in early August and found that about 50 percent of the lists were still failing the relaxed tolerance checks. As a result, RTI staff discontinued range checks for imputed IPEDS counts and further relaxed the checks for real IPEDS counts. Approximately 12 percent of the sample was processed using these final relaxed QA procedures. At the conclusion of the sample selection process, RTI staff selected samples for about 12 institutions based on whatever list RTI staff were able to obtain from the institution, without regard to tolerance intervals."}, {"section_title": "Parent Sampling", "text": "A survey of the parents of some of the students sampled for NPSAS:93 was conducted to collect supplemental data for use in student-level analyses. Parent-level inferences were not a study objective. There were two primary objectives that influenced the sample design for the parent survey. The first objective was to provide supplemental data on financing the postsecondary education of the student, focusing on those data elements that were not known from institutional sources and for which the student was not the best source of information. The second objective was to provide more complete family background data for graduating seniors, who form the initial cohort for the Baccalaureate and Beyond (B&B) longitudinal 3-12 study. An additional secondary objective was to obtain data that could be used for modelling the impact of changes in parameters that determine who is eligible for financial aid and how much aid is received. To achieve these objectives the sample design for the parent survey targeted the parents of specific subgroups of students and excluded the parents of other subgroups. The parents of graduate and first-professional students and of all students who were 24 years of age or older were excluded from the parent survey.3 The parents of all students under 24 years of age who satisfied either of the following conditions were included with certainty: the student was a graduating senior, or the student was a dependent, undergraduate student for whom the parents' total family income from all sources in 1991 was not available from the CADE abstraction of the student's records. In addition, the parents of approximately 56 percent of the aided, independent undergraduate students under 24 years of age were included in the parent sample. This sampling rate was intended to produce about 2,000 completed interviews with this group of parents. Table 3.5 provides more specific information about how the parent sample was implemented. Based on M_STDB from the student data abstraction. hBased on BAB from the student data abstraction. 'Based on M_C13 from the student data abstraction, or the student sampling stratum when M_CI3 was missing. dBased on the student data abstraction. Based on PRN20 from the student data abstraction. 6 7 3-14"}, {"section_title": "CHAPTER 4 Institutional Records Data Collection", "text": "During the institutional records data collection portion of the survey, data were obtained from student financial aid records and other administrative records maintained by the institutions. The survey design called for institution staff to complete this in as many institutions as practical; when institution staff were unable to complete the task, field staff were sent to the site to complete the institutional records data collection . As described above, software was developed to facilitate this activity. The software was designed to be used by the instiuition staff, but could be used by field staff as well. The field period was originally scheduled to begin in May of 1993; however, because of delays in obtaining the student sample frame, this task did not begin until late June of 1993 and was not completed in the majority of the institutions until October of 1993."}, {"section_title": "4.1", "text": ""}, {"section_title": "Objectives", "text": "The purpose of the institutional records collection was to gather student-level data describing each student's periods of enrollment, expected education-related expenses, resources available for financing his or her education, and financial aid that was made available to the student. Also, the NPSAS:93 project needed to obtain locating information in order to conduct the telephone interviews of students and parents. The survey year was defined as July 1, 1992 through June 30, 1993, which corresponds to the 1992-93 award year for federal financial aid. The primary source of this information consisted of administrative records and documents maintained on a routine basis by institution staff. These included student directories, enrollment files, application forms and output documents, budgets and needs analysis, award letters, and other miscellaneous documents contained in student financial aid folders. It was necessary to collect locating information so that students and their families could be contacted for the telephone interview portion of the survey. In addition to the student's local address, the institutional records collection software requested a permanent address, the address of the student's parents (if different from the permanent address), and the address of another person who would be knowledgeable of the student's whereabouts. Detailed information related to student enrollment was collected, including beginning and ending dates of terms of enrollment, type of program (credit hours or clock hours), degree program, student's status (full-time or part-time), and field of study. In institutions where every student followed the same pattern of terms (as in a semester or quarter system), beginning and ending dates of terms were entered once at the institution level and then preloaded into each student's record depending on the terms enrolled. For other institutions where beginning and ending dates of periods of enrollment were not standard for all students, this information was collected on a student-by-student basis. For students in the B&B cohort, expected date of graduation was also requested.\nThe additional data collected from students and parents are required in the NSPAS for several reasons. First, the information abstracted from the sampled institutions may represent only a portion of the financial aid received by students during the NPSAS study year either because the institution may not be aware of all sources of financial aid or because students may attend more than the sampled institution during the NSPAS year. Second, onc purpose of NPSAS is to learn more about how students and their families finance postsecondary education and financial aid is only one mechanism. Student and their families are the only knowledgeable source of information on how individual families plan for educational expenses. Third, another research issue of the NPSAS is how financial aid and other financing mechanisms can affect student plans for the future, including additional education, entry into the labor market, and family formation. Both the student and the parent interviews were conducted using dedicated CATI-LAN-based software. The system provided the following key features for the data collection activities: On-line access to locating information and history of locating efforts for each These capabilities reduced the number of discrete stages required in data collection and preparation activities, and increased capabilities for immediate error reconciliation."}, {"section_title": "4-1", "text": "In 1992-93, several companies as well the federal government processed application forms ad returned the information to the institutions on an output document. The standard application forms and the corresponding output documents are summarized in Figure 4.1. To facilitate data entry, the output documents were replicated in the design of the institutional records collection [CADE] software.  To allocate student aid, institutions must calculate each student's need for aid, defined as the difference between the cost of attendance and expected contribution from the student or family. In 1992-93, two methods of computing the costs of attendance were in general use: Pell Grant Cost of Attendance (Pell Budget) and Congressional Methodology (CM Budget). In addition, institutions can develop their own Institution Budgets, which often follow CM guidelines but employ some variations based on unique needs of the institution. The amount and type of aid awarded to students are documented in the Award Letter. There is no required format for an award letter. However, these letters typically include the following items: Student identification: the student's name, address, social security number, institution identification number; Award information: the type and amount of aid being offered, often broken down by enrollment periods; and Need analysis information: the student's cost of attendance budget, expected family contribution, financial need before awards, total awards, an remaining unmet need. In addition, the award letter requires the student to respond either by accepting or rejecting the award by a given deadline. Acceptance or rejection of the award is typically documented in the student file."}, {"section_title": "4.2", "text": ""}, {"section_title": "Institutional records collection CADE Design", "text": "The institutional records collection software --computer assisted data entry or CADE -was designed for use by institution staff in abstracting information from these types of documents. The software had to be compatible with a wide variety of computers that were likely to exist in financial aid offices in 1993. CADE was designed for use with IBMcompatible minicomputers, with a high-density disk drive, and at least 540K of memory. It was necessary to assure institution users that the use of the NPSAS CADE software would not disrupt files already stored on their computers. For this reason, CADE was designed to operate entirely from a disk drive and did not require installation on a hard drive. In addition, all diskettes were scanned for viruses prior to sending them to institutions. Finally, it was necessary to minimize the storage requirements for the data entry software, the list of sample students, and the abstracted data so that users did not have to keep track of multiple diskettes. In fact, in some of the largest institutions, two diskettes were required to transmit the software and data. CADE was designed to function as a data-entry program and contained many features to assure the quality of data entry. The software routed the user to various sections of CADE based on responses to filter questions. For example, if the user indicated that the student did not accept any aid during the NPSAS year, specific questions about the amount and source of aid were automatically skipped. For most of the items, instructions or explanations appeared in \"pop-up\" boxes which appeared as the item is presented to the user. These boxes included valid response codes and explanations and provided definitions of terms. Many questions contained edit specifications that checked the response against either a range of acceptable responses (range checks) or responses to previous items (inter-item consistency checks). Edit check routines in the software presented a question to the user if the response was outside of an expected range or was inconsistent with another response; however, for many items, users could override the edit and enter the unexpected response. This kind of \"soft\" edit was necessary to account for situations where the actual data in the student's record might be inconsistent with expectations. For example, the expected range of responses for Pell Grant awards was between $200 and $2,400. If the student actually received a grant of $175, the user would be warned: 1175 is outside the expected range. Please check your entry!\" However, after checking that the amount was recorded accurately, the user could verify the response and proceed with the data entry. A few items were deemed so critical to the study that an answer was required in order to continue with the data entry. For example, the question \"Was this student awarded any financial aid for terms that began between May 1, 1992 to April 30, 1993\" had to be answered as either \"Yes\" or \"No\" in order to proceed with data entry. The user could not skip this item."}, {"section_title": "4-3", "text": "The first CADE menu presented to the user contains three options for entering either institutional-level information or student-level information or checking on the status of each sampled student. The Institution Information section of CADE requested information about the sampled institution that would be relevant to all students enrolled in that institution. This information included names and beginning and ending dates of terms of enrollment, whether the institution made separate awards for the summer terms and, if so, the beginning and ending dates of primary and summer terms, and whether courses were measured in terms of credit or clock/contact hours or both systems or some other system. In many institutions, this information was the same for all students in the institution and if this were the case, it was preloaded into the student-level sections to avoid unnecessary duplicative data entry tasks. However, in some institutions, this information could vary from student-to-student and had to be entered separately for each student. The information concerning terms of enrollment was preloaded from the institution receipt control module of the ICS. The data were obtained either from responses to the initial mailout to chief administrators or follow-up calls with chief administrators or NPSAS institutional coordinators. The second option on the menu presented the user with the student-level portion of the CADE software. At this level, CADE consisted of six modules requesting data on: Student Addresses, with fields for up to four names, addresses and telephone numbers (student's local address, permanent address, parent's address, and another address); Enrollment during the study year, with fields for dates of enrollment, attendance status (full-time/part-time), credit or clock hours, tuition and fees, type of program degree, student level, program name, and most recent major or field of study (and expected date of graduation, for B&B cohort only); Student Characteristics, requesting student gender, race, ethnicity, social security number, high school degree or equivalent, citizenship, admissions test scores, and student's grade point average; Financial Aid Award Information, requesting information about amounts and sources of financial aid awarded to the student; Need Analysis and Budget, used to record information from the Pell, Congressional Methodology, or institution budgets; Financial Aid Application Information, abstracted from the relevant output document completed for the student. The data requested in each of these modules could exist in any of several locations on the campuses of institutions, for example, address information and enrollment information might reside in the registrar's office and data on awards in the financial aid office. For this reason, CADE was designed so that each module could be completed for all students at once. Alternatively, if all of the records did reside in one location, the entire CADE questionnaire could be completed on a student-by-student basis. At the opening screen of the student-level section, the user was presented with a list of the sampled students which could be sorted either alphabetically or by the institution's student identification number. The user selected a student and the module of interest. A display also indicated for each student which modules had been fully or partially completed and which remained empty. This indicator was a useful reminder for the user in keeping track of modules completed on each student. The Status Monitor section of CADE served a similar purpose. This section presented a summary in percentages of eligible students with complete or partially complete records and indicated what percentage of eligible students were missing key information such as telephone numbers and financial aid awards. A function in the Status Monitor allowed the user to flag a student as ineligible for the study, as might happen when a student dropped out of the institution before attending any classes during the study year. A list of CADE data elements appears in Appendix A."}, {"section_title": "Institution Data Collection", "text": "As described above, the CADE was designed for use by institutional staff in abstracting information from student records. In 483 of the 1,078 institutions that supplied CADE data (45%), this was the method of CADE institutional records collection ion. In these institutions, the tasks recruiting the institutions and institutional coordinators, instructing them in the use of CADE, and providing technical support during the records abstraction were all handled by mail or telephone. At the close of the institutional records collection task, the institutional coordinator sent the completed CADE diskette to the central office. Receipt control and quality control of this effort are described below in section 4.4. Of the remaining institutions, 512 (47%) required a visit from field staff to complete the institutional records collection and 83 (8%) were completed by abstracting in the central office copies of student records supplied by institutions (Figure 4.2). Field data collectors specially trained field staff --completed the records abstraction task using CADE on laptop computers. The self-administered CADE sent to institutions on diskettes and the field data collector CADE used with laptops were identical. In addition to CADE, the laptops contained communications software that allowed field data collectors to transfer files electronically using password-protected compressed files sent over telephone lines to the central office. Compatible software in the \"host\" computer in the central office received files, created institution-level directories, stored the files by institution, and read information from the status monitor into the receipt control system to automatically update the status of records collection at each institution.  "}, {"section_title": "Field Manager Recruitment and Training", "text": "Because so much depended on the collection of institutional data, recruiting proficient field managers was a critical task. Abt and RTI reviewed their combined networks of experienced, proven field staff to identify individuals who had the skills necessary to facilitate a high response rate in the data collection task. Field managers were selected based on their experience with studies involving institutions, particularly educational institutions, and for their capacity to achieve demanding quality standards for data collection while at the same time maintain efficient operations; the ability to control costs and hours per case was an important factor in the selection process. Field managers needed to know how to trouble-shoot difficulties that emerged in the data collection process; they had to quickly resolve problems related to securing the cooperation of institutions. Field managers were responsible for helping the field interviewers navigate the institution's labyrinth in which the student information was stored, in order to retrieve the required. The field managers were the liaison between the interviewers and the technical staff in the central office, so they had to be able to develop solutions to problems interviewers had while learning how to use laptop computers and the CADE system. The field managers played an important part in recruiting and training their own interviewer staff, so field manager candidates were judged on their ability to select and train interviewers. A manual for the field managers was developed. The manual covered all the manager's responsibilities and dealt with the specifics of data collection operations. The manual explored topics such as gaining cooperation, institutional records abstracting, reporting procedures and professionalism. One chapter dealt with the CADE system, featuring a series of practice exercises. The manual served as a framework for the training program that prepared field managers for their role. A four-day training session was conducted for all the field managers from RTI and Abt to assure consistent training across both firms. This session provided a foundation for the institutional records collection phase of the study. Because the field managers received all the training that was to be given to the field interviewers, the manager training also served as a pilot test of the interviewer training. The session generated enthusiasm for the study among the field managers. They were introduced to NPSAS and its purpose, and their responsibilities for making the study work. They were grounded in the elements of financial aid at the postsecondary level. Field managers were thoroughly schooled in the job of the field interviewers, so they could understand the interviewers' tasks and help them resolve problems and overcome obstacles presented in the course of the study. Learning the CADE program for data collection was a central focus of the training. Much of the training was devoted to practice using the software, in exercises involving realistic simulations of the situations that the interviewers were expected to encounter. These simulations, exercises developed by staff from the National Association of Student Financial 4-7 Aid Administrators (NASFAA), used the different sources and formats of student data (such as financial aid forms, enrollment rosters and transcripts) and included all phases of the data collection process, from preloading the institutional data to transmitting a completed data set to Abt's central office. NASFAA were also present to lead portions of the training sessions and provide commentary or responses to questions in other sections of training. At the end of each day, field managers and trainers discussed the day's activities; in this way, the field managers shaped the training program for the interviewers. Also, the training brought unresolved issues into focus; the field managers and the trainers developed procedures based on their discussions. Field managers were taught about the intricacies of developing contacts with the institutions, notably working with an institution's chief administrator and study coordinator, and scheduling a convenient time for the institution visit. Issues concerning data collection in an institutional setting, such as professionalism and confidentiality, were stressed. Each section of the CADE system was covered: student addresses, characteristics and enrollment data, as well as needs analysis and student budgeting. Each of the standard financial aid application forms was reviewed. Extensive opponunities to practice the application of these lessons were provided, using CADE and mock student data; this provided the field managers with an understanding of how to abstract the student data, as well as how to master the CADE system. During class, the training was usually conducted as a seminar: the trainers and the field managers worked together to solve the problems. At day's end, homework was assigned, so field managers could reinforce the lessons presented during class. Also, the field managers were instructed in administrative procedures related to the study. They were taught how to communicate using electronic mail to keep central office apprised of their progress and their problems, as well as keep in close contact with the field interviewers. They were taught how to evaluate field intet viewers. Field managers were taught how to prepare time and expenditure reports and the procedures for planning travel, as well as how to monitor costs and production."}, {"section_title": "Field Data Collector Recruitment and Training", "text": "Field data collectors were recruited from the ranks of Abt and RTI interviewers. Although field staff recruitment occurred before institutions elected to participate as either self-administered or requiring field staff, location of the interviewer was nonetheless a criteria for recruitment to NPSAS. Because the institutional records collecttion required travel to the campuses of participating institutions, a geographic spread of field data collector staff was desired to minimize expenses associated with travel and overnight stays. In addition to location, staff were recruited based on experience with education institutions or with record abstract tasks in other types of establishments (e.g., hospitals). Field data collector training followed the same format and content as described above for field managers. 75 4-8"}, {"section_title": "4.3.3", "text": "Field Procedures --Institutions Requesting Field Data Collectors Field visits were required whenever an institutional coordinator requested this assistance. Typically, the choice between the self-administered and field data collector method occurred early in the process, however, in several instances, an institution switched from the selfadministered to the field staff method after they received the CADE diskette. In either situation, the field visits followed essentially the same format. Field data collectors received the assignment of sampled student records on a laptop computer that included both the CADE record abstraction software and case-management software (described below), during the initial visit with the institutional coordinator, the institutional portion of the CADE was completed and field data collectors were briefed about the sources and location of student level information. Following the record abstraction task itself, files were transmitted back to the central office electronically."}, {"section_title": "Remote Management System", "text": "In addition to the CADE software used in the record abstract process, the laptops used by the field data collectors also contained Remote Management System (RMS) software for managing their workload of multiple institutions and electronic transfer of files and electronic mail for communication with the central office staff, field managers, and other field data collectors. The RMS consisted of three functional modules. The Manage function kept track of the student files of each institution in the field data collector's assignment, names of files for each institution, and the dates of transmission. The Manage function was used to load institution files into CADE and prepare files for transmission to the central office. A Toolbox function was used to copy files onto back-up diskettes initiate transmissions to the central office and perform basic utilities such as formatting diskettes or installing updated versions of CADE. A Newsletter was also available through the RMS to provide field data collectors with updated information on technical or administrative topics. The RMS was used to transfer files of sampled students to the field data collectors in order to initiate data collection activities for a particular institution. The software automatically updated the institution receipt control system in the central office, noting the date that each file of sampled students was mailed to the field and the date of receiving files of completed records. The RMS also allowed each field data collector to load a student sample file into CADE in order to begin work at an institution."}, {"section_title": "Initial Meeting with NPSAS Institution Coordinator", "text": "Each field data collector had the responsibility of scheduling data collection with the institution coordinator designated by the chief administrator of the institution. The initial meeting with the coordinator typically occurred the morning of the first day of data collection at the institution. The purpose of this meeting was primarily logistical so that the field data 4-9 collector became familiar with the location administrative records and daily routines of key staff at the institution. The Institution Information section of CADE was completed during this interview with the coordinator. In addition, a check list was reviewed so that the field data collector could learn the sources of information required by the survey, the hours that the information would be available, the name and telephone numbers of a contact person at each office, and the medium used to store data (computer files, hard copy, microfiche, etc.). The purpose of this checklist was to assure that the field data collector had the information necessary to complete the record abstract task with a minimum of disruption to the institutional coordinator and staff."}, {"section_title": "Record Abstraction", "text": "Following the initial visits, the task of the field data collectors was tracking down the appropriate student records and abstracting necessary information into the CADE software. In institutions that maintained integrated records, this task was straight-forward and could be completed in a relatively brief period on campus. In other situations, records might be located in different offices at various locations on campus and record abstraction could take as long as a week.\nOnce the enrollment and graduation files were provided, student samples were selected for each institution on a flow basis. A total sample of 7,953 students was selected for the record abstract process and ultimately for the student telephone interview (refer to section 2.2 for further discussion ols the telephone survey). Several types of resistance to the use of CADE were encountered. As anticipated, in some cases, the admissions office or the financial aid office did not have access to a personal computer compatible with the CADE software. Administrators who did have access to appropriate equipment had concerns about how tne external software might affect existing files or programs on their machines. Institutions that indicated reluctance to use the CADE method in the return postcard were contacted by telephone in an attempt to persuade them to reconsider. In the field test, various procedures were explored to overcome anticipated resistance to use of the CADE method. Figure 8.2 indicates the changes in the choice of CADE method among institutions at three-week intervals during the course of the field test. These data show that there was variation across time in the preferred CADE method. In July, the modal option selected was 8-14 self-administered CADE, but by November the modal choice was for a field interviewer to conduct the CADE abstraction. This is in large part due to institutions that agreed to the selfadministered method but then asked to have a project field data collector complete the task. One finding that is important to note here is the variety of actors who may get involved in the NPSAS data collection. Our first contact was with the chief administrator of the institution who, in general, was the individual responsible for making the decision to participate in the study. The second contact was with the person named as the project's institution coordinator. This was the individual with whom we discussed the data requirements of the study and the options for abstracting administrative data. In the larger institutions, and in some smaller institutions as well, the information requested in the CADE record abstract was not maintained in a single office within the institution. Because the initial request was for enrollment data, an individual in the registrar's office may have been named as the institutional coordinator. This person may have had little knowledge of the administrative files maintained by the office of financial aid so it was only when the record abstract process was initiated in the financial aid office that it was determined that the selfadministered method was not appropriate. The resulting summary of abstraction methods chosen by institutions in the field test is shown below in Table 8.6. Of the 70 institutions providing student enrollment and baccalaureate lists, sixty percent, or 42 institutions, opted to have CADE records abstraction conducted by a field interviewer. The method originally proposed in the NPSAS:93 study design --self-administered CADE --was selected by only 20 percent of participating institutions. If the trend found in Table 8.6 holds, these results indicate that a major shift may be required in the procedures used to implement the full NPSAS, because nearly 2 out of 3 institutions participating in the fic Id test selected a very different, much more expensive mode for entry of the results of record hstraction.  Figure 8.3 indicates the date of completion of student record abstractions. This chart clearly indicates variability in the timing of completed CADE record abstractions. In particular, the average time span for 13 completely self-administered institutions to complete the CADE and return the data was 7.88 weeks. It is important to note that this figure is nearly double the four-week period used in planning the field test. Table 8.7 shows the number of complete student records obtained through the record abstract portion of the institution survey. Of the original sample of 7,953 students, usable record abstracts were obtained for 7,785 students. The difference of 168 includes cases from an institution that refused to complete the record abstract task after sending in an enrollment file (119 cases) and 49 cases from participating institutions that were not complete. Of the cases with usable record abstracts, a net sample of 7,417 students eligible for the telephone interviewing component of the NPSAS: 4,177 from public institutions, 3,032 from private institutions, and 272 students from private, for-profit institutions. Of the total 7,953 selected cases, 4.7% of students were ineligible, as indicated by Table 8.7; 93.3% of the selected student sample resulted in final record abstract (final CADE) record. Comparison of CADE diskettes completed by institution staff and by field data collectors, completed during the editing of record abstract data prior to loading into CATI, showed no differences between these types of CADE users in the field test. Once agreeing to complete the record abstract task, institution staff were conscientious about providing all of the requested data. Similarly, except in some unusual circumstances were data were simply not available, field data collectors were able to track down the information requested in CADE. Thus, as a rule, most of the sections of the CADE record abstract software were completed either by institution staff or by field data collectors. An exception was the section requesting financial aid information on baccalaureate recipients for as long as they attended the sampled institution and for financial aid transcripts from other institutions that they may have attended. The intent of this section was to be able to build a history of financial aid for the B&B student's undergraduate experience. In most of the institutions, this information was simply not available in a way that was amenable to efficient record abstraction either by the institution staff or by field interviewers. Week Received Dates"}, {"section_title": "4.3.4", "text": "Institutions That Used CADE Institutions that elected to provide the information themselves were mailed the CADE diskette (including the sample of selected students) together with brief instructions on how to install the CADE software and its use. As discussed above, the CADE software was designed to be self-instructive and require very little paper instruction. Written materials included an \"800\" telephone number for a \"help-line\" where users could receive technical support. Upon completion of the record abstraction task, the institution mailed the completed CADE diskette back to either Abt or RTI, requiring a signature upon delivery."}, {"section_title": "4.4", "text": "Automatically run edit programs on each of the files received. These programs checked completed data fields in each student record and compiled statistics indicating the level of completeness at the student level and at the institution level and prepared reports based on these indicators. Receipt control and editing programs ran overnight on all new files received the previous day. Project staff reviewed edit reports to determine whether retrieval efforts were necessary prior to preloading the CADE data into the telephone computer assisted telephone interviewing (CATI) system (See \"SYSTEM EDIT RESULTS\" in Appendix C). Preload edited institution data into CATI records in order to initiate telephone interviewing with the students and parents. Generate routine production reports used by the project management to monitor overall progress in the institution survey and the backlog of cases available for CATI interviewing. The telephone survey of students and parents is described in the following chapter. Table 4.1 presents response rates for student institutional records abstraction, treating an institution as responding if any CADE data were obtained for any sample student. In some cases, only minimal information needed for tracing sample students was obtained. Table 4.1 shows that some student data were successfully abstracted for 1,079 of the 1,098 eligible institutions that provided lists for sample selection. Hence, 98.3 percent of these institutions also participated in CADE. The response rates for CADE range from 91.7 percent for private, for-profit, less-than-2-year institutions to 100 percent for several institutional sectors, including most of the public institutions. Weighted response rates are also presented in Table  4.1 based on the institution sampling weights adjusted for nonresponse to the request for student lists for sample selection. The weighted responses rates can be interpreted as the estimated percentages of eligible institutions that would participate in CADE, given that they would provide student lists for sample selection. The weighted response rates are generally comparable to the unweighted response rates, and the overall weighted response rate is 96.9 percent."}, {"section_title": "Institution Records Collection Response Rates", "text": "Response rates for institutional records abstraction are presented at the student level in Table 4.2, conditional on institutional participation in this phase of the study. Some data were abstracted for nearly all students (about 99 percent) when the institution participated in records abstraction. The student-level response rates were lowest (about 96 percent) among the institutions that sent copies of the student records to the central office (Abt or RTI) for data entry.  Field Period for Record Abstract Data Figure 4.3 displays the monthly and cumulative monthly collection of institution enrollment files/lists and Figure 4.4 displays the monthly and cumulative monthly of institutional records data. Although the initial mailing to institutions occurred in February, the institutions were unable to comply with requests for enrollment data until June (month 6 in Figure 4.3). The number of institutions providing enrollment data was uniform throughout the summer (June, July, August, and September) and the last files of enrollment data were not obtained until November. In Figure 4.4, the record abstract data from the first institution was returned in June, although significant numbers of institutions did not accumulate until September (485 institutions). Poor participation over the summer months reflect to some extent the flow of institutions providing enrollment dat i. for sampling. Summer vacations by staff in the student financial aid offices was a major factor. With the start of the academic year in fall, the pace of record abstraction increased (in September and October) and record abstract data had been collected for most of the participating institutions by the end of November. However, data collection continued through early January in order to maximize the number of participating students in the telephone survey. "}, {"section_title": "4-11", "text": ""}, {"section_title": "Choice of Method by Institution Characteristics", "text": "The postsecondary institutions agreeing to participate in NPSAS and providing student sampling lists were offered a number of options for how data were to be extracted from their institutional records for the students sampled at their institution. The preferred option was to have institutional staff use the computerized assisted data entry (CADE) system developed by study staff. The next preferred option was having contractor field staff abstract data from institutional records and enter them through CADE. For institutions failing to accept either of these methods, other less preferred self-abstraction alternatives were used (e.g., provision of computer printouts, photocopies, or hard copies of CADE screens on which information was manually entered). Both self-and field-abstraction methods yielded data in a well defined and consistent format; as expected, the \"other\" methods did not. Also, considering all data collection and processing costs, the expense of the various abstraction methods increased monotonically with the previously indicated \"preference\" of the method. The systematic incompleteness of some data items, where abstraction was provided through \"other\" approaches, suggested this approach may have been used as a way to restrict the information provided without having to deal with the CADE system or with contractor staff on campus."}, {"section_title": "4-15", "text": "Of the 1,094 institutions allowing abstraction, 493 (45 percent) initially chose the preferred method of self-abstraction. An additional 517 (47 percent) initially chose fieldabstraction, and 84 (8 percent) chose to provide record abstract data in some other way. A number of institutions changed their choice of abstraction method during the data collection period; the bulk of these changes represented shifting from an initial choice of self-abstraction to a choice of contractor staff abstraction. Because the institutional control file was not consistently updated during operations, only the initial institutional choices can be reported reliably. Institutional initial choices are shown in Table 4.3 as a function of postsecondary education sector (i.e., institutional control and highest level of offering --factors that defined strata in the sampling frame)`. Systematic differences in choice can be observed in these data. Specifically, choice of self-abstraction in the public sector generally decreased with higher levels of offering; however, no such trend was observed in the independent sector, and the trend was clearly reversed for private, for-profit institutions. Also, public institutions with highest offerings less than 4-years were most unlikely to use \"other\" methods, while doctorate-granting public institutions and less-than-two-year private, for-profit institutions were most likely to use \"other\" methods. Within the public sector of postsecondary education (and to a lesser extent in the independent sector), institutions offering doctorate and first professional programs are, on average, much larger than the institutions that do not, and student sample sizes within institutions were partially related to size. Also, student sample sizes at all institutions offering both a baccalaureate degree and programs beyond a 4-year degree were somewhat inflated, since these institutions contributed both undergraduate students and graduate-level students. Consequently, a good portion of the inverse relationship between highest level of offering and choice of self abstraction could reasonably be attributed to increasing burden (i.e., greater numbers of abstractions required) with increasing level of offering. An examination of choice of record abstract method as a function of abstracting burden is shown in Table 4.42 The specific break points for \"small,\" \"medium,\" and \"large\" burden were determined on the basis of total number of students sampled, such that about a third of the total student sample came from \"small\" burden institutions, another third from \"medium,\" and the final third from \"large.\" The relationship between increasing burden and lowered likelihood of choosing the self-abstraction method is clearly obvious in the results and is consistent within all control sectors. Other underlying factors leading to differences in choice of abstracting method are certainly at work, however. The low propensity of using \"other\" methods (principally supplying printouts or photocopies) in less than 4-year public institutions may reflect lack of ready access to central records files and/or processing equipment needed for the simplest of these approaches (i.e., provision of computer printouts). Also, the condition of being \"over committed,\" which was often expressed by many institutional coordinators at private, forprofit institutions may explain the generally lower choice of self-abstracting CADE by such institutions (and associated higher than average rates of reliance on contractor field staff and \"other\" methods). The relatively high propensity of doctorate-granting public institutions to choose other methods may be an anomaly of the small group size; however, this category of institut;ons, as defined for NPSAS sampling, represents a somewhat different population than might be first imagined (namely, institutions offering doctorate-level programs but not offering firstprofessional programs). Most of the state mega-universities offer both types of programs (and as such were placed in the \"First Professional\" stratum). While such large institutions universally have automated records systems, such systems are frequently not \"central\" (i.e., they keep computer records in separate files and frequently separate computer facilities -for undergraduates, graduate students and first professionals). Under such conditions the provision of computer printouts for the entire sample at these institutions would have involved coordination through a number of record systems. The smaller state universities offering only doctoral programs are more likely to have central records, and thus provision of printouts from this single system would be a more viable alternative for them. This hypothesis is partially supported by the greater propensity of \"medium\" burden institutions (also typically mid-sized institutions) to use the \"other\" methods. NOTE: Statistics are based on the 1,094 postsecondary institutions agreeing to participa e in the study. All percentages reported are based on row total counts. Institutions had the choice of allowing local field staff to perform the record abstractions, performing the abstractions themselves using a CADE program provided by the contractor, or providing the requisite information in some other format, such as computer printouts or photocopies of selected files. A number of institutions changed abstraction method during data collection (principally from self-abstraction to abstraction by contractor field staff; only initial methods arc reported here. b Level of Offering strata were combined within sector of control to maintain adequate cell sizes."}, {"section_title": "4-18", "text": "; 14.9 Note: Statistics are based on the 1,094 postsecondary institutions agreeing to participate in the study; all percentages are based on row total counts. Institutional burden (related to institutional size) is defined relative to the number of selected studcnts foi whom records were to be abstracted (range of 2 to 371): \"small\" as 50 or fewer, \"medium as 51 -127. \"large\" as 128 or more. a Institutions had the choice of allowing local field staff to perform the record abstractions, performing the abstractions themselves using a CADE program provided by the contractor, or providing the requisite information in some other format, such as computer printouts or photocopies of selected files. A number of institutions changed abstraction method during data collection (principally from self-abstraction to abstraction by contractor field staff); only initial methods are reported hcre. b Burden levels were combined within some institutional control levels to maintain adequate cell sizes."}, {"section_title": "4.8", "text": ""}, {"section_title": "Completeness and Validity Analysis", "text": "All data abstracted from student institutional records were subjected to edit checks for completeness before being preloaded into CATI for subsequent use during interviewing. Completeness of CADE data can be evaluated by determining the extent to which a ke7 set of elements, listed in Table 4.5, was available from institutional records for each student. Overall, aided students were expected to have more of the data elements than nonaided students simply because nonaided student records do not contain the financial aid information, such as the Pell grant index, required of aided students."}, {"section_title": "4-19", "text": "Tables 4.6 and 4.7 provide the student-based, average numbers of elements obtained from the institutional records of aided and nonaided students by institutional sector and method of record abstraction. Across institutional sectors, there were only small differences in the mean number of items abstracted with one exception, records abstracted by field staff from public institutions offering less than two-year programs. On average, less than half the critical items expect.ed for aided (49 percent) and nonaided (41 percent) students were abstracted by field staff in less-than-two-year public institutions, a result which may be related to the complaint frequently heard from field staff that many of the less than four-year public institutions had difficulty locating or \"did not have\" some of the records needed for abstraction. Particular CATI items were designed to confirm information obtained during record absti action as one measure of the validity of the abstraction methods used. Table 4.8 presents student-level agreement between institutional reports of receipt of aid and students subsequent confirmation during telephone interviewing of receipt of aid, by institutional sector and method of abstraction. Among students receiving aid, percent agreement was at least 94 4-20 percent for all sectors and methods of abstraction. In contrast, percent agreement among nonaided students was markedly lower than the aided students both across institutional sectors and methods of abstraction, perhaps because reports of nonreceipt of aid ($0.00) were confounded during record abstraction with missing data. For example, an institution may not have been aware of a student's receipt of employer aid, especially if the student did not receive federal aid.  Table 2.1 Up to 16 elements were expected for nonaided students, although up to 26 elements were possible 'Institutions had the choice of allowing !ix:al field staff to perfoan the record abstractions, perfuming the abstractions themselves using a CADE program provided by the contractor, or providing the requisite information in some other format, such as computer printouts or photocopies of selected files. A number of institutions changed abstraction method during data collection (principally from self-abstraction to abstraction by contractor field staff), only initial methods arc reported here. Ilevel of offering igAitalwere combined within sector of control to maintain adequate cell sizes."}, {"section_title": "4-23", "text": "CA ' 1) 5.)  The data abstracted from institution records were complemented with additional information collected during a telephone interview with sampled students and, for a subsample of students, with parents. The student and parent questions were programmed into a computer assisted telephone interviewing (CATI) system. Identical systems, training programs, and procedures were used at the two facilities at Abt and RTI. Data collected from institutions were preloaded into the CATI systems in order to assist students during the telephone interview. Although the initial schedule called for telephone interviewing to begin in June of 1993, because of the delays in acquiring the frames for student sampling (discussed in Chapter 4), the student and parent survey did not begin until September of 1993. Interviewing continued through March 20, 1994."}, {"section_title": "5-I", "text": "When possible, previously obtained financial aid and administrative record data were pre-loaded into the CATI system to minimize the length of the telephone interview with each respondent. The student and parent CATIs were designed so that either could be administered and, if information had been provided by the first respondent (either student or parent), questions were not repeated with the second respondent from the same 5.2"}, {"section_title": "Design of the CATI Instruments", "text": "The Student CATI for NPSAS:93 collected student self-report data concerning enrollment, educational costs, employment, financial aid and additional sources of support, specific demographic and financial characteristics of students and parents, and locating data for the first follow-up of B&B students. In addition to collecting information for those sampled students who received postsecondary financial aid, the survey was critical for collecting information on the financial characteristics of unaided, independent students as well as for those students whose financial aid records were unavailable from the institution. In this instance the students themselves were the primary source of information about their funding sources for their educati6n and education-related expenses. The NPSAS:93 Parent Survey was designed to obtain information from the parents of primarily unaided, dependent students. The sampled parents were surveyed regarding the support given to their students, their employment and financial status, and the support required from other dependents. The CATI system within the ICS consisted of three modules designed to assist in locating students and parents, conducting interviews with these respondents, and providing daily production reports for the project staff. The locating module was preloaded with address information collected from the institutions. In addition, this module contained a detailed roster that locators used to record the history and results of locating attempts, including new addresses and telephone numbers. The CATI student and parent interviews were designed to capture a variety of information about the student's educational experiences during the NPS AS year. The student interview consisted of the ten modules listed in Figure 5.1 and the parent CATI consisted of the six modules listed in Figure 5.2. A list of CATI data elements is provided in Appendix A. The student and parent CATIs were designed so that either could be administered first and, if similiar data elements had been provided by the first respondent, questions need not be repeated in the second interview. Students in the B&B cohort were administered a slightly longer CATI that included items on future plans related to education, occupation, and family formation. Institution Enrollment -Current enrollment information dealing with curriculum, level in institution, GPA (grade point average) graduation plans, as well as high school education and other degrees, licenses, and certificates earned."}, {"section_title": "2)", "text": "Enrollment and Costs -Each enrollment period between July 1, 1992 through June 30, 1993 was covered. Attendance, number of courses taken and credits earned, tuition, fees and other expenses were covered. The section included a focus on housing location and expenses: housing costs, utilities, meals, transportation, personal expenses and repayment of educational loans."}, {"section_title": "3)", "text": "Financial Aid -Grants, scholarships, student loans, work-study, employer or military assistance, or any other sources, were included in these inquires, but financial assistance from family or relatives was not included. The amount of aid, type (i.e., grant, scholarship, source (state, federal) and amount of repayment required was recorded. Additional Sources of Support -Other sources of support, the amount and types of expenses the support was used for were recorded."}, {"section_title": "5)", "text": "Employment -Employment between July 1, 1992 and June 30, 1993. Occupation, business and/or industry codes, were automatically displayed for immediate data entry."}, {"section_title": "6)", "text": "Educational Expectations -Assessment of the student's educational expectations and satisfaction with the institution, and future educational and employment expectations."}, {"section_title": "7)", "text": "Student Demographics Student's gender, race, ethnicity, functional limitations, and history of voting and community service. Financial Status Student's (and student spouse's) current assets, debts, 1991 Federal income tax, 1991 and 1992 income and expenses, and previous five years of employment."}, {"section_title": "10)", "text": "Locating Information Verification of student social security number. Locating and contacting information for B&B students' parents. The NPSAS:93 Parent Interview contained six major sections: Parental support to the student -Parental contributions and loans to the sampled student, sources and amounts of those funds Attitudes -Details about plans for graduate school and/or employment asked of parents of B&B cohort only. As indicated previously, information was preloaded from the CADE system to the CATI systems. Pre loaded information included terms of enrollment in the sampled institution (beginning and ending dates of each term of enrollment), information from the needs analysis and budget sections of CADE, including educational expenses, and detailed information on sources and amounts of financial aid. During the interview, information on amounts of awards, was summarized and presented as a total to students for verification. If students disagreed with the total amount, the interview was routed through a detailed set of questions to learn about sources of financial aid that the institutional records may not have captured; however, if the student verified the summary, this long battery of questions was skipped. For this reason, the preload feature of the NPSAS:93 data capture systems considerably reduced respondent burden. The CATI system was programmed using the Computer Assisted Survey Execution System (CASES) developed at the University of California, Berkeley. CASES is a very powerful and very flexible framework for CATI applications. Standard features include automatic scheduling of interviews to assure that attempts are made at various times throughout interviewing shifts. Call records for each sample member are time and date stamped and are used to automatically update event and disposition codes that are used in the preparation of production reports. Time stamps may be inserted throughout the CATI to calculate minutes per section. The CATI system itself includes range checks and inter-item consistency checks and routing to different sections of the questionnarie depending on responses to filter questions. The NPSAS application made frequent use of the preload feature of CASES. In addition to these standard features, customized applications were developed at AN and RTI to handle specific needs of the study. A frequent specification for items in the NPSAS was the ability to enter data in a \"grid\" format, for example, listing beginning and ending for terms of enrollment. Many of the questions concerning income, assets, and 5-5 u sources of financial aid employed a grid format. Another type of customized application was NCES-supplied standard automated coding schemes use in coding student's major field of study and student's occupation and industry. The reporting module provided the project staff with daily production reports on the results of locating and interviewing. Separate reports were developed for all students and all parents and for the students and parents in the B&B cohort. Separate reports were generated for the telephone shops at Abt and RTI as well as a summary report documenting production at both locations. In addition to these reports, which documented overall production in terms of completed interviews, additional management reports focussed on special topics, for example, locating efforts or refusal conversion efforts or interviewer level production. These reports were used by the telephone shop management at both Abt and RTI to identify and respond to problems that might affect production."}, {"section_title": "5.3", "text": ""}, {"section_title": "Survey Operations", "text": ""}, {"section_title": "Staffing and Interviewer Training", "text": "The number of interviewers required for a project the size of NPSAS exceeded the interviewing staff on hand at both locations and an extensive recruiting effort was necessary to hire additional staff. Interviewers were recruited a number of sources including newspaper advertisements, local educational institutions, and temporary agencies. Job candidates were screened for diction, maturity, and telephone presence. All new hires received a day-long general training course in basic telephone interviewing techniques and use of the CATI system. In addition, all interviewers assigned to NPSAS receieved a 4-day study-specific training. During this training, inteiviewers learned about the purposes of the NPSAS study, the structure and flow of the student and parent CATIs, item-by-item instructions, specific refusal conversion techniques, locating procedures, and administrative procedures. Training relied heavily on practice exercises so that the interviewers developed skill and familiarity with the survey instruments and basic concepts of the study. The first interviews of all new interviews were carefully monitored and both positive and negative comments were provided immediately to the interviewer."}, {"section_title": "NPSAS Telephone Interview Procedures", "text": ""}, {"section_title": "Call Scheduling", "text": "Student and parent interviews were scheduled using the CASES system scheduler, which automated the assignment and delivery of cases to telephone interviewers. The CATI automated scheduler enabled tracking of all call-backs to potential respondents through the grouping of active cases into varims queues. At the time of interviewer log-in, the scheduler automatically distributed the most appropriate calls for that work shift. The interviewer would then review the record of calls for each allocated case, to prepare for the next immediate telephone call. During the work shift, the queues were automatically searched and 5-6 the most immediate, appropriate cases were allocated for calls. Interviewers entered information obtained during the new telephone call so that the interview was conducted, or the case could be sent to the proper queue for the next appointment to be met. CATI automatically assigned next available cases in this order of priority: 1) Hard appointments to call back 2) Soft appointments to call back 3) Missed appointments 4) Records that were otherwise unresolved 5) New cases New cases appeared in the system with blank spaces in the record of calls. The first screen of a new case denoted the student's name, institution attended, and the parent's name. As calling attempts were made, the results were recorded, along with date and time of the most recent call. This scheduling method provided a highly efficient system of case assignment by reducing supervisory and clerical time, automatically monitoring appointments and call-backs, and reducing error and variation in the implementation of survey priorities and objectives."}, {"section_title": "Contact Procedures", "text": "Advance letters were sent to sampled students and parents to inform them of their selection and to review the purpose of the study. Once the interviewer indicated that the respondent had been reached, the CATI introduction screen appeared. The inti-Gduction on the screen delivered to the respondent was designed to be informative and to quickly involve the respondent in the interview. It provided a clear and efficient way of introducing both the study and the interviewer. If it was determined that the respondent had received the letter, the respondent was informed that participation in the survey was voluntary and all information would be kept confidential, and the interview was conducted. If it was determined that the respondent had not received the letter, the interviewer would explain the legal authority and purpose of the study, as well as the voluntary nature of participation and confidentiality of the data. If the respondent would not conduct the interview without having read the letter, the letter was re-mailed, and an appointment was made for a call-back in one week. If a student or parent was unable to complete an interview at the time of the first contact, the interviewer attempted to schedule an appointment at a later time. If the student was not available to schedule an appointment, the interviewer asked the person who answered the telephone for advice about when to call back to reach the respondent. In cases where respondents could not be reached through repeated attempts by telephone, interviewers were instructed to leave an \"800\" number for respondents to call back. The number could be left on an answering machine, with another member of the respondent's 5-7 household, or, in some cases, the number was included in a letter sent to the respondent's address. In each case where a number was provided for a respondent to call in, a Respondent Call-In form was completed. These forms were filed alphabetically, in a central location, near the call-in phone, in order that the interviewer assigned to the incoming call could find the case quickly. If the interview was conducted as a respondent call-in, the telephone interviewer was responsible for completing the Respondent Call-In form and recording the results. It was necessary to locate and interview over 80 percent of students and parents in the NPSAS:93 sample. Various procedures were developed for tracing and locating NPSAS respondents. If calls made to a sample member's known telephone number(s) did not result in a contact, the interviewer initiated tracing efforts using the tracing/locating module. (Locating information was preloaded into the module based upon the information obtained from institution records.) If locator contacts did not provide a new telephone number for a sample member, interviewers attempted to elicit further leads from the contact. An:), new locator information was immediately entered into the module."}, {"section_title": "Interim Codes", "text": "During the tracing and interviewing activities, interim result codes were used to document the status of cases. The codes represent each attempt to contact respondents and complete interviews. The interim codes are presented here: Tracing interim codes were used until at least two questions in the interview were completed. Pending language barriers were also noted with a provision to record whether the foreign language would be Spanish or another language. If the interviewer was dubious about the second language, Spanish was noted. The CATI system also provided for notation of whether the respondent was out of the country. Prompts in the system would help determine the date of the respondent's return."}, {"section_title": "Final Codes", "text": "After the first two questions of the interview had been answered final result codes were used. Result codes were preceded by a \"2\" when assigned for students and 300 level for the parent. The Final Codes are as follows: "}, {"section_title": "Locating", "text": "During institutional record abstraction, attempts were made to obtain up to four addresses and associated telephone numbers for each sampled student (e.g., student's local and permanent addresses and telephone numbers, parent's address and number, and an emergency contact address and number), in order to facilitate subsequent locating efforts during CATI operations) Obtained addresses and/or telephone numbers were preloaded into the CATI record for tracing, together with an indicator that the information had been abstracted from the student's institutional record. Attempts to contact sample members by telephone started with these preloaded addresses or telephone numbers. An index of the usefulness of abstracted contact information was defined as the rate of successful contacts at preloaded addresses/telephone numbers. Table 5.1 provides the number and percentage of sample members contacted at a preloaded address, as well as the number and percentage of sample members located at any address/telephone number (i.e., including those uncovered during tracing). This latter measure is indicative of the success of both the locating process itself and the utility of extracted information in providing a: least a starting point for locating. Statistics reported in the table are based on a student sample of 81,451, plus the 18,491 parents identified for telephone interviews.2 Overall, 84 percent of sampled students and 85 percent of parents were located. (Included among sample members not successfully located through extracted contact information are 2,560 students, and some number of their associated parents, for whom institutional data included no locating information.) The high percentage of B&B sample members located (93 percent) reflects the significant concentration of effort in contacting and interviewing these sample members for the longitudinal study. Graduate and first-professional students were also fairly likely to be located through extracted addresses (89 percent). \"Other undergraduates,\" however, which include students in non-baccalaureate programs, had the lowest rate of locating success (84 percent), perhaps partially due to the fact that nonbaccalaureate students tend to be a relatively more transient group than students in either four-year undergraduate programs or graduate/first-professional programs. That only 57 percent of students and parents were located at an extracted address/telephone number was not a completely unexpected result because students tend to move often (and do not always update institutional information).3 The difference in success rates across respondent groups can be readily understood by considering the nature of each population represented. Graduate and first-professional students, for example, who generally tend to be older and more established than undergraduates, were the respondent group most likely to be located at the extracted address (71 percent). In actuality, information obtained was frequently fragmented (e.g., telephone numbers without associated addresses or addresses without telephone numbers, locator information without names). A total of 722 student records were deleted from the full sample of 82,173 since address/telephone-level 1(,_atim, results had been inadvertently contaminated during operations. Because final address/phone-level results did not allow indication of students and parents contacted at the same preloaded address/telephone, location rates are probably underestimated.\nBecause the field period for the-field test was constrained, we did not attempt to locate all of the sample members. Instead, a simple random subsample of 1,000 was selected for the purpose of determining locating rates. Of this subsample, 95% were located, indicating that locating data obtained from the institutions, combined with typical locating procedures (including address correction requests on advance mail copies, requests to directory assistance, contacting the parent's of sampled students) were sufficient to locate sample members. Locating procedures began with the addresses and telephone numbers provided by the institutions. As part of the record abstraction, institutions were asked to provide up to four addresses: student's local and permanent addresses, parents' address, and the address of another person who might know of the student's whereabouts. In many instances, students in the sample lived at their parents' home and attended a local institution so that the student's local and permanent addresses and the parents' address were all the same. For this reason, the modal number of addresses and telephone numbers supplied by the institutions was one. However, in most instances this address was enough to locate the student and, if necessary, the parent."}, {"section_title": "5-9", "text": "The most difficult group to locate at one of the preloaded addresses was the \"unspecified\" student group, for whom institutional data were so minimal that even year in institution was not available. This rate among parents was also low (55 percent), but may reflect the explicit decision made during telephone interviewing to reduce parent locating efforts in order to concentrate more time and effort on locating student sample members. B&B sample members were another respondent group less likely to be located at one of the extracted address (57 percent). This is again not a surprising finding considering that B&B sample members were, by definition, new baccalaureate recipients and, therefore, would be relocating with entrance into the labor market or post-baccalaureate study. Although not at a preloaded address, members of the B&B group were nonetheless \"locatable\" through information provided by the institutional records. While undergraduates in baccalaureate programs should have been about as locatable a c the graduate/first-professional student group, undergraduates in non-baccalaureate programs three-year or less programs) almost certainly contained some individuals who completed the_r program and relocated like the B&B students."}, {"section_title": "Refusal conversion", "text": "Interviewers were trained to deal with an extensive range objections, problems and concerns expressed by respondent-. Scripted responses were provided for common objections. These responses prepared interviewers to alleviate issues of confidentiality, legitimacy, eligibility to participate in the study, and a host of other matters. Quite often respondents would seek to delay the interview, and interviewers were trained to overcome this objection as well. However, when scheduling a call at a later time was necessary, the CATI scheduling capability facilitated the process of completing the interviewer by maintaining a queue that assigned the call to the scheduled time. Note: Locating information was obtained from the institutions during record abstraction for use in contacting 81,451 student sample members for the telephone interview. Among students contributing to these analyses. 18,491 were selected for parent interviews. During operations, address/phone-level locating results for 722 records were inadvertently deleted, and thus were not included in the analyses. All percentages are based on row total counts."}, {"section_title": "5-10", "text": "' Students and parents located through data extracted during record abstraction were defined as those who answered any one of the first three interview items (or the first item in the parent interview), or whose final result code indicated at least partial administration of an interview, or whose final result code indicated that location of the sample member was in some other way resolved (e.g., located but out of the country at the time of the interview). These cases were not necessarily contacted at the address/telephone number obtained during institutional records abstraction, but such contact information would have served as a starting point for tracing. Defined as students and parents who were located at one of the addresses/telephone numbers extracted during record abstraction. Because final address/phone-level results did not allow indication of students and parents contacted at the same preloaded address/telephone, location rates are probably underestimated. The parent base was identified as those student records with the parent interview flag set. d Determination of student level was made based on a year in institution variable available for those in the final analysis files (see Chapter 6). For those not included in these files, student level was assigned according to a student level variable preloaded from extracted data. A total of 8,048 original sample members could not be classified by either method and are shown in the table as \"Unspecified.\""}, {"section_title": "Language problem recalls", "text": "When an interviewer encountered a problem with a respondent's capability of understanding English, the interviewer sought to speak to someone else in the household who could translate between English and the respondent's language. This procedure v as also followed in the case of the hearing impaired. If Spanish was the respondent's mother tongue, the interviewer referred the call to an interviewer proficient in Spanish."}, {"section_title": "5-1 1 1", "text": "Toll-free 800 number An \"800\" number was used to facilitate return telephone calls. This feature was especially useful for students or recent graduates who had no telephone on their own, but who could be reached through the mail or through family or friends, or by leaving a message with the receptionist in the student dormitory. Also, when respondents questioned the authenticity of the study, interviewers gave them the toll-free number to call; this quelled their doubts about the study's legitimacy."}, {"section_title": "Quality control", "text": "The telephone centers at Abt and RTI are equipped with a system to monitor interviewers to ensure that they are observing procedures appropriately and entering accurate and complete data. Roughly ten percent of the calls on each shift were monitored; each interviewer was monitored at least once during each shift. Supervisors who monitored the calls provided feedback quickly and constructively, so interviewer performance was enhanced; opportunities for improvement were realized and positive behavior was reinforced. The monitoring process was geared to maintaining production rates, ensuring consistency and enhancing the quality of the operation. Interviews were monitored for twenty six performance dimensions, including aspects such as identifying the interviewer, the study and its sponsor by name, noting the propose of the study, verifying the respondent's phone number and address, conveying an assurance of confidentiality, and explaining the voluntary nature of cooperation. Further, the supervisor noted whether the interviewer's use of persuasion, whether the interviewer changed the question wording or mispronounced words, whether skip patterns were observed, whether probing was appropriate, whether feedback was used, whether responses were properly entered and whether the correct result code was marked at the conclusion of the interview. Also, the interviewer's professionalism was evaluated, including attributes such as courtesy, assertiveness, persuasiveness, knowledge of the study, neutral presentation and ability to maintain control of the interview. The pace, clarity and volume of the interviewer's voice was rated, along with the interviewer's use of CATI functions, the thoroughness of comments. Once the monitoring process for an individual interviewer was completed, the supervisor appraised the interviewer as either below average, average of average, and shared the evaluation with the interviewer, along with feedback intended to improve (or reinforce) performance, before the end of the shift."}, {"section_title": "5.4", "text": "Response rates"}, {"section_title": "Student CATI Response Rates", "text": "Attempts were made to locate and interview all sample students, except those who had been identified as ineligible based on the data abstracted from the student records. Students who were deceased, out of the country, or otherwise not vailable for telephone interviewing (e.g., incarcerated) were classified as ineligible for CATI. The number of sample students who were ultimately classified as eligible for CATI was 77,003."}, {"section_title": "5-12", "text": "Students were defined to be CATI respondents if they completed at least Section A of the CATI interview. Of the 77,003 CATI-eligible sample students, 52,964 (including 298 whose data were lost because of unrecoverable system hardware failures), or 68.8 percent of the CATI eligibles, were CATI respondents as shown in Table 5.2. In addition, Table 5.2 shows that the weighted and weighted effective student CATI response rates were 67.8 percent and 72.0 percent, respectively. The weighted effective response rate for each stratum for which a nonresponse subsample was selected can be represented as where RI is the Phase 1 response rate and R2 is the response rate achieved among those units selected for the nonresponse follow-up subsample. The student CATI response rates were lowest (55.7 percent) among sample students selected from private, for-profit, less-than-2-year institutions. Because NPSAS analysis files are based on CADE and CATI data, readers should also refer to the overall response rates described in Chapter 6."}, {"section_title": "Response Rates for Parent CATI Interviews", "text": "The CATI response rates for parent interviews are shown in Table 5.3. The overall unweighted and weighted parent response rates are comparable, 61.8 percent and 62.4 percent, respectively. The weighted effective parent response rate is slightly lower, 61.4 percent, because the response rate among sample parents in the nonresponse follow-up subsample was slightly lower than the rate achieved in the Phase 1 sample. The parent CATI response rates were lowest (55.1 percent) among the parents of students sampled from private, for-profit institutiens. Because of the emphasis on R2, the response rate among those cases selected for the nonresponse subsample, a low response rate obtained in the subsample may result in the weighted effective response rate being less than the overall weighted response rate. During the subsample follow-up phase of the data collection, in part due to budget and schedule constraints, more resources were allocated to the student CATI. This resulted in lower weighted effective response rates in the parent telephone interview.  2,266 study-eligible students were not eligible for CATI because of the following reasons: 87 were deceased, 805 were out of the country, 77 were otherwise unavailable (e.g., incarcerated), and 1,297 were classifed as ineligible during CATI but later determined to be eligible (typically enrolled but dropped out before completing the term). Includes 298 students whose data were lost because of unrecoverable system hardware failures. Based on student record abstraction (CADE).  Includes 30 parents whose data were lost because of hardware problems."}, {"section_title": "5-15", "text": "bBased on student record abstraction (CADE). 5-17"}, {"section_title": "Interview Breakoff", "text": "Not all of the students and parents who were located provided complete interviews. Once sample members were contacted by telephone, some broke off the interview after a few initial questions and refused to continue. Other contacted sample members completed one or more (but not all) sections before terminating the interview. Still other sample members could not (or would not) continue, because they spoke insufficient English'. All cases of these types were defined as representing interview \"breakoff'. Because the raw CATI files contained incomplete data on a number of qualifiers of interest, examination of breakoff rates for NPSAS:93 was restricted to those cases in the final analysis files (see Chapter 6) who had at least started the interview5. Breakoff rates for both students and parents are shown in Table 5.4; students are further broken out in this table by corrected major student stratum (i.e., B&B, other undergraduate students, and other graduate/first-professional students6. A student breakoff rate of approximately 10.4 percent is quite consistent over the three student types considered, despite concerted efforts to reduce this rate in the longitudinal B&13 sample. The B&B breakoff rate shown may reflect improvement to a higher underlying base breakoff rate in this group, for whom the interview was longer. Parent breakoff rates are markedly lower than those for.students; this probably reflects the considerably shorter administration time for the parent interview. Table 5.5 shows student breakoff rates by control and highest level of offering of the institution from which the sample member was selected. Compared to students from public postsecondary institutions, students from independent (i.e., private, not-for-profit) institutions break off at marginally (but significantly --g..001) higher rates (9 percent and 11 percent, respectively). But, students at private, for-profit institutions break off at markedly higher rates (over 17 percent) than those at either public or independent postsecondary institutions. These differences probably reflect underlying differences in the typical educational clients in these different institution sectors. Breakoff rates also vary over level of offering, within the public and private sectors of institutional control. Within public institutions, breakoff rates over increasing level of offering appear to be a quadratic relationship; rates decline from either extreme to a nadir at the institutions offering only Bachelor's degrees (this could be a function of institution size, because state colleges offering only a four-year program are typically smaller than either the large public technical institutions or the large universities that offer advanced degrees). Within Bilingual (English/Spanish) interviewers were used at both sites (principally for the Puerto Rican sample and for monolingual Spanish speaking parents: however, it was infeasible to maintain bilingual interviewers for the large number of other languages spoken among some parents. \"At least starting the interview\" was defined as those who had completed at least one section of the interview or, if not, had a timing value greater than zero for interview Section 1. Restricting these analyses to the final analysis file cases should result in an underestimate of breakoff rates, of unknown (but likely small) magnitude. Because of the definitions used plus the nature and timing of the sampling, B&B sample members appear in both the undergraduate and graduate/first professional final analysis data files."}, {"section_title": "5-18", "text": "independent institutions, the principal outlier is the less-than-two-year institutions, within which student breakoff rates exceeded 20 percent. While student sample size in this cell is generally sufficient to provide stable estimates, it should be kept in mind that the number of unique institutions contributing students to this cell is quite small. Consequently, the difference could be mainly attributable to characteristics of students in one or two institutions. Note: Statistics are based on the 57,224 students and 11,281 parents retained in the final analysis files, to whom the interview was at least partially administered; percentages are based on total counts within the row. An interview was determined to be a \"break off if a sample member started the interview but did not answer enough items in the first section to be considered a \"partial\" respondent. b Restricting these analyses to cases in the final analysis files should result in breakoff rate underestimates, of some unknown (but expected small) magnitude. Students are further divided by the three major sampling strata as finally corrected; because of the definiticns used plus the nature and timing of sampling, B&B sample members appear in both the undergraduate and graduate/first-professional final analysis data files. Breakoff rates were also examined by race, gender, and year in institution (in each case crossed by major student stratum); results are shown in Tables 5.6, 5.7, and 5.8, respectively. Within each student stratum and overall, a higher breakoff propensity was observed for blacks; a lower propensity was observed for Asian/Pacific Islanders and student's of \"other\" races. With the exception of the clearly confounded rate for those of indeterminate gender (indeterminate in most cases because the sample member did not progress far enough in the interview to reach the gender question), breakoff rates were not meaningfully related to gender. Discounting results based on less than 100 observations, the major difference in breakoff rate, as a function of year in institution, was the markedly higher rate observed for unclassified undergraduates. This latter result is also partially confounded, since individuals sampled as undergraduates but for whom no information was otherwise obtained (i.e., were not abstracted from institutional records and students didn't get far enough into the interview to reach the year in institution question) as well as those legitimately reported as \"unclassified.\" '8.4"}, {"section_title": "5-19", "text": "Note: Statistics are based on the 57,224 students retained in the final analysis files, to whom the interview was at least partially administered; percentages are based on total counts within the row. a Some cells were combined to maintain adequate sample sizes. h An interview was determined to be a \"break off if a sample member started the interview but did not answer enough items in the first section to be considered a \"partial\" respondent. Rcstricting these analyses to cases in the final analysis files should result in breakoff rate underestimates, of some unknown (but expected small) magnitude.  94.3 . Note: Statistics are based on the 57,224 students retained in the final analysis files, to whom the interview was at least partially administered: percentages are based on total counts within the row. Reflects final classification; because of the definitions used plus the nature and timing of sampling, B&B sample members appear in both the undergraduate and graduate/first-professional final analysis data files. b Although gender of sample member was updated using all available information, this classification includes sample members refusing to report gender (or not getting to the gender question) during the interview and for whom no other information on gender was available. An interview was determined to be a \"break off if a sample member started the interview but did not complete it; this includes \"partial\" interview (not all sections completed) as well as those not completing enough questions to be classified as a partial respondent. d Restricting these analyses to cases in the final analysis files should result in breakoff rate underestimates, of some unknown (but expected small) magnitude. 5-22.1 1. Note: Statistics are based on the 57,224 students retained in the final analysis files, to whom the interview was at least partially administered; percentages are based on total counts within the row. Reflects final classification; because of the definitions used plus the nature and timing of sampling, B&B sample members appear in both the undergraduate and graduatclfirst-professional final analysis data files. b Generally, level in institution was based on student's status at the beginning of the school year. If requisite information was missing, however, level in institution was estimated based on input variables for degree program, the student sampling stratum, and financial aid information; thc unclassified undergraduate category includes those for whom exact undergraduate classification could not be otherwise determined as well as those reporting \"unclassified\" or \"special student\". ' An interview was determined to be a \"break off' if a sample member started the interview but did not complete it; this includes \"partial\" interview (not all sections completed) as well as those not completing enough questions to be classified as a partial respondent. d Restricting these analyses to cases in the final analysis files should result in breakoff rate underestimates, of some unknown (but expected small) magnitude. 5-23"}, {"section_title": "Indeterminate Responses", "text": "Both the student and parent CATI programs were designed to accommodate responses of \"refusal\" and \"don't know\" to any single question. Typically, refusal responses are given for items considered too sensitive by the respondent. \"Don't know\" responses may be given for any one of several reasons: (1) the respondent misunderstands the question wording, and is not offered subsequent explanation by the interviewer; (2) the respondent is hesitant to provide \"best guess\" responses, with insufficient prompting from the interviewer; (3) the respondent truly does not know the answer; or (4) the respondent chooses to respond with \"don't know\" as an implicit refusal to answer the question. Whenever they occur, indeterminate responses in the data set must be resolved by imputation or otherwise dealt with during analysis. Summaries of maximum refusal and \"don't know\" responses for undergraduate, graduate and first-professional, and parent respondents are shown in Tables 5.9, 5.10, and 5.11 respectively. In each table, statistics are provided separately, by interview section, for the items receiving the highest percentage of refusal responses, \"don't know\" responses, and a \"combination\" of the two types of indeterminate responses. Indeterminate response percentages were calculated only for those respondents reaching a given item and for whom the item was applicable. In general, item refusal rates greater than one percent are considered high. As shown in the tables, most of the maximum refusal rates were in excess of one percent. Not surprisingly, items with maximum refusal rates tended to be among the most sensitive items income and current financial status. Graduate/first-professional students and parents were more likely to refuse these items than were undergraduate students. Many of the items with the highest refusal rates among undergraduates also had the highest refusal rates among graduate/first-professional students. Monthly expenses, loan amounts, savings spent for institution expenses, student and parent income, current financial status, and receipt of remedial instruction were those items most likely to be refused by both undergraduates and graduate/first-professional students. However, graduate and firstprofessional students consistently refused these items at higher rates than undergraduate students. The types of interview items receiving the highest \"don't know\" rates, that is, in excess of five percent, fall into two categories: those appearing sensitive (i.e., SAT scores, student income, parent income, and parent support for the student), and those that appear Wholly innocuous (i.e., commuting expenses, highest education expected, and anticipated community service). The difference between the two types of \"don't know\" responses is punctuated by the difference in mean rates: 25.5 percent for the sensitive items and 7.5 percent for those not considered sensitive. Reflected in this hit;h rate is the likelihood that respondents offered \"don't know\" as an implicit refusal to ansva the particular question. Consistent with findings for the student interview, items related to income and support for education were most likely to evoke \"don't know\" responses from parents as well; the income tax liability item in the parent interview received the highest rate of \"don't know\" responses (46 percent)."}, {"section_title": "5-24 1", "text": "The \"combined\" indeterminate rates (refusal and \"don't know\") showed that the items with the highest \"don't know\" rates were also most likely to have the highest overall indeterminate rates, with the exception of the item asking graduate and first-professional students about their undergraduate loan amounts through 6/93. This result is not unexpected since \"don't know\" responses generally occur with considerably greater frequency than refusals for any given item, and thus tend to contribute much more to the combined indeterminacy rate. Among both student and parent respondents, those items with consistently high combined rates were those asking for parent income and income tax liability for 1991, particularly sensitive topics.  Note: A total of 52,697 respondents were identified as undergraduates according to their year in institution at the beginning of the NPSAS year or when first enrolled at the NPSAS institution during that year (whichever was later). ' Respondents could refuse to answer any question or indicate that they did not know the answer to any question. Items with the highest rates of the combined indeterminate responses are also shown as \"combined.\"\u00b0 The percent of respondents was calculated only for those respondents who reached the item and for whom it was applicable. This item also yielded the highest rate for graduate and first-professional students.  Note: A total of 13,399 were identified as graduate and first-professional students according to their year in institution at the beginning of the NPSAS year or when first enrolled at the NPSAS institution during that year ( whichever was later). ' Students could refuse to answer any question or indicate that they did not know the answer to any question. Items with the highest rates of the combined indeterminate responses are also shown as \"combined.\" h The percent of students was calculated only for those students who reached the item and for whom it was applicable. ' This item also yielded the highest rate for undergraduate students. Note: A total of 11,281 parents were interviewed. ' Parent could refuse to answer any question, or indicate that they did not know the answer to any question. Items with the highest rates of the combined indeterminate responses are also shown as \"combined.\" b The percent of respondents was calculated only for those parents who reached the item and for whom it was applicable."}, {"section_title": "5-27", "text": ""}, {"section_title": "5-29", "text": ""}, {"section_title": "Interview Timing", "text": "Average time for interview administration, by interview section and by major student sampling stratum', is shown in Table 5.128. The cumulative effects of break offs in each successive section introduces differential numbers of cases contributing to different section times (the number of cases is a monotone nonincreasing function over successive sections of the interview). The total interview time shown is the sum of the section times (and probably represents a more realistic estimate of administration time than that obtained only from those completing all sections of the interview)9. While overall administration time was approximately 31 minutes, time for the B&B sample members (39.6 minutes) was greater than that for non-B&B graduate/firstprofessionals (30.8 minutes), which in turn was greater than for non-B&B undergraduates (27.9 minutes). The additional time required for B&B sample members was due, in the main, to additional questions asked of this group; such questions were asked in Sections B, E, F, J, and K, in each of which administration time is greater for the B&B group. Increased administration time for non-B&B Graduate/First-professional students over that for non-B&B undergiaduates occurs principally in Sections A, C, and F, reflecting the larger number of institutions attended, more complex aid packages, and greater educational expectation detail for the graduate-level students. Overall administration time for sample members completing all sections of the student interview, crossclassified by level of offering and control of NPSAS institution from which they were selected, is shown in Table 5.1310. Between sector differences are minimal, and do not exceed what would be expected due to differential student strata sampling rates among the sectors considered\". This reflects final classification; because of the definitions used plus the nature and timing of sampling, B&B sample members appear in both the undergraduate and graduate/first-professional final analysis files. These analyses were restricted to sample members maintained in the graduate and undergraduate final analysis files. Defined cases contributed to timing results for a specific section only if: (a) the elapsed time to complete a section was positive, (b) all prior section times (if any) were positive, (c) cumulative timer showed increasing times across all prior sections (if any), and (d) section completion time did not exceed 65 minutes. Since burden is a widely accepted contributing factor to interview \"breakofr, it is likely that those who broke off the interview were taking longer to complete it than those who did not. These ,inalyses were also restricted to sample members maintained in the graduate and undergraduate final analysis files. Defined cases contributed to overall timing results only if: (a) all interview sections (A-K) were completed, (b) all section completion times were positive (nonzero), (c) cumulative interview time increased over all sections, and (d) completion time was not less than 5 minutes and did not exceed 125 minutes. Exclusion rule differences between Table 5.13 and Table 5.12 account for different total number of cases. Separate unreported analyses, crossclassifying institutional sector and major student stratum, showed no meaningful administration time differences among sectors, when student strati, was controlled."}, {"section_title": "5-31", "text": "1 t: Overall administration time within student strata for selected student characteristics are shown in Table 5.1412. Because of differential distributions across major student strata, and previously shown timing differences across strata, the relevant comparisons in this table are within student strata. No meaningful gender differences are observed, and while generally few consistent differences emerge, they may be worthy of note. Within the non-B&B undergraduate group, unclassified students took longer to complete the interview than other groups. This probably reflects two factors: (a) included in this group are individuals who could not be classified due to insufficiency of record abstract data and when abstract data were not available, additional questions were asked of students to try to capture these data during the interview; and (b) also included in the group are \"special students\", many of whom had considerably broader educational backgrounds than the typical student and for whom capturing these data took additional time. Within the B&B and non-B&B graduate-level group, administration time was consistently lower for first-professional students than for graduate students. This may reflect more straightforward educational backgrounds (e.g., fewer institutions involved) and/or less complex loan packages among the first-professional students; however, it may also reflect more work experience to report during the NPSAS year among the graduate students. Also, within all student strata groups, administration time for white students was less (usually markedly so) than that for students of other races. This may also reflect differences in educational backgrounds, loan packages, and/or work experiences to report. Exclusion rules used for statistics reported in Table 5.14 are identical to those used for Table 5.13. Note: A section was considered complete if (1) the amount of time to completion was a positive (nonzero) value;"}, {"section_title": "5-32", "text": "(2) all previous section times were positive (nonzero) values; and (3) the cumulative time had an increasing value across sections. Section completion times greater than 65 minutes were considered outliers and, therefore, excluded from timing calculations. The number of cases contributing to timing results in each cell represents only those meeting the criteria for a completed section, excluding outliers. Because of increasing cumulative break offs in each successive section, the monotone nonincreasing function of cases over increasing sections is expected. a Reflects final classification; because of thc definitions used plus the nature and timing of sampling. B&B sample members appear in both the undergraduate and graduate/firstprofessional final analysis data files. Total time is determined as thc sum of thc section times; because of unequal numberc 4..ontributing to section times, the total count is not defined (NA). 12.6 5-33 Note: All analyses were restricted to those sample members maintained in the final analysis files and for whom: (1) all interview sections (A through K) were completed, (2) the time to complete each section was a positive (nonzero) value, and (3) the.cumulative interview times increased across sections; outlier interview times of less than 5 minutes or more than 125 minutes were also excluded from timing calculations. Some cells were combined to maintain adequate sample sizes. Note: All analyses were restricted to those sample members maintained in the final analysis files and for whom: (1) all interview sections (A through K) were completed, (2) the time to complete each section was a positive (nonzero) value, and (3) the cumulative interview times increased across sections; outlier interview times of less than 5 minutes or more than 125 minutes were also excluded from timing calculations."}, {"section_title": "5-34", "text": "a Reflects final classification; because of the definitions used plus the nature and timing of sampling, B&B sample members appear in both the undergraduate and graduate/first-professional final analysis data files. The four respondents refusing to report gender during the telephone interviews, and for whom no other information on gender was availahle, were not included in the analyses. The \"other\" category includes those sample members whose race was indeterminate as well as those who reported a race other than others shown. d Generally, level in institution was based on student's status at the beginning of the school year. If requisite information was missing, however, level in institution was estimated based on input variables for degree program, the student sampling stratum, and financial aid information; the unclassified undergraduate category includes those for whom exact undergraduate classification could not be otherwise determined as well as those reporting \"unclassified\" or \"special student\".  Figure 5.4 displays the number of completed student interviews by hour of the day (based on the time zone of the originating call, that is, cer tral standard time for the Abt Telephone Center and eastern standard time for RTI's). The centers operated from 7:00 am to 10:00 pm. The most productive hours for interviewing were from 5 pm through 7 pm. However, the chart does indicate that the daytime hours were very productive as well. Early morning and late evening counts consists mainly of appointments rather than \"cold calls.\"  CHAPTER 6 FILE CREATION AND DATA ANALYSIS Data from the NPSAS:93 and other NCES data programs are made available through the Data Analysis System (DAS) and the Electronic Code Book (ECB). NPSAS:93 studentlevel data are derived from institutional records data and student and parent telephone interviews. This chapter describes how the NPSAS:93 files are organized and the processing steps completed between the collection of the raw survey data and the release of analysis files."}, {"section_title": "6.1", "text": "Overview of the 1993 NPSAS Files Table 1.1 in chapter 1 provides a summary of the data sources used in the creation of the NPSAS:93 files. For analyses, data may be drawn from any of 16 separate data sets for undergraduate students and graduate students (including first-professionals) and parents. The institutional records data (CADE) and telephone interview (CATI) files contain data either abstracted directly from institutional administrative records or entered during telephone interviews with students and parents. Data from all parent interviews are included in a single data set. Variables were constructed from either the CADE and/or CATI. For each of the derived variables, the ECB includes an indicator for the source of the information on a student level. The verbatim files include responses from \"Other, specify\" items and verbatim response to items concerning student's majors, and the industry and occupation of jobs held by the student. (Major and industry and occupation were coded into standard classificaiion schemes during the telephone interviews using software developed by NCES for this purpose and the codes for these items are in the derived variable files.) 6.2"}, {"section_title": "Editing", "text": "Following the completion of data collection, files were created for undergraduate and graduate students based on the record abstraction information, student telephone interviews, and parent telephone interviews. In addition, separate data files were created for the B&B students. For the student telephone interview data, the B&B files contained data from a section of the questionnaire that was administered only to the B&B cohort as well as data from other sections of the questionnaire. Each of these seven files (CADE and CATI data for undergraduate and graduate students and for the B&B cohort and the parent telephone interview for all students) was edited separately following range and inter-item consistency checks. Range checks are summarized in the variable descriptions contained in the ECB and DAS. Inconsistencies between data elements, either between the instruments or within instruments were resolved in the construction of the derived variables. Protocol for resolving these descrepancies are described in the variable descriptions contain in the ECB and DAS. "}, {"section_title": "Coding", "text": "All coding in the NPSAS:93 telephone survey was completed during the interview. Verbatim responses to telephone interview items concerning student major and the industry and occupation represented by student jobs were coded during the telephone interview using NCES-developed software that presents a code or several codes for the interviewer to confirm with the student/parent. Responses to other types of questions concerning future plans or reasons for declining financial aid were field-coded. Interviewer proficiency at coding respondents' answers was monitored and retraining was conducted as necessary."}, {"section_title": "Overall Study Response Rates", "text": "The students inciu.-i-A in the final NPSAS:93 analysis data base were defined to be the overall study respondents. A more stringent response definition was imposed for the sample selected as the baseline cohort for the baccalaureate and beyond (B&B) longitudinal study. The B&B response rates are considered in the second subsection below."}, {"section_title": "Base Study Response Rates", "text": "Of the 82,016 sample students selected from eligible sample institutions, 79,269 were ultimately determined to be eligible sample students. An eligible sample student was defined to be a study respondent (included in the analysis data base) if any of the following conditions were satisfied: (1) data were successfully collected for at least Section A of the student CATI interview; (2) data were successfully collected for at least Section L of the parent CAT1 interview; (3) CADE data indicated that the student received federal financial aid other than aid awarded by the Veteran's Administration or the Department of Defense; (4) the student was identified as a Pell grant recipient, including matches to the Department of Education's 1992-93 award files based on the student's social security number; or (5) a sufficient amount of CADE data were abstracted for the student, depending on student level (undergraduate, graduate, or first-professional). Using this definition of the overall study response status, Table 6.1 shows that 66,096 of the 79,269 eligible sample students were classified as respondents for an unweighted response rate of 83.4 percent. This table also presents the base study response rates by various institutional and student characteristics derived from the IPEDS IC file and from the CADE data. The final analysis file variables were not used to construct this table because they were usually defined only for the study respondents."}, {"section_title": "13e;-2", "text": "This table also presents \"weighted\" and \"effective\" response rates. The weighted response ratt.,3 are based on the student sampling weights with adjustments for institutional nonresponse and for student multiplicity (attendance at more than one NPSAS-eligible institution during the NPSAS year). These response rates can be interpreted as the estimated percentages of students attending institutions willing to provide lists for student sampling who would have been classified as respondents, if selected. The overall weighted response rate in Table 6.1 i', 79.5 percent. The weighted response rates by institutional and student categories are generally comparable to the unweighted response rates. By late February 1994, the CATI response rates had not yet achieved the study goals of a 92 percent response rate for the B&B cohort and an 85 percent response rate for the remainder of the sample. To shorten the time needed to meet the response rate goals, a nonresponse follow-up subsample was selected. Hence, Phase 1 data collection was closed out as of the close of business on Sunday, February 27, and a nonresponse follow-up subsample was selected from the remaining nonrespondents as of that point in time. One thousand of approximately 21,000 B&B nonrespondents and 5,000 of approximately 40,500 non-B&B nonrespondents were selected for the Phase 2 nonresponse follow-up subsample. No new interviewing procedures or incentives for participation were introduced for the nonresponse follow-up subsample; the interviewers simply worked the cases in the nonresponse follow-up subsample more intensively during the final weeks of data collection. The effective response rate for each stratum for which a nonresponse subsample was selected can be represented as where RI is the Phase 1 response rate and R2 is the response rate achieved among those units selected for the nonresponse follow-up subsample.  'Based on student record abstraction (CADE). The effective overall weighted response rate for the base study is shown to be 85.0 percent in Table 6.1. The effective response rate exceeds the weighted and unweighted response rates for all types of institutions and all types of students indicating that higher response rates were achieved in the nonresponse follow-up subsample consistently across all types of institutions and all types of students. Because students were included in the NPSAS:93 analysis file (i.e., considered to be a study respondent) based on availability of sufficient CADE or CATI data, or ED records for receipt of. a Pell grant, Table 6.2 summarizes the types of data that are availability for the 66,096 study respondents. Students are classified with respect to having any CADE 6-5 abstraction data, having completed at least Section A of the student CATI, or having completed at least Section L of the parent CATI, treating students with matching Pell grant data from ED as having CADE data. Most of the study respondents (79.2 percent) have student CADE and CATI data--including about 16 percent also have parent CATI data. However, 19.6 percent have only CADE abstraction (or matching Pell grant) data. Sample students were assigned to the baseline cohort for the Baccalaureate and Beyond (B&B) longitudinal study if they were awarded their baccalaureate degree at any time between July 1, 1992 and August 31, 1993. The number of eligible sample students identified as belonging to the B&B cohort was 16,316. Students were defined to be respondents for B&B cohort analyses only if they had completed at least Section A of the student CATI interview because the data collected in subsequent follow-up interviews requires baseline data for comparison. Table 6.3 shows that the total number of eligible B&B sample students who were respondents under this definition was 11,810, or 72.4 percent of the eligible B&B sample members. This table also shows that the weighted and effective response rates for the B&B baseline cohort were 76.1 and 83.4 percent, respectively. The response rates are presented in this table for various institutional and student categories of interest. The weighted response rates can be interpreted as the estimated percentages of students receiving baccalaureate degrees from institutions willing to provide lists for student sampling who would be classified as B&B cohort respondents, if selected. 'Based on student record abstraction (CADE)."}, {"section_title": "6-7", "text": ";- BEST COPY AVAILABLE 6.5"}, {"section_title": "Derived Variables", "text": "Approximately 800 variables have been constructed based on data collected in the NPSAS:93. These derived variables are listed in Appendix A. As a general rule, the constructions of derive variables that concern financial aid and other financial descriptors depend first on record abstract data from the CADE system. These data are supplemented in many cases with information collected in the telephone interviews with parents and students. As between parent and student data, precedence was generally given to parent data for variables concerning family income and assets. The rules for construction derived variables are described in the ECB and DAS."}, {"section_title": "6.6", "text": "Imputed Values Imputations were performed on seven variables that contained missing values. The imputation procedures and a comparison of the pre-and post-imputation values for these variables are presented in Appendix D."}, {"section_title": "CHAPTER 7 WEIGHTS AND VARIANCE ESTIMATION", "text": "Three sets of analysis weights have been prepared for analysis of the NPSAS:93 data. The three sets of weights are for analysis of the data collected for: (1) the 66,096 base study respondents (see Table 6.1); (2) the 11,810 B&B baseline cohort respondents (see Table 6.2); and 3the 77,624 respondents for student data abstraction (CADE) (see Table 4.2). Each set of weights contains an estimation weight for computing point estimates of population parameters and estimating population relationships (e.g., regression coefficients). Also, the base study respondents and the B&B baseline cohort respondents have 42 replicate weights for computing sampling variance estimates using the Jackknife replication technique. This chapter describes how the weight components were computed . Institution-level weight components are discussed in Section 7.1, and student-levei weight components are discussed in Section 7.2. How these weight components were utilized to compute each of the three sets of weights listed above is then summarized in Section 7.3. Sampling error estimates are discussed in the final section of this chapter. Construction of Taylor series strata and replicates for estimating variances u sing the Taylor series linearization technique is discussed. Construction of the Jackknife replicates and use of the Jackknife replicate weights for variance estimation is discussed. Standard error estimates computed using the Taylor series and Jackknife replication methods are compared, and survey design effects for estimates of population percentages for categorical variables are analyzed."}, {"section_title": "7.1", "text": ""}, {"section_title": "Institution-Level Weight Components", "text": "Institution-level weighting begins with the sampling weights based on the probabilities of selection for the primary sampling unkts (PSUs) selected into the area sample and the probabilities of selecting the individual institutions within the survey PSUs (both sample and certainty PSUs). The sampling weights of a few institutions are then adjusted to account for the fact that they were represented by more than one record on the sampling frame. Finally, adjustments are made to reduce the potential for bias that could result from institution nonresponse."}, {"section_title": "Sampling Weight Components", "text": "The sampling weight components are the reciprocals of the probabilities of selection at the first two stages of sample selection. The first weight component (WT1 on the analysis file) is the reciprocal of the probability of selecting the area PSU in which the institution is located, given by (6) in Chapter 2. The second weight component (WT2 on the analysis file) is the reciprocal of the conditional probability of selecting the sample institution at the second stage of sampling, given that the area PSU in which it is located was selected at the first 7-1 stage of sampling, which is given by (8) for institutions selected from the 86 certainty PSUs and (10) for those selected from the 90 sample PSUs. When calling the NPSAS:93 sample institutions to identify on-campus coordinators, RTI staff attempted to determine if there were any branch campuses associated with the sample institutions. If an institution had branch campuses, RTI staff attempted to determine if they were separately listed on the combined institutional sampling frame (IPEDS IC file and OPE_IDS file). If they were not separately listed, staff attempted to obtain a single list of students that represented all the branches. Five institutions with branches were identified for which only one branch was listed on the sampling frame and for which the institution was not able to provide a composite student list for all the branches. For each of these institutions, one branch was selected at random as the sample branch. Thus, the weight factor (WT3 in the analysis file) associated with this stage of subsampling is the number of branches from which one was selected at random. The affected institutions and their associated weight factors are listed below. 114266   2   219204   7  148177   2   207014   2   122436   2 In addition, there were sample institutions for which the frame contained records for multiple campuses but not for all the campuses. In this case, the preferred sampling approach was to uniquely link each campus that was not listed to the closest campus that was listed. Then, the sample was defined to include the selected campus and any linked campuses. However, for three institutions, the number of campuses that were not listed was moderately large and a decision was made that the process of uniquely linking unlisted campuses to listed campuses would be such a burden for the institution that their participation would be endangered. Hence, for these three institutions, the campus corresponding to the sample record was retained in the sample, and that record was weighted as if the listed institutions were an equal probability subsample from all the campuses. Thus, for these three institutions the subsampling weight component (WT3) is the ratio of the total number of campuses divided by the number listed on the sampling frame, as shown below. When processing the NPSAS:93 sample of institutions, RTI staff identified 10 instances where the students at an institution were linked to more than one record on the institutional sampling frame. In eight cases, there were pairs of records on the frame that both represented the same institution, either because of frame errors or because institutions had merged. In two cases, the situation was slightly different. In every case, a multiplicity adjustment to the sampling weights was implemented to account for higher probabilities of selection for students with multiple linkages to the institutional sampling frame. The eight instances involving simple pairs of institutional records are discussed below, followed by the situations for the remaining two institutions.\nThe sampling rates used for the stratified, systematic samples of students were preserved in an institution-level data base by student sampling stratum. The reciprocals of these sampling rates were the initial student weight components (WT7 in the analysis file)."}, {"section_title": "IPEDS ID WT3", "text": ""}, {"section_title": "IPEDS ID", "text": "In two of the eight cases in which a pair of sample records accessed a single institution, one sample record was selected from the IPEDS-based frame, and the other record was selected from the supplemental (OPE-IDS) frame. In the other six cases, the two sample records were both selected from the IPEDS-based frame. In every case, it was not clear that the two sample records accessed the same institution until Rn staff began making telephone calls to the schools to identify study coordinators. Other undetected multiplicities probably exist, but there appears to be no practical way to identify them. Weight adjustments were implemented for the eight institutions identified as linked to two separate frame records. For the purpose of operationally administering the sample, one of the two records was classified as ineligible, and the survey results were tracked under the other institution's identification number. However, for weighting purposes, records could not simply be ignored and treated as if they were an ineligible, duplicate frame listings because the institutions were selected into the sample if either of the frame records was selected. Therefore, RTI staff calculated the probability of the institution being selected into the sample as the probability that either Record A or Record B was selected, where these are the two records that were found to both link to the same institution. Treating these records as if they welt selected from different sampling strata (technically, different zones, or implicit strata, using the Chromy (1987) sequential sampling method), the probability of selecting the institution was computed as P(A or B) = P(A) + P(B) -P(A)P(B), (27) where the probabilities of selection, P(A) and P(B), are given by (8) or (10) in Chapter 2, depending on whether the institution was located in a certainty or non-certainty PSU. The multiplicity weight factor (WT4 in the analysis file) was then computed for these institutions as the ratio of the probability of selection that resulted from application of (8) or (10) for the individual sample record divided by the conditional probability of selection computed for the institution as shown above."}, {"section_title": "7-3", "text": "For each of these eight institutions, the multiplicity was detected soon enough that only one list of students was obtained for selection of the student sample. Therefore, no adjustment to the student sampling rates was necessary. The conditional probability of selecting a student was the rate actually used with the one student list received from the institution. In the first of the remaining two institutions, two campuses were selected into the NPSAS:93 sample. The student list received for the first campus contained the students enrolled at either campus. The list received for the second campus contained only the students enrolled at that campus. This situation was not detected until CADE data were being collected. Hence, there was no multiplicity problem for students enrolled at the first campus, but every student enrolled at the second campus had two independent chances of selection, one based on the list provided for the first campus and one based on the identical list provided for second campus. Therefore, the second campus was treated as having been selected twice. Hence, the institutional probability of selection was computed for this campus (27), where P(A) and P(B) refer to the separate probabilities of selection for the frame records representing the two campuses based on (10) in Chapter 2. The multiplicity weight factor (WT4 in the analysis file) was then computed for all students selected from the second campus (including those selected from the list provided for the first campus) as the ratio of the probability of selection that resulted from application of (10) for the individual sample record divided by the conditional probability of selection computed for the institution using (27). Moreover, since RTI received two lists of students for the second campus and selected an independent sample of students from each list, staff made a similar weight adjustment for the student-level probabilities of selection for the second campus, as described in Section 7.2 below. Two campuses of the second institution were selected into the NPSAS:93 sample. The lists received for the two campuses were not identical; however, each list contained students enrolled at the campuses of the institution. Four of these six campuses (including the two selected campuses) were listed as separate institutions on the composite (IPEDS/OPE-IDS) sampling frame. However, the two sample campuses/institutions were both certainty selections. Therefore, multiplicity adjustments were necessary only at the student level."}, {"section_title": "Nonresponse Adjustment", "text": "RTI used standard sample-based weighting class weight adjustment procedures to compensate for institution nonresponse to the request for student lists for sample selection (Ka lton and Maligulig, 1981). Institution-level response rates by institutional level, control, and size were examined to determine appropriate weighting classes. Some of the results are shown in Table 2.7. Table 7.1 presents the institution-level response rates for the weighting classes adopted to adjust for institutional nonresponse."}, {"section_title": "7-4", "text": "The weight adjustment factors (WT5 in the analysis file) shown in Table 7.1 vary from 1.02 for both public, less-than-2-year institutions and private, not-for-profit, Masterslevel institutions to 1.40 for private, not-for-profit, doctoral-granting institutions. These weight adjustments are the reciprocals of the weighted institution-level response rates shown in Table 7.1. After obtaining lists for student sampling, RTI staff were unable to abstract student data from the records of about two percent of the sample institutions (see Table 4.1). The students sampled from these institutions were still eligible for CATI data collection, so this level of institutional nonresponse does not affect the student weights computed for the base study respondents. However, it does affect the set of weights computed for analysis of data from the CADE abstraction. Therefore, another weight adjustment factor was computed to compensate for nonresponse of institutions to the CADE data collection, given response to student sampling. Response rates by the weight adjustment classes discussed above for nonresponse to the request for student sampling lists were examined. Because only about two percent of these institutions were CADE nonrespondents, similar weighting classes with little difference in response rates were collapsed. The weighting classes for institution nonresponse to CADE and the weight adjustment factors (WT6 in the analysis file) are presented in Table   7.2."}, {"section_title": "7.2", "text": ""}, {"section_title": "Student-level Weight Components", "text": "Student-level weighting begins with the sampling weights based on the sampling rates used to select stratified, systematic samples of students from the lists provided by the sample institutions. The sampling weights were then adjusted to account for the fact that some sample students attended more than one eligible institution during the NPSAS year, and, hence, had multiple linkages to the institutional sampling fr-me. A generalized raking procedure was then used to adjust the sampling weights of all the eligible students so that they sum to population totals based on ED records. In particular, control totals were established for total annual enrollment, number of Pell grant recipients, and total dollars of Pell grants awarded by post-strata. Logistic models for propensity to respond were then established and used to compensate for the potential bias due to student-level nonresponse. The logistic models for nonresponse were constrained so that most poststratification totals based on the raking models were preserved. The resulting weights included some values that were such outliers that they would have resulted in considerable variance inflation. Therefore, outlier weights were truncated and the raking models were re-run to restore the poststratification totals. Each of these weight components is discussed in the subsections that follow."}, {"section_title": "7-5", "text": "All of the students listed on the sampling frame provided by Cornell-Statutory University and many of the students on the frame provided by Pontifical Catholic University were found on two separate lists provided by these sample institutions (see Section 7.1.2). Letting, P(A) and P(B) represent the systematic sampling rates used with the two lists on which a student's name appeared, the sampling rate for each student that appeared on two lists was re-computed using (27), and this rate was used as the basis for computing the initial student weight component. The initial sample was subsampled before being fielded when the sample selected was 100 or more students greater than expected based on the frame (IPEDS) data. The reciprocals of these subsampling rates are the second student-level weight component (WT8 in the analysis file). In a few cases, this weight factor was also used to compensate for the fact that all the student lists were not received (e.g., RTI did not receive lists of students enrolled in the summer session). For most students, the subsampling adjustment factor was unity (1.00).  "}, {"section_title": "7-6", "text": ""}, {"section_title": "Multiplicity Adjustments", "text": "Students who attended more than one NPSAS-eligible institution during the NPSAS year (1992-93) would have been listed as a student eligible for sample selection if either of these institutions had been selected in to the sample. Therefore, these students have a higher probability of being selected than comparable students who attended only one NPSAS-eligible institution. The number of NPSAS-eligible institutions that a student attended during the NPSAS year is referred to as the student's multiplicity for sample selection. The simplest adjustment for multiplicity that results in unbiased estimates of population parameters is to divide the student sampling weight by the multiplicity. Therefore, the third student-level weight component (WT9 in the analysis file) is the reciprocal of the student's multiplicity. The multiplicity is was determined from the student's response in the CATI interview and was presumed to be unity (1.00) whenever it was unknown. 7-8 7.2.3"}, {"section_title": "Generalized Raking Adjustments", "text": "The sampling weights for all eligible NPSAS sample members were adjusted to control totals to ensure population coverage using a generalized raking procedure by fitting an exponential regression model . This adjustment partially compensates for differences between the NPSAS year for the survey population and that for the true target population. Control totals were established for: numbers of Pell grant recipients in the 1992-93.award year by type of institution; total dollar amounts of Pell grants in the 1992-93 award year by type of institution; and total unduplicated student enrollment in the 1992-93 academic year by type of student and type of institution. The Pell grant control totals were provided by the Department of Education and are presented in Table 7.3. The unduplicated annual enrollment totals were estimated from fall enrollment totals obtained from the 1992 Fall Enrollment Survey. Ratio estimates of total unduplicated enrollment were computed by multiplying the fall enrollment totals from the Fall Enrollment Survey by the survey estimate of the ratio of total enrollment to fall enrollment for each poststratum shown in Table 7.4. Both the 1992 fall enrollment totals and the computed ratio estimates of total enrollment, used as the control totals, are presented in Table 7.4. The generalized raking model adjusted the survey weights for all eligible sample students to simultaneously achieve the control totals for Pell grants and for total unduplicated enrollment. The mathematical formulation of the model is presented in Appendix E. The model was run for two sets of study-eligible students: (1) for all 79,269 eligible students in the 1,098 sample institutions that provided a list for student sampling (i.e., all study-eligible sample students) and (2) for the 78,289 eligible sample students in the 1,079 institutions that provided CADE data for at least one sample student. The former weight adjustment factor (WT1OS in the analysis file) was used for computing the base study weights. The latter factor (WT10C in the analysis file) was used for computing the analysis weights for the CADE data base. These generalized raking weight adjustment factors can be summarized as shown below.   Adjustments for Student-level Nonresponse By now, the CADE weights had already been adjusted for institutional nonresponse for CADE data abstraction. This weight adjustment was not applicable for the base study and B&B weights, as discussed in Section 7.1.3, because CADE nonrespondents were still eligible for CATI interviews. Hence, for the CADE weights only, the adjustment for student-level nonresponse was to compensate only for the approximately one percent of students from whom no CADE data were abstracted, among those institutions for which CADE data were obtained for at least one sample student (see Table 4.2). Therefore, simple weighting-class ratio adjustments were implemented for the CADE nonresponse adjustments. The CADE weight adjustment factors for student-level nonresponse (WT11C in the analysis file) were 7-11 1.005 for undergraduates, 1.007 for graduate students, and 1.005 for first-professional students."}, {"section_title": "Weight Set", "text": "All students who had been identified in CADE as having received federal financial aid (other than from the Veterans Administration or the Department of Defense) were defined to be base study respondents. Also, all students identified as having received a Pell grant based on matching to Department of Education administrative records, or based on the CADE and CATI data if no social security number was available, were defined to be base study respondents. Therefore, because these 28,721 sample students were study respondents by definition, they were excluded from the nonresponse weight adjustment, and their weight adjustment factor for nonresponse was set to unity (LW) for the base study weights. Logistic models for the propensity to respond were used to compensate for the potential bias due to nonresponse among the remaining eligible sample students (Folsom, 1992). Logistic models were fit for: (1) the 50,548 eligible sample students whose nonresponse adjustment factor was not set to unity as described above for the base study weights (WT1 IS in the analysis file) and (2) the 16,316 eligible sample students who were identified as having received a baccalaureate degree at any time between June 1, 1992 and August 31, 1993 for the B&B baseline cohort weights (WT11B in the analysis file). The data base of 50,548 eligible sample students for the base study weights was too large to fit a single logistic model for nonresponse. Therefore, the data file was divided into three subsets based on institutional level and control: (1) 15,659 students attending a private, for-profit institution or attending a public or private non-profit institution for which the highest level of offering was baccalaureate or less; (2) 24,818 students attending a public institution for which the highest level of offering was masters or higher; and (3) 10,071 students attending a private institution for which the highest level of offering was masters or higher. Separate logistic models for propensity to respond were run for each of these three sets of students. In addition, a fourth logistic model for propensity to respond was run for the 16,316 eligible sample students in the B&B baseline cohort. The mathematical formulation of the logistic models is presented in Appendix F. The variables that could potentially serve as predictor, or independent, variables in the logistic models had to satisfy two characteristics. First, they must have non-missing data for most of the eligible nonrespondents. Thus, institutional variables from the IPEDS data base and CADE variables with low levels of missing data were the primary variables available for the nonresponse models. Second, of course, the variables retained in the final models were those found to be predictive of response status. Student level (undergraduate, graduate, or first-professional) and the nine categories of institutional level and control used for the generalized raking were retained in each model for propensity to respond so that the generalized raking totals for unduplicated enrollment in Table 7.4 would be preserved. However, Pell grant status and dollar amount were not used in the models because all Pell recipients were excluded from the models for the base study weights, as discussed above (except for 453 imputed Pell recipients, only 74 of which were 7-12 respondents). Hence, the Pell grant control totals shown in Table 7.3 were not completely preserved by the logistic models. Potential independent variables based on CADE data that were considued but dropped because of high levels of missing data among the study nonrespondents were: (1) place of residence (on campus, off campus without parents, with parents, unspecified); (2) dependency status (dependent, independent, unknown); (3) student income; and 4parent income. The predictors of propensity to respond that were retained in the final models are presented in Table 7.5 for the three models fit for the base study weights and for the model fit for the B&B weights. Each of the retained variables was statistically significant in the final model at the 15 percent level of significance. OBE Region and gender were considered as potential explanatory variables but were not retained in any of the final models because they were not significant at the 15 percent level. The logistic models for nonresponse were first run with no constraint on the size of the weight adjustment factors. The weight adjustment factor exceeded three (3.00) for 425 of the 79,269 eligible sample students for the base study weights, and the maximum weight adjustment factor was 5.06. All models were then constrained using the technique developed by Devi Ile and Sarndal (1992) so that no weight adjustment factor exceeded three (3.00). The weight adjustment factors resulting from the final constrained logistic models for nonresponse can be summarized as shown below."}, {"section_title": "Weight Set Mean Median Maximum", "text": "Base study weights 1.20 Because the logistic model adjustments for nonresponse will be most effective if the models provide a good fit to the observed pattern of survey response, goodness-of-fit tor the four logistic models were investigated. In most logistic modeling applications, the goodness of fit is usually measured by the \"-2 log likelihood\" statistic. However, for surveys with large sample sizes, like the NPSAS, the power (the probability of rejecting the null hypothesis) is too high to yield a meaningful test. Therefore, as an alternative, RTI chose to assess the models with an approach that compares the response propensities predicted from the models with the actual response status of the students. To begin this approach, RTI staff computed the estimated response propensities based on the four models for all respondents and nonrespondents. Then, the estimated response propensities were ranked and placed into 25 percentile groups. For these 25 groups, RTI compared the mean response propensity with the actual mean response rate. Figure 7.1 presents the mean response propensities plotted against the mean response rates. The plots show strong associations which indicate that all four models have strong associations between the predicted and actual response rates."}, {"section_title": "7-13", "text": "To provide a quantitative measure, RTI staff also computed the coefficient of correlation, p, for the 25 pairs of predicted and actual response rates. The correlation All four correlation coefficients indicate strong association and are significant at less than the 0.1 percent level of significance."}, {"section_title": "Weight Truncation", "text": "When many weight factors are involved in computation of the final analysis weights for a survey, as was the case for NPSAS:93, the variability in the final weights sometimes becomes so great that sampling variances are inflated, and mean square errors can be reduced by truncating some of the largest weights and re-allocating (smoothing) the truncated weight to preserve weight totals (estimates of population totals). Therefore, after the NPSAS:93 analysis weights had been computed as the product of the weight factors discussed in the previous sections, the survey design effects or variance inflation factors due solely to variability in the final analysis weights were computed. Because students from different institutional sectors had been sampled at quite different rates (see Table 3.2), RTI computed the unequal weighting design effects within institutional sectors, as follows: where each summation, E, is over the \"n\" responding students in a particular institutional sector. The unequal weighting design effect was less than three for the base study weights for all sectors except the public, less-than-2-year institutions, for which the unequal weighting design effect was 14.30. The, efore, a truncation and smoothing adjustment was implemented for the base study and CADE weights. The unequal weighting design effect was less than three for all sectors for the B&B analysis weights, except for the private, for-profit 7-15 institutions, for which it was 3.87. Because this analysis domain was relatively small, truncation and smoothing was not necessary for the B&B weights. Examination of the upper end of the distribution of the base study weights revealed that 22 sample members had weights between 3,258 and 8,653, while the next largest weight was 2,704, and 78 sample members had weights between 2500 and 2704. Twenty of the 22 largest weights were in Stratum 20, the public, less-than-2-year institutions; Lie other two were in Stratum 16. The 20 largest weights in Stratum 20 were all for students from an institution with a measure of size that was too small by about a order of magnitude. The truncation weight factor (WT12S for the base study weights and WT12C for the CADE weights in the analysis file) ratio-adjusted these 20 largest weights down to 2,000. The next largest weight for students in this stratum was 1,709. Similarly, the two largest weights in Stratum 16 were ratio-adjusted down to 3,000. The next largest weight in this stratum was 2,645. All other weights were unaffected by the truncation weight factor."}, {"section_title": "Final Generalized Raking", "text": "The truncated analysis weights were smoothed to sum to the proper population totals by repeating the generalized raking adjustment, discussed in Section 7.2.3, to restore the population totals shown in Tables 7.3 and 7.4. These final raking adjustment factors (WT13S for the base study weights and WT13C for the CADE weights in the analysis file) ranged from 0.96 to 1.07 for the base study weights, and most adjustment factors were very close to unity. The truncation and smoothing adjustments reduced the unequal weighting design effect for students in Stratum 20 (public, less-than-2-year institutions) from 14.29 to 4.65."}, {"section_title": "7.3", "text": ""}, {"section_title": "Final Analysis Weights", "text": "The three sets of NPSAS:93 analysis weights, those for: (1) the 66,096 base study respondents; (2) the 11,810 B&B baseline cohort respondents; and 3the 77,624 respondents for student data abstraction (CADE), were computed as the products of the weight factors described in the previous sections. Those weight factors and the resulting final analysis weights are summarized in Figure 7.2. The NCES Data Analysis System (DAS) requires all analysis weights to be integers. Therefore, the final adjustment for each analysis weight was to round the weights to integral values. Twenty-three of the base study weights were less than one, eleven were less than one-half. All 23 weights were for students selected with certainty from a public, less-than-2year certainty institution in a near-certainty area PSU. The institutional poststratification 7-17 Figure 7.2 Overview of NPSAS:93 Weight Components A. Area-and institution-level weight components I. Area sampling weight (WT1) 2. Institution sampling weight (WT2) 3. Adjustment for subsampling (WT3) 4. Adjustment for multiplicity (WT4) 5. Adjustment for nonresponse of institutions for student sampling (WT5) 6. Adjustment for institution nonresponse in CADE (WT6) B Student-level we,ght components 1. Student sampling weight (WT7) 2. Adjustment for subsampling (WT8) 3. Adjustment for multiplicity (WT9) 4. Generalized raking adjustment a. for all eligibles in the 1,098 responding institutions (WT10S) h. for the B&B respondents (WT1OB = WTIOS) c. for all eligibles in the 1,078 CADE-responding institutions (WTIOC) 5. Adjustment for student-level nonresponse a. logistic models for the base study respondents (WTI1S) b. logistic model for the B&B respondents (WT11B) c. weighting classes for the CADE respondents (WTI1C) 6. Weight truncation factor a. base study respondents (WT12S) b. CADE respondents (WT12C) "}, {"section_title": "7-18", "text": "adjustment (see Table 7.1) resulted in weights less than one for these students. All weights less than one were rounded up to one."}, {"section_title": "Variance Estimation", "text": "Area PSUs and institutions were selected at the first two stages of sampling using sequential sampling from an ordered frame to facilitate formation of analysis replicates and strata for estimation of sampling variances using both the Taylor series linearization method and the Jackknife repeated replication method (see Section 2.3). The first two subsections below present methodology for estimating sampling variances using the Taylor series method and the Jackknife replication method, respectively. In the final subsection, estimates of standard errors computed using these two methods are compared, and survey design effects are examined."}, {"section_title": "Taylor Series Linearization", "text": "Taylor series variance estimates for nonlinear survey statistics are based on representation of the nonlinear statistic by its first-order Taylor series expansion and computation of its variance as if the sampling design were a nested, multistage design with a stratified sample of PSUs selected with replacement at the first stage (Woodruff, 1971). Hence, given the linearization of any nonlinear survey statistic, the essential ingredients for computation of Taylor series variance estimates are the analysis strata and analysis PSUs. Taylor series analysis strata and analysis PSUs were defined separately for the undergraduate sample and the graduate/first-professional sample because they are separate analysis domains for virtually all analyses of NPSAS data and because they comprise separate analysis files in the NCES Data Analysis System (DAS). To ensure stable estimates of sampling variances, each analysis PSU (within analysis stratum) was required to contain at least four respondents for the base study weights and at least five respondents for the B&B weights. In order that the Taylor series analysis strata and PSUs would reflect the design strata and PSUs to the extent feasible, Taylor series strata and replicates were defined separately within each of the following three subsets of the NPSAS:93 sample: (1) non-certainty area PSUs, non-certainty institutions within certainty PSUs, and (3) certainty institutions within certainty PSUs. Construction of the analysis strata and PSUs is discussed briefly for each of these three segments of le NPSAS:93 sample. Area sampling was the first stage of probability sampling for the non-certainty area [7][8][9][10][11][12][13][14][15][16][17][18][19] PSUs. Area sample PSUs or sets of PSUs were defined to be the analysis PSUs for this portion of the sample. OBE Regions or combinations of Regions were defined to be the analysis strata because they defined implicit strata in which area sample PSUs were selected. Institution sampling was the first stage of probability sampling for the non-certainty institutions within certainty PSUs. Institutions or sets of institutions were defined to be the analysis PSUs for this portion of the sample. Analysis strata were generally defined to be pairs of institutions, with the pairing based on the frame ordering. When defining analysis strata and PSUs, RTI staff attempted to not cross state boundaries, and never crossed institutional sampling strata. Student sampling wa3 the first stage of probability sampling for the certainty institutions within ceitainty PSUs. Institutions were generally defined to be the analysis strata for this portion of the sample and half the students in each institution were randomly assigned to each of two analysis PSUs. When institutions had too few students to allow this construction, two or more institutions within an institutional stratum were treated as a single analysis PSU. Given the Taylor series analysis strata and analysis PSUs, variance estimates are computed using the NCES DAS as if the sampling design were a nested multistage design in which the analysis PSUs were selected with replacement within the analysis strata."}, {"section_title": "Jackknife Replication", "text": "There are basically two types of replication techniques used for variance estimation for stratified multistage sampling designs like the NPSAS:93 design. They are balanced repeated replication (BRR) and Jackknife replications. The Jackknife procedure has generally been shown to produce variance estimators that are at least as accurate as, if not more accurate than, their BRR competitors (Kovar et al., 1988). Moreover, the Jackknife variance estimators tend to be less erratic when computing variances for small analysis domains because each Jackknife replicate contains all the sample members except those in a single analysis PSU, whereas each BRR replicate contains only half the analysis PSUs in the sample. Therefore, Jackknife replicates were defined for estimation of NPSAS:93 sampling variances, as they had been for NPSAS:90. To facilitate the Jackknife replication method, the NPSAS:93 design was modeled as if two analysis PSUs were selected within each of 42 analysis strata. Thirty to sixty replicates are usually recommended (Rust, 1986). Because the replication method results in the same number of replicates as analysis strata, 42 analysis strata should be sufficient to yield accurate, but cost-effective, replicate variance estimates. A set of full sample estimation weights and a set of weights for each replicate sample are needed to facilitate the Jackknife replication method of variance estimation. The process of defining analysis strata and analysis PSUs to use as the basis for 7-20 defining Jackknife replicates was essentially the same as described above for defining analysis strata and analysis PSUs for Taylor series variance estimation. One difference was that three sets of Taylor series analysis strata and PSUs were needed to achieve the required minimum number cf: .espondents per analysis PSU within analysis stratum: one set for undergraduate and graduate base study respondents; another for undergraduate and graduate CADE respondents; and a third for B&B baseline cohort respondents. Only a single set of analysis PSUs and analysis strata was needed to construct the Jackknife replicates for all samples. Another difference was that each Taylor series analysis stratum could contain two or more analysis PSUs, but each Jackknife analysis stratum was required to contain exactly two analysis PSUs. At the conclusion of the process of forming the Jackknife analysis strata and analysis PSUs, each sample student belonged to one of two analysis PSUs within one of 42 analysis strata. Each Jackknife replicate was formed by assigning zero weights to the members of one randomly selected analysis PSU within a single analysis stratum and ratio-adjusting the weights of the members of the stratum's other analysis PSU to preserve the analysis stratum weight total (essentially doubling those weights). All other sample members were retained in the replicate with their unaltered estimation weight. Therefore, the number of sets of replicate weights for Jackknife variance estimation is identical to the number of Jackknife analysis strata, namely 42. All weight adjustments, beginning with the first generalized raking adjustment, were then implemented independently for each set of replicate weights. Therefore, the Jackknife replication variance estimates include the variance components due to the nonresponse weight adjustments, which are ignored in the Taylor series variance estimates. Moreover, since the final step of the weight adjustment process was generalized raking to the population totals in Tables 7.3 and 7.4, whenever a function of these totals is estimated from the survey data, the Jackknife estimate of the sampling variance will be essentially zero because the estimates produced by the 42 sets of Jackknife replicate weights will be essentially identical. This is consistent with treatment of the raking totals as population totals that are known without error. Conversely, the Taylor series variance estimates do not treat the raking totals as if they were known without error."}, {"section_title": "Estimates of Sampling Error", "text": "Jackknife and Taylor series estimates of sampling variances are compared in Table 7.6 for estimates of the NPSAS:93 population distributions by institutional sector, by race/ethnicity, and by income/dependency for the undergraduate, graduate, and graduate/firstprofessional populations. Because the Jackknife variance estimates treat the population raking totals as known without error and the Taylor series variance estimates do not, the Taylor series variance estimates are considerably larger for the estimated percentages of the population belonging to tne various institutional sectors. Because the other two analysis variables are not direct functions of the raking variables, the Jackknife and Taylor series variances are comparable for these estimated distributions. However, the residual effect of 7-21 this fundamental difference in the variame estimators remains, resulting in Jackknife variance estimates that are usually less than the corresponding Taylor series variance. They are not always less because the Jackknife variance estimates account for variance components due to nonresponse weight adjustments that are ignored by the Taylor series variance estimates. Therefore, the Taylor series variance estimates, which are computationally more efficient, can generally be used for conservative statistical inferences.' One aspect of the efficiency of the NPSAS:93 sampling design was addressed by calculating the survey design effects shown in Table 7.7 using Taylor series estimates of sampling variances. The survey design effect for a statistic is the ratio of the sampling variance of that statistic under the actual sampling design divided by the variance that would have been achieved with a simple random sample of the same number of ultimate population units. It can generally be factored into components associated with the effects of: (1) stratification; (2) multistage sampling; (3) unequal probabilities of selection; and (4) weight adjustments for nonresponse. Stratification tends to decrease the design effect(and increase precision), whereas multistage sampling, unequal probabilities of selection, and weight adjustments for nonresponse usually increase the design eftect (and decrease precision). Of course, unequal probabilities of selection increase precision for estimates regarding the characteristics of population subgroups that are sampled at higher rates, but decrease precision for estimates of the characteristics of subgroups that cross strata sampled at different rates. Survey design effects were calculated for population distributions &Tined based on the following categorical variables: (1) Institutional sector Receipt of any work-study aid (4) Type of aid package received Receipt of any federal aid (5) Attendance status (13) Receipt of any Title IV aid (6) Gender of student (14) Receipt of any state aid (7) Major program of instruction (15) Receipt of any institution aid (8) Receipt of any aid (16) Receipt of any employer aid. Estimates with denominator sample sizes less than 20 or for which the estimated percentage was less than one or greater than 99 were discarded because they were likely to be unstable. The quartiles of the distributions of the design effects are presented in Table 7.7 by: 'Differences that are significantly different based on the Taylor series variance estimates will usually be significant based on the Jackknife variance estimates, also."}, {"section_title": "7-22", "text": "(1) Size of the percentage estimate, For undergraduate students, the overall median survey design effect was 3.1 for the 2,247 survey statistics that passed the above test for stability of the variance estimate. For graduate students the median was 1.6, and for the combined population of graduate and firstprofessional students the median was 2.0. "}, {"section_title": "Institutional Sector", "text": "Public, less-than-2-year  BEST COPY AVAILABLE  \nPublic. iess-than-2-year  "}, {"section_title": "Introduction", "text": "The overall goal of the NPSAS:93 field test was to evaluate the data collection schedule, systems, and procedures proposed for the full-scale study. Employing and testing methodologies in the field test that parallel the data collection procedures proposed for the main NPSAS data collection allowed these procedures to be adjusted, as necessary, before the much larger main data collection activities began. As shown in Table 8.1, the general objectives of the NPSAS:93 field test were to (1) evaluate the timing of key data collection activities; (2) evaluate data collection systems; (3) develop and test methods for increasing participation in the NPSAS; and, (4) determine whether students can be induced to take the GRE. One of the main areas investigated during the field test was the timing of key data collection activities. Much of the data required in NPSAS is time-sensitive, and institutions are on various different schedules of enrollment that only partially overlap the NPSAS data collection year. Thus, it was important during the field test to determine an optimal way to fit each institution's academic year into a standard NPSAS year beginning July 1 and extending through June 30. The NPSAS data collection must be scheduled to occur at a time during the institution year when institutions have complete enrollment and graduation lists available, because these lists form the core of the student sample frame, a central element of the overall NPSAS sample design. Other areas, such as the disbursement of financial aid in each institution, are also affected in important ways by the integration of the institutional and NPSAS years. A second objective of the field test was to evaluate the integrated data collection systems used to obtain information from institutions, students, and parents. Data collection plans for NPSAS:93 are complex, because data from institutions, students, and parents will be collected using the combined resources of three distinct, automated data collection instruments. These integrated data collection systems are designed to allow information to be collected from the mosi appropriate source and, where necessary, verify or enhance data from one source through responses from another type of respondent. Success of the NPSAS full-scale study depends on gaining the cooperation of numerous individuals within institutions, as well as gaining the coop( ration of students and parents. Thus, a third goal of the field test was to learn about the kinds of barriers to successful participation that might be expected f, .r each type of respondent and to develop methods of overcoming these barriers for the full-scale study. Finally, the fourth major goal of the NPSAS:93 field test was to investigate whether it was possible to obtain GRE test scores from a subsample of students. This feature of the B&B base year was designed to obtain these scores for students who have taken the GRE, and to persuade students who had not taken the GRE to do so.  Each of these general goals must be assessed across the sample design, data collection instruments, and data processing procedures for the full-scale NPSAS:93. The following sections discuss details of how these general areas were evaluated across c ach of the NPSAS data collection tasks during the field test."}, {"section_title": "Institution Survey", "text": "Institutions constitute the first source of information for the NPSAS. Institutions provide the enrollment files and graduation lists that form the frame for the student sample and critical locating, enrollment, and financial aid data about the students selected for the study. In the field test, procedures for enrollment list acquisition were evaluated in order to assure that a comprehensive and accurate student sampling frame could be developed using these procedures. Procedures for abstracting study data elements from administration records maintained by institutions were also evaluated. Of particular interest was an assessment computer-assisted data entry (CADE) software developed for the study and its use by institutional staff. This section describes the procedures used to contact institutions, obtain enrollment and graduation lists, and abstract financial aid and other data from institution records."}, {"section_title": "Institution Contact", "text": "Because essential sampling information and student financial aid data are obtained from institutions sampled as part of the NPSAS design, institutional participation is critical for the success of the full-scale study. For the field test, 88 institutions were asked to participate in the field study. These institutions were selected on the basis of specific criteria, not randomly, to participate in the NPSAS field trial. In order to avoid the selection into the field test pool of any institutions eligible for selection in the full NPSAS study, only institutions that were not located in NPSAS primary sampling units were selected. Of the 88 institutions selected to participate in the field trial, 70 institutions, or 80 percent, provided enrollment and graduation lists. If an institution declined to participate in the field test, the reason was recorded and another institution was substituted. Because the field test was not intended to be statistically representative, there was no intent to spend project resources on intensive refusal conversion. The initial contact with each institution was a letter to the chief administrator, sign.:td by the (then) Acting Commissioner of the Office of Educational Research and Improvement, and materials describing the purpose of the NPSAS program. These advance letters were mailed on February 14, 1992. In the interest of assuring that the letters arrived and were delivered to the chief administrator in a timely manner, the materials were sent via an express mail service. A service was used (rather than the U.S. Postal Service) so that, once delivered, the packets could be traced in the event they were misguided through the institutions interdepartmental mail. Each of the tasks requested of the sampled institutions --naming an institutional coordinator for further contacts, confirming IPEDS data, providing enrollment files, and providing information from student administrative files was clearly outlined in the advance letter. These materials also provided assurances that all data provided by the institution would remain confidential. The need for information to locate students who would be invited to participate in the study was explained, with the assurance that the coordinator would be consulted on the timing and on a means of collecting the information that would be most efficient, least time-consuming, and would provide the lowest possible burden to the staff. Endorsements from organizations with an interest in the study were included in the materials accompanying the initial letter. All institutions that did not respond following the initial mailing were contacted by telephone. The senior data collection staff reviewed each case for a possible personal call. Based on the experience of the NPSAS:90 contractor, we expected private, for-profit institutions would present two unique problems, and thus were a special focus of the NPSAS:93 field test. First, it was anticipated that these institutions would be more reluctant to participate in the research because they might perceive the research activities of NPS AS to be of marginal utility to their primary business. Second, it was anticipated that even among participating institutions the quality of data they provided would suffer because the records might be minimal or nonexistent, may have been moved to centralized locations and be difficult to retrieve, or the institution might no longer be in business."}, {"section_title": "Enrollment and Graduation List Acquisition", "text": "The enrollment and graduation files provided by participating institutions form the sample frame for the telephone surveys of students. A special focus of the NPSAS:93 field test was to examine the availability, comprehensiveness, and quality of enrollment and graduation provided by these institutions. Each institution participating in the field test was asked to submit one list containing no duplicate entries of all eligible students enrolled separated by level (e.g., undergraduate, graduate, and first-professional) for all terms beginning between July 1, 1991 and June 30, 1992. In addition, coordinators at 4-year colleges and universities were asked to submit a list with no duplicate entries of all students com2leting (or expected to complete) baccalaureate degrees between July 1, 1991 and June 30, 1992. To be eligible, a student must have a high school diploma (or its equivalent) and must be enrolled between the above dates in a course for credit, in a degree or formal award program of at least 3 months duration, or in an academically, occupationally, or vocationally specific program of 3 months or 300 hours. The likely degree of institutional participation in the record abstraction process was an important factor for planning the full-scale study."}, {"section_title": "Multiple campus institutions", "text": "The results of the NPSAS:90 data collection demonstrated potential problems generated as a result of sampled institutions having multiple campuses. Ideally, such multicampus clusters would be listed only once under the name of the main branch of the institution in the IPEDS frame of institutions. If the main branch were selected for the sample, the affiliates, as well as the main branch, would supply independent enrollment lists in order to build a comprehensive frame of students that contained no duplicate listings. However, because of mergers and acquisitions among institutions, a campus listed in IPEDS as an affiliated branch of a sampled institution may formerly have been an independent institution with a separate listing in the IPEDS. If the IPEDS information were not updated in a timely faFhion, that affiliate campus in effect had two opportunities for selection into the NPSAS sample: once as a separate institution in its own right (the out-ofdate listing) and once in its new identity as an affiliate of another institution (the current listing). Several decisions were made in developing the NPSAS:93 field test to allow appropriate inclusion of institutions listed in the IPEDS under multiple entries (as described in the previous paragraph). If both the main branch and the affiliate were selected for NPSAS, the institutional coordinator at the main branch was asked to provide enrollment lists for both sites, and for other campuses of the institution as well. If only the main branch was selected in the NPSAS frame, lists from the affiliate were not requested because they had already had a chance to be selected for the sample. If only the affiliate were selected, lists were obtained from the main branch of the institution and all of .its affiliated campuses. Procedurally, this meant checking for potential IPEDS listings for all affiliated campuses. The extent of this verification and its efficacy were important for planning the full-scale study."}, {"section_title": "8-4 I 0", "text": "Abstracting Financial Aid and Other Data from Student Records Following student sample selection, institutions were recontacted at the second stage of the survey and asked to provide locating data, data on financial aid, and data on periods of enrollment for each eligible student, to be used in conducting a telephone survey of students. The NPSAS:93 study design calls for collecting the data elements for the institution survey by providing participating institutions with Computer Assisted Data Entry (CADE) software that can be used at the sampled institution to enter the data for each eligible student. A list of the names of sampled students, as well as data describing the institution, are preloaded into the CADE software databases. However, in order to minimize the burden and risk to participating insfixtions, the CADE software was designed for use by institution staff with very modest requirements for computer equipment, skills, and study-specific instruction. The CADE software designed and tested as part of the NPSAS:93 field test operated from floppy disk drives so as to rrlt inconvenience participating institutions by consuming storage space on the hard disk drive of the computer used to conduct the data entry. Acceptance of this task by the institution, and their ability to complete the task accurately, were key questions for the field test. The field test CADE instrument was designed to allow entry of data abstracted from the institutional data files on each student in five general areas: (1) locating and student characteristics; (2) enrollment data; (3) student financial aid data; (4) student need analysis and budget data; (5) financial aid application information. The locating and student characteristics section of the CADE software allowed entry of information on up to four addresses and telephone number for each sampled student (student's local, student's permanent, parent's address, and address of another person who would know the student's whereabouts) as well as demographic information about the student (marital status, ethnicity, citizenship, high school degree), admissions test scores (SAT, ACT, GRE, and so on), and grade point average. The enrollment section of the CADE software recorded the terms enrolled, including type of program, type of credit awarded for the term, student's educational level tuition and fees, major field of study, and attendance status. A third section of the CADE system recorded data on student financial aid requests, amounts of aid received by each student, and the type of financial aid award (Federal, State, institution, Veterans' Administration or Department of Defense, graduate or first professional financial aid, and other sources of financial aid, including employers, foundations). A foufth section recorded the iesults of student need analysis and budget information (tuition and fees, room and board, Pell Grant index, Expected Family Contribution, and so on). The fifth section of the CADE 8-5 was used to record data abstracted on financial aid application information from one of the common output documents used by most institutions (Student Aid Report, Financial Aid Form Need Analysis Report, Comprehensive Financial Aid Report, or similar reports). Initial materials mailed to the institutional coordinators described procedures whereby staff at each institution would use the CADE software for the record abstraction. However, if the institutional coordinator was unable, or unwilling, to participate in this self-administered approach, project staff were instructed to explore two alternative approaches. One alternative was to attempt to download the information required by NPSAS from existing data systems maintained by the institution. A second alternative was to send project field staff to the institution to perform the record abstraction using the CADE software on laptop computers. Obviously, for cost reasons, the self-administered CADE approach was the preferred method, avoiding both costly travel to the institution and potentially expensive programming effort necessary to convert data from the institution's system to the CADE format. Moreover, we reasoned that some institutions might prefer the self-administered approach because it provided better confidentiality protection for students not selected for the study."}, {"section_title": "Telephone Survey", "text": "The 70 participating institutions in the field test provided enrollment and baccalaureate lists for a total of 7.953 students. Table 8.2 presents breakdown of the student sample by type of institution and level of student. Approximately equal numbers of eligible students were obtained in non-B&B sample at the undergraduate, graduate, and professional levels. From the baccalaureate lists, 4,621 students were identified. In conducting the telephone interviews with students, the CASES CATI system presented interviewers with screens of questions, with the software guiding interviewer and respondent through the questionnaire, automatically skipping inapplicable questions based on response patterns or suggesting appropriate wording for probes if a respondent was uncertain how to answer a question. The system also contained help screens that can be used at the interviewees discretion to help clarify the intent of a question. The NPSAS CATI system was preloaded with information obtained from the CADE institution system so students and parents could be asked to verify data obtained from institutional records. Pre loading institutional information to facilitate student and parent interviewing is an important element in the NPSAS:93 data collection plan. How well this procedure worked mechanically and whether it helped to achieve the goal of minimizing student and parent respondent burden were important issues for the field test and for planning the full-scale qudy. "}, {"section_title": "Student and Parent Participation in the Study", "text": "Attaining the high completion rate required by NCES statistical standards for the NPSAS.93 full-scale (92% for the B&B cohort and 85% overall), will require concerted effo: lo locate both students and parents and persistent effort to convert potential nonrespondents. The goal of the field test effort to locate students was designed to permit evaluation of the quality of address information obtained from the participating institutions and assess the level of effort necessary for further tracing and locating efforts. An additional goal of the field test was to learn about the reasons for refusal and successful methods of averting final refusals. Letters were mailed to all field test sample members (students and selected parents), informing them about the NPSAS and of our intention to contact them for an interview. Sample subjects were also asked to verify the addresses supplied by the institutions . For ease and convenience in responding, postpaid return postcards were enclosed (that had a \"current address\" label affixed) so that the respondents could easily provide updated address information. The student update return postcards requested that the student provide tracing information about parents, as well as obtaining corrected address and telephone number information for the student. Return postcards for parents requested similar updated or confirmed information about the student's current address and telephone numbers. Updates or confirmations were entered into the tracing and locating module (TLM) of the CATI system."}, {"section_title": "8-7 )", "text": "The NPSAS CATI system was designed so that neither the student nor the parent interview had precedence. This permitted the maximum flexibility and cost efficiency in conducting both student and parent interviews. If a parent was contacted during the process of locating a student, interviewers were permitted to conduct the interview with the parent prior to conducting the interview with the student. Similarly, if a student were contacted first, the student interview could take place even though a parent interview had not been completed."}, {"section_title": "Item Order and Item Wording", "text": "Many of the items in the student and parent questionnaires have been asked in previous rounds of the NPSAS. Nonetheless, there have been numerous additions and modifications to questions. Moreover, the desire to obtain base-year data from the B&B cohort led to the development of a number of items that did not appear on the NPSAS:90 questhnnaire. The quality of all modified and new items have been assessed by examining frequency of valid responses and, where possible, comparing responses with external data sources (for example, amounts of aid reports compared with actual administrative ranges of aid amounts)."}, {"section_title": "GRE Component", "text": "A feature of NPSAS:93 that received special attention in the field test was the outcome assessment among the B&B cohort. It was propcsed to use scores from the Graduate Record Examination (GRE) administered by the Educaticnal Testing Service (ETS) as a measure of student's achievement. As contractor for NPSAS:93, AAI contacted ETS to obtain GRE scores as long students gave their permission. An important field test issue was whether students who have not planned to take the GRE could be persuaded to do so. In the field test, procedures for obtaining GRE scores for sample members who have already sat for the exam or who had registered for the GRE (in October 1992, December 1992, or February 1993 were evaluated as were procedures to induce students to take the GRE if they had not planned to take the exam. All of these students were asked to participate in the GRE component of NPSAS. Fees to ETS for the exams were paid directly by NPSAS so that the students were not burdened with the financial expense of taking the test or of ordering additional test score reports. Of students who have neither taken nor plan to take the GRE, about 2,000 were asked to take the exam as part of the NPSAS. Two reimbursement levels ($20 and $35) and the impact of providing this reimbursement in full prior to taking the test, versus split reimbursement payments (an initial $5 payment to students prior to the exam with the balance provided after taking the exam) were tested. CATI interviews included an item asking B&B cohort students their status with respect to the GRE. Students who had already taken or registered for the exam were asked to complete the score report form designating Abt Associates as a recipient. The CATI system randomly selected students among the balance of the B&B cohort 8-8 who have never sat for the GRE and are not currently registered for the exam. This approach ensured that the exact number of appropriate respondents would be selected for the assessment component and for each of the experimental treatment subgroups. Students who agreed to take the GRE were sent registration materials in a second mailing. Students who indicated they would not take the exam were mailed refusal conversion materials stressing the importance of the NPSAS and of the GRE component. To ensure addresses were correct for sending final payments, the initial mailing included a return postcard in case the respondent changed addresses (and/or telephone) between the time of the interview and the time for final installment payments (a likely event for recent college graduates). This also provided an unobtrusive approach to maintaining contact with sample members who accepted the option (which could facilitate subsequent tracing for B&B).\nSeveral major elements of the GRE assessment option were evaluated: (1) would respondents agree during the interview to register for, and take, the GRE; (2) would the verbal agreement rate change with different cash incentives (allowing cost-efficiency analyses for the full-scale study implementation); (3) would respondents return registration forms; (4) would students who register for the exam actually sit for the exam; and (5) would incentive conditions affect those return rates (again allowing cost-efficiency analyses). Two incentive levels ($20 and $35) were included in the experiment. In addition a two-step reimbursement payment was initiated for the benefit of cost savings, because the bulk of the payments are not made unless the teEt is taken, and some individuals could forget or later decide that the reimbursement is not worth the effort. Under the split payment arrangement, $5 was mailed to the GRE students following the telephone interview, whether they agreed to register for the exam or not, the balance of the incentive was to be mailed to the student following the exam. Under the full payment arrangement, students received the full payment following the examination. Overall, the results of the GRE component were quite disappointing (Table 8.9). Of the 1,256 cases selected to be invited to take the GRE, 65.1% agreed to take the exam; only 15.4% completed registration materials, and only 11.3% actually sat for the examination. The amount of payment and payment method appears to have little effect on the initial agreement to sit for the exam. The higher amount did produce a higher percentage of students who completed registration and who actually sat for the exam, but, overall, the percentage at best was less than 20 percent."}, {"section_title": "Evaluation of Survey Administration", "text": ""}, {"section_title": "Results of the Institution Survey", "text": "The field test provided an opportunity to evaluate procedures used to recruit institutions and enhance the accuracy and completeness of the information they provided. Specifically, the institutional component of the field test focused on the following topics: (1) collection of accurate enrollment and graduation lists; (2) methods of data collection; and (3) collection of accurate cumulative information for the B&B cohort. Initial contacts with the institutions were made by mail beginning February 14, 1992. List acquisition was completed September 4, 1992. Record abstraction began July 6, 1992 and was completed November 13, 1992. Initially, 80 institutions were selected to participate in the field test. These institutions were selected to fulfill quotas for the major NPSAS strata. The selection process was designcd to ensure that institutions that may have fallen in the sample frame for the full-study were not selected to participate in the field test, thus avoiding contamination of the final NPSAS:93 sample. Of the 80 institutions originally selected to participate in the field test, eight refused and were replaced by institutions with similar characteristics. Thus, a total of 88 institutions were invited to participate in the field test. At this initial stage, institutions were counted as participating if they agreed to provide an enrollment list. Table 8.3 shows the overall participation levels among institutions. Of the 88 invited o participate, 70 institutions, or 80 percent, actually provided enrollment lists. As expected, the lowest participation was among private, for-profit institutions (60 percent, Table 8.3). Private institutions participated at a higher rate (78 percent), while the highest levels of participation was observed among public institutions, where participation was 85 percent for the field test. The typical reason for refusal across all three types of institutions was that participation in the study was too burdensome. For those institutions receiving federal funding, the survey was seen as simply causing more paperwork in addition to the existing 8-9 *ti administrative burden of complying with federal reporting regulations. For other institutions (regardless of whether they received federal funds), the goals of the study were not seen as important enough to warrant the time and expense of participation. Confidentiality of student financial information was also a concern, particularly for institutions that did hot participate in federal programs. Even when study confidentiality procedures were explained, institution representatives expressed fears of adverse reactions, including legal action, from students if the institution provided Lnancial information to a federal agency when the institution did not receive federal funding. One institution would participate only on the condition that signed consent forms were obtained from all students at the institution, a condition that proved to be infeasible within the field test schedule. "}, {"section_title": "Enrollment and Graduation Lists", "text": "The ability of participating institutions to provide comprehensive and accurate enrollment and graduation lists in a timely was a critical element of the field test. Because these lists were used to construct the student sample frame, their accuracy was key to the validity of the study. Detailed instructions were prepared for the institutions requesting that 8-10 they provide lists of students enrolled as well as each student's institution identification number and education level. The request was for an unduplicated, machine-readable list of all students enrolled between July 1, 1991 and June 30, 1992 and a separate list of expected baccalaureate recipients, including major field of study (for sampling the B&B cohort), however, the instructions also stressed that NPSAS would be very flexible in working with whatever format and medium was convenient for the institution. As part of quality control on the list acquisition procedures, the number of students in each institutions enrollment file was compared with expected numbers of students calculated from the NCES Integrated Postsecondary Education Data System (IPEDS). Total number of students and, where applicable, subtotals of undergraduate, graduate, and first professional students, and subtotals of expected baccalaureate degree recipients were compared with comparable IPEDS data. In cases of significant discrepancies, counts based on the enrollment lists were verified with participating institutions before sampling and, if necessary, additional sampling information was provided. Because the initiation of subsequent phases of the NPSAS survey --record abstraction for sample students and the telephone interview of students and parents --depended on the construction of a sample frame for each institution, the schedule for the project depends on the timely response by institutions to reqt..ests for enrollment and graduations lists. Plans for the field test and for the full-scale study call for the institutions to provide comprehensive enrollment and graduation files within a few weeks so that the record abstract portion of the survey could be initiated and completed in a sufficient number of institutions to begin interviewing of students by early summer. Table 8.4 summarizes the types of enrollment lists that were received by type of institution, and shows that 60% of the participating institutions provided machine-readable lists. Smaller institutions with less differentiated student bodies (private, for-profit institutions, 2-3 year and less than 2-year institutions) almost exclusively provided the information in hard-copy format whiie larger institutions with more diverse (in terms of levels, baccalaureate degree recipients) were mixed in their preference for hard-copy or machine-readable lists. As can be seen from Table 8.5, quality of the enrollment lists was 11 problem for institutions that provided hard copy lists instead of machine-readable lists. Among the 28 institutions providing hard copy lists, eight provided lists with duplicate entries, three provided lists not in order of education level, and six lists failed quality control checks. For machine-readable lists, sorting files as well as identifying and eliminating duplications can be done through an automated process. However, the combination of high numbers of institutions providing hard copy lists that cannot be easily sorted or checked, combined with the high rate of duplication and error, suggests that increased efforts must be made to enlist the cooperation of institutions in providing machine-readable lists of students.  Figure 8.1 shows the list acquisition time, measured in months, from the date the institutional coordinator was assigned the task by the chief administrator. The histogram indicates the percent of lists received each month, while the horizontal line indicates the cumulative percent of lists received across time. Plans for the field test and for the full-scale study call for the institutions to provide comprehensive enrollment and graduation files within a few weeks. However, the cumulative percent line in Figure 8.1 shows that less than 5% of the field test institutions provided lists by the end of the first month. only a quarter of the institutions provided lists by the end of the second month and that half took longer that three months to complete the first phase of the study. Although nearly all institutions provided lists by the end of the fifth month, the length of time required in the field test to complete this task is very problematic for achieving the schedule objectives of the full-scale project. "}, {"section_title": "8-17", "text": ""}, {"section_title": "8-18", "text": "We e kly Total MICumulative BEST COPY AVAILABLE As is the case with the institution survey field test, the field test of the student and parent telephone survey was designed to serve a number of objectives. First, the field test provided an opportunity to assess features of the CATI system, in particular, the procedures for preloading institution data collected in the CADE software into the questionnaires administered through CATI. Over 125 data elements could be preloaded from CADE to CATI, including locating data (names, addresses, and telephone numbers of students, parents, and other possible informants), as well as information abstracted from student administrative records (dates of attendance, major field of study, financial aid application data, and financial aid awards). In addition to the preload procedures, the CATI system developed for NPSAS made extensive use of grid formats that allow multiple entry of data on each screen. Finally, software was developed for computer-assisted coding of institutions attended by the student (in addition to the institution selected for NPSAS), for the student's major field of study, and for the student's occupation and industry. Second, although most of the questions used in NPSAS:93 were tested in the NPSAS:90 field test and used in the NPSAS:90 full-scale survey, many, especially those administered to the B&B cohort for the base year, were newly developed for the 1993 cycle. In addition to issues related to the technical performance of the CATI system, a goal of the field test is the assessment of how well new questions were understood by respondents and whether they provided meaningful responses. The field test also allowed the project staff to assess the extent of student locating problems and evaluatc procedures for locating students based on the address information provided by institutions. Information about this issue is quite useful in planning for the fullscale effort and assuring that adequate procedures are in place to deal with potential locating problems. Finally, requests made to students to participate in the GRE component of the study were initiated in the telephone survey and student participation in the GRE component was tracked as piece of this survey. Result of the field test of the GRE component are critical in the decision to implement this component in the full-scale study. Telephone interviewing began September 12, 1992 and ended December 18, 1992."}, {"section_title": "Interviewing Students and Parents", "text": "As indicated in the previous section, the field period for list acquisition and record abstraction from the institutions exceeded the project schedule by several months. For this reason, the telephone interviewing could not be started and completed within original project schedule. Rather than further delay key planning tasks leading to the full-scale survey, it was decided that the field test field period should be curtailed, even though this decision meant not completing as many student and parent interviews as planned. Table 8.7 shows that 7,417 eligible student records were loaded into the CATI system for student interviewing. Because of project scheduling constraints, the field period was concluded before all of these cases could be worked. A total of 4,788 student interviews were completed. A subsample of 1,000 students was selected for use in projecting the level of effort necessary to achieve the contracted completion rates of 92% among the B&B cohort tw4 85% overall. Table 8.8 presents the results. Student subsample selected from the original institutional sample of 7,417 eligible studt nts. Parents were selected during the student interview. Out of the country includes students/parents with foreign addresses who could not be reached during the field penod. Response rate = (Completed cases + Partial cases) / Net sample Of the 1,000 sample students, 21 were found to be ineligible during the telephone interview, either because they were high school students or because they did not attend courses during the NPSAS year. This low rate (2%) represents errors or oversights during the record abstract process for excluding ineligible cases. Two of the students had died. Seventeen had apparently moved out of the country. Students were classified here if their last known address was a foreign country and if interviewers had verified that they were not living at any US address supplied by institutions. Interviews were completed with 740 students and partially completed (through section A) with another 3 students to yield a response rate of 77.4% overall. Among the B&B cohort, the net sample of 229 corresponds to a response rate of 76.0%. While this is lower than the targeted figures for the full-scale, projections of production during the field test indicate that, if the field period had been extended, the target response rate would have been achieved. The average completion time was 47.5 minutes per case. Because of the additional questions administered for the base year of the B&B study, interviews among the B&B cohort averaged about 10 minutes longer, or 57.46 minutes per case. These figures are consistent with the level of effort budgeted for the full-scale study. Parmt interviews were conducted with a net sample of 406 parents of students. Interviews were completed with 282 and partially completed with an additional five to yield a response rate of 70.7%. In general, the CATI system performed as expected, although a number of minor probiems with question-wording, skip logic, and question positioning were identified and corrected during the field test. The software developed for coding institutions, major field of study, and industry/occupation of student jobs during the interview worked well procedurally. Some errors found in the logic for preloading record abstract data into the CATI system were detected and documented for revisions in the full-scale CATI system."}, {"section_title": "EVALUATION OF DATA COLLECTED IN THE FIELD TEST", "text": ""}, {"section_title": "Record Abstract Data", "text": "Record abstract data were evaluated in three ways. First, following the institution survey, eleven of the 70 participating institutions were asked to verify a limited number of data elements that had been supplied for nine of their students. The purpose was simply to assess the reliability of the record abstract process. Second, data from the record abstract were compared with similar data collected in the CATI interview. Finally, NCES staff compared individual data on Pell grant awards obtained the record abstract with Department of Education records."}, {"section_title": "Verification of Record Abstract Data with Institutions", "text": "In order to conduct a small-scale validation test, eleven institutions were asked to provide detailed information on nine students, providing a total of 99 possible students. This was accomplished by sending these institutions a CADE validation form that asked them to validate the data for nine student records. Responses were returned by institutions on 96 of the 99 students. Table 8.10 displays the percentage of student records that were updated based op the verification. It should be noted that updates imply only that the date obtained in the in:Aial record abstraction were different from the data obtained in the verification process. The outcome does not necessarily mean that the original data were incorrect, although this is 3ne explanation. Alternatively, information originally recorded may have, in fact, changed in the record system. Table 8.10 indicates a high level of agreement between the initial reports and the validation reports for Pell Grants, Federal College Work-Study Program, and Stafford Loans. The percentage of updates ranges from 1 percent to 2.1 percent. Date of first enrollment was updated in 6.25 percent of the cases. The largest differences were found on reports of Need Analysis Tuition information where 21 of the 96 student records were updated. The same level of discrepancy between initial and validation records was found for the Expected Family Contribution data. The finding of less accurate reports for these two measures parallels difficulties in collecting accurate data of this type reported in the 1990 NPSAS. "}, {"section_title": "Comparison of Record Abstract Data with Student Reports", "text": "The results of the NP3AS:93 field test permitted an examination of the degree of correspondence between information about students obtained from the institutional records through the CADE process and information about the students obtained directly from the students in the telephone survey interviews. Because there are data elements common to both sources, it is possible to determine the extent and nature of discrepancies between the two data sources for the common data elements. The variables that can be examined include both financial aid items, and data on individual characteristics such as gender, marital status, and race. The results of this analysis are reported in the full Field Test Report. As expected, agreement was generally higher among demographic items and other individual items than among financial aid items."}, {"section_title": "Comparison of Record Abstract Data with Administrative Data", "text": "Because the student's Social Security number (SSN) was collected as part of this process, it was possible to match individual student records from the NPSAS:93 field test with data from the Department of Education's administrative records on the award of Pell grants. Table 8.11 shows the results. Of the 7,417 usable CADE records of eligible student (see Table 8.7), matches were made to the Department of Education (ED) records for 6,804 students (92%). Of the 1,206 NPSAS records that indicated the student received a Pell Grant, the award was verified with ED data in 1,143 (95%) cases; NPSAS records indicating no grant funds had been received (n = 5,598) were verified in 99% percent of the cases.  Two approaches were used to evaluate data from telephone interviews. In the first, telephone interview data from the NPSAS:90 cycles were evaluated for inter-item consistency. Because these items are very similar in NPSAS:90 ana NPSAS:93, results of this analysis were useful for planning the 1993 full-scale survey. The second approach was an evaluation of data collected in reinterviews with NPSAS:93 field test respondents."}, {"section_title": "Verification Reinterviews", "text": "As part of the evaluation conducted for the NPSAS:93 field test, a reliability experiment was implemented and a subset of the student sample was reinterviewed between one and three months after their initial interview was conducted. Although the reinterview questionnaire contained only a subset of the full field test questionnaire, the same question wordings were used in each of the two interviews. Reinterviews were conducted with 237 students. The full analysis of the results of the original and verification reinterview can be found in full field test report. The results of the first analysis show that, in general, the reliability of financial aid items is low, that is responses from the interview and re-interview did not agree for many students. While there is no clear indication of the source of this low reliability, it is possible that students may not actually be aware of certain pieces of information about their own financial aid status. By including supplemental questions in the full NPSAS, it may be possible to further delineate the source of this lack of correspoildence."}, {"section_title": "Evaluation of Income and Assets in NPSAS:90", "text": "In conducting the NPSAS:93 field test, the optimal study design would have included full validation of the data collected. However, neither the time nor the resourc -s available for the NPSAS:93 field test permitted such validation to be conducted. Because oi this consideration, it was important that knowledge gained from validation analysis conducted using the NPSAS:90 data be used to guide the formulation of data collection procedures and plans for the NPSAS:93."}, {"section_title": "8-25", "text": "Given the limited time available between the NPSAS:90 data collection and the initiation of plans for the NPSAS:93 data collection, it was only possible to conduct a preliminary assessment of the NPSAS:90 data to guide the design of the general characteristics of the NPSAS:93 field test. However, since that time, a more formal report has been prepared that evaluates response rates for several questions in the parent and student surveys, and investigates the consistency between student and parent responses. From this examination, inferences may be drawn about how useful it is to ask particular questions and to combine some questions, and to cony oine some questions, and whether some questions should only be asked of one respondent. Respondents seem to have difficulty recalling values over long periods of time. This may be due, in part, to some of the NPSAS questions seeming redundant to respondents who, as a result, refuse to answer similar questions later in the interview. Among students, there is general familiarity with parental income, but students are less likely to know the amount of their parental income. The use of categorical items as a follow-up to items asking for exact dollar amounts seemed to be successful in reducing the overall levels of item nonresponse. The categorical items obtained much information that may have otherwise been lost and, therefore, were valuable in the survey. Finally, the consistency of student responses about parental income was similar, if not improved, over that obtained in the NPSAS:87. The correlation found for student categorical responses about their parents' income in the 1987 NPSAS was .72, compared with between .73 and .79 for the 1990 NPSAS. The implication of these results is that the categorical probes are very useful in this kind of survey. Also, income and asset items can be very sensitive, and perhaps other ways to collect this kind of information should be investigated in order to a obtain more comprehensive picture of student and parent income and assets."}, {"section_title": "SUMMARY AND RECOMMENDATIONS", "text": "The NPSAS:93 field test provided a great deal of useful information for planning the full-scale survey. Throughout this report, each of the various components of NPSAS:93 field test have been discussed and the results of the evaluation presented. This section discusses the general results of the field test and discusses their implications. CADE. The CADE system developed for t.le by institution staff proved to be a viable approach to completing the record abstract portion of the institution survey. Although the self-administered approach to this task was less acceptable than had been hoped, a number of institutions that chose this method were able to complete the record abstract without requiring the time and expense of field data collector visit. In both the self-administered arid field interviewer options, the CADE software performed as required and was found to have several advantages over a paper-and-pencil method. The system contains checks to remind users of the status of work completed for the sample of students, thus providing sample management capability. The system is programmed with automatic checks on acceptable ranges for response and on inter-item consistency, providing a measure of quality control for data entry. While no direct comparisons with a hardcopy version was made in this field test, several of the institution staff who had participated in NPSAS:90 commented during debriefing that the automated system required less time than the paper-and-pencil version and was therefore less of a respondent burden. (Note that 1990 procedures called for field data collectors to abstract the institutions' administrative and financial aid records. The individuals who made these comments in the NPSAS:93 field test were from institutions where staff assisted the NPSAS:90 field data collectors either by completing portions of the record abstract or by abstracting entire records for portions of the student sample.) A major feature of the CADE approach is that data collected at institutions can be quickly loaded into the CATI system for use in the telephone interviews with students and parents. These features of the CADE system and its successful use in the field test are convincing evidence for its use in the fullscale survey. CATI. Similarly, the CATI system developed for the field test was successfully implemented. Student locating information and data abstracted from institution administrative records were preloaded into to the CATI system and were used as planned during the student and parent interviews. Interviews with both students and parents were completed within the budgeted levels of minutes per case. The addresses and telephone numbers obtained through the institution survey were found to be an effective source of locating information and, if not used directly in contacting respondents, were good \"leads\" for obtaining additional locating data. Timing. The length of time necessary for institutions to complete both the list acquisition and the record abstract tasks is problematic for the maintaining the schedule of the full-scale survey. One factor contributing to this problem is that data are available at the institutions on a varying schedule. With the variety of enrollment terms outside of the traditional quarter or semester systems, many institutions are unable to compile enrollment lists that are comprehensive of the pei-nd beginning July 1 and ending the following June 30 until very close to the end of this period. Similarly, for the record abstract task, some institutions have not recorded a student's complete financial aid history over this period until quite near the end of the period. This basic problem of the currentness of institutions' records is, of course, exacerbated by the perceived and real burden placed on institution staff by participating in NPSAS. Once the administrative records are complete, the project schedule requires that both the enrollments lists and record abstract data be provided in a very short time frame. Historical Financial Aid Data. The results of two aspects of the NPSAS:93 field test lead us to urge deleting them from the full-scale study. The first of these is the request to institutions for historical data on financial aid the B&B cohort students. Two factors inhibit institutions from providing this information. First, financial aid transcripts of students who have transferred into the sampled institution contain only meager data on types and amounts of financial aid. Second, even when these data are theoretically available at the sampled institution for the years of the student's attendance, the records were often stored at off-site locations that made.their access very difficult. The problems engendered by these two factors means that any historical financial aid data collected in this manner would be incomplete and poor quality. GRE component. The poor rate of participation in this component of the study strongly suggests that consideration should be given to other methods of obtaining this sort of information."}, {"section_title": "Changes Made to the Institution Survey and CADE", "text": "Advance materials for the chief administrator of the institution were revised to better describe the urgency of providing the enrollment and graduation files, to urge that this information be provided in a machine-readable form if at all possiblz., and to explain that enrollment data may be sent in as soon as the enrollment is available for the final term of the NPSAS year. Changes were made that strongly encouraged the administrator to pass the materials on to individuals who are knowledgeable about the institution's systems used to maintain both enrollment and financial aid data. Two copies of the advance materials were mailed to the chief administrator in order to facilitate this request and frequent telephone follow-up calls with these individuals have been planned for the full-scale survey. The CADE software was revised to delete sections requesting data on financial aid prior to the NPSAS year. Also, the enrollment section of the CADE was revised to simplify recording term-by-term information about enrollment status. Finally, numerous minor changes were made to question wording and explanatory material, following the recommendations of the NPSAS Technical Review Panel. In addition to revisions to CADE, a new module was added to the project's integrated control system to help the NPSAS staff manage the volume of CADE diskettes necessary in the full-scale study. The CADE Operations Modt:le (CADE-OPS) automates much of the tasks associated with managing the flow of diskettes and files of completed data from the field data collectors and from institutions. In addition, the CADE-OPS contains a program for editing the CADE data prior to loading the student records into the NPSAS CATI system. Staffing plans for the full-scale survey were modified to enhance the availability of field staff as field data collectors. Training materials for central office staff responsible for initial and follow-up contacts with institution staff were modified to encourage more discussion with institutional coordinators on their use of CADE. The purpose of the more extensive discussion is two-fold. First, it is designed to help NPSAS staff identify any problems with the software so that they may be dealt with efficiently. Second, we hope to quickly identify those institutions that eventually required switching to a field data collector in order to assure the availability of field staff. 205 8-28"}, {"section_title": "Changes Made to the Student and Parent Survey and CATI", "text": "Major revisions made to the CATI instrument as a result of the NPSAS:93 field test included deleting the items dealing with specific types of aid awarded prior to the NPSAS year and the section of the CATI that dealt with the GRE component. In addition, although the mechanisms for preloading CADE data into the CATI system worked to a limited extent in the field test, several technical problems were identified during the field test and required additional developmental effort. In addition to these revisions, the TRP made numerous recommendations which were implemented in the revised CATI instrument. "}, {"section_title": "Overall Design", "text": "Overall, the design of the NPSAS is a sound approach to collecting information concerning the wide array of options available to students and their families for financing postsecondary education. There is no single source of information on grants and loans at the federal, state, or institution level and, even if such a source existed, it could capture other types of strategies that families use for postsecondary education. A statistically reliable and methodologically sound national survey is the only option for collecting this valuable information and making it available to policy and educational researchers. Nonetheless, NPSAS:93 is the third time thL study has been fielded and, methodologically, each round represents a new opportunity to improve the basic design. The introduction of computer assisted data entry (CADE) software to the process of abstracting student record data maintained at the institutions is perhaps the most significant methodological aspect of NPSAS:93. Our experience demonstrates that this is not only a feasible approach to abstracting these data; the data collected at the institution can be quickly loaded into the student computer assisted telephone interviewing system to facilitate the administration of the telephone survey of students and parents."}, {"section_title": "9.2", "text": ""}, {"section_title": "Institution Enlistment", "text": "Institution enlistment was the major difficulty in completion of the 1993 NPSAS. This difficulty led to a chronic delay in the project schedule because institutional records collection and student and parent telephone interviews were dependent on completion of the enrollment listing and sampling. This process should begin as soon as possible in the project schedule and consider streamlining the quality control and editing of the individual files received by 9-1 fc? , the institutions. Further, redesigning CADE and other innovative strategies may help to maintain or perhaps increase institution participation in the study."}, {"section_title": "Records Data Collection and Updating", "text": "Use of the CADE software by institutional staff as well as by contractor field staff proved quite feasible in NPSAS:93. However, as indicated in our evaluation in Chapter 4, more complete data were obtained by field staff than by institutional staff. This was not an unexpected outcome. Field data collectors working on an assignment are more conscientious than volunteer staff who have competing demands for attention. The tradeoff presented by this situation is that while some information can be obtained accurately and at relatively low cost, the amount of data requested in the NPSAS:93 CADE may have been overwhelming for institution staff. A recommendation is to carefully consider the number of data elements requested in record abstract portion of NPSAS with a goal of deleting a number of data elements to improve participation by the institutions. The essential information for the institutional records collection task is the financial aid award information, periods of enrollment, and the locating information."}, {"section_title": "9.5", "text": ""}, {"section_title": "Student and Parent Survey", "text": "Student and parent interviews are an essential complement of the record abstract data collected in NPSAS. The NPSAS:93 CATI system had a number of features that should be preserved in the future. In particular, loading information from the student information collected at the institutions proved feasible and resulted in minimizing respondent burden during the telephone interviews. Similarly, interviewing parents and students in either order allowed data from the first interview to be loaded into the second. Presenting data from the first interview for verific, in the second, or skipping questions in the second interview if the information was collected in the first, appears to have worked well and, again, further reduced the response burden. Nonetheless, portions of the NPSAS interview can be tedious. Detailed income and asset questions are difficult for respondents to answer and NPSAS:93 asked for income for two years prior to the survey. Following analyses comparing the results of questions asked about different years, collecting only one year's income data should be considered."}, {"section_title": "9.6", "text": "File Creation and Analysis NPSAS collects a wealth of information and, in the Data Analysis System (DAS) and Electronic Code Book (ECB), NCES has prepared tools for accessing these data. As a way to simplify these systems, especially the production of the electronic codebook files, NCES may want to consider combining the files of undergraduates and graduates into one file. While for some purposes, it is important to separate these types of students, the DAS software allows separate tables to be developed."}, {"section_title": "9-2 2ri 8 APPENDIX A NPSAS:93 Data Elements", "text": "Most variables listed below as derived variables (beginning about page A-11) are contained in the Data Analysis System available on the Internet at gopher.ed.gov. Other variables shown below include those collected at institutions or telephone interviews. Readers interested in variables not listed as a derived variable, or readers interested in obtaining access to the data files that will permit deriving or creating your own composite variables should contact the  A_PAACSR  A_PAAFDC  A_PAASIF  A_PABFDB  A_PABFVL  A_PACASH  A_PADIS  A_PADISP  A_PAEJST  A_PAEOTI  A_PAEUI  A_PAEXEM  A_PAEXTX  A_PAFEEI  A_PAFINC  A_PAGROS  AsPAHMDB  A_PAHMVL  A_PAMAR  A_PAMDEX  A_PAMEEI  A_PAMINC  A_PANCOL  A_PANFAM  A_PAOAGE  A_PAOINC  A_PAORDB  A_PAORVL  A_PASTAT  A_PASTLG  A_PATAX  A_PATPCH  A_PGI  A_ST41  A_ST42  A_ST91TX  A_ST92EI  A_ST920I  A_ST92TX  A_ST92UI  A_STADC  A_STAIF  A_STASR  A_STB69  A_STBFD  A_STBFV  A_STCIT  A_STCOL  A_STCSH  A_STDEAP  A_STDISW  A_STDSP  A_STE90  A_STE91 Flag of accuracy of preloaded enrollment terms Student loan default/owe grant refund Family contribution (P) annual child support received (P) annual AFDC/ADC Parent's assets include a farm (P) business/farm debt (P) business/farm value (P) cash, savings and checking Either parent a dislocated worker Either parent a displaced homemaker (P) elementary/jr high/sr. high tuition paid (P) expected 1992 other taxable income (P) expected 1992 untaxed income (P) exemptions claimed (P) expected 1992 tax paid Father's expected 1992 earned income Father's income earned from work (P) adjusted gross income from IRS form (P) home debt (P) home value Parent's marital status (P) medical/dental expenses Mother's expected 1992 earned income Mother's income earned from work Number of dependents in college -1992-93 (P) number of family members Age of older parent (P) other untaxed income (P) other real estate/investment debt (P) other real estate/investment value (P) 1991 tax return status (P) state of legal residence (P) U.S. income tax paid (P) tuition paid for how many children Pell grant index (S) resources of $4000 or more -A (S) resources of $4000 or more -B Student 1991 tax return status Student's expected 1992 earned income (S) expected 1992 other taxable income Student's expected 1992 tax paid (S) expected 1992 untaxed income (S) annual AFDC/ADC Student assets include a farm (S) annual child support received IS) born before 1/1/69 (S) business/farm debt (S) business/farm value (S) citizenship status (S) number in college (S) cash, savings, and checking (S) monthly DEAP benefits Student/spouse a dislocated worker (S) dependents other than spouse (S) parents claim as a exemption in 1990 (S) parents claim as a exemption in 1991 A_STE92  A_STEJS  A_STEXM  A_STFAM  A_STFBD  A_STFSA  A_STORS  A_STHMDB  A_STHMVL  A_STLSTA  A_STMAR  A_STMDE  A_STMODP  A_STMOVP  A_STOUT  A_STOVD  A_STOVI  A_STOW   A_STSD)(   A_STSPEI  A_STSPI  A_STSSB  A_STSTI  A_STTAX  A_STTCH  A_STUMRS  A_STUSTF  A_STVEAP  A_STVUS  A_STYRC  227  B28  530  BAB  B_AAPA  B_AAST  B_BACHLR  B_BORN69  B_CITEN  B_CNPA  B_CNST  B_COLYR  B_DEAPA  B_DEAPM  B_E90  B_E91  B_E92  B_EARN1  B_EARN2  B_FEDAID  B_IAPA  B_IAST  B_MARST  B_NIB1  B_NIB2  B_OLDAGE  B_OTHLGL A-1 (S) parents claim as a exemption in 1992 (S) elementary/junior high/senior high tuition (SI exemptions claimed (S) number of family members (S) first Bachelor's degree by 7/1/92 (S) first year federal aid received Student adjusted gross income from IRS form (S) home debt (S) home value Student's state of legal residence (S) martial status (S) medical/dental expenses (S) number of months DEAP benefits received (S) number of months VEAP benefits received (S) other untaxed income (S) other real estate/investment debt (S) other real estate/investment value (S) orphan or ward of the court Student/spouse displaced homemaker (S) spouse's expected 1992 earned income (S) spouse's income earned from work (S) annual Social Security benefits Student income earned from work Student U.S. income taxes paid (S) tuition paid for how many children (S) unpaid balance on most recent Stafford loan Unpaid balance on Stafford loans (S) monthly VEAP benefits (S) veteran of U.S. armed forces Year in college in 92-93 Other admission test scores available Cumulative grade point average (gpa) Grade point average (gpa) scale Baccalaureate and beyond From asset analysis-parents' contribution From asset analysis-student's contribution B.A. or B.S. received by July 1, 1992 Student born before 1-1-69 (S) U.S. citizen Contribution for student-parent contribution Contribution for student-student contribution Year in college in 92-93 (S) DEAP amount expected per month (5) number of months DEAP expected Was student a tax exemption for parents in 1990 Was student a tax exemption for parents in 1991 Was student a tax exemption for parents in 1992 Student earnings-summer 1992 Student earnings-school year 1992-93 When did student begin receiving federal aid From income analysis-parents' contribution From income analysis-student's contribution Student's marital status (S) nontaxable income & benefits-summer 1992 (S) nontaxable income & benifits-1992-93 Age older parent (S) legal dependents other than spouse"}, {"section_title": "BEST COPY AVAILABo", "text": ""}, {"section_title": "COG_1H3", "text": "Student's contributions from assets-primary year COG_2SUM Separate budget using CM for summer 1992 COG_3A Tuition and fees -summer 1992 term COG_3B Books and supplies -summer 1992 term C00_3C Room and board -summer 1992 term COG_3D Transportation -summer 1992 term COG_3E Miscellaneous and personal expenses-summer 1992 COG_3F Dependent care -summer 1992 COG_3G Handicapped (S) dependent other than spouse age 0-5 1992-93 C_DEP13 (S) depend other than spouse age 13 and older C_DEP612 (S) dependent other than spouse age 6-12,1992-93 C_FEDAID (S) First received aid CJIMPRPR (S) home purchase price C_LNDFLT (S) loan default C_LSTATE (S) legal state C_MARST (S) marital status C_OLDAGE Age of older parent C_OTHLGL (S) legal dependants C_PADC (P) recieve AFDC or ADC C_PAGI (P) adjusted gross income C_PARINC Parents in college C_PARMAR (P) marital status C_PCASH (PI cash, checking and saving account C_PCLM90 Did parents claim student in 1990 C_PCLM91 Did parents claim student in 1991 C_PCLM92 Did parents claim student in 1992 C_PDEBT (P) real estate/investment debt C_PDISHM (P) dislocated homemaker C_PDISWE (P) dislocated worker C_PEXMP (P) tax exemptions C_PFAMSZ (P) number of family members C_PFARMD (P) business and farm debt C_PFARMV (P) business and farm value C_PFWK1 Father earnings -1991 C_PFWK2 Father earnings -1992 C_PGI Pell grant index (PGI) C_PHLD (P) child support C_PHOMED (P) home debt C_PHOMEV (P) home value C_PINFM (P) includes farm C_PMED (P) medical/dental expenses c_mmu Mother earnings -1991"}, {"section_title": "C_PMWK2", "text": "Mother earnings -1992 C_PNOCH (P) for how many children C_PNOCOL (P) total number in college C_PNOTAX (P) 1992 nontaxable income C_POTHR (P) other untaxed income C_POTI (P) other taxable income C_PSS (P) Social Security benefits C_PSTRES (P) legal state C_PTAX (P) tax return filed C_PTUIT (P) elementary/secondary tuition C_PTXPD1 AP) 1991 U.S. income tax paid C_PTXPD2 (P) 1992 U.S. income tax paid C_PVALUE (P) real estate/investments value C_REFUND Default/owe refund C_RES85B (S) resources of $4000 in 1985 -B C_RES86B (S) resources of $4000 in 1986 -B C_RES87B (S) resources of $4000 in 1987 -B C_RES88B (S) resources of $4000 in 1988 -B  C_SINFM  C_SMED  C_SMWE2  C_SNOCH   c_slipcoL   c_sNoTAx  C_SOTHR  C_SOT/  C_SPWK1  C_SSS  C_STAGI  C_STAX  C_STAXP1  C_STEXMP  C_STUIT  C_STWK1  C_STXPD2  C_SVALUE  C_TLUNBL  C_VEAP  C_VEAPM  C_VETERN  C_WARD  C_YRHMPR  D3A  D3B  D3C  D3D  D3E  D3F  D3FED  D3G  D3H  D3I  D3J  D3K  D3L   D3m  D3N  D3ND1  D3POST  D3TYP1  D4A  D4B  D4C  D4D  D4E  DeNEED1  D4NEED2  D4TYP1  D4TYP2  D5A   D513   D5C  D5D  D5E  D5F  D5G  D5H  D5NEED1  D5NEED2  D5TYP1  D5TYP2  D6A  D6B  D6C  D6D  D6E  D6F  D6G  D6H  D6I  D6J  D6NEED1  D6NEED2  D6TYP1  D6TYP2  D7A  D7B  D7C  D7D  D7E  D7NEED1  D7NEED2  D7TYP1  D7TYP2 Type of other aid, second DEP_2SUM (S) dependency status during the summer 2992 CADE DATA ELEMENTS (S) includes farm (S) medical/dental expenses (V spouse earnings (S) for how many children (S) number in college (S) nontaxable income (S) other untaxed income (S) other taxable income (S) spouse earnings (S) Social Security benefits (S) adjusted gross income (S) tax return filed (S) 1991 U.S. income tax paid (S) 1991 tax exemptions (S) elementary/secondary tuition (S) 1991 earnings (S) 1992 U.S. income tix paid (S) real estate/investments value (S) total unpaid balance (S) VEAP amount (S) VEAP months (S) veteran (S) orphan/ward (S) year home purchased Other monthly payment D_P12CON 12-month contribution to student D_P9MCON 9-month contribution to student D_PAAI Adjusted available income D_PADJNT Adjusted business/farm net worth D_PAGI (P) adjusted gross taxable income D_PAINC (P) available/discretionary income D_PAPA (P) asset protection allowance D_PCA (P) contribution from assets D ?CAA/ (P) contribution from adjusted available income D_PCASH (P) cash and bank accounts D_PCONTR (P) contribution from income D_PCP (P) conversion percentage D_PDNE (P) discretionary net worth D_PEMPAL (P) employment allowance D_PERKIN Perkins Loan D_PERPY Perkins Loan monthly payment D_PETUT (P) elementary and secondary school tuition paid D_PPICA (P) FICA tax D_PHOME (P) home equity D_PINCSP (P) income supplement D_PINCTX (P) U.S. total income D_PLPY SLS monthly payment D_FLUS SIX (Federal Supplemental Loans for Students) D_PMDEXP (P) medical/dental expenses D_PNETW (P) net worth D_POTHR (P) other real estate and investments equity D_POTHTX (P) state and other taxes D_PSTND (P) standard maintenance allowance D_PTLAL1-:: (P) total allowances D_PTL/NC (P) total income D_PVIB (P) untaxed income and benefits D REFUND (S) refund owed D_SAGI (S) adjusted gross/taxable income D_SAINC (S) available/discretionary income D_SCON (S) contribution from income D_SEMPAL (S) employment allowance D_SETUT (S) elementary and secondary school tuition paid D_SFICA (S) FICA tax D SINCTX (S) U.S. income tax D_SMDEXP (S) medical/dental expenses D_SOTHTX (S) state and other taxes D_SPOUSE (S) spouse's loans D_SPPY (S) spouse's monthly payment D_SSTND (S) standard maintenance allowed D_SSUMLV (S) summer living allowance D_ST12CN 12-month contribution to student D_ST9CON 9-month contribution to student D STAAI (S) adjusted available income D_STADJN (S) adjusted business/farm net worth D_STAPA (S) asset protection allowance D_STCA (S) contribution from assets D_STCAAI (S) contribution from adjusted available income D_STCASH (S) cash and bank accounts D_STCP (S) conversion percentage D STDNW (S) discretionary net worth D_STFFSZ (S) family size D_STFGSL Stafford or GS). D_STOISPY Stafford monthly payment D_STHOME (S) home equity D_STINCS (S) income supplement D_STLALW (S) total allowances D_STLINC (S) total income D_STNCOL (S) number in college D_STNETW (S)  Ending year for enrollment term 02 EDO3M Ending month for enrollment term #3 E0031 Ending year for enrollment term 03 EDO4M Ending month for enrollment term #4 ED04Y Ending year for enrollment term 414 EDO5M Ending month for enrollment term 05 EDO5Y Ending year for enrollment term #5 EDO6M Ending month for enrollment term #6 5006? Ending year for enrollment term 46 1007M Ending month for enrollment term #7 E0071 Ending year for enrollment term #7 E008M Ending month for enrollment term 08"}, {"section_title": "EDO8Y", "text": "Ending year for enrollment term #8"}, {"section_title": "E00914", "text": "Ending month for enrollment term #9 E009Y Ending year for enrollment term 09 ED1OM Ending month for enrollment term #10 E010Y Ending year for enrollment term #10 ED11M Ending month for enrollment term 011 E011Y Ending year for enrollment term #11 ED12M Ending month for enrollment term 012 E012Y Ending year for enrollment term 112 Other loan #4-other schls other than Fed,St,Inst C48b Other loan #2-other schls other than Foderal,State,Instit Other loan amount #3-other schools Other loan amount #4-other schools"}, {"section_title": "CC47", "text": "Other loan name #1-other schools CC48 Other loan amount #2-other schools CC69 Other schls-name of the fellowship funded by other CC90 Name of the other source of aid-other schools CO2s What other reasons for not accepting aid-verbatim"}, {"section_title": "0134", "text": "Sponsor of prepayment plan-other specify verbatim D25a Other types of assistance by parents-verbatim 5004 Important activities and duties at the S's job E1OT Occupation verbatim text ElIT Industry verbatim text for student EJ15 Other thing student did to find job-verbatim  FOlI  F050  F053  F055  F056  F059  F061  F062  F063  F064  F067  F069  F070  F071  F072  F073  F074  F077 To find a job-sent out resumes To find a job-went to campus job placement To find a job-looked through want ads To find a job-asked friends To find a job-asked family To find a job-asked professors To find a job-attended recruiting fairs To find a job-did volunteer work in field To find job-looked at unemployment office To find job-used employment agcy/prof recruiters To find a job-placed a want ad To find a job-subscribed to trade journals To find a job-did nothing To find a job-other Attempted to change/obtain job since graduating Satisfied with the ability of instructors Satisfied with classroom buildings, library, equip Satisfied with intellectual life of the school Satisfied with the course curriculum Satisfied with social life of the school Satisfied with his/her intellectual growth Satisfied with educ, considering overall cost Satisfied with reputation of school Satisfied with security measures taken (B&B only) Program type expected or enrolled in 1993-94 Year f first contacted grad school for admission Month first applied to grad/professional school Number of graduate/professional schools applied to Admission acceptance at first choice grad school Attending graduate/professional school 01 Month start to attend grad/professional school 01 Applied for aid grad/professional schl 01 Awarded/offered aid at grad /prof school 41 Admission acceptance at 2nd choice grad school Attended graduate/professional school t2 Month start to attend grad/professional schl 02 Applied for aid at grad/professional school #2 Awarded/offered financial aid at grad/prof schl 02 Number of grad/prof schnols accepted at Plan to attend other g 1 or professional school Month will start/started at grad/professional schl Find future Job/sent out resumes"}, {"section_title": "F86C", "text": "Find job/went to campus job placement offices Find job/looked through want ads F86D"}, {"section_title": "F86E", "text": "Find job/networked w/ family, friends, others Find job/looked through interviews F86F Find job/attended recruiting fairs F860 Find job/did volunteer/internship work in field F86H Find job/job announcements-unemployment office"}, {"section_title": "F86I", "text": "Find job/employment agency, prof. recruiters F86J F86K Find job/placed a want ad Find job/subscribed to trade journals Decide to work-support family/pay fin obligation F96C Decide to work-didn't receive financial aid F96D Decide to work-personal reasons other than money F96E Decide to work-failed to meet application deadline F96F Decide to work factor-not admitd to schl of choice F96G Decide to work factor-want break from school F96H Decide to work-good job opp. / military commitment"}, {"section_title": "F96I", "text": "Factor for work-career plans indefinite"}, {"section_title": "F96J", "text": "Decide to work-need work expernce before grad schl F96K Decide to work factor-some other reason F97A Factor for future work-previous experience in area"}, {"section_title": "F97B", "text": "Factor for future work-good income to start"}, {"section_title": "F97C", "text": "Factor for future work-good income potential F970 Factor for future work-job security F97E Factor for future work-prestige and status F97F Factor for future work-interesting work F97G Factor for future work-intellectually challenging F97H Factor for future work-freedom to make decisions"}, {"section_title": "F97I", "text": "Factor for future work-interaction with people F97J Factor for future work-work independent of others F97K Factor for future work-allows great deal of travel F97L Factor for future work-allows establishment roots F97M Factor for future work-time for non-work activity"}, {"section_title": "FI57", "text": "First choice grad/first-prof school-IPEDS code FI65 Second choice grad/first-prof school-IPEDS code"}, {"section_title": "FI75", "text": "Other choice grad/first-prof school-IPEDS code FX86 Is respondent looking for work 0034 Hours of comm. service/volunteer work past 2 years G97A Important or not-becoming authority in field G97B Important or not-influencing political structure G97C Important or not-being very well-off financially 0970 Important or not-owning own business G97E Important or not-being successful in line of work G97F Important or not-being able to find steady work 0970 Important or not-being a leader in the community Other school (11-month/year of first enrollment (up to 5 schools) ."}, {"section_title": "PEM1", "text": "Other school Ill-month/year of last enrollment (up to 5 schools)"}, {"section_title": "U88A", "text": "Fields certified/eligible to teach PAYMMT zwrzavxsws"}, {"section_title": "ICO2", "text": "Industry code-spouse ICDE Industry code-parent respondent \u00a3001 Marital status of parent respondent"}, {"section_title": "\u00a3004", "text": "Amount P contributed to students school expenses \u00a3005 Other relatives, friends, family contrib."}, {"section_title": "\u00a3006", "text": "Amt contributed by other relatives, friends \u00a3007 Amount loaned by parents to S for school expenses"}, {"section_title": "\u00a3009", "text": "Provide S with addtnl help, other than money L010 Amt of addtl support provided, other than money   INLNAMT  INNEEDOR  INNONDGR  INOTHAMT  INSTAMT  INSTCWS  INSTNEED  INSTNOND  INSTPCT  LOANPCT  NONFMCST  NREFCON  NREFLOAN  OFFCOST  OTHERAID  OTHERAMT  OTHERTAX  OTHFDAMT  OTHGTAMT  OTHLNAMT  OTHRCOST  OTHRMCST  OTHSCAMT  OWEAMT  PARCONTR  PAREDUC  PARLOAN  PERKAMT  PLUSAMT  POSTED  PRICE1  PRICE2  PRICE3  REFCONTR  REFINC91  REFINC92  REFLOAN  REFPAR  RESAMT  RNEED1  RNEED2  RNEED3  RNEED4  RNEED5  RNEED6  ROOMCOST   SAI   SCHOLAMT  SEXDINC  SINGLPAR  SLSAMT  SPSINC  STAFFAMT  STAFPACK  STAPCT  STATEAMT  STATNEED  STATNOND  STGTAMT  STLNAMT  STOTHAMT  STSAVPLN  T4AMT1  T4AMT2  T4PK1AMT  TCOSTPR  TCOSTPR2 A-13  TEACHAMT  TFEDAID  TFEDGRT  TFEDLN  TFEDOTHR  TITIVAMT  TNFEDAID  TNFEDGRT  TNFEDLN  TNFEDOTH  TOTAID  TOTCOST  TOTGRT  TOTLOAN  TOTOTHR  TOTWKST  TUITCOST  UNTAXINC  WAIVAMT  WKINC  WKINCCAL  WORKPCT  AIDAPP  DEPEND2  CMPC  CMSC  MAXLOAN  TOTLOAN2  CMNEEDA-J  MERITAID  UNUSEDLN  STBUDGET  AIDAPP  DEPEND2  CMPC  CMSC  MAXLOAN  FEDTAXES  NETPRC1  NETPRC10  NETPRC11  NETPRC12  NETPRC2  NETPRC3  NETPRC4  NETPRC5  NETPRC6  NETPRC7  NETPRC8  NETPRC9  NONTUIT  NUMDEPND  NUMFEDLN   RMEIDCOST   SLS_STAF  TFESTGRT  TFESTLN  TOTFEDST  MOREHRS Worked more hours at job(s) for educ expenses MOREJOBS Take extra job to help with educ expenses NOAPPO1 Didn't apply for aid (P)-family/stu could Pay r-) NOAPPO2 Didn't apply (P)-family/student not want debt NOAPPO3 Didn't apply for aid (P)-family income too high NOAPPO4 Didn't apply for aid (P)-low student grades NOAPPO5 Didn't apply for aid (P)-too difficult to apply NOAPPO6 Didn't apply (Pi-not want to disclose finances NOAPPO7 Didn't apply (P)-student was part-time status NOAPPO8 Didn't apply for aid (P)-no money was available NOAPPO9 Didn't apply (P)-missed deadline for application NOAPPIO Didn't apply (P)-didn't know about financial aid NOAPP11 Didn't apply for aid (P)-other reason OTHRLOAN Take "}, {"section_title": "A-14", "text": "Extent parents will help repay student's loans Take out a PLUS loan Reason did not apply for aid (P)-first response Didn't apply for aid (S)-second response Didn't apply for aid (S)-third response Take out second mortgage or refinanc real estate Use funds previously set aside for retirement Take out a loan against a retirement fund Take out a school-sponsored parent loan Extent student repays parents loans for educ Obtained a signature loan Take out a Family Educ Loan from Sallie Mae Obtained a state-sponsored parent loan Loan from.non-profit underwriter, incl TERI Total elapsed time to complete parent interview SAMPLE NPSAS INSTITUTION PARTICIPATION SHEET PLEASE RETURN THIS SHEET BY FEBRUARY 10, 1993 IN THE ENCLOSED BUSINESS REPLY ENVELOPE OR FAX IT TO PAT WILLIAMS AT (800) 382-4560. Based on an estimated enrollment of XXXX students during 1992-93, we expect to sample XX students from your institution. Please review the information below and make any necessary changes. Please enter the names of each of the terms/enrollment periods that BEGIN between May 1, 1992 through April 30, 1993 inclusive. NOTE: DO NOT INCLUDE A TERM IF IT STARTED BEFORE MAY 1, 1992 OR STARTS AFTER MAY 1, 1993."}, {"section_title": "Term Name", "text": "Beginning Date Ending Date / / / / O We do not have traditional periods of enrollment (i.e. semester, quarter) that apply to all students."}, {"section_title": "NATIONAL POSTSECONDARY STUDENT AID STUDY INFORMATION SHEET", "text": "The National Postsecondary Student Aid Study (NPSAS) will be conducted during the 1992-93 school year. Data will be collected from approximately 1,400 postsecondary institutions, 78,000 students, and 27,000 parents. Listed below is a summary of activities we would Wm to complete at your institution. Information will be gathered in two phases and a choice of methods for collecting data will be provided. As the NCES contractor, we will work with your appointed coordinator to:"}, {"section_title": "PHASE ONE", "text": "Obtain enrollment files/Lists for students enrolled during any term/enrollment period that BEGINS any time between May 1, 1992 and April 30, 1993--to select a sample of students for the study. At 4-year institutions, these lists/data files should indicate whetner the students are graduating seniors, graduate students, or fust-professional students. For graduating seniors, also collect the major field of study."}, {"section_title": "PHASE TWO", "text": "Collect the locating information for sampled students and parents so that telephone interviews can be conducted. Collect detailed demographic and academic information for sampled students. Collect financial aid information for sampled students receiving aid while enrolled. Thm activities may differ somewhat according to your type of institution and method of recordkeeping. To minimize the burden on your staff, computer software has been developed to facilitate your participation in Phase Two of the study. We believe that this software is self-explanatory; however, field staff will be available for assistance. For institutional coordinators, the time for completing the data collection activities using the computer-assisted data entry software is estimated to vary from 3 to 55 hours per institution, with an average of 19.5 hours over the duration of the study. This includes time for reviewing instructions, locating existing data sources, and gathering the information needed for completing data collection activities. Send comments regarding this burden estimate or any other aspect of this collection of information, including suggestions for reducing this burden, to the U. We will work with your coordinator to accommodate your preferences as much as possible while maintaining consivent and efficient aata collection procedures."}, {"section_title": "SAMPLE REPORT", "text": "National Postsecondary Student Aid Study: 1989- "}, {"section_title": "APPENDIX D Variables With Imputations for Missing Values", "text": "The imputations performed on seven variables that contained missing values are described in the following paragraphs. A comparison of the pre-and post-imputation values for these variables is shown below."}, {"section_title": "Expected Family Contribution (EFC)", "text": "Expected Family Contribution for undergraduates There are four derived variables with alues for the expected family contribution (EFC) in NPSAS:93: EFC1 is the federal Family Contribution value as recorded from institutional records in CADE or from federal Pell Grant and Student Loan files. A recorded value was available for 49% of the sample. Because the EFC frequently changes over the course of the year (data changes resulting from verification, use of professional judgement by financial aid officers, changes in student circumstances, etc.) these values were not always consistent (CADE and the Pell file values agreed in 80% of overlapping cases; CADE and Loan file values agreed in 53%). If more than one was available, the order of priority was: CADE, Pell file, ED loan file. EFC2 is an estimated value calculated using the federal 1992-93 Congressional Methodology (CM) formulas with data for the components taken from any available source (CADE, CATI, Pell files). Values were only calculated if a dollar value (rather than an estimated range) was available for income and a sufficient number of component data elements were available for credible results (58%). The recorded EFC1 and the calculated EFC2 agreed within $500 for 75% of the cases where both values were available. EFC3 is an imputed value based on regression equations. EFC4 is the composite EFC value which represents the best estimate according to the following order of priority: First, the recorded EFC1 was used if available and if the value was consistent with the student budget and the amount of need-based aid received. If not, the formula calculation EFC2 was used. If EFC2 was not available or not consistent with the amount of need-based aid received, then the regression-based EFC3 was used. If EFC3 was too high to be consistent with the amount of need-based aid, it was adjusted downward so that the need after aid was equal to zero (in 1.1% of the cases). "}, {"section_title": "Sources", "text": "\nIncome data could be obtained from a variety of sources. For dependent and independent students who applied for financial aid, income could be obtained from financial aid forms (e.g., SAR, GAPSFAS, CFAR, etc.), from official Department of Education data bases, including the Pell recipient file and the federal student loan file (\"tape dump\"). In addition to these institutional sources, the NPSAS:93 student and parent CATI instruments contained questions about individual and family income. These latter sources, based on results from NPSAS:90, asked for income data in two ways. First, respondents were asked to provide an openended response. For those respondents who could not or would not answer the open-ended income questions, a second approach was used. Close-ended follow-up questions, which allowed respondents to choose from among a set of income categories, were asked (e.g., \"Would you estimate your (parent's) total yearly income in 1991/92 1) $30,000 or more, or 2) less than $30,000?\" Depending on which answer was selected, respondents were asked a follow-up series that tried to specify more precisely the range within which total family income fell (e.g., \"at least 30,000 but less than $50,000,\" etc.). Table D-1 shows the source for the income variable for dependent undergraduate and graduate/first professional students, whileTable D-2 shows the same information for independent students.  Even with these multiple sources, several difficulties emerged with the 1993 data. First, there were differences in the way income was reported in the CADE and CATI instruments. The CADE (institutional) data came from the financial aid applications, and reported adjusted gross income and various categories of untaxed income separately. The CATI questions asked respondents to provide \"total yearly income\" because other studies showed that respondents were unable to provide reliable responses to a detailed breakdown of types of income received almost two years earlier. In order to provide comparable information, \"total yearly income\" was created for respondents who had CADE data by adding the AGI and untaxed income. Second, the income ranges for those respondents who provided only a categorical estimate of their own or their family income, were too large to provide a meaningful number that could be used for computing an estimated expected family contribution. This necessitated estimating a specific value within the selected income interval. In past NPSAS studies, the midpoint of the range was used. This approach leads to a certain \"lumpiness\" in the distribution, since all cases falling within a particular interval are assigned the midpoint. For NPSAS:93, 2 4 respondents who chose one of the categorical responses for income were randomly assigned a value within the range they selected.' Third, even after searching among all the possible sources of income information, a large percentage of cases (about 18 percent of undergraduates and 28 percent of graduate/first professional students) were still missing income. For these respondents, total income was inputed using multiple regression. Regression equations were estimated separately by student level (undergraduate versus graduate/first professional students) and dependency status (dependent/independent). The samples used to estimate income were limited to those whose total income was $150,000 or less. The variables included in the regression estimation equations and the adjusted 122 were: Independent undergraduates (le = .53)Total financial aid received; average total income for independent students attending the same institution; age; age squared; dummy variable for part-time, part-year attendance; a dummy variable for being married; Pell grant amount; dummy variables for institutional control (private, not-for-profit, and private, for-profit) and dummy variables for the interaction of age with part-time, full-year attendance at one institution; and the interaction of age with part-time, part-year attendance. Dependent undergraduates parental income (le = .37)Total financial aid received; Pell grant amount; average total income for dependent students attending the same institution; dummy variables for attendance status (full-time, part-year; part-time, full-year at one institution; and part-time, part-year); dummy variables for living with parents, or with relatives, while attending school; dummy variables for institutional level (two-to -three-year, and less-thantwo-year); institutional control (private, not-for-profit, and private, for-profit); and dummy variables for region of the country based on OBE region (far west, and \"outlying\"). Independent graduate and first-professional students (Fe = .49)Total financial aid received; average total income for independent students attending the same institution; age; age squared; gender; dummy variables for marital status (married, and separated); Stafford loan amount; full-time, part-year attendance; dummy variable for attendance at a private, not-forprofit institution; and a dummy variable for a refined dependency status (independent with no dependents). Dependent undergraduate and first-professional students (Fe = .29) Total financial aid received; average total income for dependent students attending the same institution; Stafford loan amount; and a dummy variable for graduate or first-professional status. I Initial plans were to assign categorical responses to unique values for a continuous income variable randomly based on the empirical distribution of responses to the open-ended income questions that fell within the bounds of the categorical response. However, about 70 percent of those providing open-ended values gave numbers that fell on $5,000 boundaries (e.g.. $40,000, $45,000, etc.). Consequently, categorical responses were assigned to the $5,000 amounts within a categorical range. D 1 4 t; The regression estimates substantially increased the proportion of valid responses. The number of missing cases decreased from 13,313 (20.1 percent of the entire NPSAS sample) to 2,250 (3.4 percent). Tables D-3 and D-4 show how the distribution of total income changed as a consequence of the imputations. For both dependent and independent students, the effect of imputing missing incomes was to shift the distribution to the upper income ranges. This is expected, since low income students were more likely than higher income students to apply for aid, and so were more likely to have an income reported in institutional records (CADE) or in Education Department files. Higher income students' incomes were more likely to come from the Student or Parent CATI, which had a a higher percentage of missing values than either the CADE or Education Department data.  2.5% 3.2% 3.3% 9.1% 9.7% 8.3% 9.0% 9.2% 12.0% 5.9% 8.6% 7.1% 10.5% 5.4% 6.0% 100.0% 100.0% 1 Racelethnieity of the student.\nThe variable describing student's race has been derived from a number of sources. Race and ethnicity (Hispanic or non-Hispanic) were included in the CADE record abstract software and field data collectors attempted to gather this information from administrative records maintained by the institutions. Data recorded in CADE were loaded into the CATI instrument for verification during the telephone interview with students. If information on race or ethnicity was not collected during the institution survey, students were asked for this information during the telephone interview. Among the undergraduate and graduate student survey data records that qualified for the final analysis files, about 25 percent were missing information on race and ethnicity (Table 1), mostly because of missing data (23%). Missing data could occur because data on race or ethnicity were not available from the institution and the question was not asked of the respondent during the telephone interview, either because a break-off occurring before these items were asked or because an interview was not conducted at all. The frequency of data missing because of refusals or \"don't know\" responses was quite low (0.6% and 0.1% respectively)."}, {"section_title": "Imputation of EFC3 by regression", "text": "The sample for the regression estinntes was limited to cases which met the following conditions: (1) The source of the reported EFC1 was the FAFNAR. This was the only form in CADE which reported the Parental and Student Contributions separately for dependent applicants. (2) The EFC2 value calculated using the formula was within $500 of the reported EFC1. This was to eliminate cases where there were major differences due to professional judgement or other data inconsistencies. Eight separate sets of equations were run, depending on the number of basic data elements available for the EFC calculation (income, assets, family size) and the dependency status of the student. For dependent students the Parental (PC) and the Student Contributions (SC) were estimated separately."}, {"section_title": "D-2", "text": "Each of the eight sets actually consisted of two equations: (1) A logistic regression to predict whether the value should be set to zero or the minimum values assigned by the methodology. ($1200 for single independents; $700 or $900 for the dependent student contribution). Logistic regression was used to minimize regression bias stemming from truncated dependent variables. (2) A least squares regression to predict those values greater than zero or above the minimum. The table below shows the percentage of cases in which the logistic regression correctly predicted a minimum value and the R squared from the least squares regression which predicts the values greater than zero: income, income sq."}, {"section_title": "95%", "text": ".86 The equations were tested on a sample of cases which met the same conditions as above, but where the source ci the recorded EFC was the federal SAR. The EFC for dependent students was calculated 170, dividing the predicted PC by the number of family members in postsecondary education and adding the predicted SC (set to the minimum of $700 for first year students and $900 for others). For single independent students predicted minimum values were set to $1200. The overall correlation of the reported EFC with the predicted EFC was .81. There was an absolute difference of $200 or less in 25% of the cases, $500 or less in 40%, and $1000 or less in 50% of the cases. "}, {"section_title": "EFC for Graduate students", "text": "Expected Family Contributions for graduate and first professional students were derived following the same procedures outlined above for undergraduates. Separate sets of regressions were run, with similar results. Graduate students were less likely to have financial aid application records and only 10% filed as dependent students."}, {"section_title": "EFC4", "text": "Source: All of the direct educational expenses were summed in the variable EDCOST, the direct cost of education other than tuition and fees. The average values for undergraduate respondents were calculated by institutional type and attendance intensity (ATTNSTAT) and used to impute values for non-respondents. All of the monthly living expense components were summed and averages calculated by dependency status and local residence for each institution; these averages were used to impute the monthly expenses for undergraduates matching the same dependency/residence characteristics at the institution. The minimum value was set at $100 per month. The average monthly living expenses were multiplied by the number of months that the student was enrolled during the NPSAS year (ENLEN) to get an estimated total amount for the period of enrollment. This total plus any amount paid for school-owned housing was included in LIVEXPUN, the unadjusted household expenses while enrolled. The total unadjusted student-reported non-tuition expenses (SRNONTUN) are the sum of the direct educational expenses and the total living expenses (SRNONTUN=EDCOST+LIVEXPUN)."}, {"section_title": "D-6", "text": "The unadjusted amount LIVEXPUN assumes that the entire household expenses (including the expenses of a spouse and children) of independent students can be attributed to educational costs while the student is enrolled, even though the student may only be taking one or two courses. Among independent students with dependents, the unadjusted living expenses are directly related to income and inversly related to attendance intensity; that is, the higher the income, the higher the living expenses, and the less likely a student is attending full-time. Therefore an attendance-adjusted household expense LIVEXPAJ was estimated by including only 75% of the monthly amount during months that the student was enrolled at least half-time but less than full-time (MHT) and only 25% during months that the student was enrolled less than half-time (MPT). For married students only 50% of the household costs were included. The aitendance-adjusted non-tuition costs (SRNONTAJ) are therefore the direct educational expenses plus a part of the monthly household expenses in proportion to the attendance intensity (SRNONTAJ= EDCOST+LIVEXPAJ). Total student-reported costs were calculated as the sum of the tuition and fees charged (TUITION) plus the unadjusted or adjusted non-tuition costs described above. The unadjusted student-reported cost is TOTCOSTU (=TUITION+SRNONTUN), while the attendance-adjusted student-reported cost is TOTCOSTA(=TUITION+SRNONTAJ). If these total cost values were less than the amount of financial aid received by the student, the non-tuition component was adjusted upwards so that the total cost values were equal to the total aid. That is, it was assumed that student-reported estimates of cost were not as reliable as aid amounts reported by institutions, and that financial aid awards would not be greater than reasonable estimates of actual educational costs represented by the student budgets. CATI respondents' non-tuition estimates were adjusted upwards for 3.7% of the undergraduates and 7% of the graduate students. Of those who received aid, about half (52%) had a recorded student budget, while only 4% of those who received no aid had a budget. Student budgets were imputed for 40% of the aided undergraduates and 90% of the unaided. The 5% of students who attended more than one institution during the year or whose status changed from undergraduate to graduate during the year were excluded, since they would have had two budgets. The proportion of imputed budgets data was highest at the less than 4-year public institutions (80%) and for students with part-time part-year attendance (81%). The imputation strategy was to calculate the average full-time full-year tuition and non-tuition budget components for categories of students at each institution and then to assign these average values to individual cases with the same characteristics. The tuition component (TUITBGFT) was taken either from the amount in the reported budgets of full-time students or the amount of tuition actually charged (TUITION) full-time students, as reported in CADE or (rarely) CATI. Average full-time tuition amounts were calculated for each institution and assigned to all students in the institution with missing budget data. If the actual tuition paid was greater than the reported or imputed budget tuition, the budget tuition amount was raised to the actual tuition amount. Similarly, all standard non-tuition items (SBNONTUN) reported in the budgets (books and supplies, room and board, transportation and personal) were summed and averages calculated for all combinations of dependency (dependent, single independent, independent with dependents) and local residence (on campus, off campus, with parents), both for individual institutions and aggregated for types of institutions. Cases with missing non-tuition values were assigned the average value for the matching dependency/local residence characteristics at the institution attended. If this was not available, cases were assigned the average value by dependency/local residence for all institutions of that type. The total full-time student budget (BUDGETFT) was imputed as the sum of the full-time tuition and non-tuition values. If the imputed budget value was less than the amount of aid received, it was raised to equal the aid (TOTAID) by increasing the non-tuition component (SBNONTUN). In 1.6% of the cases the budget total reported in CADE was also adjusted upwards to equal the aid amount. Attendance-adjusted student budgets (BUDGETAJ) were estimated asfollows. The tuition component used the actual tuition charged (TUITION), which reflects differences in attendance patterns. The full-time non-tuition component (SBNONTUN) of the budget was adjusted to reflect less than half-time and less than full-year (9 months) attendance. For each case SBNONTUN was multiplied by the percentage of months enrolled half-time or more (HFT=months full-time plus months greater than half-time divided by total months enrolled) and the percentage of the academic year enrolled (PYADJUST=1 for those enrolled 9 months or more, otherwise =months enrolled/9). Then BUDGETAJ=TUITION-F(SBNONTUN*HFT*PYADJUST). For students attending only less than half-time, the adjusted budget is equal to tuition only (HFT=0); for those enrolled 9 months or more full-time, the adjusted budget includes the full-time non-tuition amount; those with mixed attendance patterns have some fraction of the non-tuition amount included."}, {"section_title": "D-11 2 ti", "text": "For graduate and first-professional students only budgets reported in CADE are included, and no imputations of full-time budgets were done. The attendance-adjusted student budgets were determined following the same procedure as for the undergraduates. Total income in calendar year 1991. Income is a critical variable for financial aid analyses. Income determines, in large part, expected family contribution, and so obtaining accurate and complete estimates of income for both dependent and independent students is of critical importance in NPSAS. This report describes the sources of income information in NPSAS:93, the completeness of this information, and the imputation strategy used to estimate income for respondents who either could not or would not answer the income questions."}, {"section_title": "Imputation", "text": "Because of the importance of race and ethnicity as analytic variables, data missing for any of these reasons was imputed. Typical imputation methods such as regression or hotdeck were considered, however, these methods require data from other variables in the imputation models. For the most part, data on race and ethnicity were missing because of an incomplete student interview so that data for other variables were missing as well. For this reason, these methods were not practical. Imputation followed a three-step process that resulted in the Post-Imputation frequency distribution in Table 1 First, the verbatim fields for the \"Other, specify\" categories of the two items were scanned and recoded, if possible. In many of these verbatim responses, the student indicated mixed ancestry (e.g., \"Black Hispanic\" or \"Hispanic-Indian\"). In these instances, the race variable and the ethnicity variable were updated accordingly. Race/ethnicity for 80 records was determined by this method. Second, if the student attended one of the historically Black colleges and universities (HBCUs), missing race data was recorded to \"Black.\" Records for 400 students were recoded in this way. The frequency of known student race among these colleges (Table 2) shows that 1,141 undergraduate and graduate students attended HBCUs and that 79% of these students were Black. Third, race/ethnicity was imputed using Census track information linked to each student record using the student's home address. In the imputation procedure, the student was assigned a race/ethnicity corresponding to the race of the majority (more than 50%) of the Census track of the student's home address. Race/ethnicity of 13,279 students was imputed using this rule. To compare actual to predicted race/ethnicity using this procedure, a predicted value for race/ethnicity was created for those students for whom race/ethnicity was known from either the record abstract or telephone survey data and who had a valid zip code. A comparison of actual and imputed race/ethnicity shows that overall [across graduate students and undergraduates combined], for about 79% of the imputed cases, the reported race was the same. Among the imputed race values, obtained agreement rates between imputed and actual were about 81% for Whites, 57% for Blacks, 39% for American Indian/Alaskan Natives, 64% for Asians, and 99% for Hispanics. RACE Race (Derived) by RACEZIP Race (Zip imputed) Filter: Only students with a reported race variable that was used to assign the derived RACE variable were used in this analysis and comparing against the imputed Race using Zipcode information . Race was imputed to a specific value only when 50% or more of the people living in that neighborhood were of the that race. 1.0 100.0 RACE Race (Cati/Cade Derived) by RACEZIP Race (Zipcode imputed) Filter: Only students with a reported race variable that was used to assign the derived RACE variable were used in this analysis and comparing against the imputed Race using Zipcode information (File: S93). "}, {"section_title": "RACEZIP", "text": ""}, {"section_title": "252", "text": "BEST COPY AVAILABLE Local residence (housing). Local residence was initially computed from the CATI variables B016 and B019. The verbatim responses for other local residence, B16A, were then used to map \"other\" responses for local residence into the appropriate categories. CADE data on local residence, Q26A, were then used to determine local residence for students for whom that data were missing in CATI. Next, the CADE locating data (student local and permanent addresses and parents address) were used to determine the local address for some students whose local address was still missing. Finally, institution sector and student age were used to create imputation classes for weighted sequential hot deck imputation for the remaining students with missing data for local residence. Pell grant amount. Pell grants are awarded to undergraduates who haven't yet received a Bachelor's or first professional degree. They are intended as a financial base, to which other financial aid awards can be added. To be eligible in 1992-93, students must have attended school at least half time. The amount of a Pell grant depends on need, cost of institution, attendance status (i.e. full time or part time, full year or part year). In Award Year 1993-93 the maximum amount was $2400. The NPSAS:93 estimate of the award amount for each student was based on, in order of priority: 1) CADE (institutional data), for which the institution supplied the social security number and NCES the ED Pell grant amount for that social security number; and 2) on CATI (student-reported data). If the institution provided a valid social security number and the student did not provide a different social security number in the CATI (student-reported data), then the ED Pell amount was used. If no award was reported for such a student, PELLAMT was set to zero. Then, the student-reported award amount was used if: 1) the social security number provided by the student appeared usable; or 2) the ED Pell amount was 0, but the student was enrolled in May or June of 1992, and the student-reported award amount was greater than 0. Finally, if the survey provided neither a valid social security number nor Pell award status, the award status was imputed. If the survey indicated that a Pell award was received but did not indicate the amount, or if the student was imputed to be a Pell recipient, then the amount of the award was imputed. Imputation classes were based on year in college, geographic region, and institution level and control."}, {"section_title": "Pre Imputation", "text": "Pell grant amount-prior to imputation Final estimate of the Stafford loan amount. If the institution provided a valid social security number and the student did not provide a different social security number in the CATI, then the ED reported award amount was used. If no award was reported for such a student, STAFFAMT was set to zero. Otherwise, the survey-reported award amount was used. Finally, if the survey provided neither a valid social security number nor Stafford award status, the award status was imputed. If the survey indicated that a Stafford award was received but did not indicated the amount, or if the student was imputed to be a Stafford recipient, then the mount of the award was imputed. Imputation classes were based on year in college, geographic region, and institution level and control.      "}]