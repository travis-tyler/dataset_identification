[{"section_title": "Abstract", "text": "The analysis of brain imaging data has attracted much attention recently. A popular analysis is to discover a network representation of brain from the neuroimaging data, where each node denotes a brain region and each edge represents a functional association or structural connection between two brain regions. Motivated by the multi-subject and multi-collection settings in neuroimaging studies, in this paper, we consider brain network discovery under two novel settings: 1) unified setting: Given a collection of subjects, discover a single network that is good for all subjects. 2) contrasting setting: Given two collections of subjects, discover a single network that best discriminates two collections. We show that the existing formulation of graphical Lasso (GLasso) cannot address above problems properly. Two novel models, UGLasso (Unified Graphical Lasso) and CGLasso(Contrasting Graphical Lasso), are proposed to address these two problems respectively. We evaluate our methods on synthetic data and two realworld functional magnetic resonance imaging (fMRI) datasets. Empirical results demonstrate the effectiveness of the proposed methods."}, {"section_title": "Introduction", "text": "Recent years have witnessed an increasing amount of data in the form of graph representations, which involve complex structures, e.g., brain networks and social networks. For instance, a brain network is composed of brain regions as the nodes and functional/structural connectivities between the brain regions as the links. The network representation of human brain as shown in right hand side of Fig. 1(a) is useful in many ways. For example, one can learn subgraph patterns in the brain networks to build classification models for disease diagnosis [11] . However, in many real-world studies, the connectivities between different brain regions are not available and should be derived from the neuroimaging data, e.g., fMRI data. In this paper, we study the prob- Figure 1: An illustration of two tasks of brain network discovery. lem of brain network discovery, which aims at inferring the functional connectivities among a set of predefined non-overlapping brain regions. Previous studies usually focus on inferring a network for a single subject or treating a collection of subjects as a single subject by concatenating the data of multiple subjects [10, 14] . As the increasing availability of neuroimaging data in recent years, we usually have one or more collections of subjects in brain datasets. The problem of discovering a network across collection of subjects is interesting and important. In this paper, we explore two novel settings on brain network discovery. The first one is to find a representative brain network on a single collection, where the discovered network is good for all subjects in the collection, while it is similar to each subject's best network. We call this setting unified network discovery, which is illustrated in Fig. 1(a) . The second setting we study is to discover a discriminative network between two collections of subjects, where the inferred network retains the differential connectivities between two collections. We call the second setting contrasting network discovery, which is illustrated in Fig. 1(b) . Such scenario is very common in neuroimaging analysis, where subjects can be grouped using different attributes, such as genders, ages, neurological diseases etc. Problems Studied: Fig. 2 presents the dicovered individual brain network for five healthy subjects in ADNI dataset using standard GLasso. Even all five subjects come from the healthy collection, one can observe that the networks are widely different from each other. This example illustrates the difficulty of discovering a representative network for a collection of subjects. Hence, inferring a network without considering the differences among subjects may lead to unsatisfactory results. Unified network discovery aims at finding a single representative network that is good for all subjects in a collection, which helps the neurology professionals to derive the common connectivity patterns for certain group of individuals. In this paper, we propose a novel algorithm called UGLasso (Unified Graphical Lasso) to address the unified network discovery problem.\nIn the contrasting setting where we have two collections of subjects, an usual question one may ask is what are the differences between the two collections. A common attempt to answer the question is to infer a network for each collection respectively and to compare them. However, this approach can be hindered in certain circumstances. For instance, in some neuroimaging datasets, the number of nodes in the network can be as large as 40 thousands. Thus, inferring two separated large-scale networks can be expensive. Besides, due to the underlying unreliability and the existence of noises in the signals, it would be difficult and time consuming for one to extract the differences between two inferred networks. Hence, solving the proposed contrasting network discovery problem is a much more efficient way to obatin the discriminative connectivity patterns between two collections. In this paper, we propose another novel algorithm called CGLasso (Contrasting Graphical Lasso) to address the contrasting network discovery problem.\nThe contributions of this paper are as follows.\n\u2022 We formulate the novel problems of unified network discovery and contrasting network discovery.\n\u2022 We show how to use a modified gradient projected to solve the two proposed problems while preserve the solution to be positive-definite.\n\u2022 We demonstrate the effectiveness of our proposed methods on synthetic datasets with ground truth and on two real-world neuroimaging datasets.\n2 Problem Formulation 2.1 Preliminary Assume we are given n observations X \u2208 R n\u00d7m from a m-variate normal distribution N (0, \u03a3), where n denotes the number of samples, m denotes the number of variables and \u03a3 denotes the covariance matrix of the distribution. The problem of estimating the inverse of covariance matrix \u0398 = \u03a3 \u22121 from X is known as the inverse covariance estimation [2, 7] . If the (i, j)-th entry of \u0398 ij is zero, then variables i and j are conditionally independent, given the other variables. The inverse covariance estimation can be cast as the problem of minimizing 1 -regularized negative log likelihood as\nwhere S = 1 n X T X is the empirical covariance matrix, ||\u0398|| 1 is the 1 -norm regularization that encourages sparse solutions, and \u03bb is a positive parameter denotes the strength of regularization. In the case where S 0, the maximum likelihood estimate (MLE) of \u03a3 \u22121 can be recovered by setting \u03bb = 0. However, in many high dimensional datasets, the number of samples n can be smaller than the number of variables m, and S can be singular. In such cases, additional regularizations, such as 1 -norm, are usually used to estimate \u0398. It is obvious that solving Eq. (2.1) leads to the 1 -regularized maximum likelihood estimation (MLE) of \u03a3 \u22121 . Stacking Approach to Multi-subject Study: In brain imaging studies, researchers usually collect data from multiple subjects. The data of the i-th subject can be represented by X i \u2208 R ni\u00d7m , where n i is the number of samples of subject i. Conventional approaches [10, 14] on multi-subject studies usually stack the data matrices of different subjects into\nEq. (2.1) to obtain a unified network \u0398 of all subjects. However, this approach does not consider the differences between subjects."}, {"section_title": "Unified Graphical Lasso", "text": "In the unified setting, we are given a collection of data matrices {X 1 , . . . , X p } with the same sets of m variables, where X i \u2208 R ni\u00d7m . And thus, we can compute a corresponding collection of empirical covariance matrices {S 1 , . . . , S p }, where\nThe goal is to derive a single estimated inverse covariance matrix\u0398 that obeys the following two criteria: i) Has high likelihood for all subjects.\nii) The single estimated inverse covariance matrix is similar to each subject's individual estimated inverse covariance matrix. The first criterion requires the solution to be quantitatively good on every subject. The second criterion put enforcement on the solution to be quantitatively similar to the estimated inverse covariance matrix of each subject in the collection.\nThe negative log likelihood of a estimated inverse covariance matrix \u0398 for subject i is defined as \u2212log det \u0398 + tr(S i \u0398). And the overall likelihood can be expressed by the average the likelihood of \u0398 for each subject:\nFurther we define the similarity between two inverse covariance matrices \u0398 i and \u0398 j as the square of the Frobenius norm of \u0398 i \u2212 \u0398 j . So the criterion 2) can be written as the minimization of\nis the estimated individual inverse covariance matrix for subject i.\nMaximizing criterion 1) is equivalent to minimizing Eq. (2.2). With the objective 1) and 2), we adopt a standard approach of combining them into a objective function with a weighting parameter \u03b1 and an 1 -norm regularization as follows, which is solved by UGLasso:"}, {"section_title": "Contrasting Graphical Lasso", "text": "In the contrasting setting, we are given two collections of data matrices {X\nq } with the same set of variables. We can compute the corresponding empirical covariance matrices {S\nq } accordingly. The goal is to derive a single discriminative inverse covariance matrix between two collections in that the likelihood is small for the first collection but large for the second. The estimated contrasting inverse covariance matrix is the one whose likelihoods on the each subject best distinguishes the two collections. Our objective function is defined as follows, which is solved by CGLasso:\nj . The log determinant terms for two collections are canceled under the contrasting setting. Although Eq. (2.4) resembles linear programming problem, the existence of 1 -norm regularization and positive-definite constraint prohibits the standard approach and makes it challenging to solve."}, {"section_title": "Algorithms", "text": "In this section we present the UGLasso and CGLasso algorithms in detail. Both objectives proposed in Sec. 2 can be transformed to the following form:\nwhere g(\u0398) is a differentiable smooth function and \u03bb||\u0398|| 1 is the non-differentiable regularization function. Specifically, we have\nF for unified setting and g(\u0398) = tr(\u015cX) for contrasting setting.\nFollowing the classic strategy in linear programming for addressing the 1 -norm minimization problems, we transform the elements of the norm into positive and negative parts and reform Eq. (3.5) as follows:\nis the column-vector of all ones which has the same length as vec(\u0398), so vec(\u0398) \nwhereg(\u0398) is the corresponding equivalent dual function of g(\u0398). In contrasting setting where g(\u0398) = tr(\u015c\u0398), the corresponding dual can be represented as\n, where * S = (\u015c, \u2212\u015c). Similar construction can be performed on the unified setting too.\nWe note that Eq. (3.7) is a smooth optimization problem with non-negativity and positive-definite constraints. If we treat the positive-definite constraint Algorithm 1 Algorithm for Solving Eq. (3.7) Require: S, \u03bb, iter max 1:\nInitialize s using Eq. (3.12)"}, {"section_title": "6:", "text": "Find the largest s satisfies Eq. (3.11) and positive definite constraint by performing the nonmonotonic Armijo backtracking line search."}, {"section_title": "7:", "text": "Compute the new projection \u0398 t+1 \u2190 P C (\u0398 t \u2212 sg t ) using Eq. (3.10)."}, {"section_title": "8:", "text": "Compute the new objective function\nCompute the new gradient g t+1 \u2190 f (\u0398 t+1 ) 10: until iter = iter max or convergence 11: Return \u0398 t+1 as inactive, the remaining constraint is a convex nonnegative cone, then one can apply projected gradient method to solve Eq. (3.7). In this work, we consider a variant of the projected gradient method that updates the solution in each iteration as\nwhere s is the step size to be selected by backtracking line search strategy and P C is a function defined by\nis the Euclidean projection of \u0398 onto convex set C. Here, we have C = {y : y > 0} is a non-negative cone. With this convex set, the solution to Eq. (3.9) is trivial, we simply project every dimension of \u0398 to the nonnegative part as\n4: Solve Eq. (3.5) to get\u0398 using Algorithm 1."}, {"section_title": "5: Return\u0398", "text": "For the selection of step size s in Eq. (3.9), we employ non-monotonic Armijo backtracking line search"}, {"section_title": "Algorithm 3 CGLasso", "text": "Require: S\n(3.11) where \u03b7 \u2208 (0, 1) is the sufficient decrease parameter (usually small) and j is the reference memory parameter typically set as 10. Armijo backtracking line search does not always decrease the objective function, but it can ensure the global convergence of the projected gradient method as well as enhance the convergence rate [6] . We also use Barzilai-Borwein initialization proposed in [3] to setup the step size in the k-th iteration:\nwhere\n). Now we recall the positive-definite constraint. We first note that it is obvious the projection in Eq. (3.9) with C = {y : y > 0} does not affect the positivedefiniteness of a matrix, i.e., if \u0398 is a positive-definite matrix, then P C (\u0398) is also positive-definite. Thus, to guarantee that our algorithm always find a positivedefinite solution, we need prove the following theorem. Proof. Since \u0398 0 \u21d0\u21d2 z \u0398z > 0, \u2200z = 0, with the spectral theorem, we have z \u0398 \u2265 \u03c3 min (\u0398)||z|| "}, {"section_title": "Accordingly, given\u0398", "text": "(i) in iteration i that is positive-definite, the line search process can always find a step size s > 0 that make\u0398\n) to be positive-definite as well. The modified projected gradient algorithm for solving Eq. 3.7 is summarized in Algorithm 1. The UGLasso and CGLasso algorithms are summarized in Algorithm 2 and Algorithm 3 respectively."}, {"section_title": "Simulated Study on Synthetic Data", "text": "Due to the lack of ground truth in real neuroimaging datasets, synthetic data is considered to be an important tool to evaluate the effectiveness of brain network discovery methods. We first evaluate our methods on synthetic data where ground truth (network structure) is available."}, {"section_title": "Evaluation on UGLasso \u2022 Dataset:", "text": "The first set of synthetic data is generated to comparing the effectiveness of the proposed unified graphical Lasso method with GLasso. We adopt the approach in [16] with some modifications to generate the synthetic precision matrices. To simulate subjects in single collection, we generate p separate sparse precision matrices of size m \u00d7 m with similar structure. Specifically, the first step is to randomly generate a basal positive definite matrix \u0398 b of size m \u00d7 m, where we control the density of \u0398 b to be \u03c1 b \u2208 (0, 1). Then we generate p different positive definite noise matrices {N 1 , . . . , N p }, each of size m \u00d7 m and density \u03c1 n . At last we add each noise matrix to the basal matrix respectively to get the collection of ground truth matrices G = {G 1 , . . . , G p }, where G i = \u0398 b +N i . By doing so, we retain the positive definiteness of each ground truth matrices as well as control the similarity among them. With the collection of ground truth matrices, we can draw p separate sample matrices of size n\u00d7m from the Gaussian distribution for each subject to simulate the fMRI signals, where n denotes the number of samples (or the number of time steps in fMRI). Without losing generality, we simply use the same n for all subjects in the collection. To numerically evaluate the compared methods, we prepare three synthetic datasets with following parameters:\n\u2022 Dataset 1 (Weak Noises) : m = 50, p = 50, \u03c1 b = 0.01, \u03c1 n = 0.005, n = 60, 80, . . . , 200.\n\u2022 Dataset 2 (Moderate Noises): m = 50, p = 100, \u03c1 b = 0.01, \u03c1 n = 0.01, n = 60, 80, . . . , 200.\n\u2022 Dataset 3 (Strong Noises): m = 50, p = 100, \u03c1 b = 0.01, \u03c1 n = 0.05, n = 60, 80, . . . , 200.\n\u2022 Experimental Protocols: We follow the approach as described above to obtain the collection of ground truths G.\nFor each choice of sample size n, we randomly draw a collection of sample matrices\n} from Gaussian distribution based on G. Then the empirical covariance matrix S for the collection can be computed using the stack approach described in Section 2. GLasso uses S to estimate the precision matrix for the collection, and UGLasso uses both S and X (m\u00d7n) . To be fair, we set the parameters for both methods to make the estimated matrices to have similar number of nonzero entities. We repeat this process 5 times for each choice of n.\nComparison between Uglasso and GLasso on three synthetic datasets in terms of F1 score on connectivity inference.\n\u2022 Evaluation Metrics: We follow [16] to define the F1 score of connectivity inference as\n, where n d is the number of true edges detected by the algorithm, n g is the number of true edges and n a is the total number of edges detected. Larger F1 score is better. In our experiments we have a collection of ground truth matrices G and a single unified basal ground truth matrix \u0398 b . To evaluate the performance of finding representative networks, we report two F1 scores in each experiment. The first one is obtained by evaluating the inferred network against \u0398 b , where the noises are excluded. This evaluation aims at assessing the ability of recovering the real representative structure from noisy signals. The second one is obtained by evaluating the inferred network against each network in G, where noises are not excluded in the evaluation.\n\u2022 Results Analysis: The results on synthetic dataset 1-3 are demonstrated in Fig. 3 , where we compare the proposed UGLasso with GLasso in terms of F1 score. The left column of Fig. 3 shows the evaluation against the basal truth matrix \u0398 b . The right column of Fig. 3 shows the evaluation against noisy ground truth matrices G. We have following observations.\n\u2022 UGLasso recovers the basal truth network better than GLasso (Fig. 3(a) , 3(c), 3(e)) consistently.\n\u2022 When we include the noises in evaluation, UGLasso achieves competitive performance compared to GLasso, and it usually outperforms GLasso when the number of samples n > 100 (Fig. 3(b), 3(d) ).\n\u2022 Due to the existence of strong noises in dataset 3, UGLasso is outperformed by GLasso (Fig. 3(f) ), where the ground truths contain much more noises than authentic signals. It is likely that GLasso over-fits the noises to achieve higher F1 score in this case."}, {"section_title": "Evaluation on CGLasso \u2022 Dataset:", "text": "The second set of synthetic data is generated to comparing CGLasso with GLasso. As in the contrasting setting where we have two collections of subjects, we need generate a ground truth matrix for each collection. Since we do not enforce the inferred network to be similar to any individual network, we simply use a precision matrix to represent the ground truth network of a collection.\nTo make the network easy to visualize, we divide the m \u00d7 m matrix into several l \u00d7 l square blocks. Then we randomly select some blocks (symmetrically) to fill in values, and leave the rest all 0s. At last we add some random noises to the matrix. we generate three synthetic datasets with parameters as follows:\n\u2022 Dataset 4: m = 50, l = 10, n = 200. The generated ground truth for collection (A) and (B) are shown in Fig. 4(a) and Fig. 4(b) respectively.\n\u2022 Dataset 5: m = 50, l = 5, n = 200. The generated ground truth for collection (A) and (B) are shown in Fig. 4(f) and Fig. 4(g) respectively.\n\u2022 Dataset 6: Same parameters as Dataset 5, different random seed is used. The generated ground truth for collection (A) and (B) are shown in Fig. 4(k) and Fig. 4(l) respectively.\n\u2022 Experimental Protocols: We compare the inferred discriminative network derived by CGLasso with the two inferred networks for collection (A) and collection (B) derived by standard GLasso. We choose the same value of \u03bb for both methods in all experiments.\n\u2022 Results Analysis: Since there is no standard protocol to evaluate contrasting inference, we demostrate the ground truths and inferred networks for synthetic datasets 4-6 in Fig. 4 . The ground truth precision matrices for collection (A) and (B) are shown in the first column and the second column respectively; the difference of the ground truths between two collections is shown in the third column; the discriminative network inferred by the proposed contrasting GLasso is shown in the fourth column; at last, the fifth column shows the network structure of\u0398\ndenote the precision matrix inferred for collection (A) and collection (B) by GLasso respectively. One can observe that CGLasso captures the differences between two collections fairly good with less noises compared to the ones derived by GLasso (\u0398 (A) \u2212\u0398 (B) ). Besides, one can also observe that GLasso has much more false positive in than CGLasso does. These results demonstrate that CGLasso is a more suitable tool for discriminative network discovery. We downloaded all records of resting-state fMRI images and treated the normal brains as healthy subjects, and AD+MCI as the morbid subjects. We applied Automated Anatomical Labeling (AAL 2 ) to extract a sequence of responds from each of the 116 anatomical volumes of interest (AVOI), where each AVOI represents a different brain region. We keep 90 cerebral regions, excluding 26 cerebellar regions. We follow the same preprocess steps in [11] to obtain the cleaned time series data.\n\u2022 Human Immunodeficiency Virus Infection (HIV): The second dataset is collected from the Chicago Early HIV Infection Study in Northwestern University [15] . The dataset contains fMRI brain images of patients with early HIV infection (morbid) as well as normal controls (healthy). The same preprocessing steps as in ADNI dataset were used to extract the cleaned time series."}, {"section_title": "Results and Discussion \u2022 Unified Setting:", "text": "The results of UGLasso on HIV data are shown in Fig. 5 and Fig. 7 . The results for unified graphical Lasso of on HIV data are shown in Fig. 8 and Fig. 9 . In all figures, we present the visualization of brain connectivity on the left and the corresponding precision matrix on the right. In each precision matrix, the diagonal blocks are referring to prefrontal lobe, other parts of frontal lobe, corpus striatum, occipital lobe, parietal lobe and temporal lobe respectively. All results are derived using \u03bb = 2.0 and \u03b1 = 0.5. By comparing Fig. 5 and Fig. 7 , we observe that the overall interconnection between different lobes are weaker in AD patients than the ones in healthy people. These degeneration may explain the AD symptoms such as difficulty thinking and understanding, confusion in the evening. Because understanding and sense of timing usually require the collaboration of several regions in brain, with degenerated connectivity between lobes, AD patients may not function well as normal people. By comparing Fig. 8 and Fig. 9 , we observe that HIV patients have increased connectivity inside occipital lobe compared to the healthy people, which is consistent with previous studies [4] . This may be explained by that people infected by HIV usually vision problems and the major functions of occipital lobe are receives visual information and interprets color, shape and distance [1].\n\u2022 Contrasting Setting: Here we attempt to see if the CGLasso finds any reasonable discriminative patterns between the healthy collection and the morbid collection in real fMRI data. The inferred networks for ADNI dataset and HIV datasets are illustrated in Fig. 6 and Fig. 10 . For ADNI dataset, as can be seen from Fig. 6 , the major differences between AD patients and normal people are located in parietal lobe and temporal lobe, which is consistent with previous studies [9, 16] . Strong decrease of connectivity in these lobes have been detected for AD patients before, which explains the symptoms such as memory loss, mental confusion etc.\nBesides, we also observe a strong connection between \"Frontal Sup Orb L\" and \"Frontal Sup Orb R\" in Figure 6 (a) (the red line across left hemisphere and right hemisphere on the top), indicating that AD patients exhibit a significant different patterns toward the activity between these two regions in frontal lobe. Previous studies [14] sometimes exclude the frontal lobe in analysis since it is thought to be unrelated to AD. However, recent works show there exists increased connectivity in the frontal lobe of AD patients [16] . CGLasso also reveals such pattern in the frontal lobe. As to the HIV dataset, from Fig. 10 we can observe that the major differences between HIV patients and healthy people are located in occipital lobe and part of parietal lobe, which is also supported by previous studies [4] . Several connections within occipital lobe are proved to be discriminative subgraph patterns that are considered to be associated with HIV. No connection is detected in temporal lobe for HIV dataset under contrasting setting, this may because HIV patients do not exhibit the mental symptoms as AD patients do."}, {"section_title": "Related Works", "text": "To the best of our knowledge, this paper is the first work exploring the brain network discovery under unified and contrasting settings. Our work is related to brain network discovery and contrasting learning."}, {"section_title": "Brain Network Discovery", "text": "Most works in this line focus on finding a network representation using sparse Gaussian graphical model (sGGM). Banerjee et al. [2] first formulated the problem of sparse maximum likelihood estimation, where they assumed that the multi-variate variables follow a certain multi-variant Gaussian distribution. Friedman et al. [7] reform the dual problem of Eq. (2.1) as a Lasso-type problem and apply the model on graphs, their method is widely referred to as GLasso. Sun et al. [14] and Huang et al. [10] utilize sGGM to infer networks for three collections of subjects related to Alzheimer's disease, where they treat each collection as a single subject. Davidson et al. [5] propose a supervised tensor-based framework to infer both brain regions and brain connectivity from fMRI data, where strong domain knowledges are required in training. Yang et al. [16] formulate a variant of GLasso called fused multiple graphical Lasso (FMGL) to derive p networks for p similar collections of subjects. FMGL is closely related to our contrasting setting, but with several major differences: (i) FMGL usually infer p separate networks for p collections where p > 2 while contrasting network discovery infers a single network between two collections; (ii) FMGL assumes that one can order the p collections properly where neighbored collections share similar network structure, while contrasting network discovery does not. (iii) For p = 2, FMGL inferres two similar networks."}, {"section_title": "Contrast Learning", "text": "Contrast learning aims at finding discriminative patterns between classes of data. Ramamohanarao et al. [13] study the problem of extracting subgraphs that is frequent in one database but infrequent in another database. Kuo et al. [12] propose to finding a contrasting cut in two collections of graphs, where the cut has a low cost for one collection but has a high cost for the other collection. The problem proposed in [12] is similar to our contrasting network discovery, but they aims at learning the imaging segmentation while we aims at learning the connectivities."}, {"section_title": "Conclusion", "text": "Neuroimaging analysis usually involves one or more collections of subjects, e.g. healthy collection v.s. morbid collection. In this paper we explore the problems of unified network discovery on a collection of subjects and contrasting network discovery on two collections of subjects. Two novel algorithms, UGLasso and CGLasso, are proposed to solve them respectively. Extensive experiments conducted on synthetic datasets and real-world datasets demonstrate the outstanding performance and usefulness of the proposed methods."}]