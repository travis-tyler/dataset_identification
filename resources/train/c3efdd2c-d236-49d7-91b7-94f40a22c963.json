[{"section_title": "Prominent but Less Productive: The Impact of Interdisciplinarity on Scientists' Research", "text": "\nBecause of its expected benefits to science and society (Rhoten and Parker 2004;Sanz, Bordons, and Zulueta 2001), scholars increasingly engage in an interdisciplinary mode of research, which \"integrates perspectives, information, data, techniques, tools, concepts, and/or theories from two or more disciplines\" (National Academies of Science, National Academy of Engineering, and Institute of Medicine 2005:188). While the practice of interdisciplinarity is not new (Abbott 2001), it is increasingly prevalent in the natural (Rhoten and Pfirman 2007) and social sciences (Brint 2005;Jacobs and Frickel 2009). Universities are reorganizing to facilitate interdisciplinary research (IDR) by developing crossdisciplinary problem-focused centers and funding cross-department and cross-college research initiatives (Biancani, McFarland, and Dahlander in press;Pray 2002). And since the mid-1980s, the National Science Foundation has supported cross-cutting funding opportunities and interdisciplinary research centers. Scientists laud IDR as a 'progressive' 'hot topic' that is 'running rampant;' arguably one 'must be interdisciplinary to be world-class' (Pray 2002). But evidence in support of this contention is sparse and \"relatively little research on many of the underlying issues has been conducted\" (Jacobs and Frickel 2009: 44). In particular, systematic investigation of IDR's effects on scientific careers has been neglected. What are the professional costs and benefits of engaging in IDR? To understand the impact of IDR on scientists' careers, we draw on two strands of organizational theory. The first documents the innovative benefits of joining diverse ideas across domains. The core idea can be found in theories of diversity, brokerage, and innovation (Burt 2004;Singh and Fleming 2010): pooling non-redundant information from disparate sources is the foundation from which novel ideas spring (Hargadon 2002;Weitzman 1998). This work suggests that bridging disconnected knowledge spaces will result in better ideas that will be rewarded in the marketplace (Lo and Kennedy in press). The second documents penalties associated with category spanning (Zuckerman 1999). Rather than being perceived as innovative, offerings spanning multiple domains have an ambiguous identity that is difficult for audiences to understand, and are thus devalued (Hsu, Hannan, and Kocak 2009). Recent empirical work describes the negative perceptions of the audience or market as reception-side penalties. However, the category literature has also theorized production-side penalties where category-spanning products are more difficult to produce. The principle of allocation suggests that investing in multiple categories limits mastery and dilutes quality, resulting in a \"Jack of all trades\" who is master of none (Hannan and Freeman 1989;Hsu 2006b). This work builds on niche-width theory that anticipates problems of focus when organizations span strategies or forms (Freeman and Hannan 1983). Recent empirical work has found that category-spanning products are lower quality, likely because the production process itself is wanting (Kov\u00e1cs and Johnson in press). This contributes to, but does not completely explain, the reception-side devaluation of category-spanning products (Kovacs and Sharkey 2014;Negro and Leung 2013). From this we gather that production-and reception-side effects can operate simultaneously. On the production side, we identify limitations in previous scholarship on penalties that accrue to category-spanning offerings. We suggest production penalties result not only from limited mastery and inferior quality, which are highlighted in the categories literature (Hsu, Hannan, and Kocak 2009;Negro and Leung 2013), but also from the cognitive and collaborative challenges of category-spanning work (Rafols, Leydesdorff, OHare, Nightingale, and Stirling 2012). Grasping ideas and perspectives from another field is cognitively taxing, working with diverse collaborators from multiple disciplines can produce frustration and conflict, and reviewers may have difficulty digesting and evaluating crossdisciplinary products. All of these challenges may lengthen the time to publication, and thereby depress scholars' productivity. On the reception-side, we expect work that draws on disparate intellectual domains to have broad appeal and achieve greater scholarly visibility. Atypical and novel combinations of ideas have greater impact (Schilling and Green 2011), so scientists may be drawn toward, rather than confused by, multicategory offerings like IDR, like Pontikes' (2013) market-makers. Following the innovation literature which finds that experimenting with new combinations is uncertain and risky (Fleming 2001), we similarly expect that IDR, like other high-risk strategies, sustains overall benefits but higher variance in reception. Although IDR boosts visibility overall (mean levels of citations), it should also increase the variance of citations that a scientist's body of work garners. Further, we refine the way in which category-spanning is typically measured. Rather than spanning two categories (i.e., crossing a single boundary), interdisciplinary research typically draws upon multiple disciplines. Theoretically, we begin with disciplinary variety as the underlying concept of interest to us (Harrison and Klein 2007). Methodologically, we operationalize this not by assessing whether a line has been crossed, as others have done (Fleming, Mingo, and Chen 2007;Hsu, Hannan, and Kocak 2009), but by measuring the variety inherent in each scientist's body of work. While this is an advance over binary measures of IDR (Clemens, Powell, McIlwaine, and Okamoto 1995;Jacobs and Frickel 2009), we take it one step furtherfollowing Leahey & Moody (2014) to account for the cognitive distance between the categories (i.e., disciplines) because all disciplinary combinations are not equally disparate. In this way, we build on recent work in the category literature that has begun to examine the similarity of (and distance between) categories (Hannan and Kovacs 2011;Kovacs and Sharkey 2014). Finally, we suggest that the penalties and benefits of interdisciplinary work depend upon the nature and intellectual life cycle of one's field. The difficulties associated with category-spanning may be reduced, and the benefits may be accentuated, when category-spanning is popular (Lo and Kennedy in press). In fields with a tradition of IDR, training may reduce the difficulties of producing IDR research and audiences may particularly value such work. Thus we assess not only the main effects of IDR on productivity and visibility, but also how field-level norms modify such effects. In sum, this study makes several contributions to organizational theory. First, we empirically distinguish betweenand assess the simultaneous effects ofproduction side and reception side processes. Second, we highlight production penalties that result from cognitive and collaborative challenges (as Zuckerman and colleagues (2003) anticipated) and disentangle potential mechanisms that manifest in different stages of the production process. Third, we move away from a binary conceptualization of 'spanning' and toward a continuous conceptualization of 'variety' that uniquely considers the dissimilarity among categories (here, disciplines). Fourth, we begin to understand the normalization of category-spanning by examining an important, contextual-level moderator of such effects: field-level interdisciplinarity. We further explicate the nature of these contributions, and then proceed to test our ideas using primary data collected on almost 900 scientists and their 30,000 scholarly papers."}, {"section_title": "ABSTRACT", "text": "Inter-disciplinary research (IDR) is being promoted by federal agencies and universities nationwide because it presumably spurs transformative, innovative science. In this paper we bring empirical data to assess whether IDR is indeed beneficial, and whether costs accompany potential benefits. Existing research highlights this tension: whereas the innovation literature suggests that spanning disciplines is beneficial because it allows scientists to see connections across fields, the categories literature suggests that spanning disciplines is penalized, because the resulting research may be lower quality or confusing to place. To investigate this, we empirically distinguish production and reception effects and we highlight a new production penalty: cognitive and collaborative challenges associated with IDR may result in slower progress, hurdles during peer review, and lower productivity (though not necessarily lower quality). We compile and analyze data on almost 900 research center-based scientists and their 32,000 published articles. Using an innovative measure of IDR that considers the similarity of the disciplines spanned, we document both penalties (fewer papers published) and benefits (increased visibility) associated with IDR, and show that it is a high-risk, high-reward endeavor. These costs and benefits depend on characteristics of the field and a scientist's place in it."}, {"section_title": "PRODUCTION PENALTIES OF INTERDISCIPLINARY RESEARCH", "text": "Disciplinary fields are an entrenched way in which scholarly activity is organized (S\u00e1 2008) and evaluated (Lamont 2009), making field-spanning offerings unexpected. Like other categories, fields are not arbitrarily constructed, but rather reflect the distinct environments that disciplines face, the formal and tacit skill sets that members acquire (Zuckerman, Kim, Ukanwa, and Rittmann 2003), and the distinct claims of jurisdiction that occasionally come into dispute (Abbott 2001). Disciplinary fields are categories that help academics parse and digest a vast intellectual terrain in order to conduct and evaluate scholarship. From the scholarship on categories, we know that when categories are not strictly adhered to, penalties ensue. Theoretically, penalties can emerge from both the production side and the reception side. For example, Zuckerman, Kim, Ukanwa, and Rittmann (2003), suggest that penalties emanate from both the production side (e.g., it's harder to produce and quality may suffer) and the reception side (e.g., it is confusing for audiences to place a category-spanning offering). Recent empirical tests have focused mostly on the reception penalties: audiences tend to overlook, devalue, or outright reject offerings that span categories because they are difficult to comprehend and do not fit existing schemas (Hsu, Hannan, and Kocak 2009). Category-spanning actors and offerings tend to be poorly received: feature-film actors who take on too many diverse roles have difficulty obtaining work (Zuckerman, Kim, Ukanwa, and Rittmann 2003); films spanning multiple genres are less appealing to audiences (Hsu 2006a); and eBay sellers who try to market their product in multiple categories are less successful in the auction (Hsu, Hannan, and Kocak 2009). Audiences penalize offerings that are difficult to classify. We turn our attention to less examined production penalties and in particular to the cognitive and collaborative challenges that slow the development of mastery. Because cognitive resources are finite, the argument goes, mastery may be more difficult to attain when time, energy, and effort are distributed across many categories (Carroll 1985;Freeman and Hannan 1983;Hannan and Freeman 1989;Hannan, Polos, and Carroll 2007). This early work in population ecology theorized, but did not empirically examine, the problems of diffused resources and focus that may disadvantage generalists in productionincluding greater competition and likelihood of mortality. Recent work does not examine mastery per se, but suggests that it accounts for lower quality products (Kovacs and Sharkey 2014;Negro and Leung 2013). We hew closer to the concept of mastery itself and argue for technical and logistical hurdles when spanning domains (Lingo and O'Mahony 2010;Zuckerman 2005). For example, research on patents suggests that category-spanning increases technological uncertainty and creates cognitive limitations (Fleming 2001;Lo and Kennedy in press). Lamont and colleagues (2006)lend support to the idea that category-spanning ideas are harder to produce: it is challenging for scholars to accommodate the concepts of multiple fields and to produce output that is standard in form and content (see also Rafols et al. 2012). In addition, when collaborative in nature, interdisciplinary research may produce epistemological or methodological conflict (Murray 2010;van Rijnsoever and Hessels 2011) between members of different fields. This lack of consensus may increase coordination costs (Cummings and Kiesler 2005;Cummings and Kiesler 2007;Shrum, Genuth, and Chompalov 2007), conflict and tension (Murray 2010;Owen-Smith and Powell 2003), and role strain (Boardman and Bozeman 2007) that may outweigh the general benefits of collaborative work (Gans and Murray 2014). Because commonality and consensus speed the implementation of ideas (Beckman 2006), such interdisciplinary divergences and tensions may slow progress toward publication. These initial production penalties, experienced as scientists plan, conduct, and write up their research, likely follow the paper into a second stage of production: the review stage. In the peer review process, experts from different fields often disagree on the merits of a paper and evaluate them differently (Lamont 2009;Mansilla 2006). Indeed, Birnbaum (1981) finds that research that does not fit neatly within the substantive bounds of 'normal science' instills irritation, confusion, and misunderstanding among reviewers and editors. This makes the road to publication challenging, and the final product may take longer to produce. We see a similar phenomenon in the patent context: approval times are longer when inventions span (or in their language, blend) categories (Lo and Kennedy in press). Thus the review process may be slower and more challenging for interdisciplinary work, which contributes to lower rates of productivity. 1 Indeed, even among highly successful interdisciplinary scholars, we find some initial evidence that production penalties result from cognitive challenges. In concurrent interviews the first author is conducting with recipients of the Mellon Foundation's New Directions fellowship (who, post-tenure in one field, received formal training in another field), we find evidence that interdisciplinary research takes additional time, commitment, and effort. It's mentally more taxing: \"I'll be reading epistemology and political philosophy\u2026.and then do research and mathematical things. It's just a bit of a stretch of the brain to do such things.\" It takes longer: \"I think part of me taking longer to go from conception to a finished published article has to do with trying to think through two separate disciplinary concerns.\" In the end, it can take a toll on one's productivity: \"I just don't feel like I've made great strides of improvement in being as productive as I could be.\" This suggests that scientists who are more engaged with interdisciplinary work will publish fewer research articles than scientists who engage in IDR less. Thus, we hypothesize a production penalty: H1: IDR is associated with lower productivity"}, {"section_title": "RECEPTION BENEFITS OF INTERDISCIPLINARY RESEARCH", "text": "Although category-spanning products are typically discounted or dismissed, we do not expect IDR to incur a reception penaltythat is, to be received poorly in the academic marketplace (i.e., rarely 1 We collect data on a sample of unpublished papers to assess whether IDR is more likely to be rejected outright. We also bring additional data on time in the review process to bear on this question. cited). As Pontikes (2012) demonstrates, research documenting penalties has largely been conducted on 'market-takers'audiences that are looking to identify, place and evaluate a product. To market-takers, multiple categories (and ambiguous classification more generally) may produce confusion and prevent a timely and effective search. In contrast, 'market-makers' are looking to identify novel, original, and groundbreaking research that redefines the (academic) market structure (Pontikes 2012). This distinction helps reconcile the reception penalties noted in the categories literature with the reception benefits noted in the recombinatory innovation literature, like Fleming's research documenting that patents that span patent classes and sub-classes receive more citations (Fleming 2001;Singh and Fleming 2010). Marketmakers see ambiguous classification not as confusing but rather appealing, and perhaps an indication of innovative work that has brought insights from one arena to illuminate another. In fact, various streams of research document the benefits that accrue to joining domains of knowledge. For example, Burt (2004) found that when managers span multiple departments, their ideas were evaluated more favorably than managers who didn't span departments. Similarly, research on innovation suggests that information pooled from disparate sources provides a (if not the) foundation from which new ideas spring (Fleming and Waguespack 2007;Hargadon 2002). This link is so tight as to warrant use of the term \"recombinant innovation\" (Weitzman 1998). The underlying idea is that bridging knowledge domains and developing new combinations serves as the foundation for innovation (Schumpeter 1912(Schumpeter [1934; Weick 1979) and will be recognized as such. For example, Singh and Fleming (2010) find that experience diversity and network size of a research team predict breakthrough patents. Research on the impact of category-spanning in science finds that atypical, category-spanning offerings have higher impact (Lo and Kennedy in press; Schilling and Green 2011; Shi, Adamic, Tseng, and Clarkson 2009;Uzzi, Mukherjee, Stringer, and Jones 2013). Interdisciplinary publications, as a form of atypical, domain-spanning publications, likely experience these same benefits. In academe, positive reception is typically gauged via citations (Evans 2008;Lynn 2014;Merton 1977). Indeed, citations give us a sense of how a paper is received by the scientific community. Among other things, 2 reference to a scholarly work indicates its usefulness and influence because it contributed, in some way, to a subsequent work. A scientist's citation count gives a good indication of his or her visibility in the scientific community, and for this reason it is factored into promotion and tenure decisions, and has been shown to impact earnings (Diamond 1986;Leahey 2007;Sauer 1988). One goal of this paper is to empirically test for such a reception benefit: whether scholarly visibility (as indicated by citation counts) accrues to interdisciplinary work, even after controlling for the size of the prospective audience (which increases when multiple audiences are targeted)."}, {"section_title": "H2: IDR is associated with higher visibility", "text": "Because we expect to find empirical support for both a production penalty (H1) and a reception benefit (H2), it is likely that IDR is a high-risk, high-reward activity that often gains prominence but sometimes does not. Even if IDR positively impacts visibility (H2), it likely increases the probability of both breakthroughs (like achieving scientific prominence) and failures (like being ignored); (Singh and Fleming 2010) document a similar effect among patents. Thus, although on average IDR will be wellreceived, we also expect IDR work to have greater variability in citations."}, {"section_title": "H3: IDR is associated with greater variability in visibility", "text": "Finally, we highlight that we are distinguishing between spanning categories and spanning distant categories. Most investigations simply assess whether boundary-spanning has occurred (Clemens, Powell, McIlwaine, and Okamoto 1995;Fleming, Mingo, and Chen 2007;Hsu, Hannan, and Kocak 2009), without considering the relationship between the spanned entities (exceptions include Braun and Schubert (2003), Rosenkopf and Almeida (2003), and Schilling and Green (2011)). The category literature is just 2 There is variation in what a citation signals (van Dalen and Henkens 2005). Citations may reflect disciplinary alliances and mutually-reinforcing citation practices, instrumental attempts to flatter potential reviewers, or even the controversial nature of an article. Authors may cite previous work in a casual way, or rely on it heavily. They may think of it highly, or dismiss it as flawed (Ferber 1986;Latour 1987;Lynn 2014;Najman and Hewitt 2003). Or are they cognitively distant, like geography and optics? Put simply, interdisciplinary research can be more or less novel, depending on the relationship between the spanned fields (Carnabuci and Bruggerman 2009: 608). Indeed, spanning two closely related categories is hardly different from not spanning at all. In terms of penalties, the cognitive and coordination challenges associated with category-spanning likely increase as the distance between domains increases. In terms of benefits, the utility and value of categoryspanning research also increases with distance: the most fertile creative products are \"drawn from domains that are far apart\" (Poincare 1952(Poincare [1908) and the best conceptual metaphors are those that create ties across great distances (Knorr Cetina 1980). We thus hypothesize that distance matters: H4: An alternate measure of IDR that does not incorporate distance will have weaker effects on productivity and visibility."}, {"section_title": "INTELLECTUAL CONTEXT and IDR", "text": "We expect the penalties and benefits associated with IDR to be shaped by each scientist's intellectual context, that is, the interdisciplinary nature of their primary field. We know from previous research that some fields, like the life sciences, are more interdisciplinary than other fields, such as electrical engineering. Data from the Survey of Earned Doctorates (Millar and Dillman 2012) reveals that the life sciences have the highest proportion of dissertations self-classified as interdisciplinary (36%), especially compared to engineering and math (26% and 21% respectively). Using a continuous measure of IDR that incorporates cognitive dissimilarity (distance), Porter and Rafols (2009) and Porter et al. (2007) find that Biotechnology (mean=0.654, on a scale of 0 to 1) and Medicine (mean=0.664) are more  . On the reception-side, this suggests that highly interdisciplinary fields, like life sciences, will expect and appreciate IDR work, whereas less interdisciplinary fields will view it as unnecessary or unusual. Thus, scholars in interdisciplinary fields should experience stronger visibility benefits for their IDR work. On the production-side, scholars in highly interdisciplinary fields may experience fewer challenges, as there are models for guidance, potentially better training in IDR, and a more amenable and productive review process for IDR, whereas scholars in less interdisciplinary fields may experience greater difficulties producing IDR. Lo and Kennedy (in press) suggest a similar logic: audience members and patent examiners are more favorable when category-spanning is more familiar and typical. To summarize: if one's disciplinary environment is conducive to (and appreciative of) IDR, then the benefits of IDR might be stronger, and the penalties less stiff: H5a: In highly interdisciplinary fields, IDR's positive association with visibility will be stronger than in less interdisciplinary fields. H5b: In highly interdisciplinary fields, IDR's negative association with productivity will be weaker than in less interdisciplinary fields. In sum, our paper assesses benefits and penalties associated with interdisciplinary research as well as field level expectations that may moderate its effects. We expect the penalties to manifest largely in the production stage, mostly in the form of increased cognitive and communicative challenges, and reduced productivity. Consistent with the innovation literature, we expect benefits to emerge in the reception stage, in the form of increased citations. Our empirical focus heeds Jacobs and Frickel's (2009: 61) call to understand how \"interdisciplinary scholarship compare[s] with otherwise similar scholarship\" and provides much needed grounding for claims about the value of IDR."}, {"section_title": "DATA and METHODS", "text": ""}, {"section_title": "Sample", "text": "To test our hypotheses, we collected archival publication data on scientists and social scientists associated with 52 NSF-funded industry/university-cooperative research centers (IUCRCs). These centers are housed at universities and conduct research that is of interest to (and partly supported by) industry, in areas as diverse as biosurface physics, civil and environmental engineering, and architectural science. We collected information about all IUCRC membersincluding doctoral students, post-docs, and faculty at all career stagesas of 2003. Our analysis is limited to the subset of 854 PhD-level scientists with a publication record as of 2005 because our key variables (IDR, productivity, and visibility) cannot be computed for scientists who have not published. Information on approximately 32,000 articles published by these center affiliates (such as number of authors, institutions represented, journal of publication, number of citations) was obtained through Thomson Reuters Web of Science (WoS). We obtained data on all the articles referenced by these 32,000 focal articles, notably their WoS subject categories (SCs), which we used to measure the extent to which each scholar's research is interdisciplinary (described below). To these data we added field-, university-, and individual-level data from various sources (also detailed below). Our main analyses involve these archival data; however, for supplementary analyses, we relied on a survey of these center-based scientists (conducted by the second author), publicly available data on journal turn-around times, and unpublished working papers for a subset of authors."}, {"section_title": "Dependent Variables", "text": "Productivity. To capture each scholar's productivity, we rely on the total number of articles published (in WoS journals) from the beginning of a scholar's career (i.e., the year they first published) until 2005. This is a conservative measure of productivity that excludes book chapters and articles published in less internationally recognized journals; it is likely more accurate than self-reported productivity used by studies that rely on survey data, such as the Survey of Earned Doctorates. Using article counts as a measure of productivity is standard in the literature on scientific productivity; indeed, the six most recent papers on scientific productivity in the NBER working paper series rely on article counts. Article counts remain standard even as collaboration increases, because it is highly correlated with co-author-weighted publication counts (Cole and Zuckerman 1985;Wagner-Dobler 1997). We do, however, assess the robustness of our results to this alternative measure. We dismiss journal impact factor (JIF) weighted productivity measures because they confound quantity with qualitythe two distinct outcomes that we expect IDR to affect differentially. We examine the number of articles at the person and person-year level in our analyses. Visibility. We measure visibility by collecting the (forward) citations that have accrued to each published article (indexed in WoS) as of 2010. Citations to an individual paper are a more precise measure of a paper's impact than (the prestige of) its journal of publication; however, we control for journal impact factor in all models. In addition to this paper-level measure, we aggregate (by taking the mean) to obtain person and person-year level measures of visibility for certain analyses. When aggregated, both productivity and visibility are highly right-skewed, so we take the natural log as previous researchers have done (Allison and Long 1987;McBrier 2003;Prpic 2002) or use a count (negative binomial) model for models of productivity. To test Hypothesis 3, where we hypothesize about variability in citations, we calculate the standard deviation of a scientists' citations."}, {"section_title": "Independent Variables", "text": "Interdisciplinary Research (IDR). Our measure of IDR is borrowed from Porter and colleagues (2007:134). We measure interdisciplinary research at the paper level, aggregating up to the person or the Web of Science assigns 1-6 SCs to each indexed journal, which we then extend to each constituent reference. As input for the measure of interdisciplinary research, we pool all SCs from the focal paper's set of references (rather than SCs of the focal paper itself, or SCs of papers that cite the focal paper), an approach that best gauges knowledge integration (Porter, Cohen, Roessner, and Perreault 2007:127). This captures the breadth of research referenced (and presumably integrated) in a paper. While the variety of SCs and their balance is specific to each focal paper's bibliography, their similarity (sij, from a SCxSC co-citation matrix which we convert to cosines) 3 is derived from the population of all WoS-indexed articles and thus shared by all focal papers. Porter's measure is a particular parameterization of the Sterling Index: where sij is the similarity between SCs i and j, pi is the proportion of referenced papers in subject category i, and pj is the proportion of referenced papers in subject category j (Rafols and Meyer 2010: 267-8). We demonstrate the IDR calculation, step-by-step, for three hypothetical articles in Appendix Table A1. Intuitively, a paper's IDR score increases as it references more, relatively unrelated SCs 4 (Porter, Cohen, Roessner, and Perreault 2007:277). The IDR score ranges from 0 to 1, with scores closer to 1 indicating greater interdisciplinarity. For paper and person-year analyses, the IDR score is zero for 551 papers and 99 person-years. These values occur when the person has a very low productivity for that year (or has a single paper that references only one subject category). Given that these low productivity years obscure the production penalty that we hypothesize for IDR papers (because low IDR scores are associated with low productivity in the years where someone publishes little), we restrict all analyses to exclude observations with zero IDR scores. The visibility analyses are robust to including these observations; the productivity effects are curvilinear with this inclusion because IDR scores of zero only occur in low productivity years, and high IDR scores are also associated with low productivity (as we hypothesize). For the person-year analysis, we use the mean IDR score of papers published in that year to predict productivity. ~ Table A1 about here ~ 3 Off-diagonal elements in a SCxSC matrix for the year 2007 would indicate the number of papers that cited both SCs (i and j) between 2002 and 2006. To construct our SCxSC matrix, we summed three square matrices for the years 1987, 1997, and 2007 for the person-level analyses. We used the matrix closest to the year of publication for the person-year level analyses. While a number of other measures of interdisciplinary research have been proposed and used in previous research, no other measure incorporates the relatedness of the categories that are joined. This is true for institution-level measures, e.g., the number of disciplines that members represent (Birnbaum 1981); person-level measures, e.g., whether an individual faculty member has a joint appointment (Jacobs and Frickel 2009); and even other publication-level measures. For example, Clemens and colleagues (1995:454) assess whether the paper is cited in a discipline other than the discipline the author(s) represented. Larivi\u00e8re and Gingras (2010)  we rely on Porter's measure of IDR (which incorporates distance, or dissimilarity, between every pair of fields) andto assess the extent to which distance matters above and beyond sheer variety (H4)we compare it to the effects of a less refined measure: the total number of unique subject categories (SCs) appearing in a scholar's pooled set of references. We ascertain the validity of this measure in a number of ways. First, by showing how the IDR score increases as a scientist references not only more, but more unrelated, fields, Table A1 provides face validity. Second, the IDR score (which is based on the variety, evenness, and distance of referenced SCs) is also positively correlated (r=0.18) with the number of SCs that characterize the focal paper itself (recall that a given paper is assigned 1-6 SCs). This is to be expected if focal paper SCs adequately capture the content of a paper that is more thoroughly gleaned from an analysis of the SCs it references, and it demonstrates convergent validity. Third, as we should expect, the mean IDR score of articles published in the interdisciplinary journals Science, Nature, and PNAS (0.71, 0.71, and 0.70, respectively) are significantly higher than the average IDR score in our sample (0.62)more than half a standard deviation above. Thus we are reassured that the measure we use adequately captures the concept of interest to us. Field IDR. We use both continuous and categorical measures of field-level IDR. The continuous measure is calculated as the average IDR score for the nine broad fields represented in our sample of scientists, including physics, chemistry, biology, social science, and most types of engineering (chemical, civil, material, electrical). The categorical measures indicate whether the scholar is in the life sciences (the highest IDR field in our sample) and whether the scholar is in electrical engineering (one of the lowest IDR fields in our sample). Data from the Survey of Earned Doctorates (using self-reports on the interdisciplinary nature of one's dissertation) confirm these as high (life sciences) and low (electrical engineering) interdisciplinary fields. Our data further corroborate this claim: 22% of the life science journals represented in our sample are interdisciplinary (i.e., their subject category is \"Multidisciplinary\"), compared to only 7% of the electrical engineering journals. We test H5 in two ways, at two levels. At the person-level, we compare two subsets of scholars: those working in a high IDR field (life sciences) and those working in a low IDR field (electrical engineering). At the person-year level, we use the continuous field level IDR score (the mean IDR score of all sampled scientists in each field), and interact that with the individual IDR score."}, {"section_title": "Control Variables", "text": "The scientific papers we analyze are nested within persons, so, when comparing publications across persons, we control for characteristics at this level -including gender, professional age, and status. Gender and professional age have been shown to influence engagement with IDR (Dahlander and Frederiksen 2012;Mansilla 2006; van Rijnsoever and Hessels 2011), as well as productivity and visibility (Leahey 2006;Maliniaka, Powers, and Walter 2013). Gender of each scientist was derived from analysis of first names, as well as information (pictures and pronouns) used on the scientists' websites. Professional age was calculated by subtracting the year of PhD receipt (obtained from CVs and Proquest Dissertation Abstracts) from 2005, the year publication data were collected. To proxy status of current institution, we use the number of faculty who are members of the National Academy of Sciences (collected from The Center for Measuring University Performance). To measure the quality of an individual scholar, we include the ranking of one's PhD-granting institution, obtained from the Academic Ranking of World Universities, (http://www.arwu.org) and reverse-coded so that higher values indicate higher quality. Although certainly a rough measure of individual quality, it helps to rule out concerns about \"smarter\" scholars being more (or less) likely to engage in IDR research. The papers we analyze are also nested within journals and fields, so we also incorporate controls at these levels. Because interdisciplinary research has been linked with the prestige of the journal in which it appears (Rinia, van Leeuwen, and van Raan 2002), we control for mean impact factor of the journals each scholar has published in. When modeling productivity, we control for average turn-aroundtime for journals in the field, obtained from Bjork and Solomon (2013). When modeling visibility, we control for average citations per paper in the field, obtained from Thomson Reuters. Because the relationship between interdisciplinary research and citation counts depends on the field (Hamilton, Narin, and Olivastro 2005;Larivi\u00e8re and Gingras 2010;Zitt, Ramanana-Rahary, and Bassecoulard 2005), and because we want to rule out increased audience size as an alternative explanation for heightened visibility, we control for field size (the number of PhDs produced in a recent year, obtained from NSF's Survey of Earned Doctorates). In addition, we control for the potential reach of each paper (Fleming & Sorenson 2004:919), captured by the number of SCs that classify the focal paper itself, rather than its references. We presume that papers classified in multiple fields will be brought to the attention of a larger body of scholars, potentially boosting citations (thus, this is another control for audience size). Although our sample is center-based, we did not include center level controls because none reached statistical significance and the intra-class correlation coefficient (0.11) suggested minimal clustering by center. In paper-level and person-year analyses, we controlled for additional variables that might influence visibility and/or productivity. Teams produce more highly cited papers, on average, than sole authors (Montpetit, Blais, and Foucault 2008;Wuchty, Jones, and Uzzi 2007), and authors who have worked together previously may face fewer production penalties compared to newly-formed collaborative teams. Thus, we included the number of authors on the paper and a binary variable indicating whether this combination of authors had published together before (in yearly analyses, these were measured as the average number of authors and the proportion of papers with repeat collaborators, respectively). We also control for both lagged cumulative publication experience as well as lagged cumulative citations (logged to remedy skewness). Finally, we controlled for publication year as older publications have more time to accrue citations than more recent publications."}, {"section_title": "Statistical Approach", "text": "Interdisciplinarity is measured at the paper-level, but some scientists' research outcomes (such as productivity) cannot be measured at the paper level, so we present models at the paper, person-year and person level of analysis. This allows us to model outcomes at the most appropriate level(s), incorporate level-specific control variables, and assess the robustness of our results. Our first analysis takes place at the person level (n=854 scientists). We rely on path analytic techniques, which are ideal for two reasons. First, we have two main outcomes of interest (productivity and visibility), and path analysis allows us to model these outcomes and their own inter-relationship simultaneously, in the same model. Second, path models allow us to examine direct as well as indirect effectsof, for example, IDR\uf0e0productivity\uf0e0visibilityand thus explicitly model the influence of intervening variables, unlike a regression-based mediation analysis. In addition, the structural equation modeling (sem) package in Stata 13, which we use to estimate path models, is ideal for handling missing data. Rather than deleting data in a listwise fashion, which is the default strategy in most statistical packages, this package relies on a full information maximum likelihood estimation procedure. This strategy permits the inclusion of all available data (Anderson 1957) and bypasses the need to impute data. We also take advantage of our data structurepanel data over the career of individual scientiststo estimate models at the paper and person-year levels of analysis. This allows us to assess the robustness of the person-level results, to better assess causal direction, explicitly test for field-level moderators, and examine the effects of paper-level variables. A Hausman test suggests a fixed-effects model is more appropriate than a random effects model (\u03c7 2 = 405.57; \u03c7 2 = 382.90; p<.0001 for productivity and visibility, respectively). We estimate fixed-effects regression models at the person-year level (n=8,779) --because a person's productivity cannot be captured at the paper levelbut also at the paper level (n=29,782) when modeling visibility. The fixed-effects models allow us to look at within-person variation on our dependent variables and control for unmeasured person-level characteristics, and thereby provide the most conservative test of our hypotheses. However, when testing Hypothesis 5 on field-level IDR, we estimate random-effect models with robust standard errors (for visibility) and population-averaged negative binomial models with robust standard errors (for productivity) because field IDR, and other field-level characteristics like journal turn-around-time and average field citations, are time-invariant and would be dropped from a fixed-effects model 5 . We use multiple imputation because of missing data for some variables; however, results are the same without multiple imputation."}, {"section_title": "RESULTS", "text": "We begin by describing our data and measures in Table 1. Panel A describes our sample of 854 scientists, all of whom have a publication record, making all our results reported herein conditional upon publication. Panel B provides descriptive statistics for all variables used in the person-year level models. There is ample variation on the key outcomes (visibility and productivity), both of which are left-skewed so we take the natural log for subsequent analyses. The key predictor, IDR, also displays variation -ranging from 0.08 to 0.88 (on a scale of 0 to 1). To detect problems of multicollinearity, we calculated the variance inflation factors (VIF). In all models the VIF scores were below six, below the recommended cutoff value of ten (Neter, Wasserman, and Kutner, 1985). In the section on robustness checks below, we demonstrate the representativeness of our sample by comparing our sample of papers with the population of WoS papers analyzed by Uzzi et al. (2013) and comparing our sample of scientists with the population of PhD-level scientists from NSF's surveys. ~ Table 1 about here ~ As expected (H1), IDR depresses scholarly productivity (see Table 2, Model 1); this effect is statistically significant at the 5% level. Recalling that the outcome variable is logged, 6 the coefficient of -0.77 suggests that a 10% increase in IDR reduces productivity by 7.7% over one's career, controlling for professional age and other factors. This effect suggests that interdisciplinary scientists do indeed experience lower productivity. This productivity penalty holds, and even gets stronger (b= -0.99**, SE=0.39), even when we weight the article count by number of coauthors (such that a paper with two authors only contributes 0.5 to a scholar's productivity). We also examine IDR's effect on productivity at the person-year level (in which IDR scores are averaged, and publications are summed). Using a model with person fixed-effects as well as time-period controls 7 (Table 3, Model 1) allows us to rule out differences in individual propensities to engage in IDR. Here, too, IDR has a significant and negative effect (b= -0.143**) on productivity. In those years when scholars do more interdisciplinary work, they publish fewer articles. Thus we find consistent support for a production penalty across both specifications, consistent with Hypothesis 1. ~ Tables 2 and 3 about here ~ However, once published IDR shines: in support of H2, we find that IDR increases scholarly visibility. The coefficient for IDR (+0.69*** in Table 2, Model 1) suggests that a 10% increase in IDR increases a scholar's citation, on average, by 6.9%. In Table 3, Model 4 (a paper level model with person and year fixed-effects), we see that IDR has a positive and significant effect on visibility. This effect also holds when we exclude 'group authors,' whose papers tend to be highly cited (results not shown). Because articles in multidisciplinary, high-impact journals like Science, Nature, and PNAS could be driving the visibility benefit, we omitted these 171 papers (0.05% of total) from all measures. All hypothesized results are robust to this change. 8 Although IDR is more visible, on average, we also find support for H3: having a record of interdisciplinary scholarship increases the overall variance of a scientist's paper (H3). In Table 2, Model 2, we see that scientists who publish more IDR are more likely to produce both frequently-cited and rarely-cited works. Here, we model the standard deviation in citations rather than mean citations (and also control for the standard deviation rather than the mean of journal impact factor). We find that interdisciplinary scientists, in addition to having a greater total number of citations, also experience more variability in citations across their papers. Scientists with a record of interdisciplinary scholarship experience more 'hits' and more 'flops' than their mono-disciplinary counterparts. To investigate this further, we examine whether it is indeed the high IDR papers that display more variability in citations (rather than highly interdisciplinary scientists having a few disciplinary papers that are not well cited). Using the median IDR score, we distinguish low IDR and high IDR papers. Then, for each scientist who had at least two papers of each type (n=647), we calculate the standard deviation of the citations received by their low IDR papers, and do the same for their high IDR papers. As theorized, the variance of scholars' high IDR papers (mean=30.4) is higher than the variance of their low IDR papers (mean=24.2), and this difference is statistically significant (p<.0015). The distance (or cognitive dissimilarity) between fields contributes to the visibility benefit and determines the productivity penalty, providing support for H4. We assess this by extracting distance out of the IDR measure: we simply calculate the total number of unique SCs referenced by a scientist (across all his papers) with no regard for their similarity (range 1-107, mean=32). When we substitute this 8 These journals are all classified into the \"Multidisciplinary\" subject category (SC). Another way to identify interdisciplinary journals is to examine those with multiple SCs assigned (e.g., the maximum is 6). At the journal level, we find that the number of SCs is negatively related to Journal Impact Factor. (-0.15), reassuring us that a correlation between interdisciplinary journals and impact is not driving the visibility benefit we document. measure for the IDR measure (see Table 2, Model 3), the positive effect on citations holds, suggesting that even spanning related fields improves citations (if only slightly), presumably by broadening one's prospective audience. However, the negative effect on productivity does not hold. In fact, this alternative measure of IDR positively affects productivity, perhaps because drawing on multiple disciplines expands the number of possible journal outlets. Simply drawing upon more SCs doesn't hinder productivity, unless those SCs are cognitively dissimilar. This suggests that it is more difficult to produce and successfully publish scholarship that spans unrelated fields (e.g., chemical engineering and anthropology) than related fields (e.g., chemical engineering and civil engineering). In supplementary analyses (not reported), we confirm these results at the paper-and person-levels of analysis by controlling for number of SCs referenced in each paper and we confirm that our IDR measure (which incorporates distance) is the only significant predictor. This offers support for the mechanism behind the production penalty we theorize: IDR is cognitively difficult and slow to produce when it blends disparate fields. Lastly, we examine how the effects of IDR depend on the nature of the field. As expected (H5b), we find that in highly interdisciplinary fields like the life sciences, IDR's negative impact on productivity is weaker than in less interdisciplinary fields like electrical engineering; where in fact, IDR fails to reach statistical significance (Table 2, Models 4 and 5). These sub-analyses fail to support H5a (field differences in the reception benefit), perhaps due to small sub-sample sizes and low statistical power. As an alternative way of assessing the interaction between IDR and field IDR, we retain the full set of observations, and interact the continuous field-level IDR measure with the individual IDR measure. Results from models at the person year level (see Table 3) reveal that both field-level IDR and the interaction term have positive and significant effects on productivity (Model 3) and visibility (Model 6). 9 In other words, field-level IDR bolsters IDR's positive effect on citations, and lessens its negative effect on productivity (in fact, the relatively flat line in Figure 1 suggests that scholars in high IDR fields do not 9 Given our interest in field-level effects, and because field-level IDR is time-invariant, fixed effects models cannot be estimated. Instead, we use robust standard errors and a set of controls to account for individual-level differences in propensity to conduct IDR research. face a productivity penalty). High IDR fields are more receptive to IDR work, and thereby invoke fewer penalties for producing this type of work (perhaps they provide better training in how to manage the cognitive challenges, and/or are more amenable to IDR in the peer review process). These effects are depicted in Figures 1 and 2, in which results from widely held perceptions and some previous empirical research (Rhoten and Pfirman 2007), we find that women are not more likely than men to engage in IDR. We also find support for Phillips and Zuckerman's (2001) middle status conformity finding. Using the number of National Academy of Science (NAS) members at one's institution as a measure of status, we find the expected inverted U-shape relationship between status and IDR: as Model 1 (Table 2) shows, the main effect of status is negative, and the squared term is positive. This suggests that scientists at both low and high status universities engage in IDR; scientists at middle-status universities are in a precarious position (where \"the prospect of classification as a full-fledged player and the threat of delegitimation both loom large\" (Phillips and Zuckerman 2001: 384)) and thus opt to conform to a disciplinary tradition. At the journal and field level, we controlled for variables that likely impact productivity and/or visibilityincluding field size, average citations per paper in the field, average turn-around time at journals in the fieldas well as lagged productivity (when modeling productivity as well as visibility) and lagged visibility (when modeling visibility as well as productivity). While they all have intuitive and often significant effects, they do not alter (or render insignificant) the main findings."}, {"section_title": "Drivers of the Productivity Penalty", "text": "Cognitive and Collaborative Challenges. We theorize that IDR projects typically experience a steep learning curve as well as communication challenges with diverse collaborators. Supplemental data sources and analyses lead us to conclude that IDR projects indeed face these communicative and collaborate hurdles. First, we examine the interaction between repeat collaboration and IDR to see whether collaborators learn to work together better over time. Indeed, working repeatedly with a similar set of collaborators reduces the productivity penalty (b=1.51*, se=.077). Second, we capitalize on survey data that we collected from a subset of scholars in the archival data. Scholars were asked about the nature of the collaboration on their most recent co-authored paper, and we match these responses to the IDR scores for these papers (n=68). The IDR scores of papers by these scholars are not significantly different from the rest of our sample. Despite the small sub-sample size, a series of t-tests reveal marginally significant challenges associated with the production of IDR. Challenging Review Process. Another possibility is that IDR work is penalized by reviewers and editors in the review process, but we find little support for this explanation. In order to examine this possibility, we collected data on the length of review process (i.e., turn-around time) for two journals represented in our data: one publishing articles with an above-average IDR score (0.64), and another publishing articles with a below-average IDR score (0.59). For the 711 articles published in these journals (written by 145 of our sampled authors), the median time under review is 85 days. However, there is no significant correlation between time under review and the paper's IDR score, and the IDR score does not predict time under review. Confidence in this finding is buttressed by an analysis of data on unpublished working papers that we collected from www.ArXiv.org. We searched for working papers by the 854 authors in our sample, and were able to locate 220 papers written by 63 authors (see Table A4). Stripping all references from these working papers, identifying each referenced journal, and matching the journals with WoS SCs allowed us to calculate IDR scores for each working paper. Comparisons of the 220 working papers with the 3983 published papers by the same authors reveal that working papers are, indeed, more interdisciplinary, and this may make them more likely to be file-drawered or rejected, thereby contributing to the productivity penalty. But because working papers are more recent (average posting date in ArXiv.org is 2009, compared to an average publication date of 1997 for the published papers), and may be more interdisciplinary simply given the upward trend in IDR (documented by Porter and Rafols 2012, and confirmed with our data in Figure 3), it is difficult to interpret this difference. As a more stringent test, we took a closer look at our sample of working papers. For each working paper, we identified which papers were subsequently published (using author updates on www.ArXiv.org and online searches for each paper). We compared working papers that eventually got published with those that did not. Here, the eventually published papers (n=122) are actually more interdisciplinary than the still unpublished papers (n=115), a difference significant at p=.054 (two-tailed), suggesting that IDR papers are not hindered in the review process. ~ Table A4 and Figure 3 about here ~ Taken together, these supplementary data and analyses support our theorizing about cognitive and collaborative challenges associated with IDR research. The second stage of productionthe review stage may not be such a roadblock; we do not find IDR work more likely to be file-drawered or rejected. Rather, the first stage of production, where authors plan, coordinate, and conduct their research is the largest hurdle for interdisciplinary work and the main source of the production penalty. Although we acknowledge that these are not causal tests but associations, we subscribe to the view that documenting the role of a mechanism (or two) empirically strengthens claims of causal connections (Gross 2009;Reskin 2003)."}, {"section_title": "Robustness Checks and Selection Issues", "text": "Sample selection concerns. Additional analyses alleviate concerns that center affiliation is driving the results we report. It is true that our sample of academic scientists (described in Table 1), while large and heterogeneous in terms of represented fields, includes only scientists affiliated with at least one research center: the IUCRCs we study. But we can leverage our longitudinal data, which includes papers written by these scientists before they joined a center. Even after restricting the sample to papers published 5 years before, and 5 years after center founding and estimating fixed-effects models (Table   A3, Models 1 and 2), the main effects of IDR (positive on visibility; negative on productivity) remain significant. Although descriptively we find that scientists are slightly more interdisciplinary in their publications after center formation, this could be attributed to the trend toward IDR (Figure 3). The interaction between IDR and center affiliation is not significant, suggesting that IDR work does not become more visible after joining a center (Model 1) and IDR work does not become less difficult (Model 2). This analysis suggests that center affiliation does not increase the benefits or reduce the cost of conducting IDR. ~ Table A3 about here ~ Our center-based sample is distinctive in some ways, but not in ways that alter the results we report. Compared to their counterparts unaffiliated with centers, university research center scientists tend to be more experienced (Bozeman et al. 2001) and productive (Biancani et al. 2014), and this may be particularly true when scholars are connected to industry (Carayol and Thi 2005) via IUCRCs. Indeed, we find that compared to the broader population of scientists in related fields (computer science, math, life sciences, physical sciences, social sciences, and engineering) represented in NSF's surveys of PhD-level scientists 10 , our sample of scientists is older, more predominantly male, and more productive (see Appendix Table A2). Concerns about the older age of our sample are alleviated by examining subsets of our data. When we restrict our sample to youngest 20% of our sample (not more than 13 years post-PhD) in the person-level analysis, and when we restrict our sample to the first five years of a scientist's career in the paper-level analysis, our results hold. An interaction between IDR and age is negative and significant (b= -0.012***) suggesting that, if anything, the penalty is stronger for older scientists (but the size of the coefficient is very small). Concerns about the preponderance of men in our sample are alleviated when interactions between female and IDR fail to reach statistical significance, suggesting that results hold for both men and women. Concerns about our especially productive sample of scientists are alleviated when we find evidence of a productive penalty in other data sources: interdisciplinary scholars (measured as those who have received their PhD in one field but work in another) report significantly fewer publications than scholars whose fields of training and work correspond in The National Survey of Postsecondary Faculty (2003). Moreover, Biancani et al. (2014) find that when Stanford faculty take a joint appointment (one measure of interdisciplinary engagement), their productivity declines. ~ Table A2 about here ~ And in other ways, our sample is representative of scientists, and scientific papers more broadly. Indeed, center affiliation is relatively common: almost one-third (32%) of faculty at extensive (R1) research universities nationwide are affiliated with a research center (Boardman and Corley 2008). And even if center-based scholars are more interdisciplinary (Ponomariov and Boardman 2010) and, as documented above, more productive, this would compress the 'true' productivity penalty, and make our test conservative. Although we cannot calculate our measure of IDR for PhD-level scientists in NSF's survey data, we can compare our sample of papers to the population of papers in Web of Science analyzed by Uzzi et al. (2013). Uzzi generously shared his measures for our sample of papers, so we can make direct comparisons in terms of both conventionality (a paper's tendency to reference journals that are commonly referenced together) and novelty (a paper's tendency to reference unusual journal pairings). The second panel of Table A2 shows that in the 1980s and 1990s, our papers are almost identical to the population of Web of Science papers in terms of conventionality. And, importantly, Table   A2 also shows that our sample of papers exhibits slightly less novelty than the population, allaying concerns that center-based scholars are innovative superstars. 11 This comparison demonstrates that they are not exceptional, and our results are likely to apply to academic science more broadly. That said, and despite our additional efforts, we acknowledge that with our research design, we cannot completely rule out that there is something distinctive about center-based scientists. Quality of IDR papers. We also find evidence suggesting that interdisciplinary papers are not distinctive in terms of their inherent quality. If IDR papers are lower in quality compared to other papers, this could explain the productivity penalty we document. If IDR papers are higher quality papers, this could explain the visibility benefit we document. Additional analyses allow us to rule out these possibilities. We control for potential quality differences at the individual-, journal-, and field-level. Recall that we specify models with person fixed effects that control for unmeasured attributes, so we can be certain that the effects we document are not attributable to individual differences in aptitude or in propensity to engage in IDR. In other words, low quality scholars are not attracted to IDR (which could explain the productivity penalty), nor are high quality scholars (which could explain the reception benefit). When we examine field-level moderators, person fixed-effects are not possible (as in Table 3, Models 3 and 6), so we control for the quality of scientists' PhD institutions and their current institution. With respect to the quality of the journal, we do see a positive correlation between IDR and journal impact (r=0.25 in Table 1). But the positive effect of IDR on citations holds even when we control for journal impact (Table 2, Model 1), when we examine journal fixed-effects (results not shown), and when we restrict our analysis to the subset of low-impact journals --those whose impact factor falls in the lowest quartile. In other words, high IDR papers in low-impact journals are more highly cited than other papers in those journals. Moreover, we found no difference in turn-around times between papers published in a high IDR journal and in a low IDR journal (see above). Thus, we doubt that IDR papers are inherently higher quality (as then they would likely move more quickly through the review process); we also doubt that they are lower quality (as then they would likely spend more time in development in the review process or be more likely to be rejected). These robustness checks are consistent with our argument that IDR, and not some unobserved heterogeneity, increases visibility and reduces productivity."}, {"section_title": "DISCUSSION", "text": "The main contribution of this paper is to empirically investigate the potential costs, as well as the widely touted benefits, of interdisciplinary scholarship. To do this, we collected and collated data from various sources for a sample of over 850 center-based scientists and their 32,000 publications, to provide the first systematic and mid-scale assessment of IDR's impact. Our results demonstrate that indeed, IDR benefits scientists: it improves their visibility in the scientific community, as indicated by cumulative citation counts. But we also document a productivity penalty associated with IDR: it depresses the number of articles that scientists publish. We see these effects in analyses of papers (i.e., high IDR papers are more highly cited), in yearly analysis (e.g., scientists publishing high IDR papers publish less in that year), and at the person-level (e.g., scientists who publish IDR have higher citations and higher variation in their citations). And when compared, the productivity penalty for IDR (standardized coefficient for direct effect = -0.06) outweighs the reception benefit (standardized coefficient for direct effect = +0.04) of engaging in this research. In other words, engaging in interdisciplinary research depresses productivity more than it increases citations. Specifically, compared to a scholar in the 20 th percentile of IDR, a scholar in the 80th percentile of IDR produces 2 fewer articles (24, rather than 26) and garners almost 50 more citations (297, rather than 253). 12 The productivity penalty is strong enough to make the total effect of IDR on citationsi.e., the direct effects reported above, plus the indirect effect of IDR on citations via productivityslightly negative (standardized coefficient for total effect = -0.28). Apparently the learning curve is steep: it takes more time, effort, diligence, and perhaps coordination to master (at least aspects of) different fields and to work with scientists trained in disparate disciplines. That said, we cannot assess whether this trade-off is harmful or beneficial for a scientist (do 50 more citations offset 2 fewer publications?). We can say that the greater visibility that this work receives is accompanied by fewer publications published. We also make several theoretical contributions to the literature. First, we re-orient away from reception penalties (which dominate the categories literature) toward production penalties, and explore a new form of production penalty. The few recent papers that examine production penalties focus on reduced quality (Kov\u00e1cs and Johnson in press), documenting through blind taste tests (i.e., controlling for audience perception), for example, that category-spanning wines are rated more negatively --presumably because of skill deficiencies on the part of the winemakers (Negro and Leung 2013). In contrast, we focus on another form of production penalty that besets category-spanning work, at least in in the realm of science: reduced productivity. Supplementary data on unpublished working papers, journal turn-around times, and a survey of authors' experiences, in addition to supplementary models incorporating individual fixed effects, suggest that productivity is hampered by communication and coordination hurdles faced in the research process, and not by the review process. This needs to be reconciled with the implicit production benefits found in the innovation and diversity literatures: better decisions (Beckman and Haunschild 2002), heightened creativity (Pelled, Eisenhardt, and Xin 1999), and organizational centrality (Powell, Koput, and Smith-Doerr 1996). Our work suggests a potential short-run (fewer papers) and longrun (higher visibility) tradeoff. Second, we push research forward by demonstrating that production penalties and reception benefits depend on characteristics of the field. The production penalty (fewer papers) is more severe in a low IDR field (like electrical engineering) than in a high IDR field (like life sciences). This suggests that the penalties of doing IDR work are not the same across fields: the cognitive and collaborative challenges can be reduced through training, or peer review processes that are amenable to IDR papers may help develop that work. More broadly, the effect of category-spanning depends on the typicality of categoryspanning in the field (see also Lo and Kennedy, in press). For example, it may be that these high IDR fields are shaped by high status actors who reap the benefit of category-spanning and normalize it for those who follow (Rao, Monin, and Durand, 2005;Sgourev and Althuizen, 2014). Third, we move beyond mere category-spanning to take into consideration the relationship between the spanned categories. Certainly, spanning two dissimilar entities is qualitatively different from spanning two similar entities, but extant research on both category-spanning (Hsu, Hannan, and Kocak 2009;Negro, Hannan, and Rao 2010) and recombinant innovation (Fleming, Mingo, and Chen 2007) largely ignores this [but see Leahey &Moody 2014, andJohnson 2014]. In contrast, the measure of IDR we use is sensitive to such differences, allowing connections between two unrelated disciplines to contribute more to the IDR score than connections between two related disciplines. The distinction is crucial and allows us to distinguish between the effects of variety (i.e., branching out into a number of other fields) and distance (i.e., branching out into cognitively distant and unrelated fields). The distance measure we use is consistent with the logic of diversity and brokeragewhere networks with structural holes are presumed to contain more distant, non-redundant knowledgebut here we actually measure distance in knowledge space. When we measure IDR as sheer variety (neglecting distance between fields), we find that the visibility benefit holds (papers that span dissimilar and even similar subfields receive more citations), but the productivity penalty does not (suggesting that the cognitive challenges and coordination costs associated with producing IDR are largely a function of the cognitive distance among fields). This is also the case when we calculate a measure of multidisciplinarity (which accounts for distance between fields represented in a scholar's oeuvre, but not between fields represented in a given paper). Taken together, these results suggest that efforts to integrate, in a single paper, the cognitive distance across disparate fields contributes to the reception benefit (i.e., greater citations) and drives the productivity penalty (i.e., fewer publications). Investigating academic science also allows us to extend Pontikes' (2012) insight about the nature of the audience to reconcile an even broader array of literatures than she recognizes. Like Pontikes suggests (2012:111), scientists are likely 'market-makers,' eager to identify and develop innovative, game-changing ideas, who are thus drawn to (rather than repelled by) multi-category offerings like IDR. And this, of course, is what we find: interdisciplinary scientists do not experience the reception side penalties (e.g., devalued, overlooked) that the category-spanning literature has widely documented among 'market-takers.' Rather, as the innovation literature suggests (without explicitly referring to it as such), scientists derive a reception-side benefit: greater visibility. However, we also confirm that IDR is a highrisk, high-reward proposition, as indicated by both more overall citations and higher variation of citations; we confirm that innovative ideas have longer tails (Singh and Fleming, 2007). Thus, Pontikes' focus on the nature of the audience places an important scope condition on not just the category-spanning literature, but also the innovation and brokerage literaturewhich has implicitly focused on marketmakers and reception-side benefits. Given that the nature of the audience matters, it would behoove scholars of both category-spanning and innovation to identify more explicitly whether the actors under study are market-takers or market-makers, and to consider how this shapes both reception benefits (such as visibility) and risks (such as variance). Because this is the first mid-scale and empirical assessment of IDR's effects on scientists' careers, extensions will be fruitful. Most illuminating might be a closer examination of the audience. The audience does not just cite and thereby contribute to a cumulative citation count. Audience members themselves come from disciplinary (or interdisciplinary) homes. In this paper we focused on the distribution of (and distance between) cited disciplines: those represented in scholars' bibliographies. But a more qualitative examination of the cited disciplines themselves, and a comparison of such referenced disciplines with the citing disciplines (that is, the disciplinary homes of scientists who reference a given paper), would allow us to assess whether certain fields (or combinations of fields) have broader appeal than others. In concurrent analyses, we find that papers with high IDR scores are likely to be cited by papers that themselves are interdisciplinary. This preliminary analysis suggests that breadth breeds further breadth, but further exploration is warranted. We also encourage extensions to broader samples and the utilization of alternative research designs. Although we studied 850 scientists from a wide range of fields and university settings, they are all affiliated with university research centers that foster connections with industry. Our analyses of potential selection biases and endogeneity concerns suggest that center affiliation does not drive the results we report here. However, a prospective research design that follows a large sample or population of academic scientists through their careers (as they move in and out of center affiliations and other interdisciplinary ventures) would complement our search for mechanisms and evidence consistent with our theorizing. Efforts underway to link NSF's Survey of Doctorate Recipients (SDR) to both Web of Science records and the NBER Patent database would be ideal in this regard. Practically, should university research administrators and federal agencies like the National Academies of Science and the National Science Foundation continue to invest in IDR? Given that IDR scholars produce useful and noteworthy research that has more impact on the scientific community, the enthusiasm for IDR is not premature. However, our analysis suggests that it is not attuned to the implicationsespecially the negative implicationsthat IDR has for individuals. Scholars who produce IDR work may be more likely to publish in top tier journals (the correlation between IDR and journal impact factor is 0.25), but their overall productivity is hampered. There is a clear production penalty associated with IDR work. Even though our analysis of working papers suggests that IDR does not hamper (and indeed seems to help) subsequent publication, additional analysis on a larger sample is warranted. The penalties of IDR may be more far-reaching than we document here. To this point, we encourage more direct examination of IDR's impact on scientific careers. We do not examine likelihood of receiving tenure, so we cannot assess whether the reduced productivity that IDR induces is offset by increased citations. This would be an important and useful extension, especially as universities re-organize to train scientists to be interdisciplinary. It is important to understand whether IDR enhances or damages career prospects for those starting out their careers. Certainly it appears that interdisciplinary scholars bear additional burdens: they struggle to master multiple domains of knowledge, to integrate them in a single work, and to coordinate with coauthors from different backgrounds. But these projects also appear to be received well by fellow scientists. More focused examination of the career outcomes of individual scientists is still needed. In addition, future research should examine forms of productivity other than publishing (e.g., patenting, consulting, advising) in order to examine whether scientists are compensating for their fewer publications with other activities. Given the continued interest and enthusiasm for interdisciplinarity (National Research Council 2014), it is important that the costs and benefits of this type of research be evaluated empirically. We have attempted a step in that direction. Even if the individual level costs of such work are substantial, the societal level benefitsin the form of more useful and valuable scienceseem clear. This suggests that scientists and the scientific community need to reassess how to evaluate scholarship if the scientists are to be encouraged to continue to engage in interdisciplinary research.    68 197968 198068 198168 198268 198368 198468 198568 198668 198768 198868 198968 199068 199168 199268 199368 199468 199568 199668 199768 199868 199968 200068 200168 200268 200368 200468 2005 Year Note: The data used to produce this figure are described in detail in the text. See page 10-11 for a description of the sample of scholars and their ISI Web of Science publications, and page 12-13 for a description of the IDR measure (Porter et al. 2007).  ideal=11.0 1.0 1.0 1.0 1.0 1-RMSEA (ideal=1) 1.0 1.0 1.0 1.0 1.0 * p<0.10 ** p<.05 *** p<.01 (two-tailed tests) a For this model we use the number of Subject Categories instead of IDR as the IV. b For this model we use the standard deviation of journal impact factor rather than its mean in the visibility equation. 49.61 * p<0.10 ** p<.05 *** p<.01 (two-tailed tests) a All models include multiple imputation and exclude observations with IDR=0. b Model 1 is Negative Binomial Regression at the person-year level with person-fixed effects. c Models 2-3 are Population-Averaged Negative Binomial Regressions at the person-year level with robust standard errors. Models do not converge with lagged cumulative productivity. d Model 4 is GLS regression at the paper-level with person and year fixed effects and robust standard errors. e Models 5-6 are Random Effects GLS Regressions at the person-year level with year fixed effects and robust standard errors. Porter's measure of Integration, above, is used to capture the extent to which a paper is interdisciplinary. This measure incorporates the variety (i.e., number) of disciplines, their balance (i.e., the evenness of the distribution), and --uniquely --their similarity (i.e., their cognitive distance) into one index. Disciplines are proxied by the 244 WoS Subject Categories (SCs; examples include Sociology, Management, and Chemical Engineering). In this example we assume there are only 4 WoS SCs, and that each paper has only 5 references. As input, we pool all SCs from the focal paper's set of references to get variety (captured by the number of SCs referenced) and balance (captured by Pi, the proportion of references falling in SCi). We then incorporate the simlarity scores for each SC-SC combination (Sij, from a SCxSC co-citation matrix which we convert to cosines and normalize) which are derived from the population of all WoS-indexed articles and thus shared by all focal papers; we highlight it in grey below. A paper's IDR score increases as it references more, relatively unrelated SCs (Porter, Cohen, Roessner, and Perreault 2007:277). The IDR score ranges from 0 to 1, with scores closer to 1 indicating greater interdisciplinarity. Ref 5 Ref 5 Ref 5 This paper references all 4 SCs, two of which are distant (SC2 and SC4, Sij=0.0001), and thus has a high IDR score: This paper references 2 highly related SCs, (SC2 and SC3, Sij=0.35, which is one of the highest cosines in our data), and thus has a low IDR score: This paper references 2 distant SCs (SC3 and SC4, Sij=0.0011), so its IDR score is higher than Article 2's IDR score: a Visibility is modeled at the paper level and productivity at the person-year level with person fixed-effects. Publications included 5 years prior and post center formation. *** p<.01; ** p<.05; * p<.10 (two-tailed tests)."}]