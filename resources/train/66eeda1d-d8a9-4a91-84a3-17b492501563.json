[{"section_title": "Abstract", "text": "Brain surface analysis is essential to neuroscience, however, the complex geometry of the brain cortex hinders computational methods for this task. The difficulty arises from a discrepancy between 3D imaging data, which is represented in Euclidean space, and the non-Euclidean geometry of the highlyconvoluted brain surface. Recent advances in machine learning have enabled the use of neural networks for non-Euclidean spaces. These facilitate the learning of surface data, yet pooling strategies often remain constrained to a single fixed-graph. This paper proposes a new learnable graph pooling method for processing multiple surface-valued data to output subject-based information. The proposed method innovates by learning an intrinsic aggregation of graph nodes based on graph spectral embedding. We illustrate the advantages of our approach with in-depth experiments on two large-scale benchmark datasets. The flexibility of the pooling strategy is evaluated on four different prediction tasks, namely, subject-sex classification, regression of cortical region sizes, classification of Alzheimer's disease stages, and brain age regression. Our experiments demonstrate the superiority of our learnable pooling approach compared to other pooling techniques for graph convolution networks, with results improving the state-of-the-art in brain surface analysis.\nAll authors are with ETS Montreal, Canada. Corresponding author: K. Gopinath."}, {"section_title": "I. INTRODUCTION", "text": "Brain surface analysis plays a crucial role in understanding the mechanisms of perception and cognition in humans [1] . However, the complex geometry of the brain surface, comprised of intricate folding patterns, poses considerable challenges in neuroscience. Notably, brain imaging data, for instance acquired by magnetic resonance imaging, typically comes in 3D, a Euclidean space, while its analysis often focuses on the thin surface of the brain, a non-Euclidean space. This fundamental difference between the domains of acquisition and analysis, coupled with the geometrical complexity of brain surfaces, severely hinders computational approaches for brain surface analysis. As an illustration, neighboring 3D voxels in a neuroimage may in fact represent points that are far apart on the brain surface, as shown on Fig. 1 . To alleviate this problem, popular surface-based methods [2] , [3] often simplify the geometry of the brain, for instance, by mapping the surface to a sphere. This process is, however, computationally expensive. For example, the widely-used surface analysis pipeline of FreeSurfer [2] requires several hours to inflate the cortical surface to a sphere, match it to an atlas and finally perform a cortical analysis. The geometry of brain surfaces similarly complicates other conventional approaches for brain analysis, such as those based on diffeomorphic transformations [4] or on spherical harmonics [5] . Fig. 1 . Complex geometry of the cerebral cortex. As illustrated, two nearby points in the volume may in fact be far apart on the cortical surface."}, {"section_title": "A B", "text": "A key application of brain surface analysis is detecting and tracking the progress of neurodegenerative disorders, such as Alzheimer's disease, which often result in a severe atrophy of brain tissues. Analyzing the geometrical changes of the brain can thus aid in the early diagnosis of such conditions. Initial work has focused on Euclidean 3D data, based for instance on the texture of magnetic resonance images [6] , [7] , in order to differentiate Alzheimer's disease from normal aging. While volumetric approaches has shown relevance in detecting global changes in a Euclidean space [1] , surfacebased methods [2] , [3] , [4] , [5] are more adequate for analyzing data on brain surfaces. For instance, the analysis of shape abnormalities on brain surfaces has improved the prediction of Alzheimer's disease [8] or the identification of stages in the disease [9] . Nevertheless, all these studies has focused on preestablished measurements of brain surface information. This paper proposes to learn and exploit the organizational structure of surface data in order to improve the prediction tasks that use data on highly complex surfaces."}, {"section_title": "A. Related work", "text": "Current machine learning approaches have achieved stateof-the-art performance in a broad range of computer vision and medical imaging applications. In particular, deep learning architectures, such as convolutional neural networks (CNNs) [10] , offer higher accuracy and speed over traditional approaches for image analysis. In neuroimaging, CNNs are now widely used for various segmentation [11] and classification [12] problems, with architectures tailored for the target task and the available imaging data. For example, various architectures have been proposed to exploit volumetric data [13] , [14] , [15] , [16] . A fundamental limitation of these models, however, is their restriction to data lying on a fixed Euclidean grid representing pixels or voxels. This restricted representation induces ambiguity when exploiting complex geometries, such as in brain surfaces, impeding the application of these Euclidean models for brain surface analysis.\nGeometric deep learning [17] generalizes deep learning models to operate on non-Euclidean domains such as graphs and manifolds. Recent advances in this field, in particular, in graph convolution networks (GCNs), have enabled convolution operations over graphs by exploiting spectral analysis, where convolutions translate into multiplications in a Fourier space [18] , [19] , [20] , [21] . In such models, convolutions are manipulated with eigenfunctions of graph Laplacian operators [22] , which can be approximated with Chebyshev [20] or Cayley polynomials [23] . These learned convolution filters are expressed in terms of mixtures of Gaussians [21] or splines [24] . Despite their advantages over standard CNNs, these models are, however, limited to a fixed graph structure and thereby not suitable for brain imaging applications involving a population of subjects. Indeed, brain surfaces have varying geometries with a different number of nodes and a distinct connectivity across meshes. This variability poses computational challenges, for example, arising from the fact that the values of a Laplacian eigenfunction can drastically differ between brains with different surface geometries [25] . To this effect, a learned synchronization can correct for differences in eigenfunctions [26] . An alignment of eigenbases [27] similarly provides a common parameterization of brain surfaces. Such aligned eigenbases enabled the direct learning of surface data across multiple brain geometries [28] . Nevertheless, these types of GCNs are limited to a fixed graph structure, for instance, with the same number of nodes. Standard pooling strategies rely in fact on such consistency of graph structures. Currently, heuristics are often used to mimic a max-pooling strategy in GCNs [18] , [20] , [29] . They include varying the number of feature dimensions across layers [18] while retaining fixed layer sizes, or relying on partition methods, for instance, based binary trees [20] or Graclus clustering [29] to coarsen the initial graph. However, these strategies are mainly used for point-wise operations in fixed-size graphs [21] , such as node classification [30] , and do not apply to the task of subject classification when the geometry varies across subjects.\nA few recent studies [31] , [32] have attempted to tackle the problem of graph classification in GCNs by incorporating adaptive pooling modules in the network. For instance, [31] performs a hierarchical clustering of nodes using their spectral coordinates, with a subsequent pooling of node features within each cluster. While this approach handles varying graph structures, clusters are defined only on node proximity in the embedding space, without considering its values. Consequently, this unsupervised pooling strategy may not be optimal for the classification or regression task at hand. More recently, a differential pooling technique [32] splits the network in two separate paths, one for computing latent features for each node of the input graph and another for predicting the node clusters by which features are aggregated. This approach ignores, however, the intrinsic localization of nodes within the graph, which is sought when the geometry is highly curved, such as, in particular, brain surfaces."}, {"section_title": "B. Contributions", "text": "This paper proposes a novel method based on GCNs for classification and regression of surface graphs. Our method includes a learnable pooling strategy which predicts optimal node clusters for each input graph, and thus can handle graphs with varying number of nodes or connectivity. This adaptive pooling technique is applied recursively to obtain a fixedsize representation, which is then used for predicting a target classification or regression value. Our method also leverages spectral embedding techniques for surface graphs [27] , offering a more powerful representation of complex surfaces like the brain cortex. This contrasts with the differential pooling approach in [32] , where nodes lack intrinsic localization within the graph.\nWe illustrate our approach on the challenging tasks of brain surface classification and regression using the well-known Mindboggle and ADNI datasets. We first compare our learnable pooling strategy to other pooling techniques for GCNs, and study the effect of input graph size (i.e., surface mesh resolution) on performance, by considering the problem of subject sex classification 1 . The ability of our pooling strategy to learn important node clusters in a supervised manner is highlighted by the relationship between these clusters and prominent anatomical regions. To further validate the regions learned by our network, we use it to predict the size of cortical regions as defined by a standard parcellation atlas. Our model is also tested on cortical surface data from the ADNI dataset to i) discriminate between control subjects and subjects suffering from different stages of Alzheimer's, and ii) regress the brain age of subjects. We choose the largest dataset ADNI [33] as it provides manual labels for subject brain age and three stages of Alzheimer's disease. Only using simple cortical measurements such as thickness and sulcul depth, our method achieves a similar performance to the state-of-the-art on the ADNI dataset [33] .\nIn summary, the major contributions of our work are as follows: -A general model for classifying and regressing graphs with varying geometry, which combines a learnable, supervised pooling strategy with the intrinsic (non-Euclidean) localization of nodes via graph spectral embedding.\n-A first fully-learned model for brain surface analysis contrasting with previous approaches based on predefined cortical features;\n-An in-depth experimental evaluation on two large-scale benchmark datasets (i.e., Mindboggle and ADNI) and four different prediction tasks (i.e., subject sex classification, cortical region size regression, Alzheimer's disease classification, and brain age regression);\n-State-of-the-art performance for ADNI stages classification and brain age prediction using cortical surface data. This paper represents a significant extension of our previous work in [34] . Specifically, we test our method on another"}, {"section_title": "\u21e52", "text": "Disease / age prediction Convolution + pooling block 1 Convolution + pooling block 2 multi-site dataset (i.e., Mindboggle) and explore two additional prediction tasks (i.e., subject-sex classification and cortical region size regression). Results of these new experiments highlight the relationship between learned clusters for these tasks and known cortical regions. This extended study also compares our model against graph pooling techniques relying on unsupervised spectral clustering [31] and differentiable pooling approaches in Euclidean space [32] , showing significant advantages compared to these techniques. Last, additional experiments are proposed to show the robustness of our method to surface mesh variability in terms of number of nodes and connectivity."}, {"section_title": "II. METHOD", "text": "We first describe a general formulation that extends standard convolutions to non-rigid geometries such as surfaces. We then detail our strategy based on graph spectral embeddding to model the intrisic localization of mesh nodes and align them across multiple surfaces. Subsequently, we present our end-toend learnable pooling strategy for the adaptive clustering of graph nodes. Finally, we provide detailed information on the overall network architecture and training procedure."}, {"section_title": "A. Convolutions on non-rigid geometries", "text": "In a standard CNN, the input is typically provided as a set of features observed over a regular grid of points like 2D pixels or 3D voxels. This information is then processed using a sequence of layers composed of a convolution operation followed by a non-linear activation function like the ReLU. Let Y (l) \u2208 R N \u00d7M l be the input feature map at convolution layer l, such that y (l) iq is the q-th feature of the i-th input node. The feature map consists of N input nodes with M l dimensions each. Assuming a 1D grid for simplicity, the output of layer l obtained by a convolution kernel of size K l is given by y\nHere, w For a general surface, points are not necessarily defined on a regular grid and can lie anywhere in a 3D Euclidean space. Such surface can conveniently be represented as a mesh graph G = {V, E} where V is the set of nodes corresponding to points and E is the set of edges between the graph nodes. Given a node i \u2208 V, we denote as N i = {j | (i, j) \u2208 E} the set of nodes connected to i, called neighbors. We extend the concept of convolution to arbitrary graphs using the more general definition of geometric convolution [21] , [28] , [24] :\nIn this extended formulation, \u03d5 ij is a symmetric kernel parameterized by \u03b8 k , which encodes the relative position of neighbor nodes j to a node i when computing the convolution at node i. For instance, \u03d5 ij can be defined as a Gaussian kernel with learnable parameters \u03b8 k = {\u00b5 k , \u03a3 k } on the local polar coordinate u ij = (\u03c6 ij , \u03b8 ij ) from node i to j [21] :\nThe relationship between conventional and geometrical convolutions is illustrated in Fig. 3 . Standard convolutions (left) can in fact be seen as a special case of geometric convolutions (right) where nodes are placed on a regular grid and kernels are unit impulses (i.e., spherical Gaussian kernels with zero variance) placed at the grid position of neighbor nodes."}, {"section_title": "B. Spectral embedding of multiple surface graphs", "text": "A significant limitation of the above geometric convolution model is its inability to process differently-aligned surfaces. Thus, since local coordinates u ij are determined using a fixed coordinate system, any rotation or scaling of the surface mesh will produce a different response for a given set of kernels. Moreover, as shown in Fig. 1 , geometric convolutions in Euclidean space are poorly-suited for complex surfaces like the highly-convoluted brain cortex.\nWe address these issues using a graph spectral embedding approach. Specifically, we map a surface graph G to a low-dimensional subspace using the eigencomponents of its the weighted adjacency matrix and D is the diagonal degree matrix with d ii = j a ij . Although binary adjacency values could be used in A, we instead define the weight between two adjacent nodes as the inverse of their Euclidean distance:\nwhere is a small constant to avoid a zero-division. Denoting as U\u039bU the eigendecomposition of L, where \u039b is the diagonal matrix of real, non-negative eigenvalues, we then compute the normalized spectral coordinates of nodes as the rows of matrix U = U\u039b \u2212 1 2 . Here, normalized components are scaled proportionally to the inverse of their eigenvalues since components with smaller eigenvalues encode more relevant characteristics of the embedded graph [35] . Based on the same principle and as in [36] , we limit the decomposition to the d = 3 first smallest non-zero eigenvalues of L. This allows capturing the important variability of surfaces, while also limiting computationally complexity.\nBecause the spectral embedding of L is only defined up to an orthogonal transformation (i.e., rotation or flip), we must align the spectral projection of different surface graphs to a common reference U ref .\nToward this goal, we use an iterative closest point (ICP) method [27] where each node i \u2208 V is mapped to its nearest reference node \u03c0(i) \u2208 V ref in the embedding space. Denoting as u i the normalized spectral coordinates of node i, the alignment task can be expressed as\nLet U ref \u03c0 be the matrix whose i-th row is u ref \u03c0(i) . The transformation between corresponding nodes is approximated as\nWe use the aligned spectral embedding U = U R to define the local coordinates corresponding to an edge (i, j) \u2208 E: u ij = u j \u2212 u i . As illustrated in Fig. 3 (right) , and based on Eq. (2), the convolution at node i therefore considers kernel responses \u03d5 ij (\u03b8 (l) k ) for neighbor nodes j, relative to the spectral coordinates of i."}, {"section_title": "C. Learnable pooling for graph convolution networks", "text": "Pooling in standard CNNs is typically carried out by aggregating values inside non-overlapping regions of features maps.\nIn graph convolution networks [18] , [19] , [20] , [21] , however, this approach is not applicable for the following reasons. First, nodes are not laid out on a regular grid, which prevents aggregation of features in pre-defined regions. Second, the density of points may spatially vary in the embedding space, hence regions of fixed size or shape are not suitable for graphs with different geometries. Last, and more importantly, input surface graphs may have a different number of nodes, while the output may have a fixed size. This is the case when predicting a fixed number of class probabilities from different brain geometries.\nWe propose an end-to-end learnable pooling strategy for the subject-specific aggregation of cortical features, inspired by the differential pooling technique of Ying et al. [32] . Our strategy, shown in Fig. 2 , splits the network in two separate paths: the first one computing latent features for each node of the input graph and the second predicting the node clusters by which the features are aggregated. The feature encoding path is similar to a conventional CNN, and produces a sequence of convolutional feature maps {Y (1) , . . . , Y (l) } with Y (l) \u2208 R N \u00d7M l . The clustering path consists of sequential convolutional blocks, however the activation function of the last block is replaced by a node-wise softmax. The output of this last block, S \u2208 [0, 1] N \u00d7C , gives for each node i the probability s ic that i belongs to cluster c. Pooled features Y pool \u2208 R C\u00d7M l are computed as the expected sum of convolutional features in each cluster, i.e.\nThe processing of aggregated node features, downstream the pooling operation, requires computing a new adjacency matrix A pool for the node clusters. Here, we define the adjacency weights between pooling clusters c and d as\nIntuitively, a pool cd is the expected number of connected nodes between clusters c and d.\nAs mentioned in [32] , the bilinear formulation of Eq. (6) faces a challenging optimization problem with several local minima. For instance, the same output Y pool in Eq (6) can be obtained by modifying either S or Y (l) . To alleviate this problem and obtain spatially-smooth clusters, we add a Laplacian regularization term to the loss function:\nwhere s i denotes the cluster probability vector of node i (i.e., the i-th row of S). This well-known regularization approach [37] penalizes connected nodes to be mapped to different clusters, with penalty proportional to the connection strength.\nD. Architecture details Figure 2 presents the overall architecture of our graph convolution network. As input, we give the network the cortical surface features x i and aligned spectral coordinates u i of each node i. For computing graph convolutions as in Eq. (2), we define the neighbors N i of node i as the k = 5 nodes nearest to i in the spectral embedding (i.e., the distance between node i and j corresponds to u i \u2212 u j 2 ) plus node i itself. While various features could be considered to model the local geometry of the cortical surface [2] , we considered sulcal depth and cortical thickness in this work, since the first one helps delineate anatomical brain regions [38] and the latter is related to ageing [39] and neurodegenerative diseases such as Alzheimer's [40] .\nThe network comprises two cascaded convolution-pooling blocks, followed by two fully-connected (FC) layers. The first block generates an N \u00d7 8 feature map and an N \u00d7 16 cluster assignment matrix, in two separate paths, and combines them using the pooling formulation of Eq. (6) to obtain a pooled feature map of 16\u00d78. In the second block, pooled features are used to produce a 16 \u00d7 16 map of features, pooled in a single cluster. Hence, the second pooling step acts as an attention module selecting the features of most relevant clusters. The resulting 1 \u00d7 16 representation is converted to a 1 \u00d7 8 vector using the first FC layer, and then to a 1 \u00d7 nb.outputs vector with the second FC layer.\nExcept for the cluster probabilities and network output, all layers employ the Leaky ReLU [41] as activation function: y\nip ). Moreover, for the graph convolution kernel \u03d5 ij of Eq. (2), we used the B-spline kernels proposed by Fey et al. [24] . Compared to Gaussian kernels [21] , this kernel has the advantage of making computation time independent from the kernel size.\nFor training, the loss function combines the output prediction loss and cluster regularization loss on the convolutionpooling block:\nwhere \u03b1 is a parameter controlling the amount of regularization. For classification tasks (i.e., disease prediction), L out is set as the cross-entropy between one-hot encoded ground-truth labels and output class probabilities. In the case of regression (i.e., brain age prediction), we use mean squared error (MSE) for this loss. Network parameters are optimized with stochastic gradient descent (SGD) using the Adam optimizer. Experiments were carried out on an i7 desktop computer with 16GB of RAM and a Nvidia Titan X GPU. The model takes less than a second for disease classification or age regression."}, {"section_title": "III. EXPERIMENTS AND RESULTS", "text": "We validate our method on two large-scale, publiclyavailable datasets: Mindboggle-101 [42] and ADNI1 [43] . The first one contains T1-weighted MRI from 101 healthy subjects (males: n=57, females: n=44, age: 20-61 years) collected from 9 different sites. We use this dataset for the tasks of subject-sex classification and cortical region size regression, since both subject-sex labels and manual for 32 cortical parcels are provided with imaging data. The ADNI1 dataset [43] is comprised of multi-sequence MRI data from 400 subjects diagnosed with mild cognitive impairment (MCI), 200 subjects with early Alzheimer's disease (AD) and 200 elderly control subjects, obtained from 55 participating sites. Both datasets contain brain surface meshes with pointwise cortical thickness and sulcal depth measurements, generated by FreeSurfer 2 . Cortical meshes in these datasets vary from 102K to 185K nodes.\nIn a first experiment, we compare the different pooling strategies for graph convolution networks and measure the impact of input graph size on the task of subject classification performance between different pooling methods. We then illustrate the network's ability to learn meaningful node clusters by predicting the size of cortical parcels from an anatomical atlas. Finally, we highlight the advantages of working in the spectral domain on the problems of disease classification (NC vs AD, MCI vs AD, and NC vs MCI) and brain age regression."}, {"section_title": "A. Comparison of different pooling methods", "text": "We compared our learnable pooling strategy against three other pooling techniques applicable to graph convolution networks: 1) taking the global average of feature maps, 2) pooling feature maps in fixed regions computed from a cortical parcel atlas, 3) pooling the same features in regions obtained by applying k-means clustering on the spectral embedding. For all tested methods, we used a network composed of two graph convolution layers followed by two fully-connected layers, as described in Section II-D. In the case of global average pooling and fixed parcellation pooling, a single pooling operation is applied after the second graph convolution. For spectral clustering pooling, nodes are grouped after each of the two convolution layers as in our learnable pooling. However, the pooling path of the network is replaced by a static node clustering. We train and test all methods on subject-sex classification Mindboggle dataset with 70-10-20 split for training, validation, and testing.\nTable III-A summarizes the results of this experiment. We see that global average pooling yields the poorest performance with a mean accuracy of 60.76%. Using atlas-defined cortical parcels to aggregate features improves accuracy slightly to 64.59%, suggesting that these parcels are informative of identifying subject sex. Moreover, applying unsupervised clustering on the spectral embedding further increases mean accuracy to a b c 67.94%, which indicates the benefits of having a hierarchy of non-fixed clusters. However, by learning clusters in a supervised manner from spectral embeddings, our method achieves the outstanding accuracy of 81.33%, an improvement of 13.39% over spectral clustering. Figure 4 gives examples of clusters for the different pooling strategies (except global average pooling which consider all nodes as part of a single cluster). While spectral clustering yields spatially-regular clusters, the distribution of these clusters is arbitrary and does not seem to match known parcels of the cortex (shown in Fig. 4b) .\nIn contrast, the clusters predicted by our pooling strategy are larger and better align with these known parcels."}, {"section_title": "B. Impact of input graph size on performance", "text": "The previous experiment considered detailed surface meshes comprised of 102K to 185K nodes, each with a fixed set of edges connecting nodes. In this second experiment, we investigate whether our method is robust to variability in the size of the surface mesh. Toward this goal, we use the same split of the Mindboggle dataset as in the first experiment, and randomly sub-sample the original mesh to 100, 1K, 5K, 10K, 25K, 50K, and 75K nodes. Because convolutions at each node use information from its k = 5 nearest neighbors, as described in Eq. (2), testing multiple sub-sampling with the same number of nodes also assesses the robustness of our model to variations in graph connectivity. We train our model on each of these reduced graph datasets to predict the sex of Mindboggle subjects.\nTable III-B gives the classification accuracy for different sizes of training graphs when testing on sub-sampled graphs of the same size or the original full-sized graph. The first case evaluates whether the same accuracy can be achieved with less information at the input of the network, whereas the second case tests if the convolution parameters learned by the network generalize to larger graphs. As expected, classification performance decreases when reducing the size of input graphs, both when testing on sub-sampled graphs and full-sized graphs. When testing on sub-sampled graphs, accuracy drops from 84.21% while training with full graphs to 55.02% for graphs with only 100 nodes. However, high accuracy of 81.33% can be achieved with training graphs of 50K nodes, about half the size of the original graphs. Furthermore, we see that our model trained with moderatelyreduced graphs can still perform well on full-sized ones. For instance, the model trained with graphs of 50K nodes achieves an accuracy of 78.94% when tested on original graphs with about twice this number of nodes. "}, {"section_title": "C. Task-specific pooling regions", "text": "In this section, we qualitatively and quantitatively evaluate the predicted clusters and feature maps learned by our network. Once more, we consider the task of classifying males vs. females subjects from the Mindboggle dataset with the architecture depicted in Fig. 2 . Figure 5 shows examples of features and clusters learned by our graph pooling model for a male and a female subject. The Fig. 5 . Feature maps and predicted clusters for the task of subject-sex classification: The first column shows examples of activation maps computed by the embedding path of our network for a female subject. The second column gives the average activation in each predicted cluster for the same subject and feature maps. Third and fourth columns depict the same information for a male subject."}, {"section_title": "Male Female", "text": "first and third columns give the distribution of four different activation maps learned by the network for the two subjects. The mean activation in each predicted cluster for the same subjects is illustrated in the second and fourth columns of the figure. We observe the diversity of depicted clusters, spawning different regions of the brain both on the cortex and around regions of the basal ganglia. Interestingly, several of the learned clusters focus on sub-cortical regions like the hippocampus (first row) and amygdala (last row) which have been linked to sex-related differences in the literature [44] . This illustrates the benefit of learning task-specific clusters in a supervised manner. Additionally, we see that predicted feature maps and clusters in both subjects are similar, demonstrating our model adapts to the specific brain geometry of individual subjects.\nWe further evaluate the relevance of learned clusters by training the same model to predict the size of 32 anatomical parcels of each brain surface, using labeled data from Mindboggle. This experiments hypothesize that the network should learn clusters which are related to the pre-defined parcels. To do so, we modify the last layer of the architecture in Fig. 2 to have 32 outputs, one for the size of each parcel, and change the loss function to mean square error. Adjusted mutual information (AMI) is used to measure the similarity between learned clusters and ground-truth parcels. AMI values range from 0 to 1, a score of 0 corresponding to random clusters and a score of 1 for clusters identical to ground-truth. Figure 7 gives the mean AMI obtained at each training epoch, and examples of predicted clusters at four different epochs are shown in Fig. 6 . In initial stages of training, the model predicts a small number of clusters corresponding mainly to the components of the spectral embedding (see the network input in Fig. 2) . In the first 500 epochs, the AMI score between predicted clusters and ground-truth parcels drops. Then, as training progresses, we observe increasing AMI values and progressively more defined clusters. At the end of training (2500 epochs), the model achieves an AMI score of 0.39. Obtained clusters appear to be a combination of different ground-truth parcels, suggesting that fully-connected layers further help regressing parcel sizes."}, {"section_title": "D. Disease classification", "text": "In the following experiment, we evaluate our method on the task of classifying subjects from the ADNI dataset as After training, the model finally learns to group multiple parcels (cyan) into on cluster pooling region. AMI score increases over epochs indicating task-dependent learning by our model. The last figure shows manual parcels with AMI score of 1 for reference. Fig. 7 . Evolution of AMI score: The adjusted mutual information score between the pooling regions and the manual parcels over multiple epochs is shown. A random overlap between learnt pooling regions and parcels is observed at initial epochs. After training, the AMI score increases with the pooling regions corresponding to ground-truth parcels.\nnormal control (NC), mild cognitive impairment (MCI) or Alzheimer's disease (AD). Specifically, we consider three different binary classification problems: NC vs AD, MCI vs AD and NC vs MCI. We compare our method against the random forest approach in [45] , which also considers surfacebased information from the ADNI dataset. To measure the contribution of the spectral embedding in our method, we also evaluate our model trained with only cortical thickness and sulcal depth as input. The same random split of 70-10-20 is employed for all three models. The classification performance of tested models is reported in Table III . We see that our method outperforms the random forest approach of [45] on all three classification problems. Relative to this approach, the proposed method yields mean accuracy improvements between 7.79% and 11.92%. A significant gain in performance is also observed when comparing against the same method trained without spectral node coordinates. This is particularly notable for NC vs MCI, where adding spectral coordinates increases the mean accuracy by 13.33%. Note that we have also tried giving the network original (x, y, z) coordinates of mesh nodes, however this led to worse results. This illustrates the advantage of using intrinsic node localization when processing surface data."}, {"section_title": "E. Brain age prediction", "text": "The last experiment demonstrates our method in a regression problem where the age of NC subjects of the ADNI dataset is predicted using pointwise surface-based measurements. In this case, the network outputs a single value, and MSE is used as a loss function. Once more, we test our method trained with or without spectral node coordinates as input. Moreover, to evaluate brain age prediction as a potential imaging biomarker for Alzheimer's, we also measure the prediction accuracy of our model on AD test subjects.\nResults of this experiments are summarized in Fig. 8 , which gives the distribution of mean absolute error (MAE) and predicted age minus real age for NC subjects and AD subjects. When testing on NC subjects, our method achieves an MAE of 4.35 \u00b1 3.19 years, which is comparable with results in the literature. As expected, a higher MAE of 6.80 \u00b1 6 years is obtained for AD subjects, since the symptoms of early Alzheimer's are similar to premature brain aging. The brain age, calculated as the predicted age minus the real age, shows a statistically significant difference with a p-value of 0.0032. This value suggests the potential application of brain age prediction as a biomarker for AD."}, {"section_title": "IV. CONCLUSION", "text": "We presented a novel strategy that enables pooling operations on arbitrary graph structures. Our experiments explored four different applications. In a first experiment, we compared different pooling techniques for graph convolution networks on the subject-sex classification task. A simple global average pooling failed to capture geometric information from consecutive layers, yielding a low performance of 60%. In comparison to employing fixed pooling regions or learning these regions with unsupervised clustering, our learnable pooling strategy offers significantly higher accuracy.\nThe second experiments involve assessing the effect of graph size on the performance of subject-sex classification. Results showed that small graphs lack information to capture the complete geometry of surfaces. However, reducing the size of the graph by 25% up to 75K node does not affect the performance of our model, while improving memory and computational requirements.\nThe third experiment explored the relationship between learned features and anatomy. The visualization of activation maps and clusters in the network revealed diversity in terms of brain regions. Several learned clusters highlighted essential regions of the basal ganglia, such as the hippocampus and amygdala, which are associated with sex-related differences Predicted age minus real age p = 0.0032 Fig. 8 . Distribution of absolute prediction error (left) and predicted minus real age (right), for NC and AD test subjects. Our learnable pooling strategy yielded graph models that could correctly capture age discrepancies between real and geometry-based ages, as expected between subjects with NC and AD.\nin the literature. Further evaluation of this result was obtained with an experiment to regress the size of cortical parcels. As expected, the trained model learns pooling regions similar to the manually annotated parcels. The fourth experiment focused on predicted stages of Alzheimer's disease from surface data, including cortical thickness and sulcal depth. Our results showed that pointwise surface values could be efficiently aggregated into a fixed number of class probabilities using the proposed network architecture. Compared to another approach exploiting surface-based features [45] , our method achieved significant improvements ranging from 7% to 11%. This performance gain is mainly due to including spectral coordinates of graph nodes as input to the network, demonstrating the importance of intrinsic node localization.\nIn a final experiment, the age of ADNI subjects was predicted using pointwise surface data. Results showed our method to provide comparable results as previous approaches in the literature, although only surface-based information is used in our method. As expected, subjects with Alzheimer's have higher discrepancies than subjects with normal cognition (Fig. 8) . The potential of the proposed method as an imaging biomarker for AD could be evaluated in a future study.\nTo summarize, our pooling strategy enables the exploration of a new family of architectures for graph convolution neural networks. The method exploits the spectral embeddings of graph nodes in order to learn spatially representative pooling patterns across the network layers. However, the proposed method depends on having datasets of comparable brain geometries. The spectral decomposition of graph Laplacian, indeed, assumes that shapes are topologically equivalent. Heterogeneity in holes and cuts in datasets of surfaces remains challenging to exploit since they may produce incompatible sets of Laplacian eigenvectors. This method is consequently inadequate for applications where significant geometrical changes exist, such as when tumors are ablated. It would be interesting to incorporate up-sampling to predict node level output measurements. A new set of methods working on multi-scale would assist brain surface data. Nevertheless, our proposed pooling strategy remains highly-relevant for a wide range of applications where surface data needs to be pooled sequentially in layers from full-size surface-valued vectors to single whole-subject characteristics."}]