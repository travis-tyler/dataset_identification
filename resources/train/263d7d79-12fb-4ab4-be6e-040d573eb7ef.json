[{"section_title": "Introduction", "text": "Q-Bank is a database management system designed to store results of pre-tested survey questions, particularly the results of cognitive research. When completed, Q-Bank will be an online interagency database of pretested survey questions and research results, maintained at NCHS and provided on a subscription basis to participating Federal statistical agencies. Q-Bank will be used to search through previously tested questions when developing new questions by both survey methodology and subject matter specialists. To this end, crafting new survey questions can be nd informed by work that has already been done. For more information on Q-Bank itself, see Beatty et al. (2005) and Miller et al. (2005). Q-Bank was developed initially for intervieweradministered household surveys, with the expectation that other types of surveys would be added in the future (e.g., self-administered, establishment, Web, and Interactive Voice Response). The goal for this project was to expand Q-Bank to include self-administered and establishment surveys. Q-Bank is operated by a steering committee formed of representatives from various federal statistical agencies 2 . A team composed of members of the steering committee was formed to add selfadministered surveys and establishment surveys to Q-Bank (known as the Self-Administered/ Establishment, or SAE, team). This team presented a proposal for new categories to the steering committee. This paper is intended to document the proposal developed by the SAE team and the reasoning behind the recommendations. Additionally, this paper lays the groundwork for a tool that will lead to research that will empirically support a more comprehensive response model that incorporates intervieweradministered, self-administered, household, and establishment surveys, considering not only verbal, but also graphical information presented as a part of a survey questionnaire."}, {"section_title": "Conceptual Framework", "text": "The SAE team was tasked with making recommendations for Q-Bank to accommodate selfadministered questionnaires and establishment surveys, 3 above and beyond what existed in Q-Bank for interviewer-administered household surveys. There are several critical differences between self-a interviewer-administered surveys and between household and establishment surveys that became apparent and yielded recommendations for adapting Q-Bank to include self-administered and establishment surveys. The three key differences between self-administered and interviewer-administered surveys are: 1) selfadministered surveys are interpreted visually rather than aurally; 2) an interviewer is not present to aid in interpretation of the questions for self-administered surveys; and 3) the respondent solely controls the pace of and the order in which the questions are read and answered in a self-administered survey. Additionally, self-administered forms have various options for layout of questions, including matrices or grids, to gather data on multiple dimensions. Each of these design differences may produce different types of response errors. Establishment surveys also have several features that distinguish them from household surveys. Establishment surveys typically ask for data expected to be found in or derived from company records. The technical nature of the requested data often requires detailed definitions or instructions, which are frequently placed in separate booklets. Moreover, establishment respondents often compile data from multiple sources or from other departments to complete a questionnaire. Our challenge was to incorporate these key differences into Q-Bank without altering its underlying framework. The SAE team strove for a framework that was founded in the literature. Tourangeau proposed four stages of the response process: comprehension, retrieval, judgment, and response (Tourangeau, et al. 2000). In addition to Tourangeau's four-stages, in selfadministered forms, an additional stage seems to precede comprehension. Prior to comprehending a question or an instruction, the respondent must perceive it. In an interview, this is assumed to happen automatically, i.e., we assume the interviewer reads the information such that a respondent can hear it. However, in a self-administered form, survey designers have less control over whether or not a respondent reads information on the form. In particular, instructions are often overlooked. Some respondents just scan the information, others make a conscious decision not to read them, and some simply do not see the instructions at all. Recent research by Redline and others (Redline et al., 2004;Redline and Dillman, 2002) demonstrates the effects of visual design on the perception of the response task in self-administered surveys, regardless of whether the survey is of establishments or households. This is accounted for in Willis and Lessler's (1999) Question Appraisal System, with the potential to encompass visual language problems in some detail. Alternative theories of the response process for establishment surveys were also utilized. We were motivated by a coding scheme that also uses a similar framework to Tourangeau's model (Forsyth et al., 1999;O'Brien, 2000), but includes establishment issues, as well as a conceptual framework of the establishment survey response process (Willimack & Nichols, 2001;Sudman et al., 2000;Edwards and Cantor, 1991). Additional steps, including the selection of the respondent and establishment priorities affecting the reporter's attention to the response task (Willimack & Nichols, 2001;Sudman et al., 2000;Edwards and Cantor, 1991), add a more holistic perspective on a respondent's approach to completing an establishment survey questionnaire. These conceptual models were used to guide the development of codes for establishment surveys. Although many of these models encompass a respondent's approach to an entire questionnaire, the basis for Q-Bank is \"question-centric,\" that is, the descriptive fields and response error indicators pertain to a particular question. Thus, the SAE team worked to re-interpret the conceptual models of the establishment survey response process and the perception of visual elements within a \"question-centric\" framework. We did this by exploring the issue of the context surrounding a survey question. Even in an intervieweradministered survey, individual questions are not cognitively addressed in isolation by the respondents hearing them. Previous questions, the presence (or absence) of an interviewer, and the sponsor of a survey all form a context in which the respondent understands and answers the question. The context for a selfadministered question includes the additional element of visual presentation. Thinking particularly of establishment surveys, we further expanded the notion of context to include the type of respondent to whom the question is addressed. In the original version of Q-Bank, questions were administered in surveys of the general population, households and/or individuals. This context also affects the response processes. If the same individuals were asked to complete surveys for their places of work, the context for answering these questions would be affected by the establishment or organizational setting. Establishment-related factors such as the availability and access to data in records, the content of those records relative to the requested data, and competing work-related priorities form a context within which each question is addressed by the respondent. Our resulting conceptual framework for incorporating self-administered and establishment surveys into Q-Bank is illustrated in Figure 1. Originally Q-Bank allowed for context effects resulting from the other questions in the survey (assuming an interviewer-administered survey that was processed aurally by the respondent in a household situation). The addition of self-administered and establishment surveys required the explicit recognition of these other factors (i.e., reading as opposed to hearing the questions, absence of an interviewer, and completing a form for an establishment) forming the context in which a question is asked and answered."}, {"section_title": "Incorporating Establishment Survey Characteristics", "text": "The following additions and modifications to Q-Bank were needed in order to incorporate characteristics of establishment surveys. The Q-Bank field, Universe, was established to distinguish surveys of establishments, businesses or organizations from surveys of the general population, households or individuals. As Universe is very broad, we added a field to specify the Target Population to which the survey is directed. Examples of Target Population for surveys of the general population are households or individuals; examples for establishment surveys include hospitals, retail businesses, schools, or manufacturing plants. We also proposed the addition of a field describing the Respondent's Role, since establishment surveys are often directed to a particular employee within an establishment or may require that multiple respondents be involved in completing the form. Examples of particular employees that are targeted for surveys are accountants, plant managers or school principals. To account for situations where multiple respondents complete a form, we adopted the compiler and/or coordinator roles, as defined by Nichols et al. (1999), who typically are individual contact persons who compile or coordinate the gathering of data from multiple sources when multiple respondents are required. The field Respondent's Role is also useful for surveys of households and individuals, where the codes include self or proxy respondent."}, {"section_title": "Efforts to Better Describe Self-Administered and Establishment Survey Characteristics", "text": "This section describes fields needed for capturing the unique characteristics of questions and answers in selfadministered surveys. The SAE team strove to limit the captured characteristics to those thought to be correlated with response problems. One purpose of providing descriptive information of the pre-tested questions is so that meta-analysis of pre-testing data can yield information on what characteristics lead to response errors. One of the key differences between self-administered and interviewer-administered surveys, as previously mentioned, is the aspect of seeing, rather than hearing, the question. In order to adequately capture characteristics of the question that could affect response error, we needed to be able to code visual characteristics of the question. The visual image will be accessible in the database through a link to the pdf file of the questionnaire. However, in order to quantify what aspects of the question lead to response errors, we needed the ability to code visual characteristics as well. In the end, the coding scheme described below was created with three goals in mind: (1) to be simple enough that it could be reliably applied, (2) to remain on schedule and within budget, and (3) to lay the foundation for a database that would be used as a research tool in the future. Thus, it was considered important to capture the location of information (i.e., a description of how the instructions, questions and response options are positioned in relation to each other), as the team thought that the visual placement of these items would be highly correlated with response error in questionnaires (for examples of this type of finding, see Redline et al., 2004;Redline and Dillman, 2002). Section 4.1 describes the initial coding scheme developed for individual questions; Section 4.2 uses this information as a foundation for describing matrices."}, {"section_title": "Describing Individual Questions", "text": "The fields for describing individual questions can be grouped under the following five headings: (1) global instructions, (2) questions, (3) response options, (4) location of information, and (5) separate instructions. Each of these pieces of an individual question is discrete and has the potential to affect response error. This section will further explore these descriptive fields of Q-Bank."}, {"section_title": "Global instructions", "text": "Global instructions provide a context for the survey and/or provide information necessary to complete the survey. Many self-administered questionnaires begin by providing respondents with instructions that will likely impact respondents' understanding of the task should these instructions be misunderstood or never read. A good example of this is a global instruction that is buried at the end of a paragraph at the beginning of the 2004 Survey of Industrial Research and Development. It reads, \"Except as noted, this report covers your entire consolidated domestic enterprises, including all U.S. subsidiaries, affiliates, and branches.\" This is a highly critical instruction aimed at respondents' understanding of the unit for which they are to report. However, Q-Bank as it was originally designed, does not allow for the capture of this kind of information. Thus, one of the first form description fields proposed is to include a field entitled Global Instructions. A second example of a Global Instruction, one that was shown to have deleterious consequences in Census 2000 when not followed by respondents is \"Please use a blue or black pen to complete this form\" (see Figure 2)."}, {"section_title": "Questions", "text": "Identifying a discrete question with precision tends to be more difficult in self-administered surveys because they often contain instructions in addition to questions, or forgo questions altogether in lieu of statements, instructions or definitions (typically called items). Furthermore, questions and answers in these surveys are composed of visual and physical characteristics, such as where each piece of information lies and navigational directions, that are not expressed to respondents in interviewer-administered surveys, but are necessary in self-administered surveys. Thus, the SAE team proposed building upon the descriptive fields already present in Q-Bank, and adding or modifying the following fields to better describe the format of questions in self-administered surveys. The first field is one that was retained from the original scheme, the text field Introductory Text. An example of Introductory Text is provided in Figure 2. The instruction \"Start Here. Before you answer Question 1, count the people living or staying at this place on October 1, 2004\u2026\" and the include/exclude instructions that follow this instruction have been shown to impact respondents' understanding of the question (Martin, et al., 2003). The style of introductory text in self-administered surveys varies from question to question within questionnaires as well as across questionnaires; thus describing it satisfactorily requires the addition of another field called Introductory Text Style. This field describes whether the introductory text takes the form of a heading or label, contextual instructions, navigational directions, or some combination. Headings or labels are basically sentence fragments or phrases. Contextual instructions provide respondents with instructions that are necessary to answer the question correctly, and navigational directions tell respondents how to move through the questionnaire. Figure 2 shows introductory text comprised of both contextual instructions \"Before you answer Question 1, count the people\u2026\" and navigational directions \"Start Here.\" Figure 2 also displays introductory text that is in the form of a heading or label: \"Person 1.\" The reason for capturing this information is to determine if, as hypothesized, the introductory text's style affects respondents' understanding of the question to follow. The field Core Question remains essentially the same between the original Q-Bank and the proposed selfadministered coding scheme, except that it has been expanded in the self-administered coding scheme to include the idea that information in self-administered surveys may not necessarily be phrased in the form of a question. This is captured through the addition of the field Core Question Style, similar to the introductory instruction style field, which identifies if an item is presented as a heading or label, instruction (or statement), or question. Figures 2 and 3 show examples of core questions expressed as questions and instructions, respectively. In self-administered surveys, instructions not only precede questions, but they often follow them as well. Thus a new text field necessary for capturing information about questions in self-administered surveys is Instructions that Follow Core Questions or Items. An example of an instruction that follows a question in an establishment survey is provided in Figure 2, question 10: \"Mark X one or more boxes\"."}, {"section_title": "Response options", "text": "Self-administered surveys often ask respondents to report answers in text boxes that contain labels. Thus, the term Label was added to the Response Options Text field in Q-Bank to capture this possibility. In Figure 2, question 1 provides an example of a response label associated with an open-answer space. The label in this case is \"Number of people.\" The same way instructions follow questions in selfadministered surveys, they often follow response options as well. Thus, we proposed another field to capture this information, Instructions Following Response Options. In Figure 3, question 9, the instructions \"Print origin, for example. . .\" follow the response options."}, {"section_title": "Location of information", "text": "Several fields are proposed to capture the location of information in self-administered surveys. As implied, the Location of Question or Stem and Response Options field attempts to describe the location of the question in relation to the response options. If there is more than one response option, as can be seen below, then there are four possibilities: the question can be above the response options or to the left, and the response options can either be aligned horizontally or vertically.  If there is only one option, then two possibilities exist: the question is either located above or to the left of the one option. However, response options are not necessarily laid out as uniformly as they are depicted here. Sometimes they are forced to conform to the space available, which leads to combinations in their configuration. Below are real-life examples of a question located: \u2022 Question above one response option can be seen in Figure 2 (question #4); \u2022 Question above a horizontal layout is provided in Figure 2 (question #6); \u2022 Question above a vertical layout is provided in Figure 3 (question #9); \u2022 Question above both vertical and horizontal response options is provided in Figure 3 (question #10); and \u2022 Question to the left of a horizontal layout is provided in Figure 3 (question #7). The fact that five different configurations can be identified in two pages of two questionnaires is a testament to the diversity of configurations that respondents are asked to contend with in selfadministered surveys. The effects of such configurations are far from understood presently, except to say that research has begun to move in the direction of exploring effects of location on response behavior (Tourangeau et al., 2004;Christian and Dillman, 2004;Smyth et al., 2004). In addition to how the question relates to the response options, another field is needed to describe the location of the response option text in relation to the answer space or box:  Figure 2 displays an example of the response option text placed to the right of the answer space (question #9, \"White\"). Figure 2 also displays an example of text above the answer space (question #5, \"Last Name\"). Figure 3 displays the same text (question #6 this time) to the left of the answer space. Once again, the fact that there are so many examples in just two pages suggests how often the location of this information is varied in questionnaires."}, {"section_title": "Separate instructions", "text": "Up to this point, we have been discussing information located in close proximity to individual questions and response options. However, in self-administered surveys another field is needed to capture instructions that are distant from the question and answers to which they pertain. These may be located at the beginning of the questionnaire, the end, or in a separate instruction booklet. Rather than trying to capture all of this information presently, we code whether or not there are Separate Instructions."}, {"section_title": "Matrices", "text": "A matrix can look different from survey to survey, but they all have the same characteristics -a series of questions and objects grouped together in rows and columns, usually as a way to save space by reducing the number of times the same question is asked or to avoid repetitive questioning about similar items. Coding matrices is necessary for self-administered surveys since they appear fairly often in both household and economic self-administered surveys."}, {"section_title": "Matrix status", "text": "First and foremost, the SAE team recognized the importance of being able to tell whether a core question/item was placed on the form individually or as part of a matrix. If the matrix field is answered with a \"yes\" response, additional fields that apply only to matrices are activated."}, {"section_title": "Core questions or items and objects", "text": "While there may be some difficulty identifying the core question or item for an individual question, the task becomes even more difficult for matrices. In a matrix, a series of questions are asked about a series of objects. The core question for a matrix is the question or statement that tells the respondent the topic of interest. The object is the person, place or thing that the question asks about. For example, in Figure 4, the core questions are formed as questions, e.g., \"What is this person's sex?\"; \"What is this person's age and what is this person's date of birth?.\" The objects these questions apply to are Persons 1 through 5, listed down the left side of the form. Figure 5 also has core items that go across the top of the matrix, but they appear as column headers rather than questions. These core items, including \"end-of-quarter balance\" and \"film and television tape rentals,\" are the topics being asked about. The objects in this case are the countries listed in the row stems appearing down the left side of the matrix. Core questions can be located in either the columns or the rows. As a result, we proposed the addition of a field named Orientation of Core Questions or Items, which has two options: in rows, or in columns. Additionally, because a core question requires an object in order to complete the question, it is necessary to identify the Objects as well. Thus we proposed the addition of a field to enumerate the objects of the question(s) posed in matrix form."}, {"section_title": "Coding introductory text in matrix questions", "text": "For a matrix, the introductory text explains the matrix in some way, usually by asking a question or providing instructions.. The Introductory Text Style for Figure 4 would be \"instruction\" because it provides instructions about what should be recorded in the matrix, while for Figure 5, it would be \"question\" because it is in question format. Even though Figure 5 asks a question, it is not the core question because the core question for a matrix is associated with a row or a column, not the entire matrix."}, {"section_title": "Sequence number", "text": "Just as individual questions are usually labeled with a number, so are items in a matrix. These numbers indicate the path respondents should take through the matrix. This number gives the user some information on where in the matrix each core question appears, and could be correlated with response errors (e.g., the first question in a matrix may be responded to differently than the last one)."}, {"section_title": "Response Error Indicators for Self-Administered and Establishment Questionnaires", "text": "The SAE team tackled the response error indicators last. The SAE team felt it would be relatively easy to describe response errors if the aspects of the questionnaire that were likely to cause response errors were adequately captured. The next task was to come up with relatively simple, easy-to-understand codes to indicate problems that are likely to occur in selfadministered and establishment survey questions. The underlying theory that guided much of this development was that problems found in household interviewer-administered surveys (i.e., verbal language problems) also exist in both self-administered and establishment surveys. Since many establishment surveys are also self-administered surveys, by nature, problems in self-administered surveys also occur in establishment surveys. Then, there is a subset of problems that are unique to establishment surveys because they are completed within the constraints of an establishment and are subject to policy issues, organizational reporting procedures, record mismatches, and multiple respondents. For this reason, the SAE team started with the household intervieweradministered response error indicators, added indicators for self-administered forms, and then added categories to account for problems unique to establishment surveys. From the original interviewer-administered, household Q-Bank, the SAE team inherited the following response error indicators: 1. Administrative difficulty 2. Problematic terms 3. Ambiguous concepts/terms 4. Overly complex verbal language 5. Context effects 6. Recall difficulty 7. Biased/sensitive 8. Inadequate response options 9. None as Tested These response error indicators were heavily biased in favor of verbal language errors that result from comprehension and recall in household intervieweradministered questionnaires. The SAE team first added categories related to perception and visual aspects of the questionnaire. The following additional categories are for selfadministered surveys only: 1. Not seen/read 2. Problematic visual cues 3. Problematic answer space \"Not seen/read\" deals with the perception problem that precedes the comprehension stage in which the respondent either does not see, or chooses not to attend to a piece of the item, which could be the instruction, part of the question text, or the response set. \"Problematic visual cues\" include any visual stimulus that could mislead the respondent. These could be inadequate or ambiguous visual or navigational cues, such as skip patterns that are not easily followed, directional symbols that are not well understood, or misuse of italics. Problematic visual cues could also be cues that are inconsistent, for example using the phrase \"skip to\" sometimes and \"go to\" other times within a questionnaire to indicate a skip pattern, or using bolding and italics interchangeably throughout the questionnaire. \"Problematic Answer Space\" deals with any difficulty the respondent could have entering their chosen answer onto the form. These new categories serve as the \"administrative difficulty\" problems with a selfadministered form. Additionally, the SAE team added response errors specific to establishment surveys. The following response error indicators appear only when questions from establishment surveys are being entered into the database or are the topic of a search. The following additional categories are for establishment surveys only: Retrieval from Records 1. Mismatch between reporting unit and org unit 2. Mismatch between requested data and records 3. Requested data not kept in records 4. Lack of access to requested data 5. Decentralized data Policy Imposed Judgment 6. Release of data prohibited 7. Not willing to divulge proprietary information 8. Release of data conditional upon authorization Personal Judgment 9. Estimation 10. Calculation/transformation of data 11. Perpetuating prior incorrect reporting strategy Retrieval from records encompasses five potential response error indicators. A \"mismatch between reporting unit and organizational unit\" occurs when the structural unit for which data are requested does not match the respondent's natural or structural way of organizing the information. A \"mismatch between requested data and records\" occurs when data for the topic of interest exist in records, but not in the manner requested. \"Requested data not kept in records\" happens when a survey asks for information that is not tracked in records at all. \"Lack of access to requested data\" indicates that the data exist in records, but the respondent has not been granted access to that information. \"Decentralized data\" signifies that the requested data do not reside in one location or with one respondent. It is necessary to consult multiple sources to retrieve information. Additionally, in establishments, when a respondent makes a judgment about how to report something, they are not only making a personal judgment (i.e. should I estimate, or try to calculate the answer?), but they are also considering authority and policies concerning reporting company data to an external entity (e.g., the survey organization). For this reason, the SAE team separated personal judgment from policy-imposed judgment. Policy-imposed judgment, at minimum, may cause the response error of item nonresponse, or it may encourage estimation. (The analogous error in household surveys is \"social desirability.\") Three codes detail the sources of policy imposed judgment; these address gradations in whether or not the data can be released at all. Three categories were added to personal judgment because they are errors that occur at the level of the respondent as an individual, not the establishment as a whole: 1) \"estimation\" -respondent estimates are more likely to be error-prone than actual figures; 2) \"calculation/ transformation of data\" -respondents may commit errors when asked to calculate figures that could be calculated on the processing side of data collection (e.g., asking the respondent to sum figures in a table); and 3) \"perpetuating prior incorrect reporting strategy\" -instead of reading instructions and generating the requested data, the respondent uses an erroneous strategy that has been used in the past for reporting this type of data. In addition to collecting what the response error indicator was, the SAE team also recommended collecting Where the Error Occurred (i.e., whether the error occurred in the question itself or in the heading or label, the contextual instructions, the navigational directions, the response options or in non-verbal components of the question). This will permit tying response errors to components of the question."}, {"section_title": "Conclusions and Future Directions", "text": "The coding scheme described in this paper meets the Q-Bank steering committee's objective to balance the practical concerns of being able to reliably apply this coding scheme to studies, along with the necessity of remaining within time and cost constraints. It also attempts to meet the theoretical concerns of working towards a day when Q-Bank also becomes a source for analyzing relationships between question characteristics and response error. Q-Bank now contains a fairly thorough description of the verbal language contained in a questionnaire, be it interviewer-administered or self-administered and household or establishment, as well as some information on the location of the verbal information on the questionnaire. The next step is to expand Q-Bank by including Web surveys. We anticipate that in addition to the fields we have created for self-administered forms, there will be additional fields necessary to describe a Web survey question. As the survey industry moves towards conducting more surveys on the Web, this will become a critical avenue for Q-Bank research. As Q-Bank grows, it may be used to support research to identify characteristics of survey questions that contribute to response errors, thus improving survey data quality more generally. We will be able to compare and contrast response error problems across types of survey and collection modes. It will enable us to investigate issues that stem from the physical and visual layout of surveys in a systematic way. Finally, when the database is complete, it will provide further support for a comprehensive response model that incorporates visual as well as verbal characteristics of survey reporting."}]