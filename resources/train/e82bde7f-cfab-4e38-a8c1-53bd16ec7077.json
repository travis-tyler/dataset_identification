[{"section_title": "", "text": "\u2022 Different from a grant -10% indirect rate -Continued agency involvement in the work -Work needs to be of mutual interest/benefit to UNL and NASS/NCSES \u2022 Graduate student training is a priority (i.e., RA funding)"}, {"section_title": "Objective", "text": "\u2022 Conduct survey methodology research to assist with design improvements and/or the redesign of surveys related to science, engineering, and agriculture. -Questionnaire design and measurement in web, mail, and mixed-mode surveys -Survey recruitment and implementation -Post-survey adjustments and imputation -Provide consultation to the agencies -Provide seminars to the agencies Questionnaire design and measurement in web, mail, and mixed-mode surveys \u2022 Review questionnaire design with an eye toward -Unimode design within surveys -Unnecessary design differences across surveys \u2022 Surveys: -National Survey of Recent College Graduates (NSRCG) -Cross-sectional survey science, engineering, or health bachelor's degree recipients in prior 2-3 years. Tracks trends in education, employment, and salaries of recent grads. Sample institutions and then graduates. -National Survey of College Graduates -Longitudinal survey of college graduates used to examine characteristics of the college-educated, occupation, work activities, salary, etc. Sample from ACS and previous NSCG. -Survey of Earned Doctorates (SED) -An annual census of doctorate recipients used to assess characteristics of the doctoral population and trends in doctoral education. -Survey of Doctorate Recipients (SDR) -Longitudinal survey of doctoral recipients in science, engineering, and health. Followed through their careers from degree to age 76 to understand educational and occupational achievements and career movements of the nationals doctoral scientists and engineers. Sample from SED."}, {"section_title": "These surveys have had to change with the times", "text": "\u2022 The SED began in 1957 and the NSRCG, NSCG and SDR all began in early 1970s -Survey methods have changed a lot since 1957 and even since the 1970s! \u2022 Most were originally conducted using paper and pencil questionnaires. \u2022 Over time, telephone and web modes were added, raising several challenges. "}, {"section_title": "etc.", "text": "\u2022 We identified many more changes across the modes. -Things were most messy when there were three modes (paper, web, and telephone). \u2022 Many of the differences were unnecessary and can easily be eliminated -Do more quality control of the web instruments contractors put together. \u2022 Others were harder to deal with (e.g., field of study and occupation questions) -Paper -find the code from a two-page \"field of study\" list printed at the end of the questionnaire. -Web -type in a response. If it matches an option in the data base, it will be auto-coded. If not, go through a series of items that start broad and narrow down to the specific answer. -Phone -Go through a series of items that start VERY broad and narrow down. \u2022 Currently reviewing the recruitment and implementation protocols for the SDR, NSRCG, and NSCG and synthesizing the last 10 years of internal research on implementation for these surveys. \u2022 We have also examined the concept of \"mode preference\" -Conventional Wisdom: People will be more likely to respond and will give better answers if we survey them in a mode they prefer. -NCSES collects mode preference in the SDR and uses that information to tailor later data collections. -But they haven't tested this idea well. -We were able to test it on Nebraskans. Why wouldn't \"mode preference\" be a useful concept? \u2022 Previous research suggests self-reports of mode preference are highly influenced by the mode in which the self-report is given (Groves & Kahn 1979). \u2022 Yet many surveyors reason that people have a mode preference and that if we cater to it, we can increase response rates. \u2022 They design survey protocols as if mode preferences are real. We needed an empirical test of catering to mode preference \u2022 We carried out that test with two questions driving us? -Are mode preferences meaningful (i.e., do they predict anything)? -Is catering to mode preferences a good practice? -If people are more likely to answer and give better responses when answering in their preferred mode then there is something real and meaningful to self-reports of mode preference. Step 1. Measure Mode Preference Step 2. Evaluate Participation"}, {"section_title": "Mode preference was measured in the 2008 Nebraska Annual Social Indicators Survey (NASIS)", "text": "\u2022 Statewide omnibus survey conducted February-August 2008 \u2022 Conducted by telephone with a listed landline sample of Nebraska residents age 19+ \u2022 AAPOR RR3: 38% \u2022 1,370 of 1,811 (75.6%) respondents indicated willingness to participate in future social research. Self-reported mode preferences were as follows: \u2022 If you received a request to do another survey like this one, would you prefer to participate in\u2026 In 2009, two follow-up surveys were conducted in which some 2008 NASIS respondents were surveyed in their preferred mode and others were surveyed in a non-preferred mode. Catering to mode preference seems to make a positive difference, BUT\u2026 \u2022 It does not overcome low overall web response rates. Those who prefer the web respond to mail and phone surveys at higher rates than they respond to web surveys. Response rates are higher for those being surveyed in their preferred mode. This finding holds when respondent characteristics are taken into account. For the phone survey, it is driven by higher cooperation rates, not higher contact rates. \u2022 Those answering in a preferred mode SHOULD NOT take advantage of opportunities to satisfice or have difficulty with tougher items."}, {"section_title": "16.2% preferred mail", "text": ""}, {"section_title": "23.6% preferred web", "text": ""}, {"section_title": "Mail", "text": "\u2022 Those answering in a non-preferred mode SHOULD take advantage of opportunities to satisfice or have difficulty with tougher questions. \u2022 Our data suggests that those answering in a nonpreferred mode take advantage of opportunities to satisfice more so than those answering in a preferred mode. Check-All-That-Apply vs."}, {"section_title": "Forced-Choice", "text": "Previous research suggests the check-all format allows satisficing response behaviors, resulting in fewer items selected. We expect those answering in a preferred mode to be willing and able to process all items, resulting in few differences across the format. We expect those answering in a nonpreferred mode to be less motivated and therefore to take advantage of the opportunity to satisfice in the check-all format leading to larger differences in the mean number of items selected between these two formats. Those in a non-preferred mode select significantly fewer items in the check-all format than in the forced-choice format.  Those in a non-preferred mode are significantly more likely to leave the item blank when it has a large rather than small answer box "}]