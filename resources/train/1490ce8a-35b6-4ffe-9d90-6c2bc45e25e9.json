[{"section_title": "Abstract", "text": "Purpose: This article presents a computer-aided diagnosis technique for improving the accuracy of the early diagnosis of Alzheimer's disease \u0351AD\u0352. Two hundred and ten 18 F-FDG PET images from the ADNI initiative \u035352 normal controls \u0351NC\u0352, 114 mild cognitive impairment \u0351MCI\u0352, and 53 AD subjects\u0354 are studied. Methods: The proposed methodology is based on the selection of voxels of interest using the t-test and a posterior reduction of the feature dimension using factor analysis. Factor loadings are used as features for three different classifiers: Two multivariate Gaussian mixture model, with linear and quadratic discriminant function, and a support vector machine with linear kernel. Results: An accuracy rate up to 95% when NC and AD are considered and an accuracy rate up to 88% and 86% for NC-MCI and NC-MCI,AD, respectively, are obtained using SVM with linear kernel.\nResults are compared to the voxel-as-features and a PCA-based approach and the proposed methodology achieves better classification performance."}, {"section_title": "I. INTRODUCTION", "text": "Alzheimer's disease \u0351AD\u0352 is the most common cause of dementia in the elderly and affects approximately 30\u03eb 10 6 individuals worldwide. With the growth of the older population in developed nations, the prevalence of AD is expected to quadruple over the next 50 yr \u0351Refs. 1 and 2\u0352, while its early diagnosis remains being a difficult task. Therefore, Alzheimer's disease is a severe global public health problem. Nowadays, a large amount of potential treatments are under study. The early stages of Alzheimer's disease are difficult to diagnose. A definitive diagnosis is usually made once cognitive impairment compromises daily living activities. A patient will progress from mild cognitive problems, such as memory loss through increasing stages of cognitive and noncognitive disturbances, and eventually causing death. 3 It has been evidenced in the literature that a reduction in the cerebral metabolic rate of glucose in some regions of the brain is related to AD. AD images present lower rate of glucose metabolism in bilateral regions in the temporal and parietal lobes, posterior cingulate gyri and precunei, and also in other parts of the brain for more severe stage of the dementia. Nevertheless, brain hypometabolism is also presented in other neurodegenerative disorders. Reductions in the cerebral metabolic rate of glucose is also detected in mild cognitive impairment \u0351MCI\u0352 subjects. Furthermore, patients labeled as MCI present a set of symptoms that might indicate the start of the disease. [4] [5] [6] [7] [8] Alzheimer's diagnosis is usually based on the information provided by a careful clinical examination, a thorough interview of the patient and relatives, and a neuropsychological assessment. [9] [10] [11] [12] An emission computed tomography \u0351ECT\u0352 study is frequently used as a complementary diagnostic tool in addition to the clinical findings. 8, 13 However, in late-onset AD, there is minimal perfusion in the mild stages of the disease and age-related changes, which are frequently seen in healthy aged people, have to be discriminated from the minimal disease-specific changes. These minimal changes in the images make visual diagnosis a difficult task that requires experienced observers. In ECT imaging, the dimension of the feature space \u0351num-ber of voxels\u0352 is very large compared to the number of available training samples \u0351usually \u03f3100 images\u0352. This scenario leads to the so-called small sample size problem, 14 as the number of available samples is greater than the number of images. Therefore, a reduction in the dimension of the feature vector is desirable before performing classification. In this work, a computer-aided diagnosis \u0351CAD\u0352 system for assisting Alzheimer's diagnosis is proposed. This methodology is based on a preliminary automatic selection of voxels of interest using t-test. Then, selected voxels are modeled using factor analysis \u0351FA\u0352. The observed variables are modeled as linear combinations of the factors and factor loadings are used to describe variability among selected voxels in terms of fewer unobserved variables. These factor loadings will be used as feature vectors and they allow us to reduce the dimension of the problem surmounting the small sample size problem. Classification is performed using three different approaches: Fitting a multivariate normal density to each group with a pooled estimate of covariance; fitting a multivariate normal density with covariance estimates stratified by group; and using support vector machine with linear kernels. This work is organized as follows. In Sec. II the image acquisition, preprocessing, feature extraction, and classification methods used in this paper are presented. In Sec. III, we summarize the classification performance obtained by applying the proposed methodology and in Sec. IV, the results are discussed. Lastly, the conclusions are drawn in Sec. V."}, {"section_title": "II. MATERIAL AND METHODS", "text": ""}, {"section_title": "II.A. Image acquisition and labeling", "text": "18 F-FDG positron emission tomography \u0351PET\u0352 images used in the preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative \u0351ADNI\u0352 database \u0351www.loni.ucla.edu/ADNI\u0352. The ADNI was launched in 2003 by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, the Food and Drug Administration, private pharmaceutical companies, and nonprofit organizations, as a 60 million, 5-year publicprivate partnership. The primary goal of ADNI has been to test whether serial magnetic resonance imaging, PET, other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of MCI and early AD. Determination of sensitive and specific markers of very early AD progression is intended to aid researchers and clinicians to develop new treatments and monitor their effectiveness, as well as lessen the time and cost of clinical trials. The principal investigator of this initiative is Michael W. Weiner, M.D., VA Medical Center and University of California-San Francisco. ADNI is the result of efforts of many coinvestigators from a broad range of academic institutions and private corporations and subjects have been recruited from over 50 sites across the U.S. and Canada. The initial goal of ADNI was to recruit 800 adults, ages 55-90, to participate in the research: \u03f3200 cognitively normal older individuals to be followed for 3 yr, 400 people with MCI to be followed for 3 yr, and 200 people with early AD to be followed for 2 yr.\n18 F-FDG PET scans were acquired according to a standardized protocol. A 30 min dynamic emission scan, consisting of six 5 min frames, was acquired starting 30 min after the intravenous injection of 5.0\u03ee 0.5 mCi of 18 F-FDG, as the subjects, who were instructed to fast for at least 4 h prior to the scan, lay quietly in a dimly lit room with their eyes open and minimal sensory stimulation. Data were corrected for radiation-attenuation and scatter using transmission scans from Ge-68 rotating rod sources and reconstructed using measured attenuation correction and image reconstruction algorithms specified for each scanner \u0351www.loni.ucla.edu/ ADNI/Data/ADNIData.shtml\u0352. Following the scan, each image was reviewed for possible artifacts at the University of Michigan and all raw and processed study data were archived.\nEnrolled subjects were between 55 and 90 \u0351inclusive\u0352 yr of age. Images were labeled as normal subjects, MCI subjects, and mild AD. General inclusion/exclusion criteria are as follows:\n\u03511\u0352 Normal subjects \u035152 images\u0352: Mini-mental state exam 15 \u0351MMSE\u0352 scores between 24 and 30 \u0351inclusive\u0352, a clinical dementia rating 16 \u0351CDR\u0352 of 0, nondepressed, non-MCI, and nondemented. The age range of normal subjects will be roughly matched to that of MCI and AD subjects. [18] [19] [20] Let us note that ADNI patient diagnostics are not pathologically confirmed; therefore, some uncertainty on the subject's labels are introduced unavoidably."}, {"section_title": "II.B. Image registration", "text": "The complexity of brain structures and the differences among the brains of different subjects make necessary the normalization of the images with respect to a common template. This ensures that the voxels in different images refer to the same anatomical positions in the brain.\nFirst, the images have been normalized using a general affine model, with 12 parameters. 21, 22 After the affine normalization, the resulting image is registered using a more complex nonrigid spatial transformation model. The deformations are parameterized by a linear combination of the lowest-frequency components of the three-dimensional cosine transform bases 23 using the SPM5 software \u0351http:// www.fil.ion.ucl.ac.uk/spm/software/spm5\u0352. A smalldeformation approach is used and regularization is achieved by the bending energy of the displacement field.\nThen, we normalize the intensities of the 18 F-FDG PET images with respect to the maximum intensity, which is computed for each image individually by averaging over the 0.1% of the highest voxel intensities. The intensity normalization process is very important to perform a voxel-by-voxel comparison between different images. We choose this intensity normalization procedure because it accomplishes two different goals: On the one hand, changes in intensity are linear and, on the other hand, the effect of possible outliers in the data is mitigated.\nClassification results using the proposed intensity normalization procedure outperforms the results obtained when intensity values are not normalized or when each 18 F-FDG PET image is linearly normalized to its maximum value. Of course, we do not claim this normalization procedure is the best one, but it allows us to obtain very good classification results, as it will be shown in Sec. III. Furthermore, this intensity normalization procedure has been successfully applied in other recent works. [24] [25] [26] [27] "}, {"section_title": "II.C. Preliminary voxel selection: t-student test", "text": ""}, {"section_title": "Each", "text": "18 F-FDG PET image has 62 322 voxels \u035147\u03eb 39 \u03eb 34\u0352 with intensity values ranging from 0 to 1. Some of them correspond to positions outside the brain. We discard those voxels which present an intensity value lower than 0.35. Basically, positions outside the brain and some very low-intensity regions inside the brain are discarded.\nThen, voxels are ranked using the absolute value two sample t-student test with pooled variance estimate\nwhere \u012a 1 and \u012a 2 denote the mean 18 F-FDG PET image of subjects labeled as 1 and 2 , respectively, n i is the number of images in population i , and S 1 2 is an estimator of the common standard deviation of the two samples.\nwhere S i is the sample standard deviation image for population i , i =1,2 which is defined as Figure 1 shows the brain image I t with the value of the t-test statistic in each voxel. In this example, normals and AD images were considered in the calculation of the image I t . Student's t-test gives us information about voxel class separability. We select those voxels i which present a t-statistic greater than a given threshold select voxels i:\nThen, those selected voxels will be modeled using factor analysis."}, {"section_title": "II.D. Feature extraction: Factor analysis", "text": "Factor analysis is a statistical method used to describe variability among observed variables in terms of fewer unobserved variables called factors. The observed variables are modeled as linear combinations of the factors plus error. We use factor analysis to reduce the feature dimension. Factor analysis estimates how much of the variability in the data is due to common factors.\nSuppose we have a set of N observable random variables, x 1 , ... ,x N with means 1 , ... , N .\nSuppose for some unknown constants ij and m unobserved random variables F j , where i 1, ... ,N and j 1, ... ,m, where m \u03fd N, we have\nwhere z i is independently distributed error terms with zero mean and finite variance different for each observable variable. The previous expression can be also written in matrix form\nwhere x is a vector of observed variables, is a constant vector of means, \u2333 is a constant N-by-m matrix of factor loadings, F is a matrix of independent, standardized common factors, and z is a vector of independently distributed error terms.\nThe following assumptions are imposed to the unobserved random matrix F: F and z are independent, the mean of F is equal to 0, and the covariance of F is the identity matrix. These assumptions allow computing the factor loadings \u2333 using a maximum likelihood approach. 28 Once the factor loadings have been estimated, we rotate them using a Varimax approach which is a change of coordinates that maximizes the sum of the variance of the squared loadings. This method attempts to maximize the variance on the new axes. Thus, we obtain a pattern of loadings on each factor that is as diverse as possible. These factor loadings will be used as features for classification purposes.\nThere are some known regions in AD and MCI brain PET image which present hypoperfusion \u0353lower-intensity values than for a given normal controls \u0351NC\u0352 image\u0354. This fact have been taken into account, preselecting those voxels in the images which present greater difference between NC, MCI, or AD images, measuring this difference using the t-test. By calculating the factor loadings considering only these preselected regions of the brain, we are not modeling the whole brain using factors but some specific regions of the images. An interpretation of the meaning of the factor loadings and why they are useful to perform classification in brain image classification arises from Eq. \u03516\u0352. Each intensity value of the selected voxels after zero-mean normalization is expressed using a new basis using the factors F. These factors, which are estimated via factor analysis, are common to all the images. Factor loadings \u2333 are the weights which are also calculated using factor analysis and they are different for each image. When images of different classes are expressed using the basis given by factors F, they roughly present similar intraclass and different interclass values of factor loadings \u2333. We take advantage of this behavior and use factor loadings as features for classification purposes."}, {"section_title": "II.E. Classification", "text": "The goal of the classification task is to separate a set of binary labeled training data consisting of, in the general case, p-dimensional patterns v i and class labels y i\nso that a classifier is produced, which maps an object v i to its classification label y i . This classifier will be able to classify new examples v.\nThere are several different procedures to build the classification rule. We utilize the following classifiers in this work. 29, 30 "}, {"section_title": "II.E.1. Multivariate normal model: Quadratic discriminant function", "text": "We suppose that v denotes a p-component random vector of observations made on any individual; v 0 denotes a particular observed value of v and 1 and 2 denote the two populations involved in the problem. The basic assumption is that v has different probability distributions in 1 and 2 . Let the probability density of v be f 1 \u0351v\u0352 in 1 and f 2 \u0351v\u0352 in 2 . The simplest intuitive argument, termed the likelihood ratio rule, classifies v 0 as 1 whenever it has greater probability of coming from 1 than from 2 . This classification rule can be written as\nThe most general form of the model is to assume that i is a multivariate normal population with mean i and dispersion"}, {"section_title": "\u035110\u0352", "text": "Hence, on taking logarithms in Eq. \u03518\u0352, we find that the classification rule for this model is: Allocate v 0 to 1 if Q\u0351v 0 \u0352 \u03fe 0 and otherwise to 2 , where Q\u0351v\u0352 is the discriminant function\nSince the terms in Q\u0351v\u0352 include the quadratic form v\u0408\u0351\u233a 1 \u22121 \u2212 \u233a 2 \u22121 \u0352v, which will be a function of the squares of elements of v and cross products between pairs of them, this discriminant function is known as the quadratic discriminant.\nIn any practical application, the parameters 1 , 2 , \u233a 1 , and \u233a 2 are not known. Given two training sets, v 1 \u03511\u0352 , ... ,v n 1 \u03511\u0352 from 1 and v 1 \u03512\u0352 , ... ,v n 2 \u03512\u0352 from 2 , we can estimate these parameters using the sample mean and sample standard deviation."}, {"section_title": "II.E.2. Multivariate normal model: Linear discriminant function", "text": "The presence of two different population dispersion matrices renders difficult the testing of hypothesis about the population mean vectors; therefore, the assumption \u233a 1 = \u233a 2 = \u233a is a reasonable one in many practical situations. The In that case, contrary to the quadratic case, we estimate the pooled covariance matrix"}, {"section_title": "II.E.3. Support vector machines with linear kernels", "text": "Support vector machines \u0351SVMs\u0352 have recently been proposed for pattern recognition in a wide range of applications by its ability for learning from experimental data. SVMs separate a given set of binary labeled training data with a hyperplane that is maximally distant from the two classes \u0351known as the maximal margin hyperplane\u0352. Linear discriminant functions define decision hypersurfaces or hyperplanes in a multidimensional feature space\nwhere w is the weight vector and w 0 is the threshold. w is orthogonal to the decision hyperplane. The goal is to find the unknown weight vector w which defines the decision hyperplane. 30 Let v i , i =1,2, ... ,l be the feature vectors of the training set. These belong to two different classes, 1 or 2 , which for convenience in the mathematical calculations will be denoted as +1 and \u03ea1. If the classes are linearly separable, the objective is to design a hyperplane that classifies correctly all the training vectors. This hyperplane is not unique and it can be estimated maximizing the performance of the classifier, that is, the ability of the classifier to operate satisfactorily with new data. The maximal margin of separation between both classes is a useful design criterion. Since the distance from a point v to the hyperplane is given by z = \u0349g\u0351v\u0352\u0349 / \u0288w\u0288, the optimization problem can be reduced to the maximization of the margin 2 / \u0288w\u0288, with constraints by scaling w and w 0 so that the value of g\u0351v\u0352 is +1 for the nearest point in 1 and \u03ea1 for the nearest point in 2 . The constraints are the following:\nor, equivalently, minimizing the cost function J\u0351w\u0352 =1/ 2\u0288w\u0288 2 subject to"}, {"section_title": "III. RESULTS", "text": "The performance of the classification is tested on a set of 210 18 F-FDG PET images \u035152 normals, 114 MCI, and 53 AD\u0352 using the leave-one-out method: The classifier is trained with all but one image of the database and the remaining image, which is not used to define the classifier, is then categorized. In that way, all 18 F-FDG PET images are classified and the success rate is computed from the number of cor- rectly classified subjects. This cross-validation strategy has been previously used to assess the discriminative accuracy of different multivariate analysis methods applied to the early diagnosis of Alzheimer's disease, 26 discrimination of frontotemporal dementia from AD, 13 and in classifying atrophy patterns based on magnetic resonance imaging images. 31 Initially, threshold was set to 2000 voxels. Classification schemes used in this work are binary approaches; therefore, they are able to perform classification based on a training set of two previously labeled items. Nevertheless, we work with three classes of 18 F-FDG PET images, NC, MCI, and AD. Thus, we perform three different classification tasks. First, we consider only NC and AD images; second, NC versus MCI; and lastly we perform classification of NC subjects versus MCI-AD. The proposed methodology is compared to the voxel-as-features \u0351VAF\u0352 approach with linear support vector machine classifier. Figure 3 shows the correct rate versus the dimension m of factors F \u0351number of unobserved random variables F\u0352. Linear classifiers perform better than the multivariate normal model with quadratic discriminant function. In general, the accuracy rate increases concomitantly with the dimension of factors for small values of m. Best correct rates are achieved when m is in the range 13-20 when we use SVM with linear kernel and multivariate normal model with linear discriminant function. Specifically, a correct rate up to 95% is reached when m = 17 for SVM with linear kernels. For m \u03fe 20, performance of the classification task decreases with the dimension of the number of factors."}, {"section_title": "32-34", "text": ""}, {"section_title": "III.A. NC versus AD", "text": "The ADNI database excludes advanced AD affected subjects. The ADNI database represents some ideal laboratory conditions, as subjects are recruited following a restricted clinical criteria to select only early AD related issues. Therefore, these favorable conditions allows proving the discrimination ability of the classifier in the discrimination of the Alzheimer's disease in an early stage of the disease. Figure 4 shows the specificity and the sensitivity versus the number of factors for the three classifiers used in this work. The quadratic discriminant function reaches higher specificity values but very low sensitivity for a number of factors greater than 10. Specificity and sensitivity results obtained using the multivariate normal model with linear discriminant function and linear SVM are roughly the same except for number of factors greater than 30, where the linear discriminant function performs slightly better.\nWe have also drawn a scatter plot with samples from the loading factors when m = 2 in Fig. 5 . Figure 5 shows that even in the case in which two factor loadings are considered, most of the values for NC and AD images are clearly separated. Figure 6 shows the correct rate versus number of factor loadings F when NC and MCI patients are considered. Our method presents, in that case, lower performance than when the NC and AD images were considered. This is due to the fact that images labeled as MCI present subjects with very different intensity patterns: Patients with memory complaint verified by a study partner, subjects presenting abnormal memory function, frontotemporal dementia, possibly MCI to AD converters, and subjects with general cognition and functional performance sufficiently preserved such that a diagnosis of Alzheimer's disease cannot be made at the time of the screening visit. This wide variability in the MCI group makes the automatic computer-aided diagnosis method presented in this paper to work slightly worse than when only NC and AD subjects were considered. Equally to the NC-AD case, the quadratic classifier performs worse than the linear ones. Furthermore, from F =1 to F = 14, support vector machines with linear kernels performs better than the multivariate normal model with linear discriminant function, while for F \u03fe 16 they perform similarly. Our method presents better performance when the number of factor loadings is greater than 16. Specifically, the accuracy rate is greater than 85% and up to 88%. Figure 7 shows the specificity and the sensitivity versus the number of factors. Let us note that the number of NC and MCI images are 52 and 114, respectively. In that case, the use of imbalanced training data set causes a possible poor performance of specificity or sensitivity since they are sample prevalence dependent. Quadratic discriminant function performs very bad in that case, reaching very high specificity values and very low sensitivity for a small number of factors. On the other hand, when the number of factors is greater than 35, quadratic discriminant function is not suitable for classification purposes since one obtains a sensitivity nearly 1 and a very low specificity. In that case, SVM linear and linear discriminant functions perform much better. Figure 7\u0351a\u0352 shows that the linear discriminant function presents higher and more stable specificity values \u0351\u03f30.9\u0352 for a wide range of number of factors. Figure 7\u0351b\u0352 shows a similar behavior for SVM linear when sensitivity is considered: High and very stable values \u0351\u03f30.9\u0352 for a wide range of number of factors."}, {"section_title": "III.B. NC versus MCI", "text": ""}, {"section_title": "III.C. NC versus MCI,AD", "text": "The number of NC and \u0355MCI,DTA\u0356 images are 52 and 167, respectively; therefore, in that case, is inappropriate to calculate specificity or sensitivity values. A low specificity value in this largely unbalanced data set may not reflect a high false negative rate. Figure 8 depicts the accuracy rate versus the number of factors when NC subjects and a group composed of MCI and AD images are considered. In that case, SVM performs clearly better than the multivariate normal classifier with linear and quadratic discriminant function. Let note that this scenario is more difficult than classi- fying NC versus MCI or NC versus AD. In Sec. III D, the diversity of MCI subjects was pointed out. In that case, a new source of heterogeneity is included in the problem and this causes the accuracy rate to be slightly lower than when only NC and MCI were considered. Despite that, the proposed methodology reaches a correct rate greater than 85% when the number of factor loadings F ranges from 8 to 19. In that case, our method also outperforms the VAF approach when using SVM."}, {"section_title": "III.D. Performance for different threshold values \u03b5", "text": "Up to now, threshold has been set to 2000 voxels. Simulations were also performed selecting 3000, 4000, and 5000 voxels \u0351results not shown\u0352. In that case, the classification performance was slightly lower than when 2000 voxels were considered. Nevertheless, the computation time increases and the factor analysis algorithm is not able to reach convergence for large values of the number of factors. Therefore, either the relative convergence tolerance for varimax rotation \u03511.510 \u22128 \u0352 needs to be reduced or the iteration limit \u0351250 iterations\u0352 increased, leading to lower computational performance.\nWe also compare the performance of the method using different threshold values \u0351 = 2000, 1500, 1000, 500\u0352. Table I presents "}, {"section_title": "IV. DISCUSSION", "text": "Multinormal models with linear or quadratic discriminant functions exhibit a strong dependence on outliers as they consider that the distribution of feature vector for populations 1 and 2 follow a Gaussian distribution. Thus, this assumption could lead to poor results in the case in which the Gaussian assumption does not hold, as for instance, due to the presence of outliers in the populations. This is a typical problem which arises when the number of samples is small. Results in this work showed better performance when using SVM with linear kernel than in the case in which linear and quadratic discriminant functions were used. Specifically, in the case in which normal controls versus AD and MCI patients were considered, linear and quadratic discriminant functions obtained very poor performance. This could be due to the fact that including MCI and AD in the same group increases the heterogeneity of the feature vector and, therefore, the Gaussian assumption is not valid in that case. For this reason, and based on the results shown in the Sec. III, support vector machine with linear kernel is the preferred of the classifiers used in this work. In order to discuss the behavior of multinormal models for classification and the performance of this classification methods for different number of samples, simulations have been repeated using a k-fold cross-validation scheme with k = 2. Specifically, when k =2, N / 2 images are used as training sets and the remaining N / 2 images are considered test images. This allows checking the performance of the classification methods when a smaller number of images is considered, and therefore, it allows us to investigate the small size sample problem empirically. In Table II , the mean accuracy rate for m =10 to m = 20 number of factor loadings obtained using leave-one-out and k-fold with k = 2 are compared. The table shows that the best performance is obtained when leave-one-out is used as validation method which is an expected result. Nevertheless, let us note that good performance is also obtained when only half of the available images are used to train the classifier.\nIn this work, we do not propose an analytical procedure to estimate the value of the number of factor loadings. Nevertheless, some order selection technique could be possibly used to select the number of factor loadings. We estimate the performance of the classification task for a different number of . This procedure allows to easily estimate an \"optimal\" value of the threshold in the sense that it lets us select those values of the number of factor loadings which better discriminate between populations. Thus, the plot of the accuracy rate versus the threshold enables to visually select an useful value \u0351or a range of values\u0352 of for classification.\nOverfitting generally occurs when a model is excessively complex, such as having too many degrees of freedom, in relation to the amount of data available. In our case, this is a possible scenario when using support vector machines with nonlinear kernels or nonlinear discriminant functions to perform classification. For this reason, we only use SVM with linear kernel. Therefore, choosing linear classifiers, or in that case, SVM with linear kernel, is supported not only by the good performance results but also by the fact that this choice lets us to avoid possible overfitting.\nIn addition, we would like to point out that choosing a wide range of values and calculating the performance of the classification task is enough for our practical purposes for different reasons. On the one hand, the estimation of a very precise and specific number of factors m is not critical as the accuracy of the proposed methodology is high for a wide range of m values for the data set studied \u0351as instance, from m =2 to m = 20 when NC versus AD was considered, and m \u03fe 7 and m \u03fe 8 when NC versus MCI and NC versus MCI,AD\u0352. On the other hand, if we choose to estimate the threshold value using an analytical procedure, lastly, the performance of this analytical selection procedure based on a statistical criterion should be checked in terms of accuracy, specificity, and sensitivity for different values of the number of factor loadings in order to prove the validity of the method used to estimate the order model. And, actually, this is the experimental method we use in that work to estimate useful values of the number of factors. Furthermore, the estimation of the number of , equal to the training procedure, only needs to be performed once as a \"batch\" process. In Ref. 35 , a CAD system for an automatic evaluation of the neuroimages is presented. In that work, principal component analysis \u0351PCA\u0352 based methods are proposed as feature extraction techniques, enhanced by other linear approaches such as linear discriminant analysis or the measure of the Fisher discriminant ratio for feature selection. These features allow surmounting the so-called small sample size problem. We have used the PCA-based feature extraction method presented there as the input vector of a SVM classifier with linear kernel in order to compare to our proposed methodology. For the data set used in this work, best values of the correct rate, specificity, and sensitivity using both approaches are shown in Table III .\nDespite of the fact that an empirical estimation of m is enough for our purposes, we suggest a possible method to calculate an optimal m value using an analytical procedure studying the value of the log-likelihood. Figure 9 plots the log-likelihood versus m \u0351the number of factors\u0352 in the case in which NC versus MCI,AD images are considered. It can be seen how increasing m also increases the ability of the factor loadings to model the data more accurately \u0351the loglikelihood increases\u0352. Log-likelihood values when m is small \u0351from m =1 to m =7\u0352 present a great variability. This coincides with the same range of values in which our algorithm does not provide high correct rate, possibly due to the fact that the number of factors is not enough to model correctly the variability of the data.\nThe factor analysis model in Eq. \u03516\u0352 can also be specified as\nwhere cov\u0351z\u0352 is a p-by-p diagonal matrix of specific variances and p is the number of observed variables. Finally, we study the values of cov\u0351z\u0352 representing the mean of the diagonal values of matrix cov\u0351z\u0352 for each group NC versus AD, NC versus MCI, and NC versus MCI,AD in Fig. 10 . In these three cases the values are very similar and, as it was expected, they decrease as the number of factors increase, because for greater values of m, the factors are more capable to model the observed random variables."}, {"section_title": "V. CONCLUSIONS", "text": "In this work, an automatic procedure to assist the diagnosis of early Alzheimer's disease is presented. The proposed methodology is based on the selection of voxels of interest using the t-test and a posterior reduction of the feature dimension using factor analysis. Factor loadings were used as features of three different classifiers: Two multivariate Gaussian mixture models, with linear and quadratic discriminant function, and a support vector machine with linear kernel which was found to achieve the highest accuracy rate. The best results were obtained when normal and Alzheimer's disease subjects were considered. Specifically, an accuracy rate greater than 90% were obtained in that case for a wide range of number of factors. Furthermore, results were com- pared to the voxel-as-features and a PCA-based approach. The proposed methodology was found to perform clearly better. "}, {"section_title": "ACKNOWLEDGMENTS", "text": ""}]