[{"section_title": "", "text": "Respondents, nonresponse adjusted 4 Estimated bias 5 Rela  1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight adjusted for subsampling) and the mean of all sample cases (using the base weight adjusted for subsampling). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the respondent cases. 3 Bias in the sample mean is estimated as the product of the base weighted non-response rate and the difference between the mean of respondent cases (using base weight adjusted for subsampling) and the mean of nonrespondent cases (using the base weight adjusted for subsampling). 4 Base weight, adjusted for subsampling, multiplicity, and non-response. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 Categories were defined by quartiles. NOTE: Detail may not sum to totals because of rounding. FTE = full time equivalent. GASB = Governmental Accounting Standards Board. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2011-12 National Postsecondary Student Aid Study (NPSAS:12). Appendix J. Nonresponse Bias Analysis J-20  Estimated bias 5 Relative bias 2 Total female graduate enrollment 7 1 Estimated bias 5 Relative bias 2 Total undergraduate enrollment 7 1 Estimated bias 5 Relative bias 2 Total female graduate enrollment 7 1   Estimated bias 5 Relative bias 2 Total undergraduate enrollment 7 1  Estimated bias 5 Relative bias 2 Total female graduate enrollment 7 1  1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight adjusted for subsampling) and the mean of all sample cases (using the base weight adjusted for subsampling)."}, {"section_title": "NPSAS:12 Data File Documentation", "text": "2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the respondent cases. 3 Bias in the sample mean is estimated as the product of the base weighted non-response rate and the difference between the mean of respondent cases (using base weight adjusted for subsampling) and the mean of nonrespondent cases (using the base weight adjusted for subsampling). 4 Base weight, adjusted for subsampling, multiplicity, and non-response. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 Categories were defined by quartiles. NOTE: Detail may not sum to totals because of rounding. FTE = full time equivalent. FASB = Financial Accounting Standards Board. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2011-12 National Postsecondary Student Aid Study (NPSAS:12).\n\n2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the respondent cases. 3 Bias in the sample mean is estimated as the product of the base weighted non-response rate and the difference between the mean of respondent cases (using base weight adjusted for subsampling) and the mean of nonrespondent cases (using the base weight adjusted for subsampling). 4 Base weight, adjusted for subsampling, multiplicity, and non-response. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 Categories were defined by quartiles. NOTE: Detail may not sum to totals because of rounding. FTE = full time equivalent. FASB = Financial Accounting Standards Board. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2011-12 National Postsecondary Student Aid Study (NPSAS:12).    1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight adjusted for subsampling) and the mean of all sample cases (using the base weight adjusted for subsampling).\n\nJ-65  2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean (2) -Mean 3Percent receiving federal grant aid 3 1 NPSAS:12 Data File Documentation J-67  2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean (2) -Mean 3Total undergraduate enrollment 3 1   2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 1 Base weight, adjusted for multiplicity, subsampling and nonresponse. 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 3 Categories were defined by quartiles. NOTE: Detail may not sum to totals because of rounding. FTE = full time equivalent. FASB = Financial Accounting Standards Board. GASB = Governmental Accounting Standards Board. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2011-12 National Postsecondary Student Aid Study (NPSAS:12).\nJ-69 Table J-13. Institution nonresponse bias analysis after nonresponse and poststratification adjustments, by select variables for public less-than-2-year institutions: 2012\nJ-75  1After post-stratification adjustment Mean Difference Full sample, base weighted 2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 1After post-stratification adjustment Mean Difference Full sample, base weighted 2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean (2) -Mean 3Percent receiving institution grant aid 3 1 NPSAS:12 Data File Documentation J-77  1After post-stratification adjustment Mean Difference Full sample, base weighted 2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean (2) -Mean 3Total female undergraduate enrollment 3 1   1After post-stratification adjustment Mean Difference Full sample, base weighted 2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 Maryland,New Jersey,New York,Pennsylvania;Great Lakes = Illinois,Indiana,Michigan,Ohio,Wisconsin;Plains = Iowa,Kansas,Minnesota,Missouri,Nebraska,North Dakota,South Dakota;Southeast = Alabama,Arkansas,Florida,Georgia,Kentucky,Mississippi,North Carolina,South Carolina,Tennessee,Virginia,West Virginia;Southwest = Arizona,New Mexico,Oklahoma,Texas;Rocky Mountains = Colorado,Idaho,Montana,Utah,Wyoming;Far West = Alaska,California,Hawaii,Nevada,Oregon,Washington. 3 Categories were defined by quartiles. NOTE: Detail may not sum to totals because of rounding. FTE = full time equivalent. GASB = Governmental Accounting Standards Board. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2011-12 National Postsecondary Student Aid Study (NPSAS:12).\nJ-79  2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean (2) -Mean 3Percent receiving institution grant aid 3 1 NPSAS:12 Data File Documentation J-81  2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean (2) -Mean 3Total female undergraduate enrollment 3 1   2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 Maryland,New Jersey,New York,Pennsylvania;Great Lakes = Illinois,Indiana,Michigan,Ohio,Wisconsin;Plains = Iowa,Kansas,Minnesota,Missouri,Nebraska,North Dakota,South Dakota;Southeast = Alabama,Arkansas,Florida,Georgia,Kentucky,Mississippi,North Carolina,South Carolina,Tennessee,Virginia,West Virginia;Southwest = Arizona,New Mexico,Oklahoma,Texas;Rocky Mountains = Colorado,Idaho,Montana,Utah,Wyoming;Far West = Alaska,California,Hawaii,Nevada,Oregon,Washington. 3 Categories were defined by quartiles. NOTE: Detail may not sum to totals because of rounding. FTE = full time equivalent. GASB = Governmental Accounting Standards Board. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2011-12 National Postsecondary Student Aid Study (NPSAS:12).\nJ-83  2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean (2) -Mean 3Percent receiving institution grant aid 3 1 NPSAS:12 Data File Documentation J-85  2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean (2) -Mean 3Total female undergraduate enrollment 3 1  1 Base weight, adjusted for multiplicity, subsampling and nonresponse. 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas,Minnesota,Missouri,Nebraska,North Dakota,South Dakota;Southeast = Alabama,Arkansas,Florida,Georgia,Kentucky,Mississippi,North Carolina,South Carolina,Tennessee,Virginia,West Virginia;Southwest = Arizona,New Mexico,Oklahoma,Texas;Rocky Mountains = Colorado,Idaho,Montana,Utah,Wyoming;Far West = Alaska,California,Hawaii,Nevada,Oregon,Washington. 3 Categories were defined by quartiles. NOTE: Detail may not sum to totals because of rounding. FTE = full time equivalent. FASB = Financial Accounting Standards Board. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2011-12 National Postsecondary Student Aid Study (NPSAS:12).  2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean (2) -Mean 3Percent receiving institution grant aid 3 1   3Mean 1-Mean 3Mean (2) -Mean 3Total female undergraduate enrollment 3 1   3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean (2) -Mean 3Percent receiving institution grant aid 3 1   3Mean 1-Mean 3Mean (2) -Mean 3Total female undergraduate enrollment 3 1   3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean (2) -Mean 3Percent receiving student loan aid 3 1   3Mean 1-Mean 3Mean 2 1 Base weight, adjusted for multiplicity, subsampling and nonresponse. 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. \nJ-97  3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean 2 1 Base weight, adjusted for multiplicity, subsampling and nonresponse. 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington.   3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean (2) -Mean 3Percent receiving institution grant aid 3 1   3Mean 1-Mean 3Mean (2) -Mean 3Total female undergraduate enrollment 3 1   3Mean 1-Mean 3Mean 2 1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight) and the mean of all sample cases (using the base weight). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the respondent cases. 3 Bias in the sample mean is estimated as the product of the base-weighted non-response rate and the difference between the mean of respondent cases (using base weight) and the mean of nonrespondent cases (using the base weight). 4 Base weight, adjusted for multiplicity, unknown eligibility and non-response. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles computed at the institution level. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.   Relative bias 2 Institution region 6 New England  1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight) and the mean of all sample cases (using the base weight). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the respondent cases. 3 Bias in the sample mean is estimated as the product of the base-weighted non-response rate and the difference between the mean of respondent cases (using base weight) and the mean of nonrespondent cases (using the base weight). 4 Base weight, adjusted for multiplicity, unknown eligibility and non-response. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles computed at the institution level. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.    1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight) and the mean of all sample cases (using the base weight). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the respondent cases. 3 Bias in the sample mean is estimated as the product of the base-weighted non-response rate and the difference between the mean of respondent cases (using base weight) and the mean of nonrespondent cases (using the base weight). 4 Base weight, adjusted for multiplicity, unknown eligibility and non-response. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles computed at the institution level. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category. NOTE: Detail may not sum to totals because of rounding. FTB = first time beginner. FTE = full time equivalent. GASB = Governmental Accounting Standards Board.   and  1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight) and the mean of all sample cases (using the base weight). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the respondent cases. 3 Bias in the sample mean is estimated as the product of the base-weighted non-response rate and the difference between the mean of respondent cases (using base weight) and the mean of nonrespondent cases (using the base weight). 4 Base weight, adjusted for multiplicity, unknown eligibility and non-response. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles computed at the institution level. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category. NOTE: Detail may not sum to totals because of rounding. FTB = first time beginner. FTE = full time equivalent. GASB = Governmental Accounting Standards Board. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2011-12 National Postsecondary Student Aid Study (NPSAS:12).\n2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the respondent cases. 3 Bias in the sample mean is estimated as the product of the base-weighted non-response rate and the difference between the mean of respondent cases (using base weight) and the mean of nonrespondent cases (using the base weight). 4 Base weight, adjusted for multiplicity, unknown eligibility and non-response. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles computed at the institution level. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $   and   1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight) and the mean of all sample cases (using the base weight). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the respondent cases. 3 Bias in the sample mean is estimated as the product of the base-weighted non-response rate and the difference between the mean of respondent cases (using base weight) and the mean of nonrespondent cases (using the base weight). 4 Base weight, adjusted for multiplicity, unknown eligibility and non-response. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles computed at the institution level. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category. NOTE: Detail may not sum to totals because of rounding. FTB = first time beginner. FTE = full time equivalent. FASB = Financial Accounting Standards Board. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2011-12 National Postsecondary Student Aid Study (NPSAS:12).  \nand  1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight) and the mean of all sample cases (using the base weight). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the respondent cases. 3 Bias in the sample mean is estimated as the product of the base-weighted non-response rate and the difference between the mean of respondent cases (using base weight) and the mean of nonrespondent cases (using the base weight). 4 Base weight, adjusted for multiplicity, unknown eligibility and non-response. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles computed at the institution level. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.  Relative bias 2 Institution region 6 New England  1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight) and the mean of all sample cases (using the base weight). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the respondent cases. 3 Bias in the sample mean is estimated as the product of the base-weighted non-response rate and the difference between the mean of respondent cases (using base weight) and the mean of nonrespondent cases (using the base weight). 4 Base weight, adjusted for multiplicity, unknown eligibility and non-response. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles computed at the institution level. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category. NOTE: Detail may not sum to totals because of rounding. FTB = first time beginner. FTE = full time equivalent. FASB = Financial 1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight) and the mean of all sample cases (using the base weight). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the respondent cases. 3 Bias in the sample mean is estimated as the product of the base-weighted non-response rate and the difference between the mean of respondent cases (using base weight) and the mean of nonrespondent cases (using the base weight). 4 Base weight, adjusted for multiplicity, unknown eligibility and non-response. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles computed at the institution level. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $  1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight) and the mean of all sample cases (using the base weight). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the respondent cases. 3 Bias in the sample mean is estimated as the product of the base-weighted non-response rate and the difference between the mean of respondent cases (using base weight) and the mean of nonrespondent cases (using the base weight). 4 Base weight, adjusted for multiplicity, unknown eligibility and non-response. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles computed at the institution level. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category. NOTE: Detail may not sum to totals because of rounding. FTB = first time beginner. FTE = full time equivalent. FASB = Financial Accounting Standards Board. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2011-12 National Postsecondary Student Aid Study (NPSAS:12).  3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean 2 NPSAS:12 Data File Documentation J-183  3Mean 1-Mean 3Mean 2 1 Base weight, adjusted for multiplicity, unknown eligibility and nonresponse. 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 3 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles computed at the institution level. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.   3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean 2 1 Base weight, adjusted for multiplicity, unknown eligibility and nonresponse. 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 3 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles computed at the institution level. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category. NOTE: Detail may not sum to totals because of rounding. FTB = first time beginner. FTE = full time equivalent. GASB = Governmental Accounting Standards Board.\nJ-187   3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean 2 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 3 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles computed at the institution level. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.   3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean 2 1 Base weight, adjusted for multiplicity, unknown eligibility and nonresponse. 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 3 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles computed at the institution level. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.   3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean 2 1 Base weight, adjusted for multiplicity, unknown eligibility and nonresponse. 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 3 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles computed at the institution level. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.   3Mean 1-Mean 3Mean 2  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 3 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles computed at the institution level. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.   Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 1 Base weight, adjusted for multiplicity, unknown eligibility and nonresponse. 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 3 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles computed at the institution level. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.   Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 3 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles computed at the institution level. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.   Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 3 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles computed at the institution level. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.   Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 3 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles computed at the institution level. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.    1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight) and the mean of all sample cases (using the base weight). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the respondent cases. 3 Bias in the sample mean is estimated as the product of the base-weighted nonresponse rate and the difference between the mean of respondent cases (using base weight) and the mean of nonrespondent cases (using the base weight). 4 Base weight, adjusted for multiplicity, unknown eligibility and nonresponse. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.     1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight) and the mean of all sample cases (using the base weight). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the respondent cases. 3 Bias in the sample mean is estimated as the product of the base-weighted nonresponse rate and the difference between the mean of respondent cases (using base weight) and the mean of nonrespondent cases (using the base weight). 4 Base weight, adjusted for multiplicity, unknown eligibility and nonresponse. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.   1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight) and the mean of all sample cases (using the base weight). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the respondent cases. 3 Bias in the sample mean is estimated as the product of the base-weighted nonresponse rate and the difference between the mean of respondent cases (using base weight) and the mean of nonrespondent cases (using the base weight). 4 Base weight, adjusted for multiplicity, unknown eligibility and nonresponse. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.    1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight) and the mean of all sample cases (using the base weight). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the respondent cases. 3 Bias in the sample mean is estimated as the product of the base-weighted nonresponse rate and the difference between the mean of respondent cases (using base weight) and the mean of nonrespondent cases (using the base weight). 4 Base weight, adjusted for multiplicity, unknown eligibility and nonresponse. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category. NOTE: Detail may not sum to totals because of rounding. FTB = first time beginner. FTE = full time equivalent. GASB = Governmental Accounting Standards Board. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2011-12 National Postsecondary Student Aid Study (NPSAS:12).    1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight) and the mean of all sample cases (using the base weight). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the respondent cases. 3 Bias in the sample mean is estimated as the product of the base-weighted nonresponse rate and the difference between the mean of respondent cases (using base weight) and the mean of nonrespondent cases (using the base weight). 4 Base weight, adjusted for multiplicity, unknown eligibility and nonresponse. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.     1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight) and the mean of all sample cases (using the base weight).\n2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the respondent cases. 3 Bias in the sample mean is estimated as the product of the base-weighted nonresponse rate and the difference between the mean of respondent cases (using base weight) and the mean of nonrespondent cases (using the base weight). 4 Base weight, adjusted for multiplicity, unknown eligibility and nonresponse. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.     1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight) and the mean of all sample cases (using the base weight). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the respondent cases. 3 Bias in the sample mean is estimated as the product of the base-weighted nonresponse rate and the difference between the mean of respondent cases (using base weight) and the mean of nonrespondent cases (using the base weight). 4 Base weight, adjusted for multiplicity, unknown eligibility and nonresponse. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.     1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight) and the mean of all sample cases (using the base weight). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the respondent cases. 3 Bias in the sample mean is estimated as the product of the base-weighted nonresponse rate and the difference between the mean of respondent cases (using base weight) and the mean of nonrespondent cases (using the base weight). 4 Base weight, adjusted for multiplicity, unknown eligibility and nonresponse. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.      1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight) and the mean of all sample cases (using the base weight). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the respondent cases. 3 Bias in the sample mean is estimated as the product of the base-weighted nonresponse rate and the difference between the mean of respondent cases (using base weight) and the mean of nonrespondent cases (using the base weight). 4 Base weight, adjusted for multiplicity, unknown eligibility and nonresponse. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.      1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight) and the mean of all sample cases (using the base weight). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the respondent cases. 3 Bias in the sample mean is estimated as the product of the base-weighted nonresponse rate and the difference between the mean of respondent cases (using base weight) and the mean of nonrespondent cases (using the base weight). 4 Base weight, adjusted for multiplicity, unknown eligibility and nonresponse. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.     1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight) and the mean of all sample cases (using the base weight). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the respondent cases. 3 Bias in the sample mean is estimated as the product of the base-weighted nonresponse rate and the difference between the mean of respondent cases (using base weight) and the mean of nonrespondent cases (using the base weight). 4 Base weight, adjusted for multiplicity, unknown eligibility and nonresponse. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category. NOTE: Detail may not sum to totals because of rounding. FTB = first time beginner. FTE = full time equivalent. FASB = Financial Accounting Standards Board. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2011-12 National Postsecondary Student Aid Study (NPSAS:12).\nJ-299  3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean 2 NPSAS:12 Data File Documentation J-301  3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean 2 1 Base weight, adjusted for multiplicity, unknown eligibility and nonresponse. 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 3 Enrollment and Stafford Loan categories were defined by quartiles. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category. \nJ-303  3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean 2 1 Base weight, adjusted for multiplicity, unknown eligibility and nonresponse. 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 3 Enrollment and Stafford Loan categories were defined by quartiles. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.   3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean 2 Maryland,New Jersey,New York,Pennsylvania;Great Lakes = Illinois,Indiana,Michigan,Ohio,Wisconsin;Plains = Iowa,Kansas,Minnesota,Missouri,Nebraska,North Dakota,South Dakota;Southeast = Alabama,Arkansas,Florida,Georgia,Kentucky,Mississippi,North Carolina,South Carolina,Tennessee,Virginia,West Virginia;Southwest = Arizona,New Mexico,Oklahoma,Texas;Rocky Mountains = Colorado,Idaho,Montana,Utah,Wyoming;Far West = Alaska,California,Hawaii,Nevada,Oregon,Washington. 3 Enrollment and Stafford Loan categories were defined by quartiles. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.   3Mean 1-Mean 3Mean 2 NPSAS:12 Data File Documentation J-311  3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean 2 Maryland,New Jersey,New York,Pennsylvania;Great Lakes = Illinois,Indiana,Michigan,Ohio,Wisconsin;Plains = Iowa,Kansas,Minnesota,Missouri,Nebraska,North Dakota,South Dakota;Southeast = Alabama,Arkansas,Florida,Georgia,Kentucky,Mississippi,North Carolina,South Carolina,Tennessee,Virginia,West Virginia;Southwest = Arizona,New Mexico,Oklahoma,Texas;Rocky Mountains = Colorado,Idaho,Montana,Utah,Wyoming;Far West = Alaska,California,Hawaii,Nevada,Oregon,Washington. 3 Enrollment and Stafford Loan categories were defined by quartiles. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.   3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean 2 3Mean 1-Mean 3Mean 2  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 1 Base weight, adjusted for multiplicity, unknown eligibility and nonresponse. 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 3 Enrollment and Stafford Loan categories were defined by quartiles. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.   Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 NPSAS:12 Data File Documentation J-321  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 1 Base weight, adjusted for multiplicity, unknown eligibility and nonresponse. 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 3 Enrollment and Stafford Loan categories were defined by quartiles. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.   Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 1 Base weight, adjusted for multiplicity, unknown eligibility and nonresponse. 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 3 Enrollment and Stafford Loan categories were defined by quartiles. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.   Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 1 Base weight, adjusted for multiplicity, unknown eligibility and nonresponse. 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 3 Enrollment and Stafford Loan categories were defined by quartiles. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.   Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 1 Base weight, adjusted for multiplicity, unknown eligibility and nonresponse. 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 3 Enrollment and Stafford Loan categories were defined by quartiles. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.   Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 NPSAS:12 Data File Documentation J-335  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 NPSAS:12 Data File Documentation J-337  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 1 Base weight, adjusted for multiplicity, unknown eligibility and nonresponse. 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 3 Enrollment and Stafford Loan categories were defined by quartiles. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.   Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 NPSAS:12 Data File Documentation J-339  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 NPSAS:12 Data File Documentation J-341  Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 1 Base weight, adjusted for multiplicity, unknown eligibility and nonresponse. 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 3 Enrollment and Stafford Loan categories were defined by quartiles. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category.  NPSAS:12 Data File Documentation J-343  1 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = California, Nevada, Oregon, Washington; Outlying Areas = Alaska, Hawaii, and Puerto Rico. Alaska and Hawaii were reclassified from the West to the Outlying Areas for the purposes of NPSAS.  The 2011-12 National Postsecondary Student Aid Study (NPSAS:12) sampling design was a stratified two-stage design. A stratified sample of postsecondary institutions was selected with probabilities proportional to a composite measure of size at the first stage, and a stratified systematic sample of students was selected from sample institutions at the second stage. At the first stage, about 24 percent of the eligible institutions were selected, but the institution sampling rates varied considerably by institution sampling strata. 1 At the second stage, potential first-time beginners and students at for-profit institutions were sampled at higher rates than other students. Statistical analyses were conducted with software that properly accounts for this complex survey design. Most commonly used statistical computing packages (e.g., SAS and SPSS) assume that data were obtained from a simple random sample-they assume that the observations were independent and identically distributed. When the data were collected according to a complex sampling design, the simple random sampling assumption usually led to an underestimate of the sampling variance, which led to artificially small confidence intervals and liberal hypothesis test results (i.e., rejecting the null hypothesis when it is in fact true more often than indicated by the nominal Type I error level; Carlson, Johnson, and Cohen 1993). Statistical strategies that were developed to address this issue include first-order Taylor-series expansion of the variance equation, bootstrap replication, balanced repeated replication, and the Jackknife approach (see, e.g., Wolter 1985). Special-purpose software packages that were developed for analysis of complex sample survey data include SUDAAN, WesVar, R, and Stata. Evaluations of the relative performances of these packages were reported by Cohen (1997). SAS and SPSS added procedures to address this issue, as well. The National Center for Education Statistics (NCES) also developed a software tool called PowerStats for analysis of complex survey data. Following is a list of these software packages and their websites, which have more information about the software and how to purchase or use them: \u2022 SUDAAN: http://www.rti.org/sudaan; \u2022 WesVar: http://www.westat.com/expertise/information_systems/WesVar; \u2022 Stata: http://www.stata.com; \u2022 SAS: http://www.sas.com; \u2022 SPSS: http://www.spss.com; \u2022 R: http://cran.r-project.org/web/packages/survey/index.html; and \u2022 NCES PowerStats: http://nces.ed.gov/datalab/index.aspx. When computing standard errors by using Taylor-series approximation, analysts should use the variables ANALSTR and ANALPSU in specifying analysis strata and analysis primary sampling units (PSUs). This method of variance estimation may overestimate the variance because it does not always account for the finite population correction (FPC) at the institution stage of sampling. Additionally, the analyst should specify the study weight (WTA000). Below is an example of generic SUDAAN code to produce estimates and standard errors, using Taylor-series approximation and not accounting for the FPC. The symbols /* and */ in the code indicate the beginning and end of a comment, respectively. The dataset must be sorted by the analysis strata and analysis PSUs in order to run this analysis. When computing standard errors by using Taylor-series approximation, the analyst could, alternatively, use the variables FANALSTR, FANALPSU, FANALSSU, and PSUCOUNT in specifying analysis strata, analysis PSUs, analysis secondary sampling units (SSUs), and estimated number of PSUs in the stratum. This method of variance estimation accounts for the FPC at the institution stage of sampling. Below is an example of generic SUDAAN code to produce estimates and standard errors with the use of Taylor-series approximation accounting for the FPC. The dataset must be sorted by the analysis strata, analysis PSUs, and analysis SSUs in order to run this analysis. When computing standard errors by using bootstrap replication, the analyst should specify the bootstrap weights WTA001-WTA200 in addition to specifying WTA000. For domains with very small sample sizes, e.g., students in tribal colleges, all replicate weights may be zero, and hence, standard errors cannot be computed (the software will give an error). In this situation, the domain of interest could be collapsed or combined with another domain in order to have sufficient sample size for computing standard errors. Below is an example of generic SUDAAN code to produce estimates and standard errors with the use of bootstrap replication. The dataset does not have to be sorted to run this analysis. R survey package 1 mydesign<-svrepdesign( type=\"BRR\", weights=~WTA000, repweights= \"WTA00 \", combined.weights=FALSE) 1 For the R survey package, \"mydesign\" can be renamed to any name for an R object to hold the specification of the survey design. For the without replacement design, the R survey package does not account for the second stage of sampling. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2011-12 National Postsecondary Student Aid Study (NPSAS:12). A quick analysis of NPSAS:12 data can be performed without using one of the software packages for analysis of complex survey data. The design effects tables that are presented in appendix L can be used to make approximate adjustments to the standard errors of survey statistics computed with the use of standard software packages that assume simple random sampling designs. For example, table L-8 shows design effects (DEFFs) and square roots of design effects (DEFTs) for undergraduate students at private nonprofit 4-year doctorate-granting institutions. If one has computed a statistic not listed in the table (e.g., mean Stafford Loan amount) for this domain of students, then the summary statistics from table L-8 suggest that the standard error computed from the statistical software package be multiplied by a survey DEFT of about 1.72 (the median for this domain). The range of DEFTs shown in table L-11 for this domain, however, is 0 to 2.66; therefore, one cannot be confident about the actual design-based standard error without performing the analysis with one of the software packages specifically designed for analysis of data from complex sample surveys.       This cycle of NPSAS occurs 4 years after the last data collection in response to the need to collect periodic information on financial aid programs. The large-scale and rapid changes in federal policy concerning postsecondary student aid necessitate such frequent studies. Eligibility restrictions change, size of grant and loan amounts fluctuate, and the balance between various aid options changes dramatically. A recurring study like NPSAS is essential to helping predict future costs for financial aid because loan programs create continued obligations for the federal government as long as the loans are being repaid. The major purpose of NPSAS:12 field test was to plan, implement, and evaluate operational and methodological procedures, instruments, and systems that are proposed for use in the full-scale study, particularly procedures that had not been previously tested. Several experiments were conducted during the field test, including: \u2022 Propensity design experiment-an experiment to predict a sample member's propensity to respond to the survey and to tailor incentives to increase responses of low propensity groups; \u2022 Informational video-testing an informational video to communicate information about the study and how to respond; and \u2022 Instrumentation experiments-testing new data collection features and implementations, such as visual sliders scales and scale start positions. Chapter 1 of this report provides an overview of NPSAS. Chapter 2 describes the sampling design and the steps used to select institution and student samples. Chapter 3 describes the design, outcomes, and evaluation activities associated with institution data collection. Chapter 4 provides details on the student interview design, data collection, outcomes and evaluations. Chapter 5 includes information on the student records matching activities and outcomes. Postdata collection data file processing is described in Chapter 6, including editing, weighting, imputation, bias analysis, and variance estimation. Chapter 7 gives recommendations for the full-scale study. aid was implemented beginning with NPSAS:2000. Institutions that offered only correspondence courses, provided these same institutions were also eligible to distribute federal Title IV student aid, were first included in NPSAS:04. Institutions in Puerto Rico were not originally included in NPSAS in 1987, but were subsequently added to administrations of NPSAS between 1993 and 2008. Puerto Rico institutions are not included in the 2012 administration of NPSAS."}, {"section_title": "Appendix J. Nonresponse Bias Analysis", "text": "NPSAS:12 Data File Documentation J-43  8,865 or more 110 20 44.49 40.31 86.69 -4.19 -10.38 -4.19 -10.38 44.49 42.58 -1.91 -4.48 \u2020 Not applicable. # Rounds to zero. \u2021 Row is suppressed. Contains less than 5 unweighted nonrespondents. * p < .05. 1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight adjusted for subsampling) and the mean of all sample cases (using the base weight adjusted for subsampling). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the respondent cases. 3 Bias in the sample mean is estimated as the product of the base weighted non-response rate and the difference between the mean of respondent cases (using base weight adjusted for subsampling) and the mean of nonrespondent cases (using the base weight adjusted for subsampling). 4 Base weight, adjusted for subsampling, multiplicity, and non-response. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 Categories were defined by quartiles. NOTE: Detail may not sum to totals because of rounding. FTE = full time equivalent. FASB = Financial Accounting Standards Board. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2011-12 National Postsecondary Student Aid Study (NPSAS:12).\n\n\nNPSAS:12 Data File Documentation J-131   1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight) and the mean of all sample cases (using the base weight). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the respondent cases. 3 Bias in the sample mean is estimated as the product of the base-weighted non-response rate and the difference between the mean of respondent cases (using base weight) and the mean of nonrespondent cases (using the base weight). 4 Base weight, adjusted for multiplicity, unknown eligibility and non-response. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 Enrollment and Stafford Loan categories were defined by quartiles. Categories of institution percent receiving grants, graduation rate, percentages of core revenues, and expenses per enrollment were defined using quartiles computed at the institution level. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category. NOTE: Detail may not sum to totals because of rounding. FTB = first time beginner. FTE = full time equivalent. GASB = Governmental Accounting Standards Board. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2011-12 National Postsecondary Student Aid Study (NPSAS:12).       \u2021 Row is suppressed. Contains less than 30 unweighted nonrespondents. * p < .05. 1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight) and the mean of all sample cases (using the base weight)."}, {"section_title": "J-49", "text": "Table J-9. Institution nonresponse bias analysis before and after weight adjustment, by select variables for private for-profit less-than-2-year institutions: 2012   Percent receiving student loan aid 7 1-36 Average net price among students receiving grant or scholarship     Private institution tuition and fees as percent of core revenues (FASB reporting) 7 63 or less  1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight adjusted for subsampling) and the mean of all sample cases (using the base weight adjusted for subsampling). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the respondent cases. 3 Bias in the sample mean is estimated as the product of the base weighted non-response rate and the difference between the mean of respondent cases (using base weight adjusted for subsampling) and the mean of nonrespondent cases (using the base weight adjusted for subsampling). 4 Base weight, adjusted for subsampling, multiplicity, and non-response. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 Categories were defined by quartiles. NOTE: Detail may not sum to totals because of rounding. FTE = full time equivalent. FASB = Financial Accounting Standards Board. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2011-12 National Postsecondary Student Aid Study (NPSAS:12).   1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight adjusted for subsampling) and the mean of all sample cases (using the base weight adjusted for subsampling)."}, {"section_title": "Appendix J. Nonresponse Bias Analysis J-54", "text": ""}, {"section_title": "J-59", "text": "2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the respondent cases. 3 Bias in the sample mean is estimated as the product of the base weighted non-response rate and the difference between the mean of respondent cases (using base weight adjusted for subsampling) and the mean of nonrespondent cases (using the base weight adjusted for subsampling). 4 Base weight, adjusted for subsampling, multiplicity, and non-response. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 Categories were defined by quartiles. NOTE: Detail may not sum to totals because of rounding. FTE = full time equivalent. FASB = Financial Accounting Standards Board. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2011-12 National Postsecondary Student Aid Study (NPSAS:12)."}, {"section_title": "Variable", "text": "After nonresponse weight adjustment mean; respondents, nonresponse adjusted 1 1After poststratification adjustment Mean Difference Full sample, base weighted 2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 NPSAS:12 Data File Documentation J-71  2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 1 Base weight, adjusted for multiplicity, subsampling and nonresponse. 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington.   1After post-stratification adjustment Mean Difference Full sample, base weighted 2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 NPSAS:12 Data File Documentation J-73  1After post-stratification adjustment Mean Difference Full sample, base weighted 2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean (2) -Mean 3Percent receiving student loan aid 3 1   1After post-stratification adjustment Mean Difference Full sample, base weighted 2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean (2) -Mean 3Percentage of full-time, first time degree/certificate-seeking undergraduate students who received any grant aid 3 1 Base weight, adjusted for multiplicity, subsampling and nonresponse. 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 3 Categories were defined by quartiles. NOTE: Detail may not sum to totals because of rounding. FTE = full time equivalent. GASB = Governmental Accounting Standards Board. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2011-12 National Postsecondary Student Aid Study (NPSAS:12)."}, {"section_title": "Appendix J. Nonresponse Bias Analysis J-138", "text": ""}, {"section_title": "Appendix J. Nonresponse Bias Analysis J-152", "text": ""}, {"section_title": "J-223", "text": ""}, {"section_title": "Appendix J. Nonresponse Bias Analysis J-250", "text": ""}, {"section_title": "Student Universe", "text": "Students eligible for NPSAS:12 field test were those who attended a NPSAS eligible institution during the 2010-11 academic year and who were \u2022 enrolled in either: (a) an academic program; (b) at least one course for credit that could be applied toward fulfilling the requirements for an academic degree; (c) exclusively noncredit remedial coursework but determined by the institution to be eligible for Title IV aid; or (d) an occupational or vocational program that required at least 3 months or 300 clock hours of instruction to receive a degree, certificate, or other formal award; \u2022 not currently enrolled in high school; and \u2022 not solely enrolled in a General Educational Development (GED) or other high school completion program."}, {"section_title": "Institution and Student Samples", "text": "NPSAS:12 field test institution sample included all levels (less-than-2-year, 2-year and 4-year) and controls (public, private nonprofit and private for-profit) of Title IV eligible postsecondary institutions in the United States. The student sample was randomly selected from lists of students enrolled at sampled institutions between July, 1 2010 and April 30, 2011."}, {"section_title": "Institution Sample", "text": "The institution samples for the field test and full-scale studies were selected simultaneously, prior to the field test study, using stratified random sampling with probabilities proportional to a composite measure of size (Folsom, Potter, and Williams 1987). Institution measure of size was determined using annual enrollment data from the most recent IPEDS 12-Month Enrollment Component and first time beginner (FTB) enrollment data from the most recent IPEDS Fall Enrollment Component. Using composite measure of size sampling ensures that target sample sizes are achieved within institution and student sampling strata, while also achieving approximately equal student weights across institutions. For NPSAS:12, the field test institution sample was selected using statistical procedures rather than purposively as had been done in past NPSAS cycles. This provided more control to ensure that the field test and the full-scale institution samples have similar characteristics. It also allowed inferences to be made to the target population, supporting the analytic needs of the field test experiments and instrument. From the stratified frame, a total of 1,970 institutions were to participate in either the fullscale or field test study. From the 1,970 institutions selected for participation a subsample of 300 institutions was selected using simple random sampling within institution strata to comprise the field test sample. The remaining 1,670 institutions comprise the sample for the full-scale study. This sampling process eliminated the possibility that an institution would be burdened with participation in both the field test and full-scale samples and maintained the representativeness of the full-scale sample. Figure 1 shows the flow of institution sampling activities. Unlike in past NPSAS cycles no schools were designated as \"certainty institutions,\" that is, while there were some certainty institutions in the initial sample selection, these institutions were not guaranteed to be selected for the full-scale study. All institutions in NPSAS:12 sample were eligible for either the full-scale or field test studies. The institution strata used for the sampling design, which are based on institution level, control, and highest level of offering, include: Instructional Programs (CIP) code of the largest program at less-than-2-year institutions; (5) the Office of Business Economics Region from the IPEDS header file (Bureau of Economic Analysis of the U.S. Department of Commerce Region); (6) state and system for states with large systems, e.g., the SUNY and CUNY systems in New York, the state and technical colleges in Georgia, and the California State University and University of California systems in California; and (7) the institution measure of size. The objective of this implicit stratification was to approximate proportional representation of institutions on these measures. Table 1 shows sampled institutions' eligibility rates, rates of providing student enrollment lists, and past NPSAS participation by institution stratum. Overall, almost 100 percent of the sampled institutions met the eligibility requirements; of those, approximately 51 percent provided enrollment lists. The institution response rate is below what has been obtained for other NPSAS field tests, primarily due to the termination of sampling and institution contacting once a sufficient number of lists for sampling were received. "}, {"section_title": "Student Sample", "text": "NPSAS:12 year covers the time period between July 1 and June 30, to coincide with the federal financial aid award year. To facilitate timely completion of data collection and data file preparation, institutions were asked to submit enrollment lists for all eligible students enrolled at any time between July 1 and April 30 or, for institutions with continuous enrollment, between July 1 and March 31. The March 31 deadline for continuous enrollment institutions was used for the field test due to the compressed data collection schedule and will not be used in the full-scale. Because previous cycles of NPSAS have shown that the terms beginning in May and June add little to enrollment and aid totals, May-June starters were excluded to allow institutions to provide enrollment lists earlier which, in turn, allowed the student interview process to begin earlier. In the full-scale study, poststratification of survey estimates based on IPEDS records on enrollment and National Student Loan Data System (NSLDS) records on financial aid distributed will adjust for the survey year's inclusion of any terms that begin by April 30 and the consequent exclusion of a small number of students newly enrolled in May or June. To create the student sampling frame, each participating institution was asked to submit a list of eligible students. The requests for student enrollment lists specifically indicated how institutions should handle special cases, such as students taking only correspondence or distance learning courses, and foreign exchange, continuing education, extension division, and nonmatriculated students. The data required for each enrollee were the following: \u2022 student's name; \u2022 student ID; \u2022 Social Security number; \u2022 date of birth; \u2022 date of high school graduation (month and year); \u2022 degree level during the last term of enrollment (undergraduate, masters, doctoralresearch/scholarship/other, doctoral-professional practice, or other graduate); \u2022 class level if undergraduate (first year, second year, third year, fourth year, or fifth year or higher); \u2022 major; \u2022 CIP code; \u2022 indicator of whether the institution received an Institutional Student Information Record (ISIR) ( an electronic record summarizing the results of the student's Free Application for Federal Student Aid [FAFSA] processing) from the Central Processing System (CPS); \u2022 FTB status; and \u2022 contacting information, such as cell phone number, local telephone number and address, permanent telephone number and address, campus e-mail address, and permanent e-mail address. Requesting contact information for eligible students prior to sampling allowed for student record abstraction and student interviewing to begin shortly after sample selection which helped to ensure the management of the field test schedule for data collection, data processing, and file development. Student sample sizes for the field test were formulated to ensure representation of various types of students. Specifically, the sample included a large number of potential first-time beginners to provide a sufficient sample size to obtain a sample yield of at least 1,000 students for BPS field test. As shown in table 2, NPSAS:12 field test sample included 4,530 students of which 4,130 were potential FTBs, 200 were other undergraduate students, and 200 were graduate students. There were six student sampling strata: \u2022 two sampling strata for undergraduate students: \u2212 FTB and \u2212 other undergraduate; \u2022 four sampling strata for graduate students \u2212 masters; \u2212 doctoral-research/scholarship/other; \u2212 doctoral-professional practice; and \u2212 other graduate students. The student sampling procedures implemented in the field test were comparable to those planned for the full-scale study. Students were sampled at fixed rates according to student education level and institution sampling strata. Sample yield was monitored and sampling rates were adjusted when necessary, resulting in a statistical sample of the required sample size for the field test. The same approach will be used for the full-scale study. The expected and actual student sample sizes, by student type and level of institution, are shown in table 2. Overall, the application of predetermined sampling rates yielded a sample that met expectations. Graduate students, 2-year institutions, and FTB students yielded overall samples slightly above expectations. Sample sizes will be monitored closely in the full-scale study to ensure that the desired sample distribution is achieved. Institutions providing student enrollment lists were asked to correct any of the following problems: \u2022 the education level of each student was not included or was unclear; \u2022 first-time beginning students were not identified (unless the institution explicitly indicated that no such students were enrolled at the institution); or \u2022 the number of students listed was inconsistent with the latest IPEDS data as described below. Reviewing the counts from the enrollment lists provided by institutions against the student full-year enrollment counts from the 2009 IPEDS 12 Month Enrollment Component was performed as a quality control checks were performed. Lists were reviewed and unduplicated, if necessary, to ensure students were listed only once per list. . Since IPEDS does not have unduplicated annual FTB counts, the unduplicated FTB counts from the enrollment lists provided by institutions were checked against adjusted full-year enrollment counts. Percentages of FTBs computed from the 2009 IPEDS Fall Enrollment Component were multiplied by full-year undergraduate enrollment counts from the 2009 IPEDS 12 Month Enrollment Component to estimate the numbers of full-year FTBs. Upper and lower bounds were formed around the IPEDS counts to create a range. If the student enrollment list count was within the prescribed range, the enrollment list passed quality control; otherwise, it failed. 3 For undergraduates and graduates, the list failed if its count was either 50 percent less or 50 percent more than the IPEDS enrollment count. For FTBs, the failure occurred if the list count was either 50 percent less or 100 percent more than the adjusted IPEDS enrollment count. In the interest of time, student samples for the field test were selected from the first 150 institutions that provided lists that passed the above quality control checks. Table 3 shows the field test student sample by institution type. About 59 percent of the overall student sample was enrolled in public institutions, 28 percent were enrolled in private forprofit institutions, and 12 percent were enrolled in private nonprofit institutions. Approximately 48 percent of all students sampled were enrolled in 2-year institutions. "}, {"section_title": "First-Time Beginners Sample", "text": "To be eligible for BPS field test, students must have begun their postsecondary education for the first time after completing high school on or after July 1, 2010. Close attention was paid to accurately identifying FTBs in NPSAS field test to avoid unacceptably high rates of misclassification (e.g., false positives). 4 High rates of misclassification can and have resulted in (1) excessive cohort loss, (2) excessive cost to \"replenish\" the sample, and (3) an inefficient sample design (excessive oversampling of \"potential\" FTBs) to compensate for anticipated misclassification error. To address this concern, participating institutions were asked to provide additional information for all eligible students and matching to administrative databases was utilized to further eliminate false positives prior to sample selection. Participating institutions were asked to provide the FTB status and high school graduation date for every enrolled eligible student. High school graduation date was used to remove students from the frame that did not meet NPSAS eligibility requirement of high school completion. FTB status along with class level and student level were used to exclude misclassified FTB students in their third year or higher and/or not those who were not an undergraduate student. FTB status along with date of birth were used to identify students older than 18 to send for presampling matching to administrative databases. If the FTB indicator was not provided for a student on the lists but the student was 18 years of age or younger and did not appear to be dually enrolled in high school, the student was sampled as an FTB. Otherwise, if the FTB indicator was not provided for a student on the list and the student was over the age of 18, then the student was sampled as an \"other undergraduate\" but would be included in BPS cohort if identified during the student interview as an FTB. Prior to sampling, students over the age of 18 listed as potential FTBs were matched to NSLDS records to determine if any had a federal financial aid history predating NPSAS year (earlier than July 1, 2010 for the field test). Since NSLDS maintains current records of all Title IV federal grant and loan funding, any student with data showing disbursements from the prior year could be reliably excluded from the sampling frame of FTBs. Given that about 60 percent of FTBs receive some form of Title IV aid in their first year, this matching process could not exclude all listed FTBs with prior enrollment, but significantly improved the accuracy of the list prior to sampling, yielding fewer false positives. After undergoing NSLDS matching, students over the age of 18 still listed as potential FTBs were matched to the National Student Clearinghouse (NSC) for further narrowing of potential FTBs based on evidence of earlier enrollment. Matching to NSLDS identified about 19 percent of cases as false positives and matching to NSC identified about 14 percent of cases as false positives. In addition to NSLDS and NSC, a subset of potential FTBs on the student sampling frame was sent to CPS for matching to evaluate the benefit of the CPS match for the full-scale study. Of the 58,690 students sent, CPS identified about 10 percent as false positives (table 4). Overall, matching to all sources identified about 32 percent of potential FTB students over the age of 18 as false positives, with many of the false positives identified by CPS also identified by NSLDS or NSC. The matching appeared most effective among public 2-year and private for-profit institutions. While public less-than 2-year institutions have a high percent of false positives, they represent a small percentage of the total sample. Since this presampling matching was new, the FTB sample size was set high to ensure that a sufficient number of true FTBs would be interviewed. In addition, FTB selection rates were set taking into account the error rates observed in NPSAS:04 and BPS:04/06 within each sector. These rates were adjusted to reflect the improvement in the accuracy of the frame from the NSLDS and NSC record matching. Sector-level FTB error rates from the field test will be used to help determine the rates necessary for full-scale student sampling.  "}, {"section_title": "Chapter 3. Institution Data Collection Design, Outcomes, and Evaluation", "text": "Institutions selected for NPSAS:12 field test were contacted and encouraged to participate in the study by institution contactors who worked with institution coordinators (ICs) to obtain enrollment lists and, subsequently, student records for the students selected for the field test sample. An institution website was used to aid in data collection, offering a variety of methods through which institutions could provide the requested data. Outcomes and evaluations of the field test design and systems provided information for consideration in planning for the full-scale study."}, {"section_title": "Institution Data Collection Design and Systems", "text": "NPSAS:12 field test institution data collection was facilitated by institution contactors and an institution website and occurred in a stepwise fashion, beginning with the designation of an IC, then submission of an enrollment list, and finally submission of student records data for sampled students. This section provides details of the design and systems used for institution data collection."}, {"section_title": "Institution Recruitment and Student Enrollment List Acquisition Design and Systems", "text": "The initial step in institution data collection was to verify the contact information for the chief administrator (CA) of each sampled institution by telephone. The updated contact information was entered into the Institution Contacting System (ICS), which was used to track the status of each institution and schedule appropriate follow-up. Each interaction with the institution as well as the overall progress of institution recruitment and list collection was recorded in the ICS. Once contact information for the CA was verified, institution recruitment and student list acquisition proceeded in the following three steps: 1. The CA of each sampled institution was sent a letter, asking them to designate an IC to coordinate data collection at their school. 2. The IC was mailed a request to complete the Institution Registration Page (IRP) on NPSAS institution website. Term information used to set a deadline for providing the student list was entered on the IRP. 3. The IC uploaded the student enrollment list to the NPSAS website (or, alternatively provided as an encrypted, password-protected file via e-mail or a secure fax transmission to an electronic fax server)."}, {"section_title": "Student Records Collection Design and Systems", "text": "NPSAS:12 field test used four modes for student record abstraction: (1) Case Mode, in which institution staff entered data directly into the web-based system one student at a time; (2) Grid Mode, in which institution staff entered data directly into the web-based system for multiple students at a time in a format resembling a grid; (3) Template Upload, in which institution staff downloaded an Excel template, entered data into it, then uploaded it back to the website; and (4) Data Files Upload, in which institution staff created data files following provided specifications. The student records instrument consisted of four components grouped by topic: (1) Contact Information; (2) Student Info and Budget, which collected student characteristics and need analysis information; (3) Enrollment, which collected degree program and progress, term, tuition and placement test information; and (4) Aid Awarded, which collected information about federal, state, institution, graduate, and government/private aid. ICs were first asked to complete their institution-level information (term system, placement tests, and institution grants and scholarships). After providing these data, the ICs could provide data for students and choose which mode worked best for the institution: Case-Mode, Grid-Mode, Template Upload, or Data Files Upload."}, {"section_title": "Institution Data Collection", "text": "This section provides a summary of information and materials provided to institutions on the website, a summary of log-in procedures, and a summary of data security measures\nBased on feedback from institutions, institution contactors and other project staff, minor changes will be implemented for the full-scale study. References to \"Title IV eligibility\" will be revised in study materials and additional language will be added to the student list instructions that coordinators need to include students enrolled at any time during NPSAS year, not just the term that includes the end date. The Institution Registration Page (IRP) instruction will be revised so that it is clear which students NPSAS is focused on (those enrolled at any time during NPSAS year) and to include skip logic for schools with distinct academic terms or continuous enrollment. Lastly, the student list variable intended to capture whether the school has an ISIR (Institutional Student Information Record, an electronic record summarizing the results of the student's Free Application for Federal Student Aid [FAFSA] processing) for each student will be removed because it was not useful. The Student Records procedures, materials, and system were well-received by institution staff and successful in collecting the data with high participation rates. These will be carried through to the full-scale study. Changes planned include improving the upload error messages by rank ordering them and specifying whether data are missing or invalid, researching what can be done to address the requested reminders to save data (e.g., adding an auto-save feature, changing the timeout criteria, or alerting the user when a timeout is approaching), adding various notes and instructions to the template regarding which fields can or cannot have data pasted into them, expanding or minimizing column or row sizing, and cautioning the user against overwriting the validations and macros programmed into the template."}, {"section_title": "Institutions Website", "text": "NPSAS:12 field test institutions website was designed to provide institutions with reliable, user-friendly access to all study documents and instructions, as well as a secure platform for providing the requested electronic enrollment lists and student record data. The website was used for the institution data entry tasks: completing the Designate a Coordinator form, completing the IRP, uploading a Student List, providing Institution Information (step 1 of Student Records collection) and providing Student Records data."}, {"section_title": "Contacting Institutions", "text": "The endorsement of NPSAS by organizations and associations concerned with postsecondary education was used to encourage institutions to participate and to confirm the legitimacy of the study. Twenty-six such organizations had endorsed NPSAS in 2008 and they all renewed their endorsement of NPSAS:12. The list of endorsing organizations was featured on project correspondence, including all letters and brochures, as well as the project website. Prior to institution recruitment each of the 300 sampled institutions was called by a trained institution contactor in order to verify the address, confirm eligibility for the sample (as appropriate), and update contact information for the institution's chief administrator (e.g., the president or chancellor). Institution recruitment began with an initial mailing to the chief administrators at each institution. Two days later, the institution contactors made follow-up calls to the chief administrators' offices to prompt for designation of ICs. Once the IC was named, the next step was to confirm study participation with the designated IC and to set a customized deadline date for the student list, based on the institution's term structure. ICs followed up with each institution to prompt for completion of the list by the scheduled due date."}, {"section_title": "Student Enrollment List Acquisition", "text": "The instructions for compiling and uploading the student enrollment list were available to institutions on NPSAS institution website, by postal mail, or by e-mail. All of the institutions accessed the instructions on the institution website-none requested the instructions by postal mail or by e-mail. Institutions were asked to provide enrollment list information for all students enrolled at any time between July 1, 2010, and April 30, 2011 (March 31 for continuous enrollment institutions) (see section 2.2.2). Institutions were encouraged to upload their student enrollment list using the secure upload interface on the website. The following data items were requested for each listed student; \u2022 name; \u2022 Social Security number (SSN); \u2022 student ID number (if different from SSN); \u2022 student level (undergraduate, masters, doctoral-research/scholarship/other, doctoralprofessional practice, other graduate); \u2022 first-time beginner (FTB) indicator; \u2022 class level of undergraduates (first year, second year, etc.); \u2022 date of birth (DOB); \u2022 high school graduation date (month and year); \u2022 Classification of Instructional Programs code or major; \u2022 Indicator of whether the institution received an Institutional Student Information Record (ISIR) (an electronic record summarizing the result of the student's Free Application for Federal Student Aid [FAFSA] processing) from the Central Processing System (CPS); and \u2022 Contact information (local and permanent street address and telephone number and school and home e-mail address). Multi-campus institution systems with centralized record-keeping systems were encouraged to submit a single student enrollment list encompassing all their sampled institutions. Five such systems provided lists for their sampled institutions. Field test protocols called for half of the field test institutions to be included in the student sample. Completion rates were monitored by stratum. Once the target participation rate in each stratum was obtained follow-up efforts for that stratum were curtailed. Remaining institutions which indicated they were working to complete a list by their scheduled deadline were allowed to upload their lists."}, {"section_title": "Student Records Collection", "text": "The first step in the student records collection effort was to send the student sample to CPS to obtain financial aid application data. Data elements retrieved from CPS were loaded into the student records database. Elements added included an indicator of whether the student had been matched successfully to the CPS system and selected variables for use in student records software edit checks. The student records system was customized by loading names of institution financial aid programs and up to 12 state financial aid programs to assist in identifying common types of financial aid received by students. Once the student records list was initialized for a particular institution, an informational packet on the student records collection process was sent to the designated IC. These packets included instructions for accessing NPSAS:12 field test institution website and logging in the secure site with their ID and password. In addition, staff made follow-up telephone calls to notify institutions that student records data collection had begun. Using daily status reports that summarized the progress of the institutions, staff called institutions periodically to prompt completion of student records collection."}, {"section_title": "Institution Data Collection Outcomes", "text": "This section provides results of institution recruitment and the data collection activities associated with enrollment lists and student records. Of the 300 eligible institutions, 95 percent agreed to participate. The requirements for field test sampling were met after 150 of these 300 institutions submitted lists, the remaining institutions were not required to provide lists. Of the 150 institutions from which students were sampled, 97 percent provided data from student records for the sample students."}, {"section_title": "Institution Recruitment and Student Enrollment List Acquisition", "text": "Eligible sample institutions were asked to participate at two points in the survey process: (1) at the sample selection stage, by providing a comprehensive list of enrolled students for sample selection and (2) after the sample selection stage, by providing data from student records for the sample students. Consequently, the potential for institution nonresponse existed at these two points in the survey process. Of the 300 eligible institutions, 95 percent of the chief administrators agreed to participate; all of these appointed an IC to assist with study requirements. The first request of the ICs was to provide a student enrollment list to be used for selecting the student sample. Approximately 10 institutions explicitly refused to provide an enrollment list. As discussed in section 2.2.2, the first 150 lists that met the sample size goals by institution stratum and passed QC checks were used for selecting samples of students. A total of 240 lists were received. Once a sufficient number of lists were received per stratum, follow-up with nonresponding institutions was suspended. List provision varied by type of institution, but, in general, about half of the institutions in each stratum provided lists that were used for sampling students. The percentage of institutions providing enrollment lists across strata ranged from about 43 percent to 59 percent. The lowest participation rates were among the public less-than-2-year institutions."}, {"section_title": "Student Records Collection Outcomes", "text": "The second request of the ICs was to provide student records for the sample students. Of the 150 institutions from which students were sampled, 97 percent provided data from student records for the sample students. At the institution level, an institution was classified as having completed the student records collection if data were obtained for at least one sample student. Table 5 shows the institution participation rate for student records and the method used by institution characteristics. The high proportion of institutions providing student records data (97 percent) indicates that there were no major hindrances for institution record abstraction in the field test. Most institutions (60 percent) chose to key the data into the web-based student records application as their primary mode; specifically, 36 percent chose Case-Mode and 24 percent choose Grid-Mode. However, 60 percent of the private, for-profit institutions preferred to produce and upload data files. Part of the reason for this is the presence of three systems that reported for multiple campuses in the sample through one coordinator. These coordinators were encouraged to use the Data Files Upload mode because they were reporting for multiple schools and more students.. Data Collection,Outcomes, NPSAS:12 Data File Documentation  Table 6 shows student records collection rates, by institution characteristics. From the 97 percent of institutions that provided student records data, student-level data were obtained for 93 percent of eligible sample members. "}, {"section_title": "Institution Data Evaluation", "text": "Several evaluation activities were conducted for institution data collection activities. These included evaluations of enrollment list acquisition, accuracy, and format; and student records acquisition and format."}, {"section_title": "Enrollment List Acquisition", "text": "The acquisition of enrollment lists was evaluated by the number received as well as by the dates when they were received. Institutions were asked to send enrollment lists between January 24, 2011, and April 15, 2011. The month the lists were received is important because sampled students were sent to data collection on a flow basis: the later the list was received the shorter the time available for the sample member to complete the survey before the end of data collection. Table 7 shows the flow of student list receipt by institution calendar system and month. Approximately 75 percent of the lists arrived during the first 2 months of the year. Ultimately, 150 of the 300 eligible institutions in NPSAS:12 field test sample provided student enrollment lists (see section 3.2.3) that were used for sampling students. "}, {"section_title": "Enrollment List Accuracy and Format", "text": "Instructions detailing the student data elements to be included on enrollment lists were provided on NPSAS institution website to improve the accuracy of the lists. However, institutions made some format errors when submitting enrollment lists such as excluding specified data elements and providing data which were not requested. Accuracy errors were identified by comparing institution-provided data to Integrated Postsecondary Education Data System (IPEDS) data for the institutions. Institutions submitting counts with discrepancies were contacted to reconcile the data. Table 8 presents a summary of lists received that contained format or accuracy problems. Approximately 77 percent of enrollment lists that were used for sampling did not have any problems. "}, {"section_title": "Evaluation of Student Records Quality", "text": "The student records format was evaluated for item-level completeness of institution submissions. Table 10 shows student records completion rates for key data elements overall and by method of abstraction (case mode, grid mode, CSV upload, Excel upload). Variability in item-level response reflects the variability of institution record-keeping; not all data elements are available at every institution. However, most of the key data elements have a high percentage of item-level completeness. Furthermore, all types of abstraction methods achieved high completion rates overall. Marital status and having at least two telephone numbers were two items with low completion rates (47 percent and 31 percent, respectively) because these are often not included in student records. Not all students receive financial aid; however 77 percent of students received some amount of federal, state, institution, or some other type of aid. ICs who provided the data were often financial aid personnel. Thus, they were familiar with this type of information and knew how to access it quickly and accurately.  (BPS) follow-up study, was designed for web and telephone administration. Sample members were primarily located using batch address and telephone sources and were asked to complete the interview between March and June 2011. Analysis and evaluation of data collection results provided information for consideration in planning the fullscale study."}, {"section_title": "Student Interview Design and Systems", "text": "NPSAS:12 field test student interview was based in part on core data elements used in previous NPSAS student interviews, and in part on a human capital framework redesign. The interview, tested prior to data collection in focus groups and through cognitive interviewing, consisted of seven sections and included two instrument experiments. This section provides the details of the student interview design and of the various systems used to support the instrumentation process and data collection. Experiments are described in Chapter 5."}, {"section_title": "Student Interview Design", "text": "The content of NPSAS:12 field test interview included core data elements used in previous NPSAS student interviews as well as elements identified through a redesign that used a human capital framework to create new questions focused on student decision making. New student interview items were developed and refined with input from the study's Technical Review Panel (TRP) and through feedback from focus groups and cognitive interviewing of items. The core data elements maintained in NPSAS:12 field test student interview included such long-standing NPSAS items as student high school characteristics, postsecondary enrollment and characteristics, field of study, financial aid sources and amounts, student employment and earnings, credit cards, parent and family characteristics, student demographic characteristics, and limiting mental or physical conditions. New student interview items were created largely to collect base year data for BPS follow-up study, and were designed using an econometric framework that addresses student persistence in postsecondary education. These new interview items included questions centering on students' anticipated labor market outcomes of persistence (financial and nonfinancial), foregone wages, probabilistic estimates of persistence in education and persistence in field of study, and other constructs suggested by behavioral economics. Planning for redesign of NPSAS interview included focus groups conducted in March and April of 2010. A total of 50 students from less-than-2-year, 2-year, and 4-year public and nonprofit postsecondary institutions participated in six focus groups in the Philadelphia, Pennsylvania area. Focus group questions were designed to elicit student feedback to broad questions about terminology, experiences, and decisions related to students' postsecondary education. The full NPSAS:12 field test interview, particularly newly designed items, were then tested through web and telephone cognitive interviews conducted from October to December of 2010. Forty-eight participants, evenly divided among less-than-2-year, 2-year, and 4-year public and nonprofit postsecondary institutions, provided feedback to refine student interview items. NPSAS:12 field test student interview consisted of seven sections, grouped by topic. Respondents were guided through each section of the interview according to skip logic that took into account information recorded as the respondent progressed through the interview. Following are descriptions of the seven interview sections. 1. Enrollment collected information on the respondent's attendance at the sampled institution (referred to as NPSAS institution) in the 2010-11 school year, including degree or certificate type, dates attended, enrollment intensity, and undergraduate or graduate year. It also captured high school completion information, dates of any previous degrees, and enrollment information for any additional schools attended in the 2010-11 school year. The section concluded by obtaining information about the respondent's date of birth, marital status, and gender. First-time beginners (FTBs) also received questions about their intent to complete their degree at NPSAS institution, expected degree completion date, and likelihood of degree completion. 2. Education Experiences gathered information on the respondent's high school experiences (e.g., math courses, Advanced Placement and International Baccalaureate participation), NPSAS institution major or field of study, remedial coursework since high school, and highest degree ever expected. Students identified as FTBs were asked to identify their last high school and received questions about family and friend support for persistence in college, academic and social integration at NPSAS institution, difficulty scheduling classes at NPSAS institution, and services used at NPSAS institution. 3. Financial Aid collected information on grants, scholarships, federal loans and private loans the respondent received during the 2010-11 school year; the amount borrowed and amount owed for undergraduate or graduate education; whether family or friends would assist in repaying loans; and the cost of books and supplies in the 2010-11 school year. This section concluded with questions about undergraduate-level work-study jobs and assistantships and graduate-level assistantships, fellowships, and traineeships. Respondents who were FTBs received additional questions about their thoughts on borrowing loans and whether the cost of attending NPSAS institution met their expectations. 4. Current Employment captured information about nonschool related employment the respondent had during the school year (e.g., number of jobs, earnings, hours worked, commute time, impact on course-taking, and campus access). Respondents who were FTBs also received questions about their work plans if they had not been enrolled at NPSAS institution in the 2010-11 school year and the ways in which their life might change once they complete their education."}, {"section_title": "5.", "text": "Income and Expenses collected information about annual income, family and household composition, use of credit cards, residence and commute time while attending NPSAS institution, and receipt of untaxed benefits. Respondents who were FTBs also received questions about day care and college costs for their dependent children; the number and cost of supporting other dependents; use of federal loans, private loans, money received from family and friends, and credit cards for expenses; and a set of discount rate questions. 6. Background obtained information about student demographic characteristics, including citizenship and immigration age, first language, foreign language use, ethnicity and race, parents' highest level of education, military service, and disability status. Respondents who were FTBs also provided ratings of their physical and mental health."}, {"section_title": "7.", "text": "Locating, which only first-time beginning respondents received, collected contact information for the follow-up study. Coding systems. Assisted coding systems (coders) were used in the interview to standardize the collection and coding of the respondent's postsecondary institutions attended during NPSAS year, last high school, major or field of study, and hypothetical occupation. The name or title of each of these items was entered as a text string in each coder, and a keyword search conducted on an underlying database returned a list of possible matches. Following are descriptions of the individual coding systems and sources: \u2022 The postsecondary institution coder was developed from the set of institutions contained in the Integrated Postsecondary Education Data System (IPEDS), developed by NCES (http://nces.ed.gov/ipeds/). \u2022 The major coder was constructed using the 2010 Classification of Instructional Programs taxonomy, also developed by NCES (http://nces.ed.gov/ipeds/cipcode). \u2022 The occupation coder was built from the Occupational Information Network Online (O*NET OnLine) database (http://onetonline.org). \u2022 The high school coder was developed using the Private School Universe Survey for private schools (http://nces.ed.gov/surveys/pss/) and the Common Core of Data for public schools (http://nces.ed.gov/ccd/)."}, {"section_title": "Student Interview Data Collection", "text": "The NPSAS:12 student interview data collection involved training of data collection staff and locating, contacting, and interviewing sample members. A study website and help desk were used to provide information and guidance to sample members. Two experiments were also conducted in the field test and are described in chapter 5.\nField test data collection experience influenced plans for the full-scale study. Based on feedback from interviewing staff, training time for telephone interviewers will be expanded to include additional hours for independent study and for providing additional practice cases. Field test interviewers' strategies for gaining cooperation will also be incorporated into full-scale training sessions. In terms of tracing and locating sample members, while pleased with the results provided by our tracing vendors, there are plans to continue to examine our suite of tracing services and add any that may provide additional value. Propensity experiments conducted during the field test revealed that sample members in particular sectors required more computer-assisted telephone interviewing (CATI) efforts than other groups and were more likely to participate via telephone than web. In order to increase participation among sample members in those challenging sectors, cases in sectors that have historically participated via CATI will immediately receive outbound calls during the first weeks of their data collection period, rather than allowing 3 weeks for web participation. This will allow additional CATI time while also allowing us to get an earlier start on tracing activities. E-mail has proven to be an effective way to communicate with NPSAS sample members, who are busy and have numerous demands on their time, to encourage their participation, since some sample members do not respond well to repeated telephone contacts. For the full-scale study, frequent e-mail messages will be sent to sample members to remind them that they have been selected for the study and encourage their participation. Rather than offering varying incentive amounts to different groups of sample members, it is likely that an incentive amount of $30 will be offered to all sample members throughout the course of data collection. Generally, field test data indicated no substantial problems during data collection with NPSAS student interview. However, revisions to specific questions in the field test interview will be made for the full-scale interview based on continued detailed analysis of field test data, including examination of items with high missingness, as well as on feedback from a Technical Review Panel Meeting conducted in August of 2011 and from an additional round of cognitive interviews of approximately 25 FTBs across institution types. The revision process will also include an attempt to reduce the timing burden of the interview by carefully selecting items for removal and continuing refinement of interview questions for overall efficiency and comprehension by respondents. All methodological features of the instrument, including assisted coding systems and help text and conversion text, will continue to be used for the full-scale survey and as always, special attention will be paid to successfully training full-scale interviewers on the use of these instrument features."}, {"section_title": "Interviewing", "text": "Data collection for NPSAS:12 field test interview consisted of two phases: the early response phase and the production phase)."}, {"section_title": "Early response phase. This phase began in March 2011 and lasted approximately 3 weeks.", "text": "Data collection began with a mailing or e-mail, or both, to sample members encouraging them to complete the NPSAS survey over the Web. The telephone interview was available to sample members who contacted the Help Desk, but no outbound telephone contacts were made. The early response phase began in waves, based on when sample member information was received from institutions and batch tracing procedures were completed. Sample members who completed the interview were eligible to receive an incentive of $15, $30, or $45 based on their response propensity experiment group assignments (see section 5.3). 2. Production phase. The production phase began approximately 3 weeks after the start of the early response phase on April 28, 2011. During the production phase telephone interviewers called sample members to encourage completion of the interview either online or over the telephone. Sample members who completed the interview during the production phase were eligible to receive the same incentives as during the early response phase. Both the Web and telephone versions of the survey were available to sample members throughout the entire data collection, although the Web survey was encouraged during the early response period. The Web and telephone versions of the survey were identical except that the telephone version included instructions for the telephone interviewer administering the survey."}, {"section_title": "Student Interview Data Collection Quality Control", "text": "A number of QC procedures were implemented throughout the course of NPSAS:12 student interview data collection. These procedures included frequent interview monitoring of telephone interviewers, quality circle feedback meetings, and interviewer debriefings at the conclusion of the study. QCS and project staff regularly monitored live and recorded telephone interviews throughout NPSAS:12 data collection. Monitoring was conducted to meet the following data quality objectives: \u2022 identification of problem items in the interview, \u2022 reduction in the number of interviewer errors, \u2022 improvement in interviewer performance through reinforcement of effective strategies, and \u2022 assessment of the quality of the data collected. In addition, Quality Circle meetings were held to serve as a tool for ensuring communication between project staff, call center staff, and telephone interviewers. These meetings were used to reinforce content from training and the goals of the study and to provide interviewers with the opportunity to share their experiences and strategies for gaining cooperation. At the conclusion of NPSAS:12 data collection, project staff held a debriefing meeting with interviewers to learn more about their experiences, and administered an anonymous survey of the interviewers. The interviewers reported positive overall experiences working on the study and provided suggestions for improving full-scale data collection. Project staff prepared a summary of the debriefing meeting and survey for consideration when planning the full-scale survey."}, {"section_title": "Student Interview Data Collection Outcomes", "text": "Approximately 89 percent (n =3,860) of eligible sample members were located with about 250 sample members determined to be ineligible for the study. Of eligible sample members who were located, 69 percent completed the full interview. An additional 60 partial interviews were completed by sample members who completed at least the enrollment portion but did not complete the entire survey."}, {"section_title": "Student Locating Results", "text": "Locating rates, shown in table 11 ranged from 96 percent for students enrolled at private nonprofit 4-year doctorate-granting institutions to 80 percent for students enrolled at private for-profit 2-year institutions. Among the total undergraduate students, potential first-time beginners were located at a significantly lower rate than other undergraduate students (87 percent compared to 97 percent) (\u03c72 = 47.5582, p < .0001). Graduate students (95 percent) were more easily located than undergraduate students overall (88 percent) (\u03c72 = 10.1045, p < .001). "}, {"section_title": "Interview Response Rates", "text": "The NPSAS:12 interview was completed by 2,720 cases, approximately 63 percent of the eligible sample (table 12). The response rate ranged from 52 percent for public less-than-2-year institutions to 76 percent for private nonprofit 4-year doctorate-granting institutions. Potential FTBs were less likely to respond than other undergraduates (57 percent compared with 88 percent) (\u03c72 = 201.0550, p < .0001). Graduate and professional students (77 percent) completed at a higher rate than undergraduate students (62 percent) (\u03c72 = 24.7043, p < .0001). Completion by phase. As described in section 4.2.1, NPSAS:12 student interview was initiated in two phases: the early response phase and the production phase. Of the 2,720 cases that completed the interview, 42 percent (1,160 cases) completed in the early response phase and 58 percent (1,570 cases) completed in the production phase. Fifty-three percent of respondents at private nonprofit institutions completed the interview in the early response period, while only 41 percent of respondents at public and private for-profit institutions completed in the early response period; students at these schools were more likely to complete in the production phase (\u03c72 = 15.7218, p < .0001) (table 13). Completion by mode As described in section 4.2, sample members were offered two modes to complete NPSAS:12 survey: online using the Web survey or over the telephone with a telephone interviewer. Telephone prompting began 3 weeks after the start of data collection, during which time the Web survey was available. Sample members were eligible to receive the same incentive amount ($15, $30, or $45) for completing the survey through either mode. The majority of respondents (2,090 cases, or 79 percent) completed the Web survey, and the remaining 570 respondents (22 percent of completions) completed the interview by telephone (table 14). Of web respondents, 1,150 (55 percent) completed without telephone contact, and 940 cases (45 percent) completed the Web survey after contact with a telephone interviewer Graduate and professional students were more likely to complete the Web survey than undergraduate students (\u03c72 = 19.2248, p < .0001). Potential first-time beginners were more likely to complete the Web survey than other undergraduates (\u03c72 = 10.8840, p < .0001). The overall distribution of completed interviews by mode is shown in figure 2. Telephone interviews comprised 21 percent of all completed interviews while web interviews with telephone contact were 35 percent of all interviews completed. Web interviews without telephone contacts represented 43 percent of completed interviews. Data Collection,Outcomes,and Evaluation NPSAS:12 Data File Documentation O-39 "}, {"section_title": "Interview Timing Burden", "text": "To ensure that the burden associated with completing NPSAS:12 interview was minimal, the time required for each student respondent to complete the field test interview was collected and analyzed. Special attention was paid to differences by mode, items with consistently high administration times, and the time required to navigate particular interview paths. To calculate the time required to complete the interview, whether administered online or by telephone, a time stamp was embedded on each web screen, or form, of the interview. A start timer recorded the clock time on a respondent's or interviewer's computer when a form was first loaded, and an end timer recorded the clock time when the Next button was clicked. For each form administered, time was calculated by subtracting the start time from the end time. Total instrument time was calculated by summing across the times recorded for each form. Only cases that completed the entire field test interview in one session were included in the analyses, except for the analysis of individual forms (table 17) in which forms completed during partial interviews were included. Outliers, defined at the interview and form levels as exceeding two standard deviations, were also excluded. Overall, NPSAS:12 field test interview averaged 36.2 minutes to complete, with web interviews averaging 34.5 minutes and telephone interviews taking significantly longer at 41.7 minutes (t(2,050) = 10.7, p < .0001). This difference is attributed to the time required for interviewers to read questions and other text aloud to respondents. Average section completion times are shown in table 15 together with the average times to complete each section by mode. For all sections, administering the interview by telephone required more time than online; all differences were significant-enrollment (t(990) = 6.71, p < .0001); education experiences (t(2,050) = 5.07, p < .0001); Financial Aid (t(1,220) = 6.63, p < .0001); Current Employment (t(730) = 8.32, p < .0001); Income and Expenses (t(980) = 8.86, p < .0001); Background (t(1,370) = 11.35, p < .0001); and Locating (t(1,450) = 11.07, p < .0001). Average times to administer each form were compared across all forms in the instrument, except those in the Locating section which can, unavoidably, require long administration times. Among the five highest overall form times were the coders for occupation (N12EXOCC), major (N12MAJ1), and high school attended (N12HSCDR). The coder for expected occupation after degree completion (N12EXOCC) had the longest average form time at almost 79 seconds (table 16). The 10 forms with the highest average administration times, excluding the coders and the Locating section, are listed in table 17. The form asking respondents in which months of the 2010-11 academic year they attended NPSAS institution (N12NENRL) had the longest average form time at 39 seconds. There was no pattern observed such that one type of form required more time to administer than another type. The items that showed the longest administration times in the field test tended to contain complex wording or formats.  Data Collection,Outcomes,and Evaluation NPSAS:12 Data File Documentation O-41 Each question will be reviewed in developing the full-scale interview to try to achieve quicker administration times. The items that showed the longest administration times in the field test tended to contain complex wording or formats. For example, N12DCLWHY required a respondent to pick the one best choice from among several lengthy statements describing why they were concerned about repaying student loans. N12SRVMATRX was a two-part question asking for the frequency with which a respondent used several institution services and how important the service was in their decision to remain at NPSAS institution. However, some items necessitate complex formats so administration times will be weighed against the value of the information collected. The time required to complete NPSAS:12 interview varied by the student's status as a FTB, other undergraduate, or graduate student. Table 18 shows the average interview time for FTBs both overall and for each section, by mode of administration. FTBs had a much longer path through the interview, requiring, on average, 42.1 minutes to complete the interview across modes. Interview time for the telephone interview (48.0 minutes) was significantly longer than the online interview (38.89 minutes; t(690) = 15.41, p < .0001). All sections of the FTB interview were significantly longer for the telephone interview compared to the online interview: enrollment (t(760) = 6.1, p < .0001), education experiences section (t(820) = 9.61, p < .0001), financial aid section (t(830) = 7.3, p < .0001e current employment (t(630) = 12.59, p < .0001); income and expenses (t(760) = 9.6, p < .0001); background (t(960) = 11.83, p < .0001), and locating (t(1,450) = 11.07, p < .0001). The majority of questions specific to FTBs were in the education experiences section. Other undergraduate students were administered an interview with considerably fewer questions than FTBs, and were not asked any of the questions in the locating section. They averaged 26.1 minutes to complete NPSAS:12 field test interview. Table 19 shows the average interview times for other undergraduate students overall and for each section, by mode of administration. Comparing across modes, the education experiences section was longer in the online mode than in the telephone mode (t(390) = 2.21, p = .0274), the only section longer in online mode than in telephone. The background section was significantly longer for the telephone interview when compared to the online interview (t(350) = 3.28, p < .01). Graduate students, like other undergraduates, were administered fewer questions than FTBs and were not asked the questions in the locating section. They averaged 20.0 minutes to complete the interview overall. Compared across modes, there were no significant differences in completion times by section. Table 20 shows the average interview time for graduate students overall and for each section, by mode of administration. The NPSAS:12 field test reliability reinterview was completed by a random sample of 340 FTBs at least 3 weeks following completion of the main NPSAS:12 interview. The reinterview included about 70 items from the original interview (see section 4.5.3 Student Reinterview Analysis for further description of the reliability reinterview). Overall, it required an average of 9.3 minutes to administer with the telephone reinterview requiring significantly more time (10.0 minutes) than the online reinterview (8.4 minutes; t(100) = 4.06, p < .0001). Table 21 provides the reinterview average times overall and by mode of administration. "}, {"section_title": "Number of Calls to Sample Members", "text": "On average, nine calls were made per sample member during the interview period, excluding the early response phase where no outbound calls were made. Average call counts for completed cases varied by mode of administration. The average number of telephone calls by institution characteristics and student type are shown in table 22. Respondents who completed the interview over the telephone with a telephone interviewer required fewer calls (six) than cases that completed the interview over the Web with telephone contact (nine) (t(4,350) = -8.23, p < .0001). Web interview respondents who completed the interview during the early response phase did not receive any calls. Table 23 shows the call counts by response status and mode of administration. Appendix O. Data Collection,Outcomes,and Evaluation NPSAS:12 Data File Documentation O-45 "}, {"section_title": "Refusal Conversion", "text": "Refusal aversion techniques were integrated into telephone interviewer training and were reinforced throughout data collection in Quality Circle meetings. Interviewers were encouraged to share their experiences gaining sample member cooperation and seek guidance from the group. Sample members who refused to complete the interview were placed in a separate queue and worked by a subset of interviewers selected for additional refusal conversion training. Overall, 14 percent of eligible cases ever refused; of these, about 26 percent of cases subsequently completed the interview (table 24). "}, {"section_title": "Potential FTB Identification", "text": "When asked to identify FTBs, institutions have had difficulty differentiating FTBs who are simply new to the institution from true FTBs, that is, those enrolling in postsecondary education for the first time since completing high school. As described in section 2.3, although presampling matching was conducted to help identify true FTBs, there were still some students identified by the institutions as FTBs who were determined during the interviews not to be (false positives). Likewise, some sample students were identified as FTBs during the interview who were not identified as such by their institutions (false negatives). Table 25 shows that, of the 2,440 students who were sampled as potential FTBs and completed an interview, about 440 were not FTBs for a false positive rate of 18 percent unweighted Appendix O. Data Collection,Outcomes,and Evaluation NPSAS:12 Data File Documentation O-47 (15 percent weighted). Conversely, of the 280 students who were sampled as other undergraduate or graduate students and completed an interview, about 10 were FTBs, for a false negative rate of 3 percent unweighted (2 percent weighted). With the help of the presampling matching, the false positive rate observed in the field test was much reduced from the rate of over 50 percent observed in NPSAS:04. "}, {"section_title": "Evaluation of the Student Interview", "text": "Evaluation of NPSAS:12 student interview included analyses of the data collected in the instrument coders, help text access rates, success rates for conversion text, item nonresponse, and the reinterview."}, {"section_title": "Instrument Coders", "text": "Recoding. Twenty-five percent of the major and occupation codes chosen in the student interviews were randomly selected for recoding. Recoding of postsecondary institution and high school codes selected in these coders was not done because text strings provided by respondents would presumably have directly matched school name codes chosen. In the major and occupation coders, text strings provided by respondents and standardized names of codes in the database were often not direct matches. The expert coding staff assessed the accuracy of major and occupation codes chosen in the interview based on the text string provided by the respondent. Across modes of administration and across coders, expert coding staff generally agreed with the codes chosen for text strings in the interview. Overall, expert coding staff agreed with major and occupation codes chosen in the interview 95 percent of the time, recoded responses to a new code about 4 percent of the time, and were unable to choose a code due to vague text strings about 1 percent of the time. Only the major coder showed significant differences in recode rates between modes of administration. Expert coders recoded major codes chosen by web respondents 3 percent of the time and recoded those chosen by telephone interviewers 0 percent of the time (z = 1.99, p < .05). Table 26 shows the rate of recoded values-same as original code, recoded to different value, or text string too vague to code-chosen by the expert coding staff for the major and occupation coders in the interview. Upcoding. Upcoding is a process by which project staff try to identify an appropriate code for interview responses that were not coded during the interview. Text strings from web interviews generally required more upcoding than text strings from telephone interviews because interviewers received special training on coders and were therefore more skilled at identifying appropriate codes than sample members. The high school, major, and occupation coders showed significant differences in upcoding rates between modes of administration. For the high school coder, the upcoding rate among web interviews (33 percent) was higher than for telephone interviews (6 percent) (z = 9.54, p < .001). For the major coder, 24 percent of web interviews required upcoding, compared to 3 percent of telephone interviews (z = 10.86, p < .001). Similarly, the upcoding rate on the occupation coder was higher for web interviews (19 percent) than for telephone interviews (1 percent) (z = 11.68, p < .001). Results of the upcoding process are shown in table 27. "}, {"section_title": "Help Text", "text": "Respondents or interviewers were able to click on a help button provided on each NPSAS:12 interview screen for both general instrument and question-specific help. The general instrument help provided answers to FAQs about web browser settings and response types (i.e., how to respond using a check box, dropdown box, or radio button). The question-specific help provided definitions of key terms and phrases used in question wording and response options and provided any other explanations thought to help clarify and standardize the meaning of questions for respondents. The number of times that respondents or interviewers clicked the help button on each screen relative to the number of respondents who were administered the question determined the rate of help text access for that screen. The screen-level rate of help text access was analyzed overall and by mode of interview administration to identify screens that may have been problematic for users. For forms administered to at least 25 respondents, the overall mean rate of help text hits per screen was less than 1 percent. Help text was accessed approximately 1 percent of the time during interviews by telephone interviewers, compared with less than 1 percent of the time by web respondents (z = 3.73, p < .001). The interview question asking the number of times the respondent formally changed majors, frequency of major change (N12CHGNUM), had the highest overall rate of help text access, at 4 percent with no significant mode difference. Table 28 shows the interview questions administered to at least 25 respondents and for which help text was accessed at a rate of at least 2 percent. "}, {"section_title": "Student Reinterview Analysis", "text": "Reliability of self-reported responses to interview questions, a measure of how constant responses remain over time, was evaluated using a reinterview containing 70 items selected from the main interview. Items were selected for the reinterview because they were assumed to be unlikely to change over time and most were newly-designed and/or critical main interview items. A random sample of 340 FTBs who had completed a full main interview was selected for reinterview. Only FTBs were selected because the large majority of newly-designed items in the main interview were administered only to FTBs. An effort was made to select equal numbers of cases in the high and low propensity incentive groups. The reinterview sample was contacted beginning 3 weeks after completion of the main interview and asked to complete a 10 minute reinterview. The reinterview was completed by 180 respondents (response rate of 54 percent) and took an average of 9.3 minutes to complete. About 57 percent of the low propensity sample completed the reinterview while 50 percent of the high propensity sample completed. No significant difference in likelihood to participate was found between the low and high propensity incentive groups. Sample members selected for reinterview were allowed to complete the reinterview either online or by telephone, in whichever mode was more convenient. Of the 180 respondents who completed the reinterview, 48 percent completed online and 52 percent completed by telephone. Respondents who completed the initial interview via telephone tended to complete the reinterview by telephone (94 percent). However, of those who completed the initial interview online, approximately one-third switched modes and completed the reinterview by telephone. Response rates are shown overall, by main interview, and by reinterview completion mode in table 29.  Table 30 shows reliability estimates for the items included in the reinterview, by main interview section. For each item, the number of cases, percent agreement between the interview and reinterview, and relational statistic are shown. For discrete items, percent agreement was based on the extent to which responses to the initial interview matched exactly to the reinterview responses. For continuous items, responses were considered in agreement if the initial interview responses were within one standard deviation of the reinterview responses. Items using sliders to capture responses were treated as continuous due to the number of possible response values. For example, a slider using a scale from 0 to 10 with midpoints allows for 21 possible response values. Analyses were conducted only for respondents with responses on both the interview and the reinterview; not all questions were applicable to all respondents. The relational statistics quantified the strength of association between the pairs of items being compared; for each statistic, 1.00 was indicative of a perfect correlation (i.e., an exact match between the item on the initial interview and the same item on the reinterview for all respondents). The relational statistic, Cramer's V, was used for items with discrete, unordered response categories (e.g., yes/no). Kendall's tau-b (\u03c4 b ) estimated the relationship between items with ordered categories (e.g., excellent, fair, poor). Lastly, the Pearson product-moment correlation coefficient (r) was used for items yielding interval responses and for items using sliders (e.g., salary and likelihood of degree completion). and a relational statistic of 1.00. It should be noted that these items were administered to a small number of respondents. The reinterview included questions about the respondent's legal residence. Reliability results for State of legal residence showed 97 percent agreement and a relational statistic of 0.98. The item Permanent address zip code had 94 percent agreement with a relational statistic of 1.00. The respondent was also able to indicate a permanent address outside the U.S. on the item Permanent address zip code: outside the U.S. Though the item had a high percentage of agreement (99 percent), it had a relatively low relational statistic (0.49)-a result that can occur given a small change between initial interview and reinterview responses, especially when there's little variation in the initial interview responses. Respondents with an associate's degree were asked to categorize their degree on Type of associate's degree. Reliability for this item was low with a percentage of agreement of 51 percent and a relational statistic of 0.49. Low consistency on the item might stem from response option wording that was unfamiliar to the respondent. Providing examples of specific associate degrees beside each type of associate's degree may help clarify the response options for the full-scale interview. At the end of the interview, respondents were asked to rate their physical and mental health, using response options ranging from excellent to poor. These items, Physical health and Mental health, showed low rates of agreement (46 percent and 53 percent, respectively) and low relational statistics (0.40 and 0.37, respectively). Vague item wording could have been the source of the discrepancies, as respondents may have rated their health on a particular day as opposed to their health overall. For the full-scale interview, clarification can be added to item wording to improve reliability. Overall, results of the reinterview analysis indicate the survey yields data of high quality, with consistently reliable results. The majority of items (49 out of 70) have a percentage agreement of 80 percent or higher."}, {"section_title": "Item Nonresponse", "text": "Rate of nonresponse was a data quality measure used to identify troublesome interview items and better understand the experiences of sample members in completing the interview. Total nonresponse rates were calculated for items with missing data (including don't know responses) that were administered to at least 100 respondents. Overall, the item-level nonresponse analysis yielded 10 out of 450 interview items with more than 15 percent missing data. 5 The item with the highest rate of nonresponse was amount of veteran's education benefits (N12VETBENAMT). Of the 140 respondents who received this item, approximately 51 percent did not provide an amount for their veteran's education benefits. Amount of employer grants or scholarships (N12EMPGRTAMT) also had a high nonresponse rate, with 22 percent of respondents failing to provide an amount for their employer grants. Questions about the number of Advanced Placement (AP) exams used for placement and credit at NPSAS institution returned relatively high rates of nonresponse. One-third of respondents did not provide a number for the AP exams used for placement on N12APNMHLCRS, and about 29 percent did not provide a number for the AP exams rejected for placement and credit on N12APNOTHING. Questions about the number of remedial courses taken since high school also produced high nonresponse rates. Approximately 15, 18, and another 18 percent of respondents did not provide the number of remedial courses taken in English (N12REMENGL), in reading (N12REMREAD), and in writing (N12REMWRITE), respectively. The item interactions with students are more positive than negative: right (N12SOCPEERRT), had a nonresponse rate of 18 percent and was a right-start slider, which typically yielded higher nonresponse when compared to the left-and center-start sliders (see section 5.1 Slider Experiment for a discussion of instrument sliders with different start points). The two remaining items with nonresponse rates greater than 15 percent were text strings that specified an 'other' response. Approximately 27 percent of respondents did not enter a text string on reason not attending NPSAS institution: other specify (N12NOATTSP) and 20 percent did not enter a text string on type of associate's degree: other specify (N12ASSOCSP). Item-level nonresponse rates were also examined by mode of administration. There were significant differences in nonresponse rates between web and telephone modes for eight of the 10 interview items with more than 15 percent of data missing; only amount of employer grants or scholarships (N12EMPGRTAMT) and type of associate's degree: other specify (N12ASSOCSP) did not show a significant mode difference. All eight items had a higher rate of nonresponse among web interviews. Table 31 summarizes the item-level nonresponse for items administered to at least 100 respondents with a rate of more than 15 percent missing data. "}, {"section_title": "Student Interview Conclusions", "text": "NPSAS:12 FT interviews were conducted from March 29, 2011, to June 30, 2011. Of the 4,590 eligible sample members in NPSAS:12 sample, 3,860, or 89 percent, were successfully located. Successful locating methods included batch searches, such as CPS and Phone Append, and address update information provided by both sample members and their parents. Overall, 460 cases, or 11 percent of the eligible sample, required intensive tracing, and 66 percent of these cases were located. Locating methods attempted during NPSAS:12 data collection included text message reminders and frequent e-mail contacts. Of the 4,590 sample members in NPSAS:12 sample, 2,720, or 63 percent, completed an interview. Forty-two percent (1,160 cases) completed in the early response phase and 58 percent (1,570 cases) completed in the production phase. Seventy-nine percent of interviews were completed on the Web, and 21 percent of interviews were completed by telephone. Of web respondents, 1,150 (55 percent) completed without telephone contact, and 940 cases (45 percent) completed the web survey after contact with a telephone interviewer. Sample members who completed the interview received $15, $30, or $45 based on their predicted propensity to participate. The NPSAS:12 field test student interview was based in part on core data elements used in previous NPSAS student interviews, and in part on a human capital framework redesign primarily used to create new base year questions for BPS follow-up study. The interview was tested prior to data collection in focus groups and through cognitive interviewing, and consisted of seven sections. On average, NPSAS:12 interview took 36.2 minutes to complete. Overall, web interviews were significantly shorter at 34.5 minutes than telephone interviews were at 41.7 minutes. The time required to complete the interview varied by student's status as an FTB, other undergraduate, or graduate student. FTBs had a much longer path through the interview, requiring, on average, 42.1 minutes to complete the interview. The other undergraduate group took an average of 26.1 minutes to complete the interview, and graduate students took an average of 20 minutes to complete the interview. On average, the reinterview administered to a random sample of FTBs, took 9.3 minutes. An evaluation of the quality of the data provided by NPSAS:12 student interview showed that methodological features built into the instrument such as the design of assisted coding systems, as well as training and supervision of interviewing staff, aided in the successful administration of the interview. Overall, expert coding staff agreed with major and occupation codes chosen in the interview 95 percent of the time, recoded codes chosen on these coders to a new value about 4 percent of the time, and were unable to choose a code based on too vague a text string about 1 percent of the time. Text strings from web interviews on the instrument coders generally required more upcoding by expert coding staff than did text strings from telephone interviewers. The appearance of conversion text in the instrument appeared to improve question response. Seventy percent of the cases where conversion text was triggered in the interview were converted to a response after the conversion text was displayed. Help text on individual interview screens was accessed less than 1 percent of the time. The item-level nonresponse analysis yielded just 10 out of 450 interview items with more than 15 percent missing data. Results from the reinterview showed that the majority of reinterview items (49 out of 70) had 80 percent or higher agreement with responses chosen in the main interview. Debriefing of interviewers at the end of data collection indicated that frequent monitoring of telephone interviewers and quality circle training and feedback meetings were useful as data collection QC procedures. Most interviewers indicated that they felt they had all the tools necessary to successfully administer NPSAS:12 student interview and provided recommendations for future training topics, particularly focusing on gaining sample member cooperation to complete interviews. "}, {"section_title": "Slider Experiment Results", "text": "In examining the rate of nonresponse across starting positions, questions with the slider starting at the right were significantly more likely to have nonresponse when compared to left and center start items (table 32). For N12SOCPEER, the right start position had significantly higher nonresponse than both the center (z = 9.7469, p < .0001) and left start sliders (z = 9.8215, p < .0001). Similar results were found for N12SENBEL where right start sliders were significantly different than center (z = 6.7815, p < .0001) and left start sliders (z = 6.4867, p < .0001). Right start sliders were also significantly different from center (z = 5.9804, p < .0001) and left (z = 5.2391, p < .0001) start slider positions for the third item, N12SOCSAT. No statistically significant differences were observed when the other starting positions were compared. The results suggest that respondents were more likely to accept a right start \"strongly agree\" response as their own, assuming that the slider need not be moved to register that value as their response. Appendix O. 12 Data File Documentation O-59 Setting aside those items for which no response was indicated, the distributions of responses to each question were then compared. Overall, as shown in table 33, the right start slider position resulted in a significantly higher percentage of \"5-very satisfied\" responses on all three questions compared to the center start position, but not when compared to the left start position. For N12SOCPEER, the right start position had a significantly higher number of \"very satisfied\" (5) responses than the center start position (z = 2.1916, p < .05). Similarly, the results for \"very satisfied\" on N12SENBEL were also significantly higher on right start sliders than center (z = 2.4481, p < .01). Likewise the responses to N12SOCSAT were significantly higher for right start sliders compared to center start sliders (z = 2.1038, p < .05) for the \"very satisfied\" (5) response. No other significant differences in response distributions were found for the other starting positions. Taken together, these results suggest that the starting position of a VAS slider can influence the distribution of responses obtained in an interview. A greater percentage of nonresponse was observed when the slider position started at the highest rating, suggesting that sample members assume it is unnecessary to move the slider to register a 5, \"strongly agree\" response. Right start sliders also showed a higher concentration of extreme positive values compared to left and center start sliders. For the full-scale study, only four questions will use the VAS layout, with a scale from 0, \"No chance at all,\" to 10, \"Absolutely likely\" and a center start position. Each of the questions asks respondents to speculate as to the likelihood that they will complete their chosen degree and work in their desired occupation after postsecondary education. "}, {"section_title": "Discount Rate Experiment", "text": "Questions based on the economic theory of discount rate, which estimates the present value of future monies, were tested in NPSAS:12 field test interview in an effort to better understand the relative value students place on a postsecondary education when the benefits of that education are not immediately known. Since individual discount rates reflect subjective time preferences, they are difficult to estimate, but could provide insight on differences in college attendance decisions, such as by institution level if longer enrollment periods indicate a willingess to wait for higher income later. "}, {"section_title": "Discount Rate Experiment Results", "text": "More respondents preferred to wait for either of the larger payouts, rather than take the offer of $750 in 1 month. Among those offered $1,500 or $750, 59 percent preferred to wait (z = 5.5461, p < 0.001); 63 percent preferred to wait when offered a choice of $1,125 or $750 (z = 7.9545, p < 0.001). However, as shown in figure 5, this was not universal across institution levels. Respondents attending 4-year, doctorate-granting institutions were significantly more likely to choose to wait to receive a larger payout than respondents from less-than 2-year ($1,500: z = 3.0020, p < .01; $1,125: z = 2.6852, p < .01), 2-year ($1,500: z = 3.5312, p < .001; $1,125: z = 4.1080, p < .0001), or 4-year non-doctorate granting ($1,500: z = 2.6985, p < .01; $1,125: z = 3.8033, p < .0001) institutions. No other differences among the institution levels were found. Students enrolled in private, nonprofit institutions were more likely to wait for the higher amount (77 percent) than were students enrolled in public institutions (62 percent; \u03c7 2 =24.1946, p < .0001) or in private, for-profit institutions (47 percent; \u03c7 2 =63.8859, p < .0001). Likewise, students in public O-63 institutions were more likely to wait for the higher amount than were students enrolled in private, for-profit institutions (\u03c7 2 =28.1595, p < .0001). Willingness to wait for the higher dollar amounts was also evaluated for a number of student and student aid characteristics. As shown in table 34 students who reported using the financial aid services offered by their NPSAS institution (\u03c7 2 = 16.8895, p < .0001), received student loans for the 2010-11 academic year (\u03c7 2 =9.3420, p < .0022), had no private grants or scholarships (\u03c7 2 = 14.1200, p < .0002), did not turn down any loans offered them (\u03c7 2 = 6.2692, p < .0123), and took fewer classes to avoid having to take out more loans (\u03c7 2 = 13.5037, p < .0002) were less likely to wait for the higher dollar amounts. No other differences tested were statistically significant, including gender, whether or not the sample member applied for financial aid, importance of financial aid services to decision to remain at NPSAS institution, selection of less expensive institution to avoid loans, receipt of federal or private loans during the academic year, and receipt of the maximum amount of federal loans. FTBs who preferred waiting to receive the higher cash gift were routed to a follow-up question asking the minimum amount they would be willing to wait the 6 months or 1 year period to receive. The distribution of their responses is shown in table 35. Students tended to respond with either the highest or lowest values in the range, or to a response choice with a round number, specifically $1,000. "}, {"section_title": "Response Propensity Modeling Experiment", "text": "A third experiment was included in NPSAS:12 field test to test an approach that used incentives strategically to increase participation among low responding groups rather than to simply increase response rates overall. Survey nonresponse can result in bias that produces inaccurate estimates and compromised data quality, and nonresponse bias is sometimes addressed by attempting to increase the overall survey participation rate by pursuing cases most likely to be interviewed. However, bias could be inadvertently increased by adding more cases that are similar to those who have already responded (Merkle and Edelman 2009). If, instead, low responding cases are brought into the response pool, they could potentially increase the weighted response rate resulting in less biased survey estimates. The response propensity experiment was designed to answer three research questions: \u2022 Can a response propensity model be developed to predict a sample member's propensity to participate in NPSAS:12 interview? \u2022 Can the strategic use of incentives change the likelihood that a sample member will participate? \u2022 Can bias be reduced by increasing the proportion of low propensity cases in the response pool? The first step of the response propensity approach was to use information known prior to data collection (e.g., frame variables) to develop a predictive model of a given sample member's likelihood to respond. The methodology proceeded as follows: \u2022 Step 1 -Using prior administrations of NPSAS, identify variables which predict propensity to respond. \u2022 Step 2 -Prior to the start of data collection, estimate a NPSAS:12 case's propensity to respond to the interview. \u2022 Step 3 -Offer different incentive amounts to high and low propensity cases and evaluate their effect on interview participation. \u2022 Step 4 -Evaluate the predictive ability of the response propensity model and determine if bias is reduced in experimental cases. Developing a response propensity model. The propensity model was built using data from the 100,110 eligible cases in NPSAS:04 full-scale data collection. The variables determined to be predictive of a sample member's response propensity are listed in table 36 (higher odds ratios indicated a stronger influence in predicting response propensity). different propensity groups for the grant aid variables and are illustrative of the results across all key variables examined. The weighted estimate in both the low propensity control and treatment groups was not statistically different. Examining nonresponse bias for variables known for respondents and nonrespondents is another method by which to determine whether or not the response propensity approach reduced nonresponse bias. Methods to estimate bias, and the variables used, are similar to those described in section 7.2 except that, for this propensity analysis, results after weighting were not analyzed because the effect of data collection on bias was being examined rather than the effect of weight adjustments. Nonresponse bias was computed for high and low propensity cases, low propensity cases who received $30 (control group) and who received $45 (experimental group), and high propensity cases combined with low propensity cases who received $30 and with those who received $45. Table 40 shows the results of the comparisons. None was statistically significant. "}, {"section_title": "Informational Video", "text": "Initial contacts with sample members at the start of a data collection have traditionally been made using letters, e-mails, and sometimes telephone calls, with each form of communication including similar information about the study. These communications are designed to inform the sample member about the study, how to complete the survey, and what incentives would be associated with participation. In NPSAS:12 field test, an experiment was conducted to test the effectiveness of an online video as a method to also communicate study information. It was expected that the video would be an effective method of communicating information to, and engaging, sample members and, therefore, would result in higher participation rates among those who viewed the video than those who did not view the video. To test this, half of the field test sample was randomly assigned to a control group, receiving the usual data collection notification materials in their traditional form, postal mail and email. The other half of the field test sample received the same notification materials, but also received an invitation to view an informational video about the study. The video was posted to YouTube and a URL was included in the letter. E-mails sent to sample members subsequent to the initial letter also included hyperlinks to the video on YouTube. The participation rate of this experimental group was then compared to the control group. In the first follow-up with the FTBs, the 2012/14 Beginning Postsecondary Students Longitudinal Study (BPS:12/14), this base year exposure to the informational video will again be tested. The expectation is that displaying recognizable characters and themes from the video would improve recognition of, and willingness to participate in, the follow-up survey. Such \"branding\" would be particularly beneficial for longitudinal studies, like BPS, which require one or more followup interviews beyond the base year. In BPS:12/14 field test, characters from NPSAS:12 field test video will be used in communications with those sample members initially offered the video during NPSAS:12 field test. Treatment group participation rates for panel maintenance and interviewing will be compared to the control group to evaluate the effectiveness of the branding."}, {"section_title": "Informational Video Results", "text": "No overall difference in participation rates was observed between cases that were informed about the video in the letter and those that were not. Response rates between the control and treatment groups were identical; 50 percent of those who got the information about the video responded and 50 percent of those who did not receive information about the video responded. There were also no differences across the various institution sectors, as shown in table 41. Due to limitations of YouTube, where the video was posted, it was impossible to know whether or not those who received a link to the video actually viewed the video.  National Student Loan Data System. Successful matching to NSLDS can occur only for sample members who have received federal loans and/or Pell Grants. NSLDS files are historical; thus, information about receipt of such loans and grants was available not only for NPSAS field test study year, but also for prior years (where applicable). Table 43 shows historical match rates for eligible sample members, which does not necessarily mean that the match was for the current NPSAS year. In total, 46 percent of eligible sample members were matched to the NSLDS historical loan database. NSLDS Pell Grant matches were obtained for 50 percent of the eligible sample members. "}, {"section_title": "Chapter 7. Postdata Collection Data File Processing and Preparation", "text": "This chapter describes the weighting, nonresponse bias analysis, and variance estimation for the NPSAS:12 field test. For NPSAS:12, the field test institution sample was selected using statistical procedures rather than purposively as had been done in past NPSAS cycles. This provided more control to ensure that the field test and the full-scale institution samples have similar characteristics. It also allowed inferences to be made to the target population, supporting the analytic needs of the field test experiments and instrument. NPSAS:12 field test data files are not publically available; they were prepared as a test of methods and systems only."}, {"section_title": "Weighting", "text": "Statistical analysis weights were computed for interview respondents so that these respondents would represent the target population described in section 2.1. The statistical analysis weights compensated for the unequal probability of selection of institutions and students in the field test sample. The weights were also adjusted for multiplicity, nonresponse, and poststratification at the institution and student levels. The institution weight was computed and then used as a component of the student weight. The student analysis weight was computed for interview respondents as the product of the following nine weight components: \u2022 institution sampling weight (WT1); \u2022 institution subsampling adjustment (WT2); \u2022 institution multiplicity adjustment WT3); \u2022 institution nonresponse adjustment (WT4); \u2022 institution poststratification adjustment (WT5); \u2022 student sampling weight (WT6); \u2022 student multiplicity adjustment (WT7); \u2022 student nonresponse adjustment (WT8); and \u2022 student poststratification adjustment (WT9). Each weight component, described below, represents either a probability of selection or a weight adjustment. All nonresponse and poststratification adjustments were modeled using RTI's proprietary generalized exponential models (GEM) (Folsom and Singh 2000), which are similar to logistic models using bounds for adjustment factors and bounds on variance inflation. SUDAAN software (RTI International 2008) was used to run GEM. Weighting procedures were consistent with previous administrations of NPSAS."}, {"section_title": "Initial Institution Weight Components", "text": "There were three initial institution weight components. Institution sampling weight (WT1). The sampling weight for each sample institution was the reciprocal of its probability of selection. The probability of selection for institution i was Therefore, the institution sampling weight was assigned as follows: WT1 = 1/\u03c0 r (i). Institution subsampling adjustment (WT2). The subsampling weight for each sample institution was the reciprocal of its probability of selection for the field test subsample (see section 2.2.1). The probability of selection for institution i was Therefore, the institution subsampling weight was assigned as follows: WT2 = 1/\u03c0 r (i). Institution multiplicity adjustment (WT3). Each institution on the sampling frame initially had one chance of selection and an associated probability of selection. However, during institution recruitment and student list sampling, two institutions were identified that had two or more records listed on the IPEDS frame. In one case, the institution recently changed which campuses they report to IPEDS, and the other institution sent one student list covering multiple campuses. In both cases the campuses were merged for sampling purposes. These institutions were treated as having multiple chances of being selected into the sample because each institution had an initial probability of selection, but the additional institutions represented on the list also had probabilities of selection. Therefore, the weight of the sample institution, which is based on the initial probability of selection, needed to be adjusted to account for the actual probability of selection for the group of institutions represented by the list. The number of chances of the institution's being selected was based on the number of institutions that were represented on the enrollment list. Both institutions had several chances of selection, so the multiplicity adjustment was performed by first estimating the probability that any record could be selected:   Institution poststratification adjustment (WT5). To ensure population coverage, the product of the institution weight after nonresponse adjustment and the student frame counts was adjusted, with the use of GEM, to control totals for enrollment. Control totals were established for 12-month enrollment by institution type (sector10). Initially sector10 was crossed with enrollment size (small versus large) as the control total variables but the small field test institution sample size did not allow for this refined categorization. The enrollment control totals were calculated using the sampling frame based on the 2009 IPEDS Enrollment file. The poststratification was conducted using enrollment counts rather than institution counts because all NPSAS analyses will be at the student-level and not at the institution-level. Additionally, the institutions were selected with probability proportional to size (pps) with the size measured as counts of students. This method of sampling does not yield an accurate estimate of institutions. Table 45 presents the variables associated with the control totals and the average weight adjustment factors, by these variables. The institution poststratification weight adjustment factors from GEM met the following constraints: \u2022 minimum: 0.85; \u2022 median: 1.10; and \u2022 maximum: 1.60. "}, {"section_title": "Initial Student Weight Components", "text": "There were two initial student weight components."}, {"section_title": "Student sampling weight (WT6).", "text": "The overall student sampling strata were defined by crossing the institution sampling strata with the student strata within institutions. The sample students were systematically selected from the enrollment lists at institution-specific rates that were inversely proportional to the institution's probability of selection. Specifically, the institution-specific sampling rate was the overall student sampling rate divided by the institution's probability of selection, or where f s = the overall student sampling rate and \u03c0 r (i) = the institution's probability of selection. The student sampling weight was calculated as the reciprocal of the institution-specific student sampling rates, or WT6 = 1/f s /i."}, {"section_title": "Student multiplicity adjustment (WT7).", "text": "Students who attended more than one eligible institution during the 2010-11 academic year had multiple chances of being selected. That is, they could have been selected from any of the institutions they attended. Therefore, these students had a higher probability of being selected than was represented in their sampling weight. This multiplicity was adjusted by dividing their sampling weight by the number of institutions attended that were eligible for sample selection. \u2022 e-mail address count; and \u2022 mailing address count. In anticipation of potential convergence problems due to the small field test student sample size, fewer variables were included in the field test model than for the full-scale study. The possibility of including an age group variable in the model was considered but the percent missing was too large. Interaction terms were not identified and therefore not included in the model. Pell Grant status and Stafford Loan status were dropped from the adjustment model because of singularity, which prevents the model from running properly. Singularity occurs when a combination of variables can be used to determine the values of another variable. In the nonresponse adjustment model, the students who did not receive a Pell Grant were the same students who had Pell Grant amount equal to zero dollars; this was also the case for the Stafford Loan status variable. Table 46 shows the final predictor variables used in GEM to adjust the weights and the average weight adjustment factors resulting from these variables. The weight adjustment factors met the following constraints: \u2022 minimum: 1.00; \u2022 median: 1.18; and \u2022 maximum: 4.50. Appendix O. 12 Data File Documentation O-83  "}, {"section_title": "Student poststratification adjustment (WT9).", "text": "To ensure population coverage, the student weights were further adjusted, with the use of GEM, to known population control totals (control totals) for key variables. Control totals were established for 12-month undergraduate and graduate student enrollment, by institution type (sector10). Control totals for financial aid receipt and amounts were not necessary because the goal of the field test analyses is not to produce financial aid-related estimates. Control totals were established for the following: \u2022 Undergraduate 12-month enrollment by institution type \u2212 public less than 4-year; \u2212 public 4-year non-doctorate-granting;  Table 48 summarizes the institution weight distributions and the variance inflation due to unequal weight effects (UWE), by type of institution. The median institution weight ranges from 13 for public 4-year doctorate-granting institutions to 102 for private for-profit less-than-2-year institutions. The mean institution weight ranges from 13 for public 4-year doctorate-granting institutions to 250 for private for-profit less-than-2-year institutions. The UWE is 6.25 overall and ranges from 1.0 for public 4-year doctorate-granting institutions to 3.9 for students in private forprofit 2-year institutions. O-87  Table 49 summarizes the student weight distributions and the variance inflation due to UWE, by student type and type of institution. The median student weight ranges from 2,333 for students in private, nonprofit less-than-4-year institutions to 18,160 for graduate, masters, and doctoral-research/scholarship/other students. The mean student weight ranges from 2,340 for students in private nonprofit less-than-4-year institutions to 24,079 for graduate, masters, and doctoral-research/scholarship/other students. The UWE is 1.9 overall and ranges from 1.0 for students in private nonprofit less-than-4-year institutions to 2.8 for students in private nonprofit 4-year nondoctorate-granting institutions."}, {"section_title": "Weighting Adjustment Performance", "text": ""}, {"section_title": "Nonresponse Bias Analysis", "text": "The bias in an estimated mean based on respondents, R y , is the difference between this mean and the target parameter, \u03c0 (i.e., the mean that would be estimated if a complete census of the target population was conducted and everyone responded). This bias can be expressed as follows: The estimated mean based on nonrespondents, NR y , can be computed if data for the particular variable are available for most of the nonrespondents. The true target parameter, \u03c0, can be estimated for these variables as follows: where \u03b7 is the weighted unit (or item) nonresponse rate. For the variables that are from the frame, rather than from the sample, \u03c0 can be estimated without sampling error. The bias can then be estimated as follows: or, equivalently, This formula shows that the estimate of the nonresponse bias is the difference between the mean for respondents and nonrespondents multiplied by the weighted nonresponse rate. Nonresponse bias analysis was conducted for institutions and students. The nonresponse bias was estimated for variables known-that is, nonmissing-for most respondents and nonrespondents. While extensive data are available for all institutions from IPEDS, given the use of the field test data, only variables used in the institution nonresponse weight adjustments described in section 7.1 were used for bias analysis. First, for the institution-level variables, the nonresponse bias was estimated and tested to determine if the bias is significant at the 5 percent level. Second, nonresponse adjustments were computed, and the variables listed were included in the nonresponse model. The nonresponse adjustments were designed to significantly reduce nonresponse bias for variables included in the model. Third, after the final weights were computed, any remaining bias was estimated for the same set of variables, and statistical tests were performed to check the remaining significant nonresponse bias. As shown in table 50, the percent significant bias was reduced after weighting adjustments for the variables included in the bias analysis. For detailed results of the institution nonresponse bias analysis, see tables 52-59. Of the 4,350 eligible students, the response rate was about 63 percent. The student nonresponse bias was estimated for variables known-that is, nonmissing-for most respondents and nonrespondents. Variables used in the student nonresponse weight adjustments were used for bias analysis, except for the paradata. The following variables were used to assess student-level nonresponse bias: \u2022 institution type (sector10); \u2022 region; \u2022 institution total enrollment from IPEDS (categorical); \u2022 student type (sampled); \u2022 FTB status (sampled); \u2022 CPS record indicator; \u2022 Pell Grant status; \u2022 Pell Grant amount (categorical); \u2022 Stafford Loan status; \u2022 Stafford Loan amount (categorical); The same set of three steps described above for the institution nonresponse bias were conducted using the variables listed above. As shown in table 51, the student weighting adjustments increased the percent significant bias. Measurable bias was reduced after the nonresponse weighting adjustments for the variables included in the bias analysis. However, the poststratification adjustment to IPEDS enrollment totals resulted in an increase in the measurable bias. The poststratification was necessary to match the enrollment counts with known population totals.   1 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virgina, West Virgina; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = California, Nevada, Oregon, Washington. Appendix O. 12 Data File Documentation O-97    1 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virgina, West Virgina; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = California, Nevada, Oregon, Washington. Appendix O. 12 Data File Documentation O-101      "}, {"section_title": "Variance Estimation", "text": "For probability-based sample surveys, most estimates are nonlinear statistics. For example, a mean or proportion, which is expressed as \u03a3wy/\u03a3w, is nonlinear because the denominator is a survey estimate of the (unknown) population total. In this situation, the variances of the estimates cannot be expressed in closed form. There are multiple procedures for estimating variances of survey statistics, and the Taylor-series linearization procedure 8 was used for field test analyses. The Taylor-series variance estimation procedure is a well-known technique used to estimate the variances of nonlinear statistics. The procedure takes the first-order Taylor-series approximation of the nonlinear statistic and then substitutes the linear representation into the variance formula appropriate for the sample design. Woodruff (1971) presented the mathematical formulation of this procedure. For stratified multistage surveys, the Taylor-series procedure requires analysis strata and analysis primary sampling units (PSUs), also called replicates, as defined from the sampling strata and PSUs used in the first stage of sampling. The first step was to identify the PSUs used at the first stage of sample selection. The PSUs are the 150 participating institutions. The next step was to sort the PSUs by the 10 institution strata, then by certainty versus noncertainty, and then by the selection order for the noncertainty institutions and by IPEDS ID for the certainty institutions. Two PSUs needed to be collapsed together because one PSU had no responding students. Analysis PSUs were then paired to form analysis strata. 9 This process resulted in 72 analysis strata"}, {"section_title": "Administrative Records Matching Recommendations", "text": "Administrative records matching for the full-scale study will be very similar to procedures conducted in the field test for CPS and NSLDS. A match with the CPS database for FAFSA data will occur for both the 2011-12 and 2012-13 academic years. Each student in the sample will be sent to CPS twice for the 2011-12 application data (once as students are selected for data collection purposes and one final time for deliverable data purposes). Each student will be sent to CPS one time for the 2012-13 data to be used in final data files. Students will likely be matched with the NSLDS database for federal loan and Pell Grant data at three different times during data collection. Two interim matches will be performed in order to have preliminary data with which to work, as well as a one last match for final data. In addition to matching with CPS and NSLDS, the full-scale study will involve administrative records matches with these additional databases: National Student Clearinghouse (NSC) for enrollment and degree data, ACT (for test scores and related information), and The College Board (for SAT test scores and related information). The match with NSC will be performed once all students have been sampled and will only occur one time towards the end of data collection. The database matches with ACT and The College Board will occur in a similar timeframe and will also be performed only one time for all sampled students."}, {"section_title": "Postdata Collection Data File Editing and Preparation Recommendations", "text": "Full-scale student interview and student records data will be edited, processed, and prepared for delivery in a very similar manner to the procedures described for the field test in chapter 6. While there are no revisions to the full-scale plans based on the field test experience, procedures will be enhanced to allow data to be analyzed, released for both public and restricted use, and used in reports. Additional steps necessary to prepare and finalize full-scale data include; \u2022 data will be subject to disclosure analysis and avoidance techniques; \u2022 derived variables will be created based on data from all sources; \u2022 weights will be created; \u2022 key variables will be stochastically imputed; and \u2022 nonresponse bias analysis will be conducted."}]