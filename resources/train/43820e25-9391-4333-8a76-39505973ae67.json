[{"section_title": "Abstract", "text": "Functional linear regression analysis aims to model regression relations which include a functional predictor. The analog of the regression parameter vector or matrix in conventional multivariate or multiple-response linear regression models is a regression parameter function in one or two arguments. If, in addition, one has scalar predictors, as is often the case in applications to longitudinal studies, the question arises how to incorporate these into a functional regression model. We study a varying-coefficient approach where the scalar covariates are modeled as additional arguments of the regression parameter function. This extension of the functional linear regression model is analogous to the extension of conventional linear regression models to varying-coefficient models and shares its advantages, such as increased flexibility; however, the details of this extension are more challenging in the functional case. Our methodology combines smoothing methods with regularization by truncation at a finite number of functional principal components. A practical version is developed and is shown to perform better than functional linear regression for longitudinal data. We investigate the asymptotic properties of varying-coefficient functional linear regression and establish consistency properties."}, {"section_title": "Introduction", "text": "Functional linear regression analysis is an extension of ordinary regression to the case where predictors include random functions and responses are scalars or functions. This methodology has recently attracted increasing interest due to its inherent applicability in longitudinal data analysis and other areas of modern data analysis. For an excellent introduction, see Ramsay and Silverman (2005) . Assuming that predictor process X possesses a square-integrable trajectory (i.e., X \u2208 L 2 (S), where S \u2282 R), commonly This is an electronic reprint of the original article published by the ISI/BS in Bernoulli, 2010, Vol. 16, No. 3, 730-758 . This reprint differs from the original in pagination and typographic detail. with a functional response Y \u2208 L 2 (T ) and T being a subset of the real line R, where \u00b5 X (s) = E(X(s)), s \u2208 S and \u00b5 Y (t) = E(Y (t)), t \u2208 T (Ramsay and Dalzell (1991) ). In analogy to the classical regression case, estimating equations for the regression function are based on minimizing the deviation \u03b2 * (s, t) = argmin\nand analogously for (1.1). To provide a regularized estimator, one approach is to expand \u03b2(\u00b7, \u00b7) in terms of the eigenfunctions of the covariance functions of X and Y , and to use an appropriately chosen finite number of the resulting functional principal component (FPC) scores of X as predictors; see, for example, Silverman (1996) , Silverman (2002, 2005) , Besse and Ramsay (1986) , Ramsay and Dalzell (1991) , Rice and Silverman (1991) , James et al. (2000) , Cuevas et al. (2002) , Cardot et al. (2003) , Hall and Horowitz (2007) , Cai and Hall (2006) , Cardot (2007) and many others. Advances in modern technology enable us to collect massive amounts of data at fairly low cost. In such settings, one may observe scalar covariates, in addition to functional predictor and response trajectories. For example, when predicting a response such as blood pressure from functional data, one may wish to utilize functional covariates, such as body mass index, and also additional non-functional covariates Z, such as the age of a subject. It is often realistic to expect the regression relation to change as an additional covariate such as age varies. To broaden the applicability of functional linear regression models, we propose to generalize models (1.1) and (1.2) by allowing the slope function to depend on some additional scalar covariates Z. Previous work on varying-coefficient functional regression models, assuming the case of a scalar response and of continuously observed predictor processes, is due to Cardot and Sarda (2008) and recent investigations of the varying-coefficient approach include Fan et al. (2007) and Zhang et al. (2008) .\nFor ease of presentation, we consider the case of a one-dimensional covariate Z \u2208 Z \u2282 R, extending (1.1) and (1.2) to the varying-coefficient functional linear regression models Here, \u00b5 X|Z (s) and \u00b5 Y |Z (t) denote the conditional mean function of X and Y , given Z. Intuitively, after observing a sample of n observations,\n, the estimation of the varying slope functions can be achieved using kernel methods, as follows: The necessary regularization of the slope function is conveniently achieved by truncating the Karhunen-Lo\u00e8ve expansion of the covariance function for the predictor process (and the response process, if applicable). To avoid difficult technical issues and enable straightforward and rapid implementation, it is expedient to adopt the two-step estimation scheme proposed and extensively studied by Fan and Zhang (2000) .\nTo this end, we first bin our observations according to the values taken by the additional covariate Z into a partition of Z. For each bin, we obtain the sample covariance functions based on the observations within this bin. Assuming that the covariance functions of the predictor and response processes are continuous in z guarantees that these sample covariance functions converge to the corresponding true covariance functions evaluated at the bin centers as bin width goes to zero and sample size increases. This allows us to estimate the slope function at each bin center consistently, using the technique studied in Yao et al. (2005b) , providing initial raw estimates. Next, local linear smoothing (Fan and Gijbels (1996) ) is applied to improve estimation efficiency, providing our final estimator of the slope function for any z \u2208 Z.\nThe remainder of the paper is organized as follows. In Section 2, we introduce basic notation and present our estimation scheme. Asymptotic consistency properties are reported in Section 3. Finite-sample implementation issues are discussed in Section 4, results of simulation studies in Section 5 and real data applications in Section 6, with conclusions in Section 7. Technical proofs and auxiliary results are given in the Appendix."}, {"section_title": "Varying coefficient functional linear regression for sparse and irregular data", "text": "To facilitate the presentation, we focus on the case of a functional response, which remains largely unexplored. The case with a scalar response can be handled similarly. We also emphasize the case of sparse and irregularly observed data with errors, due to its relevance in longitudinal studies. The motivation of the varying-coefficient functional regression models (1.3) and (1.4) is to borrow strength across subjects, while adequately reflecting the effects of the additional covariate. We impose the following smoothness conditions:\n[A0] The conditional mean and covariance functions of the predictor and response processes depend on Z and are continuous in Z, that is,\nare continuous in z and their respective arguments, and have continuous second order partial derivatives with respect to z.\nNote that [A0] implies that the conditional mean and covariance functions of predictor and response processes do not change radically in a small neighborhood of Z = z. This facilitates the estimation of \u03b2(z, s, t), using the two-step estimation scheme proposed by Fan and Zhang (2000) . While, there, the additional covariate Z is assumed to take values on a grid, in our case, Z is more generally assumed to be continuously distributed. In this case, we assume that the additional variable Z has a compact domain Z and its density f Z (z) is continuous and bounded away from both zero and infinity."}, {"section_title": "Representing predictor and response functions via functional principal components for sparse and irregular data", "text": "Suppose that we have observations on n subjects. For each subject i, conditional on Z i = z i , the square-integrable predictor trajectory X i and response trajectory Y i are unobservable realizations of the smooth random processes (X, Y |Z = z i ), with unknown mean and covariance functions (condition [A0]). The arguments of X(\u00b7) and Y (\u00b7) are usually referred to as time. Without loss of generality, their domains S and T are assumed to be finite and closed intervals. Adopting the general framework of functional data analysis, we assume, for each z, that there exist orthogonal expansions of the covariance functions G X,z (\u00b7, \u00b7) (resp. G Y,z (\u00b7, \u00b7)) in the L 2 sense via the eigenfunctions \u03c8 z,m (resp. \u03c6 z,k ), with non-increasing eigenvalues \u03c1 z,m (resp. \u03bb z,k ), that is,\nInstead of observing the full predictor trajectory X i and response trajectory Y i , typical longitudinal data consist of noisy observations that are made at sparse and irregularly spaced locations or time points, providing sparse measurements of predictor and response trajectories that are contaminated with additional measurement errors (Staniswalis and Lee (1998) , Rice and Wu (2001) , Yao et al. (2005a Yao et al. ( , 2005b ). To adequately reflect the situation of sparse, irregular and possibly subject-specific time points underlying these measurements, we assume that a random number L i (resp. N i ) of measurements for X i (resp. Y i ) is made, at times denoted by S i1 , S i2 , . . . , S iLi (resp. T i1 , T i2 , . . . , T iNi ). Independent of any other random variables, the numbers of points sampled from each trajectory correspond to random variables L i and N i that are assumed to be i.i.d. as L and N (which may be correlated), respectively. For 1\nbe the observation of the random trajectory X i (resp. Y i ) made at a random time S il (resp. T ij ), contaminated with measurement errors \u03b5 il (resp. \u01eb ij ). Here, the random measurement errors \u03b5 il and \u01eb ij are assumed to be i.i.d., with mean zero and variances \u03c3 2 X and \u03c3 2 Y , respectively. They are independent of all other random variables. The following two assumptions are made.\n[A2] For each subject i, L i\n\u223c N ) for a positive discrete-valued random variable with EL < \u221e (resp. EN < \u221e) and\nIt it surprising that under these \"longitudinal assumptions\", where the number of observations per subject is fixed and does not increase with sample size, one can nevertheless obtain asymptotic consistency results for the regression relation. This phenomenon was observed in Yao et al. (2005b) and is due to the fact that, according to (2.3), the target regression function depends only on localized eigenfunctions, localized eigenvalues and cross-covariances of localized functional principal components. However, even though localized, these eigenfunctions and moments can be estimated from pooled data and do not require the fitting of individual trajectories. Even for the case of fitted trajectories, conditional approaches have been implemented successfully, even allowing reasonable derivative estimates to be obtained from very sparse data (Liu and M\u00fcller (2009) \nConditional on Z i = z, the FPC scores of X i and\nrespectively. For all z, these FPC scores \u03b6 z,im satisfy E\u03b6 z,im = 0, corr(\u03b6 z,im1 , \u03b6 z,im2 ) = 0 for any m 1 = m 2 and var(\u03b6 z,im ) = \u03c1 z,m ; analogous results hold for \u03be z,ik . With this notation, using the Karhunen-Lo\u00e8ve expansion as in Yao et al. (2005b) , conditional on Z i , the available measurements of the ith predictor and response trajectories can be represented as"}, {"section_title": "Estimation of the slope function", "text": "For estimation of the slope function, one standard approach is to expand it in terms of an orthonormal functional basis and to estimate the coefficients of this expansion to estimate the slope function in the non-varying model (1.2) (Yao et al. (2005b) ). As a result of the non-increasing property of the eigenvalues of the covariance functions, such expansions of the slope function are often efficient and require only a few components for a good approximation. Truncation at a finite number of terms provides the necessary regularization. Departing from Yao et al. (2005b) , we assume here that an additional covariate Z plays an important role and must be incorporated into the model, motivating (1.4). To make this model as flexible as possible, the conditional mean and covariance functions of the predictor and response processes are allowed to change smoothly with the value of the covariate Z (Assumption [A0]), which facilitates implementation and analysis of the two-step estimation scheme, as in Fan and Zhang (2000) . Efficient implementation of the two-step estimation scheme begins by binning subjects according to the levels of the additional covariate Z i , i = 1, 2, . . . , n. For ease of presentation, we use bins of equal width, although, in practice, non-equidistant bins can occasionally be advantageous. Denoting the bin centers by z (p) , p = 1, 2, . . . , P , and the bin width by h, the pth bin is [z\n2 ) with h = |Z| P , where |Z| denotes the size of the domain of Z, z\n(1) \u2212 h/2 \u2261 inf{z: z \u2208 Z} and z (P ) + h/2 \u2261 sup{z: z \u2208 Z} (note that the last bin is [z\n2 )} be the index set of those subjects falling into bin [z \u2212 h 2 , z + h 2 ) and n z,h = #N z,h the number of those subjects."}, {"section_title": "Raw estimates", "text": "2 ), we use the Yao et al. (2005a) method to obtain our raw estimates\u03bc X,z (p) (\u00b7) and\u03bc Y,z (p) (\u00b7) of the conditional mean trajectories and the raw slope function estimate\u03b2(z (p) , s, t). The corresponding raw estimates of \u03c3 2 X and \u03c3\nwith respect to d 0 and d 1 , and setting\u03bc X,z (p) (s) to be the minimizerd 0 , where \u03ba 1 (\u00b7) is a kernel function and b X,z (p) is the smoothing bandwidth, the choice of which will be discussed in Section 4. We define a similar local linear scatterplot smoother of\u03bc Y,z (p) (t). According to Lemma 2 in the Appendix, raw estimates\u03bc X,z (p) (s) and\u03bc Y,z (p) (t) are consistent uniformly for z (p) , p = 1, 2, . . . , P , for appropriate bandwidths b X,z (p) and b Y,z (p) . Extending Yao et al. (2005b) , the conditional slope function can be represented as\nfor each z, where \u03c8 z,m (\u00b7) and \u03c6 z,k (\u00b7) are the eigenfunctions of covariance functions G X,z (\u00b7, \u00b7) and G Y,z (\u00b7, \u00b7), respectively, and \u03b6 z,m and \u03be z,k are the functional principal component scores of X and Y , respectively, conditional on Z = z.\nTo obtain raw slope estimates\u03b2(z (p) , s, t) for p = 1, 2, . . . , P , we first estimate the conditional covariance functions\nat each bin center, based on the observations falling into the bin, using the approach of Yao et al. (2005b) . From \"raw\" covariances,\n,h and p = 1, 2, . . . , P , and the locally smoothed conditional covarianceG X,z (p) (s 1 , s 2 ) is defined as the minimizer b 0 of the local linear problem\nwhere \u03ba 2 (\u00b7, \u00b7) is a bivariate kernel function and h X,z (p) a smoothing bandwidth. The diagonal \"raw\" covariances G X,i,z (p) (S ij , S ij ) are removed from the objective function of the above minimization problem because\n, where \u03b4 jl = 1 if j = l and 0 otherwise. Analogous considerations apply for\n, respectively, and the differences (\u1e7c X,z (p) (s) \u2212G X,z (p) (s, s)) (and analogously for Y ) can be used to obtain estimates \u03c3\nwith respect to b 0 , b 11 and b 12 , and settingC XY,z (p) (s, t) to be the minimizerb 0 , with smoothing bandwidths h 1,z (p) and h 2,z (p) .\nIn (2.3), the slope function may be represented via the eigenvalues and eigenfunctions of the covariance operators. To obtain the estimates\u03c1\nNote that we estimate the conditional mean functions and conditional covariance functions over dense grids of S and T . Numerical integrations like the one on the left-hand side of (2.4) are done over these dense grids using the trapezoid rule. Note, further, that integrals over individual trajectories are not needed for the regression focus, in that we use conditional expectation to estimate principal component scores, as in (4.1). Due to the fact that\nwe then obtain preliminary estimates of \u03c3 z,mk = E(\u03b6 z,m \u03be z,k ) at the bin centers z (p) , p = 1, 2, . . . , P , by numerical integration,\nWith (2.3), (2.4), (2.5) and (2.6), the raw estimates of \u03b2(z (p) , s, t) ar\u1ebd\nFurther details on the \"global\" case can be found in Yao et al. (2005b) ."}, {"section_title": "Refining the raw estimates", "text": "We establish in the Appendix that the raw estimates\u03bc X,\n, s, t) are consistent. As has been demonstrated in Fan and Zhang (2000) , there are several reasons to refine such raw estimates. For example, the raw estimates are generally not smooth and are based on local observations, hence inefficient. Most importantly, applications require that the function \u03b2(z, s, t) is available for any z \u2208 Z.\nTo refine the raw estimates, the classical approach is smoothing, for which we adopt the local polynomial smoother. Defining\nthe local polynomial smoothing weights for estimating the qth derivative of an underlying function are\nwhere\nT is a unit vector of length r + 1 with the (q + 1)th element being 1 (see Fan and Gijbels (1996) ). Our final estimators are given b\u0177\nDue to the assumption that the variance of the measurement error does not depend on the additional covariate, the final estimators of \u03c3 2 X and \u03c3 2 Y can be taken as simple averages,\u03c3\n(2.8)\nRemark 1. The localization to Z = z, as needed for the proposed varying coefficient model, coupled with the extreme sparseness assumption [A2], which adequately reflects longitudinal designs, is not conducive to obtaining explicit results in terms of convergence rates for the general case. However, by suitably modifying our arguments and coupling them with the rates of convergence provided on page 2891 of Yao et al. (2005b) , we can obtain rates if desired. These are the rates given there, which depend on complex intrinsic properties of the underlying processes, provided that the sample size n is everywhere replaced by nh, the sample size for each bin.\nRemark 2. In this work, we focus on sparse and irregularly observed longitudinal data. For the case where entire processes are observed without noise and are error-free, one can estimate the localized eigenfunctions at rates of L 2 -convergence of (nh) \u22121/2 (see Hall et al. (2006) ), whereh is the smoothing bandwidth. For the moments of the functional principal components, a smoothing step is not needed. Known results will be adjusted by replacing n with nh when conditioning on a fixed covariate level Z = z; see Cai and Hall (2006) and Hall and Horowitz (2007) ."}, {"section_title": "Asymptotic properties", "text": "We establish some key asymptotic consistency properties for the proposed estimators. Detailed technical conditions and proofs can be found in the Appendix.\nThe observed data set is denoted by For\u00f1 \u221d \u221a n, define the event\nwhere n z (p) ,h is the number of observations in the pth bin and\u00f1 \u221d \u221a n means that there exist c 0 and C 0 such that 0 < c 0 \u2264\u00f1/ \u221a n \u2264 C 0 < \u221e. It is shown in Proposition 1 in the Appendix that P (E n ) \u2192 1 as n \u2192 \u221e for P \u221d n 1/8 , as specified by condition (xi). The global consistency of the final mean and slope function estimates follows from the following theorem."}, {"section_title": "Theorem 1 (Consistency of time-varying functional regression). Under conditions [A0], [A1], [A2] and [A3] in Section 2 and conditions", "text": "To study prediction through time-varying functional regression, consider a new predictor process X * with associated covariate Z * . The corresponding conditional expected response process Y * and its prediction\u0176 * are given by\nTheorem 2 (Consistency of prediction). For a new predictor process X * with associated covariate Z * , it holds under the conditions of Theorem 1 that\nare given by (3.2) and (3.3)."}, {"section_title": "Finite-sample implementation", "text": "For the finite-sample case, several smoothing parameters need to be chosen. Following Yao et al. (2005a) , the leave-one-curve-out cross-validation method can be used to select smoothing parameters\n, individually for each bin. Further required choices concern the bin width h, the smoothing bandwidth b and the numbers M and K of included expansion terms in (2.7). The method of cross-validation could also be used for these additional choices, but this incurs a heavy computational load. A fast alternative is a pseudo-Akaike information criterion (AIC) (or pseudo-Bayesian information criterion (BIC)).\n[1] Choose the number of terms in the truncated double summation representatio\u00f1 \u03b2(z (p) , s, t) for M (n) and K(n), using AIC or BIC, as in Yao et al. (2005b) .\n[2] For each bin width h, choose the best smoothing bandwidth b * (h) by minimizing AIC or BIC.\n[3] Choose the bin width h * which minimizes AIC or BIC, while, for each h investigated, we use b * (h) for b.\nFor [1], we will choose M and K simultaneously for all bins, minimizing the conditional penalized pseudo-deviance given by\nwhere P = 2P K for AIC and P = (log n)P K for BIC, with respect to K. Here, for i \u2208 N p ,\nT and with estimated principal components\nAnalogous criteria are used for the predictor process X, selecting K by minimizing AIC(K) and BIC(K). Marginal versions of these criteria are also available.\nIn step [2] , for each bin width h, we first select the best smoothing bandwidth b * (h) based on AIC or BIC and then select the final bin width h * by a second application of AIC or BIC, plugging b * (h) into this selection as follows. For a given bin width h, define the P -by-P smoothing matrix S 0,2 whose (p 1 , p 2 )th element is \u03c9 0,2 (z (p1) , z (p2) , b). The effective number of parameters of the smoothing matrix is then the trace of S T 0,2 S 0,2 (cf. Wahba (1990) ). This suggests minimization of\nleading to b * (h), wher\u00ea\nThe definition of pseudo-BIC scores is analogous. In step [3] , to select the bin width h * , we minimize\nor the analogous BIC score, using b * (h) for each h, as determined in the previous step."}, {"section_title": "Simulation study", "text": "We compare global functional linear regression and varying-coefficient functional linear regression through simulated examples with a functional response. For the case of a scalar response, the proposed varying-coefficient functional linear regression approach achieves similar performance improvements (results not reported). For the finite-sample case, there are several parameters to be selected (see Section 4). In the simulations, we use pseudo-AIC to select bin width h and pseudo-BIC to select the smoothing bandwidth b and the number of regularization terms M (n) and K(n). The domains of predictor and response trajectories are chosen as S = [0, 10] and T = [0, 10], respectively. The predictor trajectories X are generated as X(s) = \u00b5 X (s) + 3 m=1 \u03b6 m \u03c8 m (s) for s \u2208 S, with mean predictor trajectory \u00b5 X (s) = (s + sin(s)), the three eigenfunctions are \u03c8 1 (s) = \u2212 \nThe additional covariate Z is uniformly distributed over [0, 1] . For z \u2208 [0, 1], the slope function is linear in z, \u03b2(z, s, t) = (z + 1)(\u03c8 1 (s)\u03c8 1 (t) + \u03c8 2 (s)\u03c8 2 (t) + \u03c8 3 (s)\u03c8 3 (t)) and the conditional response trajectory is E(Y (t)|X, Z = z) = \u00b5 Y,z (t) + 10 0 \u03b2(z, s, t)(X(s) \u2212 \u00b5 X (s)) ds, where \u00b5 Y,z (t) = (1 + z)(t + sin(t)). We consider the following two cases.\nExample 1 (Regular case). The first example focuses on the regular case with dense measurement design. Observations on the predictor and response trajectories are made at s j = (j \u2212 1)/3 for j = 1, 2, . . . , 31 and t j = (j \u2212 1)/3 for j = 1, 2, . . . , 31, respectively. We assume the measurement errors on both the predictor and response trajectories are distributed as N (0, 1 2 ), that is, \u03c3 Example 2 (Sparse and irregular case). In this example, we make a random number of measurements on each trajectory in the training data set, chosen with equal probability from {2, 3, . . . , 10}. We note that, for the same subject, the number of measurements on the predictor and the number of measurements on the response trajectory are independent. For any trajectory, given the number of measurements, the measurement times are uniformly distributed over the corresponding trajectory domain. The measurement error is distributed as N (0, 1 2 ) for both the predictor and the response trajectories.\nIn both examples, the training sample size is 400. An independent test set of size 1000 is generated with the predictor and response trajectories fully observed. We compare performance using mean integrated squared prediction error (MISPE) 1 1000\nanalogously for the global functional linear regression, where (X * j , Y * j , Z * j ) denotes the data of the jth subject in the independent test set. In Table 1 , we report the mean and standard deviation (in parentheses) of the MISPE of the global and varying-coefficient functional linear regression over 100 repetitions for each case. This shows that in this simulation setting, the proposed varying-coefficient functional linear regression approach reduces MISPE drastically, compared with the global functional linear regression, both for regular and sparse irregular designs.\nTo visualize the differences between predicted conditional expected response trajectories, for a small random sample, in both the regular and sparse and irregular design cases, we randomly choose four subjects from the test set with median values of the integrated squared prediction error (ISPE) for the varying-coefficient functional linear regression. The true and predicted conditional expected response trajectories are plotted in Figure  1 , where the left four panels correspond to the regular design case and the right four to the sparse irregular case. Clearly, the locally varying method is seen to be superior."}, {"section_title": "Applications", "text": "We illustrate the comparison of the proposed varying-coefficient functional linear model with the global functional linear regression in two applications."}, {"section_title": "Egg-laying data", "text": "The egg-laying data represent the entire reproductive history of one thousand Mediterranean fruit flies ('medflies' for short), where daily fecundity, quantified by the number of eggs laid per day, was recorded for each fly during its lifetime; see Carey et al. (1998) for details of this data set and experimental background.\nWe are interested in predicting future egg-laying patterns over an interval of fixed length, but with potentially different starting time, based on the daily fecundity information during a fixed earlier period. The predictor trajectories were chosen as daily fecundity between day 8 and day 17. This interval covers the tail of an initial rapid rise to peak egg-laying and the initial part of the subsequent decline and, generally, the egglaying behavior at and near peak egg-laying is included. It is of interest to study in what form the intensity of peak egg-laying is associated with subsequent egg-laying behavior, as trade-offs may point to constraints that may play a role in the evolution of longevity.\nWhile the predictor process is chosen with a fixed domain, the response process has a moving domain, with a fixed length of ten days, but a different starting age for each subject, which serves as the additional covariate Z. Due to the limited number of subjects in this study, we use a pre-specified discrete set for the values of Z: Figure 1 . In one random repetition, the true (solid) conditional expected response trajectories and predicted conditional expected response trajectories via the global functional linear regression (dot-dashed) and the varying-coefficient functional linear regression (dashed) are plotted for four randomly selected subjects in the independent test set with median integrated squared prediction error. The left four panels and the right four correspond to the regular and sparse irregular cases, respectively. Z = {17, 19, 21, 23, 25, 27, 29, 31, 33} with a pre-specified bin width h = 2. For subject i with z i \u2208 Z, measurements U ij on the predictor trajectory are the daily numbers of eggs on day j + 7, and measurements V ik on the response trajectory correspond to the daily number of eggs on day k + z i for j = 1, 2, . . . , 10 and k = 1, 2, . . ., 10. The numbers of subjects in these bins are 30, 29, 18, 29, 22, 19, 19, 17 and 36, respectively . For each bin, we randomly select 15 subjects as the training set and the remaining subjects are used to evaluate the prediction performance, comparing the performance of the global and the varying-coefficient functional linear regression. The prediction performance is quantified by mean squared prediction error (MSPE), defined for each subject i in the test set as ik denote the predicted daily fecundities corresponding to V ik using the global (resp. the proposed varying-coefficient (local)) functional linear regression.\nThrough pseudo-AIC, the global functional linear regression selects two and three principal components for the predictor and response trajectories, respectively, while the varying-coefficient functional linear regression uses two principal components for both trajectories. After smoothing, the slope functions estimated by the varying-coefficient models are plotted in Figure 2 for different values of Z and the estimated slope function for the global functional linear regression is plotted in the left panel of Figure 3 . Box plots of the ratio MSPE l (i)/ MSPE g (i) for subjects in the test data set are shown in the right panel of Figure 3 for different levels of the covariate Z. There is one outlier above the maximum value for Z = 18 which is not shown. For most bins, the median ratios are seen to be smaller than 1, indicating an improvement of our new varying-coefficient functional linear regression. Denoting the average MSPE (over the independent test data set) of the global and the varying-coefficient functional linear regression by MSPE g and MSPE l , respectively, the relative performance gain (MSPE l \u2212 MSPE g )/MSPE g is found to be \u22120.0810 so that the prediction improvement of the varying-coefficient method is 8.1%.\nBesides prediction, it is of interest to study the dependency of the future egg-laying behavior on peak egg-laying. From the changing slope functions in Figure 2 , we find that, for the segments close to the peak segments, the egg-laying pattern is inverting the peak pattern, meaning that sharper and higher peaks are associated with sharp downturns, pointing to a near-future exhaustion effect of peak egg-laying. In contrast, the shape of egg-laying segments further into the future is predicted by the behavior of the first derivative over the predictor segment so that slow declines near the end of peak egglaying are harbingers of future robust egg-laying. This is in accordance with a model of exponential decline in egg-laying that has been proposed by M\u00fcller et al. (2001) . "}, {"section_title": "BLSA data with scalar response", "text": "As a second example, we use a subset of data from the Baltimore Longitudinal Study of Aging (BLSA), a major longitudinal data set for human aging (Shock et al. (1984) , Pearson et al. (1997) ). The data consist of 1590 male volunteers who were scheduled to be seen twice per year. However, many participants missed scheduled visits or were seen at other than scheduled times so that the data are sparse and irregular with unequal numbers of measurements and different measurement times for each subject. For each subject, current age and systolic blood pressure (SBP) were recorded during each visit. We quantify how the SBP trajectories of a subject available in a middle age range between age 48 and age 53 affect the average of the SBP measurements made during the last five years included in this study, at an older age. The predictor domain is therefore of length five years and the response is scalar. The additional covariate for each subject is the beginning age of the last five-year interval included in the study. After excluding subjects with less than two measurements in the predictor, 214 subjects were included for whom the additional covariate ranged between 55 and 75. We bin the data according to the additional covariate, with bin centers at ages 56.0, 59.0, 62.0, 65.0, 68.5 and 73.0 years and the numbers of subjects in each of these bins are 38, 33, 38, 32, 39 and 34.\nWe randomly selected 25 subjects from each bin for model estimation and used the remaining subjects to evaluate the prediction performance. In contrast to the egg-laying data, the predictor measurements in this longitudinal study are sparse and irregular. Pseudo-BIC selects two principal components for the predictor trajectories for both global and varying-coefficient functional linear regressions. Using the same criterion for relative performance gain as in the previous example, the varying-coefficient functional linear regression achieves 11.8% improvement compared to the global functional linear regression. Estimated slope functions are shown in Figure 5 and predictor trajectories in Figure 4 .\nThe shape changes of the slope functions with changing covariate indicate that the negative derivative of SBP during the middle-age period is associated with near-future SBP. Further into the future, this pattern is reversed and an SBP increase near the right end of the initial period is becoming predictive."}, {"section_title": "Concluding remarks", "text": "Our results indicate that established functional linear regression models can be improved when an available covariate is incorporated. We implement this idea by extending the functional linear model to a varying-coefficient version, inspired by the analogous, highly successful extension of classical regression models. In both application examples, the increased flexibility that is inherent in this extension] leads to clear gains in prediction error. In addition, it is often of interest to ascertain the effect of the additional covariate. This can be done by plotting the regression slopes for each bin defined by the covariate and observing the dependency of this function or surface on the value of the covariate.\nFurther extensions that are of interest in many applications concern the case of multivariate covariates. If the dimension is low, the smoothing methods and binning methods that we propose here can be extended to this case. For higher-dimensional covariates or covariates that are not continuous, one could form a single index to summarize the covariates and thus create a new one-dimensional covariate which then enters the functional regression model in the same way as the one-dimensional covariate that we consider.\nAs seen in the data applications, the major applications of the proposed methodology are expected to come from longitudinal studies with sparse and irregular measurements, where the presence of additional non-functional covariates is common."}, {"section_title": "Appendix: Auxiliary results and proofs", "text": "We note that further details, such as omitted proofs, can be found in a technical report that is available at http://www4.stat.ncsu.edu/~wu/WuFanMueller.pdf.\nA bivariate kernel function \u03ba 2 (\u00b7, \u00b7) is said to be of order (\u03bd, \u2113) with \u03bd = (\u03bd 1 , \u03bd 2 ) if it satisfies\nand\nwhere \u03bd! = \u03bd 1 ! \u00b7 \u03bd 2 !. Similarly, a univariate kernel function \u03ba 1 (\u00b7) is of order (\u03bd, \u2113) for a univariate \u03bd = \u03bd 1 when (A.1) and (A.2) hold for \u2113 2 \u2261 0 on the right-hand side while integrating over the univariate argument u on the left. We introduce the following technical conditions:\n(i) The variable S has compact domain S. Given Z = z, S has conditional density f S,z (s). Assume, uniformly in z \u2208 Z, that \u2202 \u2113 \u2202s \u2113 f S,z (s) exists and is continuous for \u2113 = 2 on S and, further, inf s\u2208S f S,z (s) > 0, analogously for T .\n(ii) Denote the conditional density functions of (S, U ) and (T, V ) by g X,z (s, u) and g Y,z (t, v), respectively. Assume that the derivative\nexists for all arguments (s, u), is uniformly continuous on S \u00d7 R and is Lipschitz continuous in z, for \u2113 = 2, analogously for g Y,z (t, v). (iii) Denote the conditional density functions of quadruples (S 1 , S 2 , U 1 , U 2 ) and (T 1 , T 2 , V 1 , V 2 ) by g 2X,z (s 1 , s 2 , u 1 , u 2 ) and g 2Y,z (t 1 , t 2 , v 1 , v 2 ), respectively. For simplicity, the corresponding marginal conditional densities of (S 1 , S 2 ) and (T 1 , T 2 ) are also denoted by g 2X,z (s 1 , s 2 ) and g 2Y,z (t 1 , t 2 ), respectively. Denote the conditional density of (S, T, U, V ) given Z = z by g XY,z (s, t, u, v) and, similarly, its corresponding conditional marginal density of (S, T ) by g XY,z (s, t). Assume that the derivatives\ng 2X,z (s 1 , s 2 , u 1 , u 2 ) exist for all arguments (s 1 , s 2 , u 1 , u 2 ), are uniformly continuous on S 2 \u00d7 R 2 and are Lipschitz continuous in z for \u2113 1 + \u2113 2 = \u2113, 0 \u2264 \u2113 1 , \u2113 2 \u2264 \u2113 = 2, analogously for g 2Y,z (t 1 , t 2 , v 1 , v 2 ) and g XY,z (s, t, u, v) .\n(viii) Univariate kernel \u03ba 1 and bivariate kernel \u03ba 2 are compactly supported, absolutely integrable and of orders (\u03bd, \u2113) = (0, 2) and ((0, 0), 2), respectively.\n(ix) Assume that sup (z,s)\u2208Z\u00d7S E(E(X(s) \u2212 \u00b5 X,Z (s)) 4 |Z = z)) < \u221e, and analogously for Y . (x) The slope function \u03b2(z, s, t) is twice differentiable in z, that is, for any (s, t) \u2208 S \u00d7 T , The bin width h is chosen such that P \u221d n 1/8 .\nConsider the pth bin and let\n,h is asymptotically distributed as N (n\u03c0 p , n\u03c0 p (1 \u2212 \u03c0 p )) due to the normal approximation to a binomial random variable. Thus,\nis the probability density function of the standard normal distribution. Due to\nWe next prove the consistency of the raw estimate of the mean functions of predictor and response trajectories within each bin. Consider a generic bin [z \u2212 h/2, z + h/2), with bin center z and bandwidth h, and let b X,z and b Y,z be smoothing bandwidths used to estimate \u00b5 X,z (s) and \u00b5 Y,z (t), h X,z and h Y,z for G X,z (s 1 , s 2 ) and G Y,z (t 1 , t 2 ), respectively, h 1,z and h 2,z for C XY,z (s, t), and b X,z,V and\nFor a positive integer l \u2265 1, let {\u03c8 p (t, v), p = 1, 2, . . . , l} be a collection of real functions \u03c8 p : R 2 \u2192 R satisfying the following conditions:\n[C1.1a] The derivative functions\nexist for all arguments (t, v) and are uniformly continuous on T \u00d7 R.\nwhere g Y,z (t, v) is the conditional density of (T, V ), given Z = z.\nProof. Note that |\u03a8 pn,z (t) \u2212 \u00b5 p\u03c8,z (t)| \u2264 |\u03a8 pn,z (t) \u2212 E\u03a8 pn,z (t)| + |E\u03a8 pn,z (t) \u2212 \u00b5 p\u03c8,z (t)| and E|\u03c4 pn | = O(1) implies that \u03c4 pn = O p (1). Standard conditioning techniques lead to\n, perform a Taylor expansion of order \u2113 on the integrand:\nwhere t * is between t and t 1 . Hence, \nwhere the constants do not depend on z. To bound E sup t\u2208T |\u03a8 pn,z (t) \u2212 E\u03a8 pn,z (t)|, we denote the Fourier transform of \u03ba 1 (\u00b7) by \u03b6 1 (t) = e \u2212iut \u03ba 1 (u) du, and letting \u03d5 pn,z (u) =\nDecomposing \u03d5 pn,z (\u00b7) into real and imaginary parts,\nwhere m \u2208 N z,h , analogously for the imaginary part. As a result, we have\n) as a function of Z m is continuous over the compact domain Z and is consequently bounded. Let c 2 = 2 sup Zm\u2208Z E(\u03c8 2 p (T m1 , Y m1 )) < \u221e. Hence, we have (A.4) where the constant c 2 ( |\u03b6 1 (u)| du)/(2\u03c0) does not depend on z.\nThe result follows as condition [A1] implies that n z,h goes to infinity uniformly for z \u2208 Z as n \u2192 \u221e and\n. We next extend Theorem 1 in Yao et al. (2005a) under some additional conditions.\nProof. The proof is similar to the proof of Theorem 1 in Yao et al. (2005a) .\nOur next two lemmas concern the consistency for estimating the covariance functions, based on the observations in the generic bin [z \u2212 h/2, z + h/2). Let {\u03b8 p (r 1 , r 2 , v 1 , v 2 ), p = 1, 2, . . . , l} be a collection of real functions \u03b8 p : R 4 \u2192 R with the following properties:\nand are uniformly continuous on\n, r 2 , v 1 , v 2 ) dr 1 dr 2 dv 1 dv 2 exists and is finite, uniformly bounded on Z;\nProof. This is analogous to the proof of Lemma 1.\nThe proof of the next result is omitted. \nTo estimate variance of the measurement errors, as in Yao et al. (2005a) , we first estimate\n) with smoothing bandwidth b X,z,V (resp. b Y,z,V ) and denote the estimates by\u1e7c X,z (s) (resp.\u1e7c Y,z (t)), removing the two ends of the interval S (resp. T ) to get more stable estimates of \u03c3 2 X (resp. \u03c3 [ \nand analogously for\u03c3 Proof. The result follows straightforwardly from Corollary 1.\nWhile Lemma 3 implies consistency of the estimator of the variance, we also require an extension regarding estimation of the cross-covariance function. Let {\u03b8 p (s, t, u, v), p = 1, 2, . . . , l} be a collection of real functions\u03b8 p : R 4 \u2192 R.\n[C2.1c] For \u2113 \u2265 |\u03bd| + 2 and any pair of \u2113 1 and \u2113 2 such that \u2113 = \u2113 1 + \u2113 2 , \u2113 1 \u2265 \u03bd 1 + 1 and \u2113 2 \u2265 \u03bd 2 + 1, we have, uniformly in z \u2208 Z, bandwidth h 1,z and h 2,z satisfy\nProof. The proof is analogous to that of Lemmas 1 and 3.\n[C6] Uniformly in z \u2208 Z, bandwidths h 1,z and h 2,z satisfy\nLemma 6 (Convergence of the cross-covariance function between X and Y ).\nProof. The proof is similar to that of Lemma 4. (Courant and Hilbert (1953) ). Let I \u2032 Y,z (resp. I \u2032 X,z ) be the set of indices of the eigenfunctions \u03c6 z,k (t) (resp. \u03c8 z,m (s)) corresponding to eigenvalues \u03bb z,k (resp. \u03c1 z,m ) of multiplicity one. We obtain the consistency of\u03bb z,k (resp.\u03c1 z,m ) for \u03bb z,k (resp. \u03c1 z,m ), the consistency of\u03c6 z,k (t) (resp.\u03c8 z,m (s)) for"}, {"section_title": "Consider the real separable Hilbert space L", "text": "is of multiplicity one, and the uniform consistency of\u03c6 z,k (t) (resp.\u03c8 z,m (s)) for \u03c6 z,k (t) (resp. \u03c8 z,m (s)) as well.\nFor f, g, h \u2208 H Y , define the rank one operator f \u2297 g : h \u2192 f, h g. Denote the separable Hilbert space of Hilbert-Schmidt operators on\n2 is the adjoint of T 2 and {u j : j \u2265 1} is any complete orthonormal system in H Y . The covariance operator G Y,z (resp.G Y,z ) is generated by the kernel for small h (large P \u221d 1/h) and small b. Moreover, the usual boundary techniques can be applied near the two end points. Consequently,we have (1 + o(1)).\nDue to the compactness of Z, the above o-term is uniform in z \u2208 Z. This implies that The convergence results in Theorem 1 imply that T (Y * (t)\u2212\u0176 * (t)) 2 dt P \u2192 0, as desired."}]