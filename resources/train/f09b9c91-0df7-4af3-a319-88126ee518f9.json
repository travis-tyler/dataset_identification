[{"section_title": "List of Figures", "text": ""}, {"section_title": "List of Tables", "text": ""}, {"section_title": "Introduction", "text": "What Is PIAAC? The Program for the International Assessment of Adult Competencies (PIAAC) is a cyclical, large-scale study of adult skills and life experience focusing on education and employment that was developed and organized by the Organization for Economic Cooperation and Development (OECD). In the United States, the study was conducted in 2011\u221212 with a nationally representative sample of 5,000 adults between the ages of 16 and 65. Similar nationally representative samples of adults were surveyed in each of the 22 other participating countries. 1 The goal of PIAAC is to assess and compare the basic skills and the broad range of competencies of adults around the world. The assessment focuses on cognitive and workplace skills necessary for successful participation in 21st-century society and the global economy. Specifically, PIAAC measures relationships between individuals' educational background, workplace experiences and skills, occupational attainment, use of information and communication technology, and cognitive skills in the areas of literacy, numeracy, and problem solving in technology-rich environments."}, {"section_title": "History of International Adult Literacy Assessments", "text": "PIAAC builds on knowledge and experiences gained from previous international adult assessmentsthe International Adult Literacy Survey (IALS) and the Adult Literacy and Lifeskills Survey (ALL). Statistics Canada initiated both of these earlier assessments. PIAAC improves upon the content frameworks of these assessments as well as their design and methodologies. PIAAC seeks to ensure continuity with previous literacy surveys and includes items from IALS and ALL. PIAAC improves on quality assurance standards, extends the definitions of literacy and numeracy, presents the problem-solving domain to emphasize skills used in technology-rich environments, and provides more information about individuals with low levels of literacy by assessing reading component skills."}, {"section_title": "What Does PIAAC Measure?", "text": "PIAAC is designed to assess adults in different countries over a broad range of abilities, from simple reading to complex problem-solving skills. To do this, PIAAC defines four core competency domains of adult cognitive skills that are seen as key to facilitating the social and economic participation of adults in advanced economies: literacy, reading components, numeracy, and problem solving in technology-rich environments. All participating countries and regions are required to assess the literacy and numeracy domains, but the reading components and problem solving in technology-rich environments domains are both optional. The United States assessed all four domains. For a list of the subject experts for each of those domains, see appendix A. For a more detailed description of the four domains, see appendix B. PIAAC tasks developed for these domains are authentic, culturally appropriate, and drawn from real-life situations that are expected to be important or relevant in different contexts. Tasks are intended to be reflective of adults' daily lives across cultures, even if not every adult is necessarily familiar with every task. PIAAC is not designed to provide individual scores, but rather it measures how groups of adults perform on the domains. Each respondent takes only a portion of the items in each domain."}, {"section_title": "Literacy", "text": "The PIAAC literacy framework expands on the definition of literacy previously used in IALS and ALL. PIAAC broadly defines literacy as \"understanding, evaluating, using and engaging with written text to participate in society, to achieve one's goals and to develop one's knowledge and potential.\" (Organization for Economic Cooperation and Development [OECD], 2012.)\nThe PIAAC literacy framework expands on the definition of literacy previously used in IALS and ALL. PIAAC broadly defines literacy as \"understanding, evaluating, using and engaging with written text to participate in society, to achieve one's goals and to develop one's knowledge and potential.\" (Organization for Economic Cooperation and Development [OECD] 2012.) The purpose of this expanded definition is to highlight the ranges of cognitive processes involved in literacy, focus on a more active role of individuals in society, and include various text types, both in print and electronic formats, in the measurement of literacy. PIAAC items include continuous texts (e.g., text in sentences and paragraphs), non-continuous texts (e.g., schedules, graphs, and maps), and electronic texts (including hypertext or text in interactive environments, such as forms and blogs). Task activities are presented in home, work, and community contexts, addressing various purposes adults pursue in their everyday lives. Based on the PIAAC framework, literacy tasks include items in paper-and-pencil and computer-based delivery modes that cover a range of difficulties-low, middle, and high-to present a comprehensive picture of the level of adult literacy skills in each country or region."}, {"section_title": "Reading components", "text": "The primary goal of the PIAAC reading components measure is to provide information about the literacy skills of adults at the lower end of the literacy spectrum-specifically, whether they have the foundational skills to develop the higher literacy and numeracy abilities necessary for functioning in society. The reading components assessment focuses on elements of reading that are comparable across the range of languages in the participating countries: reading vocabulary, sentence comprehension, and basic passage comprehension.\nThe primary goal of the PIAAC reading components is to provide information about the literacy skills of adults at the lower end of the literacy spectrum-specifically, whether they have the foundational skills to develop the higher literacy and numeracy abilities necessary to function in society. The reading components assessment focuses on elements of reading that are comparable across the range of languages in the participating countries and regions: reading vocabulary, sentence comprehension, and basic passage comprehension."}, {"section_title": "Numeracy", "text": "The primary goal of PIAAC's numeracy assessment is to evaluate basic mathematical and computational skills that are considered fundamental for functioning in everyday work and social life. Numeracy in the PIAAC framework is defined as \"the ability to access, use, interpret, and communicate mathematical information and ideas, to engage in and manage mathematical demands of a range of situations in adult life.\" (Organization for Economic Cooperation and Development [OECD], 2012.) Problem solving in technology-rich environments PIAAC represents the first attempt to assess problem solving in technology-rich environments on a large scale and as a single dimension in an international context. PIAAC defines problem solving in technology-rich environments as \"using digital technology, communication tools, and networks to acquire and evaluate information, communicate with others, and perform practical tasks.\" (Organization for Economic Cooperation and Development [OECD], 2012.)\nThe primary goal of PIAAC's numeracy assessment is to evaluate basic mathematical and computational skills that are considered fundamental for functioning in everyday work and social life. Numeracy in the PIAAC framework is defined as \"the ability to access, use, interpret, and communicate mathematical information and ideas, to engage in and manage mathematical demands of a range of situations in adult life.\" (Organization for Economic Cooperation and Development [OECD] 2012.) The PIAAC numeracy domain is built on previous large-scale assessments of this domain, schooloriented assessments, and a review of requirements of workplace skills, adult learning, and mathematics and statistics education. The tasks that measure this domain involve managing a situation or solving a problem in a practical context-in home, work, or community settings. These tasks ask respondents to work with numbers, proportions, measurements, and statistical concepts, and then call for participants to compute, interpret, and communicate the results and mathematical content. The situations and problems presented in these tasks involve objects or pictures, text, numbers, graphs, and technology-based displays. They also require basic mathematical skills in computation, proportions and percentages, an understanding of measurement concepts and procedures, and working with simple formulas. Respondents also encounter more complex items that require using models to predict future needs, and an understanding of basic statistical concepts and displays. In addition, PIAAC numeracy assessment items \u2022 are set in authentic and culturally appropriate contexts, \u2022 measure different levels of ability, and \u2022 use the standard measuring systems of the participating country or region. Numeracy tasks include items in paper-and-pencil and computer-based delivery modes that cover a range of difficulties-low, middle, and high-to present a comprehensive picture of the level of adult numeracy skills in each country or region. Exhibit B-3. Description of PIAAC proficiency levels on the numeracy scale: 2012 Proficiency levels and cut scores for numeracy"}, {"section_title": "Skill use and the background questionnaire", "text": "In addition to focusing on the direct measurement of adult competencies in the three main cognitive domains of literacy, numeracy, and problem solving in technology-rich environments, PIAAC also examines adults' intrapersonal, interpersonal, and professional skills through a background questionnaire. For a more detailed description of the skill use and the background questionnaire, see appendix C."}, {"section_title": "Reporting Results", "text": "PIAAC results are reported in two ways: as scale scores on a 0-500 scale in three domains (literacy, numeracy, and problem solving in technology-rich environments), 2 and as percentages of adults reaching established proficiency levels. PIAAC reports five proficiency levels for literacy and numeracy (Below level 1, Level 1, Level 2, Level 3, and Level 4/5) and four levels for problem solving in technology-rich environments (Below level 1, Level 1, Level 2, and Level 3). Across all countries, only 2 percent of adults performed at Level 5 on many of the variables in the literacy and numeracy scales. This report follows OECD reporting conventions by combining the top two proficiency levels for the literacy and numeracy scales. There were fewer items in the problem solving in technology-rich environments assessment to define the scale, which accounts for why there are fewer proficiency levels in this scale. Appendix B provides information about interpreting the proficiency levels. The purpose of this First Look report is to introduce PIAAC data through the presentation of data figures and tables and selected findings. 3 However, readers are cautioned not to draw causal inferences. Many of the variables examined in this report may be related to one another, but the complex interactions and relationships among them have not been explored. The variables examined here are also just a few of the variables that can be examined in these data; they were selected to demonstrate the range of information available from the study. The release of this report is intended to encourage more in-depth analysis of the data using more sophisticated statistical methods. All statistically significant differences described in this report are at the .05 level. No statistical adjustments to account for multiple comparisons were used. Differences that are statistically significant are discussed using comparative terms such as \"higher\" and \"lower.\" Differences that are not statistically significant are either not discussed or referred to as \"not measurably different\" or \"not statistically significant.\" In the latter case, failure to find a difference as statistically significant does not necessarily mean that there was no difference. It could be that a real difference cannot be detected by the significance test because of small sample size or imprecise measurement in the sample. If the statistical test is significant, this means that there is convincing evidence (though no guarantee) of a real difference in the population. However, it is important to remember that statistically significant results do not necessarily identify those findings that have policy significance or practical importance. See appendix C for more information about statistical testing."}, {"section_title": "Trend Results", "text": "This report presents overall trend comparisons over time for the total adult population in the areas of literacy and numeracy. In literacy, comparisons are made between PIAAC (2012) and both ALL (2003\u22122008) and IALS (1994IALS ( \u22121998. In numeracy, trend comparisons are made between PIAAC (2012) and ALL (2003ALL ( \u22122008. In both the literacy and numeracy domains, approximately 60 percent of the items are common between PIAAC and previous international surveys to ensure the comparability of these domains."}, {"section_title": "Selected Findings", "text": "Average scores on the PIAAC literacy scale for adults age 16 to 65 ranged from 250 in Italy to 296 in Japan. The U.S. average score was 270. Compared with the U.S. average score, average scores in 12 countries were higher, in 5 countries they were lower, and in 5 countries they were not significantly different (see figure 1-A). Average scores on the PIAAC numeracy scale for adults age 16 to 65 ranged from 246 in Spain to 288 in Japan. The U.S. average score was 253. Compared with the U.S. average score, average scores in 18 countries were higher, in 2 countries they were lower, and in 2 countries they were not significantly different (see figure 1-B). Average scores on the PIAAC problem solving in technology-rich environments scale for adults age 16 to 65 ranged from 275 in Poland to 294 in Japan. The U.S. average score was 277. Compared with the U.S. average score, average scores in 14 countries were higher and in 4 countries they were not significantly different (see figure 1-C). Twelve percent of U.S. adults age 16 to 65 performed at the highest proficiency level (4/5) on the PIAAC literacy scale. The percentage of adults performing at this level was higher than in the U.S. in 7 countries (Japan, Finland, Netherlands, Australia, Sweden, Norway, and Canada), lower in 11 countries (Denmark, Poland, Czech Republic, Austria, France, Ireland, Republic of Korea, Slovak Republic, Cyprus, Spain, and Italy), and not significantly different in 4 countries (England and Northern Ireland-United Kingdom, Flanders-Belgium, Estonia, and Germany) (see figure 2-A). Nine percent of U.S. adults age 16 to 65 performed at the highest proficiency level (4/5) on the PIAAC numeracy scale. The percentage of adults performing at this level was higher than in the U.S. in 15 countries (Finland, Japan, Sweden, Flanders-Belgium, Norway, Netherlands, Denmark, Germany, Australia, Austria, Slovak Republic, Canada, Czech Republic, England and Northern Ireland-United Kingdom, and Estonia), lower in 3 countries (Republic of Korea, Italy, and Spain), and not significantly different in 4 countries (France, Poland, Ireland, and Cyprus) (see figure 2-B). Six percent of U.S. adults age 16 to 65 performed at the highest proficiency level (3) on the PIAAC problem solving in technology-rich environments scale. The percentage of adults performing at this level was higher than in the U.S. in 8 countries (Japan, Finland, Sweden, Czech Republic, Canada, Germany, Netherlands, and Australia), lower in 2 countries (Ireland and Slovak Republic), and not significantly different in 8 countries (Poland, Denmark, Flanders-Belgium, Norway, England and Northern Ireland-United Kingdom, Estonia, Austria, and Republic of Korea) (see figure 2-C). The percentage of U.S. adults between 55 and 65 years old who scored at the highest proficiency level (4/5) on the PIAAC literacy scale was higher than the international average for this age group. For all other age groups, the percentage of adults performing at the highest proficiency level on the PIAAC literacy scale was not significantly different for U.S. adults and the international average for their peers (see figure 3-A). The percentage of U.S. adults between 55 and 65 years old who scored at the highest proficiency level (4/5) on the PIAAC numeracy scale was not significantly different than the international average for this age group. For all other age groups, the percentage of adults performing at the highest proficiency level (4/5) on the PIAAC numeracy scale was lower for U.S. adults than the international average for their peers (see figure 3-B). The percentages of U.S. adult males and U.S. adult females age 16 to 65 performing at the highest proficiency level (4/5) on the PIAAC numeracy scale were lower than the international averages for their peers at this level (see figure 4-B). The percentage of adults age 16 to 65 born in the United States who performed at the highest proficiency level (4/5) on the PIAAC literacy scale was higher than the percentage of adults born outside the United States who performed at this level (see figure 5-A). The percentage of U.S. adults age 16 to 65 with graduate or professional degrees who performed at the highest proficiency level (4/5) on the numeracy scale was lower than the international average for adults with graduate or professional degrees who performed at this level (see figure 6-B). The percentage of White U.S. adults age 16 to 65 at the highest proficiency level (4/5) on the literacy scale was higher than the percentages of their Black or Hispanic peers at this level (see figure 7-A). Among adults age 16 to 65 who use information and communication technology (ICT) skills the most at work (top quintile), the percentage of U.S. adults performing at the highest proficiency level (3) on problem solving in technology-rich environments was not significantly different than the international average for adults who use ICT skills most at work (see figure 8-C). The percentage of employed U.S. adults age 16 to 65 performing at the highest proficiency level (4/5) on the PIAAC numeracy scale was lower than the international average of employed adults at this level (see figure 9-B). The percentage of U.S. adults age 16 to 65 with excellent health performing at the highest proficiency level (3) on the problem solving in technology-rich environments scale was higher than their peers who reported having poor, fair, or good health at this level (see figure 10-C). In the United States, there was no statistical difference in the percentage performing at the highest proficiency level (3) on problem solving in technology-rich environments between adults age 16 to 65 who reported having a learning disability and those who reported not having a learning disability (see figure 11-C). On the literacy scale, the average score for adults age 16 to 65 in the United States on PIAAC (2012) was not significantly different from the score for U.S. adults on ALL (2003-08), but was lower than the score for U.S. adults on IALS (1994-98) (see table 1-A). On the numeracy scale, the average score for adults age 16 to 65 in the United States was lower on PIAAC (2012) than on ALL (2003-08) (see table 1-B).       1 Country-and region-specific results are available at http://www.oecd.org/site/piaac/publicdataandanalysis.htm. NOTE: Percentages of adults age 16 to 65 by 10-year age intervals appear in parentheses. Due to differences in the sampling and data collection, the percentage distribution for the categories in the problem solving in technology-rich environments scale may differ from the percentages for the same categories in the literacy and numeracy scales (see appendix C for more detailed information  1 Country-and region-specific results are available at http://www.oecd.org/site/piaac/publicdataandanalysis.htm. NOTE: Percentages of adults age 16 to 65 by gender appear in parentheses. Due to differences in the sampling and data collection, the percentage distribution for the categories (e.g., male and female) in the problem solving in technology-rich environments scale may differ from the percentages for the same categories in the literacy and numeracy scales (see appendix C for more detailed information). Detail may not sum to totals because of rounding.     1 Country-and region-specific results are available at http://www.oecd.org/site/piaac/publicdataandanalysis.htm. NOTE: A scale of \"skill use at work\" was created from information that adults age 16 to 65 reported about the frequency with which they used particular skills at work. For each of the skills asked about, respondents who are currently employed could report using it \"never,\" \"less than once a month,\" \"less than once a week but at least once a month,\" \"at least once a week but not every day,\" or \"every day\" at work. Based on these responses, a scale was created using a generalized partial credit model (GPCM) to quantify the distribution of skill use at work. The top quintile of this scale indicates adults whose skill use at work is at the highest reported levels; the bottom quintile of this scale indicates adults whose skill use at work is at the lowest reported levels. Percentages of adults age 16 to 65 who fall into each of the quintiles are reported in parentheses to the right of the quintile labels. The quintile boundaries are based on the international PIAAC database, so they do not contain exactly 20 percent in each of the countries. Respondents who reported that they never used a skill on any of the items that formed part of the set of reading skills used at work, numeracy skills used at work, or ICT skills used at work constitute a separate category not shown here.  1 Country-and region-specific results are available at http://www.oecd.org/site/piaac/publicdataandanalysis.htm. NOTE: Percentages of adults age 16 to 65 by employment status appear in parentheses. Due to differences in the sampling and data collection, the percentage distribution for the categories in the problem solving in technology-rich environments scale may differ from the percentages for the same categories in the literacy and numeracy scales (see appendix C for more detailed information). The unemployed comprise all persons above a specified age who during the reference period were in the following categories: without work, that is, were not in paid employment or self-employment during the reference period; currently available for work, that is, were available for paid employment or self-employment during the reference period; and seeking work, that is, had taken specific steps in a specified recent period to seek paid employment or self-employment.    "}, {"section_title": "Figures and Tables", "text": ""}, {"section_title": "More Information About PIAAC", "text": "This report is a first look at the PIAAC 2012 results and provides findings for only a few select results. For more PIAAC 2012 results and information about the assessment: \u2022 Preview and print a selection of data on the performance of U.S. adults on the PIAAC assessment for various topics across all three domains: http://nces.ed.gov/surveys/piaac/index.asp. \u2022 Find more information about the international results: http://www.oecd.org/site/piaac/. \u2022 Explore PIAAC data in the International Data Explorer (IDE): http://www.oecd.org/site/piaac/ publicdataandanalysis.htm. \u2022 Access public-use data files that are available at http://nces.ed.gov/surveys/piaac/index.asp. \u2022 Access restricted-use data files that are available to NCES Restricted-use Data Licensees. More information on licenses can be found at http://nces.ed.gov/pubsearch/licenses.asp."}, {"section_title": "Overview", "text": "PIAAC defines four core competency domains of adult cognitive skills that are seen as key to facilitating the social and economic participation of adults in advanced economies: Reading components Numeracy Problem solving in technology-rich environments 1 As described in appendix C, PIAAC is administered in either paper-and-pencil mode or via computer interface, depending on the assessment domain. Literacy and numeracy are offered in both paper-andpencil and computer modes. Reading components, which are designed to provide information about the literacy skills of adults at the lower end of the literacy spectrum, are offered only in paper-andpencil mode. Problem solving in technology-rich environments is administered via computer only. The OECD oversees the work of several teams of experts in the development of assessment frameworks in each of the domains (see appendix A). Assessment frameworks are available at http:// www.oecd.org/site/piaac/publications.htm. Information about the item development and proficiency level setting process will be included in a forthcoming PIAAC technical report from OECD."}, {"section_title": "\u2022 \u2022 \u2022", "text": "The reading vocabulary section asks participants to identify the best word that should be used to label different graphic illustrations. This task measures whether participants can identify common, concrete print words used in everyday adult interactions in the community, home, and workplace. It is not meant to determine the vocabulary knowledge (breadth or depth) of the participants. The sentence comprehension section asks participants to identify whether sentences of varying grammatical/syntactic complexity make sense. This task measures whether participants can understand and correctly judge the accuracy of the content of sentences. The basic passage comprehension section asks participants to make a choice between a correct and an incorrect word to complete a sentence within a passage. This task measures whether respondents comprehend text in context and can appropriately use words in ways that characterize fluency. The reading component portion of the assessment is optional for countries and regions participating in PIAAC. In countries and regions that adopt the reading components tasks, participants who decide not to take the computer-based assessment, and those who fail to pass the computer-administered information and communication technology (ICT) skills and literacy/numeracy \"Core\" items, are directed to the reading components tasks. (Additional information about the administration of the assessment and the \"Core\" items can be found in appendix C.) Data from the reading components portion of the assessment are not reported separately in this First Look report, but can be accessed from the International Data Explorer (IDE) at http://www.oecd.org/site/piaac/publicdataandanalysis.htm. Exhibit B-1. Description of PIAAC proficiency levels on the literacy scale: 2012 Proficiency levels and cut scores for literacy"}, {"section_title": "Literacy task descriptions", "text": "Level 5 (376 -500) At this level, tasks may require the respondent to search for and integrate information across multiple, dense texts; construct syntheses of similar and contrasting ideas or points of view; or evaluate evidenced based arguments. Application and evaluation of logical and conceptual models of ideas may be required to accomplish tasks. Evaluating reliability of evidentiary sources and selecting key information is frequently a key requirement. Tasks often require respondents to be aware of subtle, rhetorical cues and to make high-level inferences or use specialized background knowledge."}, {"section_title": "Level 4 (326 -375)", "text": "Tasks at this level often require respondents to perform multiple-step operations to integrate, interpret, or synthesize information from complex or lengthy continuous, non-continuous, mixed, or multiple type texts. Complex inferences and application of background knowledge may be needed to perform successfully. Many tasks require identifying and understanding one or more specific, non-central ideas in the text in order to interpret or evaluate subtle evidence-claim or persuasive discourse relationships. Conditional information is frequently present in tasks at this level and must be taken into consideration by the respondent. Competing information is present and sometimes seemingly as prominent as correct information.\nTasks in this level require the respondent to understand a broad range of mathematical information that may be complex, abstract or embedded in unfamiliar contexts. These tasks involve undertaking multiple steps and choosing relevant problem-solving strategies and processes. Tasks tend to require analysis and more complex reasoning about quantities and data; statistics and chance; spatial relationships; change, proportions, and formulas. Tasks in this level may also require comprehending arguments or communicating well-reasoned explanations for answers or choices."}, {"section_title": "Level 3 (276 -325)", "text": "Texts at this level are often dense or lengthy, including continuous, non-continuous, mixed, or multiple pages. Understanding text and rhetorical structures become more central to successfully completing tasks, especially in navigation of complex digital texts. Tasks require the respondent to identify, interpret, or evaluate one or more pieces of information, and often require varying levels of inferencing. Many tasks require the respondent construct meaning across larger chunks of text or perform multi-step operations in order to identify and formulate responses. Often tasks also demand that the respondent disregard irrelevant or inappropriate text content to answer accurately. Competing information is often present, but it is not more prominent than the correct information.\nTasks in this level require the respondent to understand mathematical information which may be less explicit, embedded in contexts that are not always familiar and represented in more complex ways. Tasks require several steps and may involve the choice of problem-solving strategies and relevant processes. Tasks tend to require the application of number sense and spatial sense; recognizing and working with mathematical relationships, patterns, and proportions expressed in verbal or numerical form; interpretation and basic analysis of data and statistics in texts, tables, and graphs."}, {"section_title": "Level 2 (226 -275)", "text": "At this level, the complexity of text increases. The medium of texts may be digital or printed, and texts may be comprised of continuous, non-continuous, or mixed types. Tasks in this level require respondents to make matches between the text and information, and may require paraphrase or low-level inferences. Some competing pieces of information may be present. Some tasks require the respondent to cycle through or integrate two or more pieces of information based on criteria, compare and contrast or reason about information requested in the question, or navigate within digital texts to access-and-identify information from various parts of a document.\nTasks in this level require the respondent to identify and act upon mathematical information and ideas embedded in a range of common contexts where the mathematical content is fairly explicit or visual with relatively few distractors. Tasks tend to require the application of two or more steps or processes involving calculation with whole numbers and common decimals, percents and fractions; simple measurement and spatial representation; estimation; interpretation of relatively simple data and statistics in texts, tables, and graphs."}, {"section_title": "Level 1 (176 -225)", "text": "Most of the tasks at this level require the respondent to read relatively short digital or print continuous, non-continuous, or mixed texts to locate a single piece of information which is identical to or synonymous with the information given in the question or directive. Some tasks may require the respondent to enter personal information onto a document, in the case of some non-continuous texts. Little, if any, competing information is present. Some tasks may require simple cycling through more than one piece of information. Knowledge and skill in recognizing basic vocabulary, evaluating the meaning of sentences, and reading of paragraph text is expected.\nTasks in this level require the respondent to carry out basic mathematical processes in common, concrete contexts where the mathematical content is explicit with little text and minimal distractors. Tasks usually require simple one-step or two-step processes involving performing basic arithmetic operations; understanding simple percents such as 50 percent; or locating, identifying, and using elements of simple or common graphical or spatial representations."}, {"section_title": "Below Level 1 (0 -175)", "text": "The tasks at this level require the respondent to read brief texts on familiar topics to locate a single piece of specific information. Only basic vocabulary knowledge is required, and the reader is not required to understand the structure of sentences or paragraphs or make use of other text features. There is seldom any competing information in the text and the requested information is identical in form to information in the question or directive. While the texts can be continuous, the information can be located as if the text were non-continuous. As well, tasks below level 1 do not make use of any features specific to digital texts. Level 1: Generic medicine (Item ID: C309A321) Difficulty score: 219 The stimulus is a short newspaper article entitled \"Generic medicines: Not for the Swiss\". It has two paragraphs and a table in the middle displaying the market share of generic medicines in 14 European countries and the United States. The test-taker is asked to determine the number of countries in which the generic drug market accounts for 10 percent or more of total drug sales. The test-taker has to count the number of countries with a market share greater than 10 percent. The percentages are sorted in descending order to facilitate the search. The phrase \"drug sales\", however, does not appear in the text; therefore, the test-taker needs to understand that \"market share\" is a synonym for \"drug sales\" in order to answer the question.\nProblem solving in technology-rich environments PIAAC represents the first attempt to assess problem solving in technology-rich environments on a large scale and as a single dimension in an international context. PIAAC defines problem solving in technology-rich environments as \"using digital technology, communication tools, and networks to acquire and evaluate information, communicate with others, and perform practical tasks.\" (Organization for Economic Cooperation and Development [OECD], 2012.) Digital technology has revolutionized access to information and communication capabilities over the past two decades. In particular, the Internet has increased instantaneous access to large amounts of information and has expanded instant voice, text, and graphics capabilities across the globe. In order to effectively operate in these environments, it is necessary to have \u2022 \u2022 knowledge of how various technological environments are structured (e.g., an understanding of the basics of the environment, including how to use command names, drop-down menus, naming protocols for files and folders, and links in a web page); and the ability to interact effectively with digital information; understand electronic texts, images, graphics, and numerical data; and locate, evaluate, and critically judge the validity, accuracy, and appropriateness of the accessed information. These skills constitute the core aspects of the problem solving in technology-rich environments domain. Items in this domain present tasks of varying difficulty in simulated software applications using commands and functions commonly found in email, web pages, and spreadsheets. These tasks range from purchasing particular goods or services online and finding interactive health information to managing personal information and business finances. PIAAC recognizes the diversity of digital technologies and the fact that they are evolving at a rapid pace, but due to implementation constraints, the first round of PIAAC was limited to using computers and simulated computer networks. The tasks assessing problem solving in technology-rich environments were only administered via computer and therefore only those taking the computerized assessment received a score in this domain. Exhibit B-5. Description of PIAAC proficiency levels on the problem solving in technology-rich environments scale: 2012 Proficiency levels and cut scores for problem solving in technology-rich environments Problem solving in technology-rich environments task descriptions Level 3 (341 -500) At this level, tasks typically require the use of both generic and more specific technology applications. Some navigation across pages and applications is required to solve the problem. The use of tools (e.g., a sort function) is required to make progress toward the solution. The task may involve multiple steps and operators. In terms of cognitive processing, the problem goal may have to be defined by the person, and the criteria to be met may or may not be explicit. There are typically high monitoring demands. Unexpected outcomes and impasses are likely to occur. The task may require evaluating the relevance and the reliability of information in order to discard distractors. Integration and inferential reasoning may be needed to a large extent."}, {"section_title": "Below Level 1: Election results (Item ID: C302BC02)", "text": "Difficulty score: 162 The stimulus consists of a short report of the results of a union election containing several brief paragraphs and a simple table identifying the three candidates in the election and the number of votes they received. The test-taker is asked to identify which candidate received the fewest votes. He or she needs to compare the number of votes that the three candidates received and identify the name of the candidate who received the fewest votes. The word \"votes\" appears in both the question and in the table and nowhere else in the text."}, {"section_title": "Numeracy task descriptions", "text": "Level 5 (376 -500) Tasks in this level require the respondent to understand complex representations and abstract and formal mathematical and statistical ideas, possibly embedded in complex texts. Respondents may have to integrate multiple types of mathematical information where considerable translation or interpretation is required; draw inferences; develop or work with mathematical arguments or models; justify, evaluate, and critically reflect upon solutions or choices."}, {"section_title": "Level 2 (291 -340)", "text": "At this level, tasks typically require the use of both generic and more specific technology applications. For instance, the person may have to make use of a novel online form. Some navigation across pages and applications is required to solve the problem. The use of tools (e.g., a sort function) can facilitate the resolution of the problem. The task may involve multiple steps and operators. In terms of cognitive processing, the problem goal may have to be defined by the person, though the criteria to be met are explicit. There are higher monitoring demands. Some unexpected outcomes or impasses may appear. The task may require evaluating the relevance of a set of items to discard distractors. Some integration and inferential reasoning may be needed."}, {"section_title": "Level 1 (241 -290)", "text": "At this level, tasks typically require the use of widely available and familiar technology applications, such as email software or web browser. There is little or no navigation required to access the information or commands required to solve the problem. The problem may be solved regardless of one's awareness and use of specific tools and functions (e.g., a sort function). The task involves few steps and a minimal number of operators. At a cognitive level, the person can readily infer the goal from the task statement; problem resolution requires one to apply explicit criteria; there are few monitoring demands (e.g., the person does not have to check whether they have used the adequate procedure or made progress toward the solution). Identifying contents and operators can be done through simple match; only simple forms of reasoning (e.g., assigning items to categories) are required; there is no need to contrast or integrate information."}, {"section_title": "Below Level 1 (0 -240)", "text": "Tasks are based on well-defined problems involving the use of only one function within a generic interface to meet one explicit criterion without any categorical, inferential reasoning or transforming of information. Few steps are required and no subgoal has to be generated. Exhibit B-6. Examples of problem solving in technology-rich environments items Items that exemplify the pertinent features of the proficiency levels in the domain of problem solving in technology-rich environments are described below. Level 3: Meeting Rooms (Item ID: U02) Difficulty score: 346 This task involves managing requests to reserve a meeting room on a particular date using a reservation system. Upon discovering that one of the reservation requests cannot be accommodated, the test-taker has to send an email message declining the request. Successfully completing the task involves taking into account multiple constraints (e.g., the number of rooms available and existing reservations). Impasses exist, as the initial constraints generate a conflict (one of the demands for a room reservation cannot be satisfied). The impasse has to be resolved by initiating a new sub-goal, i.e., issuing a standard message to decline one of the requests. Two applications are present in the environment: an email interface with a number of emails stored in an inbox containing the room reservation requests, and a web-based reservation tool that allows the user to assign rooms to meetings at certain times. The item requires the test-taker to use information from a novel web application and several email messages, establish and apply criteria to solve a scheduling problem where an impasse must be resolved, and communicate the outcome. The task involves multiple applications, a large number of steps, a built-in impasse, and the discovery and use of ad hoc commands in a novel environment. The test-taker has to establish a plan and monitor its implementation in order to minimize the number of conflicts. In addition, the test-taker has to transfer information from one application (email) to another (the room-reservation tool). Level 2: Club Membership (Item ID: U19b) Difficulty score: 296 This task involves responding to a request for information by locating information in a spreadsheet and emailing the requested information to the person who asked for it. The test-taker is presented with a word-processor page containing a request to identify members of a bike club who meet two conditions, and a spreadsheet containing 200 entries in which the relevant information can be found. The required information has to be extracted by using a sort function. The item requires the test-taker to organize large amounts of information in a multiple-column spreadsheet using multiple explicit criteria and locate and mark relevant entries. The task requires switching between two different applications and involves multiple steps and operators. It also requires some amount of monitoring. Making use of the available tools greatly facilitates identifying the relevant entries. Level 1: Party Invitations (Item ID: U01A) Difficulty score: 286 This task involves sorting emails into pre-existing folders. An email interface is presented with five emails in an inbox. These emails are responses to a party invitation. The test-taker is asked to place the response emails into a pre-existing folder to keep track of who can and cannot attend a party. The item requires the test-taker to categorize a small number of messages in an email application in existing folders according to a single criterion. The task is performed in a single and familiar environment and the goal is explicitly stated in operational terms. Solving the problem requires a relatively small number of steps and the use of a restricted range of operators and does not demand a significant amount of monitoring across a large number of actions. tion issues with the background questionnaire and assessment software package, interviews that could not be completed due to software issues were considered \"completes\" for the purposes of response rate calculations. The overall weighted response rate for the household sample was 70.3 percent. For respondents who did not complete any tasks on any of the literacy scales, no information is available about their performance on the literacy items they were missing. Completely omitting these individuals from the analyses would have resulted in unknown biases in estimates of the literacy skills of the national population because refusals cannot be assumed to have occurred randomly. For respondents who answered the background questionnaire but refused to complete the assessment for reasons other than language issues or a mental disability, proficiency values were imputed based on the covariance information from those who completed the survey. The final household reporting sample-including the imputed cases-consisted of 5,010 respondents. These 5,010 respondents are the 4,898 respondents who completed the background questionnaire, plus the 112 respondents who were unable to complete the background questionnaire for literacyrelated reasons. The sample was subject to unit nonresponse from the screener, background questionnaire, assessment (including reading components), and item nonresponse to background questionnaire items. Although the screener had a unit response rate above 85 percent, the background questionnaire had a unit response rate below 85 percent and thus required an analysis of the potential for nonresponse bias according to the National Center for Education Statistics (NCES) statistical standards."}, {"section_title": "Nonresponse Bias", "text": "The nonresponse bias analysis of the household sample revealed differences in the characteristics of respondents who participated in the background questionnaire compared with those who refused. In a bivariate unit-level analysis at the background questionnaire stage, estimated percentages for respondents were compared with those for the total eligible sample to identify any potential bias owing to nonresponse. Multivariate analyses were conducted to further explore the potential for nonresponse bias by identifying the domains with the most differential response rates. These analyses revealed that the subgroup with the lowest response rates for the background questionnaire had the following characteristics: (1) Hispanic, (2) age 26 and older with no children in the household, and (3) reside outside the Northeastern United States in areas with low levels of linguistic isolation (a low percentage who have some difficulty speaking English) and with unemployment rates exceeding approximately 5 percent. In general, younger persons were found to be more available to participate, as were those with children ages 16 and younger, and women. However, the variables found to be significant in the bivariate analysis-those used to define areas with low response rates-were used in weighting adjustments. The analysis showed that weighting adjustments were highly effective in reducing the bias. The general conclusion was that the potential amount of nonresponse bias attributable to unit nonresponse at the background questionnaire stage was likely to be negligible. \u2022 The assessment was administered in CBA and PBA modes."}, {"section_title": "\u00a2 \u00a2", "text": "The CBA consisted of three \"testlets\" of tasks at Stage 1 (9 items) and four \"testlets\" at Stage 2 (11 items). Each respondent completed two testlets that included items from two of the three domains. The PBA consisted of two paper-based assessment booklets, one contained literacy items and one contained numeracy items. Each booklet contained 20 items for the participant to complete and each participant completed only one booklet type. \u2022 The reading components were completed by a participant after completing the literacy or numeracy booklet. Reading components were also completed by a respondent who failed the CBA Core Stage 2 or the PBA Core. Problem Solving in Technology-Rich Environments: U.S. Sample The PIAAC assessment design was developed to route respondents to the most appropriate delivery mode as a means to help assure the most reliable, valid, and comparable assessment of skills. The computer-based assessment (CBA) was chosen for those demonstrating information and communication technology (ICT) skills, while the remaining respondents received the paper-based assessment (PBA). The scores for respondents that had no computer experience, failed the ICT skills test, or refused the CBA did not contribute to the estimation of the item parameters for the problem solving in technologyrich environments domain. The design of the PIAAC assessment contained only literacy and numeracy in the PBA because the problem solving in technology-rich environments assessment, by definition, was suitable only for respondents familiar with ICT environments. Exhibit C-1 illustrates the stages of the assessment administration and the weighted percentages of U.S. respondents at each stage of the assessment. Weighting and Variance Estimation A complex sample design was used to select assessment respondents. The properties of a sample selected through a complex design could be very different from those of a simple random sample in which every individual in the target population has an equal chance of selection and in which the observations from different sampled individuals can be considered statistically independent of one another. Therefore, the properties of the sample for the complex data collection design were taken into account during the analysis of the data. One way of addressing the properties of the sample design was by using sampling weights to account for the fact that the probabilities of selection were not identical for all respondents. The sampling weights were further adjusted for nonresponse to the screener and background questionnaire, extreme weights were trimmed, and weights for all respondents calibrated to the U.S. Census Bureau's 2010 American Community Survey population totals for those age 16 to 65. Since literacy-related nonrespondents to the screener, the background questionnaire, and the assessment are similar in proficiency, the weights of the literacy-related nonresponse cases were not adjusted during the screener-level nonresponse adjustment. Instead, the background questionnaire weights for the background questionnaire and assessment literacy-related cases were adjusted to account for the literacy-related screener nonrespondents. This adjustment was necessary primarily to allow the literacy-related background questionnaire and assessment nonrespondents to represent the literacy-related screener nonrespondents in the calibration procedure. All population and subpopulation characteristics based on the PIAAC data used sampling weights in their estimation. The statistics presented in this report are estimates of group and subgroup performance based on a sample of respondents, rather than the values that could be calculated if every person in the nation answered every question on the instrument. Therefore, it is important to have measures of the degree of uncertainty of the estimates. Accordingly, in addition to providing estimates of percentages of respondents and their average scale scores, this report provides information about the uncertainty of each statistic in the form of standard errors on the U.S. PIAAC website at http://nces.ed.gov/surveys/piaac/results/ summary.aspx. Because the assessment used clustered sampling, conventional formulas for estimating sampling variability (e.g., standard errors) that assume simple random sampling and hence independence of observations would have been inappropriate for this report. For this reason, the PIAAC assessment used a paired jackknife replication approach (sometimes referred to as JK2) to estimate standard errors (Rust and Rao, 1996)."}, {"section_title": "Overview of the Skill Use Scales in PIAAC", "text": "In PIAAC, the skills of a population were measured not only directly through the cognitive instruments, but also indirectly through the background questionnaire that asked respondents to report on their use of skills both in and outside of work. The frequency and type of activities associated with reading, writing, numeracy, and information technology were targeted in the background questionnaire using multiple items that were similarly worded to apply to activities both in and outside of work. In addition, other areas, particularly those involving intrapersonal, interpersonal, and other generic \"soft\" skills not included in the direct assessment, were addressed through a set of selfreported questions. Examples of some of these questions include how individuals deal with new information and the extent to which individuals like to learn new things. This set of questions made up a module within the questionnaire that was specifically developed for the PIAAC project: the Job Requirements Approach (JRA) Module (Green 2009). When the PIAAC Consortium developed and implemented C-6 PIAAC, they were able to construct 13 scales based on a cross-country analysis of comparability, reliability, and convergent as well as discriminant validity. The 13 scales are: These scales were constructed using item response theory (IRT), more specifically the generalized partial credit model (GPCM), and person-specific levels of skill use were estimated using weighted likelihood estimation (WLE; Warm 1989, Muraki 1992, Penfield & Bergeron 2005. Scale values were derived for all respondents who reported at least some limited activities in each of these domains. Those who reported no skill use in each of the 13 areas are not represented on any of these 13 scales. Nevertheless, they provide important information with respect to the percentage of people in each participating country who do not use particular types of skills either in or outside of work."}, {"section_title": "Skill Use Derived Variables", "text": "The skill use variables reported on page 18 consist of respondents' answers to multiple questions in the background questionnaire. Reading skills used at work include respondents' answers to questions regarding frequency of engagement in the following activities: read directions or instructions; read letters, memos or emails; read articles in newspapers, magazines or newsletters; read articles in professional journals or scholarly publications; read manuals or reference materials; read bills, invoices, bank statements or other financial statements; read diagrams, maps or schematics. Numeracy skills used at work include respondents' answers to questions regarding frequency of engagement in the following activities: calculate prices, costs or budgets; use or calculate fractions, decimals or percentages; use a calculator; prepare charts, graphs or tables; use simple algebra or formulas; use more advanced math or statistics such as calculus, complex algebra, trigonometry or use of regression techniques. ICT skills 3 used at work includes respondents' answers about the frequency with which they use email; use the Internet to understand issues related to work; use of a word processor; use of programming language to program or write computer code; participate in real-time discussions on the Internet. The respondents' answers to questions regarding the frequency use of these skills range from \"never,\" \"less than once a month,\" \"less than once a week but at least once a month,\" \"at least once a week but not every day,\" to \"Every day.\" Respondents who are in the higher quintiles used these skills more often than the respondents who are in the lower quintiles."}]