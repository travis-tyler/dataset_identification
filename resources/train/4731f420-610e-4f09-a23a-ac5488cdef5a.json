[{"section_title": "", "text": "Phase III of the Agricultural Resource Management Survey (ARMS) is the longest and most detailed sample survey data collection that NASS conducts. On this survey NASS collects highly detailed economic data from agricultural producers nationwide. The ARMS Phase III Core version is a self-administered data collection instrument that uses a mail-out/mail back methodology. For 2004, an incentive experiment was conducted using this component of the ARMS. The use of incentives, particularly monetary ones, in surveys has proved to be effective at increasing response rates. Five treatment groups, including a control group, were used for the incentive experiment. Prepaid and promised indirect cash incentives -in the form of $20 automated teller machine (ATM) cards -and priority mail were used as stimuli. The prepaid incentives were the most effective at increasing response rates and decreasing data collection costs. When delivered by priority mail, the ATM card incentive increased the mail response rate by 13.8 percentage points over the control group. When delivered by first class mail, a 10.7 percentage point increase in the mail response rate was achieved. The promised incentives delivered by first class mail elicited a 7.1 percentage point increase over the control group. All three of these response rate increases over the control group were statistically significant at the \u03b1=0.05 level. At the end of the data collection period, which included face-to-face follow-up for all mail nonrespondents, all treatment groups provided with an incentive had higher overall response rates than those groups that were not provided an incentive. Specifically, the prepaid incentive delivered in first class mail achieved a 7.0 percentage point increase in the overall response rate; the prepaid incentive delivered in priority mail achieved a 9.0 percentage point increase in the overall response rate; and the promised incentive delivered in first class mail achieved a 5.0 percentage point increase in the overall response rate. The mail response rate increases led to fewer costly face-to-face follow-up contacts being required for all mail non-respondents per ARMS data collection protocol. In addition, only about a third of all ATM card recipients cashed their ATM cards by the expiration date. The low rate of cashing and the need for fewer face-to-face contacts for the prepaid incentive groups led to a lower cost for these groups than the other treatment groups. The combination of higher response rates and lower costs for the prepaid incentive groups provides impetus for the use of incentives for future ARMS Phase III data collections, particularly mail data collections."}, {"section_title": "RECOMMENDATIONS", "text": "1. Indirect monetary incentives should be used on a large scale for the ARMS Phase III Core mail-out/mail-back sample. In particular, prepaid ATM cards sent by first class or priority mail should be implemented on a regular basis for the Core sample. 2. Priority mail should be considered in the mail-out/mail-back methodology that is implemented to deliver the questionnaire and other survey material to sampled operations. When used in combination with a prepaid ATM incentive, it was an effective component of a mail-out/mail-back data collection strategy. However, when used alone, priority mail was not effective at increasing mail response rates. 3. Any future incentives use should be included as one piece of the larger mail data collection strategy. The mail-out/mail-back data collection methodology used for this research was a comprehensive plan that incorporated many important components to ensure high response, including an advance letter, and two mail-out packets with a reminder postcard in-between. The number and quality of contacts is crucial to obtaining reasonable mail response rates for mail-out/mail-back surveys. 4. Any incentives considered for other components of ARMS should be part of a research plan designed to test incentive effectiveness. This experiment did not examine the use of incentives with the face-to-face data collection methodology or for the other versions of the ARMS questionnaires. 5. Prior to implementing the use of indirect monetary incentives into operational programs, NASS should first conduct research to determine whether there are any negative wide ranging effects of their use, especially in terms of response rates to programs not using incentives. This may include the creation and maintenance of new data items stored in the NASS data warehouse. ARMS Phase III has been problematic because, compared with other NASS surveys, its response rates are low, and its data collection costs are high. Prior to 2003, all ARMS Phase III data were collected by faceto-face enumeration because of the length and complexity of the questionnaire. A self-administered Phase III \"Core\" questionnaire developed for the 2003 survey was used for part of the sample for mailout/mail-back data collection; this sixteen page questionnaire collected the most critical ARMS Phase III information with half the number of pages as in the full questionnaire. Face-to-face nonresponse follow-up interviews were conducted on all mail nonrespondents. In addition, part of the 2003 Phase III sample was field enumerated using the Core form. These cases had not previously received the questionnaire in the mail. Analysis by ERS and NASS determined that the data collected through the mail were comparable to those collected by face-to-face methodology Since the utility of a self-administered ARMS Phase III Core form was demonstrated in 2003, NASS has sought ways to implement its use and increase the overall response rate as well as contain data collection costs. Offering potential respondents incentives is a proven technique to increase response rates on a variety of surveys conducted by several agencies and companies (Church, 1993;James and Bolstein, 1992;James and Bolstein, 1990;Singer, 2002). An incentive experiment was implemented with the 2004 ARMS Phase III Core to test whether monetary incentives would be effective at increasing response rates for this survey while being cost effective.\nThe following recommendations are offered from this research project. 1. Indirect monetary incentives should be used on a large scale for the ARMS Phase III Core mail-out/mail-back sample. In particular, prepaid ATM cards sent by first class or priority mail should be implemented on a regular basis for the Core sample. 2. Priority mail should be considered in the mail-out/mail-back methodology that is implemented to deliver the questionnaire and other survey material to sampled operations. When used in combination with a prepaid ATM incentive, it was an effective component of a mail-out/mailback data collection strategy. However, when used alone, priority mail was not effective at increasing mail response rates. 3. Any future incentives use should be included as one piece of the larger mail data collection strategy. The mailout/mail-back data collection methodology used for this research was a comprehensive plan that incorporated many important components to ensure high response, including an advance letter, and two mail-out packets with a reminder postcard in-between. The number and quality of contacts is crucial to obtaining reasonable mail response rates for mailout/mail-back surveys."}, {"section_title": "METHODS", "text": "The 2004 ARMS Phase III Core sample size was 15,900 farm/ranch operations across 15 states. The 15 states were those with the highest agricultural value of sales and included Arkansas, California, Florida, Georgia, Illinois, Indiana, Iowa, Kansas, Minnesota, Missouri, Nebraska, North Carolina, Texas, Washington, and Wisconsin. The total sample was stratified by state, ARMS farm value of sales (as maintained on the sampling frame), and type of operation (also as maintained on the sampling frame); then, five sub-samples, each of size 2,000, were systematically selected. The subsamples were drawn such that each was equally represented by the strata. Once the sub-samples were drawn, NASS Field Offices had the opportunity to remove operations from the Core sample that had previous data reporting arrangements with the office. This resulted in each sub-sample being slightly less than 2,000. The standard mail-out/mail-back data collection methodology for the 2004 ARMS Phase III Core sample consisted of: (1) a cover letter and questionnaire mailed on December 28, 2004, (2) a post-card reminder and \"thank you\" sent to the entire sample on January 13, 2005, (3) a cover letter and second questionnaire mailed to all non-respondents on January 31, 2005, and, finally, (4) starting February 21, 2005, face-to-face interviews attempted for all remaining mail nonrespondents. In addition to the standard data collection methodology, prepaid and promised indirect cash incentives -in the form of $20 automated teller machine (ATM) cards -and priority mail were used as stimuli. Combinations of these stimuli were administered to four of the five sub-samples mentioned above; a fifth sub-sample received no stimuli and served as the control for this project. Collectively, these five sub-samples formed the five treatment groups used for this project. Table 1 contains descriptions of the treatment groups. All treatment groups received cover letters that included some uses of the ARMS data. Five such uses were included in the cover letters used for treatment groups not receiving incentives, and three uses were included in the cover letters used for treatment groups that received incentives. These unintentional differences may have confounded our results, as they may have contributed positively or negatively to the response rates. However, we expect the confounding effect to be minimal since all letters were written to positively impact response. The sub-samples in the pre-paid incentive treatment groups all received cover letters that: (1) explained the incentive was a \"thank you\", (2) described the uniqueness of the ARMS, and (3) justified the use of the incentive by its overall cost savings to the government. The actual ATM card incentive was delivered to recipients in the same packet as the first questionnaire and was affixed to a standard 8\u00bd inch x 11 inch sheet of paper that reiterated a \"thank you\" and included instructions on how to use the card. Appendix A contains the treatment group survey materials and includes copies of the cover letters used for each treatment group, a copy of the ATM card instruction sheet, and a photo of the actual ATM card that was used. The $20 ATM cards were supplied by JPMorgan Chase bank and were usable in nationwide ATM machines that displayed the NYCE \u00ae , Pulse \u00ae , Maestro \u00ae , or Cirrus \u00ae logos. The cards were also usable at point-of-sale (POS) (i.e., retail) establishments that allow the use of debit cards as payment; however, this fact was not revealed to card recipients. In addition to the $20 incentive, the ATM cards were loaded with an extra $4 to cover any transaction charges. The cards were preactivated and were immediately usable when the recipients received them. The personal identification number (PIN) needed to use the card was embossed on the front of each card after the words \"THANK YOU\". The front of each card also included the embossed message, \"FOR HELP 1-888-424-7828\"; this toll-free telephone number was answered by NASS staff. Finally, all ATM cards expired on June 30, 2005 (there was no provision for extending this date). See Appendix A for a photo of the actual ATM card. If a card recipient lost or could not use the card, a replacement could be requested by calling the toll free phone number listed on the instruction sheet. For the few cases where this occurred, only the replacement card was used in our analysis. The decision to use $20 ATM cards as incentives was essentially made by default. Actual cash was preferred by the authors; however, NASS and USDA senior management were concerned with accountability when using cash. Checks were also considered, but the U.S. Treasury Department (the would-be issuer of the checks) was concerned with logistical issues related to check usage. There were no such concerns from NASS, USDA, or the Treasury Department for the ATM cards. The decision to offer $20 as the incentive amount was also largely a result of that value being the only viable option because many, if not most, ATMs only dispense cash in $20 increments. Priority mail was also used as a stimulus because evidence from survey literature has shown the use of priority mail increases overall response rates, especially when used in combination with monetary incentives (Moore and An, 2001). Group 5 includes all respondents in the treatment group who received an ATM card (whether their questionnaires were received in the mail or by a face-to-face interview) and Treatment Group 5' includes only those respondents in the treatment group who received an ATM card because they returned their questionnaires in the mail."}, {"section_title": "RESULTS", "text": ""}, {"section_title": "Response Rates", "text": "Similar to previous incentive research conducted on various populations, the results of this project showed that incentives for the farm and ranch operator population increased response rates. Table 2 shows the response rates by treatment group (treatment group descriptions are given in Table 1). Response rates are also broken out by the different mailing and contacts employed as part of the data collection methodology. As shown in Table 2, all treatment groups that received an incentive had higher response rates than the control group (Treatment Group 1). The prepaid $20 ATM incentive sent by priority mail (Treatment Group 4) had the highest mail and overall response rates, at 43.9 percent and 72.4 percent, respectively. The second highest mail and overall response rates were achieved with the prepaid incentive sent by first class mail (Treatment Group 2) with a mail response rate of 40.8 percent and an overall response rate of 70.4 percent. The promised $20 ATM incentive response rates, at 37.2 percent for mail and 68.4 percent overall, surpassed the treatment groups that received no incentives, but lagged those that provided prepaid incentives. Priority mail alone was ineffective at increasing response rates. The mail and overall response rates were compared for statistical significance using ttests across all treatment group combinations. All tests were done at an overall \u03b1=0.05 level of significance (with a Bonferroni adjustment for the ten comparisons). The details of the statistical significance testing are described in Appendix B. All three treatment groups that received ATM card incentives had significantly higher mail and overall response rates than the control group, with the two prepaid treatment groups outperforming the promised incentive. The specific differences are shown below and detailed in Appendix B. In terms of mail response, all three treatment groups that received ATM card incentives had significantly higher response than the control group. Specifically: In terms of overall response, all three treatment groups that received ATM card incentives had significantly higher response than the control group. Specifically: \u2022 Treatment Group 2 had a significantly greater overall response rate than Treatment Groups 1 and 3. \u2022 Treatment Group 4 had a significantly greater overall response rate than Treatment Groups 1 and 3. \u2022 Treatment Group 5 had a significantly greater overall response rate than Treatment Group 1. Recall cover letter differences (regarding uses of ARMS data) between the treatment groups may have confounded these results.  ( , , , , ) where IC0910 = 1 for mail completes, IC0910 = 3 for non-mail completes, IC0910 = 5 for out of scope records, IC0910 = 8 for refusal records, and IC0910 = 9 for inaccessible records. 2/ Initially, all treatment groups contained 2,000 records, but field offices removed operations with whom they had previous data collection agreements. 3/ Overall Mail Returns includes all returns from the 1 st and 2 nd mailings and postcard in addition to all mail returns received after face-to-face follow-up started. "}, {"section_title": "Response Over Time", "text": "Rapidity of response is an important element in the data collection process. If an early mail response is received, further follow-up contacts are unnecessary. Response rates over time can be seen for each treatment group in Figures 1 and 2. Figure 1 includes only mail response, while Figure 2 tracks overall response for each treatment group. As shown in both graphs, the treatment groups provided with incentives (i.e., Treatment Groups 2, 4, and 5) out-performed the control group (Treatment Group 1) with respect to both mail and overall response rates throughout the data collection process. Obtaining these responses earlier makes the entire data collection process more efficient and cheaper by allowing processing to start earlier and by reducing the number of costly face-to-face follow-up contacts."}, {"section_title": "Response By State", "text": "Although this experiment was not designed to measure state-level effects on response rates, it is interesting to notice the different trends across states. In most states, those operators in the treatment groups that received ATM card incentives tended to have the highest response rates. However, there were some interesting exceptions such as in Texas where the treatment group with the highest rate of response was the Control Group! Figure 3 illustrates state-level response rates by treatment group. Appendix C contains more detailed information on state-level response rates. "}, {"section_title": "Response by Farm Type and Size", "text": "Also of interest is the incentive effect on different farm types and sizes; however, the sample was not selected in such a way as to allow statistical testing by these attributes. Some groups have very few records, making the data for those groups unreliable. These data are shown for information only and cannot be used to make any general inferences about the response patterns for any specific group or the impact of incentive use on its response pattern. Figure 4 shows the breakdown of response rates by farm type and Figure 5 shows the value of sales categories. See Appendices D and E for more details on the response rates by farm type and size."}, {"section_title": "A Word on Nonresponse Bias", "text": "This study did not address the potential effect on nonresponse bias of offering ATM card incentives for the ARMS Phase III Core. Nonresponse bias arises when survey nonrespondents are systematically different from respondents with respect to the data collected on the survey. Because ATM card incentives significantly increased response rates, it is possible that they also changed the operator/operation characteristics composition of remaining nonrespondents. If the make-up of the nonrespondent pool changed, incentive use may have either reduced or exacerbated nonresponse bias, dependent upon whether incentive use made nonrespondents less alike or more alike. Although Figures 3, 4, and 5 provide some insight into how the incentives affected response from sub-populations of the ARMS Phase III Core sample, the sample sizes for this study's treatment groups were insufficient to properly assess nonresponse bias. "}, {"section_title": "ATM Card Use", "text": "Prior to survey year 2004, indirect monetary incentives were not used at NASS. Therefore, it was unclear what to expect when mailing almost 6,000 ATM cards to potential survey respondents. Other government agencies have used ATM cards on a large scale and found that a sizable percentage of card recipients do not cash their cards (Kay, et al., 2001)."}, {"section_title": "ATM Card Use: How many recipients cash them?", "text": "Using a $20 ATM card incentive to attempt to boost response could be costly if everyone were to cash his/her card. The cover letter emphasized that the ATM card was provided with the purpose of passing on some of the savings of mail data collection over a face-toface interview. As it turned out for the ARMS Core, not nearly all recipients of ATM cards used them. Table 4 shows the percentages of recipients who used their ATM cards for monetary withdrawal by treatment group. Just over a third (38.6 percent) of all card recipients cashed their ATM cards. This rate differed greatly by incentive treatment group and response. The most striking difference is that between respondents and nonrespondents. Respondents cashed their cards at an average rate of 47.6 percent, while nonrespondents cashed them less than five percent of the time. It could be that non-respondents never saw the card because they threw the packet in the trash without opening it. Also, non-respondents may have felt that they did not deserve the money because they did not complete the questionnaire. Also noteworthy is that mail respondents cashed their cards at a higher rate than face-toface interviewed respondents (this can be concluded from Table 4). The cover letter emphasized that the ATM card was provided with the purpose of passing on some of the 1/ Numbers include distinct operators who withdrew money using ATM cards. 2/ Includes mail and face-to-face respondents. 3/ Includes respondents and non-respondents. 4/ Operators in Treatment Group 5 should not have received an ATM card unless they responded to the survey. However, some returned partially completed forms that were determined to be inadequately completed after the card had been mailed out. 5/ Treatment Group 5 includes all respondents in the treatment group who received an ATM card (whether their questionnaires were received in the mail or by a face-to-face interview) and Treatment Group 5' includes only those respondents in the treatment group who received an ATM card because they returned their questionnaires in the mail. savings of mail data collection, instead of requiring a face-to-face interview. Assuming they read (and remembered) the cover letter, those respondents who completed the survey with a face-to-face interview may have felt that they were not entitled to the money or that the card was not valid unless the questionnaire was mailed back. Respondents in the two prepaid incentive treatment groups (Treatment Groups 2 and 4) cashed their cards at a rate of about 41 percent while respondents in the promised treatment group (Treatment Group 5) cashed their cards at a rate of over 60 percent. This could be because the respondents who were promised the incentive felt as if they deserved the card because they fulfilled their side of an economic agreement. By making receipt of the card contingent on filling out the questionnaire, NASS may have made these respondents more likely to feel that they had earned the money. The prepaid respondents, on the other hand, may have seen the ATM card as a gesture of goodwill, and not felt as if they necessarily deserved it. The low card cashing rate directly impacts the cost of the incentive experiment, which will be discussed later in this report."}, {"section_title": "ATM Card Use: When did farm operators cash them?", "text": "Also of interest is the relationship between the date the card was used and the date the questionnaire was returned. Table 5 depicts ATM card usage with respect to the date the questionnaire was received at the National Processing Center (NPC). Figure 6 shows a frequency distribution of the time difference between when each respondent returned his questionnaire and when he used his ATM card. The date each respondent returned his questionnaire is referred to as the \"NPC Date\" in Figure 6. Table 5 and Figure 6 show that about one-half of the people in the prepaid treatment groups (Treatment Groups 2 and 4) cashed their cards within a period of two weeks before or after they returned their questionnaires. These respondents perhaps linked cashing the card to the task of filling out the questionnaire. Another one-third of respondents cashed their ATM cards several weeks after completing the questionnaire. These people may have cashed them at this point because the expiration date (which was June 30, 2005) was approaching.  "}, {"section_title": "ATM Cards: How much do they cost?", "text": "An important issue with offering ATM card incentives is the various costs associated with using them. Table 6 breaks down overall ATM card-related costs for the treatment groups. Card use for all operators in each treatment group (respondents or nonrespondents) is included here. Overall costs, including printing, mailing, and interviewing costs for the incentive experiment are presented later in Section 3.3. There are several transaction fees associated with ATM card use besides the actual withdrawal amount. This is why each card was loaded with $24, instead of just the $20 incentive amount. These fees include withdrawal and purchase fees, balance inquiries fees, and failure fees. In addition, since the ATM cards could be used at pointof-sale (POS) debit card machines, users could withdraw money at a variety of locations (this fact was not explicitly stated to card recipients; hence the low numbers of such fees). These fees are presented separately in Table 6. 1/ Numbers include distinct operators who withdrew (or tried to withdraw) money using ATM/Debit cards. 2/ Includes amount of withdrawal/purchase as well as any transaction fees imposed by the ATM owner/retailer. 3/ JPMorgan Chase transaction fee. JPMorgan Chase was the issuer of the ATM cards. 4/ Treatment Group 5 includes all respondents in the treatment group who received an ATM card (whether their questionnaires were received in the mail or by a faceto-face interview) and Treatment Group 5' includes only those respondents in the treatment group who received an ATM card because they returned their questionnaires in the mail."}, {"section_title": "ATM Card Use: Who used them?", "text": "The following tables summarize ATM card use among ARMS Phase III Core respondents by various demographics and other descriptive variables. Tables 7, 13, and 15 use sampling frame control data, while the other tables use reported ARMS data. One particularly interesting thing in Tables 7-15 is that operators who tended to leave questions unanswered also tended to leave their ATM cards unused.   -1950 1950-1959 1960-1969 1970-1979 1980-1989 1990-1999 "}, {"section_title": "Overall Costs", "text": "Cost is a major issue to be considered if NASS is to continue using incentives for the ARMS III Core. Data collection costs for mail and face-to-face interviewing need to be combined with incentive costs to create an overall picture of the dollar value of using incentives. As shown in Table 16, overall costs and the average costs per sample and completed record are lowest for the two prepaid incentive groups (Treatment Groups 2 and 4). This is due to two important factors in the data collection methodology and incentive use. First, only a little over one-third of card recipients cashed their ATM card. This saved NASS several thousand dollars in ATM card withdrawals and fees. Second, face-to-face interviews are much more costly than selfadministered (mail) questionnaires with an incentive. Because the prepaid incentive treatment groups achieved such large increases in mail response over the other treatment groups, overall data collection costs for those two groups were lower. Table 17 contains cost data for a \"mail-only\" data collection scenario. Note that since these data came from a methodology that uses faceto-face follow-up, the costs may not truly represent a mail-only scenario. Table 17 also shows the cost of each additional mail return obtained from each treatment group over the control group (Treatment Group 1). Relative to the cost of each mail-returned questionnaire in the control group, the cost per additional mail return for each treatment group is quite high. Indeed, assuming all options are possible, it may be more economical to use a different data collection methodology instead of \"mail-only with incentive\". This suggests that the use of indirect monetary incentives, as implemented in this project, may not be economical for a hypothetical mail-only data collection methodology. However, for ARMS Phase III, since even the highest mail response rate obtained from any of the treatment groups was only 43.9 percent, it is likely that a mail-only approach will never be acceptable to NASS, ERS, or the Office of Management and Budget (OMB). 1/ Treatment Group 5 includes all respondents in the treatment group who received an ATM card (whether their questionnaires were received in the mail or by a faceto-face interview) and Treatment Group 5' includes only those respondents in the treatment group who received an ATM card because they retuned their questionnaires in the mail. "}, {"section_title": "Data Quality", "text": "Incentives for the ARMS Phase III Core increased response and lowered cost. Another aspect of the incentive experiment was to determine if the use of incentives affected the quality of data provided by respondents. It is difficult to assess data quality in a selfadministered survey. If someone reports that he/she has 200 acres of corn, we have no way to probe for more information or verify the accuracy of this answer. If our control data or other survey data in the questionnaire indicate something looks suspicious in the selfreported response, then we may impute a \"more accurate\" value. However, the only way to really know the \"true\" value of, say, corn acreage is to go out to the farm and make the measurement ourselves. Obviously such a quality check is not practical for NASS surveys. This makes it very difficult to make the claim that farmer A filled out a survey form more accurately than farmer B. There is no easy way to definitively determine whether one group of farmers tended to fill out their ARMS Phase III Core forms more accurately than another group of farmers. However, there are some ways of looking at our data that could possibly shed light on the issue of data quality. Three data quality tests were performed to evaluate the data obtained from respondents in the different treatment groups. The tests included: (1) comparing data quantity (i.e., the number of questions answered), (2) comparing keyed data to edited data, and (3) comparing reported data with NASS control data. For tests (1) and (3), only keyed data were used. Keyed data are those as they were recorded by the respondent and then keyed at NPC. Minimal editing of any sort was done to the data before keying. For test (2), both keyed and edited data were used."}, {"section_title": "Data Quantity", "text": "Because the ARMS III Core questionnaire is sixteen pages long, fatigue may result in a respondent skipping questions, particularly at the end of the questionnaire. With this in mind, one way to assess data quality is to take a closer look at data quantity, that is, tallying the number of questions answered by each respondent in each of the 11 sections of the ARMS Phase III Core questionnaire. While related, our notion of data quantity is not the same as item nonresponse since not all questions apply to all respondents; hence, the specific sets of questions respondents are expected to answer differ greatly. Table 18 compares average data quantity among the treatment groups. As shown in Table 18, those respondents provided with prepaid incentives (Treatment Groups 2 and 4) answered slightly more questions than those in the other treatment groups. It is possible that respondents who received the prepaid incentive felt that the questionnaire was more important than those who did not receive the incentive. Consequently, they may have been more diligent in completing the questionnaire, which resulted in them providing answers to more questions. However, even those differences that are statistically significant (\u03b1=0.05) are small and probably not practical when looking at the questionnaire as a whole. that this average is significantly greater than at least one other treatment group's average in this section of the survey. The treatment groups with statistically significant differences (based on a two-side t-test at the \u03b1=0.05 level, computed with a Bonferroni adjustment to correct the error term for multiple pairwise comparisons): Section F: the average number of questions answered was significantly different between treatment groups (4 and 3) and also between (4 and 5); Section G: (4 and 5); Section I: (4 and 1), (4 and 3), and (2 and 3); Section J: (4 and 5); Total: (4 and 3), and (4 and 5)"}, {"section_title": "Keyed Data versus Edited Data", "text": "Data quality may also be assessed by looking at the amount of post-data collection edits that were required to correct inconsistent or apparent misreported data. Specifically, we wish to determine whether the amount of editing required for data provided by incentive recipients was different from non-recipients. More editing may be an indication of satisficing (i.e., the respondent provides a superficial response that appears reasonable or acceptable, without going through all the cognitive steps involved in the questionanswering process). In addition to the quality issues raised by data requiring significant editing, data editing itself is labor intensive. Therefore, if data quality were improved, Field Offices would benefit significantly in terms of staff time. Table 19 shows the mean number of answers that were changed in each section from the keyed data to the final edited data. Higher numbers indicate that there were more answers edited for that treatment group. These numbers do not account for multiple changes to a single question. Although the number of changes is small across all sections, the data provided by respondents in the prepaid treatment groups (Treatment Groups 2 and 4) required the most editing overall, particularly in Section K (Farm Operator and Household Characteristics). It is unclear why this would be the case unless more editing resulted from the slightly larger number of responses provided in this section by those in the prepaid treatment groups. The difference in the number of edits required is small (one question at the most on average), but this is the one element of analysis that may be negative for the use of incentives. All other measures showed that incentives improved or did not impact any component of the survey being analyzed. that this average is significantly greater than at least one other treatment group's average in this section of the survey. The treatment groups with statistically significant differences (based on a two-side t-test at the \u03b1=0.05 level, computed with a Bonferroni adjustment to correct the error term for multiple pairwise comparisons): Section G: the mean number of answers that were changed from the keyed data to final edited data between treatment groups (4 and 5); Section K: (1 and 2), (1 and 4), (2 and 3), and (3 and 4); All Changes: (1 and 2) and (2 and 3)"}, {"section_title": "Reported Data versus NASS Control Data", "text": "Another way to assess data quality in the ARMS Phase III Core is to look at the difference between what operators report and what we expect them to report (i.e., sampling frame control data). Looking at this difference provides us with some insight into self-reporting consistency (of course, it also may tell us something about control data accuracy). However, ultimately it is really just a comparison between two possible indications of truth. We would expect that, on average, the respondents from each treatment group should have similar discrepancies between what they report and their control data. Significant differences in these discrepancies among the different treatment groups could indicate that certain treatment groups may be supplying NASS with \"better\" or \"worse\" data. For the purpose of making this comparison between self-reported data (as keyed at NPC) and control data, we focus upon one variabletotal land operated (control data code 100 and item code 26 on the ARMS Phase III Core questionnaire). Due to the way variables for ARMS are calculated and the small number of items collected on the survey that have identical sampling frame control variables, this variable was the only one that could be tested. Because the distribution of total land operated contains many outliers and near-zero values, a nonparametric rank F-test was used. Appendix G contains more details of the test procedure. Applying this test produced an F-test statistic of 0.96 with a p-value of 0.4259; therefore, at the \u03b1=0.05 significance level, there was not sufficient evidence to reject the claim that the mean ranking is the same for each treatment group. That is, no significant difference was evident across treatment groups in how respondents reported their total land operated, relative to their control data. Table 20 contains the data associated with the test. "}, {"section_title": "Data Quality Summary", "text": "The greatest benefit NASS derives from using incentives is the economical increase in response rates for the ARMS Phase III Core. One natural concern that goes along with this is that while these incentives may buy NASS better response, they will not necessarily buy NASS better data. It is easy to argue that an incentive will get more data for a particular survey (in the form of more responses), but what if those data are also more likely to be inaccurate? Taking note of the limitations in our analysis, the data quality results show that incentives likely did not bring us \"worse\" data quality. In fact, operators in Treatment Group 4 ($20 prepaid ATM card incentive delivered in priority mail), on average answered significantly more questions and there was no significant difference in \"accuracy\" as measured by comparing the sampling frame value for total land to the reported value. However, we did see a slight increase in the amount of editing required for the prepaid incentive groups over the other treatment groups. In general, incentives helped provide NASS with more data, as well as possibly better data (or at least no worse)."}, {"section_title": "CONCLUSION", "text": "Indirect monetary incentives in the form of $20 ATM cards proved to be effective at increasing survey response rates for the ARMS Phase III Core. Mail response rates for the prepaid incentive groups were significantly higher than the control group, saving a large amount of money on costly face-to-face follow-up interviews. Overall response rates were higher as well, providing NASS and ERS with more completed records for analysis. The data provided by respondents in the incentive groups were comparable to those collected in the control group. There was slightly less item nonresponse for the incentive groups, but slightly more editing required. There were no differences in the control data for total land compared to the survey data collected for total land. Finally, this project was unable to investigate whether or not the use of indirect monetary incentives affected nonresponse bias."}, {"section_title": "Any incentives considered for other", "text": "components of ARMS should be part of a research plan designed to test incentive effectiveness. This experiment did not examine the use of incentives with the face-to-face data collection methodology or for the other versions of the ARMS questionnaires. 5. Prior to implementing the use of indirect monetary incentives into operational programs, NASS should first conduct research to determine whether there are any negative wide ranging effects of their use, especially in terms of response rates to programs not using incentives. This may include the creation and maintenance of new data items stored in the NASS data warehouse.  Multiple comparisons of the binomial parameters of interest (i.e., the proportion, ! i , of survey response in each treatment group) were tested using the t-test statistic, given below, for the overall mail and overall response rates. One-sided tests were used for those comparisons that involved Treatment Group 1 (i.e., the control group) since only positive treatment effects were of interest (and expected). Two-sided tests were used for all comparisons not involving Treatment Group 1. A Bonferroni adjustment was made to control the error rate for the family of ten significance tests conducted. Refer to Snedecor and Cochran (1989) for more details on this test. For one-sided tests comparing treatment groups with control: For two-sided tests comparing treatment groups with each other (excluding control group): for treatment group t = 2 3 4 5 , , , and         1/ Treatment Group 5 includes all respondents in the treatment group who received an ATM card (whether their questionnaires were received in the mail or by a face-to-face interview) and Treatment Group 5' includes only those respondents in the treatment group who received an ATM card because they returned their questionnaires in the mail."}, {"section_title": "APPENDIX C: Response Rates by State and Treatment Group", "text": "The calculations for each of the columns are below."}, {"section_title": "Postage", "text": "Postage includes postage for all mailings to the sampled operations and the return postage for completed questionnaires. Specifically, Postage was calculated as follows: where: i = the treatment group number n i = the sample size of treatment group i r (t1)i = the number of surveys returned/completed in treatment group i by date t1 (received before the date NASS began following up with a second survey mailing) A i = postage cost associated with mailing the pre-survey letter ($0.301 for i=1,2,3,4,5) B i = postage cost associated with the initial form mail-out ($0.967 for i=1,2,5, $3.850 for i=3,4) C i = postage cost associated with the post-card follow-up/thank you ($0.187 for i=1,2,3,4,5) D i = postage cost associated with mailing the second, follow-up survey form ($0.967 for i=1,2,5, $3.850 for i=3,4) mc i = the number of questionnaires returned in the mail for treatment group i E i = postage cost associated with a return questionnaire mailing ($0.70 for all treatment groups) I 5 = 1 if i=5 ; 0 otherwise r 5 = the number of overall respondents in treatment group 5 F 5 = postage costs associated with mailing ATM cards to respondents in treatment group 5 ($0.301)"}, {"section_title": "NPC Printing", "text": "The NPC Printing includes the costs to print all survey-related materials (i.e., cover letters, questionnaires, etc.). Specifically, NPC Printing was calculated as follows: where: i = the incentive (treatment) group number A i = printing costs associated with mailing the pre-survey letter (Cover Letter: $0.045 for i=1,2,3,4,5 + ARMS Brochure: $0.26 for i=1,2,3,4,5 ) B i = printing costs associated with the initial form mail-out (Cover Letter: $0.045 for i=1,2,3,4,5 + Farm Foundation Flyer: $0.26 for i=1,2,3,4,5 + Report Form: $1.040 for i=1,2,3,4,5 + Debit Card Instructions: $0.045 for i=2,4,5) C i = printing costs associated with the post-card follow-up/thank you ($0.030 for i=1,2,3,4,5) D i = printing costs associated with mailing the second, follow-up survey form (Cover Letter: $0.045 for i=1,2,3,4,5 + Report Form: $1.040 for i=1,2,3,4,5 + Debit Card Instructions: $0.045 for i=5) E i = printing costs associated with the enumerator follow-up (Report Form: $1.040 for i=1,2,3,4,5) I 5 = 1 if i=5 ; 0 otherwise r 5 = the number of overall respondents in treatment group 5 F 5 = printing costs associated with mailing ATM cards to respondents in treatment group 5 (Cover Letter: $0.045 for i=5 + Debit Card Instructions: $0.045 for i=5)"}, {"section_title": "NPC Extra Admin", "text": "This amount is the National Processing Center's (NPC) best estimate for the labor cost associated with administering the incentive program for all treatment groups. It includes 20 staff days at the GS 5/6 level for statistical assistants. The work performed consisted of preparing the cards in the initial mail packages, preparing 15 waves of mailings for the Treatment Group 5 \"promise to pay\" respondents (i.e., this was to send out their ATM cards after they returned their questionnaires), and processing replacement ATM cards as requested by the respondents. The charge for 20 staff days was approximately $6,000. Since individual charges were unavailable for each treatment group, for simplicity, the total was evenly divided between each of the treatment groups that provided an incentive."}, {"section_title": "ATM Card Charges", "text": "ATM Card Charges includes all charges associated with ATM card usage among the recipients. They include: ATM withdrawals, ATM withdrawal fees, POS purchases, POS purchase fees, ATM balance inquiry, ATM failure fees, POS purchase failure fees. Refer to Table 6 for details on each of these individual charges."}, {"section_title": "39", "text": "ATM Card Admin ATM Card Admin includes the amount JPMorgan Chase charged for the physical cards. This was computed as follows: ATM Card Admin = $0.85 n i for i=2,4,5 where: n i = the sample size of incentive i NASS was charged a \"per card\" cost of $0.85 (appearing in the above equation). Not included is the trivial expense for the few replacement cards that were requested. Note: Not included is an additional $1,000 \"training\" charge from JPMorgan Chase. This was excluded as it is a one-time charge that would not necessarily be incurred in the future."}, {"section_title": "NASDA Costs", "text": "NASDA Costs includes enumeration costs for face-to-face follow-up interviews. Face-to-face follow-up interviews were attempted for all sampled operations that did not return a questionnaire in the mail by February 21. In addition, a small number of follow-up interviews were conducted for operators who mailed back \"completed\" questionnaires with inadequate data. The following steps detail how the NASDA Costs were calculated: 1. An average enumeration cost was computed by state for every sampling unit in the sample that needed to be enumerated. Those who mailed in their surveys by February 21 were given an average enumeration cost of zero and taken out of the denominator since no enumerator visit was necessary for them. The total 2004 ARMS Phase III NASDA expenditures in each state were then divided among everyone who did not mail in their survey by this date. (These average costs were all computed at the state level since enumeration costs were so variable across the states for ARMS Phase III.) Specifically: 2. Sum the average enumeration costs for all the sampling units in each treatment group to get the estimated total NASDA expenditure among each treatment group. These results 40 can be found in the NASDA Costs column of Table F1. The following explains how the nonparametric F-test was used to test if there was evidence of a difference in reporting total land operated (relative to control data) across treatment groups:"}, {"section_title": "Total", "text": "\u2022 For each operation, calculate the absolute value of the difference between the NASS control data value (controlTypeID 100) for total land operated and self-reported total land from ARMS Phase III Core (Item Code 26). \u2022 Take this absolute difference and \"standardize\" it by dividing by the control value. This provides the percentage difference between what is reported and NASS' control value. \u2022 Take all of these percentage differences and rank them from smallest to largest. (Any \"ties\" in the ranking process were given the mean of the ties' ranks. So if the three smallest absolute differences were the same, they would each get a ranking of (1+2+3)/3, or 2). \u2022 Test the hypothesis that the mean rank is the same for each treatment group -this is done with the nonparametric rank F-test (Neter, et al., 1996): This test resulted in an F-test statistic of 0.96 with a p-value of 0.4259. The test was re-run using a non-standardized difference (i.e., Y ij = |controlTypeId100 ij -IC26 ij | ); this resulted in an F-test statistic of 0.71 with a p-value of 0.5862."}]