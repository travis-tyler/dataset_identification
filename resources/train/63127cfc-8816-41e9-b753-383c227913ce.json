[{"section_title": "Abstract", "text": "We present a semi-parametric generative model for predicting anatomy of a patient in subsequent scans following a single baseline image. Such predictive modeling promises to facilitate novel analyses in both voxel-level studies and longitudinal biomarker evaluation. We capture anatomical change through a combination of population-wide regression and a non-parametric model of the subject's health based on individual genetic and clinical indicators. In contrast to classical correlation and longitudinal analysis, we focus on predicting new observations from a single subject observation. We demonstrate prediction of follow-up anatomical scans in the ADNI cohort, and illustrate a novel analysis approach that compares a patient's scans to the predicted subject-specific healthy anatomical trajectory."}, {"section_title": "Introduction", "text": "We present a method for predicting anatomy based on external information, including genetic and clinical indicators. Specifically, given only a single baseline scan of a new subject in a longitudinal study, our model predicts anatomical changes and generates a subsequent image by leveraging subject-specific genetic and clinical information. Such voxel-wise prediction opens up several new areas of analysis, enabling novel investigations both at the voxel level and at the level of derivative biomarker measures. For example, voxel level differences between the true progression of a patient with dementia and their predicted healthy anatomy highlight spatial patterns of disease. We validate our method by comparing measurements of volumes of anatomical structures based on predicted images to those extracted from the acquired scans.\nOur model describes the change from a single (or baseline) medical scan in terms of population trends and subject-specific external information. We model how anatomical appearance changes with age on average in a population, as well as deviations from the population average using a person's health profile. We characterize such profiles non-parametrically based on the genotype, clinical information, and the baseline image. Subject-specific change is constructed AE A listing of ADNI investigators is available at http://tinyurl.com/ADNI-main."}, {"section_title": "Data", "text": "used via AD DREAM Challenge: https://www.synapse.org/ #!Synapse:syn2290704 from the similarity of health profiles in the cohort, using a Gaussian process parametrized by a population health covariance. Given the predicted change, we synthesize new images through an appearance model.\nStatistical population analysis is one of the central topics in medical image computing. The classical correlation-based analysis has yielded important characterization of relationships within imaging data and with independent clinical variables [2, 11, 12, 14] . Regression models of object appearance have been previously used for atlas construction and population analysis [2, 14] . These methods characterize population trends with respect to external variables, such as age or gender, and construct clinically relevant population averages. Longitudinal analyses also characterize subject-specific temporal effects, usually in terms of changes in the biomarkers of interest. Longitudinal cohorts and studies promise to provide crucial insights into aging and disease [11, 12] . Mixed effects models have been shown to improve estimation of subject-specific longitudinal trends by using inter-population similarity [3, 15] . While these approaches offer a powerful basis for analysis of biomarkers or images in a population, they require multiple observations for any subject, and do not aim to provide subject-specific predictions given a single observation. The parameters of the models are examined for potential scientific insight, but they are not tested for predictive power. In contrast, we define the problem of population analysis as predicting anatomical changes for individual subjects. Our generative model incorporates a population trend and uses subject-specific genetic and clinical information, along with the baseline image, to generate subsequent anatomical images. This prediction-oriented approach provides avenues for novel analysis, as illustrated by our experimental results."}, {"section_title": "Prediction Model", "text": "Given a dataset of patients with longitudinal data, and a single baseline image for a new patient, we predict follow-up anatomical states for the patient. We model anatomy as a phenotype y that captures the underlying structure of interest. For example, y can be a low-dimensional descriptor of the anatomy at each voxel. We assume we only have a measurement of our phenotype at baseline y b for a new subject. Our goal is to predict the phenotype y t at a later time t.\nWe let x t be the subject age at time t, and define \u0394x t x t \u00a1x b and \u0394y t y t \u00a1y b .\nWe model the change in phenotype y t using linear regression:\nwhere \u03b2 is the subject-specific regression coefficient, and noise N \u00d40,\u03c3 2 \u00d5 is sampled from zero-mean Gaussian distribution with variance \u03c3 2 ."}, {"section_title": "Subject-Specific Longitudinal Change", "text": "To model subject-specific effects, we define \u03b2 \u03b2 H\u00d4g, c, f b \u00d5, where\u03b2 is a global regression coefficient shared by the entire population, and H captures a deviation from this coefficient based on the subject's genetic variants g, clinical information c, and baseline image features f b . We assume that patients' genetic variants and clinical indicators affect their anatomical appearance, and that subjects with similar health profiles exhibit similar patterns of anatomical change. We let h G \u00d4\u00a4\u00d5, h C \u00d4\u00a4\u00d5, h I \u00d4\u00a4\u00d5 be functions that capture genetic, clinical and imaging effects on the regression coefficients:\nCombining with (1), we arrive at the full model\nwhich captures the population trend\u03b2, as well as the subject-specific deviations\nFor a longitudinal cohort of N subjects, we group all T i observations for subject i to form \u0394y i \u00d6 y i1 , y i2 , ...y iT i \u00d7. We then form the global vector \u0394y \u00d6\u0394y 1 , \u0394y 2 , ..., \u0394y N \u00d7. We similarly form vectors \u0394x, h G , h C , h I , g, c, f b and , to build the full regression model:\nwhere is the Hadamard, or element-wise product. This formulation is mathematically equivalent to a General Linear Model (GLM) [9] in terms of the health\nWe employ Gaussian process priors to model the health functions:\nwhere covariance kernel function \u03c4 2 D K D \u00d4z i , z j \u00d5 captures the similarity between subjects i and j using feature vectors z i and z j for D \u00c8 \u00d8 G, C, I\u00d9. We discuss the particular form of K\u00d4\u00a4, \u00a4\u00d5 used in the experiments later in the paper."}, {"section_title": "Learning", "text": "The Bayesian formulation in (4) and (5) can be interpreted as a linear mixed effects model (LMM) [10] or a least squares kernel machine (LSKM) regression model [5, 8] . We use the LMM interpretation to learn the parameters of our model, and the LSKM interpretation to perform final phenotype predictions.\nSpecifically, we treat\u03b2 as the coefficient vector of fixed effects and h G , h C , and h I as independent random effects. We seek the maximum likelihood estimates of parameters\u03b2 and \u03b8 \u00d4 \u03c4 2 G , \u03c4 2 C , \u03c4 2 I , \u03c3 2 \u00d5 by adapting standard procedures for LMMs [5, 8] . As standard LMM solutions become computationally expensive for thousands of observations, we take advantage of the fact that while the entire genetic and the image phenotype data is large, the use of kernels on baseline data reduces the model size substantially. We obtain intuitive iterative updates that project the residuals at each step onto the expected rate of change in likelihood, and update\u03b2 using the best linear unbiased predictor."}, {"section_title": "Prediction", "text": "Under the LSKM interpretation, the terms h\u00d4\u00a4\u00d5 are estimated by minimizing a penalized squared-error loss function, which leads to the following solution [5, 7, 8, 16] :\nfor some vector \u03b1. Combining with the definitions of the LMM, we estimate coefficients vectors \u03b1 G , \u03b1 C and \u03b1 I from a linear system of equations that involves our estimates of\u03b2 and \u03b8. We can then re-write (4) as\nand predict a phenotype at time t for a new subject i:"}, {"section_title": "Model Instantiation for Anatomical Predictions", "text": "The full model (3) can be used with many reasonable phenotype definitions.\nHere, we describe the phenotype model we use for anatomical predictions and specify the similarity kernels of the health profile."}, {"section_title": "Anatomical Phenotype", "text": "We define a voxel-wise phenotype that enables us to predict entire anatomical images. Let \u03a9 be the set of all spatial locations v (voxels) in an image, and I b \u00d8I b \u00d4v\u00d5\u00d9 v\u00c8\u03a9 be the acquired baseline image. We similarly define A \u00d8A\u00d4v\u00d5\u00d9 v\u00c8\u03a9 , to be the population atlas template. We assume each image I is generated through a deformation field \u03a6 \u00a11 AI parametrized by the corresponding displacements \u00d8u\u00d4v\u00d5\u00d9 v\u00c8\u03a9 from the common atlas to the subject-specific coordinate frame [14] , such that I\u00d4v\u00d5 A\u00d4v u\u00d4v\u00d5\u00d5. We further define a followup image I t as a deformation \u03a6 Bt from the baseline image I b , which can be composed to yield an overall deformation from the atlas to the follow-up scan\nUsing displacements u \u00bd \u00d4v\u00d5 as the phenotype of interest in (1) captures the necessary information for predicting new images, but leads to very high dimensional descriptors. To regularize the transformation and to improve efficiency, we define a low-dimensional embedding of u \u00bd \u00d4v\u00d5. Specifically, we assume that the atlas provides a parcellation of the space into L anatomical labels L \u00d8 \u03a8 \u00d9 L l 1 .\nWe build a low-dimensional embedding of the transformation vectors u\u00d4v\u00d5 within each label using PCA. We define the relevant phenotypes \u00d8y l,c \u00d9 as the coefficients associated with the first C principal components of the model that capture 95% of the variance in each label, for l 1 . . . L.\nWe predict the phenotypes using (8) . To construct a follow-up image I t given phenotype y t , we first form a deformation field \u00d4 \u03a6 \u00a11 At by reconstruction from the estimated phenotype y t , and use \u00d4 \u03a6 At assuming an invertible transformation. Using the baseline image, we predict a subsequent image via \u03a6 Bt \u00d4 \u03a6 At \u00a5\u03a6 \u00a11 AB . Note that we do not directly model changes in image intensity. While population models necessitate capturing such changes, we predict changes from a baseline image. We also assume that affine transformations are not part of the deformations of interest, and thus all images are affinely registered to the atlas."}, {"section_title": "Health Similarities", "text": "To fully define the health similarity term H\u00d4\u00a4, \u00a4, \u00a4\u00d5, we need to specify the forms of the kernel functions K G \u00d4\u00a4, \u00a4\u00d5, K C \u00d4\u00a4, \u00a4\u00d5, and K I \u00d4\u00a4, \u00a4\u00d5.\nFor genetic data, we employ the identical by state (IBS) kernel often used in genetic analysis [13] . Given a vector of genetic variants g of length S, each genetic locus is encoded as g\u00d4s\u00d5 \u00c8 \u00d8 0, 1, 2\u00d9, and\nTo capture similarity of clinical indicators c, we form the kernel function\nwhere diagonal weight matrix W captures the effect size of each clinical indicator on the phenotype, and \u03c3 2 C is the variance of the clinical factors. We define the image feature vectors f b as the set of all PCA coefficients defined above for the baseline image. We define the image kernel matrix as\nwhere \u03c3 2 I is the variance of the image features."}, {"section_title": "Experiments", "text": "We illustrate our approach by predicting image-based phenotypes based on genetic, clinical and imaging data in the ADNI longitudinal study [6] that includes two to ten follow-up scans acquired 0.5 \u00a1 7 years after the baseline scan. We use affine registration to align all subjects to a template constructed from 145 randomly chosen subjects, and compute non-linear registration warps \u03a6 AI for each image using ANTs [1] . We utilize a list of 21 genetic loci associated with Alzheimer's disease (AD) as the genetic vector g, and the standard clinical factors including age, gender, marital status, education, disease diagnostic, and cognitive tests, as the clinical indicator vector c. We learn the model parameters from 341 randomly chosen subjects and predict follow-up volumes on a separate set of 100 subjects. To evaluate the advantages of the proposed predictive model, we compare its performance to a population-wide linear regression model that ignores the subject-specific health profiles (i.e., H 0)."}, {"section_title": "Volumetric Predictions", "text": "In the first simplified experiment, we define phenotype y to be a vector of several scalar volume measurements obtained using FreeSurfer [4] . In addition to the population-wide linear regression model, we include a simple approach of using the baseline volume measurements as a predictor of the phenotype trajectory, effectively assuming no volume change with time. Since in many subjects, the volume differences are small, all three methods perform comparably when evaluated on the whole test set. To evaluate the differences between the methods, we focus on the subset of subjects with substantial volume changes, reported in Fig. 1 . Our method consistently achieves smaller relative errors than the two baseline approaches."}, {"section_title": "Anatomical Prediction", "text": "We also evaluate the model for full anatomical scan prediction. To quantify prediction accuracy, we propagate segmentation labels of relevant anatomical structures from the baseline scan to the predicted scan using the predicted warps. We compare the predicted segmentation label maps with the actual segmentations of the follow-up scans. The warps computed based on the actual follow-up scans through the atlas provide an indication of the best accuracy the predictive model could achieve when using warps to represent images. Similar to the volumetric predictions, the full model offers modest improvements when evaluated on the entire test set, and substantial improvements in segmentation accuracy when evaluated in the subjects who exhibit large volume changes between the baseline scan and the follow-up scan, as reported in Fig. 2 . In both experiments, all components h g , h c and h I contributed significantly to the improved predictions.\nOur experimental results suggest that the anatomical model depends on registration accuracy. In particular, we observe that directly registering the follow-up scan to the baseline scan leads to better alignment of segmentation labels than when transferring the labels through a composition of the transformations from the scans to the atlas space. This suggests that a different choice of appearance model may improve prediction accuracy, a promising direction for future work.\nTo demonstrate the potential of the anatomical prediction, we predict the follow-up scan of a patient diagnosed with dementia as if the patient were healthy. Specifically, we train our model using healthy subjects, and predict follow-up scans for AD patients. In Fig. 2 we illustrate an example result, comparing the areas of brain anatomy that differ from the observed follow-up in the predicted healthy brain of this AD patient. Our prediction indicates that ventricle expansion would be different if this patient had a healthy trajectory."}, {"section_title": "Conclusions", "text": "We present a model to predict the anatomy in patient follow-up images given just a baseline image using population trends and subject-specific genetic and clinical information. We validate our prediction method on scalar volumes and anatomical images, and show that it can be used as a powerful tool to illustrate how a subject-specific brain might differ if it were healthy. Through this and other new applications, our prediction method presents a novel opportunity for the study of disease and anatomical development."}]