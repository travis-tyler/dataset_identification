[{"section_title": "Abstract", "text": "Abstract-Discriminative methods commonly produce models with relatively good generalization abilities. However, this advantage is challenged in real-world applications (e.g., medical image analysis problems), in which there often exist outlier data points (sample-outliers) and noises in the predictor values (feature-noises). Methods robust to both types of these deviations are somewhat overlooked in the literature. We further argue that denoising can be more effective, if we learn the model using all the available labeled and unlabeled samples, as the intrinsic geometry of the sample manifold can be better constructed using more data points. In this paper, we propose a semi-supervised robust discriminative classification method based on the least-squares formulation of linear discriminant analysis to detect sample-outliers and feature-noises simultaneously, using both labeled training and unlabeled testing data. We conduct several experiments on a synthetic, some benchmark semi-supervised learning, and two brain neurodegenerative disease diagnosis datasets (for Parkinson's and Alzheimer's diseases). Specifically for the application of neurodegenerative diseases diagnosis, incorporating robust machine learning methods can be of great benefit, due to the noisy nature of neuroimaging data. Our results show that our method outperforms the baseline and several state-of-the-art methods, in terms of both accuracy and the area under the ROC curve."}, {"section_title": "INTRODUCTION", "text": "DISCRIMINATIVE methods learn a mapping from the input feature space to the output label space for a task of classification (or regression). Such methods usually achieve good classification (or regression) results, compared to the generative methods, when there is enough number of training samples. But they carry out limited abilities when there are a small number of labeled data [1] . On the other hand, when noise contaminates the data, discriminative models usually fail to find an optimal mapping. In many realworld applications, the data are usually contaminated by different levels of noise. In some cases, a whole bunch of samples are affected (e.g., deviations in neuroimaging data due to radiation or patient movements during the imaging process), and therefore not useful for the learning task. These types of deviations are often denoted as sample-outliers. On the other hand, sometimes only some specific predictor values or features are infected, known as intra-sample-outliers (or feature-noises).\nVarious efforts have been made to add robustness to different learning methods. For instance, Suzumaura et al. [2] and Xu et al. [3] introduced robustness to the conventional support vector machine formulation by proposing various regularization terms or suppressing the influence of the outliers. In other works, Kim et al. [4] and Croux et al. [5] proposed robust variations of Fisher/ Linear Discriminant Analysis (LDA) method, and Li et al. [6] introduced a worst-case LDA, by minimizing the upper bound of the LDA cost function. These methods are all robust to sample-outliers. On the other hand, some methods were proposed to deal with the feature-noises, such as [7] , [8] . Many previous methods use Robust Principal Component Analysis (RPCA) [9] , to deal with featurenoises in an unsupervised manner. Furthermore, many robust approaches that denoise the data while training the model do not offer straightforward strategies to deal with the testing data. Often, the denoising procedure of the training and the testing data are conducted separately (e.g., in [10] ), which might induce a bias to the whole learning process. One solution is to denoise the training and the testing data together, provided that the testing data are available. Therefore, we propose to take advantage of them as unlabeled data during the training phase. Under such semi-supervised setting, the constructed discriminative model can be more reliable, particularly for the cases with the small-sample-size problem. This could be attributed to the fact that more samples are being used to model the intrinsic geometry of the sample manifold.\nThe main application we are anticipating in this paper is the diagnosis of neurodegenerative diseases, based on neuroimaging data. This is a challenging problem, as the data is pretty much prone to noise and often there is a limited number of samples. Hence, there is a calling need for robust machine learning methods for such applications. Neurodegenrative diseases are debilitating and incurable conditions caused by progressive degeneration or death of the cells in the brain nervous system. These diseases affect millions of people around the world. Alzheimer's Disease (AD) and Parkinson's Disease (PD) are among the most common types. Although neurodegenerative diseases manifest with diverse pathological features, the cellular level processes resemble similar structures. For instance, in AD, deposits of tiny protein plaques result into brain damage and progressive loss of memory [11] , while PD is mainly initiated by a selective loss of dopaminergic neurons in the Substantia Nigra (SN) brain region, leading to declining in the generation of a chemical messenger, dopamine. Lack of dopamine yields loss of ability to control body movements, along with several non-motor problems (e.g., depression, and anxiety) [12] . These diseases are often incurable; thus, early diagnosis and treatment are crucial to slow down their progression in the initial stages.\nThe challenges for building reliable diagnosis models include: (1) It is usually burdensome to acquire noise-free imaging data from the patients. Different sources of noise may affect the acquired data, including a wide variety of noises in the neuroimage acquisition procedure, the imposed artifacts due to preprocessing, and the large amount of inter-subject variabilities; (2) To build a good diagnosis model, through learning a classifier, we need a sufficiently large number of labeled subjects. However, acquiring reliably enough labeled data is costly and time-consuming. Therefore, models that can take advantage of unlabeled data (subjects that we are not certain about their disease) could be of great interest; (3) Different neurodegenerative diseases often affect different regions of the brain, i.e., only certain regions of the brain are associated with the disease. Thus, using all features can undermine the diagnosis performance, and we need to identify the imaging biomarkers for each specific disease while learning the diagnosis model.\nTo deal with the aforementioned challenges, we propose a semisupervised discriminative classifier, to take advantage of the available unlabeled testing data. This leads to a more substantial number of samples, which can yield better modeling of the intrinsic geometry of the sample manifold. As a result, our model jointly estimates the noise model (both sample-outliers and featurenoises) on the whole labeled training and unlabeled testing data and simultaneously builds a discriminative model upon the denoised training data. Unlike many previous works on denoising medical images, we do not define the problem of denoising separately from the analysis part. In the sense that if a sample (or a feature value) does not act in accordance with others in building the model, it should be counted as a sample-outlier (or a feature-noise). This observation suggests that intertwining the denoising procedure with the learning framework will help to identify the sampleoutliers and feature-noises more efficiently while learning a robust classification model. It is important to note that denoising and outlier detection has a long history in the area of medical image analysis and computing. The inter-and intra-subject variabilities, the noise sourced from the images devices, and the pre-processing errors emerge the study of robust methods for analyzing medical imaging data. For instance, in the recent years, several attempts have been made for denoising the medical images [13] , [14] , [15] , [16] or detecting outliers [17] , [18] , as a preprocessing step to any analysis on medical images."}, {"section_title": "Background and Overview of the Proposed Method", "text": "In this paper, we introduce a novel classification model based on LDA, which is robust against both sample-outliers and featurenoises, referred to as robust feature-sample linear discriminant analysis (RFS-LDA). The original LDA formulation finds the mapping between the sample space and the label space through a linear transformation matrix, maximizing a so-called Fisher discriminant ratio [4] . In practice, the major drawback of LDA is the smallsample-size problem, which arises when the number of available training samples is much less than the dimensionality of the feature space [19] . Original LDA finds the mapping by incorporating covariance matrices of the input feature matrices. In cases where the number of samples is much less than the number of features, these matrices are probably rank-deficient [20] . A reformulation of LDA based on the reduced-rank least-squares problem (known as LS-LDA) [20] tackles this problem. LS-LDA finds the mapping b b 2 R l\u00c2m by solving the following problem:\nwhere Y tr 2 R l\u00c2N tr is a binary class label indicator matrix, for l different classes (or labels), and X tr 2 R m\u00c2N tr is the matrix containing\nfactor that compensates for the different number of samples in each class [20] . As a result, the mapping b b is a reduced rank transformation matrix [8] , [20] , which could be used to project a test data x tst 2 R m\u00c21 onto an l-dimensional space. Note that directly minimizing (1) avoids the small-sample-size problem by not using the covariance matrices. After it projects the samples to the output space, we need a simple step to infer the class labels. LDA maximizes inter-class variance, while minimizing the intra-class variance, in the mapped space. Thus, we expect that in the mapped space, same-class samples to be closer to each other. The class labels could, therefore, be simply determined using a k-NN strategy.\nTo make LDA robust against noisy data, Fidler et al. [7] estimate a robust basis, which consists all the discriminative information for classification or regression. In the testing phase, the estimated basis identifies the outliers in samples (images in their case) and then calculates the coefficients using a subsampling approach. On the other hand, Huang et al. [8] proposed a general formulation for Robust Regression (RR) and classification (i.e., Robust LDA or RLDA), where, they first denoise the training feature values using a strategy similar to RPCA [9] , and then build the above LS-LDA model using the denoised data. In the testing stage, they denoise the testing samples using the denoised training data. This separate denoising procedure could not effectively form the underlying geometry of sample space to denoise the data. Furthermore, RR [8] only accounts for feature-noises by imposing a sparse noise model constraint on the features matrix, despite the fact that the least-squares data fitting term in (1) is vulnerable to large sample-outliers.\nRecently, in robust statistics, it is found that ' 1 functions are able to make more reliable estimations [21] than ' 2 least-squares fitting functions. This has been previously adopted in many applications, including robust face recognition [22] and robust dictionary learning [23] . Reformulating the objective in (1) with ' 1 loss entails the following problem:\nWe incorporate this fitting function to deal with the sampleoutliers, in this paper. We also adopt a strategy to simultaneously denoise the data from feature-noises. This is done through a semisupervised setting to take advantage of all labeled and unlabeled data, and build the structure of the sample space more robustly. (Fig. 1b) , traditional methods usually fail to build reliable models.\nSemi-supervised learning has long been of great interest in different fields, because it can make use of unlabeled or poorly labeled data to achieve better prediction models [24] , [25] . For instance, Joulin and Bach [26] introduced a convex relaxation and used their model in different semi-supervised learning scenarios. In another work, Cai et al. [27] proposed a semi-supervised discriminant analysis, where the separation between different classes is maximized using the labeled data points, while the unlabeled data points estimate the structure of the data. Belkin et al. [28] similarly used the unlabeled data for regularization. In contrast, we incorporate the unlabeled testing data in our formulation to better estimate the intrinsic geometry of the sample manifold and denoise the data, while building the discriminative model upon the labeled training data. By incorporating the unlabeled testing data (Fig. 1c) , we learn the classification model, while denoising both training and testing data and detecting sample-outliers.\nWe apply our method for the diagnosis of neurodegenerative brain disorders. Specifically, in this study, we use two popular databases: PPMI [29] and ADNI [30] . The former aims at investigating PD and its related disorders, while the latter is designed for diagnosing AD and its prodromal stage, known as Mild-Cognitive Impairment (MCI). In addition, to validate the proposed method, we further conduct experiments on synthetic data, as well as some benchmark datasets for semi-supervised learning."}, {"section_title": "Contributions", "text": "The contributions of this paper are multi-fold: (1) We propose an approach to dealing with the sample-outliers and feature-noises simultaneously and build a robust discriminative classifier. The sample-outliers are penalized through an ' 1 fitting function. (2) Our proposed model operates under a semi-supervised setting, where the whole data (i.e., labeled training, and unlabeled testing samples) are incorporated to build the intrinsic geometry of the sample space, which leads to better data denoising. (3) We further select the most discriminative features for the learning process through regularizing the weights matrix with an ' 1 norm. This is especially of great interest for the neurodegenerative disease diagnosis, where the features from different regions of the brain are extracted, but not all the regions are associated with a certain disease. Thus, the most discriminative regions associated with the disease would be identified, leading to a more reliable diagnosis model."}, {"section_title": "THE PROPOSED METHOD: RFS-LDA", "text": "Suppose we have N tr training and N tst testing samples, each with a m dimensional feature vector, which leads to a set of N \u00bc N tr \u00fe N tst total samples. Let X 2 R m\u00c2N denote the set of all samples (both training and testing), in which each column indicates a single sample, and also let y i 2 R 1\u00c2N their corresponding ith labels. In general, with l different labels, we can define Y 2 R l\u00c2N . Thus, X and Y are composed by stacking up the training and testing data as: X \u00bc \u00bdX tr X tst and Y \u00bc \u00bdY tr Y tst . Our goal is to determine the labels of the test samples, Y tst 2 R l\u00c2N tst . Note that, throughout the paper, bold capital letters denote matrices (e.g., A), while bold lowercase letters denote vectors (e.g., a). All non-bold letters denote scalar variables. a ij is the scalar in the row i and column j of A. ha 1 ; a 2 i denotes the inner product between a 1 and a 2 . kak "}, {"section_title": "Formulation", "text": "All the available samples, both labeled and unlabeled, are arranged into a matrix, X 2 R m\u00c2N , each of whose columns represents the feature vector of a sample. To achieve a robust classifier, we seek to denoise this matrix. Following [31] , [32] , this could be done by assuming that X can be spanned on a low-rank subspace and therefore should be rank-deficient. This assumption supports the fact that samples from same classes are more correlated [8] , [32] and linearly-dependent. Accordingly, the original matrix X is decomposed into the summation of two counterparts, D 2 R m\u00c2N and E 2 R m\u00c2N . The former represents the denoised data matrix, while the latter is the error matrix. This is similar to RPCA [9] , used in many computer vision applications. With this decomposition, we can assume that the denoised data matrix shall be rankdeficient and the error matrix sparse.\nBut as one can easily infer, this process of denoising does not incorporate the label information and is, therefore, unsupervised. Nevertheless, recall that we are also seeking a mapping between the denoised training samples and their respective labels. So, matrix D should be spanned on a low-rank subspace that would lead to a good classification model of its sub-matrix, D tr . We incorporate the regression model in (2) as the fitting function to compute a mapping b b. A schematic illustration of the proposed method is depicted in Fig. S1 of the supplementary material, which can be found on the Computer Society Digital Library at http://doi. ieeecomputersociety.org/10.1109/TPAMI.2018.2794470.\nTo ensure the rank-deficiency of the matrix D, like many previous works [9] , [31] , [32] , we approximate the rank function using the nuclear norm (i.e., the sum of the singular values of the matrix). The noise is modeled using the ' 1 norm of the matrix, which ensures a sparse noise model on the feature values. Accordingly, the objective function for RFS-LDA under a semi-supervised setting would be formed as\nwhere the first term is the ' 1 regression model introduced in (2) . This term only operates on the denoised training samples from matrix D with a row of all 1's added to it (denoted asD), to counter for the bias in the linear model. The second and third terms, together with the first constraint, are similar to the RPCA formulation [9] . They denoise the labeled training and unlabeled testing data together, and in combination with the first term, we ensure that the denoised data also specifies a favorable regression. The last term is a regularization on the learned mapping coefficients, to avoid trivial or unexpectedly large values. The hyperparameters h, 1 and 2 are the scalar regularization hyperparameters, which will be discussed in detail later.\nThe regularization on the coefficients could be posed as a simple norm of the matrix, b b. But, in many applications, like ours (disease diagnosis), many of the features in the feature vectors are redundant. This is because we extract features from different brain regions, but not all the regions contribute to a certain disease. Therefore, it is desirable to determine which features are the most relevant and the most discriminative for the task. Following [11] , [22] , [33] , we are seeking a sparse set of weights that ensures incorporating the most discriminative features. Therefore, we propose a regularization on the weights matrix as a combination of the ' 1 and Frobenius norms R\u00f0b b\u00de \u00bc kb bk 1;1 \u00fe gkb bk F :\nEvidently, the solution to the objective function in (3) is not easy to achieve. This is because it contains a quadratic term, and the minimization of the ' 1 fitting function is not straightforward, due to its indifferentiability. To this end, we formalize the solution with a similar strategy as in Iteratively Re-weighted Least Squares (IRLS) [21] . The ' 1 fitting term is approximated by a conventional ' 2 least-squares, in which each of the samples in theD matrix is weighted with the reverse of their regression residual. Additionally, since we regularize the weights b b using a combination of ' 1 and ' 2 norms, the non-zero elements would represent the selected features by the algorithm. In order to reflect this to feature denoising scheme, we define a projection operator P b b \u00f0:\u00de. This operator projects the values of the non-selected features (respective to zero values in b b) to zero, to decrease their effect in minimizing the rank of the matrix D (in the second term). Therefore, the new problem would be\nwhere\u00e2 a is a diagonal matrix, the ith diagonal element of which is the ith sample's weight\n; 8 i; j 2 f0; . . . ; N tr g; i 6 \u00bc j;\u00e2 a ij \u00bc 0:\nHyperparameter d is a small positive number (10 \u00c04 in our experiments), to prevent from any division by zeros in (6) . In the next section, we introduce an algorithm to solve this optimization problem.\nOur work is closely related to the RR formulations in [8] , where the authors impose a low-rank assumption on the training data feature values and an ' 1 assumption on the noise model. The discriminant model is learned similarly to LS-LDA, as described in (1) . Whereas, we observed that to have a more robust regression model, we need to establish a strategy where we can weight the samples. This is because the ' 1 noise model in [8] can only discard a controlled amount of sparse noise in the feature values, not the whole samples. On the other hand, our model operates under a semi-supervised setting, where both labeled training and unlabeled testing samples are denoised simultaneously, leading to a more robust denoising model. Also, our model further selects the most discriminative features to learn the regression model, by regularizing the learned weights and enforcing a sparsity condition on them.\nTo optimize the objective function in (5), we use the Alternating Direction Method of Multipliers (ADMM) [34] . The detailed optimization steps, along with the comprehensive analysis of the algorithm, its convergence properties and an upper bound for the time complexity of the proposed algorithm are provided in the supplementary material, available online."}, {"section_title": "EXPERIMENTS", "text": "To evaluate the proposed approach, we compare our method against several baselines and state-of-the-art methods in different scenarios. The first experiment evaluates our method on a synthetic set of data, which highlights how the proposed method is robust against sample-outliers or feature-noises separately, or when they occur at the same time. Then we employ some benchmark semisupervised learning datasets and report results in comparisons with some baseline and state-of-the-art methods. The results of these two experiments (i.e., on synthetic and benchmark data) are reported in the supplementary material, available online. We then apply the proposed RFS-LDA method to the problem of neurodegenerative brain disorder and disease diagnosis.\nFor the choice of hyperparameters, a set of possible values are first predefined, and the best hyperparameters are selected through 10-fold cross-validation, for all the competing methods. The RFS-LDA hyperparameters (as in Eq. (5)) are set with the same strategy as in [8] \nand r (controlling the fmgs in the iterative optimization algorithm) is set to 1.01. We have set L 1 ; L 2 ; L 3 and g through inner-cross-validation grid-search in the range \u00bd10 \u00c04 ; 10."}, {"section_title": "Datasets", "text": "In this study, we use two real-world databases for two different brain neurodegenerative diseases, namely PD and AD. The first set of data is obtained from the Parkinson's Progression Markers Initiative (PPMI) database [29] , with the MRI data from 374 PD and 169 normal control (NC) subjects. The second dataset comes from the Alzheimer's disease neuroimaging initiative (ADNI) database, which includes MRI and FDG-PET data. We used 93 AD patients, 202 MCI patients, and 101 NC subjects, each with complete MRI and FDG-PET data. The subjects' brain images are preprocessed and regions of interest (ROI) features are extracted for each subject. For more detailed information about these two datasets and the preprocessing steps for feature extraction refer to the supplementary material, available online."}, {"section_title": "Baseline Methods", "text": "We compare our proposed method with different baseline methods, including the conventional LS-LDA [20] , RLDA [8] , and linear Support Vector Machine (SVM). Another baseline method can be defined as running the same procedures as in the proposed method but disjointly. Therefore, we apply RPCA on the matrix X separately to first denoise, and then classify the denoised data using LS-LDA (denoted as RPCA+LS-LDA) [8] . To analyze the effectiveness of the feature selection strategy of the proposed method, we also include baseline methods which use sparse feature selection (SFS) together with SVM (SFS+SVM), and RLDA (SFS+RLDA). Except for RPCA+LDA, the other methods in comparison do not incorporate the testing data. In order to have a fair set of comparisons, we also compare against the transductive Matrix Completion (MC) approach [32] and the semi-supervised formulation of SVM (S 3 VM) [35] . These two methods incorporate the unlabeled testing data in the process of training their models. Additionally, in order to further evaluate the effect of the ' 1 norm regularization on the weights matrix b b, we also report results for RFS-LDA when regularized by only gkb bk F (denoted as RFS-LDA \u00c3 ), rather than the regularization term introduced in (4). Finally, we report results using the supervised version of our proposed method, which is denoted as supervised RFS-LDA (S-RFS-LDA). In S-RFS-LDA, we train our model using only the training data, where X in (5) is replaced with X tr . In this way, we can examine the effect of using unlabeled testing data in the prediction model."}, {"section_title": "Disease Diagnosis", "text": "We evaluate our method with two popular datasets for neurodegenerative disease diagnosis, PPMI and ADNI, for diagnosis of PD and AD, respectively. These datasets, subject information, preprocessing steps, and feature extraction are explained in Section C of the supplementary material, available online.\nResults. The first row in Table 1 shows the diagnostic accuracy of the proposed technique (RFS-LDA) in comparisons with different baseline and state-of-the-art methods using 10-fold crossvalidation. The results show that the proposed method outperforms all others. This can be attributed to the fact that our method better deals with feature-noises and sample-outliers. Recall that samples and their corresponding feature vectors extracted from the neuroimaging data are quite prone to noise, as discussed earlier. Therefore, some of the samples might not be useful, and some might be contaminated by a certain amount of noise. Our method can deal with both types of noises, as supported by the results. The second disease diagnosis experiment is conducted on ADNI, in which the goal is to discriminate normal controls from mild cognitive impairment and AD subjects. Therefore, NC subjects form our negative class, while the positive class is defined as AD in one experiment, and MCI in the other. The diagnosis results of the AD versus NC and MCI versus NC are reported in the second and third rows in Table 1 , respectively. As it can be seen, in comparison with the state-of-the-art, our method achieves better results in terms of both accuracy and the area under ROC curve.\nIt is worth noting that running the model using a 10-fold crossvalidation for the PD versus NC (543 subjects), AD versus NC (194 subjects), and MCI versus NC (303 subjects) experiments on a PC (Intel Core i7 @ 2.30 GHz and 8.00 GB of memory), with a parallel implementation in MATLAB (i.e., using parfor for 4 workers) took approximately 6, 2 and 3.5 hours, respectively. Additionally, to test the statistical significance of the obtained results, we further conducted a Fisher exact test [36] on the accuracy score achieved by each of the methods. This test verifies that the method is significantly more accurate (with a p-value of p < 0:05) than randomly assigning the samples to the two classes. The results of this statistical test indicated that the proposed method achieves a p-value of even less than 0.001. This shows that there are no random associations with the obtained results. However, for some of the compared baseline methods, a p-value of p > 0:05 was observed, which is not appealing. These methods are marked with a y sign in Table 1 . It is important to note that the comparisons between the supervised (S-RFS-LDA) and the semi-supervised (RFS-LDA) versions of the proposed algorithm in both Table 1 and Fig. S5 of the supplementary material, available online, show that including the unlabeled testing data improves the results by a relatively notable margin. This can be because including more samples gives us a better representation of the sample manifold, leading to better denoising of simultaneous training and testing data, in a way that a better classifier is built.\nAlthough the studies on Parkinson's disease using modern machine learning techniques are scarce, there are quite a few studies in the literature for Alzheimer's disease. State-of-the-art machine learning approaches for this purpose either aim at developing feature selection techniques or focus on designing delicate classifiers. The first type usually use sophisticated techniques for feature selection [37] , [38] , feature learning [39] , or feature extraction [40] , [41] , [42] and then an straightforward classification technique (like SVM) is utilized. The second type develops task-specific classifiers to enhance the classification accuracies, e.g., [43] , [44] , [45] . In contrast, our method constructs the sample manifold using all labeled and unlabeled data to denoise the features and also selects the best features for classification, with a classification loss robust to sampleoutliers. In Table 2 , we compare our method with several state-ofthe-art methods for Alzheimer's disease diagnosis. The table includes all the information about the dataset and the methods they used for obtaining those results. This is only to show where our method stands among the previous works in the same field.\nAs discussed earlier, in medical imaging applications many sources of noise contribute to the acquired data, and therefore methods that can deal with noise and outliers are of great interest. Our method enjoys from a single optimization objective that can simultaneously suppress sample-outliers and feature-noises, which, compared to other methods, exhibits a good performance. One of the interesting functions of the proposed method is the regularization on the mapping coefficients with the ' 1 norm, which would select a compact set of features to contribute to the learned mapping. The magnitude of the coefficients would show the relevance of the specific features for building the prediction model. In our application, the features from the whole brain regions are extracted, but not all the ROIs are associated with the disease (e.g., AD, MCI or PD). By exploring the learned coefficients by our method, we can determine which brain regions are highly associated with a certain disease. Identification of Disease Biomarkers. To extract these most relevant ROIs, we select the ROIs that were given larger weights in 50 percent of the ten repetitions of the 10-fold cross-validation tests. Fig. 2a visualizes the most relevant regions for PD on a raw brain template, including the middle frontal gyrus right, pons, substrata nigra left and right, red nucleus left, pallidum left, pautmen left, caudate right, inferior temporal left, and superior temporal gyrus right. As in the previous studies in the literature [46] , [47] , deep brain and striatum areas are known to play crucial roles for PD. Our study also confirms these clinical findings. Same experimental settings for AD and MCI identifies the top regions selected by our algorithm in AD versus NC and MCI versus NC classification scenarios (Figs. 2b and 2c, respectively) . These regions, including middle temporal gyrus, medial front-orbital gyrus, postcentral gyrus, caudate nucleus, cuneus, and amygdala, have also been reported to be associated with AD and MCI in the literature [11] , [48] . The analysis of such selection of brain regions can be further incorporated for future clinical studies.\nMethod Discussions. To analyze the effect of the sample-outlier detection in the proposed framework, we employ a dimensionality reduction technique to facilitate the visualization of the data points. We project the samples of the AD versus NC experiment into the 2-D space using t-SNE [49] . The t-SNE projection technique visualizes high-dimensional data by giving each sample a location in a twodimensional map. The map created by the t-SNE reveals the neighborhood structure of the sample manifold at many different scales [49] . This is particularly important for our application, in which the high-dimensional neuroimaging data lie on several different lowdimensional manifolds since the samples come from different subjects with or without the neurodegenerative disease. Fig. 3 shows the t-SNE projection in the 2-D space. In this figure, the samples, which received the smallest weights in their respective elements in the\u00e2 a weight matrix (as in Eq. (5)), are shown in the top part of the figure. We also depict the samples detected as outliers using the RANSAC [50] algorithm in the bottom part of the figure. Notably, as it is obvious in the figure, the samples detected as sample-outliers by our algorithm are those which are more controversial for the task of classification and lie outside the main neighborhood of each class. This is attributed to the fact that we detect them jointly with the classifier learning framework. On the other hand, the outliers detected by RANSAC are not always the best in terms of discriminability. This suggests that unsupervised outlier detection methods might not perform well when the aim is to learn a classifier or a regression model. In other words, in many learning tasks, the definition for sample-outliers might be different based on what the goal is.\nOne of the important hyperparameters in the proposed RFS-LDA is 1 , as in Eq. (5), which controls the noise term. Modifying this hyperparameter leads to altered noise levels, detected by our algorithm. To analyze its effect on the learning performance, we fix all other hyperparameters and run the algorithm with different values of L 1 , and therefore 1 (as discussed at the beginning of Section 3). The changes in the AUC for each of our experiments are illustrated in Fig. 4 . As can be seen, the proposed method achieves reasonably good results with a wide range of the values of the hyperparameter.\nIt is worth noting that the proposed method works under a semi-supervised setting, which is interesting for the application of disease diagnosis. When performing the diagnosis for new patients, all subjects whose clinical diagnosis has not been finalized (i.e., they are still in the process of evaluations and clinical monitoring) can yet be included in model building as unlabeled samples, to build a potentially more reliable classifier."}, {"section_title": "CONCLUSION", "text": "In this paper, we proposed a novel approach for discriminative classification, which is robust against both sample-outliers and featurenoises. Our method enjoys a semi-supervised setting, where all the labeled training and the unlabeled testing data are used to detect outliers and are denoised simultaneously. We have applied our method to several datasets, including synthetic, semi-supervised learning benchmark, and neurodegenerative brain disease diagnosis datasets, specifically for Parkinson's disease and Alzheimer's disease. The results showed that our method outperformed all competing techniques. As a direction for the future works, one can develop a multi-task learning reformulation of the proposed method to incorporate diagnosis from multiple modalities of neuroimaging data or extend the approach for the case of incomplete data."}, {"section_title": "ACKNOWLEDGMENTS", "text": ""}]