[{"section_title": "Introduction", "text": "Neuroimaging has opened up an exciting non-invasive window into the human brain over the past few decades. This interdisciplinary field has attracted scientists from areas such as medicine, engineering, mathematics, physics, statistics, computer science, and psychology (Epstein et al., 2001) . Imaging modalities such as magnetic resonance imaging (MRI) and magnetoencephalography (MEG) along with more traditional methods such as electroencephalography (EEG) have made it possible to non-invasively study various aspects of the human brain with unprecedented accuracy. MRI-related techniques such as structural MRI (sMRI), functional MRI (fMRI) and diffusion MRI (dMRI) have the benefit of providing localized spatial information about the brain structure and function as well as detailed functional and structural connectivity maps. These techniques have provided new insight into the human brain and have brought hope to researchers trying to unravel the secrets of one of the most complex systems in the universe, the human brain. Structural MRI has made it possible to visualize the brain at high spatial resolution (one cubic millimeter or less) (Liang and Lauterbur, 2000) . sMRI high resolution images of the brain are ideal for studying various brain structures and also for detecting physical abnormalities, lesions and damages. dMRI is an imaging technique for visualization of anatomical connections between different brain regions (Le Bihan et al., 2001; Merboldt et al., 1985) . Functional MRI measures brain activity by detecting changes in the blood oxygenation (DeYoe et al., 1994; Ogawa et al., 1990) . fMRI makes it possible to study functional regions and networks of the brain as well as temporal associations among them.\nUnfortunately, brain disorders are major health problems in the US and the rest of the world that not only impair the lives of millions of people but also impose huge financial burdens on societies (DiLuca and Olesen, 2014; Ernst and Hay, 1994; Rice, 1999) . Moreover, there are no clinical tests to identify many brain disorders such as schizophrenia. One of the major hopes underlying the advanced neuroimaging tools mentioned above is to provide new understanding of brain disorders such as schizophrenia, bipolar disorder, autism spectrum disorder (ASD), Alzheimer's disease (AD), major depressive disorders, attention-deficit hyperactivity disorder (ADHD) and mild cognitive impairment (MCI). Brain disorder research aims at understanding the impact of each disease on the brain's function and structure from the cellular to system level, as well as the pathogenesis of these complex disorders. As a result, thousands of studies have been published on different aspects of brain disorders to show aberrations of some features (structural or functional) in a patient group usually in comparison with a healthy cohort (Jack et al., 1997; Jafri et al., 2008; Lorenzetti et al., 2009; McAlonan et al., 2005) . While these studies are valuable in terms of finding relevant disease biomarkers, they are not sufficient for direct clinical diagnostic/prognostic adoption. The main reason is that many of these findings are statistically significant at the group level, but the individual discrimination ability of the proposed biomarkers is not typically evaluated. Since classification provides information for each individual subject, it is considered a much harder task than reporting group differences.\nIn recent years, there has been a growing trend in designing neuroimaging-based prognostic/diagnostic tools. As a result, there have been a lot of efforts using neuroimaging methods to automatically discriminate patients with brain disorders from healthy control or from each other (Kl\u00f6ppel et al., 2012) . Many of these studies have reported promising prediction performances with the claim that complex diseases can be diagnosed robustly, accurately and rapidly in an automatic fashion. However, until now, these tools have not been integrated into the clinical realm. We believe that the main reason for this is that many of the studies of this nature, despite the promising results on a specific research dataset, are not designed to generalize to other datasets, specifically the clinical ones.\nThe purpose of this study was two-fold. First, we reviewed a large number of MRI-based brain disorder diagnostic/prognostic studies in schizophrenia, ASD, ADHD, depressive disorder, MCI and Alzheimer's disease. These studies are compared in a number of key aspects such as type of features, classifier and reported accuracies. Next, we formed our opinion on the issues associated with how machine learning is applied in neuroimaging and have suggested solutions that might address these pitfalls. Considering the immense potential of neuroimaging tools for clinical adoption, careful implementation and interpretation of machine learning in neuroimaging is crucial. Machine learning is a relatively new domain for many neuroimaging researchers coming from other fields and therefore pitfalls are unfortunately not rare. We attempt to identify and emphasize some common mistakes that resulted in these shortcomings and biases. At the end, we discuss emerging trends in neuroimaging such as data sharing, multimodal brain imaging and differential diagnosis."}, {"section_title": "Group difference vs. classification", "text": "As pointed out in the Introduction section, many brain disorder studies have shown abnormality in the average sense in one or more brain features in a patient cohort in comparison with a healthy group using statistical tests. The success of such methodology is usually measured by the means of p-values. On the other hand, the goal of single subject prediction is to automatically classify each subject into one of the groups in the study (e.g., healthy vs. patient). The success of classification studies is usually measured by accuracy.\nThese two problems are very different in essence as they try to address distinct research questions. In general, showing group differences is much easier compared to single subject prediction. To better illustrate the difference between these types of analysis, we show an example in Fig. 1 . Suppose there are two groups each with 100 samples (subjects) and we have measurements of one brain feature for each subject. Fig. 1A shows a case where the mean values of the two groups are different as measured by a two-sample t-test. The difference is statistically significant (p-value = 0.001). However, if one tries to classify subjects based on a threshold on this brain feature (the dotted red line placed between the mean of two groups), a weak classification rate of 60.0% will be achieved. The reason for this is the range of values for that specific feature is highly overlapping for the two groups. So, a highly significant group difference does not necessarily translate into a strong classification result. But the opposite is also true, as high classification based on a feature doesn't necessarily mean that group-level mean differences exist. Fig. 1B shows a case where the two-sample t-test on the two groups is not significant (p-value = 0.86) but the classification based on two thresholds (red dotted lines placed between each mode of group 2 and mean of group 1) is very strong (94.5%). In this case, the abnormality is bidirectional, which does not cause significant mean differences but makes it possible to separate the groups with two thresholds (dotted lines). Interestingly, bidirectional abnormalities are observed in neuroimaging studies (Arbabshirani and Calhoun, 2011; Calhoun et al., 2006b ). Fig. 1C shows a case where strong group differences and successful classification go hand in hand. The abnormality is one-directional and the mean difference is very significant (p-value b 2e\u221216). The mean of two groups is so far apart that the values of most of the samples of the two groups do not overlap. Therefore, a strong classification rate of 93.5% is achieved (based on one threshold).\nA B C Fig. 1 . Comparison of group difference analysis and classification in three different scenarios using toy data. Group difference is analyzed by two-sample t-tests and classification is performed by simple thresholding (red dotted lines). Each group/class has 100 samples. A: Significant group difference (p-value b 0.001) but poor classification (60.0%). B: Insignificant group difference (p-value = 0.865) but high classification accuracy (94.5%). C: Significant group difference (p-value b 2e\u221216) and high classification accuracy (93.0%).\nSignificant group difference doesn't necessarily cause high classification and vice versa.\nThe main purpose of example in Fig. 1 is to show that group level analysis and classification are two different methods for different problems. We will return to this example later for criticism of selecting features based on p-value."}, {"section_title": "Survey of MRI-based single-subject prediction of brain disorders", "text": "Based on a search on PubMed from 1990 to 2015, 1 more than 500 papers on MRI-based single subject prediction of brain disorders were found. Fig. 2 summarizes the paper selection procedure for this study. More than 200 papers were eventually selected for this survey (112 AD/MCI, 64 schizophrenia, 19 depressive disorders, 20 ASD and 22 ADHD papers). We limited our search to journal papers in English published up to December 2015. In a few instances, the full paper was not found and therefore those studies were excluded from this survey. Also, in cases of very similar papers from the same authors, only one was selected. Key aspects of each study such as modality, machine learning method, sample size and extracted features were investigated. A list of all abbreviations used in the tables and the manuscript itself is provided in Table 1 .\nMild cognitive impairment/Alzheimer's disease MCI entails cognitive decline more than what is expected for an individual's age and education level, but not to the extent that it interferes notably with activities of daily life . Unfortunately, more than 50% of the MCI patients progress to dementia within 5 years (Gauthier et al., 2006) . So, it is considered a prodromal phase to dementia especially the AD type (Gauthier et al., 2006) . The heterogeneous etiology of MCI includes degenerative diseases (AD, fronto-temporal lobe degeneration, dementia with Lewy bodies) as well as vascular and psychiatric disorders (Petersen and Negash, 2008) . AD is the most common neurodegenerative disorder, which is increasingly prevalent among adults aged 65 years and older. AD is characterized by the progressive impairment of neurons and their connections, which result in decline and loss of cognitive functions. In 2007, it was estimated that more than 26 million people suffer from AD worldwide (Brookmeyer et al., 2007) . In 2001 it was predicted that AD will triple in prevalence by 2050 (Hebert et al., 2001) . The detection of AD is based on clinical examinations and an evaluation of the patient's perception and behavior. Considering the prevalence and severity of MCI/AD, the largest number of neuroimaging-based, automatic prediction/classification publications has been devoted to these conditions. Also, a number of these studies, investigated the possibility of automatic classification of stable MCI (sMCI) from progressive MCI (pMCI) using neuroimaging data (Eskildsen et al., 2013) . Table 2 summarizes the 112 studies that we reviewed in this survey including 16 studies that also reported sMCI versus pMCI classification results."}, {"section_title": "Schizophrenia", "text": "Schizophrenia is among the most prevalent mental disorders and affects about 1% of the population worldwide (Bhugra, 2005) . This devastating, chronic heterogeneous disease is usually characterized by disintegration in perception of reality, cognitive problems, and a chronic course with lasting impairment (Heinrichs and Zakzanis, 1998) . Considering the absence of standard clinical test for schizophrenia, there is a growing interest in automatic diagnosis of schizophrenia based on neuroimaging features. We surveyed 64 papers, which are tabulated in Table 3 ."}, {"section_title": "Depressive disorders", "text": "Major depressive disorder (MDD) or unipolar depression characterized by a pervasive low mood, self-esteem and lack of interest in enjoyable activities is a common mental illness affecting adolescents. The lifetime prevalence of MDD is approximately 15-20% (Kessler et al., 2003; Lewinsohn et al., 1986) . It is estimated that by the year 2020, depression will account for 15% of the disease burden in the world ranking second after heart disease (Kessler et al., 1994) . We reviewed 19 studies that used neuroimaging for automatic diagnose MDD. Those studies are listed in Table 4 ."}, {"section_title": "Autism spectrum disorder", "text": "Autism spectrum disorder (ASD) is a serious neurodevelopmental condition characterized by impaired social communication, deficits in social-emotional reciprocity, deficits in nonverbal communicative behaviors used for social interaction and stereotypic behavior (Association et al., 2003) . Although the causation of autism is still largely unknown, it has been suggested that genetic, developmental, and environmental factors could be involved alone or in combination as possible causal or predisposing effects toward developing autism (Minshew and Payton, 1988; Wing, 1997) . ASD has an estimated prevalence of 1:68 in the US (Baio, 2012) . We surveyed 20 papers in automatic diagnosis of ASD using MRI-based features. Those studies are listed in Table 5 ."}, {"section_title": "Attention deficit hyperactivity disorder", "text": "Attention deficit hyperactivity disorder (ADHD) is one of the most commonly found functional disorders affecting children. Approximately 3-10% of school aged children are diagnosed with ADHD (Biederman, 2005; Dey et al., 2012) . Currently, no biological-based measure exists to detect ADHD and instead, behavioral symptoms are investigated to identify it. Despite all the research efforts, the root cause of ADHD is still unknown. In 2011, a global competition (called ADHD-200) was held in order to use neuroimaging as well as phonotypic measures to automatically detect ADHD (Consortium et al., 2012) . Most of the studies reviewed in this survey were responses to that challenge. The main characteristics of those studies are tabulated in Table 6 ."}, {"section_title": "Analysis of the survey", "text": "In Fig. 3 we illustrate a couple of key aspects of this survey. Fig. 3A shows the number of papers published in each year for each disease type. The number of studies has been growing significantly since 2007. There is a peak for ADHD studies in 2012-2013 mainly due to ADHD-200 competition (Consortium et al., 2012) which attracted many scientists. The total number of studies for each modality and each disorder is illustrated in Fig. 3B . It is clear that structural MRI is the most popular modality especially for MCI/AD studies thanks to Alzheimer's disease neuroimaging initiative (ADNI) dataset. Combined rest and task fMRI studies are most popular for ADHD and schizophrenia studies. Surprisingly, multimodal studies are more common compared to either task fMRI or diffusion MRI studies. Fig. 3C shows the overall accuracy against the total sample size used in the studies. Interestingly, almost all studies that reported very high accuracies, had sample sizes smaller than 100. The reported overall accuracy decreases with sample size in most of disorders such as schizophrenia and ADHD. This pattern raises a serious concern regarding generalizability of many of those studies with small sample sizes. Fig. 3D shows the sample size distribution. The dashed lines represent mean (red) and median (blue) sizes, which are 186 and 88 respectively. Finally, Fig. 3E illustrate the distribution and summary statistics of reported overall accuracy for each disorder along with the distribution of accuracies reported for classification of pMCI from sMCI (16 studies). On average (red diamonds) MCI/AD studies (usually against a healthy group) reported the highest accuracies and sMIC versus pMCI studies reported the lowest accuracies.\nBased on Tables 2-6, the most common extracted features in the surveyed studies are volume and cortical thickness from structural MRI, the activation maps and functional connectivity among ROIs or ICA components from fMRI data and fractional anisotropy from dMRI data. Most common feature reduction methods (not reported in the tables) were based on PCA or univariate statistical tests.\nIn terms of classification methods, support vector machine (SVM) was by far the most popular method. Different flavors of SVM such as linear, non-linear with different kernel, SVM with recursive feature elimination, SVM with L1 regularization and SVM with L1 and L2 regularization (elastic net) have been used for classification of various (2013b) disorders. Linear discriminant analysis (under different names) and logistic regression were also popular classification methods among the surveyed studies."}, {"section_title": "Predicting continuous measures", "text": "Most of the studies surveyed above, conducted the diagnosis of a disorder (i.e., assigning a categorical label to each subject) using classification techniques. Pattern regression considers the problem of estimating continuous rather than categorical variables, which can be more challenging as compared to classification. Clinically, pattern regression can be used to estimate the disease stage and progression. Therefore, there is a growing interest in estimating continuous variables such as cognitive scores for brain disorders using neuroimaging measurements. We didn't survey those papers, but we will point out to some of those studies in this section.\nWang et al. proposed a general methodology for estimating continuous clinical variables from high-dimensional imaging data . Sato et al. used interregional cortical thickness measurements to estimate Autism Diagnostic Observation Schedule (ADOS) score in ASD patients . Stonnington et al. used relevance vector regression (RVR) to predict number of cognitive scores such as Dementia Rating Scale (DRS) and Alzheimer's Disease Assessment Scale (ADAS) based on structural MRI measures (Stonnington et al., 2010) . Tognin et al. used RVR to predict Positive and Negative Syndrome Scale (PANSS) scores of subjects at high risk of psychosis based on gray matter volume and cortical thickness measurements (Tognin et al., 2013) . Yue et al. showed relationship between functional connectivity and neuropsychological assessment scores such as Rey-Osterrieth Complex Figure Test (CFT) in amnestic MCI patients (Yue et al., 2015) . Zhang et al. used MRI, PET and CSF data to predict Mini Mental State Examination (MMSE) and ADAS scores in MCI and AD patients (Zhang and Shen, 2012a) .\nDetecting/characterizing at risk healthy subjects\nThe majority of studies surveyed above tried to automatically diagnose one or more disorders in patients. However, detecting or characterizing healthy individuals who are at high risk of brain disorders could potentially delay or prevent future symptoms. There has been a lot of such studies using genetics information but detecting or characterizing at risk subjects based on neuroimaging data is rare. Mour\u00e3o-Miranda et al. used functional MRI to detect subjects at high risk of mood disorders . Guo et al. characterized activity of default-mode network in unaffected siblings of schizophrenia patients using resting-state functional data (Guo et al., 2014b) . In another study, Fan et al. studied structural endophenotypes in unaffected family members of schizophrenia patients using machine learning methods (Fan et al., 2008a) ."}, {"section_title": "Semi-supervised studies", "text": "The main focus of this work is on supervised studies. However, in the cases that collecting data is difficult and costly, semi-supervised learning might help. This method uses both labeled and unlabeled data for building the predictive model. For example, predicting MCI patients that convert to AD (sMCI vs. pMCI) requires following up with the subjects for a long time which makes the data collection cumbersome. One remedy is to use labeled AD and healthy data to build a model for predicting labels for MCI subjects. A few studies have utilized this strategy and achieved accuracies comparable or even better than those obtained from fully supervised learning (Batmanghelich et al., 2012; Filipovych et al., 2011; Moradi et al., 2015; Zhang and Shen, 2012b) ."}, {"section_title": "Common machine-learning pitfalls in neuroimaging", "text": "In this section, common pitfalls among the surveyed papers are discussed."}, {"section_title": "Feature selection bias", "text": "Most of the papers we surveyed consisted of two consecutive parts: group difference analysis and classification. Usually, statistical tests such as t-tests are used to show group differences on a set of extracted features in the first part of the study, which is followed by a classification approach to assess the discrimination ability of those features on a single subject basis. Unfortunately, it is not rare to see that the results of first part (group differences) are used to select features for the classification part. In general, any use of test samples in any part of the training (such as feature extraction, feature selection and classifier training) poses a bias. Selecting features for classification based on the results of group tests that were conducted on the whole dataset is a form of double dipping and therefore leads to a biased (inflated) result (Bishop, 2006; Demirci et al., 2008b) .\nThis form of feature selection also has another major problem. The significance of group statistical tests, which are the basis of feature selection in some of the studies, is mostly based on p-values. However, the relationship between p-value and discrimination power is not straightforward. Fig. 1 shows the p-value of a two-sample t-test as well as overall accuracy based on one or two thresholds in three different scenarios. It is seen that low p-value doesn't necessary mean a strong feature (Fig. 1A) and high p-value doesn't mean a weak feature (Fig. 1B) . However, if the abnormality is one-directional, then a very low p-value might translate to high classification accuracy (Fig. 1C) . So, by discarding features just based on the result of statistical tests sensitive to group mean, valuable discriminatory information could be lost. Instead of feature selection based on univariate group-level statistical tests, more common filtering and wrapper methods should be used (Blum and Langley, 1997; Hall and Smith, 1998; Kohavi and John, 1997) . Filtering methods assign scores to each feature from which a number of top ones can be selected. A good filtering method should be sensitive to the discriminative power of the features. Most of these methods are univariate and therefore each feature is treated independently from other features. Filtering methods have the advantage of low computational cost, but their main drawback is ignoring the relationship among features.\nWrapper methods, on the other hand, consider selection of a set of feature as a search problem. Different combinations are evaluated and finally the best set of features is selected. A popular wrapper method is the recursive feature elimination (RFE) algorithm (Guyon et al., 2002) . Wrapper methods are computationally much more expensive than filtering methods, but can result in superior performance by considering interaction among features.\nThere are methods that aim at combining both filtering and wrapper methods. Minimum-redundancy maximum relevancy (mRMR) is one the methods popular for genetic feature selection. MRMR tries to select features with maximum mutual information with class labels while (2012) minimizing the mutual information among those features (Brown et al., 2012a) . Finally, there are embedded feature selection methods (Guyon and Elisseeff, 2003) . These methods combine classification and feature selection into one unified step. Embedded methods learn the features that contribute the most to the accuracy of the model during the training phase. One of the common categories of the embedded methods is using regularization to enforce the learning algorithm to find more parsimonious models with lower complexity and therefore with fewer parameters. A post training analysis of the model coefficients, determines the selected features. Examples of regularization algorithms used in embedded feature selection methods are LASSO, elastic net and ridge regression (Hastie et al., 2004; Ng, 2004; Park and Hastie, 2007; Zou and Hastie, 2005) ."}, {"section_title": "Overfitting", "text": "Overfitting happens when a model describes noise in the data rather than the underlying pattern of interest. Overfitting results in very good performance on the observed data and very poor performance on unseen data. Using models that are very complex or have many parameters on datasets with small number of samples and large number of features are more susceptible to overfitting. Neuroimaging datasets have limited number of samples and millions of voxels per sample. Based on Fig. 3D , the majority of surveyed studies built predictive models based on a very small number of subjects. It is evident from Fig. 3C that overall reported accuracy decreases with sample size in our survey. Therefore, it is plausible that many surveyed studies suffer from overfitting problem. It should be noted that by definition, overfitted models work well on the training data and poor on the test data. However, if the process of training and testing is repeated (by varying the model parameters) until a desirable performance on the test data is achieved, the model will likely overfit both the train and test datasets and will not generalize to an unseen dataset. Cross validation and regularization are common methods to control for overfitting. As mentioned earlier, more complex models have a greater chance of overfitting the data. For example, non-linear SVM is more powerful compared to linear SVM but has more hyperparameters and therefore Table 2 -6. A: Total number of papers for two-year intervals for each modality. The legend shows the color code for each disorder. This legend also applies to parts B and C. B: Number of publications per modality for each disorder C: Scatter plot of overall reported accuracy versus the total sample size. D: Histogram of the sample sizes of the surveyed studies. Vertical dashed lines indicate mean (red) and median (blue) sample size among all studies, which are 186 and 88 respectively. E: Disorder specific violin plots of reported overall accuracies of the surveyed papers with embedded boxplots and data points (each black dot corresponds to a study). This part also includes the accuracies of sMCI vs. pMCI studies. Red diamonds indicate the mean accuracy.\nis also potentially more capable of explaining noise in the data. As discussed in the previous section, proper feature selection can also help avoiding overfitting."}, {"section_title": "Reporting classification results", "text": "The result of classification is basically a confusion table/matrix also known as a contingency table. The confusion matrix summarizes the results in a table layout where each column represents the predicted class and each row represents the actual class. Confusion matrix is m \u00d7 m where m represents the number of classes. In the case of binary classification, several statistical measures can be computed from the 2 \u00d7 2 confusion matrix, such as sensitivity (or recall), specificity, positive predictive value (or precision), negative predictive value, F1 score, odds ratio, kappa and false negative rate. Confusion matrix and some of the performance measures are shown in Fig. 4 . In order to understand the performance of a classifier, it is important to report at least sensitivity/ specificity or precision/recall along with the overall accuracy. We highly encourage reporting the confusion matrix itself as well. Some of the studies in this review just reported the overall accuracy, which can be very uninformative especially when classes have unequal sample sizes (Alberg et al., 2004) . Suppose there are 20 patients and 80 controls in a test dataset. Reporting 80% accuracy is completely uninformative since the classification of all subjects as healthy could also result in 80% (one of the scenarios). This problem is easily detectable by looking at the confusion matrix or sensitivity and specificity measures. In unbalanced sample size cases, balanced specificity and sensitivity is more desirable than higher overall accuracy; therefore, measures such as F1 score (harmonic mean of precision and recall) are preferred for evaluating the classifier. The other very common way of reporting results for a binary classifier is by plotting \"receiver operating characteristic\" (ROC) curve. The ROC curve is the plot of sensitivity against \"1-specificity\" by changing the discrimination threshold and therefore provides a complete picture of classifier's performance. The ROC curve is usually summarized by the area under the curve (AUC), which is a number between 0 and 1.\nThe other common reporting issue is unjustified comparison of the achieved overall accuracy with the random chance. This issue is critical in this field due to small sample sizes. For example, an 80% achieved overall accuracy might not be significantly different from a 50% random chance in a statistical sense in a two-class problem if the sample size is too small. Any achieved accuracy in a test sample is just one estimate of the population accuracy. Like any other statistics, a confidence interval can be computed for that measure. In the case of a two-class problem, a binomial confidence interval can be computed for overall accuracy that serves as the basis for comparison with random chance, or any other accuracy. In our example (80% accuracy), if the test sample size is 10, then the 95% exact binomial confidence interval would be [0.444 0.975], which includes the random chance probability (0.50) and therefore is not statistically above chance (significance level of 0.05). Calculating this interval is straightforward using most of statistical and technical computing software such as R and Matlab. This approach should be employed when repeating the classification experiment for number of times is not feasible. However, in most cases, the null distribution of chance is empirically computable by randomly assigning labels to test samples and repeating classification for a number of times. This method, known as a permutation or randomization test, makes it possible to calculate the desired confidence interval of the chance, which consequently could be compared against the achieved classification accuracy using the correct labels (Collingridge, 2013; Fisher et al., 1960; Good, 2006; Mehta et al., 1988) . Recently, for special cases such as SVM, fast analytical estimation of permutation testing has been proposed (Gaonkar and Davatzikos, 2013) . Also, it has been shown that p-value for permutation testing can be written in the form of an infinite series whose terms are efficiently computable (Gill, 2007) ."}, {"section_title": "Comparison of accuracies across studies", "text": "It was frequently observed that authors claim that their proposed classification framework outperformed some other studies (and sometimes all other studies) just on the basis of overall accuracy. Considering the number of variables in each study-such as sample size, scanner parameters, sample age distribution, patients' status (e.g., severity, medication), modality, length, type and design of study (for fMRI studies), preprocessing parameters, number and type of extracted features and type of classifier-such a comparison is essentially meaningless. Even in the case of standard neuroimaging datasets, the statistical comparison discussed in the previous section, should be employed to compare the results."}, {"section_title": "Hyperparameter optimization", "text": "Hyperparameter optimization or model selection is choosing a set of parameters for the learning algorithm in order to maximize the performance of the algorithm. Hyperparameters should be chosen during training, usually via an inner loop cross validation inside the training data. SVM, which is one of the most popular classifiers in this review and in neuroimaging in general (Orr\u00f9 et al., 2012) , has at least one hyperparameter (linear SVM) called soft margin. In addition to soft margin, non-linear SVM has one or more hyperparameters depending on the kernel (e.g. sigma/gamma for RBF kernel and degree for polynomial kernel). Some of the studies that we reviewed just used the default values for these parameters. A lack of parameter optimization can degrade the classification performance significantly. To show this, a toy example is illustrated in Fig. 5 . SVM with three different kernels is used to classify this simulated two-class problem. In the top row, 1.0 is chosen for soft margin hyperparameter (which is the default of most machine learning software packages) for all kernels, degree of 3 was chosen for the polynomial kernel and gamma of 0.01 was selected for RBF kernel. In the second row, the hyperparameters are optimized. First, it is evident that the linear kernel failed to learn the non-linear pattern under both settings. Increasing the polynomial kernel degree by one, dramatically improved the classifier. Also, increased soft margin value, significantly improved SVM with RBF kernel. So, both the choice of kernel and hyperparameters are crucial for building a successful SVM-based classifier. SVM hyperparameters are usually selected based on a grid search over plausible values."}, {"section_title": "Machine learning in neuroimaging: shortcomings and emerging trends", "text": "Machine learning has more than two decades of history in neuroimaging and despite all of the promising results of numerous studies, it is still immature and not ready for integration into clinical healthcare. In this section, we review some of the challenges and emerging solutions."}, {"section_title": "Sample size in neuroimaging studies", "text": "The most limiting factor in this field is by far the limited sample size issue. As summarized in Fig. 3B , the majority of studies in this review and in general have sample size of less than 150. This sample size is miniscule in comparison with other fields in which machine learning is used. As an example, ImageNet, 2 which is commonly used as standard computer vision dataset, has over one million samples and 1000 classes. As a result of such big datasets, dramatic improvement has been achieved in the field of computer vision in the past few years. However, sample size limitations in neuroimaging pose several problems. First, the classifier performance is directly affected by the sample size. It is shown that large training data sets increase classification accuracy (Franke et al., 2010; Kl\u00f6ppel et al., 2009) . Small sample size does not represent the patient population and therefore promising results may not generalize to other patient groups. In a study conducted by Nieuwenhuis et al., it was shown that for small training sample sizes (N b 130) the predictive model for classification of schizophrenia patients based on sMRI was not stable (Nieuwenhuis et al., 2012) . More than 63% of the studies we reviewed didn't meet this criterion. Large datasets may reduce problems with disease heterogeneity as they can represent the whole spectrum of the disorder. Although there are some machine learning methods that are less sensitive to data, a limited number of data samples can cause model overfitting, resulting in poor generalization of the method to independent data sets (Pereira et al., 2009 ).\nTo understand the etiology of complex conditions such as mental health, we must develop a better understanding of the structure of the signals and measurements we make of the brain. Thanks to advances in imaging and assaying technology, we can gather increasingly detailed information about individuals, but the cost and complexity of these techniques means that individual researchers may not have sufficient data to build a compact and informative representation of the data. For example, a single sMRI may have tens of thousands of voxels, but a single site may have only a hundred subjects in their study. With increasingly complex data, the classical \"curse of dimensionality\" would seem to indicate that there is no way to determine signal from noise in this setting. To address the \"small N\" problem in other settings, many researchers have proposed open sharing of data to leverage data from multiple sites as well as commercial cloud computing infrastructures to handle the additional computational burden. In the past few years, several multi-site data sharing initiatives such as FBIRN, MCIC and COBRE for schizophrenia, ADNI for Alzheimer's disease, ABIDE for ASD, ADHD-200 for children with ADHD and Functional Connectomes project for healthy have been started.\nIn neuroscience, measurements often come from human subjects; in some cases legal, ethical, and sociological concerns may preclude or prohibit such open sharing. In particular, local administrative rules, concerns about re-identification of study participants, and a desire to maintain control over data in ongoing research projects may prevent 2 http://image-net.org/. individual research sites from sharing the data (Sarwate et al., 2014) . The status quo is a patchwork of institution-to-institution data use agreements whose complexity stymies automated analyses across more than a handful of data sets."}, {"section_title": "Operating on decentralized data", "text": "We believe that a more convenient and scalable solution will come from design and implementation of algorithms which learn from data distributed across research groups. These algorithms shall include feature learning as well as classification, prediction and inference. Dropping the requirement of moving the data, these algorithms will better match the current decentralized and efficient organization of research society and substantially lower barriers to entry for collaborative work. The resulting network effect will enable new innovative opportunities for research that we cannot envision today. The need for such approaches to general data computation is realized by some researchers (Bai et al., 2005) but not yet fully appreciated by the neuroimaging field. The field is currently in the state of establishing central repositories of anonymized raw data (Bockholt et al., 2009; Buccigrossi et al., 2007; Di Martino et al., 2014; Jack et al., 2008; Keator et al., 2008; Landis et al., 2016; Marcus et al., 2007; Poldrack et al., 2013; Scott et al., 2011; Van Essen et al., 2013) . In the past 10 years, release of multi-site neuroimaging datasets such as: FBIRN, MCIC for schizophrenia (Ford et al., 2009; Gollub et al., 2013) , ADNI for Alzheimer's disease , ABIDE for ASD (Di Martino et al., 2014) , ADHD-200 for children with ADHD (Consortium et al., 2012) and Functional Connectomes project for healthy subjects (Biswal et al., 2010) have been started.\nCertainly, access to raw data is the best way to drill down to the finest details and resolve any inconsistencies due to data handling. However, even in the centralized repositories, it is often more convenient to start analysis from a point in the processing pipeline where less detailed but possibly more informative features are generated. Furthermore, there are three categories of data that pose challenges for public availability for easy access: (1) data that are non-shareable due to obvious re-identification concerns, such as extreme age of the subject or a zip code/disease combination that makes re-identification simple; (2) data that are non-shareable due to more complicated or less obvious concerns, such as genetic data or other data which may be re-identifiable in conjunction with other data not under the investigator's control; and (3) data that are non-shareable due to the local institutional review boards (IRBs) rules or other administrative decisions (e.g., stakeholders in the data collection not allowing sharing). For example, even with broad consent to share the data acquired at the time of data collection, some of the eMERGE sites were required to re-contact the subjects and re-consent prior to sharing within the eMERGE consortium, which can be a permanent show-stopper for some datasets (Ludman et al., 2010 ). An extensive account of the problems that go along with these concerns is given by Sarwate et al. (2014) . An example of how a decentralized data feature learning algorithm could use decentralized data joint ICA in given by Baker et al. (2015) . In short, the algorithm performs a joint ICA on datasets distributed across research sites which enables one to perform temporal ICA on fMRI data as an increasingly large data sample becomes available when many research groups join the collaboration. Importantly, Baker et al. have demonstrated (on synthetic data) that with their approach the estimated components are virtually identical for the pooled data (i.e. a central repository), two sites with data split in half, multiple sites with data evenly split across, and even a very large number of sites with very few subjects at each of them. Once globally consistent features are available they may be used in building classification algorithms.\nNevertheless, decentralized data computation under serious privacy concerns will need additional protection besides simple protection from only sharing summaries and not the raw data samples. A solution for this setting has been offered in the \u03b5-differential privacy model and explained extensively in the neuroimaging context with published examples (Dwork, 2006; Sarwate et al., 2014) . This approach defines privacy by quantifying the change in the risk of re-identification as a result of publishing a function of the data. Notably, privacy is a property of an algorithm operating on the data, rather than a property of the sanitized data, which reflects the difference between semantic and syntactic privacy. Importantly for our applications, it can be applied to systems which do not share data itself but instead share data derivatives (functions of the data). Algorithms that guarantee differential privacy are randomized in how they manipulate the data values (e.g., by adding noise) to bound the risk. Enabling individual subject prediction in the classification framework is one of the applications where the abovedescribed approaches can provide the most benefit-especially for rare conditions that are easy to identify by cross referencing when raw data is openly shared and hard to collect enough data at a single site to provide high generalization. The former is perfectly addressed by applying \u03b5-differential privacy approach to classification (Chaudhuri et al., 2011) , while the latter can be addressed by running decentralized algorithms over multiple sites. As mentioned already, differentially private algorithms provide guarantees by necessarily lowering the quality of the solution due to the required noise addition. The same happens to differentially private classifiers (Chaudhuri et al., 2011) and the effect is an undesirable increase in prediction error (Sarwate et al., 2014) . Fortunately, combining the approaches (differential privacy and decentralized algorithms) can improve the situation considerably by dropping classification error from 25% to 5% while preserving all privacy guarantees (Sarwate et al., 2014) .\nIn these \"big data\" times, the need for computation on large-scale datasets creates the best climate for software for distributed computation. Many useful and powerful projects came to the scene such as Apache Spark (Zaharia et al., 2010) and H2O (H2O [WWW Document], 2015) . On closer inspection, these implementations are essentially striving for the efficiency of computation given a big data overload (typically easy to get data stored centrally). They suggest optimization toward an environment that is quite orthogonal to what we have to deal with-hard to get to and expensive to collect data spread across research labs around the nation and the world. The goal of decentralized approaches that we are describing here stands principally as preserving correctness of the computation while minimizing the data passed around and reducing the number of iterations. The tools and methods are not conflicting and decentralized data algorithms can and shall take advantage of what is being developed for large-scale computation in the distributed computing community."}, {"section_title": "Differential diagnosis and disease subtype classification", "text": "Using machine learning methods, promising results have been reported for automatic diagnosis of various cognitive and neurodegenerative disorders, usually from healthy controls based on neuroimaging features. However, one of the main challenges in psychiatric and neurology diagnoses is to differentially diagnose a disorder that shares symptoms with multiple other disorders. Examples of such overlapping disorders are schizophrenia, bipolar, schizoaffective, unipolar and mood disorders. Except for differentiating MCI for AD, only a few considered much needed automatic differential diagnosis in the studies we surveyed. Costafreda et al. used fMRI with a verbal fluency task to classify schizophrenia, bipolar and healthy controls (Costafreda et al., 2011b) . Calhoun et al., and Arribas et al. both used fMRI with an auditory oddball task and an ICA approach to extract features from the default model network and the temporal lobe of the brain (Arribas et al., 2010; Calhoun et al., 2008) . Both of these studies reported high differential accuracy between schizophrenia and bipolar disorder. Pardo et al. used a combination of volumes of 23 ROIs derived from structural MRI along with 22 neurophysiological test scores to automatically classify schizophrenia, bipolar and healthy controls (Pardo et al., 2006) . Recently, Schnack et al. proposed using gray matter densities for classification schizophrenia, bipolar and healthy controls (Schnack et al., 2014) . Koutsouleris et al., used gray matter maps from structural MRI to classify schizophrenia from mood disorder . Ota et al. combined volumetric measures derived from structural MRI with fractional anisotropy from dMRI in selected ROIs to classify schizophrenia from MDD (Ota et al., 2013) . Sacchet et al. proposed using gray matter volumes of caudate and ventral diencephalon to differentiate MDD, bipolar and remitted MDD patients .\nPathologies like autism and schizophrenia are spectrum disorders with multiple etiologies under the umbrella of the same diagnostic category. While classification of these disorders using the generic category is commonly used to find diagnostic biomarkers, one of the key issues in mental healthcare is the differential diagnosis of patients across several disease subtypes. Common binary patient-control classification ignores the underlying heterogeneity of the disorder. Usually, the treatment path used for these subtypes differs from each other and therefore the correct subtype diagnosis is very important. For example, several cognitive deficits are observed in schizophrenia patients, but the magnitudes of such symptoms are highly variable among the patients. To reduce this phenotypic heterogeneity two major subtypes named \"cognitive deficit\" and \"cognitively spared\" have been defined (Green et al., 2013; Jablensky, 2006) . These two subtypes exhibit different genetic and cognitive profiles (Green et al., 2013; Morar et al., 2011 ). An automatic classification of schizophrenia subtypes has been rarely studied. Ingalhalikar et al. proposed that unsupervised spectral clustering of multi-edge graphs built from a structural connectivity network among 78 ROIs be used to identify subtypes of autism and schizophrenia (Ingalhalikar et al., 2012) . Gould et al. proposed using whole brain, voxel-based morphometry to classify schizophrenia patients with cognitive deficit from those that are cognitively spared (Gould et al., 2014) .\nThere are several studies on automatic differentiation of stable MCI from progressive MCI (those that convert to AD within a certain amount of time). Most of these studies reported modest accuracies around 65-80% (Plant et al., 2010; Salvatore et al., 2015; Tangaro et al., 2015; Tong et al., 2014; Wolz et al., 2011; Zu et al., 2015) . ADHD subtype studies are scarce and limited to few studies such as the one by Sato et al. with the intent to automatically differentiate ADHD-IA, ADHD-HI and ADHD-C using resting-state fMRI .\nAgain, one major limitation in differential diagnosis and disease subtype classification is the limited sample size. In most of the current datasets, the number of subjects in each disease subtype is small and therefore provides limited ability to develop robust single-subject predictor to accurately differentiate them."}, {"section_title": "Multimodal neuroimaging studies", "text": "Each imaging modality provides a different view of brain function or structure, and data fusion capitalizes on the strengths of each imaging modality/task and their inter-relationships in a joint analysis. This is an important tool to help unravel the pathophysiology of brain disease (Calhoun et al., 2006a; Sui et al., 2012) . Recent advances in data fusion include integrating multiple (task) fMRI data sets (Kim et al., 2010; Sui et al., 2009 Sui et al., , 2015 from the same participant to specify common versus specific sources of activity to a greater degree than traditional general linear model-based approaches. This can increase confidence in conclusions about the functional significance of brain regions and of activation changes in brain disease. In addition, the combination of function and structure may provide more informative insights into both altered brain patterns and connectivity in brain disorders (McCarley et al., 2008; Michael et al., 2009; Sui et al., 2011) . These findings suggest that most studies favor only one data type or do not combine modalities in an integrated manner, and thus miss important changes which are only partially detected by each modality ). On the other hand, multimodal fusion provides a more comprehensive description of altered brain patterns and connectivity than a single modality, which has shown increasing utility in answering both scientifically interesting and clinically relevant questions."}, {"section_title": "Single-subject prediction using multimodal neuroimaging data", "text": "There is increasing evidence from multimodal studies that patients with brain disorders exhibit unique morphological characteristics, connectivity patterns, and functional alterations, which could not have been revealed through separate unimodal analyses as typically performed in the majority of neuroimaging experiments. Hence, applying classification techniques to these characteristics could identify biomarkers for psychiatric diseases. This could expedite differential diagnosis, thus leading to more appropriate treatment and improved outcomes for patients with brain disorders. There has been number of studies showing the benefits of combining both rest and task fMRI data for group differences in functional connectivity between schizophrenia patients and controls (Arbabshirani and Calhoun, 2011; Cetin et al., 2014) . The change of functional connectivity from rest to task contains novel information present in neither of the states, which could be beneficial for single subject prediction (Arbabshirani and Calhoun, 2011) . Based on these evidences, future studies might benefit from combining resting-state and task-based data for classification of brain disorders.\nAs another example, MCI is difficult to diagnose due to its rather mild and nearly insignificant symptoms of cognitive impairment. Wee et al. integrated information from DTI and resting fMRI by employing multiple-kernel SVM, yielding statistically significant improvement (N7.4%) in classification accuracy of predicting MCI from HC by using multimodal data (96.3%) compared to using each modality independently (Wee et al., 2012) . There are additional studies that demonstrate the potential of the fusion of structural and functional data combined with multi-modal classification techniques to provide more accurate and early detection of brain abnormalities (Fan et al., 2008b) . By taking advantage of these two complementary approaches, Sui et al. proposed a framework based on mCCA + jICA, that allows both high and weak connections to be detected and shows excellent source separation performance . It enables robust identification of correspondence among N diverse data types and enables one to investigate the important question of whether certain disease risk factors are shared or are distinct across multiple modalities, which can also serve as multimodal feature selection method for schizophrenia (Sui et al., 2013a (Sui et al., , 2013b . Similarly, Jie et al. adopted SVM-FoBa to classify between bipolar versus unipolar disorders by combining GM and ALFF features, achieving an accuracy of 92% This suggests that using complimentary multimodal biomarkers may be more informative and effective to discriminate brain disorders .\nThere are number of recent studies looking at combined biomarkers of sMRI, FDG-PET, and CSF (mostly for ADNI dataset) to discriminate between AD, MCI and HC (Gray et al., 2013; Xu et al., 2015; Zhang et al., , 2014 . Similarly, a few studies combined functional and structural data to build such predictive models (Dai et al., 2012b; Dyrba et al., 2015) . Most of those studies reported superior performance of models built based on multimodal features compared to those based on a single modality (Calhoun and Sui, in press )."}, {"section_title": "Deep learning in neuroimaging", "text": "In recent years, deep learning methodology has made significant improvement in representation learning and classification in various areas such as speech recognition, natural image classification and text mining. Two main features have made deep learning very attractive to machine learning researchers. First deep learning in contrast with traditional machine learning methods is capable of data-driven automatic feature learning. This important capability removes the subjectivity in selecting the relevant features especially in cases where too many features exist or prior knowledge in selecting features is not conclusive. The second important feature of deep learning is the depth of models. By applying a hierarchy of non-linear layers, deep learning is capable of modeling very complicated data patterns in contrast with traditional shallow models.\nTypical approaches in single subject prediction in neuroimaging consist of selecting features sometimes from thousands of voxels. As reviewed in this report, the basis for such a feature selection is usually inefficient univariate statistical tests. Recently, deep belief networks, a class of deep learning, has been applied to both structural and functional MRI data . Plis et al. showed that deep learning methods could produce physiologically meaningful features and reveal relations from high dimensional neuroimaging data . Hjelm et al. applied restricted Boltzmann machines (RBM) to identify intrinsic networks in fMRI data . They showed that RBMs could extract spatial networks and their activation with the accuracy of traditional matrix factorization methods such as ICA. Provably, deep models need exponentially smaller number of parameters in order to model the same thing shallow models can model (Bengio, 2012 (Bengio, , 2013 . Moreover, deep learning structures such as RBM are generative models and therefore it can be sampled from. This way it is easy to access uncertainty in the estimates compared to the point estimates of matrix factorization models. Furthermore, for deep learning RBM could be stacked to obtain deeper models as needed. This cannot be readily done with ICA, NMF, or sparse PCA.\nRecently, deep learning is employed in classification of patients using neuroimaging data. Suk et al. used stacked autoencoder (another class of deep learning) to discriminate patients with AD from those with MCI (Suk et al., 2013) . Kim et al. used deep learning for classification of schizophrenia patients from healthy controls based on functional connectivity patterns. They showed that their approach outperforms SVM by a significant margin .\nDeep learning is a very promising tool for understanding the neural basis of brain disorders by extracting hidden patterns from highdimensional neuroimaging data (Kriegeskorte, 2015) . In our opinion, this method has the potential to improve brain disorder diagnosisespecially if larger neuroimaging datasets become available and/or improved methods of training based on existing data are developed ."}, {"section_title": "Standard machine learning competitions in neuroimaging", "text": "The machine learning field has benefited hugely from standard competitions in many applications. In such competitions, usually the participants are provided with a labeled training dataset and an unlabeled testing dataset. The participants try to develop the best predictive model based on the training dataset, predict the labels of the provided testing dataset and submit the results. Such a setting ensures that the results are not biased. These competitions usually attract many groups, even those with less domain knowledge and expertise. By providing a standard dataset and some initial preprocessing, the participants can primarily concentrate on the machine learning aspect of the analysis.\nDue to all of the data sharing problems previously discussed, such competitions are rare in neuroimaging. The ADHD-200 competition was held in 2011 with the goal of predicting ADHD from healthy controls in children and adults, using resting-state fMRI along with anatomical and phonotypical data of 776 subjects (491 TDC and 285 ADHD) for training along with additional 197 subjects for testing (Consortium et al., 2012) . The competition was a successful example of large-scale ADHD data sharing among several sites. However, the 'winning' team of ADHD-200 competition didn't use the imaging data in their predictive model (just the phenotypical data), which caused discussion in the community about usefulness of brain data in diagnosing a brain disorder (Brown et al., 2012b; Consortium et al., 2012) .\nMore recently, The IEEE MLSP workshop held a schizophrenia classification challenge with the goal of automatic classification of schizophrenia patients from healthy controls using just brain imaging features (Silva et al., 2014) . Functional network connectivity values of resting-state fMRI along with ICA loadings of source-based morphometry of sMRI were calculated from 144 subjects (75 healthy controls, 69 schizophrenia patients) and shared with participants. Interestingly, 245 teams participated in the competition and the winning team achieved an AUC of around 0.90. Moreover, by combining the top three models, an AUC of around 0.94 was achieved (Silva et al., 2014) . In our opinion, sharing ready to use, well-defined features as opposed to imaging data itself, was one of the success factors of the MLSP competition in both attracting numerous groups and also achieving high accuracy results. That experience shows that imaging data has a lot of predictive potential at least in the case of separating schizophrenia patients from healthy controls.\nWe believe that the field of neuroimaging can benefit a lot from standard machine learning competitions such as the ones discussed above. Such competitions can assess the realistic, unbiased, discriminative power of brain data for detecting brain disorders. Also, by attracting a large number of participants, a variety of machine learning methods will be examined for the specific problem. By providing brain features, machine learning experts with less neuroimaging domain knowledge can participate and develop predictive models."}, {"section_title": "Summary and conclusions", "text": "Previous single-subject prediction surveys\nIn this study, we comprehensively reviewed past efforts in neuroimaging-based single subject prediction in several brain disorders such as MCI, AD, ASD, ADHD, schizophrenia and depressive disorders. Previous reviews include disease-specific surveys such as schizophrenia (Calhoun and Arbabshirani, 2012; Dazzan, 2014; Demirci et al., 2008b; Kambeitz et al., 2015; Veronese et al., 2013; Zarogianni et al., 2013) , autism spectrum disorder (Retico et al., 2013 (Retico et al., , 2014 ), Alzheimer's disease (Falahati et al., 2014; Kl\u00f6ppel et al., 2008) and in general (Kl\u00f6ppel et al., 2012; Lemm et al., 2011; Orr\u00f9 et al., 2012) as well as modality-specific reviews such as machine learning based on fMRI (Sundermann et al., 2014) . Also, there are few children specific reviews such as a recent one by Levman et al. on multivariate analyses studies in neonatal and pediatric patients (Levman and Takahashi, 2015) . Probably the most comprehensive review so far is the recent one by Wolfers et al., where they reviewed about 120 single subject prediction studies in schizophrenia, mood disorders, anxiety disorders, ADHD and ASD (Wolfers et al., 2015) . While there is some overlap among the mentioned studies and this survey, to our knowledge, this is by far the largest survey in the field based on the number of papers reviewed (about 240 papers). Moreover, as discussed previously, the majority of single subject prediction studies have been published in recent years; consequently, an updated survey is much needed. In this work, several pitfalls such as feature selection bias, incomplete reporting of results, unfair comparison across studies and improper hyperparameter selection were discussed and suggestions to address those issues were provided. Moreover, emerging trends in this exciting field such as decentralized data sharing, differential diagnosis and disease subtype classification, multimodal neuroimaging, applications of deep learning in neuroimaging and merits of standard machine learning competitions were discussed in detail."}, {"section_title": "Limitations", "text": "There are several limitations in this work. We limited our search to MRI-based English journal papers in specific disorders. There are other single subject prediction studies that are based on other modalities such as EEG and MEG. Also, other brain disorders such Parkinson disease and anxiety disorders were not reviewed in this work. From the studies we reviewed, we tried to extract the key features as it relates to the machine learning. Many of those studies contained multiple experiments under different scenarios but we just reported one of them (usually the most successful one) here. Also, there are many important details in each study and for that reason interested readers should always refer to each reference for full information on experiment setup and other details. The other reason for this, is the chance of error in reported information in Table 2 -6, considering the large number of surveyed studies despite our efforts to be as accurate as possible.\nIn terms of common pitfalls, we mostly focused on the potential problems from the machine learning point of view. There are many other important potential issues in topics such as experimental design, effect of head motion and other factors such as the impact of draining veins on fMRI studies (Boubela et al., 2015; Power et al., 2012 Power et al., , 2014 Power et al., , 2015 , wakefulness of subjects during rsfMRI (Tagliazucchi and Laufs, 2014) and the selecting of preprocessing steps (Vergara et al., 2015) . Effect of those potential issues on single subject prediction deserves a full paper by itself.\nIn conclusion, we are optimistic about the use of brain imaging for single subject prediction, and many of the issues we recommend are within reach. Larger studies are available and repositories with pooled data across studies are growing rapidly (Eickhoff et al., 2016) ."}]