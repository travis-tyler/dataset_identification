[{"section_title": "Abstract", "text": "Abstract Accurate diagnosis of Alzheimer's disease and its prodromal stage, i.e., mild cognitive impairment, is very important for early treatment. Over the last decade, various machine learning methods have been proposed to predict disease status and clinical scores from brain images. It is worth noting that many features extracted from brain images are correlated significantly. In this case, feature selection combined with the additional correlation information among features can effectively improve classification/regression performance. Typically, the correlation information among features can be modeled by the connectivity of an undirected graph, where each node represents one feature and each edge indicates that the two involved features are correlated significantly. In this paper, we propose a new graph-guided multi-task learning method incorporating this undirected graph information to predict multiple response variables (i.e., class label and clinical scores) jointly. Specifically, based on the sparse undirected feature graph, we utilize a new latent group Lasso penalty to encourage the correlated features to be selected together. Furthermore, this new penalty also encourages the intrinsic correlated tasks to share a common feature subset. To validate our method, we have performed many numerical studies using simulated datasets and the Alzheimer's Disease Neuroimaging Initiative dataset. Compared with the other methods, our proposed method has very promising performance."}, {"section_title": "Introduction", "text": "Alzheimer's disease (AD) is one of the most common forms of dementia characterized by progressive cognitive and memory deficits. It has been reported that one in every 85 persons in year 2050 will be likely affected by this disease (Brookmeyer et al. 2007 ). The increasing incidence of AD makes this disease a very important health issue and also huge financial burden for both patients and governments (Hebert et al. 2001; Bain et al. 2008) . Thus, it is very important to develop methods for timely diagnosis of AD and its predromal stage, i.e., mild cognitive impairment (MCI). Over the last decade, many machine learning methods have been used for early diagnosis of AD and MCI based on different modalities of biomarkers, e.g., structural brain atrophy delineated by structural magnetic resonance imaging (MRI) (Du et al. 2007; McEvoy et al. 2009; Fjell et al. 2010; Yu et al. 2014) , metabolic alterations characterized by fluorodeoxyglucose positron emission tomography (FDG-PET) (De Santi et al. 2001; Morris et al. 2001) , and pathological amyloid depositions measured by cerebrospinal fluid (CSF) (Bouwman et al. 2007; Fjell et al. 2010) . Typically, these methods learn a binary classification model from training data and use this model to predict disease status (i.e., class label) of the testing subjects.\nBesides classification of disease status, accurate prediction of clinical scores such as Mini-Mental State Examination (MMSE) score and Alzheimer's Disease Assessment Scale-Cognitive Subscale (ADAS-Cog) is also important and useful since they can help evaluate the stage of AD pathology and predict future progression. Specifically, as a brief 30-point questionnaire test, MMSE is commonly used to screen for cognitive impairment. It can be used to examine a patient's arithmetic, memory and orientation (Folstein et al. 1975) . As another important clinical score of AD, ADAS-Cog is a cognitive testing instrument widely used in clinical trials. It is designed to measure the severity of the most important symptoms of AD (Rosen et al. 1984) . Several studies based on regression methods have been conducted to estimate MMSE and ADAS-Cog using the extracted features from MRI and FDG-PET. For example, Duchesne et al. (2005) used linear regression models, Wang et al. (2010) developed a highdimensional kernel-based regression method, and Cheng et al. (2013) proposed a semi-supervised multi-modal relevance vector regression method. However, almost all of these regression methods model different clinical scores separately and do not use the class label information which is often available in practice.\nAlthough the classification of disease status and the prediction of clinical scores are different tasks, there exists inherent correlation among them since the underlying pathology is the same Stonnington et al. 2010) . In the literature, Zhang and Shen (2012) proposed multi-modal multi-task (M3T) learning to predict both class label and clinical scores jointly. M3T formulated the estimations of class label and clinical scores as different tasks. The l 2;1 penalty was used to deliver sparse models with a common feature subset for each task. Their experimental results indicated that selecting a common feature subset for different correlated tasks could achieve better prediction of both class label and clinical scores than choosing the feature subset for each task separately. Although benefiting from using the commonality among different correlated tasks, M3T method did not incorporate the correlation information among features. Actually, many features extracted from brain images such as structural MRI are statistically correlated significantly. In this case, feature selection combined with the additional correlation information among features can improve classification/regression performance (Yang et al. 2012) .\nIn this paper, we extract effective correlation information among features by constructing a sparse undirected feature graph. This undirected graph uses all features as nodes. Also, two features are connected by an edge in the graph if there is statistically significant partial correlation between them. In practice, we can use many existing highdimensional precision matrix estimation methods (Friedman et al. 2008; Cai et al. 2011) to construct this undirected graph. Based on this undirected feature graph, we propose a new graph-guided multi-task learning (GGML) method to predict both class label and clinical scores simultaneously. Specifically, we utilize a new latent group Lasso penalty to encourage the significantly correlated features to be in or out of the models together. This new penalty also encourages the intrinsic correlated tasks to share a common feature subset. It is very useful for us to acquire robust and accurate feature selection. Computationally, the optimization problem for our proposed GGML method can be solved by the traditional group Lasso algorithm efficiently (Yuan and Lin 2006) . Theoretically, our proposed GGML method includes M3T method as a special case. To validate our proposed GGML method, we have conducted many numerical studies using simulated datasets and the Alzheimer's Disease Neuroimaging Initiative (ADNI) (http://www.loni.ucla.edu/ADNI) dataset. Compared with the other methods, our proposed GGML method acquired very promising results.\nThe remainder of this paper is organized as follows. In the ''Materials'' section, we introduce the ADNI dataset used in this study. In the ''Method'' section, we show how to extract useful correlation information among features and describe our proposed new method. In ''Simulation study'' and ''Analysis of the ADNI dataset'' sections, we compare our method with the other methods by simulation study and also the analysis of the ADNI dataset. In the ''Discussion'' section, we discuss some possible extensions of our proposed method. Finally, we conclude this paper in the ''Conclusion'' section."}, {"section_title": "Materials Data", "text": "Data used in this paper were obtained from the ADNI database. As a $ 60 million, 5-year public-private partnership, the ADNI was launched in 2003 by the National Institute on Aging (NIA), the National Institute of Biomedical Imaging and Bioengineering (NIBIB), the Food and Drug Administration (FDA), private pharmaceutical companies and non-profit organizations. The main goal of ADNI was to test whether serial MRI, PET, other biological markers, and clinical and neuropsychological assessments could be combined to measure the progression of MCI and early AD. To that end, 800 adults with age between 55 and 90 were recruited from over 50 sites across the US and Canada. Approximately, 200 cognitively normal controls (NC) and 400 MCI individuals were followed for 3 years and 200 individuals with early AD were followed for 2 years (see http://www.adni-info.org for up-todate information). The general inclusion/exclusion criteria of the subjects are described in Zhang and Shen (2012) . In this paper, we use data from 199 subjects who have complete baseline MRI, FDG-PET, and CSF data. These 199 subjects include 50 AD subjects, 97 MCI subjects, and 52 NC subjects. The detailed demographic information about these 199 subjects is summarized in Table 1 ."}, {"section_title": "Data preprocessing", "text": "Imaging preprocessing was performed for MRI and PET. For MRI, the preprocessing steps include anterior commissure (AC)-posterior commissure (PC) correction, intensity inhomogeneity correction (Sled et al. 1998) , skull stripping (Wang et al. 2011) , cerebellum removal based on registration with atlas, spatial segmentation (Zhang et al. 2001 ) and registration (Shen and Davatzikos 2002) . After registration, we obtained the subjectlabeled image based on the Jacob template (Kabani et al. 1998 ) with 93 manually labeled regions of interest (ROI). For each of the 93 ROIs in the labeled MRI, we computed the volume of gray matter as a feature. For each PET image, we first aligned the PET image to its respective MRI using affine registration. Then, we got the skull-stripping image using the corresponding brain mask of MRI and computed the average intensity of every ROI in the PET image as a feature. Besides MRI and PET, we used CSF Ab42, CSF t-tau and CSF p-tau as CSF features. For each subject, we finally obtained 93 MRI features, 93 PET features, and 3 CSF features. We also had the class label, MMSE and ADAS-Cog scores for each subject."}, {"section_title": "Methods", "text": "In this section, after introducing some notations, we will first discuss how to extract the correlation information among features. Next, in order to show how to utilize this correlation information clearly, we first introduce the graph-guided single-task learning (GGSL) method. Then, as an extension of this method, our proposed graph-guided multi-task learning method will be described."}, {"section_title": "Notation", "text": "For a set A, we denote jAj as the number of elements in A. For a matrix B, we denote B T and B \u00c01 as the transpose and the inverse of matrix B, respectively. We also denote "}, {"section_title": "Extract the correlation information among features", "text": "The correlation information is often measured by the Pearson correlation between each pair of features. We can use sample Pearson correlation coefficients to identify the statistically significant correlated features. One issue with this method is that it only estimates the marginal linear dependence between a pair of features without considering the influence of other features and common driving influences. Such issue can be overcome by using partial When the number of features p is small and the sample size n is big enough (bigger than p), it is easy to get good estimates of partial correlation coefficients. In this case, many previous studies (Hampson et al. 2002; Lee et al. 2011 ) have used partial correlations to identify the significant correlated features. However, in the high-dimensional case with the number of features p bigger than the sample size n, the conventional methods for estimating partial correlation may result in over-fitting of the data (Ryali et al. 2012) . In this case, it is difficult to get accurate estimates of partial correlation coefficients.\nFor our proposed method introduced in the next section, in order to incorporate the correlation information among features, instead of requiring accurate estimation of q \u00c3 ij s, we only need to estimate which pairs of features are partially correlated, i.e., estimate the set E \u00bc f\u00f0i; j\u00de : i\\j and q \u00c3 ij 6 \u00bc 0g. It is well known that the partial correlation coefficients are proportional to the off-diagonal entries of the precision matrix X (Meinshausen and B\u00fchlmann 2006). Thus, estimating E is equivalent to estimating the set f\u00f0i; j\u00de : i\\j and x ij 6 \u00bc 0g. In this way, many existing methods (Meinshausen and B\u00fchlmann 2006; Friedman et al. 2008; Cai et al. 2011) can be used to estimate E effectively.\nIn this paper, we will use the graphical Lasso (Friedman et al. 2008) or the neighborhood selection method (Meinshausen and B\u00fchlmann 2006) to estimate E and denote its estimate as\u00ca. Furthermore, we represent\u00ca as an undirected graph G with p nodes and j\u00caj edges, where each node represents one feature and each edge indicates that two involved features are partially correlated significantly. Figure 1 shows an example on how to transform the estimated precision matrixX into the estimated undirected graph G. In the graph G, features i and j are connected if and only ifx ij 6 \u00bc 0."}, {"section_title": "Graph-guided single-task learning (GGSL) method", "text": "In this section, we assume that the undirected feature graph G has been constructed. For each i \u00bc 1; 2; . . .; p, denote N i as the set including the ith feature and its neighbors in the graph G, i.e., N i \u00bc fj :x ji 6 \u00bc 0g.\nTo show how to use the correlation information represented by G, we consider the single-task learning first and then generalize this idea to multi-task learning. Without loss of generality, considering the tth task, we want to use the following linear model to predict the response variable Y t ,\nwhere \nThus, the true coefficient vector B t can be represented as\nwhere X shows the partial correlations among different features, and C t reflects the marginal correlations between the features and the tth response variable Y t . Furthermore, the Eq. (3) can be expanded as follows:\n. .\nWe observe that the coefficients vector B t \u00bc \u00f0b 1t ; b 2t ; . . .; b pt \u00de T is the sum of p parts, where the ith part, \u00f0x 1i c it ; x 2i c it ; . . .; x pi c it \u00de T , is the ith vertical part in the right side of the above equations (4). In addition, for each i, if there is no marginal correlation between the ith feature and the response variable Y t , i.e., c it \u00bc 0, then the components in the ith part \u00f0x 1i c it ; x 2i c it ; . . .; x pi c it \u00de T will be zero simultaneously due to the common factor c it . Furthermore, if the ith feature and the response variable Y t are correlated marginally, then c it 6 \u00bc 0 and the set of candidate nonzero components in the ith part is fj : x ji 6 \u00bc 0g, which can be well estimated by the set N i including the ith feature and its neighbors in the estimated undirected graph G.\nMotivated by the decompositions shown in Eq. (4), we assume that there is a latent decomposition of the coefficients vector B t into p parts, V 1t ; . . .; V it ; . . .; V pt , where V it is a p-dimensional latent vector representing the ith vertical part in the right side of Eq. (4). In order to incorporate the correlation information represented by the undirected graph G, a group penalty term will be used to encourage the ith latent vector V it to be zero or have nonzero components only for the indices in the set N i . Hence, we use the following (GGSL method to estimate B t :\nsubject to B t \u00bc P p i\u00bc1 V it and supp\u00f0V it \u00de N i for each 1 i p, where supp\u00f0V it \u00de is the index set of the nonzero components in the vector V it . In the optimization problem (5), s it is a positive weight for the ith part and tth task. Similar to the methods for adaptive Lasso (Zou 2006 ) and group Lasso (Yuan and Lin 2006) , we can set s it \u00bc ffiffiffiffiffiffi ffi\nwhere c is a positive parameter andb it is an initial estimate of b it . In our experiments, we choseb it as the sample correlation coefficient between X i and Y t . Both the positive parameter c and the tuning parameter k were chosen by cross-validation. Our experimental results indicate that this method can acquire good performance in general. Theoretically, the GGSL method is very general and covers the popular Lasso method as a special case. Specifically, if we ignore the correlation information among features, we can set the undirected graph G as an empty graph with no edge. In this case, if setting constant weights s it s, we can show that P p i\u00bc1 s it kV it k 2 / jB t j 1 , and the GGSL method is the same as the Lasso method (Tibshirani 1996) . In general, we can estimate a sparse undirected graph G for modeling the significant partial correlation information among features. The GGSL method can utilize this correlation information effectively and thus acquires good prediction performance."}, {"section_title": "Graph-guided multi-task learning (GGML) method", "text": "For the multi-task learning, we aim at estimating q response variables simultaneously. Similar to the above GGSL method, for each t, we assume that the coefficient vector B t can be decomposed as\nis a p-dimensional latent vector satisfying supp\u00f0V it \u00de N i . Furthermore, in order to make use of the intrinsic correlation among these q tasks (response variables), we also assume that the decompositions of q coefficient vectors B 1 ; B 2 ; . . .; B q have the same pattern, i.e., Similar to the GGSL method discussed in ''Graph-guided single-task learning (GGSL) method'' section, we can set the weight s i \u00bc ffiffiffiffiffiffi ffi\nc . The cross-validation method can be used to choose the best c and the best tuning parameter k for different tasks separately. Note that the penalty term in (6) along with the additional constraints not only encourage the significantly partially correlated features to be in or out of the model jointly, but also choose a common feature subset for different tasks. Due to the use of both the correlation information among features and the intrinsic commonality among different related tasks, our proposed GGML method can acquire better prediction performance than the methods not using or only using part of these two kinds of information.\nAs an interesting remark, we note that the M3T method (Zhang and Shen 2012 ) is a special case of our proposed GGML method. In particular, when we ignore the correlation information among features, we can set the undirected graph G as an empty graph with no edge. In this case, if setting constant weights s i s, we can show that\nwhere B i\u00c1 is the ith row of the coefficient matrix B. Thus, our proposed GGML method is exactly the same as the M3T method using the l 2;1 penalty."}, {"section_title": "Objective function optimization", "text": "For our proposed GGML method, we need to solve the optimization problem (6). We can transform this constrained optimization problem into a simple unconstrained optimization problem by feature duplication.\nDenote X \u00c1N i as the sub-matrix of X with column indices in N i , and denote V i N i \u00c1 as the sub-matrix of V i with row Brain Struct Funct (2016) 221:3787-3801 3791 indices in N i . Furthermore, denoteX \u00bc \u00f0X \u00c1N 1 ; X \u00c1N 2 ; . . .;\njN i j\u00de as the duplicated feature matrix and\nT as the \u00f0 P p i\u00bc1 jN i j\u00de \u00c2q coefficient matrix. Then, we can check that XB \u00bcX\u1e7c and (6) is equivalent to the following unconstrained optimization problem:\nThe above problem (7) is a traditional group Lasso problem which can be solved efficiently by the blockwise majorization decent algorithm (Yang and Zou 2013) . Denote the estimate of B asB. In the application stage, given a testing subject x \u00c3 , for the tth task, we can estimate\nt is a class label and by\u0176\nt is a continuous response variable."}, {"section_title": "Simulation study", "text": "In this section, we perform numerical studies using simulated examples. For each example, we compare our proposed GGML method with (1) the Lasso method which learns different tasks separately; (2) the GGSL method which uses the correlation information among features and learns different tasks separately, and (3) M3T method which learns different tasks jointly while ignoring the correlation information among features. We implement Lasso, GGSL, and M3T methods as shown in ''Objective function optimization'' section to predict the response variables. Similar to the measures used in Zhang and Shen (2012) , the classification accuracy and the Pearson's correlation coefficient (CC) are also used here to evaluate the classification and regression performances, respectively. In addition, we also use the root-mean-square error (RMSE) to evaluate the regression performance."}, {"section_title": "Simulated examples", "text": "We study three simulated examples. Each example has one classification task and two regression tasks. We set p \u00bc 100, \nExample 3 The features fx sj : 1 j 15g are generated from the same model as shown in Example 1. In addition, the features \u00f0x s16 ; x s17 ; . . .; x sp \u00de $ N\u00f00;X \u00c01 \u00de, wher\u1ebd\nEach off-diagonal entry in M is generated independently and equals 0.5 with probability 0.05 or 0 with probability 0.95. The diagonal entry of M is 0. Here, d is chosen such that the conditional number ofX is equal to p \u00c0 15. Finally,X is standardized to have unit diagonals.\nAfter generating each column of the response matrix Y by model (1), we replace the elements in the first column of Y by their signs (positive or negative) to simulate class labels. For all examples, we generate 40 training samples, 40 validation samples, and 400 testing samples. All the models are fitted on the training data. The validation data are used to choose the tuning parameters and the testing data are used to evaluate different methods. For each example, we repeat the simulation 30 times. Figure 2 shows the binary maps of the true precision matrices and Fig. 3 shows the corresponding feature graphs of these three examples. All these three graphs are sparse. For Examples 1 and 3, useful features (i.e., features with nonzero regression coefficients) are only connected with useful features. For Example 2, one useful feature is connected with one useless feature. In addition, for each example, different tasks are highly correlated since they share the same useful features. It is very interesting to study whether correlation information among features represented by the feature graph and the correlation information among tasks can be incorporated to improve the prediction performance. Table 2 shows the comparison of different methods using these three simulated examples. As shown in Table 2 , for all these three examples, the GGSL method and GGML method acquire better performance than the Lasso method and the M3T method, respectively. This indicates that the extracted partial correlation information from features can be utilized to improve the prediction performance. In addition, the GGML method and M3T method also acquire better performance than the GGSL method and the Lasso method, respectively. It indicates that learning different correlated tasks jointly can also improve the prediction performance. For these three simulated examples, since our proposed GGML method incorporates both the partial correlation information among features and the intrinsic correlation information among different related tasks, it delivers the best performance in all cases. In the next section, we will further compare these four methods using the ADNI dataset."}, {"section_title": "Simulation results", "text": ""}, {"section_title": "Analysis of the ADNI dataset", "text": "For the ADNI dataset, we estimate one class label and two clinical scores (i.e., MMSE and ADAS-Cog) using the MRI, FDG-PET and/or CSF features. Since there are two binary classification problems (AD vs. NC, and MCI vs. NC), we perform two sets of experiments. The first set of experiments uses the AD/NC dataset including only AD and NC subjects. The second set of experiments uses the MCI/NC dataset including only MCI and NC subjects. For each set of experiments, we consider four cases: (I) use only MRI features; (II) use only PET features; (III) use both MRI and PET features (denoted as MRI \u00fe PET); (IV) use all MRI, PET and CSF features (denoted as MRI \u00fe PET \u00fe CSF).\nTo evaluate the performance of different methods, we used the tenfold cross-validation (CV) strategy. Specifically, the whole samples were partitioned randomly into ten subsets. Each time only nine subsets were chosen for training and the remaining one was used for testing. We repeated this process ten times with each of the ten subsets used exactly once as the testing data. Furthermore, in consideration of possible bias due to the random partition in the tenfold CV, we repeated the whole 10-CV process 30 times. In the training process, each column of the training data was normalized to have mean 0 and standard deviation 1. For all methods, we performed another inner fivefold CV on the training data to choose the tuning parameters."}, {"section_title": "Partial correlation among different features", "text": "In the first step of the GGSL and GGML methods, we need to extract the effective correlation information from features. Note that, only the training data matrix of features were used to estimate the sparse undirected graph G representing the significant partial correlation among features. Figure 4 shows the binary maps of the estimated precision matrices. Binary maps in the first two columns indicate that many features within the same modality (e.g., MRI or PET) are partially correlated statistically significantly. However, as shown by the binary maps in the third column, the partial correlation between MRI features and PET features are not statistically significantly in most cases. Furthermore, the comparison between the binary maps in the first row and the second row indicates that the partial correlation information extracted from AD/NC data is similar to that of MCI/NC data. Similar to the example shown in Fig. 1 , we can transform the estimated precision matrices to some undirected graphs. The feature graphs corresponding to the estimated precision matrices are shown in Fig. 5 .\nThis graph information will be used in the GGML and GGSL methods."}, {"section_title": "Classification results", "text": "The classification accuracies of different methods are shown in Table 3 . All methods deliver higher classification accuracy for the AD/NC dataset than the corresponding classification accuracy for the MCI/NC dataset. For the AD/NC dataset, when we use only MRI features or PET features, the GGSL method and GGML method acquire better classification performance than the Lasso method and the M3T method, respectively. This indicates that the extracted partial correlation information from features can be utilized to improve the classification performance. In addition, when we use both MRI and PET features or all the MRI, PET, and CSF features, since it is relatively easy to discriminate AD subjects from NC subjects in this case, all four methods acquire similar high classification accuracies.\nFor the MCI/NC dataset, on the one hand, the comparison between GGSL and Lasso (or GGML and M3T) indicates that using the extracted partial correlation information among features improve the classification performance significantly. On the other hand, the comparison between GGML and GGSL (or M3T and Lasso) shows that the joint classification and regression could provide better classification performance than the separate classification. Since our proposed GGML method incorporates both the partial correlation information among features and the intrinsic correlation information among different related tasks, it delivers the best classification performance. "}, {"section_title": "Regression results", "text": "For regression tasks, we need to predict both the MMSE score and the ADAS-Cog score. Tables 4 and 5 show the comparison of regression performance on the AD/NC data and the MCI/NC data, respectively. As shown in Tables 4  and 5 , our proposed GGML method acquires promising performance in most cases. For example, when we use all the features to predict the MMSE score, for the AD/NC data, our proposed GGML method achieves the highest correlation coefficient 0.745 while the corresponding correlation coefficients for Lasso, GGSL, and M3T are 0.709, 0.723 and 0.724, respectively. For the MCI/NC data, GGML also has the best performance with correlation coefficient 0.382 while the corresponding correlation coefficients for Lasso, GGSL, and M3T are 0.303, 0.325 and 0.364, respectively. In addition, when we use all the features to predict the ADAS-Cog scores, for the AD/NC data, our proposed GGML method achieves the highest correlation coefficient 0.740 while the corresponding correlation coefficients for Lasso, GGSL, and M3T are 0.664, 0.719 and 0.718, respectively. For the MCI/NC data, GGML also has the best performance with correlation coefficient 0.472 while the corresponding correlation coefficients for Lasso, GGSL, and M3T are 0.336, 0.464 and 0.426, respectively.\nIt is interesting to note that for the MCI/NC dataset, the PET and CSF data seem to be not useful for the prediction of MMSE score. All four methods acquire poor prediction of the MMSE scores when only the PET data are used. In addition, compared with the cases only using MRI data, both M3T and GGML methods acquire worse performance when the additional PET/CSF data are used. Similar to the previous discussion about classification performance, the comparison between GGSL and Lasso (or GGML and M3T) indicates that using the extracted partial correlation information among features improves the prediction of MMSE and ADAS-Cog scores significantly. In addition, the comparison between GGML and GGSL (or M3T and Lasso) shows that joint classification and regression could deliver better prediction performance than the separate regression of MMSE (or ADAS-Cog) on the features. Since our GGML method incorporates both the partial correlation information among features and the intrinsic correlation information among different tasks, it delivers the best prediction of the MMSE and ADAS-Cog scores."}, {"section_title": "Most discriminative brain regions", "text": "In this subsection, we investigate the most discriminative brain regions for the diagnosis of disease status and the prediction of the MMSE and ADAS-Cog scores. For each method, we repeated the whole 10-CV process 30 times and acquired 300 different models using different training datasets. Figure 6 shows the selection frequency of each of 93 ROIs for the AD/NC classification task using only MRI features, where the selection frequency for each ROI is defined as Frequency \u00bc 100 \u00c2 The times of being selected in the 300 models 300 :\nFor each method, some ROIs are always selected while some ROIs are seldom selected. Compared with Lasso and M3T, the GGSL and GGML methods tend to select more ROIs since they use the feature graph information and encourage the significantly partially correlated features to be selected jointly. According to the selection frequency, we compare the top ten selected ROIs of different methods for different tasks. Tables 6, 7 and 8 show the indices of the top ten selected ROIs of the four methods for different tasks (classification or regression), different datasets (AD/ NC or MCI/NC) and different modalities (MRI or PET). Table 9 contains the full names of the ROIs.\nAs shown in Tables 6, 7 and 8, for different tasks, the top ten selected ROIs of the single-task learning methods such as Lasso and GGSL are different while the top ten selected ROIs of the multi-task learning methods such as M3T and GGML are the same. We can also observe that the top ten selected ROIs for the cases using MRI Bold values represent the best performance for a particular measure\nThe reported values are the averaged correlation coefficient with standard deviation. Bold values represent the best performance for a particular measure.\nThe reported values are the averaged correlation coefficient with standard deviation. 18, 22, 38, 44, 46, 69, 80, 83, 84, 90 12, 18, 23, 26, 41, 68, 69, 73, 81, 87 GGSL 18, 22, 30, 44, 58, 69, 80, 83, 84, 90 12, 18, 26, 35, 41, 68, 69, 73, 79, 87 M3T 9, 18, 22, 46, 48, 69, 80, 83, 84, 90 12, 23, 26, 35, 62, 68, 69, 73, 81, 87 GGML 18, 22, 30, 44, 48, 67, 80, 83, 84, 90 7, 12, 23, 26, 35, 62, 68, 69, 73, 87 MCI/NC Lasso 17, 28, 40, 48, 63, 64, 69, 83, 86, 92 2, 37, 39, 41, 54, 55, 63, 68, 81, 87 GGSL 17, 22, 30, 40, 46, 64, 69, 76, 83, 92 11, 12, 23, 26, 28, 29, 38, 40, 41, 87 M3T 17, 40, 46, 48, 53, 63, 64, 69, 83, 86 12, 35, 41, 62, 64, 68, 73, 79, 81, 87 GGML 22, 40, 45, 46, 61, 64, 69, 76, 83, 86 11, 12, 26, 29, 38, 40, 41, 47, 79, 87 9, 15, 18, 19, 22, 40, 80, 83, 84, 90 12, 18, 23, 26, 62, 63, 68, 69, 73, 79 GGSL 19, 22, 48, 58, 62, 67, 80, 83, 84, 85 7, 12, 23, 26, 35, 41, 62, 68, 69, 73 M3T 9, 18, 22, 46, 48, 69, 80, 83, 84, 90 12, 23, 26, 35, 62, 68, 69, 73, 81, 87 GGML 18, 22, 30, 44, 48, 67, 80, 83, 84, 90 7, 12, 23, 26, 35, 62, 68, 69, 73, 87 MCI/NC Lasso 17, 33, 40, 44, 48, 53, 62, 64, 69, 86 4, 23, 24, 33, 41, 61, 62, 68, 84, 87 GGSL 22, 45, 46, 48, 61, 64, 69, 76, 83, 86 11, 12, 23, 26, 28, 29, 38, 40, 41, 87 M3T 17, 40, 46, 48, 53, 63, 64, 69, 83, 86 12, 35, 41, 62, 64, 68, 73, 79, 81, 87 GGML 22, 40, 45, 46, 61, 64, 69, 76, 83, 86 11, 12, 26, 29, 38, 40, 41, 47, 79, 87 Brain Struct Funct (2016) 221:3787-3801 3797 features are not very similar to the top ten selected ROIs for the cases using PET features. One possible reason is that MRI features and PET features provide complementary information for the diagnosis of AD. However, for each case, the top ten selected ROIs of the four methods are similar. For example, for the AD/NC classification task using MRI features, Table 6 indicates that the ROIs with indices 18, 80, 83, 84, and 90 are frequently selected by all four methods. It is interesting to point out that both GGML and M3T methods also select 9, 18, 46, 48, 61, 62, 80, 83, 84, 90 12, 23, 26, 30, 35, 62, 73, 76, 81, 92 GGSL 18, 30, 48, 58, 62, 67, 80, 83, 84, 85 7, 12, 23, 26, 30, 35, 62, 69, 73, 92 M3T 9, 18, 22, 46, 48, 69, 80, 83, 84, 90 12, 23, 26, 35, 62, 68, 69, 73, 81, 87 GGML 18, 22, 30, 44, 48, 67, 80, 83, 84, 90 7, 12, 23, 26, 35, 62, 68, 69, 73, 87 MCI/NC Lasso 10, 17, 18, 38, 45, 46, 69, 72, 83, 87 10, 12, 14, 19, 35, 39, 41, 62, 64, 88 GGSL 17, 45, 46, 61, 62, 69, 72, 76, 83, 87 11, 12, 28, 29, 35, 38, 41, 71, 79, 87 M3T 17, 40, 46, 48, 53, 63, 64, 69, 83, 86 12, 35, 41, 62, 64, 68, 73, 79, 81, 87 GGML 22, 40, 45, 46, 61, 64, 69, 76, 83, 86 11, 12, 26, 29, 38, 40, 41, 47, 79, 87 Table 8 , the 48th ROI is frequently selected by Lasso and GGSL for the regression task (ADAS-Cog) using AD/NC data. This indicates that the multi-task learning methods such as GGML and M3T incorporate the clinical score information for the classification task.\nOn the other hand, as shown in Table 8 , both GGML and M3T methods select the 22th ROI frequently for the regression task (ADAS-Cog) using AD/NC data while this ROI is not one of the top ten selected ROIs of Lasso and GGSL for this task. However, as shown in Table 6 , the 22th ROI is frequently selected by Lasso and GGSL for the classification task (AD vs NC). This indicates that the multi-task learning methods such as GGML and M3T incorporate the class label information for the regression task. Furthermore, as shown in Tables 6, 7 and 8, for the study using AD/NC data and MRI features, the common top ten selected ROIs of Lasso for different tasks are the ROIs with indices 18, 80, 83, 84 and 90. The common top ten selected ROIs of the GGSL method for different tasks are the ROIs with indices 58, 80, 83, and 84. Most of these ROIs are the top ten selected ROIs of our proposed GGML method. In Figs. 7 and 8, we visualize the top ten selected ROIs of our proposed GGML method when different datasets (AD/NC or MCI/NC) and different modalities (MRI or PET) are used. Most of the selected regions, e.g., uncus right (22), hippocampal formation right (30), uncus left (46), middle temporal gyrus left (48), hippocampus formation left (69), middle temporal gyrus right (80) and amygdale right (83), are known to be highly correlated with AD and MCI by many studies using group comparison methods (Jack et al. 1999; Misra et al. 2009; Zhang and Shen 2012) ."}, {"section_title": "Discussion", "text": "In this section, we first discuss some issues about constructing the undirected feature graph G. Then, some possible extensions of our proposed method will be discussed.\nConstruction of the undirected feature graph G Before performing our proposed GGML method, we need to construct an undirected feature graph G representing the significant correlation information among features. In ''Extract the correlation information among features'' section, we proposed to use the graphical Lasso method to construct this graph. For some datasets, the constructed graph G may include many edges corresponding to weak or even wrong partial correlation due to bad estimation of the precision matrix. In this case, by thresholding of the estimated precision matrix, we can construct a sparse undirected graph for representing only the most reliable partial correlation.\nFurthermore, besides partial correlation information among features, we can also combine other useful information (e.g., some prior information about features) to construct this graph G. Our proposed GGML method can be used for any given undirected feature graph G representing the relationships among different features. Use of the structure information among different subjects\nOur proposed GGML method utilizes both the correlation information among features and the intrinsic correlation information among different response variables. Actually, we can also generalize GGML method to incorporate the structure information among different subjects. Similar to the locality preserving projection (LPP) method (He and Niyogi 2004 ), we can model the structure information among different training subjects as another sparse undirected graph S. Here, S has n nodes and each node represents one subject. The connectivity of the graph S can be defined by the k nearest neighbors, i.e., subjects x s and x l are connected by an edge if x s is among the k nearest neighbors of x l , or x l is among the k nearest neighbors of x s . In order to use the structure information among different training subjects represented by S, we can preserve the neighborhood structure of subjects, i.e., encouraging the predicted response variables\u0177 s \u00bc B T x s and\u0177 l \u00bc B T x l to be close if the sth and the lth subjects are connected in the undirected graph S."}, {"section_title": "Conclusion", "text": "In summary, we propose a new graph-guided multi-task learning method to incorporate the correlation information among features and the intrinsic correlation information among different tasks. To use the correlation information among features, our proposed GGML method encourages the partially correlated features to be in or out of the model jointly. Furthermore, in order to acquire more robust and accurate feature selection, our proposed GGML method encourages different tasks to share a common useful feature subset. Theoretically, our proposed GGML method is very general and includes the M3T method as a special case. The experimental results on the simulated examples and the ADNI dataset also show the advantage of the proposed GGML method over the existing methods."}]