[{"section_title": "", "text": ".10 Average minutes to complete B&B:2000/01 field test student interview, by interview section  Tables   Table 3.12 Average minutes to complete B&B:2000/01 field test student interview, by interview section and teaching status 38  Table 3.14 B&B:2000/01 field test contact and interview rates for hard-to-reach respondents, by percentage of calls where an answering machine was reached 41 Table 3.15 B&B:2000/01 field test interview results, by call-ins to toll-free study number   Graduate enrollment Table 4.4 Current employment Table 4.5 Comparison of B&B:2000/01 CATI respondents and nonrespondents Table 4.6 Student interview item nonresponse for items with more than 10 percent \"don't know\" or \"refused\" 51   Table 4.7 Item-level rates of help text access Table 4.8 Success rates for online coding procedures    As in the previous studies, B&B collects base-year data during the National Postsecondary Student Aid Study (NPSAS) data collection. For example, NPSAS:93 served as the base year for data collection for the first Baccalaureate and Beyond Longitudinal Study (B&B:1993). These students were identified in NPSAS:93 as baccalaureate recipients during the 1992-93 school year. The first follow-up took place one year after graduation (B&B:93/94) and the second occurred four years after graduation (B&B:93/97). As with B&B:93/94, the current study will collect follow-up data from students who were identified as baccalaureate recipients in the NPSAS:2000 survey, one year after graduation. And, as with B&B:93/94 and the prior NCES Recent College Graduates (RCG) study series, it will allow study of graduates' experiences as undergraduates and their initial forays into graduate education and the labor market. It also offers an important opportunity to study early outcomes of newly qualified 1."}, {"section_title": "NPSAS institutional sample", "text": "Effectively, all U.S. institutions eligible for Title IV aid' that offered academically or vocationally oriented postsecondary programs were eligible for NPSAS:2000.2 Specifically, to be eligible for NPSAS:2000, a non-military-academy educational institution must offer an educational program designed for persons who have completed secondary education; offer more than just correspondence courses; offer at least one academic, occupational, or vocational program of study lasting at least 3 months or 300 clock hours; offer courses that are open to more than the employees or members of the company or group (e.g., union) that administers the institution; be located in the 50 states, the District of Columbia, or Puerto Rico; and U.S. military academies were excluded due to their atypical funding/tuition base. Institutions providing only vocational, recreational, remedial, or correspondence courses, or only inhouse courses for their own employees, were excluded. Institutions selected for the NPSAS:2000 field test were not selected for the full-scale study because participation in both surveys was considered excessively burdensome. To accomplish this, the field test sample was selected after institutions large enough to be certainty institutions in the full-scale survey were deleted from the field test institutional sampling frame. Then, a stratified simple random sample of institutions was selected for the field test, using the same 22 strata as the full-scale study. Although no probability-based inferences were planned for the field test, a probability-based sample was used because the complement of the field test sample will be used for the full-scale study-sampling frame. An important benefit of this method of selecting the institutions for the field test is that a more up-to-date institutional sampling frame could be constructed for the full-scale survey without loss of the ability to generalize to the full population. The full-scale sampling frame will be constructed from the 1998-99 IPEDS IC file, which became available after the field test sample had been selected. Institutions that had been selected for the field test sample will be deleted from the full-scale sampling frame so that they will not be selected for the full-scale sample. The probability of selection for the full-scale study will be adjusted for institutions on the sampling frame based on the probability that they were not selected for the field test sample. Nearly twice as many institutions as needed were selected in the simple random sample for the field test so that the field test sample could be selected purposively from this sample. Three institutions in Puerto Rico were purposively selected to evaluate the viability of alternative methods of locating and interviewing students located there, and to check on whether the improved response rates, which RTI achieved in Puerto Rico in NPSAS:96, would continue. Clusters of institutions were selected in several cities to provide an adequate number of students for testing the field interviewing procedures. The remaining field test institutions were selected to represent the 22 institutional strata. In total, 74 institutions were selected for the field test with the expectation that this sample size would yield 66 institutions that both were eligible and would provide lists for student sampling. A breakdown of sampled institutions by original institutional stratum is provided in table 2.1. This table also shows, in total and by institutional stratum, eligibility rates and rates for providing student lists. Overall, 98 percent of the sampled institutions met NPSAS eligibility requirements, and of those, about 86 percent provided lists for student sampling.   'Percent is based on overall total within column. 7Percent is based on number sampled within row. 3Percent is based on number eligible within row. 4lncludes two institutions which agreed to provide lists but did not do so in the time provided 5A school was classified as \"high ed\" if it was in the top 20 percent of its stratum in terms of the numbers of baccalaureate students graduating with education degrees. 6A school was classified as \"low ed\" if it was not in the top 20 percent of its stratum in terms of the numbers of baccalaureate students graduating with education degrees. 7Includes one institution which agreed to provide lists but did not do so in the time provided. NOTE: Most first-professional-granting institutions award doctor's degrees as well as first-professional degrees. "}, {"section_title": "NPSAS student sample", "text": "Not all students enrolled in eligible institutions were considered eligible for NPSAS. In addition to being enrolled at a NPSAS-eligible institution during the appropriate time frame (for the field test, between July 1, 1998, andApril 30, 1999; for the full-scale study, between July 1, 1999, andJune 30, 2000), NPSAS-eligible students had to be: enrolled in either (1) an academic program; (2) at least one course for credit that could be applied toward fulfilling the requirements for an academic degree; or (3) an occupational or vocational program that required at least 3 months or 300 clock hours of instruction to receive a degree, certificate, or other formal award. Simultaneously, they could not be concurrently enrolled in high school, nor enrolled solely in a GED or other high school completion program. Students who received a baccalaureate degree at any time between the appropriate dates for the field test (between July 1, 1998, andJune 30, 1999) were eligible for the NPSAS and the Baccalaureate and Beyond studies. Students were selected from \"unduplicated\"3 student lists provided by participating institutions, using the same procedures to be implemented in the full-scale study. While schools were made aware of student eligibility requirements, as in previous waves of NPSAS, the bulk of the student eligibility determination was accomplished after sampling from the provided lists (i.e., during record abstraction or student interviewing). Incorrect information provided by institutions as to student status resulted in some other misclassification errors, which were also corrected after sampling. Students were stratified within selected institutions into seven strata. Separate strata were established for baccalaureate degree completers, undergraduates, first-professional students, and other graduate students. The baccalaureate stratum was subdivided into two mutually exclusive strata based on whether students' major field of study was business or another field. Three graduate strata were defined as: students in master's degree programs, students in doctorate degree programs, and other graduate students. Stratum sampling rates were predetermined for each institution to yield the desired stratum sample sizes and minimum institution sample sizes. Business baccalaureate recipients were sampled at lower sampling rates than other baccalaureate recipients because large proportions of all baccalaureate degrees are awarded to business majors. Differential sampling rates were also used for the three types of graduate students in order to get adequate representation of students pursuing doctoral degrees and to limit the sample size for \"other\" graduate students, who are of limited inferential interest. Established sampling rates were applied to the unduplicated student lists to attain the sample using stratified systematic sampling procedures. The sample was constrained so that (1) no less than 25 students were to be selected from each institution, even if the sampling rate had to be raised; and (2) the total sample 3In some instances, the lists could be unduplicated by the supplying institutions. However, in many cases, institutions were unable (or unwilling) to supply unduplicated lists, and the unduplicating process was accomplished by contractor staff. from an institution did not exceed the expected sample size based on the 1997-98 IPEDS information by more than 50, even if the rates had to be reduced. The sample size was monitored by strata and sampling rates were adjusted, where appropriate."}, {"section_title": "B&B:2000/0I Field Test Report", "text": "The achieved field test student sample sizes are shown in table 2.2 by institutional type and student stratum. About half of the overall sample, more than half of the baccalaureate sample, and almost half of the other undergraduate sample were selected from public institutions (reflecting the higher undergraduate enrollment in such institutions); however, the graduate/first-professional sample had a slightly higher percentage selected from private, not-for-profit institutions than from public institutions. During the full-scale study, the sample sizes in each student stratum will be closely monitored and the sampling rates adjusted, as necessary, to achieve target sample sizes. Table 2.3 shows the base-year NPSAS field test response rates for the B&B cohort. A total of 1,302 potential baccalaureate degree recipients were identified using institutionally provided lists of students who graduated or were candidates to graduate between July 1, 1998, andJune 30, 1999. Of the 1,302 potential baccalaureate degree recipients sampled during the base year, 196 were from institutions that submitted data-file CADE. The collection of CADE information via data file was a procedural test, and these cases were not intended to be loaded into the computer-assisted telephone interviewing (CATI) system because there would not be sufficient time to work these cases in CATI during the base year field test. Therefore, the 196 data file cases were excluded from both the base year and follow-up samples. Additionally, 61 of the 1,302 B&B sampled students were subsequently determined in the NPSAS field test survey to be ineligible. Of the remaining 1,045 students sampled as B&B who were eligible for NPSAS, 797 (76.3 percent) participated in NPSAS. Students in public institutions had the highest NPSAS response rates for the B&B cohort (80.2 percent). The field test student sample was selected to represent the various institutional sectors so that we could properly test the study instruments and operations. However, since we are not interested in making statistical inferences from field test data, ensuring population coverage was not as critical for the field test design as it will be in the full scale study.   3.\n"}, {"section_title": "B&B follow-up student sample", "text": "The sampling frame for the B&B:2000/01 field test was constructed by considering the following types of students from the NPSAS:2000 field test: students who were located and interviewed during the NPSAS:2000 field test, and confirmed to be baccalaureate recipients between July 1, 1998, andJune 30, 1999; students who were sampled as members of the B&B cohort, and located but not interviewed in the NPSAS:2000 field test; students who were sampled as baccalaureate recipients, but not located for the NPSAS:2000 field test; FST CO P! AVAILABLE 11 B&B:2000/01 Field Test Report 24 2. Design and Method of the Field Test students who were sampled as baccalaureate recipients but classified as exclusions4 for NPSAS; and students who were sampled as baccalaureate recipients but did not meet the NPSAS eligibility requirements specified in section A.2. For each of the above categories, table 2.4 shows the distribution of NPSAS:2000 field test and the B&B follow-up sample. The first three types of students listed above formed the three sampling strata for the B &B:2000 /O1 field test. All students were selected from the first stratum. Within the other two strata, the students were sorted by whether or not they were included in the NPSAS incentive experiment,5 and then they were further sorted by the nine-level institutional sector. Within each stratum, a systematic sample was selected from the sorted frame, which ensured proportional representation of the students within strata by whether or not they were included in the incentive experiment and by sector. The total B&B follow-up field test sample size consisted of 855 students, 672 of whom were not in the NPSAS incentive experiment, and 183 of whom were in the experiment. The B&B follow-up sample distribution by institutional sector is shown in table 2.5. None of the exclusions or ineligible students was sampled."}, {"section_title": "B.", "text": "Data collection design 1.\n"}, {"section_title": "Locating", "text": "The basic B &B:2000 /O1 design involved tracing sample members to their current location and conducting a computer-assisted telephone interview with them about their experiences since the NPSAS:2000 field test interview approximately one year earlier. The data collection activities, including locating, are shown in figure 2.1. While the flow shown is sequential for any given case, these activities were quite dynamic. At any given time during the locating/interviewing period, different sample members were at markedly different stages in the flow. 4 Students who had died or were incarcerated, institutionalized, or out of the country for the duration of the data collection period were classified as exclusions for NPSAS.   "}, {"section_title": "Pre-CATI locating", "text": "Locating information obtained during the NPSAS:2000 field test was incorporated into the B&B:2000/01 field test locator database, and sent in batch mode to the U.S. Postal Service National Change of Address (NCOA) system and Telematch in February of 2000. These services provided updated address and telephone number information respectively. Following the first round of NCOA and Telematch batch processes and after updating of the locator database with the new information, a student mailing was sent to all sample members one week before data collection started. The purpose of the mailing was to inform them of the study and their rights as participants. In addition, the student mailing gave sample members the opportunity to complete and return an address update sheet. Each sample member received a lead letter, address update sheet, information leaflet, and business reply envelope (see appendix B). All locating inforination obtained from the student mailing was entered into the locator database. b."}, {"section_title": "CATI locating", "text": "Locating and tracing activities took place concurrently with efforts to gain cooperation from and interview sample members. When assigned a case, the telephone interviewer called the telephone number designated by the system as the best number (i.e., the number among all available locator numbers that appeared to have the greatest potential for contacting the sample member) and attempted to interview the designated sample member. When the person answering the call said that the sample member could not be reached at that number, the interviewer asked the person how to contact the sample member. If this query did not provide the information needed, the interviewer initiated tracing procedures, using all information available to call other contact persons in an attempt to locate the student. When all tracing options available to the interviewer were exhausted without success, the case was assigned to RTI's Tracing Operations Unit (TOPS) for intensive tracing. c."}, {"section_title": "Intensive tracing efforts", "text": "TOPS had access to both proprietary and public-domain data. It had real-time access to several consumer databases, which contained current address and phone listings for the majority of consumers with a credit history. In addition to the propriety databases, TOPS had access to various other information sources, such as data miners, commercial list houses, and NCOA via leased line. These sources provided the following searches: name, address, neighbor, business, phone matching searches, and status as decedent, incarcerated, incapacitated, or military personnel. TOPS employed these various information sources to locate respondents. A two-tiered intensive-tracing plan was used to locate B&B sample members. The first tier involved identifying sample members with social security numbers (SSNs) and processing that information through consumer database searches. If a search generated a new telephone number, that case was sent back to CATI for telephone interviewing. If a new address was generated, but no telephone number, tracers called directory assistance or accessed other databases to obtain telephone numbers for CATI. This first level of effort minimized the time that cases were out of production. All remaining cases (those lacking new information from the SSN search) underwent a more intensive level of tracing in the second tier. This approach involved the following procedures: (1) checking directory assistance for telephone listings at various addresses; (2) using electronic reverse-match databases to obtain the names and telephone numbers of neighbors and then calling the neighbors; (3) calling persons with the same unusual surname in small towns or rural areas to see if they were related to or knew the sample member; (4) contacting the current or last known residential sources such as the neighbors, landlords, current residents, tax assessors, realtors, and other business establishments related to previous addresses associated with the sample member; (5) calling colleges, military establishments, and correctional facilities to follow up on leads generated from other sources; and (6) checking various tracing Web sites. Tracers checked new leads produced by these tracing steps to confirm the addresses and telephone numbers for the sample members. When the information was confirmed, that case was returned to CATI for telephone interviewing. If the information could not be confirmed (e.g., there were no working telephone numbers or numbers for relevant neighborhood sources were unpublished), the case was reviewed by team leaders in TOPS and the RTI Telephone Survey Unit."}, {"section_title": "C. Data files", "text": "The final B&B:2000/01 field-test data file will be prepared in accordance with NCESspecified Electronic Codebook (ECB) format guidelines. Activities important to ensuring quality across data file preparation methods are described below."}, {"section_title": "1.", "text": "\nFirst-year enrollment experiences Table 4.1 presents the results of reliability analyses for the set of items pertaining to first-year enrollment experiences. Percent agreement ranges from 75.9 to 91.1 percent and the relational statistic ranges from 0.71 to 0.88. The item with the lowest reliability is the number of jobs held while enrolled during the respondent's first year of postsecondary education, with 75.9 percent agreement and a relational statistic of 0.71. This is not surprising given that the time referent for these questions is approximately 4 to 5 years in the past. Given the amount of time that had passed since the activities in question, the temporal stability of the two remaining items is quite good. "}, {"section_title": "Data availability throughout data collection period", "text": "During data collection, RTI consistently updated master data files containing completed case data. Batch processes included methods for extracting raw data from Computer-Assisted Survey Execution System (CASES) interview data files. A master data file structure was retained across all CATI applications in order to concatenate data into a single set of data containing all respondents. CATI data were extracted periodically to facilitate advance preparation of variable documentation. This also enabled review of frequency distributions and data analysis while data were still being collected."}, {"section_title": "2.", "text": "\n\nEmployment in 1999 Table 4.2 presents the results of reliability analyses for the set of items pertaining to work and income for calendar year 1999. The measures of temporal stability for income earned from work in 1999 are exceptionally high, with 96.9 percent agreement and a relational statistic of 0.93. Percent agreement is also good for the item representing whether the respondent worked for pay in 1999, but the relational statistic is lower at 0.49. This is likely because 92 percent of all respondents reported working both during the main interview and during the reinterview. Another factor contributing to the unusually high reliability of these employment and income items is the period referenced. These questions asked about employment in the calendar year in which most B&B respondents, by definition, graduated from college. Most respondents had recently started working in their first \"real\" job and likely remembered their income for that period.  3. Table 4.3 presents reliability results for items related to current and anticipated graduate enrollment. Overall temporal stability for this series of items is quite good. Percent agreement for this series of items ranges from 81.0 to 97.5, and the relational statistic ranges from 0.51 to 0.90. The most reliable item, which asked about current enrollment in a graduate program, had 97.5 percent agreement and a relational statistic of 0.90.\n"}, {"section_title": "Data editing", "text": "Data became available for editing during the data collection period, which allowed for feedback to the field on data quality and more accurate analysis of response data. As a quality check, the original skip logic was recreated to ensure that respondents followed the appropriate path within the CASES instruments. These edit checks were important for correcting errant paths that the interviewer may have followed but later corrected. Edit checks also ensured that for particular data elements, responses occurred logically. This process also allowed evaluation of the other case: when questions the respondent should have been asked were missed. Reserve codes indicated instances where raw data were updated to reflect the proper logical path. During data collection, interviewing staff was able to notify project staff of CATI irregularities via \"problem sheets\" so that problems in instrument logic could be corrected. "}, {"section_title": "Instrument design", "text": "The B&B:2000/01 field test student telephone interviews were conducted using CATI technology. In preparation for the development of the CATI instrument, a comprehensive set of data elements was developed from a thorough review of the data elements used for the B&B:93 cohort, their relationship to the NPSAS:2000 data elements, and their relevance to current research and policy issues. A preliminary set of B&B:2000/01 data elements was refined with input from the study's Technical Review Panel (see appendix A for a list of members) as well as from NCES and other Department of Education staff. The final set of data elements, presented in appendix C, was approved by the Office of Management and Budget (OMB) before data collection started. Based on the set of data elements, the CATI instrument was structured by identifying section topics and determining the progression of items within sections. Individual items were designed with several goals in mind: (1) using NPSAS:2000 and B&B:93/94 items when feasible; (2) ensuring consistency with NPSAS:2000 and B&B:93/94 items when items were not identical; and (3) identifying and preparing wording for item verifications and probes as necessary. Facsimile instruments are provided in appendix D. Instrument sections were reviewed on a flow basis by NCES and by selected contractor and subcontractor staff. As depicted in figure 2.2, the first section determined eligibility for sample members who did not participate in NPSAS:2000. The following sections collected information pertaining to postsecondary enrollment since high school completion, respondent demographics, post-baccalaureate education and employment, and experiences with teaching. To minimize the interview burden on respondents, the CATI instrument used existing data whenever feasible. Base-year data from the NPSAS:2000 field test interview were preloaded into the CATI interview; this dictated the flow of many portions of the interview. Certain questions were asked only if the data were missing from the prior interview. The CATI interviews were programmed using CASES 4.3 software. The CATI system presented interviewers with screens of questions to be asked of the respondents, with the software guiding the interviewer and respondent through the interview, automatically skipping inapplicable questions based on prior response patterns. Wording for probing and verification was suggested when a respondent provided a response that was out of range for a given item. As the CATI instrument was being designed and programmed, instrument documentation was entered into an integrated data dictionary system (DDS), which subsequently enabled users to produce deliverable data files with CATI variable documentation. An abbreviated instrument was developed for the purpose of interviewing special respondent groups such as sample members whose primary language was Spanish. The facsimile abbreviated instrument, presented in appendix D, focused on the respondent's post-baccalaureate enrollment and work experiences.   Once all CATI sections had been programmed, test cases were developed and preloaded for testing the instrument and for training telephone and field interviewers. Project staff and staff from NCES systematically tested the CATI instrument before the interviewer training. Finally, preload files containing data from NPSAS:2000 and the Department of Education databases were prepared and loaded into the CATI system to both guide the interview and assist sample member locating efforts. Data collection ensued only after all these tasks were complete."}, {"section_title": "E.", "text": ""}, {"section_title": "Training of interviewers", "text": "The field test training program was designed to maximize the trainees' active participation. Training manuals included a training guide, an interviewer's manual, and a question-by-question specification manual. Training for telephone interviewers and supervisors was conducted in March 2000 and consisted of lectures, demonstrations, and hands-on practice exercises with the instrument and online coding modules. Trainees were introduced to the procedural aspects of conducting B&B:2000/01 and were given a thorough review of the questionnaire. Interviewers were also trained in techniques for gaining cooperation with sample members, parents, and other contacts, as well as techniques for addressing the concerns of reluctant participants and avoiding refusals. A copy of the training agenda and the table of contents from the training manual are located in appendix E."}, {"section_title": "F.", "text": "Telephone interviewing CATI locating and interviewing were conducted from March 28, 2000, through July 2, 2000. CATI procedures included attempts to locate, to gain cooperation from, and to interview study sample members by telephone. For NPSAS:2000 field test nonrespondents, NPSAS and B&B eligibility determination was also necessary. A reliability reinterview consisting of a subset of items from the full instrument was conducted for a subsample of respondents (79). Locating information gleaned from the pre-CATI locating sources described above was preloaded for each case. Additionally, information previously collected through NPSAS:2000 was preloaded to personalize interviews and to reduce respondent burden. An automated call-scheduler assigned cases in the CATI sample to interviewers based on time of day, day of week, existence of precise appointments, and type of case. Scheduler case assignment was designed to maximize the likelihood of contacting and interviewing sample members. Cases were assigned to various queues for this purpose. Some of the queues included new cases, Spanish-language cases, initial refusals, and various appointment queues (firm appointments set by the sample member, appointments suggested by locator sources, and appointments for cases which were initial refusals). For each case, a calling roster determined the names and telephone numbers for the interviewers to call. The roster included school-provided and/or student-provided address information (student permanent, student local, parent, and other contacts) from the NPSAS:2000 field test. Up to six roster-lines were preloaded with contact information. New roster-lines were added as necessary during the field test as the result of CATI tracing and intensive tracing efforts.\n"}, {"section_title": "19", "text": "B&B:2000/01 Field Test Report"}, {"section_title": "3?", "text": "2. Design and Method of the Field Test Once located, some cases required special treatment. To gain cooperation from those who initially refused to participate (including locator sources who acted as \"gatekeepers,\" preventing access to the sample member), certain interviewers were trained in refusal conversion techniques. Sample members and their locator sources who spoke only Spanish, primarily located in Puerto Rico, were initially assigned to bilingual CATI interviewers."}, {"section_title": "G. Integrated management system (IMS)", "text": "All aspects of the study were under the control of an integrated management system (IMS) which consisted of several components, or modules:: This modular structure allowed for the streamlining of related tasks and resulted in a centralized, easily accessible repository for project data and documents. The Management module of the IMS contained tools and strategies to assist the project staff and the NCES project officer in managing the study. All information pertinent to the study could be found here via the World Wide Web in a secure desktop environment: schedules, monthly progress reports, project plans and specifications, information related to the Technical Review Panel (TRP), and project deliverables. Also available in the management module was the latest version of the CATI instrument for testing and review, daily Receipt Control System (RCS) module status reports, and daily data collection reports. The RCS module monitored activities related to data collection, including tracing and locating, thereby enabling project staff to perform stage-specific activities, track case status closely, identify problems early, and implement solutions effectively. Several applications used the RCS's locator data for daily tasks: The mailout program produced mailings to parent/contacts and sample members, the query system enabled administrators to review the locator information and status for a particular case, and the mail return system enabled project staff to update the locator database as mailings or reply sheets were returned or forwarded. The RCS also interacted with the Tracing operation system database, sending locator data between the two systems as necessary. The CATI module managed development of the CATI instrument within the DDS. The DDS consisted of a set of linked relational files and associated utilities for developing and documenting the instrument. Developing the CATI instrument with the DDS ensured that all variables were linked to their item/screen wording and were thoroughly documented. Also included within the CATI module was online coding software (\"user exits\") that collected detail on schools attended, enrollment, industry, occupation, and field of study. The Field Case Management System (FCMS) module facilitated activities performed by the field interviewers. The FCMS allowed field staff to conduct tracing and personal interviewing activities, to communicate with RTI staff via electronic mail, to transmit completed cases, to receive new cases, and to transmit production time and expense (PT&E) data back to"}, {"section_title": "RTI.", "text": "The Web-based Assignment/Transfer System enabled the field supervisor to make all case assignments to field interviewers as well as to track progress of cases being worked in the field."}, {"section_title": "H.", "text": "Methodological experiments and evaluation approaches 1."}, {"section_title": "Purpose of evaluation procedures", "text": "Evaluating field test procedures can lead to improved procedures for the subsequent fullscale study. Each major component of the field test was evaluated. The methodology consisted of both formative and summative evaluations. Formative evaluations were of an ongoing nature, designed to assess tasks at intermediate stages so that the effects of employing alternate methodologies could be analyzed and modifications could be made and assessed before the affected task was completed. Summative evaluations will be used to optimize procedures in the full-scale study. A summary of B&B:2000/01 field test evaluations that were planned and implemented is provided in table 2.6. Various measures were employed to assess the quality of data collection, including quality assurance (or quantitative) monitoring, qualitative monitoring, and quality circle meetings. The primary objective was to pinpoint any problems with the questionnaire and correct them early in data collection. Quality assurance monitoring assessed the quality of the telephone interviewing, with respect to question delivery and coding of responses. It is explained in greater detail in chapter 4. Qualitative monitoring evaluated whether interviewing procedures were implemented as intended and were effective. The utility of the interview items was also assessed. On occasion, monitoring revealed the need for individual interviewer retraining (e.g., better explanation of the nature of the study, or techniques for refusal avoidance) which was conducted immediately. Weekly quality circle meetingsduring which interviewers, supervisors, and project technical staff met to discuss data collection issuesproved valuable in identifying problems with the instrument as well as in building rapport among interviewers and technical staff. Summaries of the meetings were rapidly disseminated to all interviewers and supervisors so that those who were unable to attend also benefited. The study design included a component for direct evaluation of data quality. A reliability reinterview was administered to a randomly selected subsample of field test respondents to assess the short-term stability of selected items. The results of the reinterview analysis are presented in chapter 4.  Analyze silent monitoring quality control data. Analyze CATI operational parameters (e.g., numbers of calls per case, total interviewer hours per completed interview). Debrief interviewers, refusal converters, bilingual interviewers, monitors, and supervisors. Analyze administration time statistics, overall, within section, and for individual questions and blocks of questions. Analyze rates of interview nonresponse, early and subsequent break-off, types of response inconsistencies detected during interview administration, and nonresponse patterns. Analyze effect of prior receipt of financial incentive on response rates. Analyze response reliability of reinterviews for selected items (subsample). Assess feedback from mock interviews conducted with TRP members. Compare estimates for selected variables between CATI respondents and nonrespondents."}, {"section_title": "Online coding", "text": "Analyze success/accuracy of online coding of IPEDS code, industry/occupation, and major course of study. \nThe B&B:2000/01 field test instrument included tools that allowed computer-assisted online assignment of codes to literal responses for postsecondary education institutions attended, B&B:2000/01 Field Test Report 52 63 is ZEST COPY AVAILABLE major field of study, occupation, and industry. Online coding systems are designed to improve data quality by capitalizing on the availability of the respondent at the time the coding is performed. To assist with the online coding process, interviewers are trained to use effective probing techniques to ensure each response is appropriately coded. Interviewers can request clarification or additional information if a particular text string cannot be successfully coded on the first attempt, an advantage not afforded when coding occurs after the interview is complete. Because both the literal string and selected code are captured in the data file for field of study and occupation/industry responses, subsequent quality control recoding by project staff can be easily incorporated into data collection procedures. Institutional coding was used to assign a six-digit IPEDS identifier for each postsecondary institution the respondent reported attending, other than those collected during their earlier interviews. To facilitate coding, the IPEDS coding system asked for the state in which the school was located, followed by the city, and finally the name of the postsecondary institution. The system relied on a look-up table, or coding dictionary, of institutions which was constructed from the 1997-98 IPEDS IC file. Additional information in the dictionary, such as institutional level and control, was retrieved for later use (e.g., branching) once the institution was properly coded. Major field of study, occupation, and industry coding used a dictionary of word/code associations. The online procedures for these coding operations consisted of four steps: (1) the interviewer keyed the verbatim text provided by the respondent; (2) the dictionary system displayed words that were associated with the words in the text string and the interviewer was given the choice of either accepting a word that might help in terms of coding, or ignoring a word that was of no help; (3) standard descriptors associated with identified codes were displayed for the interviewer; and (4) the interviewer selected a standard descriptor that was listed. Ten percent of the major, occupation, and industry coding results were sampled and examined. The verbatim strings were evaluated for completeness and for the appropriateness of the assigned codes. None of the verbatim strings in the sample was too vague to properly evaluate. Four of the occupation and industry strings, and only one string for the major field of study, required recoding. Furthermore, none of the recoded cases resulted in a shift across broad categories. Table 4.8 shows the results of the online coding procedures.  "}, {"section_title": "Incentive experiment", "text": "The field test included a methodological experiment to determine if respondents who received an incentive to participate in the base year study (NPSAS:2000 field test) would demand an incentive to participate in the B&B:2000/01 follow-up study. The assessment was a follow-up to the incentive experiment conducted as part of the NPSAS:2000 field test. The NPSAS:2000 field test included an investigation of the impact on response rates of offering financial incentives to selected sample members, based on their survey status. Offers of financial incentives proved effective in enhancing response rates for particular categories of sample members (e.g., preliminary refusals) and was approved for use in the full-scale NPSAS survey. The experiment implemented as part of the B&B:2000/01 field test was designed to answer a series of additional questions pertaining to the B&B cohort. It was particularly important to determine what impact, if any, the offer of a financial incentive during the base year of a longitudinal survey may have on the likelihood of sample members' response to the subsequent follow-up survey. Specifically: What percentage of the sample members who were mailed an incentive during the NPSAS field test would inquire about an incentive in the B&B follow-up? Among those who inquired about an incentive, what percentage would agree (or not agree) to do the survey in the absence of an incentive? What percentage would agree (or not agree) to participate in the follow-up survey if initially offered the same incentive as in the base year? Overall, 183 B&B sample members were sent an incentive mailing, including $5, during the NPSAS field test. Of those, 132 completed the interview and received an additional $15 incentive payment, while the remaining 51 did not complete the interview (25 were located in NPSAS but refused or time ran out; and the remaining 26 were not located in NPSAS). The B&B incentive experiment focused only on those 183 sample members who were mailed an incentive during the NPSAS field test (whether they completed the interview or not). It is important to decide how to handle these students in the full-scale B&B follow-up study. A split-sample experimental design was implemented as part of the B&B follow-up field test (see figure 2.3): (1) The 183 sample members who were mailed an incentive letter as part of the NPSAS data collection effort were stratified by respondent/nonrespondent status, school level, and school control. (2) Based upon this stratification, cases were allocated to a \"control\" group or an \"experimental\" group so that the two groups were identical in terms of respondent status, school level, and school control characteristics. A variable denoting the sample members' experiment status (control or treatment) was preloaded into the CATI system. The offering of the incentive (or lack thereof) was presented to the sample members as follows: Sample members in the \"experimental\" or \"treatment\" group received an incentive letter with $5 cash. The letter explained the study and informed the sample members that they would receive a check for an additional $15 upon completion of the full interview. The letters were sent via express mail approximately 7 days after the initial student mailing was sent. Sample members in the \"control\" group did not receive an incentive mailing (they did, however, receive the initial student lead letter and information leaflet sent to all sample members). Sample members who inquired about the incentive during an interview were told: \"I'm sorry, but for this study we are not offering an incentive payment.\" The interviewer then recorded whether the sample member agreed to continue the interview or terminated the interview. "}, {"section_title": "Student locating and interviewing", "text": "The conduct of interviews for list-based sample surveys such as B&B:2000/01 involves two sequential steps: locating (identifying an initial telephone number at which the sample member can be reached) and interviewing (convincing the sample member to cooperate and conducting the interview). The level of time and effort required to complete these steps with sample members can vary considerably. Some sample members may be reached and interviewed on the first attempt at contact. Others may require considerable tracing (contacting of parents, former roommates, etc.) before they are successfully located and interviewed. The time allowed for the B&B:2000/01 field test was more limited than will be the case in the full-scale survey. Therefore, procedures for those most difficult to locate and interview were constricted, with consequent adverse impact on final locating and CATI response rates. However, a relatively high percentage of sample members were located and interviewed as part of the B&B:2000/01 field test, given the time constraints associated with conducting a field test. This is at least partially because the B&B:2000/01 field test is a relatively quick follow-up. Base-year data were collected for respondents only one year earlier, which aids the success of locating efforts. Even for this highly mobile population of recent college graduates, locating is much easier one year after initial contact than in a later follow-up. a."}, {"section_title": "Lead letter and locator mailing to students", "text": "One week before the start of data collection for the field test, sample members were sent an advance mailing that included an address update sheet. Each sample member was asked to review, correct, and return the sheet. Letters were mailed to 706 sample members, with 149 cases out of a total sample of 855 unmailed because of incomplete address information. Of these 706 sample member letters, 115 address update sheets with new or confirmed information were received (14 percent of the total sample). Results of locating and interviewing effort Figure 3.1 presents a schematic of the outcomes of student locating and interviewing and related case-resolution activities. Student interview data were collected exclusively by CATI. No field interviewing or questionnaire mailings were conducted as part of the field test. Data collection for the field test lasted approximately 14 weeks, from March 28 through July 2, 2000. As shown in figure 3.1, attempts were made to locate 855 student sample members. Overall, 769 (90 percent) were located, 71 (8 percent) were not located, 6 (<1 percent) were considered \"exclusions,\"I and 9 (1 percent) were determined to be ineligible for the study based on their responses to the eligibility questions in the questionnaire. Student interviewing results are also shown schematically in figure 3.1. A total of 695 (of the 840 remaining cases after removing the exclusions) were interviewed. The majority of these cases (662) completed the entire interview, while 33 completed only a partial or abbreviated interview. A small number of the partial interviews (4) were classified as such because the respondent broke off after completing part of the interview. A break-off represented an explicit or implicit refusal or the arising of some other matter requiring the attention of the respondent, but such cases could not be converted or recontacted to complete the interview by the end of the data collection period. A substantial number (29) of these partial interviews, however, resulted from the administration of an abbreviated interview that consisted of a minimal set of questions from the full interview. Over half of this group represented interviews with Spanish-speaking respondents. Of the remaining 160 cases not interviewed, 71 were not located. A total of 74 potentially eligible students who were located were not interviewed. Of these, 59 were explicit final refusals for which subsequent attempts at interviewing were determined to be infeasible or unwise. Not interviewed cases also included .15 sample members for whom time ran out before they could complete the interview; such cases clearly reflect, at least in part, the constricted data collection period.2 An overall student CATI response rate for the B&B:2000/01 field test can be calculated as the number of respondents interviewed divided by the initial sample size minus the exclusions: Student CATI response rate = 695/(855-15) = 82.7 percent. B&B exclusion cases consisted of those whose status (generally obtained through some contacted third party) was determined to be such that attempts at locating/interviewing them during the CATI operational period would be futile. The designation \"exclusions\" indicates that, even though the status of the case was successfully resolved, such cases were considered \"out-of-scope\" for locating and interviewing operations. Among the six B &B:2000 /O1 field test sample members classified as exclusions, five were determined to be out of the country for the duration of the data collection period and one sample member was verified as deceased. 'This group likely contained an unknown number of implicit refusal casesi.e., individuals who after first contact used answering machines or friends/relatives as gatekeepers, as well as those who continued to make (and then break) appointments for an interview \"in the future. Locating and response rates for base-year respondents and nonrespondents Table 3.1 provides results for the B&B:2000/01 field test student locating and interviewing (for those located) by respondent status in the base-year study (NPSAS:2000 field test).3 Some significant differences in locating and interviewing rates are evident. In terms of locating, just over 94 percent of the NPSAS field test respondents were located, compared to approximately 74 percent of NPSAS nonrespondents. There were also differences in interview rates among the two groups. Among base-year respondents, 93 percent of the located sample members successfully completed interviews during the B&B follow-up field test. The interview rate was nearly 30 percent lower for NPSAS:2000 field test nonrespondents, with 66 percent of the located sample members completing interviews. As shown in table 3.2, nearly one-quarter (23.6 percent) of the completed interviews for the B&B field test were obtained during the first week of interviewing. By the end of the third week of data collection, over half (51.4 percent) of the interviews had been completed. After that, the locating and interviewing effort was much more difficult and time consuming, with the remaining interviews being collected over the last 11 weeks of data collection. 3 The statistics in table 3.1 exclude the nine B&B:2000/01 ineligible sample members determined during CATI interviewing and the six exclusions; they do not exclude any potential ineligibles likely to be part of the unlocatables.  d."}, {"section_title": "Source of locating information for completed interviews", "text": "Locating students in a longitudinal study to interview them is a complex task requiring multiple sources of information. Leads developed through one source may need to be verified using another data source or locating technique. Table 3.3 presents the original source of the telephone number at which the interview was completed. Tracing leads obtained via telephone during CATI data collection were the most important sources of these numbers, accounting for 211 of the final 695 completed interviews, or 30 percent. The remaining 70 percent came from a variety of other sources, including locating information collected during the base-year (NPSAS:2000) study (28.4 percent), pre-datacollection activities using NCOA or Telematch address and telephone number processing (23.3 percent), returns from student prenotification letters (9.2 percent), centralized tracing by TOPS (5.6 percent), and, finally, student call-ins to the study's toll-free number (3.2 percent).  e."}, {"section_title": "MST COPY AVAMLABLE", "text": ""}, {"section_title": "Student prenotification letter and address updates", "text": "Not surprisingly, student contact and interview rates varied considerably based on whether or not sample members returned the address/telephone update sheet sent to them as part of the prenotification mailing (see table 3.4). While the return rate for these sheets was modest (with 114 of 840 eligible sample members returning update sheets, or 13 percent), the contact and interview rates for those who did return the forms was nearly universal. Of the 114 sample members who returned an update sheet, 113 were located by interviewers. Among those cases, 110 (97.4 percent) completed the field test interview. The four who did not complete the interview were refusals. In short, while the percentage of update forms returned was relatively low, the contact and interview rates among those who did return such forms was extremely high. "}, {"section_title": "E-mail contact with sample members", "text": "As part of the B&B:2000/01 field test effort, the use of e-mail as a means of contacting otherwise hard-to-reach sample members (i.e., those requiring 10 or more call attempts) was evaluated. E-mail addresses were collected from sample members both during the base year interview (NPSAS:2000) and as part of the update sheets sent to sample members. Approximately 8 weeks into data collection, these e-mail addresses were used to contact sample members who had not yet completed the B&B:2000/01 field test interview. The e-mail message briefly described the study, indicated our previous attempts to reach the sample member, and encouraged the student to contact us via telephone or e-mail to complete the survey or to establish a date and time for an interviewer to call. By this stage of data collection, there were 73 sample members for whom we had a valid e-mail address (i.e., we had an e-mail address and when the message was sent it was not returned as \"undeliverable\"the sign of a \"bad\" e-mail address). Among these, 61 (83.6 percent) were ultimately located and nearly 87 percent of those contacted completed the interview (see table 3.5). E-mail appears to have been an effective mode of communication for establishing contact with otherwise hard-to-reach sample members. "}, {"section_title": "Intensive locating during data collection", "text": "Intensive tracing efforts were required for cases in which preloaded CATI locating information failed to result in contact with the sample member. These intensive tracing activities were as follows. Cases with valid addresses that were not located during the CATI operations were sent to FastData for telephone number updates. New information was then returned to CATI for further follow-up. Cases returned from FastData without additional information were assigned to TOPS for intensive tracing. Cases without valid mailing addresses were also assigned to receive intensive tracing from TOPS. The final locate and interview rates for cases requiring centralized tracing are provided in table 3.6. Of the 141 cases sent to the tracing unit for intensive locating efforts, 85 were located (60.3 percent) and, of those located, nearly 85 percent were interviewed. In sum, although not all sample members were found using centralized tracing techniques, these techniques did result in contracts for a majority of the cases in which they were used. For hard-to-locate sample members, generally no single source of information is adequate to achieve the level of locating required. Rather, a successful locating effort requires blending multiple sources of information. Centralized tracing was conducted as part of the field test for cases in which telephone leads were exhausted during the CATI phase of data collection. Table 3.7 provides an overview of the sources used during intensive tracing of the hard-to-reach B&B:2000/01 field test sample members. Note that although the table provides information on the number and percent of sample members who were ultimately located when a particular source was used, most of the cases were traced using multiple sources. Contact was made with sample members in percent of the cases where information from consumer databases was used as part of the locating effort. Directory assistance was a factor in locating about the same percentage of hard-to-locate sample members (58.5 percent). Over half of the cases for which address search databases (54.9 percent) or reverse telephone look-ups (54.4 percent) were used resulted in contact with a sample member. If more extensive searches were required (such as generic database name searches or Internet searches), the percentage of sample members located was reduced. These techniques were used only if previous search efforts failed to provide sufficient locating information. Fewer than half of those cases for whom these more extensive tracing techniques were required were ultimately located (47 percent for name searches and 43.4 percent for Internet searches). Finally, just one in five (22.2 percent) of those for whom neighbor searches were required were located.4 2."}, {"section_title": "Refusal conversion efforts", "text": "Refusal conversion procedures were used to gain cooperation from individuals who refused to participate when contacted by telephone interviewers. Refusals came not only from sample members, but also from spouses, housemates, parents, and other gatekeepers. When either a sample member or a gatekeeper refused to participate in the locating or interviewing effort, the case was referred to a specially trained refusal-conversion specialist in the Telephone Survey Unit. There were 195 initial refusals among the field test sample (23 percent of the initially fielded sample of 855). Most refusals came from sample members (122 refusals), although 73 refusals were by other contacted individuals (see table 3.8). In all, 59 percent of the cases with initial refusals were successfully converted into completed interviews. The conversion rate was 52 percent among refusing sample members. The success of converting refusals varied according to the sample member's response status in the base-year study (see table 3.9). Among respondents to the NPSAS:2000 field test interview, 68 percent of the sample members who initially refused to be interviewed (or whose gatekeeper refused) ultimately completed the B &B:2000 /O1 field test interview. In contrast, only 26 percent of the base-year nonrespondents were successfully converted. 3."}, {"section_title": "Reliability reinterview", "text": "A subsample of eligible sample members who completed the B&B:2000/01 field test interview was selected to participate in a reliability reinterview, containing a subset of items from the initial interview. A random selection algorithm was programmed directly into the CATI instrument. Sample members selected for the reinterview were informed of their selection at the end of the initial interview and allowed an opportunity to agree to the reinterview or to refuse it at that time. A total of 83 respondents were selected for the reliability reinterview. Due to the built-in delay in administering the reinterview (a delay of approximately 3-4 weeks from the initial interview) and the need to complete reinterviews during the same time frame as the field test interview, those selected for reinterview were more likely to be those sampled and interviewed early during the data collection period for the field test. Such individuals were those most easily located and convinced to participate in the initial interview. Consequently, the reported agreement and reinterview rates are probably higher than if the reinterview respondents had been sampled subsequent to the initial data collection effort."}, {"section_title": "4.", "text": "\n"}, {"section_title": "Interview burden and effort", "text": "This section of the field test report reviews the effort and burden associated with the B&B:2000/01 field test student interview. We examine the interview's length by considering the timing analysis statistics. This information is useful because it provides evidence that can reduce respondent burden, reduce data collection effort and cost, and improve data quality. Then we consider the effort required to locate and interview sample members for the study using the average interview time. During CATI instrument development, project staff embedded time stamps at the start and end of the interview, as well as the beginning and end of each interview screen, which could include up to eight related items. The time stamps measured the elapsed time to complete each segment of the interview, and enabled project staff to monitor the time required to complete specific interview items, the online coding programs, sections of the interview, and the entire interview. The time (in minutes) needed to conduct a student interview is shown by interview section in table 3.10. Sections are listed in the table in the order in which they were presented. Certain sections of the interview applied to selected groups of respondents (see figure 2.2,) so timing results are presented for the overall cohort, and by subgroup. For example, Section A was designed for base-year nonrespondents, so the number of cases in that group was less than for the rest of the instrument. Respondents who were currently teaching skipped the post-baccalaureate employment section and proceeded directly to the teaching section. Table 3.10 presents timing results for the B&B:2000/01 field test cohort. Overall average administration time to complete the student interview was 18 minutes. There was no difference in average completion time due to base-year response status (see table 3.11). Both respondents and nonrespondents to the NPSAS:2000 field test took an average of 18 minutes to complete the interview.5 For respondents who had taught since graduating (see table 3.12), the average interview time was 21 minutes compared to 17 minutes for those who had not taught. The Technical Review Panel reviewed the administration time and then recommended certain items for deletion in the full-scale study. Items to be excluded typically showed a lack of temporal stability or extremely low variance of responses (see chapter 5). Interview administration time, however, reflected only a small fraction of the time required to obtain a completed interview. Time was spent by locator/interviewers in locating, scheduling call-backs, attempting refusal conversion, and other related activities. This time was spent whether or not interviews were obtained. The average locator/interviewer time requirement for each completed interview was slightly more than 2 hours. 'All the original Section C items were moved to other sections. To avoid introducing confusion into the CATI programming, however, the remaining sections have not been relettered. NOTE: A section was considered complete if the amount of time to complete the section was greater than zero and the section completion flag was set. Section outliers were removed from the timing calculations (2 in section A, 1 in section B, 3 in section D, 1 in section F, and 2 in section G).  'All the original Section C items were moved to other sections. To avoid introducing confusion into the CATI programming, however, the remaining sections have not been relettered. NOTE: A section was considered complete if the amount of time to complete the section was greater than zero and the section completion flag was set. Section outliers were removed from the timing calculations (2 in section A, 1 in section B, 3 in section D, 1 in section F, and 2 in section G).  All the original Section C items were moved to other sections. To avoid introducing confusion into the CATI programming, however, the remaining sections have not been relettered. NOTE: A section was considered complete if the amount of time to complete the section was greater than zero and the section completion flag was set. Section outliers were removed from the timing calculations (2 in section A, I in section B, 3 in section D, 1 in section F, and 2 in section G "}, {"section_title": "Interviewer hours", "text": "During the B&B:2000/01 field test, telephone interviewers worked a total of 1,374 hours to obtain completed interviews from 695 sample members who completed full or partial interviews and 79 individuals who completed reliability reinterviews in CATI. Excluding the time each interviewer spent in training or attending quality circle meetings, and the additional time that team leaders and other senior telephone interviewers allotted to supervision and monitoring, this represented an average of 2.0 hours per completed interview. Since the average time to administer the interview was just over 18 minutes, most interviewer time clearly was spent in other activities, primarily in locating and contacting. In addition to the telephone interviewers, supervisors and monitors worked 488 hours during the field test, or approximately 1 hour for every 3 hours of telephone interviewing. Another 202 hours were attributed to refusalconversion training, quality circle meetings, and debriefmg meetings. The level of effort required to complete interviews varied considerably across shifts and days of the week. As shown in table 3.13, the lowest \"hours per complete interview\" average was obtained on Mondays, particularly during the evening shift (average of 1.5 hours per complete). The highest hours per complete were incurred on Thursdays during the day shift (average of 2.7 hours per complete). The relatively small size of the interviewing staff on a particular shift (ranging from two to five interviewers per shift) makes it difficult to determine whether these variations are due to the availability (or lack thereof) of sample members during those periods or to variations in interviewer efforts on those shifts. "}, {"section_title": "Number of Calls", "text": "Telephone interviewers made 15,347 telephone calls during the field test, with an average of 17.9 calls per sample member.6 An average of 15.4 calls was required to obtain the 695 completed cases. Slightly more than half of the completed telephone interviews (52.5 percent) were completed with fewer than 10 calls, 32.0 percent required 10 to 29 calls, and 15.5 percent of the completed cases required 30 or more call attempts. Of the 15,347 calls made, 1 in 5 resulted in contact with an individual. Nearly half of the contacts (47.1 percent or 7,234 calls) resulted in contact with an answering machine, reflecting the heavy usage of such devices among this population. An answering machine was reached at least once (and often multiple times) for three of every four B&B sample members. The remaining one-third (33.7 percent) of the calls made were other noncontacts (busy, ring/noanswer, fax line, pager, etc.)."}, {"section_title": "7.", "text": "Answering machines, messages, and call-ins Answering machines are an increasing problem for all studies conducted by telephone. Whether the machines are used to screen unwanted calls or used to facilitate \"on the go\" lifestyles, answering machines pose an obstacle to contacting sample members and completing interviews. On average, the higher the percentage of calls resulting in an answering machine disposition, the greater the average number of call attempts required to complete the interview. Where no machine was encountered, an average of 6.0 calls was required to obtain a completed interview. If fewer than 50 percent of the call attempts reached an answering machine, it took an average of 14.5 call attempts to complete the interview. Finally, among cases where an answering machine was reached on 50 percent or more of the call attempts, it took on average 24.9 call attempts to complete an interview. Answering machines are not, however, insurmountable barriers. Table 3.14 provides the contact and interview rates for hard-to-reach cases. As the table shows, ultimately we were able to contact over 90 percent of those hard-to-reach sample members where an answering machine was encountered on one or more call attempts. This high success rate holds even for cases where a machine was encountered on 50 percent or more of the call attempts. Likewise, completed interviews were obtained from 85 percent of these hard-to-reach cases, despite the interviewer reaching an answering machine on one or more attempts. Answering machines can also serve as a vehicle for making contact with a sample member. Messages left on answering machines are the functional equivalent of \"electronic lead letters,\" notifying sample members of an impending call from an interviewer. During the B&B field test, the following message was left the first and fourth time an answering machine was encountered at a particular telephone number: \"I am calling for the U.S. Department of Education about a research study in which (sample member name) has been selected to participate. Please ask (him/her) to call \"(name)\" at 1-800-555-5555, and refer to ID number \"(unique ID)\" to complete the study. Thank you.\" The message (1) notified the sample members that they had been selected for a research study and (implicitly) that they would be recontacted in the near future, and (2) encouraged them to call in to complete the interview. As shown in table 3.15, a sizable portion of the field test sample did call in to complete the interview. In total, 165 callers used the toll-free number established for the study. Among these, 86.1 percent (142 cases) completed the interview upon calling in. Among those who did not complete the interview when they called in, the calls were a relatively even mix of refusals by the sample member, contact persons calling to provide new locating information for the sample member, or contacted individuals calling to say they did not know the sample member or did not know where to contact him or her.  "}, {"section_title": "Overview of the incentive experiment", "text": "As explained in section 2.H.1, the incentive experiment implemented during the B&B:2000/01 field test was designed to examine the likelihood that sample members in the follow-up study who received an incentive payment in the base-year study would respond to a request for a follow-up study. In particular, the questions of interest involved (1) whether or not sample members who received a modest incentive ($20) for their participation in the NPSAS:2000 field test study would participate in the follow-up without an incentive, and (2) if such respondents received an incentive at the beginning of the follow-up study, whether they would respond more quickly than those not receiving an incentive, thereby lowering the level of effort for such cases. NPSAS:2000 sample members who received an incentive in the base-year study were randomly assigned to one of two groups for the B&B field test: an incentive group and a control group. Incentive group members received a $20 incentive the first week of data collection to complete the interview; control group members received no incentive. All other survey activities, such as contacting/interviewing, tracing/locating efforts, and the like, were similar for the two groups. Sample members selected to receive an incentive were sent a personalized letter delivered by express overnight service. Enclosed with the letter was a $5 bill and instructions for completing the interview by calling a toll-free telephone number. After successfully completing the B&B:2000/01 interview, whether by call-in to the toll-free number from the sample member or through a subsequent call from a telephone interviewer, each respondent in the treatment group received an additional payment of $15 by personalized check."}, {"section_title": "Results of the incentive experiment", "text": "Of the 179 cases in the field test who were eligible for the experiment, 90 cases were selected to receive an incentive, and 89 cases were assigned to a control group. As shown on "}, {"section_title": "54", "text": "BEET COPY AVAILABLE completed the interview during the time frame in which the experiment was conducted. Among the control group, completed interviews were obtained by half of the sample members (45 of 89 cases). The difference between those who received an incentive and those who did not is not statistically significant. In terms of level of effort (see table 3.17), it took fewer call attempts to obtain completed interviews with sample members who received the incentive mailing at the outset of the study compared to those in the control group (9.6 versus 12.2 call attempts). These results, however, are not statistically significant either. Finally, the experiment was also designed to allow interviewers to note whether or not sample membersparticularly those in the control groupinquired about the incentive at the outset of the study. Among respondents who did not receive an incentive mailing, only six asked about the possibility of receiving an incentive for their participation in the B&B:2000/01 followup: two refused to participate and four continued to participate upon learning that they would not be eligible for the incentive.  \nInterviewers were also able to use the Fl function key for quick access to student information, a calculator, roster lines, and case-level comments. Problem sheets: Problem sheet issues and types of details to include were also discussed."}, {"section_title": "Reliability of interview responses", "text": "The temporal stability of a subset of interview items was evaluated through reinterview. Reinterviews were administered to a randomly selected subsample of 79 respondents who completed the full interview within the first 6 weeks of data collection and agreed to participate in the reinterview. The reinterview included items that were newly designed for the B&B:2000/01, or revised since being used in either B&B:93/94 or NPSAS:2000. The items assessed facts rather than attitudes, because valid and reliable responses needed to remain stable for the time between initial interview and reinterview. A paper facsimile of the reinterview is provided in appendix D. Reinterview respondents were contacted 5 to 7 weeks after they completed the initial interview, and their responses in the initial interview and the reinterview were compared. Two measures of temporal stability were computed for all paired responses. The first, percent agreement, was determined in one of two ways. For categorical variables, the interview/reinterview responses agreed when there was an exact match between the two responses. For continuous variables, the two responses were considered to match when their values fell within one standard deviation unit of each otheri The second measure evaluated temporal stability using one of three relational statistics: Cramer's V, Kendall's tau-b (T3), and the Pearson product-moment correlation coefficient (r). Which of the three statistics was used depended on the properties of the particular variable. That is, Cramer's V statistic was used for items with discrete, unordered response categories (e.g., yes/no responses). Kendall's tau-b (tb) statistic, which takes into account tied rankings,2 was used for questions answered using ordered categories (e.g., never, sometimes, often). For items yielding interval or ratio scale responses (e.g., income), the Pearson product-moment correlation coefficient (r) was used. Analyses were based on the respondents who completed reinterviews. Effective sample sizes are presented for all results because analyses needed to be restricted to cases with determinate responses to the relevant items in both interviews. Because not all items were applicable to all respondents (e.g., some questions were asked only of graduate students or those currently employed), variation exists in the number of cases on which the reliability indices were based for the items considered. For administering the reinterview, information from the initial interview was preloaded to ensure that school-specific and job-specific items were asked for the same school and job across the two interviews."}, {"section_title": "0.84", "text": "Analyses were conducted only for respondents with determinate responses on both the 'nitial interview and the reinterview; not all questions were applicable to all respondents. 2 Unless otherwise indicated, this percentage reflects an exact match of the paired responses. 3 Unless otherwise indicated, the relational statistic used here is Kendall's tau-b (T13). 4 This percentage reflects values that fall within one standard deviation unit of each other. 5 The relational statistic used here is Pearson's product moment correlation coefficient, r. NOTE: Analyses are based on 79 respondents to the reliability reinterview."}, {"section_title": "Graduate enrollment", "text": "Current enrollment in an undergraduate program, a vocational program, or a nondegree program also has very high percent agreement (95.5 percent) but a low relational statistic (0.55). The overwhelming majority of respondents (92 percent) reported no enrollment in these types of programs in both the main interview and the reinterview. However, of those who said that they were enrolled in a nongraduate program during the main interview, 50 percent reported no enrollment by the time of the reinterview. The main interviews were conducted from March through July, and the reinterviews were conducted during the months of May and June. It is possible that the observed response reversal was due to real change; respondents could have been enrolled in a term that ended before the reinterview took place. The least reliable item in this series asks whether respondents claimed the Lifetime Learning Tax Credit when they filed their 1999 taxes. For this item, percent agreement is relatively high at 81.0 percent, but the relational statistic is only 0.51. The Lifetime Learning Tax Credit is still relatively new, and many respondents did not know what it was. The response options for this question included two different values for no: \"0\" = \"Never heard of it (the tax credit),\" and \"2\" = \"No.\" Evaluation of the reinterview data shows that all of the respondents who initially reported not having heard of the tax credit, simply reported not having taken the tax credit when reinterviewed. This is more a function of the structure of the reinterview than the actual response stability of the question. Of those who initially reported having taken the tax credit, 25 percent reported not having taken the credit during the reinterview. Of those who initially reported not taking the credit, all responded consistently during the reinterview. Respondents' plans to enroll in graduate school in the future have high percent agreement (84.6 percent) and a marginally acceptable relational statistic (0.68). Of the respondents who originally reported that they plan to enroll in a graduate program in the next 10 years, 17 percent changed responses by the time of the reinterview. Only 10 percent of those who initially reported that they did not plan to enroll reversed responses between interview and reinterview. "}, {"section_title": "Current employment", "text": "Measures of temporal stability for items about current employment are presented in table 4.4. Overall temporal stability for these items is mixed. Percent agreement ranges from 69.8 to 100.0 and the relational statistic ranges from 0.58 to 1.00. The indicator of whether or not respondents are currently teaching shows perfect reliability. Reliability measures for the items representing the number of employees working for the respondents' company and whether the respondents' current job is related to their undergraduate major are very good. Percent agreement is 86.8 and 89.2, and the relational statistic is also very high for both (0.89 and 0.87, respectively.) The least reliable question in this series was that pertaining to flexible job schedules. Percent agreement is only 69.8 and the relational statistic is 0.58. Evaluation of the interview and reinterview data shows that there was quite a bit of response instability. Of those who initially reported having inflexible job schedules, 28 percent reported having a \"somewhat flexible\" schedule by the time of the reinterview. Of those who originally had \"somewhat flexible\" job schedules, 25 percent changed responses by the reinterview and reported \"very flexible\" job schedules. Among those who indicated having \"very flexible\" job schedules during the main interview, 17 percent reported having \"somewhat flexible\" schedules at the time of the reinterview. It is possible that working conditions at the time of the interview influenced responses to this question. For example, a \"very flexible\" schedule might not seem so flexible when things are really busy. The item that asked respondents if they would consider their current job to be a career job has only moderately acceptable reliability. Percent agreement is 76.5 and the relational statistic is 0.58. Most respondents (65 percent) reported that their current job was a career job during both the main interview and the reinterview. "}, {"section_title": "LE", "text": "Of those who did not indicate that the current job was a career job, the distribution of interview and reinterview responses was spread among the remaining responses without much of a pattern. There were several problems with the administration of this question in the field test. First, the question was read as if it were a \"yes/no\" question, and if the response was \"no,\" then interviewers were instructed to probe and code the answer. However, respondents had difficulty understanding the intent of the question, so they did not know how to respond to the probe. Second, the remaining response options (other than \"yes\") were not mutually exclusive. It could have been that respondents were working in their current job to \"pay the bills\" and to \"prepare for graduate school,\" which substantially reduces response consistency over time. For the fullscale survey, this item will be revised so that (1) the intent of the question is more clear, and (2) the response options will be mutually exclusive. "}, {"section_title": "Comparison of CATI respondents with nonrespondents", "text": "Using institutional record data obtained during the base year study (NPSAS:2000), we were able to compare the distribution between B&B follow-up CATI respondents and nonrespondents for selected items. Table 4-5 shows a comparison of the B&B:2000/01 field test CATI respondents and nonrespondents for seven variables. Age is the only variable with a significant difference between the distribution of the respondents and nonrespondents, suggesting the possibility of nonresponse bias associated with the variable. For some categories within all of the primary variables except race/ethnicity, there are significant differences between CATI respondents and nonrespondents, also suggesting the possibility of nonresponse bias. For example, a higher percentage of nonrespondents are male than are respondents, and a lower percentage of nonrespondents are federal aid recipients than are respondents. An extensive nonresponse bias analysis is planned for the full-scale survey; however, since the field test data were not used to make population inferences, more extensive nonresponse bias analyses were deemed unnecessary.  "}, {"section_title": "Indeterminacies among CATI respondents", "text": "Special keyed entry (of F3 or F4) by the interviewers allowed the CATI interview to accommodate responses of \"don't know\" and refusal to every item. Refusal (RE) responses to interview questions were most common for items considered sensitive by respondents, while \"don't know\" (DK) responses may have resulted from a number of potential circumstances. The most obvious reason a respondent will offer a DK response is that the answer is truly unknown or in some way inappropriate for the respondent. DK responses may also be evoked when (1) question wording is not understood by the respondent (with no explanation by the interviewer), (2) the respondent hesitates to provide a \"best guess\" response (with insufficient prompting from the interviewer), and (3) a respondent implicitly refuses to answer a question. RE and DK responses introduce indeterminacies in the data set and must be resolved by imputation or subsequently dealt with during analysis. They need to be reduced to the greatest extent possible. Overall item nonresponse rates were low, with only seven items containing over 10 percent missing data. These items are shown in table 4.6, and are grouped by interview section. Item nonresponse rates are calculated based on the number of sample members for whom the item was applicable and asked. Items with the highest rates of nonresponse were those pertaining to income. Many respondents were reluctant to provide information about personal and family finances and, among those who are not reluctant, many simply did not know. In addition, the items pertaining to the Lifetime Learning Tax Credit also garnered a high number of DK responses. These DK responses are most likely attributable to respondents' unfamiliarity with the tax credit because of its relatively recent implementation. Table 4.6Student interview item nonresponse for items with more than 10 percent \"don't know\" or \"refused\" "}, {"section_title": "Help text", "text": "Online help text was available for every screen in the CATI instrument. Having additional information available at the touch of a key (F10) was very beneficial to interviewers, particularly at the beginning of data collection, to immediately alleviate any confusion with questions while they were still on the telephone with the respondent. Help text screens displayed information designating to whom the item applied, type of information that was requested in the item, and definitions of words or phrases in the item. Counters were used to determine the number of times each help screen was accessed, making it possible to identify items that were confusing to interviewers or respondents. Table 4.7 presents CATI items having the highest rates of help text usage, along with their rates of indeterminacy. An analysis of the number of help text accesses revealed seven items for which the help text was accessed more than 10 times. The items pertaining to the Lifetime Learning Tax Credit collected the most accesses to help text (88 out of 664 times the item was administered), almost certainly because of student unfamiliarity with the tax credit. The help text included a thorough explanation of the Lifetime Learning Tax Credit that telephone interviewers were able to read to respondents unfamiliar with the credit. 'The rate presented is the number of times the help text for each item was accessed, divided by the number of times that particular item was administered. 2The rate of indeterminancy is the number of \"don't know\" and \"refused\" responses divided by the number of times the item was administered. A number of questions containing confusing terms or phrases were identified by their high counts of help text access. These items included questions about teacher internships, remedial courses, and employer benefits. The available help text with term definitions was vital in helping telephone interviewers explain any unknown terms to respondents. As a result, respondents were able to better understand and answer the survey items."}, {"section_title": "D.", "text": "\n"}, {"section_title": "CATI quality circle meetings", "text": "As mentioned in section 2.H, Quality circle meetings were vital components of the field test operation and evaluation. During these regularly scheduled meetings, interviewers, supervisors, and project technical staff met to discuss issues pertinent to locating respondents and conducting CATI interviews in the most effective manner. These meetings proved to be a good tool for communication, as they provided a forum to discuss many elements of the CATI instrument. Telephone interviewers attended the quality circle meetings on a rotating basis to ensure representation of various experiences, opinions, and challenges faced. Summaries of discussions and decisions were distributed to all telephone interviewers and supervisors in a newsletter. An electronic copy of this newsletter was sent to project staff not in attendance so those who did not attend the meeting could also benefit. The quality circle meetings were instrumental in providing prompt and precise solutions to problems encountered by interviewers. Several modifications were made to the CATI instrument as a result of these meetings, including wording changes to clarify items for respondents. Quality circle meetings not only helped interviewers be more effective in interviews, but also gave project staff feedback that was influential in making the survey extremely clear for respondents and interviewers alike. The feedback and resulting changes ensure that any CATI issues that were problematic in the field-test instrument will be modified and improved in the full-scale study."}, {"section_title": "Some of the issues covered in quality circle meetings included:", "text": "Changes to the instrument: Minor modifications to the instrument which were made after interviewer training were explained and demonstrated to be sure interviewers were aware of these changes and could work with them effectively. Instrument logic: Concerns about the instrument path logic were raised, resulting in modifications to the instrument based on telephone interviewer input. For example, interviewers found that students who were enrolled in school and working part-time often received questions relating to job benefits. Because students who work part-time do not often receive benefits, a change was made in CATI to route these respondents around the benefit items. Item wording: Misinterpretation of questions was addressed consistently. For example, respondents often misinterpreted \"Other than [BA school], have you attended any other colleges or postsecondary schools since you graduated from high school?\" because they did not include graduate and/or professional schools when answering this item. The item was changed to read \"Other than [BA school], have you attended any other colleges or postsecondary schools, including graduate and professional schools, since you graduated from high school?\" to eliminate confusion and to collect the necessary information. Help screens: Interviewers were reminded of the help text feature, which was available for every CATI item through the F10 function key. The help text screens provided additional explanation to allow interviewers to verify the intent of questions, as well as definitions of terms with which the interviewer or respondent were not familiar."}, {"section_title": "Quality assurance CATI monitoring", "text": "Monitoring of telephone data collection leads to better interviewing and better-quality survey data as well as to improvements in costs and efficiency in telephone facilities. Monitoring in the B&B:2000/01 field test helped to meet four important quality objectives: (1) reduction in the number of interviewer errors; (2) improvement in interviewer performance by reinforcing good interviewer behavior; (3) assessment of the quality of the data being collected; and (4) evaluation of the overall survey design for full-scale implementation. Monitors listened to up to 20 questions as the interviews were in progress and, for each question, evaluated two aspects of the interviewer-respondent interchange: whether the interviewer (1) delivered the question correctly and (2) keyed the appropriate response. Each of these measures was quantified, and daily, weekly, and cumulative reports were produced for the study's IMS. During the data collection period, 1,0793 items were monitored. The majority of the monitoring was conducted during the first half of data collection. Toward the end of data collection, monitoring efforts were scaled back due to the lighter caseload being worked by telephone interviewers, the greater experience of the remaining interviewers, and the satisfaction by project staff that the process was in appropriate control. Figure 4.1 shows error rates for question delivery; figure 4.2 shows error rates for data entry. Both presentations provide upper and lower control limits for these measures.4 Throughout the monitoring period, error rates remained within acceptable limits, typically below 1 percent. Among the 1,079 items observed, there were two CATI question delivery errors and nine data entry errors.  Time period (In weeks) Figure 4.2Monitoring error rates for CATI data entry ."}, {"section_title": "13", "text": "Time period (in weeks) Recommendations for the Full-Scale Study The B&B:2000/01 field test was successful in providing useful information with respect to planning for the full-scale study. While many aspects of the survey design and instrumentation worked quite well, some field test outcomes and evaluation results, documented in chapters 3 and 4 of this report, justify procedural and substantive modifications to the full-scale survey implementation. Major recommendations are summarized below by topical area."}, {"section_title": "A.", "text": "Sampling of baccalaureate recipients 1."}, {"section_title": "Change in eligibility requirements", "text": "Eligibility requirements in the NPSAS:2000 field test for the B&B cohort accepted all sample members who were awarded a baccalaureate degree at any time during the NPSAS year. For the NPSAS:2000 full-scale study, eligibility requirements also stipulated that respondents be enrolled at some point during the NPSAS year. This requirement was added because many questions in the interview referred to enrollment during the NPSAS year. These questions were awkward and inappropriate for respondents who had not been enrolled during that period, but had received a degree. The B&B cohort for the full-scale follow-up will adhere to these eligibility requirements as well. Specifically, eligibility for the B&B:2000 cohort will require that the sample member be enrolled and receive a baccalaureate degree anytime between July 1, 1999, and June 30, 2000."}, {"section_title": "Sampling of base-year nonrespondents", "text": "In addition to sampling all of the NPSAS:2000 respondents verified to be B&B eligible, we will select half of the NPSAS B&B sample nonrespondents for the B&B:2000/01 sample. Based on results from the field test, this sample of base-year nonrespondents is expected to have a yield of 50 percent in the B&B follow-up survey. That is, half the members of this sample are expected to be verified as B&B eligible and to respond in the follow-up survey and half are expected to consist of ineligibles, false positives, nonrespondents in B&B, or several of these combined. Since the proportion of B&B false negatives (i.e., students not selected as potential B&B sample members but who were determined in CATI to be B&B eligible) was extremely small (1.5 percent), no attempt to represent these students in the full-scale survey is planned. The NPSAS B&B nonrespondents can be classified as students who were sampled as B&B and located but who refused to be interviewed in NPSAS; 57 B&B:2000/0I Field Test Report students who were sampled as B&B and located but time ran out before a NPSAS interview could be completed; or students who were sampled as B&B but not located for NPSAS. Overall, 44.8 percent of B&B nonrespondents in the NPSAS field test were interviewed during the B&B follow-up field test with students in the second category above the most likely to complete a B&B interview (68.2 percent), followed by students in the third category (43.9 percent), and then by students in the first category (32.4 percent). For the full-scale follow-up, we will be able to sample from the three groups of base-year nonrespondents at rates proportional to the response rates achieved in the follow-up field test in order to achieve the expected yield."}, {"section_title": "Effect of false positives and false negatives", "text": "During the NPSAS:2000 field test, 79 of the 797 students sampled as B&B (9.9 percent) were found during the NPSAS interview not to be B&B eligible (false positives), and 12 of the 817 students sampled as other undergraduates, graduates, or first-professionals from 4-year institutions (1.5 percent) were found during the NPSAS interview to be B&B eligible (false negatives; see the NPSAS:2000 Field Test Methodology Report for more details). To account for the false positives and false negatives in NPSAS full-scale sample selection, more B&B students and fewer other undergraduate students than necessary will be selected. For the B&B:2000/01 field test, there were no false negatives because all sample students were either verified during NPSAS to be B&B eligible, or were sampled for B&B. However, 9 of the 125 NPSAS nonrespondents in the B&B sample (7.2 percent) were false positives. The full-scale B&B sampling plan will account for the expected false positives from the sample of NPSAS nonrespondents."}, {"section_title": "C. Use of targeted incentives to sample members", "text": "The use of monetary incentives was shown in an experiment conducted as part of the base-year (NPSAS:2000) study to be an effective means of reducing nonresponse among some types of nonrespondents, in particular those who initially refused to be interviewed (see NPSAS:2000 Field Test Methodology Report). The lingering question for the B &B:2000 /O1 1-year follow-up is whether those who received an incentive in the base year would demand an incentive before completing the follow-up survey. If a significant portion did insist on receiving an incentive before completing the survey, then it might be advisable to simply send an incentive at the outset of the full-scale data collection effort to those who received an incentive in the base year. However, if there was little apparent difference in the response rates of those who received an incentive at the start of the study versus those who did not receive an incentive in the followup experiment (i.e., those in the control group), then the recommendation would seem to favor using incentives in a more conservative, targeted manner. The results of the incentive experiment described in Chapter 3 seem to argue for the latter approachthat is, using incentives in a targeted manner to reduce nonresponse, rather than mailing incentives to all of the sample members who received an incentive in the base year. Although the overall number of cases examined was relatively small, the evidence does not appear strong enough to warrant the expenditure of resources on incentives to all base-year incentive recipients at the outset of the study. Instead, it is recommended that the same incentive B&B: 2000/01 Field Test Report 58 protocols used in the base-year study (NPSAS:2000) be implemented for the full-scale B&B:2000/01 to reduce nonresponse among particular sets of sample members. These protocols include: Targeting for incentive receipt only (1) those who refuse to complete the study initially and (2) those for whom only a valid address is available (i.e., there is no valid telephone number). Incentives may also be targeted to a third group: those with high call counts (30 or more call attempts) for whom a valid mailing address is available. Incentive recipients will receive a letter, sent via express mail, which explains the study and expresses the need for their cooperation. These mailings should also include a five-dollar bill. Sample members will be instructed that if they complete the survey, they will be sent a check for an additional $15. We believe this protocol will effectively reduce the level of nonresponse for the B&B:2000/01 follow-up study, while also conserving resourcesusing them in a targeted manner."}, {"section_title": "Early e-mail contact with sample members", "text": "The field test experience also seemed to indicate that e-mail was an effective mode for establishing contact with some sample members. A high percentage of those contacted via email either called in directly to complete the interview or set up a convenient time to complete the survey at a later date. In the field test, e-mail contact was used as a tool for reducing nonresponse. E-mails were sent relatively late in the course of data collection and were targeted only at those who had not completed the interview by the eighth week of the interviewing effort. For the full-scale study, we recommend using e-mail earlier in the processat the outset of data collectionas a means of making early contact with the sample members. A mailing should be sent to all respondents for whom a valid e-mail is obtained from either the base-year study or the student update sheets. The content of the e-mail should be similar to that of the initial prenotification letter, indicating the purpose of the study and requesting the sample member either to call a toll-free number to complete the survey or to notify us via e-mail or telephone of a more convenient time to complete the survey. E-mail should also be used periodically throughout data collection as a means of establishing contact with sample members who prove difficult to reach by telephone. The early use of e-mail as an alternative means of communication should help increase the initial contact rates with these otherwise hard-to-reach sample members."}, {"section_title": "E. Student CATI", "text": "We recommend a number of revisions to the field test student follow-up CATI interview for use in the full-scale B&B follow-up survey. These suggestions are based on (1) examination of field test interview results, including item indeterminancies; (2) results of timing analyses; (3) quality circle debriefings with telephone interview staff; and (4) discussions with the study Technical Review Panel (see appendix A for a list of panel members). These recommended changes are listed by instrument section and individual data element in table 5.1."}, {"section_title": "59", "text": "B&B:2000/0I Field Test Report "}, {"section_title": "Revise", "text": "Change time reference to first year of enrollment.\nWe will ask for the ages of dependent children rather than getting the number of children within specified age ranges. If there are more than six dependent children, we will collect the ages of the six youngest.\nWe will reword the response options so that the option for teaching assistants clearly refers to the institution in which the respondent is enrolled.\nWe will collect this response as continuous rather than in categories. If respondent does not know, we will probe to find out if less than 50 or over 1000.\nThe revised question text will clarify that we are asking about whether or not the employer allows respondents to work regularly away from the office or telecommute. Also, it will specify that \"home or other location\" includes flexi-place work.\nWe will change the wording of the response option to: Encouraged by employer. F_CERTTY 1 -3 Type of certification/licensure\nWe will separate this query into two parts: up to 3 occupational licenses/certificates required by law, and up to 3 professional licenses/certificates not required by law but required for career advancement. Follow-up for both will collect amount of time required to prepare for license/certificate, sponsor of license/certification, and reason for obtaining license/certification.\nThis item is currently asked of respondents who are not working, but are looking for work. For the full-scale survey, we will ask respondents who are currently working how they found their current job as well.\nWe currently collect up to 5 certifications. We will collect up to 3 fields of certification for the full-scale survey. We will also collect only the general field and delete the detail. We will collect the general fields of certification: \nWe will revise the response options as follows. Remove \"advanced professional certificate\" since it takes more than one year to earn it. Do not get field detail if respondent has \"emergency certification.\" If respondent has a \"regular/standard state certificate, a probationary, or a temporary certificate,\" then follow up with the field detail.\nThis question was asked only of certified teachers. In the fullscale survey, we will ask all teachers.\nWe will make this series of items consistent with fields of certification.\nWe will include an option for teachers who teach in a \"selfcontained\" classroom.\nWe will ask this question of current teachers who were base-year nonrespondents.  You were selected last year to participate in a U.S. Department of Education study of students enrolled during 1998-99 (National Postsecondary Student Aid Study). We are conducting a one-year follow-up study of recent bachelor's degree recipients (Baccalaureate and Beyond) and we need your participation to learn about your transition from college to work or to graduate school."}, {"section_title": "B_REM1", "text": "Required to take any remedial or developmental courses during first year of enrollment Revise Change wording to make more clear to respondents. Many students do not know that the courses they are required to take are \"remedial.\" The new wording will read: During your first year, did you take any basic or remedial English or math courses for which credit did not apply toward your degree, or that were in addition to those required for your degree. "}, {"section_title": "D_INC99", "text": "Respondent's income for calendar year 1999 Delete Given that respondents, by definition, were enrolled in school in the past year, their income for 1999 will likely only span the months from graduation through the end of the year. Furthermore, we ask for the salary of the current job in the employment section."}, {"section_title": "D_REPAY, D_RPYAMT", "text": "Amounts owed for undergraduate loans, repayment status, repayment amount."}, {"section_title": "Add", "text": "While we get most of the necessary information about respondents' undergraduate financial aid in the base-year survey, we need to ask the following: Amount borrowed for undergraduate education Amount currently owed Is the respondent in repayment Are parents are helping with repayment Has any part of the loan been forgiven, or is employer assisting with repayment What is the monthly amount of repayment Amount borrowed from family and friends Amount owed to family and friends D_RNTAMT Monthly rent amount\nWe currently only ask monthly mortgage amount for respondents who own homes. We will ask for monthly rental payments to get a better picture of respondents' major monthly expenses.   \nWe will ask respondents if they feel prepared to \"manage the classroom.\"\nWe will ask respondents the number of students they teach per day."}, {"section_title": "F_APRSAM Add", "text": "We will add a question to determine if the current job is the same as the job held in April."}, {"section_title": "F_CURJOB Current job as beginning of career Revise", "text": "We will revise question wording and response options to: Which of the following best describes your current job? 1= The start of your career in your current occupation 2 = Continuing in the career you had before graduation 3 = Preparing for graduate school 4 = Preparing for another job 5 = Temporary jobdeciding on future education/career 6 = Pays the bills/only job available 7 = Other "}, {"section_title": "63", "text": "B&B:2000/0I Field Test Report "}, {"section_title": "F_TELWRK and F_TELOFN Frequency of working away from office", "text": "Revise Rather than asking two separate questions, we will ask how often respondents work away from the office and allow \"never\" as a response."}, {"section_title": "F_TRNREQ", "text": "Employer support ofjobrelated training"}, {"section_title": "F_JOBSRH Job search activities", "text": ""}, {"section_title": "F_TRAVEL", "text": "Amount of time spent traveling over the past year"}, {"section_title": "Delete", "text": "This item was asked of respondents who were not working and were not enrolled in school. We will delete this question from the full-scale survey because of the small number of responses.  Reconunendation G_CRTFD\nThis question is not appropriate for the 1-year follow-up as it takes at least 5 years to earn."}, {"section_title": "Fields of certification", "text": ""}, {"section_title": "G_NATCRT", "text": ""}, {"section_title": "National Board Certification", "text": ""}, {"section_title": "G_PRVCRT", "text": ""}, {"section_title": "Certifications from private organizations", "text": ""}, {"section_title": "G_PRPCLS", "text": ""}, {"section_title": "Prepared for classroom management", "text": ""}, {"section_title": "G_TCHSB", "text": "Subjects taught at school 1 and 2"}, {"section_title": "G_NUMCLS", "text": ""}, {"section_title": "Number of sections/periods taught per day", "text": ""}, {"section_title": "G_NUM STD", "text": "Number of students taught"}, {"section_title": "65", "text": "B&B:2000/01 Field Test Report "}, {"section_title": "G_NOAPW", "text": "The study is being conducted for the U.S. Department of Education's National Center for Education Statistics (NCES) by the Research Triangle Institute (RTI), a nationally recognized research organization located in North Carolina. An interviewer from RTI will call to conduct a telephone interview with you in the near future. The interviewer will ask you about your early career experiences, educational achievements, community activities and level of debt. The interview will take about 15 to 25 minutes. Please be assured that both NCES and RTI follow strict confidentiality procedures to protect the privacy of study participants and the confidentiality of the information collected. We need your help in collecting these data. Your participation is voluntary but your responses are important to make the results of this study accurate and timely. Enclosed you will find a leaflet with a brief description of the study, how you were selected, and confidentiality procedures. We would also like your help in updating our records. Please take a few minutes to verify, correct, or update the enclosed Address Update Information Sheet and return it to RTI in the enclosed postage-paid envelope. If you have any questions about the study, please contact Dr. John Riccobono, Project Director, at RTI. The toll free number is 1-800-334-8571. Persons who are hearing or speech impaired may call us (toll free) at 1-919-541-6538 (TTY/TDD). We sincerely appreciate your assistance and thank you for helping us conduct this important study.  Baccalaureate and Beyond Longitudinal Study (B&B:2000/01). The study will build upon the information collected in the National Postsecondary Student Aid Study (NPSAS), for which you were selected to participate not long ago. The follow-up to that study will begin in the coming weeks and I would like to urge your continued participation in this important study. B&B collects information about students who graduated from fouryear colleges and universities in the academic year 1998-1999. The study provides data about the early career experiences and educational achievements of bachelor's degree recipients. The results of previous B&B studies have been used by policymakers to better understand how the level of undergraduate education debt affects decisions concerning graduate school, employment, and family formation. The study is being conducted for the U.S. Department of Education's National Center for Education Statistics (NCES) by the Research Triangle Institute (RTI), a nationally recognized research organization located in North Carolina. Please be assured that both NCES and RTI follow strict confidentiality procedures to protect the privacy of study participants and the confidentiality of the information collected. An interviewer from RTI will call to conduct a telephone interview with you in the near future. The interview will take about 25 minutes to complete, although many interviews will be shorter than that. Your participation is completely voluntary. However, we do need your help in collecting these data. Your responses are important to make the results of this study accurate and timely. Enclosed you will find a leaflet with a brief description of B&B, how you were selected, and confidentiality procedures. We would also like your help in updating our records. Please take a few minutes to vet*, correct, or update the enclosed Address Update Information Sheet and return it to RTI in the enclosed postage-paid envelope. If you have any questions about the study, please contact Dr. John Riccobono, Project Director, at RTI. The toll free number is 1-800-334-8571. Persons who are hearing or speech impaired may call us (toll free) at 1-919-541-6538 (TTY/TDD). We sincerely appreciate your assistance and thank you for helping us conduct this important study. "}, {"section_title": "Spanish Letter Carta del Gary Phillips Traduccion al Espanol", "text": "El alio pasado, usted fue seleccionado para participar en un estudio acerca de estudiantes matriculados durante 1998-1999 para el Departamento de Educacion de los Estados Unidos (El Estudio Nacional sobre Asistencia Economica para Estudiantes en Escuelas Post-secundarias o en ingles the National Postsecondary Student Aid Study, NPSAS). Estamos realizando un segundo estudio (el estudio Mas Alla de los Estudios Universitarios) para ampliar la informaci6n recopilada en NPSAS y necesitamos su participacion para aprender sobre su transici6n de la universidad al trabajo o a los estudios graduados. El estudio se realiza por Research Triangle Institute (RTI) para el Centro Nacional de Estadisticas sobre la Educacion (NCES), parte del Departamento de Educaci6n de los Estados Unidos. RTI es una organizacion de investigacion reconocida a nivel nacional que esta ubicada en Carolina del Norte. Un entrevistador de RTI lo Ilamard para realizar una entrevista con usted por telefono pronto. El entrevistador le preguntard acerca de las primeras experiencias en la carrera, los logros educativos, las actividades comunitarias, y el nivel de deuda. La entrevista durard aproximadamente 15-25 minutos. Tenga la seguridad en saber que NCES y RTI exigen el mantenimiento de confidencialidad para proteger la privacidad de los participantes en estudios de investigacion y la confidencialidad de la informaci6n recopilada. Necesitamos su ayuda para recopilar estos datos. Su participacion es completamente voluntaria pero sus respuestas son imprescindibles para asegurar que los resultados de este estudio son precisos. Adjuntado encuentre un folleto que contiene una descripcion breve del estudio, asi como la manera en que usted fue seleccionado y el procedimiento de confidencialidad. Ademas, nos gustaria su ayuda para actualizar nuestros archivos. Favor de tomar unos minutos para verificar, corregir, o poner al dia el Formulario para Actualizar la Direccion del Domicilio adjuntado y devolverlo a RTI en el sobre sellado adjuntado. Si tiene cualquier pregunta acerca del estudio, favor de comunicarse con el director del proyecto, Dr. John Riccobono de RTI. El numero telefonico gratuito es 1-800-334-8571. Personas con un impedimento auditivo o de habla pueden Ilamar al numero (gratuito) 1-919-541-6538 (TTY/TDD). Le agradecemos sinceramente de antemano su asistencia y su ayuda en la realizacian de este estudio importante. De acuerdo a la Ley de Reduccion de Papeleo de 1995, (1998)(1999): @dgnyy [If A_BANPS@BANPS equals 1 and @DGNMM is greater than or equal to 7, and @DGNYY 1998 or @DGNMM is less than 9 and @DGNYY equals 1999, then A_BBELG=1. Go What is the highest level of education your spouse has completed? 1 = DID NOT COMPLETE HIGH SCHOOL 2 = HIGH SCHOOL DIPLOMA OR EQUIVALENT 3 = VOCATIONAL/TECHNICAL TRAINING 4 = LESS THAN 2 YEARS OF COLLEGE 5 = TWO OR MORE YEARS OF COLLEGE/ASSOCIATE'S DEGREE 6 = BACHELOR'S DEGREE 7 = MASTER'S DEGREE OR EQUIVALENT 8 = MD, LLB, JD OR OTHER ADVANCED DEGREE 9 = PHD OR EQUIVALENT"}, {"section_title": ">D_SPED99<", "text": "Was your spouse enrolled in college or graduate school during the 99-2000 school year? IF YES, PROBE TO FIND OUT IF FULL-TIME OR PART-TIME 0 = NO 1 = YES, FULL-TIME 2 = YES, PART-TIME 3 = YES, MIXED ENROLLMENT >D_SPAID< [If D_SPSED is less than 3 or D_MAR is not equal to 2, go to D_CAR] Did your spouse ever receive any student loans to help pay for his/her undergraduate education?  [If E_DEGTYP equals 1 or (E_DEGTYP is greater than or equal to 5 and E_DEGTYP is less than or equal to 7)go to E_GRDRSN] [If (E_DEGTYP is greater than or equal to 12 and E_DEGTYP is less than or equal to 13) or (E_DEGTYP is greater then or equal to 18 and E_DEGTYP is less than or equal to 19) go to E_GRDRSN]   99'= KINDERGARTEN 1 = FIRST GRADE 2 = SECOND GRADE 3 = THIRD GRADE 4 = FOURTH GRADE 5 = FIFTH GRADE 6 = SIXTH GRADE 7 = SEVENTH GRADE 8 = EIGHTH GRADE 9 = NINTH GRADE 10 = TENTH GRADE 11 = ELEVENTH GRADE 12 = TWELFTH GRADE 13 = UNGRADED [If G_FSTGRD1/2/3/4/5/6 is less than or equal to 0, go to G_TCHSB] In your first teaching job, was the workload given to you by your school (the students or classes that you teach) more difficult than those of other teachers at your school? Is the workload given to you by your school (the students or classes that you teach) more difficult than those of other teachers at your school? 1 = YES 2 = NO 3 = NOT SURE [Go to G_HLPNEW] >G_HLPNEW< ENTER 1 = AGREE, 2 = DISAGREE [If G_NUMSNC is greater than 1] In thinking about your first teaching job, would you agree or disagree that your school is/was effective in helping new teachers with... Do you agree or disagree that your school is/was effective in helping new teachers with... What is your academic year base salary at your current job, not including extra pay for What was your academic year base salary at your most recent job, not including extra pay for things like summer teaching, coaching, or extracurricular activities? RANGE ($1,000-$90,000): [If G_NUMSNC is less than or equal to 1, go to G_TCHSAT] >G_FSTSAL< What was your academic year base salary at your first job, not including extra pay for things like summer teaching, coaching, or extracurricular activities? RANGE ($1,000-$90,000): [if CSS1UXST eq 1] 1 = ENTER USEREXIT 2 = SKIP OVER USEREXIT [if CSGRDSCH eq 3,4,5,6,7,8,9,10,11,12,13 "}]