[{"section_title": "Abstract", "text": "Summary: Drug development for CNS disorders faces the same formidable hurdles as other therapeutic areas: escalating development costs; novel drug targets with unproven therapeutic potential; and health care systems and regulatory agencies demanding more compelling demonstrations of the value of new drug products. Extensive clinical testing remains the core of registration of new compounds; however, traditional clinical trial methods are falling short in overcoming these development hurdles. The most common CNS disorders targeted for drug treatment are chronic, slowly vitiating processes manifested by highly subjective and context dependent signs and symptoms. With the exception of a few rare familial degenerative disorders, they have ill-defined or undefined pathophysiology. Samples selected for treatment trials using clinical criteria are inevitably heterogeneous, and dependence on traditional endpoints results in early proof-of-concept trials being long and large, with very poor signal to noise. It is no wonder that pharmaceutical and biotechnology companies are looking to biomarkers as an integral part of decision-making process supported by new technologies such as genetics, genomics, proteomics, and imaging as a mean of rationalizing CNS drug development. The present review represent an effort to illustrate the integration of such technologies in drug development supporting the path of individualized medicine."}, {"section_title": "GENOMICS PAST AND FUTURE", "text": "In the mid-1990s, DNA and oligonucleotide microarray technology started to revolutionalize studies of gene expression enabling analysis of the activity of tens of thousands of transcripts at the same time. [1] [2] [3] In the field of neurosciences and neurology, this technology began to be first employed during the late 1990s and immediately raised high expectations for its capacity to increase molecular understanding in behavior and disease. 4, 5 It soon became evident that the sensitive technology had pitfalls and was to be used carefully. 6, 7 Study design, sample dissection, and preparation required a special attention as small variations in animal handling, diurnal or stress state, dissection, RNA extraction, or batch difference in reagents or arrays could easily bias the experimental outcome unless taken in account and controlled in experimental design. At the same time, the use of microarrays also in the field of neurobiology had expanded exponentially: studies analyzing heterogeneous large brain areas, even whole brain homogenates were published, pooling of samples was common before the power of individual variation and statistical approaches was understood. Caution in the use of postmortem brain samples was brought by research by the Pritzker Neuropsychiatric Disorders Research Consortium who working on gene expression changes in affective disorders underlined the importance of postmortem interval and agonal period affecting the pH of brain and RNA integrity and thus influencing the data more than actual disease or medication status. 8 -10 Undoubtedly, microarrays are among the most mature among the molecular profiling tools in the genomics and genetics research. Today, microarrays are being used in combination with other methods and traditional expertise. In neurobiology, on one hand focused approaches are being employed to study detailed questions combining laser capture microdissection (LCM) or capture of single cells 11 and microarray technology using different amplification procedures or without, using highly sensitive custom microarrays. 12 On the other hand, microarray gene expression profiling is a routine method used among others to, e.g., characterize different phenotypes created by reverse genetics approaches. In drug discovery and development, microarrays are being used preclinically in target identification and validation, in toxicology to profile compounds for toxicity (so called toxicogenomics), to identify biomarkers and clinically, to monitor compound efficacy and/or toxicity. Predictive genomics signatures have become a practice in cancer genomics to monitor disease severity and predict outcome and response to different treatments. Similar efforts are ongoing in different disease areas: in transplantation genomics, signatures of kidney biopsies and blood are soon wished to complement the traditional pathology-based disease classification of acute and chronic rejection."}, {"section_title": "GENOMIC AND DRUG DEVELOPMENT", "text": "Gene expression profiling is used in drug discovery at various stages: for the identification and validation of new targets, in early compound screening, using cell culture, tissue coculture, or animal-based models to select compounds based on their efficacy to reach the expected target or pathway or to kill a compound based on its toxicity profile.\nIdentification of new targets can be achieved by generating a matrix of data using different sources, such as diseases or a series of compound treatments based on the correlation of gene expression data and phenotypical variations. This idea underlines already the necessity to proceed to an integration of external data. Gene expression changes need to be associated with functional end points and/or physiological modifications of the biological system observed (cells, neuronal network, of structure).\nA similar approach-using genomic data generated with the profiling of reference compounds-is being used in early compound screening for toxicology, where toxicogenomics has dramatically changed the way toxicology is being performed today. Toxicogenomics is the study of gene expression patterns designed to detect-up and down-regulation of genes associated with drug toxicity risk. Toxicogenomic markers for adverse side effects can influence selection and optimization of lead compounds before human studies.\nCell culture-based or 1-or 3-day studies in rodents are replacing the traditional long 2-or 4-week studies at the compound selection phase, which is faster because gene expression changes related to toxicity can be seen earlier than changes at microscopic level, cheaper and more ethical than the old approach (FIG. 1) . Molecular toxicology including gene expression profiling has made it easier to identify the toxicity mechanism. Toxicogenomics signatures are being used as biomarkers of toxicity and can eventually help monitoring compound safety in phase 1 (FIG. 2) .\nNowhere in the realm of drug development has the expectations of the impact of genomics been greater than in the area of preclinical toxicology. Transcriptome analysis, along with other technologies, i.e., genetics, proteomics, has the potential to radically improve the drug safety assessment process by allowing the identification of potential toxicity issues earlier, and thus proceed only with those molecules that have the best efficacy and safety profiles. The gene expression analysis allows the mechanistic characterization of the toxicity, the different microscopic observations can be further differentiated at the molecular level, leading to an understanding of the cause. The safety evaluation supported by the new technology can then directly discriminate the on-and off-target activities. From this in depth assessment, some valuable information can be extracted to organize the screening of new compound devoid of the side effect.\nToxicogenomics will help the understanding of mechanisms of toxicity, predict toxicity, develop in vivo and in vitro surrogate models and screens, and develop toxicity biomarkers to monitor safety in clinical trials.\nThe investigation of the preclinical disease model in parallel with the investigation of the disease should soon offer new perspectives in the drug selection process. The preclinical models are, most of the time, an approximation of the human disease and the underlying complexity of the disease mechanism may be only partially reflected in a model, and different models may reflect different aspects of the disease. The molecular understanding of the clinical situation and its appropriate mapping to different models should provide a rational base for an optimized selection of the models according to the targeted mechanism of action.\nCellular systems or animal models are used to study gene expression profiles affected by the compound. In the field of neurosciences, animal models are a requisite for studies of the efficacy in target organ, the CNS. One aim in the preclinical studies is to identify noninvasive biomarkers for clinical use reflecting central efficacy. Imaging and the \"omics\" measuring secreted biomarkers in peripheral fluids, such as serum, plasma, CSF, saliva, or urine are used. Gene expression profiling of whole blood or peripheral blood monocytic cells (PBMCs) can also be used as an efficacy readout. As examples, profiling of PBMCs has recently been shown to be able to differentiate responders from nonresponders after interferon \u2424 therapy in multiple sclerosis (MS), 13 different transcriptomics profiles of whole blood have been shown to correlate with seizure freedom in children treated with valproate 14 and monocytes from Alzheimer's disease (AD) patients have shown different responses to antidepressant treatment in comparison to control subjects. 15, 16 The case control studies can be sometimes misleading because the appropriate control may escape a simple definition, even more so in the field of neuroscience. However, the progressive accumulation of data, coming from small study, should provide a strong base of knowledge: independent studies on Parkinson disease 17 point to the same biological mechanisms, one being already a suspect-mitochondria and energy handling-and the other providing a new view of the pathogenesis-ubiquitination. The main challenge on the field remains the accessibility of the target organs: this strongly pushes to identify alternate sources of information such as surrogate markers in the blood and to investigate non invasive methods. The CNS area may represent the best field of investigation for combinatorial technologies exercise, mixing the results of genomic data and the clinical imaging capacity of investigation.\nAt the proof-of-concept (PoC) phase, gene expression profiling of blood, PBMCs, and CSF can be used to monitor efficacy and safety. If safety-or efficacy-related genomics profiles have been previously identified, a quantitative real-time fluorescence based PCR (QRT-PCR or Taqman assay) assays in 98-or 360-well format or custom-designed microrarrays can be used to monitor a selected genomic signature. Often, however, PoC stage is the first time species specific information from humans can be gained and genomic scale profiling is commonly used."}, {"section_title": "MOLECULAR SIGNATURE AND EVOLVING TECHNOLOGIES", "text": "Studying gene expression patterns in the CNS using DNA microarrays is challenging because of the existence of many different neuronal subtypes results in intricate anatomical and functional heterogeneity within the CNS.\nA goal of modern molecular and cellular neuroscience is to assay gene expression from homogeneous populations of cells within a defined region without potential contamination by expression profiles of adjacent neuronal subtypes and non-neuronal cells. This is a difficult task that requires a combination of approaches and technologies to unravel the complexity of brain nuclei function and dysfunction in the context of neurological pathologies.\nRegional genomic analysis is a powerful approach for identification of transcripts that are enriched in a specific region, lamina, or nuclei that differs from adjacent or connected regions. Going one level further, single-cell profiling techniques have the potential to quantify simultaneously expression levels of the entire genome in a given neuron, thereby allowing for the previously unobserved gene interaction(s) to become more evident. 18, 19 A major drawback of using microarray is the relatively large amount of RNA required. Affymetrix standard protocol recommends to start with 5 g of total RNA. However, studying brain nuclei function means regional or even single-cell profiling and therefore considerably lower amounts of RNA. Improvement of technologies and development of new ones allow this progression in the understanding of science. One of the most important technical advance for genomic profiling of single-cell or single population is the integration of LCM, RNA amplification, and subsequent cDNA array analysis. 20 -22 The fidelity of the RNA amplification step is critical to the extraction of meaningful information from microarray experiments. The issue of more than one round of amplification is the loss of linearity. Though, few new inventive amplification methods-in addition to the classical T7-based amplification-have shown promising results; NuGen describes an isothermal mRNA amplification method, which generates micrograms of labeled cDNA from 5 ng of total RNA. Highly reproducible GeneChip array performance (R2 \u03fe 0.95) was achieved with independent reactions starting with 5-100 ng Universal Human Reference total RNA. A good correlation was shown between the Affymetrix Standard Protocol (5 g of total RNA) and NuGen linear amplification method (20 ng starting RNA). 23 The quality of the RNA extracted from LCM, especially of postmortem brains or paraffin-embedded samples from the archives, is very often problematic: the sample may deteriorate before or during sectioning or during slide staining of formalin fixation, and inadequate extraction and isolation methods. 24, 25 Some strict RNA integrity standards have to be established and strictly respected, i.e., the 260/280 and 260/230 ratios as well as the rRNA 28/18S ratio. Even knowing that RNA integrity is very important, it is not always possible to ensure it. Today, ExpressArt mRNA amplification technology based on TRinucleotide primers allows the complete amplification of all mRNA fragments in severely degraded RNA samples and works very well with extremely low limits of input RNA amounts (picogram range). ExpressArt enables full-length cDNAs to be generated from mRNAs or full-size second DNA strands on singlestranded DNA templates.\nImprovements in microarray platform sensitivity is another field of development. Different microarray platforms, i.e., Affymetrix, Agilent, Illumina have worked along this line. Novartis has invented its own high sensitivity chip platform to meet some needs in clinical development. The Novartis Evanescent Resonance platform (NovaChips) approaches an alternative route. Instead of amplifying the relevant biological material before hybridization, NovaChips exploit a physical (optical) amplification scheme to enhance signal intensities. Thanks to a nanostructured surface giving rise to local energy confinement of the incident light, the fluorophore labels attached to the samples are excited much more efficiently leading to increased fluorescence signals and improved limits of detection, thereby lifting low expressed genes above background levels. 12 On NovaChips, the standard protocol (without amplification!) can be used for samples containing only 10 ng of total RNA. Below this limit, the NuGen protocol provides a linear amplification alternative with no loss in data fidelity. The powerful combination of the (physical) NovaChip fluorescence enhancement by evanescent field excitation and NuGen linear amplification protocol enables the entire genome profiling of minute samples where any other microarray technology fails.\nA dilution and correlation study starting with 10 ng of total RNA (rat brain) reduced by a factor 3 down to the picogram level demonstrated that above 1 ng of total RNA input we measure a constant level of present genes with excellent correlation between the individual concentrations. Below 1 ng of RNA, the percentage of present genes starts to diminish. Confidence values between two technical replicates are still above 0.95. Analyzing the data shows that 97% of the genes being detected with only 10 pg (!) of total RNA are also present at 10 ng; see FIGS. 3-6. In other words, performing gene expression analysis with NovaChips from samples with a total RNA input in the picogram range-without a second round of RNA amplification-is possible and will introduce only minor bias to the expression profiles.\nThis opened up a new possibility: the full genome profiling with CSF. Working with postmortem brain tissues represent a real challenge. First, because the availability of postmortem material is limited, furthermore postmortem material from unmedicated patients is even rarer. The limitation of the number associated to the extreme diversity among the brains in respect to age, race, postmortem interval, medication history, lifestyle, and other factors represent a real challenge for the interpretation of the data. 26 Gene expression profiling of CSF represents a real challenge-because the number of cells in the CSF is extremely limited-but is the only way to describe gene expression changes occurring throughout the course of a disease. 26 Cepok et al. 27 demonstrated that the patterns of CSF pathology correlate with disease progression in multiple sclerosis, showing that a high predominance of B cells was associated with more rapid disease progression, whereas a predominance of monocytes was found in patients with slower progression. The information on gene expression levels contained in the B cells or monocytes would allow a deeper molecular understanding of disease progression. RT-PCR on CSF indicates that the expression of interleukin 1 (IL-1), a proinflammatory mediator is increased in Alzheimer patients in comparison to control individuals, consistent with hypotheses linking inflammation and AD. Similarly, the expression of the gene for GFAP, which is involved in astrocytosis also increased in CSF from AD patients, whereas it was undetectable in single-cell populations (Ginsberg, S.D., New York Academy of Sciences eBriefing, unpublished data). While it was still hoped for only a few months ago, the whole genome profiling of CSF is now possible thanks to the combination of different optimized technologies, i.e., NuGen/EspressArt technologies and NovaChip platform offering huge new perspectives in the understanding of neurodegenerative diseases and disease progression, i.e., AD, MS, and therefore offering different options for pharmacotherapeutics."}, {"section_title": "PHARMACOGENETICS", "text": "Pharmacogenetics and pharmacogenomics offer the potential of developing DNA-based tests to help maximize drug efficacy and enhance drug safety by stratifying the patient population. A lot of data on association of genetic variation and drug metabolism, response, and clinical outcomes as well as data on adverse events have been generated. 28 Pharmacogenetic research began in the 1950s with the discovery of genetic alterations in metabolic enzymes that were the cause of adverse reactions. Most antipsychotic drugs are lipophilic compounds and undergo extensive metabolism by cytochrome P450 enzymes. There is a tremendous interindividual variability in the efficacy of biotransformation of antipsychotic drugs, resulting in marked differences in the pharmacokinetic of the drug and plasma concentrations during treatment at fixed doses. Polymorphism of CYP2D6 and of other drug metabolizing enzymes may thus lead to very large differences in drug exposure and possibly also to toxicity also to toxicity or ineffective drug concentrations in some subjects. This is illustrated by the introduction of atomoxetine, a potent inhibitor of the presynaptic norepinephrine transporter, which is metabolized by CYP2D6, for which in clinical studies atomoxetine dosing was initiated and capped at lower doses in CYP2D6 poor metabolizers than in extensive metabolizers. 29 Pharmacogenetic investigations are numerous in the case of schizophrenia. Findings in genetic studies have ruled out monogenic transmission indicating that the schizophrenic population is genetically and pharmacologically heterogeneous. It is pointed out that for the detection of subpopulations within schizophrenia, clinical investigations with antipsychotic drugs have to proceed beyond the demonstration of therapeutic efficacy to the identification of treatment-responsive form(s) of illness. With regards to efficacy, 40 -80% of patients fail to respond or demonstrate only a partial response to typical antipsychotic agents (risperidone, olanzapine). Even among responders, some psychopathologies, including mood dysregulation and cognitive deficits, appear unchanged or even worsened. Recent studies suggest that the 5-hydroxytryptamine receptor 2A (5-HT 2A ) gene (HTR2A) T102C and G-1438A polymorphisms may influence treatment response of risperidone or olanzapine for schizophrenia's negative symptoms (social withdrawal), and the HTR6 T-267C polymorphism has been linked to risperidone response for positive symptoms (delusions and hallucinations). 30 Weight gain with antipsychotic drugs is another example where pharmacogenetics and individual variability can help monitoring side effect. The 5-HT 2A receptor (HTR2C) is again a leading candidate gene considered to influence this phenomenon. Evidence for a role in human obesity comes from two case-control studies demonstrating an association between a common HTR2C promoter single nucleotide polymorphism (\u03ea759 C/T) and body mass index. 31 Leptin is another prime potential susceptibility gene for obesity given its role in the regulation of appetite and energy expenditure. Templeman et al. 31a also report that a leptin promoter polymorphism was associated with antipsychotic-induced weight gain and that pretreatment plasma levels of leptin were associated with HTR2C genotype."}, {"section_title": "ANTIBODIES AND ASSAYS FOR BIOMARKER ANALYSIS", "text": "Assays for the analysis of peptide or protein biomarker (BM) are nearly always immunoassays in various ELISA formats using various antibodies and combination of antibodies. Because protein and peptide biomarkers are largely unknown in psychiatric diseases, 32 immunoassays for analysis of the levels of biomarkers in neurological disease are mainly confined to neurodegenerative diseases, exemplified here by AD.\nA\u2424 peptides, derived from cleavage of the amyloid precursor protein and TAU and hyperphosphorylated forms of TAU have been analyzed as potential markers for status and progression of disease. 33 The levels of these molecules were detected with various assays based on a variety of antibodies with different specificity. Changing levels of A\u242442 versus A\u242440 in CSF were correlated with disease progression and status and differentiation of early and late onset of disease. 34 A\u2424 peptides are considered be a central cross point of mechanisms leading to the genesis of AD. However, variable and conflicting results were published. 34, 35 Levels of aggregates of A\u2424 peptides and certain A\u2424 fragments were claimed to be correlated with cognitive impairment, associated with the disease 36 N-terminal-deleted A\u2424 fragments seem to correlate with MCI to AD transition. Blood plasma levels of A\u2424 peptide were very low and not conclusive and TAU is not detectable in plasma. Therefore, as expected from brain physiology CSF samples are the most important source for brain biomarkers. Unfortunately, turnover of CSF is relatively high compared with blood, which limits accumulation of proteins and additionally, subject of substantialinter-individual variability.\nCombination of the CSF levels of A\u2424 peptides A\u242442, A\u242440, with TAU and hyperphosphorylated Tau were shown to accumulate to sensitivity and specificity of AD diagnostic of above 80%; however, final proof is still only possible by postmortem tissue pathology. 37 No significant changes of these biomarkers were observed under the currently marketed treatments of AD patients. Possibly, these treatment are symptomatic and did not hit the A\u2424 peptide pathways.\nTAU protein was shown to be a nonspecific marker of active neurodegeneration. Increased levels are also found in acute brain injury. The comparability of results with the different assays was not demonstrated and for some published clinical data, no information about the applied analytical method is available. 38 Comparison of biomarker data across studies have to be done very carefully to avoid misleading conclusions. Only recently, driven by Luminex assays (Innogenetics) a trend toward use of one analytical method, widely accepted in the field, occurs. 39 Recent analysis of naturally occurring anti-A\u2424 antibody levels identified differences in AD versus controls and correlation of early AD onset with decreased levels of anti-\u2424 antibodies. 40 These antibodies potentially interfere with the assays used for the measurement of A\u2424 peptides. Such interference adds to interindividual variability and over a time course. Therefore, conclusions based on A\u2424 peptide levels have to be taken very cautiously.\nInterference of antibodies with A\u2424 assays need special consideration in passive or active immunization approached to AD treatment, such as the ELAN immunodrug AN-1792. [41] [42] [43] Apolipoprotein (Apo) E4 was shown to correlate with higher prevalence of AD. Detection can be done with a specific immunoassay or by genetics analysis. The latter is typically applied in recent studies, combined with other alleles of ApoE for lower costs of analysis."}, {"section_title": "ANTIBODIES AND ASSAYS FOR SENSITIVE AND SPECIFIC ANALYSES OF BM", "text": "Most immunoassays used for biomarker analysis are variants of ELISA. For these assays, two types of antibodies have been used. 44 Polyclonal antibodies derived from sera of immunized animals (e.g., rabbit or sheep) or monoclonal antibodies derived from hybridoma cells, immortalized B cells from immunized animals. Derived from single B cells, monoclonal antibodies are of one structure, detecting one specific epitope. Despite the fact that monoclonal antibody (mAb) generation is more laborious and resource consuming compared with polyclonal antibodies, superior specificity and easy purification render mAb the preferred tool for assays with high specificity.\nAntibody generation technologies need to improved from three perspectives: time lines for obtaining the antibodies to the BM of interest, solutions to targets proven difficult in animals (e.g., toxic proteins) and costs. Phage display technology is considered a potential solution. The technology is based on the in vitro binding of recombinant antibodies bound to bacteriphages. It allows inclusion of certain analytical constrains into the selection process, rather than selection from antibodies with limited unknown variability derived from animals. Additionally, the affinity of the antibodies can be modified after initial selection. 45 Whereas timelines for initial selection are short, affinity maturation can lead to long delays. Currently, no commercial bioanalytical kit, based on antibodies derived from phage display or other display technologies (e.g., ribosome display) is on the market. Therefore, the value of these technologies for the generation of specific high-affinity antibodies for immunoassays remains to be demonstrated. However, one such antibody has been approved recently for therapeutic purposes.\nImprovements have also been made to antibody generation in animals, resulting in reduced time lines. Shorter immunization procedures [e.g., RIMMS (rapid immunization at multiple sites)] can be applied. Additionally, the generation and maintenance of hybridoma cells for the production of monoclonal antibodies has been improved, mostly driven by new era of therapeutic antibodies.\nAlthough display technologies have the burden of intellectual property and license issues, antibody generation in animals is subject to animal welfare policy. However, currently, animal-derived antibodies remain the standard tool to generate assays for biomarkers. Alternatives to antibodies as specific detection tool in assays are on the horizon (e.g., DNA-Aptamers). Lower molecular weight, physicochemical properties different from antibodies, and absence of such molecules from biological samples at to high potential, are to be proven. Assays for BM have also been improved, mostly driven by the challenge to analyze several BM from limited sample volumes.\nVariants of ELISA have become commercially available that allow multiplexed analysis, meaning analysis of multiple biomarkers together in a single assay run. Two types of platforms are available. Antibodies to capture the analyte are spotted either on beads (e.g., latex) or planar surfaces (e.g., polystyrene). 46 Despite more complex assay development for the need to address crossreactivity between the individual detection systems, these assays are highly attractive for the similarity to genomics platform and the possibility of profiling applications.\nMass spectrometry-based methods as applied in proteomics profiling bear the promise of antibody-independent assays. However, the sensitivity of the methods for reliable quantification of biomarkers currently do not fulfill demands. Combination of antibodies to enrich the analytes, followed by MS analysis have advantages over ELISA-based methods for the simultaneous analysis of different species for which no specific antibodies are available (e.g., certain A\u2424 fragments)"}, {"section_title": "ANTIBODIES AND ASSAYS FOR BIOMARKERS: DIRECTION", "text": "Assay sets for the analysis of important pathways are needed to evaluate efficacy and toxicity. Such assay sets are need to analyze samples from animal safety and efficacy studies. Because sample volume from animal studies is very limited, the assays have to be highly multiplexed and available for the usual experimental animal species. Additionally, for the limited access to biofluids in smaller animals directed to analysis of tissue homogenates.\nThe analytes addressed by these multiplexed assay sets will be key molecules in molecular pathways, (e.g., caspases, kinase signaling cascades and their targets, wnt pathway members).\nSuch assays complement genomics analyses with relevant information on the protein level at appropriate sensitivity to generate signatures of drug effects. Similar assays will be applied to analyze biomarkers in early clinical studies (proof of concept). For routine monitoring in late clinical studies and for theranostics, the assays need to be robust, validated, with lower multiplex level and compatible with analysis systems at bedside or doctor's office.\nDevelopment of assays based on aptamers and improvement of mass spectrometry-based technologies may allow analyses of biomarkers for which immunoassays are not satisfying.\nSpecific issues with BM assays such as reference material of the analytes, baseline endogenous BM levels, and matrix interference need to be addressed, together with criteria for the definition of assay range and assay validation. 47 Only valid data measured with high-quality assays allow proper decision making for drug development and comparison of data in cross-study meta analysis. 48 Common guidelines for the development of bioanalytical methods for BM analysis are a needed prerequisite and under preparation."}, {"section_title": "IMAGING", "text": "Today's drug development is largely mechanism driven, or at least has this as an ambition. For a rational development of a neuroactive drug, a number of issues should be clarified, the first one related to drug distribution: Does the drug reach the target? Is the concentration sufficient? Is the residence time in the target tissue as desired?\nSecond comes aspects related to interactions with the primary molecular target: Does the drug interact with the assumed molecular target? Is there an interaction with sufficient number of target molecules per cell? Is the interaction of sufficient duration?\nFinally: Is there a cellular response? Is this cellular response of sufficient degree and duration to induce a therapeutic outcome?\nOnly rarely is it possible in human trials to assess a drugs availability to target tissue, and almost never when we consider drugs with targets in the brain. It has been necessary to extrapolate from animal experiments or rely on indirect measures of drug concentration such as that measured in CSF or using functional read-outs as indication that the drug enters the brain and access the target.\nWith the recent years' development of positron emission tomography (PET) methodology, especially with respect to chemistry and potentials to label an increasing number of organic molecules, 49 it has become a reality to measure directly the distribution and kinetics of drugs in the brain. A labeled drug administered to a human subject can be recorded with very high sensitivity and quantitation accuracy in the brain and its spatial distribution recorded with 4-to 5-mm resolution. The kinetics can be recorded dynamically up to a time set by the half-life of the radionuclide.\nSuch pharmacokinetic studies can be performed at different phases of development of a new drug, or for scientific purposes with an existing drug. A few of these possibilities are exemplified below."}, {"section_title": "PET microdosing", "text": "PET study includes the administration of a chemical entity labeled with a PET radionuclide of the order of 200 -800 MBq of radioactivity. Hence, the injectate contains predominantly nonradioactive compound, at an amount of typically about 5 g. This low amount of drug is sufficiently low that no toxic effects are to be expected and typically such administrations are only made a few times and in very selected populations. Authorities have therefore accepted to allow a significantly reduced toxicological package as safety backup for this type of study, denoted PET microdosing.\nRepresentative of what is currently recommended by the European Medicines Agency (EMEA) and Food and Drug Administration (FDA) can be found on their web sites. Major points from the EMEA guidance are acute toxicity testing in a single species and genotoxicity (http:// www.emea.eu.int/pdfs/human/swp/259902en.pdf). The FDA also endorses acute toxicity in one species, and will waive genotoxicity testing (http://www.fda.gov/ CDER/GUIDANCE/6384dft.htm).\nThese limited requirements, together with the increased labeling potential, makes it realistic to use PET microdosing very early in drug development and learn how the drug distributes in brain and other organs.\nUnder the conditions given below, a PET microdosing study may indicate the passage of drug over the bloodbrain-barrier, its relative concentration in the brain in relation to plasma concentration, and the rate of exchange between brain and plasma. This information can be of utmost value, e.g., in predicting whether the access to brain is restricted under the influence of P-glycoproteins and other efflux pump systems. Although a range of techniques exist today, including animal experiments, the impact of the BBB may be an obstacle for neuroactive drugs and the preclinical methods are not always fully predictable. The possibility to model the relation between plasma and brain opens for more advanced modeling where the brain concentration profile is used in the modeling rather than plasma.\nA PET microdosing study also has the potential to record drug accumulation throughout the whole body and can alert for possible side effects. The PET microdosing concept is especially valuable during the development of new PET tracers, where it allows a new potential tracer to be explored in humans without too extensive preclinical toxicology.\nFurther advances in the use of PET in CNS drug development are occurring in two areas: CNS drug receptor specific radioligands and radioligands for evaluating specific targets in neurodegenerative diseases, especially the amyloid tracers. The latter topic is the basis for a great deal of ongoing preclinical and clinical work and has been recently reviewed. 50 The value of receptor specific radioligands for in vivo imaging was demonstrated by the reduction in binding of D2 dopamine receptor radioligands in striatum that occurs during treatment with antipsychotics. Extent of D2 receptor occupancy correlates with and can even predict therapeutic and adverse effects to a wide range of compounds in this class, although the relationship is stronger for typical antipsychotics with higher affinity for the D2 dopamine receptor than atypical antipsychotics. 51, 52 By way of contrast, doses and exposure limits for drugs entering clinical trials are typically estimated from blood sampling in preclinical studies. Although they are eminently practical, blood or plasma exposure may be poorly related to exposure in brain; due to the blood-brain barrier, special clearance mechanisms, unique physiology, and species differences in target density and distribution. Ignoring the significance of brain exposure can result in costly failures. Recently, several major pharmaceutical companies conducted full-scale clinical trials of the 5HT1a serotonin receptor antagonist pindolol in combination with a selective serotonin reuptake inhibitor (SSRI), based on the idea that this could augment the antidepressant effect of the SSRI. Each of those programs failed, at total costs of a billion dollars. PET studies with the 5HT1a receptor tracer 11 C-WAY100635 conducted after the trials determined that the typical dose of pindolol used was associated with receptor occupancy rates of 19%. 53 This is far below what receptor theory would predict as necessary for an antagonist to compete with a readily available agonist, and well below the 75% occupancy rates observed to be effective in preclinical studies specifically testing the 5HT1a receptor. Thus, the hypothesis was never actually tested. PET and singlephoton emission computed tomography (SPECT) studies with receptor specific tracers have identified dose occupancy relationships for a number of psychopharmaceuticals. Clinically potent doses of antagonists of D2 dopamine, 5HT2a serotonin, M2/3 muscarinic, H2 histamine, and most recently the NK-1 (neurokinin) receptors are associated at least 50% occupancy (whereas potent doses of agonists of opiate receptors, benzodiazepine sites, and again the 5HT1a serotonin receptor are associated with much lower occupancy rates (see, for example, FIG. 7) . 54 In addition, duration of occupancy can also be assessed with serial studies during the elimination phase of the drug. 55 Knowing the dose occupancy relationship of a drug and the duration of central occupancy, can completely reshape the usual long, costly, trial and error approach to phase 2.\nDevelopment of target specific radiotracers has been, and to a large extent still is, a research effort undertaken by academic PET centers. Candidates are identified from marketed drugs, patent literature, and in some instances drug company compound libraries although even then the involvement by the drug company has characteristically ended with the material transfer agreement. Compound characteristics essential for in vivo imaging of specific targets (usually membrane surface receptors) have been identified after years of empirical research by academic PET centers. These include affinity at least 10-fold higher than the Bmax of the target receptor and lipophilicity between 1.0 and 3.5, as well as having a suitable precursor for radiolabeling as the final synthesis step. None of these are necessary nor even desirable criteria for developing clinical candidates. At least moderate lipophilicity favors penetration into the brain and accumulation in lipid compartments (as occurs with many of the SSRIs, for example) in no way limits their usefulness as a therapeutic. As a consequence, although medicinal chemistry resources have made enormous strides in compound development, this has not necessarily translated into rich libraries of in vivo imaging candidates. In the last few years, this has started to change. Several large pharmaceutical companies are building internal imaging groups, including medicinal chemistry resources dedicated to creating radiolabeled derivatives specifically for in vivo imaging. A serious commitment of major pharmaceutical company discovery resources to radioligand lead selection could revolutionize the role of PET and SPECT in early drug development. To illustrate the impact, shortly after the registration of the first serotonin reuptake inhibitors, attempts were made by many academic PET centers to develop a site specific tracer for the serotonin transporter as a tool for understanding the drugs and the diseases they were treating. Almost all of these were initiated by academic investigators relying on center resources. Roughly 14 compounds were tested over a 10-year period, including each of the marketed SSRIs. Most failed due to high nonspecific signal, likely due to high lipophilicity that may have contributed to their therapeutic efficacy. A highly specific ligand was finally developed in Toronto with some support from a pharmaceutical company. Several derivatives with significantly lower lipophilicity than predecessors were developed and tested, leading to the selection of 11 C-DASB. 56 Occupancy of the serotonin transporter by clinical doses of SSRIs was first reported using this radioligand in November 2001, 9 months after the expiration of the patent on Prozac. 57 In contrast to this timeline is an example of parallel testing of several cancer therapeutics before phase 1 clinical studies using PET. Several analogs of the acridine derivative DACA, a"}, {"section_title": "FIG. 7.", "text": "A PET study with the NK 1 receptor-specific tracer 18 F-SPARQ showing dose-dependent occupancy by the emesis drug aprepitant, a selective NK 1 antagonist. The first image on the left is pretreatment (baseline) showing highest uptake in striatum followed by cerebral cortex. With two increasing doses of aprepitant, a pronounced reduction in available NK 1 receptors is observed. The study suggested that over 90% receptor occupancy is required for a therapeutic effect.\nDNA-intercalating antitumor candidate, were radiolabeled with 11 C and tested for in vivo tumor penetrability as part of the clinical lead selection process using PET imaging. 58 Recently, three different ligands for the metabotropic glutamate subtype 5 receptor were tested in parallel at the imaging center in Merck Research Laboratories (Merck & Co., Inc., Whitehouse Station, NJ) to select the optimal radioligand for in vivo imaging before clinical testing of antagonists for this drug target. 59 Such an infusion of resources by a large pharmaceutical company, including compound libraries, medicinal and analytic chemistry, and pharmacology as support, could dramatically reshape the incidence rate and diversity of new radiotracers."}, {"section_title": "PROTEOMICS/METABONOMICS", "text": "Because gene expression analysis (transcriptomics) only indirectly represents the status of a biological system, the development of methods for analysis of the proteome and of the metabolome (the global set of proteins and endogenous metabolites present in a biological sample) is currently advancing rapidly. Availability of such tools is essential for discovery of disease and drug action biomarker in humans. 60 The purpose of comparative proteomic profiling is to identify differences between two or more sets of samples (e.g., treatment vs placebo; healthy vs disease) at the level of protein expression. However, the proteome is intrinsically significantly more complex than gene expression profiling. In particular, it is expected that the human plasma proteome contains several hundred thousand different proteins and peptides, with a dynamic range of more than 1010 between the most and least abundant species. As a consequence, there is no unique technology available today that would allow one to profile a given biological sample in a single experiment, as is the case for gene expression profiling. Rather, to perform proteomic profiling, a number of complementary technologies is usually used.\nMetabolomic profiling has emerged as the most recent global profiling approach. Its goal is to identify differences in abundance of endogenous metabolites and other small molecular weight molecules. As such, it certainly complements transcriptomics and proteomics approaches but is also unique as regulation of endogenous metabolites probably reflect more accurately the physiological state of system than gene or protein expression changes. Even if it is expected that the number of endogenous metabolites in mammalian systems is not going to be as huge as the number of proteins (especially in plasma), little is known about metabolome complexity 61 As suggested above, a significant number of technologies and therefore strategies for protein and metabolite profiling coexist today. Several such techniques have been available for years. Two-dimensional gel electrophoresis for instance was developed in the mid-1970s and was the first method allowing the separation and visualization of several hundreds of proteins in parallel. However, a common trend observed in recent years is the continuous improvement of available tools (resolution power, reproducibility, ability to deal with very large amounts of data, etc.), making proteomic and metabolomic profiling more powerful techniques than ever.\nDue to the complexity of the proteome and of the metabolome, profiling experiments usually consist in several successive steps. One possible example of profiling strategy is described here. In a first step, the samples containing the complex mixtures need to be separated using gel-based or chromatographic methods. Then, differences are determined by statistically analyzing either images (two-dimensional electrophoresis) or mass spectra, resulting in the generation of lists of most significantly up-or down-regulated features (gel spots, mass peaks). The next step consists in determining the identity of the selected regulated proteins or metabolites, usually using mass spectrometry based methods. The identity of a protein consists in its amino acid sequence, including whenever possible information about posttranslational modifications. The identity of a metabolite consists in its elemental composition and class and whenever possible the exact molecular structure. Finally, data interpretation such as pathway analysis and integration with other type of data (gene expression profiling, clinical data) can be performed. All in all, proteomic and metabolomic profiling experiments are still significantly more time consuming and labor intensive than gene expression profiling. 62 However, proteomics and metabolomics methods have a potential significant advantage compared with gene expression profiling for human biomarker identification. Body fluids, especially blood derivatives from clinical drug development, which are the most easily accessible samples available from human subjects, contain either few cell types (blood) or very few cells at all. In the case of diseases of the CNS for instance, one can wonder if candidate biomarkers of disease or drug action can be found at all by analyzing the transcriptome of white blood cells. Body fluids, however, always contain protein and endogenous metabolites, making them ideal substrates for proteomic and metabolomic profiling analysis and biomarker discovery. Ideally of course, the body fluid to be analyzed should be as close as possible to the site on injury (disease biomarkers) or to the drug action (efficacy or safety biomarker). Of specific interest to the neuroscience field is the CSF. 31, 63 A large number of protein and peptide markers identified in CSF and associated with neurological or CNS conditions has already been described. It is expected that many more will be found in the coming years. However, several parameters need to be taken in consideration to identify CSF markers of clinical use. First, it is necessary to have access to a sufficient number of CSF samples from individual subjects. This will allow one to follow changes in the proteome or metabolome of these subjects with time, rather than comparing subjects at a given time point. Second, sampling procedures need to be very well defined and applied to reduce as much as possible analytical variability."}, {"section_title": "62", "text": ""}, {"section_title": "FUTURE DIRECTIONS", "text": "Pharmacogenomics is a cutting-edge technology for studying how populations of specific genes control an individual's response to drug therapy. Associated with pharmacogenetics, such studies are identifying genetic variations in gender, race, and ethnic backgrounds that contribute to the success or failure of therapy.\nThis new approach to medicine already applied to oncology gives physicians a genetic snapshot of each patient's potential response to drug therapy-before treatment even begins. Using gene expression profiling, one can study which of the genes that activate or break down drugs are active in a particular patient. These expression profiles are fingerprints that identify patients likely to benefit from treatment or suffer a toxic reaction.\nAs data accumulate on the genetic basis for how the body metabolizes, transports, and responds to drug increases, it may become possible to select many drugs and their dosages based on the individual patient's inherited ability to metabolize, eliminate, and respond to these medications (FIG. 8) ."}, {"section_title": "Consortiums", "text": "Two publicly funded centers accept and make available gene expression data. In the United States, the National Center for Biotechnology Information, a part of the National Library of Medicine within the National Institutes of Health provides the Gene Expression Omnibus (GEO) database, at http://www.ncbi.nlm.nih.gov/ geo/. In Europe, the European Bioinformatics Institute, a part of the European Molecular Biology Laboratory, provides the ArrayExpress database, at http://www.ebi. ac.uk/arrayexpress/.\nBoth of these sites hold gene expression data sets developed by the efforts of researchers around the world. In keeping with the wide-ranging interests of the researchers who contribute expression data to these databases, representation consists of expression profiles from a variety of organisms and experimental models.\nThe International Genomics Consortium (IGC), a nonprofit medical research organization, was established to expand upon the discoveries of the Human Genome Project and other systematic sequencing efforts by combining genomic research, bioinformatics, and diagnostic technologies in the fight against complex genetic diseases. IGC is funded by the pharmaceutical industry, the biotechnology industry, private foundations, and the University of Arizona. IGC is currently focusing on cancer, but the future fields of focus include MS, cardiovascular diseases, and diabetes. IGC's gene expression data will be made publicly available in the GEO.\nThe National Institute of Standards and Technology (NIST) is working together with industry in establishing a Consortium on Gene Expression Metrology. The aim of this consortium is to develop universal measurement methods to characterize microarray performance, including measures of signal-to-noise ratio, signal-to-background ratio, dynamic range (from minimum to maximum quantifiable amount), and selectivity/specificity.\nThe National Institute of Aging, together with several major pharmaceutical companies and imaging vendors, has organized a consortium to address imaging as a means of measuring disease progression in Alzheimer's Disease: the Alzheimer's Disease Neuroimaging Initiative or ADNI (http://www.loni.ucla.edu/ADNI/). This initiative is a 5-year public-private partnership to test whether serial MRI, PET, other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment and early Alzheimer's disease. The goals of ADNI include development of uniform standards for acquiring longitudinal, multisite imaging data on patients with Alzheimer's disease, mild cognitive impairment, and elderly controls; creation of a data repository, and to determine those methods that provide maximum power to determine treatment effects in trials involving Alzheimer's Disease and mild cognitive impairment.\nConsortiums would be needed in disease biomarkers of complex diseases especially in the field of neurology and neuropsychiatry-such studies would greatly benefit from a complementary approach of \"omics\" (transcrip -FIG. 8 . The integrative approach. The sophisticated combination of clinical research, genetic, genomic, proteomic, imaging data, and information technologies is expected to revolutionize medical treatment allowing a better understanding of the molecular basis of neurological diseases, disease progression as well as patient stratification, increasing the chances of success of a given therapy and leading us toward personalized, or individualized medicine.\ntomics, proteomics, metabolomics) and other technologies such as imaging. Progression of MCI to AD, progression of CIS to MS, and prediction of outbreak of psychosis are just some examples where predictive markers could clearly make a difference for the patient. A uniform effort would be also required in consistently joining the \"omics\" data all the way from discovery to preclinical and clinical development. To truly increase our understanding of the disease and treatments, it is of utmost importance to feedback from the bedside to the basic researches and to make this knowledge available in common databases."}, {"section_title": "Target entities versus block busters", "text": "FDA has recently provided guidelines to industry on the submission of pharmacogenomic data and encourages voluntary submission of experimental gene expression data indicating the importance of the integration of pharmacogenomics into drug development (Guidance for Industry-Pharmacogenomics Data Submission, March, 2005; http://www.fda.gov/cber/gdlns/pharmdtasub.pdf). The agency is considering granting 3-year marketing exclusivity to drugs that are shown to have a positive effect in specific populations. The FDA is also currently discussing how to reimburse companies for pharmacogenomics tests. It is still difficult to convince industry to develop drugs tailored to specific segments of the population. FDA hopes that such offers would encourage companies to test if their products work depending on a patient's genetic makeup (http://insidehealthpolicy.com)."}, {"section_title": "Individualized medicine", "text": "Identification of disease and treatment response-related endophenotypes and tailored treatment of them rather than treating everyone according to the same global practice has come stay. The purpose is to maximize the benefit and minimize the risk for each patient. In oncology, identification of responders and nonresponders is a common practice before initiation of a suitable treatment. Similarly, slowly but surely, pharmacogenomic testing will in the future precede initiation of therapies for neurological and psychiatric diseases. This requires a special effort from the pharmaceutical industry and a willingness to commit to diagnostics. FDA has signaled the way-it is our turn now."}]