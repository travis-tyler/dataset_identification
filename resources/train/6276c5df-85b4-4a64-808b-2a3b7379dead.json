[{"section_title": "", "text": "A draft of the B&B:08/12 interview was field tested in 2011 to assess item wording, evaluate data quality, and determine the length of the interview. In addition, the field test included experiments designed to test both varying item layouts and the efficacy of a set of financial incentives for participation. Appendix C describes the development process and results of the field test. B&B:08/12 employed a data collection technique known as responsive design. Responsive design approaches can have many goals; for B&B:08/12, the responsive design efforts were intended to reduce bias in survey estimates due to nonresponse. Key elements of the B&B:08/12 responsive design were intended to identify sample members who were most likely to contribute to nonresponse bias if they did not respond and to reduce nonresponse bias by increasing response among the identified cases. B&B:08/12 tested a responsive data collection design based on a multivariate distancing measure known as the Mahalanobis distance. Prior to the start of data collection, all sample cases were randomly assigned to control and treatment groups. A Mahalanobis calculation based on the multivariate distance between the baseline respondent average and an individual nonrespondent was calculated at three points during the B&B:08/12 data collection. Nonrespondents with high Mahalanobis distances were considered the most likely to contribute to nonresponse bias, and those in the treatment group were selected for targeted interventions. Cases targeted during the first intervention point received a $15 increase in their promised incentive. Cases targeted as part of the second intervention received a $5 prepay via FedEx. Cases targeted at the third, and final, intervention were offered an abbreviated interview. Results of the experiment showed that the $5 prepaid incentive and abbreviated interview offer increased response rates. However, nonresponse bias analyses did not show significant reductions in nonresponse bias between the treatment and control groups across sampling frame variables or those variables included in the Mahalanobis model. In the full-scale study, 14,600 (or 85 percent) of the 17,140 sample members in the B&B:08/12 cohort completed an interview. Of these completed interviews, 92 percent were completed on the web and 8 percent were completed by telephone. The interview averaged 35.3 minutes to complete overall, with web interviews averaging 34.6 minutes and telephone interviews 43.2 minutes."}, {"section_title": "Administrative File Matching", "text": "In addition to the student interview, data collection for B&B:08/12 included record matching to the Central Processing System (CPS), the National Student Loan Data System (NSLDS), and the National Student Clearinghouse (NSC). These sources provided information from federal financial aid applications, federal records on student loans and Pell Grants, and respondents' postbaccalaureate enrollment, respectively. The CPS contains records for individuals who applied for federal financial aid. About 13 percent of the B&B:08/12 sample had a record in the CPS in the 2012-13 school year. The NSLDS includes only historical records for sample members who received federal loans or Pell Grants. About 13,210 study members (77 percent) matched to NSLDS loan records and 8,810 (51 percent) matched to NSLDS Pell Grant records. B&B:08/12 interview nonrespondents were matched to the NSC StudentTracker database, which provides information on postsecondary enrollment, degree, and certificate records on behalf of participating postsecondary institutions. Overall, a record match for a student's enrollment at any NSC-participating institution was obtained for about 1,150 (45.5 percent) of the B&B:08/12 nonrespondents. B&B staff imputed missing data for many derived variables in accordance with mass imputation procedures described by Krotki, Black, and Creel (2005). After replacing missing data where values could be deduced with certainty based upon logical relationships among observed variables, the weighted sequential hot deck method was used to replace the remaining missing data by imputing plausible values from statistically selected donor cases (Cox 1980;Iannacchione 1982). To reduce error due to imputation, B&B staff performed quality checks throughout imputation. In particular, staff compared the distributions of values before and after imputation and examined the raw data, as needed, to resolve apparent anomalies. Appendix G shows the item response and nonresponse rates among all students for each variable subject to imputation and the pre-and postimputation means and distributions for continuous and categorical variables, respectively."}, {"section_title": "Analysis Weights and Variance Estimation", "text": "Three weights were developed for analyzing the B&B:08/12 data. A bookend weight was developed for analyzing NPSAS items in combination with items directly from or derived from the B&B:08/12 interview. A panel weight was developed for analyzing NPSAS items in combination with items directly from or derived from both the B&B:08/09 interview and the B&B:08/12 interview. A panel transcript weight was developed for analyzing items from all three interviews and transcripts. 2 All weights were adjusted for nonresponse and were also adjusted to Integrated Postsecondary Education Data System (IPEDS) and NPSAS:08 control totals. Two procedures for estimating the variance of survey statistics are the Taylor series linearization procedure and the bootstrap replication procedure. Variables used for both of these variance estimation procedures are available on the B&B:08/12 data files. B&B:08/12 data are available as restricted-use data files or as public use data via the NCES web tools (QuickStats and PowerStats), found at http://nces.ed.gov/datalab. The primary analysis file, from which PowerStats was constructed, shows data for 14,570 respondents and contains over 2,600 variables, developed from multiple sources. Restricted-use data files, associated codebooks, and other documentation are available to researchers who have obtained a license from NCES. See http://nces.ed.gov/statprog/instruct.asp to obtain a license. Additional information on obtaining a data license is available in the NCES Restricted-Use Data Procedures Manual at https://nces.ed.gov/statprog/rudman/. Anyone may analyze B&B:08/12 data through QuickStats and PowerStats. These tools permit analysis without disclosing individual respondents' data to the user. In addition, QuickStats and PowerStats suppress or flag estimates that fail to meet reporting standards. QuickStats allows users to generate simple tables and graphs quickly and easily. PowerStats allows users to generate more complex tables and estimate simple linear or logistic regression models. Unless otherwise indicated, a criterion probability level of .05 was used for all tests of significance conducted for the B&B:08/12 evaluations. Throughout this publication, reported numbers of sample institutions and students have been rounded to protect the confidentiality of institutions and individuals. As a result, row and column entries in tables may not sum to their respective totals, and reported percentages may differ somewhat from those that would result from these rounded numbers."}, {"section_title": "List of Tables", "text": ""}, {"section_title": "Background and Purpose", "text": "NCES conducts several studies to respond to the need for a national and comprehensive database related to postsecondary education. These studies address issues such as access, choice, enrollment, persistence, attainment, continuation into graduate and professional schools, and the benefits of postsecondary education to individuals and to society. 2 B&B is one of several studies sponsored by NCES to address these issues, specifically studying bachelor's degree recipients over time. NCES is authorized to conduct B&B by the following legislation: B&B provides a longitudinal study of the education, work, financial, and personal experiences of individuals who have completed a bachelor's degree at a given point in time. Three distinct B&B cohorts, each sampled almost a decade apart, have allowed researchers to evaluate how baccalaureate degree recipients have fared at differing times in recent history. Students were identified as bachelor's degree recipients through the B&B base-year study, the National Postsecondary Student Aid Study (NPSAS). NPSAS is a nationally representative trend study of postsecondary students designed to determine how students and their families pay for postsecondary education. The first B&B cohort was identified in 1993 as part of NPSAS:93. That cohort was subsequently interviewed in a B&B follow-up in 1994 (B&B:93/94), which included a collection of transcript data. The B&B:93 cohort was surveyed again in 1997 (B&B:93/97) and in 2003 (B&B:93/03). A second B&B cohort began with NPSAS:2000 and involved only a 1-year follow-up in 2001 (B&B:2000/01). NPSAS:08 identified the third and current B&B:08 cohort. B&B:08/09 was conducted 1 year after the base-year NPSAS:08 data collection and included a transcript collection, and B&B:08/12 was conducted 4 years after the base-year study. Figure 1 shows the data collection timelines for the base-year and subsequent follow-up studies for each B&B in the series. B&B covers a number of topics of interest to policymakers, educators, and researchers. The study allows for analysis of both the participation and progress of bachelor's degree completers in the workforce and the relationship of degree type and focus to employment status, income, and ability to repay debt. The study also collects data on entry into, persistence through, and completion of postsecondary education, and B&B:08/12 includes an oversample of students who earned bachelor's degrees in a science, technology, engineering, and mathematics (STEM) field. A special emphasis of B&B is the examination of pathways and experiences of new elementary and secondary school teachers. Many issues related to teacher preparation, entry into the profession (e.g., timing and ease of entry), persistence in teaching, and career movement within education can be examined. Two B&B studies (B&B:93/94 and B&B:08/09) have collected postsecondary transcript data that provide a unique opportunity for analysts to review what courses students take in college and explore relationships between collegiate coursetaking and respondents' postbaccalaureate experiences (work, graduate school, etc.). Although the focus and principal content of B&B interviews in each of these three cohorts have remained relatively consistent, expert panels and other reviews of the interviews have helped to shape and alter questions as needed for relevancy. The B&B:08/12 survey was revised to provide data related to a human capital framework (Becker 1994). B&B also gathers extensive information on bachelor's degree recipients' undergraduate experiences, demographic backgrounds, expectations regarding graduate study and work, and participation in community service. Table 1 summarizes the schedule for the major B&B:08/12 activities. Electronically documented, restricted-access research files (with associated codebooks) and the NCES online application PowerStats have been constructed and are available for use by researchers. In addition to this data file documentation, B&B:08/12 has produced a First Look report that provides descriptive information for the B&B:08/12 cohort, special tabulations on issues of interest to the higher education community, and descriptive reports of significant findings for dissemination to a broad Chapter 2."}, {"section_title": "Schedule and Products", "text": ""}, {"section_title": "Sampling Design", "text": "Identification of the B&B:08/12 sample required a multistage process that began with selection of the NPSAS:08 sample of institutions and was followed by selection of students within these institutions. A third stage confirmed that sample members who, as of NPSAS:08, were expected to complete a bachelor's degree in the 2007-08 academic year were indeed baccalaureate recipients during that academic year and, therefore, were eligible to be sampled for the B&B:08 cohort. All eligible sample members (as determined by the B&B:08/09 interview and the transcripts) were included in the B&B:08/12 sample.\nThe respondent universe for the B&B:08/12 field test consisted of students who completed degree requirements for a bachelor's degree between July 1, 2006, andJune 30, 2007, at "}, {"section_title": "Respondent Universe and Sample", "text": "To be eligible for inclusion in the B&B:08 cohort, a student 3 must have been a student at an institution included in the NPSAS:08 institution universe."}, {"section_title": "Universe for B&B:08/12", "text": "NPSAS:08 Institution universe. To be eligible for the NPSAS:08 sample, institutions had to meet certain criteria during the 2007-08 academic year. They must have \u2022 been eligible to distribute Title IV funds; \u2022 offered an educational program designed for persons who had completed at least a high school education; \u2022 offered at least one academic, occupational, or vocational program of study lasting at least 3 months or 300 clock hours; \u2022 offered courses that were open to persons other than the employees or members of the company or group (e.g., union) that administers the institution; \u2022 been located in one of the 50 states, the District of Columbia, or Puerto Rico; and \u2022 not been a U.S. service academy. Institutions that provided only vocational, recreational, or remedial courses or only in-house courses for their own employees were excluded. U.S. service academies were also excluded because of their unique funding/tuition base. These institution eligibility conditions were consistent with previous NPSAS administrations, with two exceptions. First, the criterion of being eligible to distribute Title IV aid was implemented beginning with NPSAS:2000, 4 and second, previous NPSAS studies excluded institutions that offered only correspondence courses. NPSAS:08 included such institutions if they were eligible to distribute Title IV student aid. NPSAS:08 Student universe. To be eligible for NPSAS:08, students had to be enrolled in a NPSAS-eligible institution in any term or course of instruction at any time from July 1, 2007, through June 30, 2008. Students also had to meet the following requirements: \u2022 be enrolled in any of the following: an academic program; at least one course for credit that could be applied toward fulfilling the requirements for an academic degree; or an occupational or vocational program that required at least 3 months or 300 clock hours of instruction to receive a degree, certificate, or other formal award; \u2022 not be currently enrolled in high school; and \u2022 not be enrolled solely in a General Educational Development (GED) or other high school completion program. NPSAS:08 Student sample. Sample institutions provided lists of their eligible students. As student lists were received from institutions, students were sampled by means of stratified systematic sampling with predetermined sampling rates that varied by student stratum. NPSAS:08 oversampled potential baccalaureate recipients to allow sufficient numbers to be included in the B&B follow-up studies and, thus, stratified them separately from other undergraduate students. Business majors make up a high proportion of baccalaureates; therefore, business majors were undersampled among potential baccalaureate recipients to ensure that the sample did not consist primarily of business majors. Additionally, STEM majors; National Science and Mathematics Access to Retain Talent (SMART) Grant recipients; and Academic Competiveness Grant (ACG) recipients were oversampled to allow the analysis of sufficient numbers of these analytically important subgroups. Further, institutions that awarded proportionally higher numbers of degrees in education majors were oversampled to ensure sufficient numbers for analysis. There were 20 student strata, as follows: 1. in-state potential baccalaureate recipients who were business majors; 2. out-of-state potential baccalaureate recipients who were business majors; 3. in-state potential baccalaureate recipients who were STEM majors and SMART Grant recipients; 4. out-of-state potential baccalaureate recipients who were STEM majors and SMART Grant recipients; 5. in-state potential baccalaureate recipients who were STEM majors and not SMART Grant recipients; 6. out-of-state potential baccalaureate recipients who were STEM majors and not SMART Grant recipients; 7. in-state potential baccalaureate recipients in all other majors who were SMART Grant recipients; 8. out-of-state potential baccalaureate recipients in all other majors who were SMART Grant recipients; 9. in-state potential baccalaureate recipients in all other majors who were not SMART Grant recipients; 10. out-of-state potential baccalaureate recipients in all other majors who were not SMART Grant recipients; 11. in-state other undergraduate students who were SMART Grant recipients; 12. out-of-state other undergraduate students who were SMART Grant recipients; 13. in-state other undergraduate students who were ACG recipients; 14. out-of-state other undergraduate students who were ACG recipients; 15. in-state other undergraduate students who were not SMART Grant or ACG recipients; 16. out-of-state other undergraduate students who were not SMART Grant or ACG recipients; 17. master's degree students; 18. doctoral degree students; 19. other graduate students; and 20. first-professional students. Unlike studies in which an eligible sample member's response to the survey instrument determines whether a particular case is included in the analytic file, the NPSAS family of studies determine study response on the basis of the presence of data on a key subset of variables. This has the effect of preserving some sample members who do not respond to the survey instrument, but for whom robust administrative data exist. A study respondent was defined as any sample member who was eligible for the study and had valid data from any data source for the following variables: \u2022 student type (undergraduate or graduate/first-professional); \u2022 date of birth or age; \u2022 sex; and  Table 3 shows the number of NPSAS:08 students sampled, the number of eligible students, and the unweighted and weighted percentages of study respondents, by institution characteristics. Table 3. NPSAS:08 Sampled and eligible students and response rates, by institution characteristics: 2007-08"}, {"section_title": "Institution characteristics 2", "text": "Sampled students Eligible students 3 Study respondents 1 Unweighted percent  1 A study respondent was defined as any eligible sample member for whom sufficient key data were obtained from one or more sources, including interviews, institution records, and the U.S. Department of Education's Central Processing System (CPS). 2 Institution characteristics were based on data from the sampling frame formed from IPEDS:2004-05 and refreshed from IPEDS:2005-06. 3 Sample member eligibility was determined during the interview or from institution records in the absence of an interview. 4 The base weight was used to produce the estimates in this column. NOTE: Percentages were based on the unrounded count of eligible students. IPEDS = Integrated Postsecondary Education Data System. Detail may not sum to totals because of rounding. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2007-08 National Postsecondary Student Aid Study (NPSAS:08). In previous NPSAS studies from which a B&B cohort was derived, lists of potential baccalaureate recipients were collected with the student list of all enrolled undergraduate and graduate/first-professional students. However, these baccalaureate lists often could not be provided until late spring or summer when baccalaureate recipients could be positively identified; this negatively affected the data collection schedule. To encourage an earlier receipt of enrollment lists, 4-year institutions were asked to include an indicator (B&B flag) of students who had received or might receive a baccalaureate degree during the NPSAS year (between July 1, 2007, andJune 30, 2008). 8 Institutions were instructed to make this identification before spring graduation. Four-year institutions were also asked to include an indicator of class level for undergraduates (1st year, 2nd year, 3rd year, 4th year, or 5th year). From NPSAS:2000, it was estimated that about 55 percent of the 4th-and 5th-year students would be baccalaureate recipients during the NPSAS year and that about 7 percent of 3rd-year students would also be baccalaureate recipients. This class-level indicator was used when the B&B flag was not provided for any students. Because most enrollment lists were received before June 30, 2007, and many were received before April, some sample students identified by the institution as baccalaureate candidates were determined during the NPSAS interview not to be baccalaureate recipients (false positives). Likewise, some sample students not identified by the institution as baccalaureate candidates were determined during the NPSAS interview to have received baccalaureate degrees (false negatives) during the specified time frame. See the 2007-08 National Postsecondary Student Aid Study (NPSAS:08) Full-scale Methodology Report for additional details on the NPSAS:08 sampling (Cominole et al. 2010). B&B:08/09 sample. The sample for B&B:08/09 included 25,050 NPSAS:08 sample members who had been identified as potentially eligible for membership in the B&B:08 cohort. In the first follow-up, eligibility for the B&B:08 cohort was based primarily on information obtained from the student's transcript. For students who lacked a transcript, eligibility was based on responses provided during the NPSAS:08 student interview. For students without either a transcript or a NPSAS:08 interview, eligibility was based on institution records or the enrollment list provided by the NPSAS:08 institution at the time of student sampling. Table 4 shows the distribution of the 25,050 NPSAS:08 sample members by the sources of information used to determine their eligibility. Table 5 shows the final eligibility status, based on transcripts, of the 18,000 B&B:08 sample members whose baccalaureate receipt was confirmed in the NPSAS:08 interview.  In addition to transcripts being collected from the sampled institutions for NPSAS:08 interview respondents, they were requested for the 7,050 NPSAS:08 interview nonrespondents who were either confirmed in student records 9 to be degree candidates or listed by the NPSAS:08 sample institution as bachelor's degree candidates. Approximately 5,150 NPSAS:08 nonrespondents were determined to be eligible or eligibility could not be determined for B&B:08/09 based on transcript data. In order to have full population coverage of the B&B:08/09 sample, a subsample of 500 of the 5,150 NPSAS:08 nonrespondents was selected for inclusion in the first follow-up. To maximize the eligibility rate among the subsample, the 5,150 NPSAS:08 nonrespondents were stratified based on study respondent, transcript, National Student Clearinghouse (NSC), and student record statuses. The NSC data on degree completion were used to identify eligible students but could not identify ineligible students with certainty. Within each stratum, the nonrespondents were first sorted by institution sector to ensure the representativeness of the sample and were also sorted by the NPSAS:08 sampling weight within sector. Then the sample was drawn within each stratum with probabilities proportional to the NPSAS:08 sampling weight. The sampling rates used in each stratum were different in order to maximize response and eligibility rates while also representing the various types of sample members. The B&B:08/09 sample was not designed to be representative at the state level. Table 6 shows the distribution of the potential baccalaureate recipients without a NPSAS:08 interview and the subsample. # Rounds to zero. 1 Students without a NPSAS:08 interview who were not identified as a potential bachelor's degree recipient from student records were identified from the enrollment list. NOTE: A small number of students who were not NPSAS:08 study respondents without transcripts, but who were potentially eligible based on NSC, student records, or the enrollment list, were combined into one stratum for sampling purposes. NSC = National Student Clearinghouse. Detail may not sum to totals because of rounding. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2007-08 National Postsecondary Student Aid Study (NPSAS:08) and 2008/09 Baccalaureate and Beyond Longitudinal Study (B&B:08/09). A B&B:08/09 interview respondent was defined as any sample member who had a completed, partial, or abbreviated B&B:08/09 interview. A student transcript respondent was defined as any sample member who had a transcript provided by the NPSAS:08 institution. A combined interview and transcript respondent was both an interview and a transcript respondent. Table 7 shows details of the B&B:08/09 sample, including the total number of sample members, the number of eligible sample members, and the unweighted and weighted response rates, by institution control, for the interview, There were three types of respondents in B&B:08/12, each corresponding to one of the three analysis weights created for use in this survey. A bookend respondent was defined as any NPSAS:08 study respondent who had a completed, partial, or abbreviated B&B:08/12 interview. A panel respondent refers to any NPSAS:08 study respondent who had a completed, partial, or abbreviated interview for both B&B:08/09 and B&B:08/12. A transcript panel respondent was a panel respondent who also had a transcript provided by the NPSAS:08 institution. Table 9 shows the number of students sampled, the number of eligible students, and the unweighted and weighted response rates, by institution control, for the bookend, panel, and transcript panel respondents. The weighted bookend response rate was 77 percent; the weighted panel response rate was 68 percent; and the weighted transcript panel response rate was 64 percent. Table 9. Number of B&B:08/12 sampled students and eligible students and unweighted and weighted response rates, by institution characteristics: 2012 Chapter 3."}, {"section_title": "Interview Design, Data Collection, Outcomes, and Evaluation", "text": "The B&B:08/12 interview was designed for web and telephone administration. Sample members were primarily located using address and phone sources and were asked to complete the interview between August 2012 and April 2013. Analyses and evaluation of data from an interview field test as well as from previous full-scale B&B studies provided information for consideration when planning B&B:08/12."}, {"section_title": "Interview Design and Systems", "text": "The B&B:08/12 interview consisted of seven sections, grouped by topic. It also offered an abbreviated interview that consisted of selected questions from all sections. This section of the chapter provides the details of the interview design and systems."}, {"section_title": "Interview Design", "text": "As the second follow-up survey of 2007-08 college graduates, the B&B:08/12 interview was designed to collect and update postsecondary enrollment and employment information, as well as key demographic information. The interview design incorporated longstanding questions created for the B&B:93 and B&B:2000 cohorts and the B&B:08/09 interview, in addition to new and revised items developed with input from the study's Technical Review Panel (TRP) and NCES that were tested in the B&B:08/12 field test. For a list of TRP members, see appendix A; for a list of the final set of interview data elements, see appendix B. Design of the full-scale interview began with a review of items asked in the field test and methodological analyses from the field test, such as interview timing, item nonresponse, and help text analysis (see appendix C for the field test facsimile and methodological analysis). This review identified items needing clarification or simplification and items to be included in cognitive testing. The cognitive testing process elicited respondent feedback to broad themes including question terminology and relevance. Recruited respondents represented three groups of particular interest: those enrolled in postbaccalaureate education, those who graduated with a bachelor's degree in a STEM field, and those who had prepared for or were interested in teaching at the K-12 level. Twenty-nine cognitive interviews were completed during three rounds of cognitive testing that took place between October 2011 and February 2012. Project staff monitored cognitive interviews through in-person observation and audio recording. Respondent feedback was reviewed by the B&B:08/12 TRP and NCES and used to refine interview items for the full-scale interview. The B&B:08/12 full-scale interview consisted of seven sections, grouped by topic (figure 2). Respondents were guided through each section of the interview according to skip logic that took into account information recorded as the respondent progressed through the interview. The following are descriptions of the seven interview sections: 1. Eligibility. This section determined interview eligibility for any sample member for whom eligibility status had not yet been determined. Eligible sample members completed bachelor's degree requirements at their NPSAS institution between July 1, 2007, andJune 30, 2008, and were awarded the degree by June 30, 2009. To allow for interview routing for subsequent items, respondents were asked to report on their current marital status, whether they shared financial responsibilities with anyone, and household composition. Respondents who failed to meet eligibility requirements were asked to provide contact information so that project staff could review their eligibility status. After review, project staff recontacted any eligible respondents. 2. Undergraduate Education. The undergraduate education section was administered only to B&B:08/09 nonrespondents. This section collected the names of all colleges, universities, or trade schools attended by respondents prior to receiving a bachelor's degree from the NPSAS institution. For each institution, respondents were asked to provide dates of attendance and enrollment intensity. Respondents who reported attending multiple undergraduate institutions were asked questions about transferring credits to the NPSAS institution. Finally, respondents were asked whether they were satisfied with their choice of major and the quality of education obtained from the NPSAS institution. 3. Postbaccalaureate Education/Training. This section of the interview gathered information from respondents about any postsecondary enrollment since earning their bachelor's degree. If respondents attended any schools after receiving a bachelor's degree from the NPSAS school, they were asked to provide school name(s), type of enrollment (degree or otherwise), field(s) of study, enrollment intensity, degree(s) earned, reasons for attendance, and any financial aid received. Respondents who had not enrolled since receiving a bachelor's degree were asked about their intentions for future enrollment, including whether they had taken a graduate or professional school entrance exam, planned field of study, expected enrollment intensity, and financial assistance. Students who reported receiving education loans were asked for additional information on the repayment status, loan amounts, and monthly payments for those loans. 4. Postbaccalaureate Employment. In the employment section, respondents were asked about all paid employment since receiving their bachelor's degree. Those who had been employed were asked a series of questions about each of their employers, including name, employment dates, job title and duties, earnings, and hours worked per week for the starting and ending jobs with that employer. Respondents were also asked additional items for up to three employers. These items include employer type, industry, benefits, relation of job to bachelor's degree, and job satisfaction. Respondents were also asked about job searches since graduating with a bachelor's degree and activities while not working.\nB&B:08/12 interview content was based on previous interviews created for B&B:93 and B&B:2000 cohorts and on the B&B:08/09 interview. The field test included data elements developed with input from the study's Technical Review Panel (TRP) and from NCES. (See appendix A for a list of TRP members.) Core data elements maintained in the B&B:12 field test interview included degree attainment, continuing or graduate education, employment, debt and finances, family formation, volunteerism, and interest in or preparation for K-12 teaching. Additionally, information on teachers' professional experiences was collected to allow a comparison of teaching professions with other occupations. Of particular relevance to B&B:08/12 are questions concerning whether U.S. colleges and universities are preparing enough graduates in the fields of science, technology, engineering, and mathematics (STEM) and whether college graduates with training in these fields are using that training in the workplace or pursuing graduate education in STEM fields. Items to address these research questions were included in the B&B:08/12 field test. Development of the B&B:08/12 interview included conducting cognitive interviews to provide feedback on the survey. A total of 24 cognitive interviews were conducted in March and April 2011. The cognitive interview process was designed to elicit respondent feedback to broad questions about terminology, experiences, and decisions related to their continued education, employment, and interest and preparation for K-12 teaching. The 24 respondents were divided into three groups of particular interest: those enrolled in postbaccalaureate education, those who graduated with a bachelor's degree in a STEM field, and those who had prepared for or were interested in teaching at the K-12 level. The feedback helped construct new items of interest and refine current survey items. Help text also aided in standardizing the data collected. Each question form in the survey included access to help text that provided details about interview questions and often provided definitions of unfamiliar terms contained in the questions or response options. Respondents could access help text by clicking on the button located under each question in the bottom center portion of the screen and, in some cases, by clicking on linked keywords in the item text (the results of the experiments are discussed in section 2.3). Pop-up messages were used to clarify any inconsistent or out-of-range values entered by respondents. Once the instrument was developed and programmed, rigorous testing was conducted. Project staff and NCES used mock scenarios to test the skip logic, question wording, screen layout, and overall efficiency of the instrument. To identify any potential problems, testing was done from a variety of locations, using a range of connection options, and at various times of day. The entire instrument development process was facilitated by the use of RTI's Hatteras system (described in detail in section 1.3). The use of Hatteras allowed project staff to coordinate testing efforts with NCES. The reliability of responses to interview questions was evaluated through a reinterview containing 29 selected items from the main interview (described in section 3.5.4). Reinterviews were conducted approximately 3 to 4 weeks after the initial interview and generally in the same administration mode as the initial interview. To permit analysis of the reliability of the 29 items, a subsample of approximately 320 interview respondents was randomly selected for reinterview."}, {"section_title": "Kindergarten-12th", "text": "Grade (K-12) Teaching. All respondents were asked about their teaching experience and interest. Respondents who were not currently teaching but had prepared to teach or had considered teaching were asked to discuss any applications for teaching jobs or reasons why they had not applied to be teachers. The bulk of the section included questions for current K-12 teachers only. These questions included teacher certification type; content area certifications; teaching positions held; number and name(s) of primary and secondary schools where they had worked since graduating with a bachelor's degree; grades and subjects taught; perceived level of preparation for teaching; experiences as a K-12 teacher; earnings; hours worked per week; level of job satisfaction; plans for staying in teaching and for moving into other education-related positions; and awareness of teacher loan forgiveness programs. 6. Student Background. In this section, respondents were asked to provide demographic information (e.g., citizenship, military status, and number and age of any dependents). A set of financial items asked about their income and expenses, including mortgage, car, and student loans, and saving for retirement."}, {"section_title": "7.", "text": "Locating. This section of the interview collected contact information to be used for potential follow-up studies. B&B project staff developed an abbreviated interview that included a subset of key items from each section. The abbreviated interview was used as a treatment in the responsive design experiment described in section 3.4.7 and as a final offer to remaining nonrespondents at the end of data collection. Interview administration. Project staff developed a single mixed-mode instrument to be administered via web and telephone. For telephone interviews, the interviewer accessed the web instrument through RTI's Computer-Assisted Telephone Interviewing Case Management System (CATI-CMS), which prioritized cases and provided the appropriate screens and scripts to read during CATI. Interview features. The B&B:08/12 instrument was designed to minimize differences in presentation across modes to the extent possible. For example, the self-administered web survey included features that replicated the guidance and support that a telephone interviewer would provide. To ensure that high-quality data were obtained, and to make the interview process as efficient as possible across the mixed-mode presentation, key features of the mixed-mode design included the following: \u2022 question wording that worked in both aural and visual presentations; \u2022 on-screen instructions for telephone interviewers that helped ensure proper and consistent oral administration of the various types of questions (e.g., whether the response options were to be read aloud); \u2022 help text on every form in the interview to standardize terms and clarify question meaning (help text results are discussed in section 3.5.2); \u2022 conversion text to encourage responses to critical items when these items were left unanswered (conversion text results are discussed in section 3.5.4); and \u2022 pop-up messages to clarify any inconsistent, out-of-range, or improperly formatted values entered by respondents. Mock scenarios were used to rigorously test the skip logic, question wording, screen layout, and overall efficiency of the instrument. Testing was conducted from a variety of internet browsers, using a range of internet connection options, and at various times of day. The instrument development process was facilitated by the use of RTI's Hatteras system (described in detail in section 3.1.2). The use of Hatteras allowed project staff to coordinate testing efforts with NCES and communicate necessary changes. Coding systems. Assisted coding systems (coders) were used to standardize the collection and classification of postsecondary institutions, major or field of study at postbaccalaureate institutions, occupations, and K-12 schools. Respondents or telephone interviewers entered text strings, then matched their entry with options returned from a keyword search linked to an underlying database of standardized terms. The following are descriptions of the individual coding systems and sources: \u2022 The postsecondary institution coder was created from the set of institutions contained in the 2011 IPEDS, developed by NCES (http://nces.ed.gov/ipeds/). The IPEDS database contains all Title IV eligible institutions in the United States and its territories. For any institutions not listed in the database, respondents were asked to provide the control (e.g., public or private) and level (e.g., 2-year or 4-year) of the institution. \u2022 The major coder was constructed using the 2010 Classification of Instructional Programs taxonomy, developed by NCES (http://nces.ed.gov/ipeds/cip2010). For any majors or fields of study not listed in the database, respondents were asked to provide a general major area and a specific discipline. \u2022 The occupation coder was built from the 2010 Occupational Information Network Online (O*NET OnLine) database (http://online.onetcenter.org). For any occupations not listed in the database, respondents were asked to provide a general area, a specific area, and finally, a detailed classification area for the occupation. \u2022 The elementary and secondary school coder (El/Sec Coder) was used to code any elementary or secondary schools where respondents taught. NCES data sources used for schools in the El/Sec Coder were the 2007-10 Private School Universe Survey for private schools (http://nces.ed.gov/surveys/pss/) and the 2009-12 Common Core of Data for public schools (http://nces.ed.gov/ccd/). Respondents were asked to indicate whether the school was public or private and to provide the city and state where it was located. The El/Sec Coder then displayed a list of possible schools, and the respondent was asked to select the correct school from the list. If the right school was not found in any of the data sources, the entered text string was retained, and respondents were asked to supply the control (e.g., public or private) of the school; the names of the school's district, county, or both; and the highest and lowest grade levels offered at the school."}, {"section_title": "Data Collection Systems", "text": "The B&B:08/12 full-scale study employed several systems to collectively develop, implement, and support instrumentation and processes for data collection and reporting. Hatteras Survey Engine and Survey Editor. The web survey instrument was developed using Hatteras, a web-based system in which project staff developed, reviewed, tested, modified, and communicated changes to specifications and code for the instrument. All information relating to the instrument was stored in a SQL Server database and was made accessible through web browser interfaces. Hatteras interfaces and tools included the following: \u2022 Specifications. Hatteras provided the tools and user interface for developing interview specifications. Specification content included wording at the form, question, item, and response option levels; help text content; item-level data documentation; and form-level question administration documentation. Capabilities of the Hatteras system allowed instrument designers to import any relevant specifications used in prior studies, create skip logic and item documentation, and search a library of survey items. Instrument designers were also able to take advantage of a comprehensive comment tracking system to communicate instrument changes and testing results with programmers. A web interface provided access for project staff at remote locations and NCES staff to test and comment on the instrument throughout its development. \u2022 Programming code. For simple instrument questions and items, Hatteras automatically translated specifications into program code. For questions involving complex routing, multiple question wording or response option conditions, or nonstandard page layout or behavior, programmers entered custom programming code-HTML, JavaScript, and C#.NET script-into the Hatteras custom code interface. This code was stored in the SQL Server database, together with the instrument specifications for compilation by the survey execution engine. \u2022 Instrument testing and execution. Hatteras allowed immediate testing of specification and code content via a web URL. Based on the specifications and custom code, the survey execution engine automatically handled such web instrument functions as backing up and moving forward, recording instrument timing data, displaying critical-item wording, validating user input, displaying conditional instructions based on interview mode (web, telephone, or field), and linking to context-specific help text. \u2022 Survey sites and data transfer-web/telephone. For web and telephone data collection, the Hatteras system was installed on the NCES surveys web servers and SQL Server database. Web respondents accessed the survey directly by web browser after logging in with a user ID and password or by following a supplied direct-login link. RTI's telephone interviewers accessed the same NCES web survey site by means of a web browser process launched from the CATI-CMS (described below). All connections to the NCES web interview were secured with Secure Sockets Layer (SSL) encryption. Automated processes transferred data between the NCES database and RTI's database via a secure, encrypted connection. RTI's database was housed in an Enhanced Security Network (ESN), a separate storage network that was certified to meet the standards required for protection of data classified as National Institute of Standards and Technology moderate. User access to the ESN is restricted by two-factor login security. The systems used to support the B&B:08/12 full-scale data collection included the Integrated Management System (IMS), the Receipt Control System (RCS), CATI-CMS, and the intensive Tracing Operations System (TOPS).\nThe interview web instrument was developed using Hatteras, a web-based system developed by RTI in which project staff developed, reviewed, tested, modified, and communicated changes to specifications and code for the instrument. All information relating to the instrument was stored in an SQL Server database and was made accessible through web browser interfaces. Hatteras provided specification, programming, and testing interfaces for the B&B instrument. All aspects of the study were monitored using the Integrated Management System (IMS), a project management tool designed to give project staff and clients ready access to a repository of reports, critical project information, and deliverables. Daily reports and management information from all the major systems across the study resided in the IMS. The IMS contained the current project schedule, monthly progress reports, daily data collection reports and status reports, project plans and specifications, project deliverables, instrument specifications, a link to the instrumentation system, staff contacts, the project bibliography, and a document archive. The Receipt Control System (RCS) was used to control and monitor all activities related to data collection, including tracing and locating. Through RCS applications, project staff were able to perform such activities as e-mailing to groups of sample members, preparing lead letters and followup mailings, executing batch tracing, reviewing locating information, tracking case status, and viewing telephone interviewer (TI) comments. The RCS was fully integrated with the Computerassisted telephone interviewing Case Management System (CATI-CMS) and Tracing Operations System (TOPS) so that all systems needing sample member data accessed a single database. Case status changes in the interview, CATI-CMS, or TOPS automatically triggered updates to RCS during overnight processes, providing integration between the data collection systems and the ability to identify problems early and implement solutions effectively. The CATI-CMS scheduled telephone calls to be made by TIs and tracked call outcomes. Within the CATI-CMS, TIs had the ability to send a reminder username/password e-mail to callers who chose to complete the interview via the Web and to initiate SMS text reminders to those who requested this service. TOPS was used to locate the sample members that CATI operators could not locate. TOPS allowed tracers to review all case data, including TI comments in the CATI-CMS, and to use various search methods to locate sample members."}, {"section_title": "IMS.", "text": "All aspects of the study were monitored using the IMS, a project management tool designed to give project staff and clients ready access to a repository of reports, critical project information, and deliverables. Daily reports and management information from all the major systems across the study resided in the IMS, accessible via the web, and protected by SSL encryption and a password-protected login. The IMS contained the current project schedule; monthly progress reports; daily data collection reports and status reports (generated by RCS); project plans and specifications; project deliverables; instrument specifications; a link to the instrumentation system; staff contacts; the project bibliography; and a document archive."}, {"section_title": "RCS.", "text": "RCS refers to the control system database and its integrated set of applications used to control and monitor all activities related to data collection, including tracing and locating. Through the control system applications (bundled under the name Symphony), project staff were able to perform such activities as e-mailing groups of sample members, preparing lead letters and follow-up mailings, sending cases for batch tracing, reviewing locating information, tracking case statuses, and viewing comments from telephone interviewers. The control system was fully integrated with both the CATI system and TOPS, such that all systems accessed sample member data from a single database. Case status changes in the interview, CATI, or TOPS systems automatically triggered updates to the RCS during overnight processes, providing seamless integration among the data collection systems as well as the ability to identify problems early and implement solutions effectively."}, {"section_title": "CATI-CMS.", "text": "The CATI-CMS scheduled telephone calls to be made by telephone interviewers and tracked call outcomes. Cases who could not be located were set to a need tracing status, which made them available immediately for TOPS. Quality control supervisors and project managers used the CATI-CMS to manage cases based on factors such as call frequency, call outcomes, and institution sector. Managers could also re-assign cases or put cases on hold and review them as necessary. Within the CATI-CMS, telephone interviewers had the ability to send a reminder username/password e-mail to callers who wished to self-complete the interview. All data captured by telephone interviewers were entered via the CATI-CMS into the control system database in the ESN. These data were thus immediately accessible to TOPS and other control system utilities (such as e-mail and postcard reminder generation applications)."}, {"section_title": "TOPS.", "text": "During data collection, TOPS allowed tracers to work with cases for whom CATI operators had been unable to locate the sample members. TOPS enabled tracers to review all of a case's data, including comments left by telephone interviewers in CATI and use various search methods to attempt to find current contact information. When TOPS found new locating data, the data were stored in the RCS, where the data became immediately available to control system utilities for reminder e-mails and letters, as well as to CATI for immediate call scheduling. If a tracer in TOPS located a sample member via telephone, the call was switched to a CATI operator for immediate interviewing. TOPS supervisors were able to manage the tracer's loads and review cases as needed.\nCases who could not be located by other methods were sent to TOPS. These included cases who did not have a telephone number to load into the CATI-CMS at the start of data collection and cases for whom all known numbers resulted in a dead end during initial data collection efforts. Intensive tracing was divided into two tiers: TOPS-1 and TOPS-2. The first tier (TOPS-1) searched for sample members in consumer databases by matching SSNs. Database batch searches included LexisNexis, Experian, and Accurint. If this search resulted in a new telephone lead, the case was immediately returned to CATI for follow-up by telephone interviewers, minimizing the time that cases were unavailable for outbound dialing. If the search resulted in a new address only, tracers used directory assistance searches to locate a telephone number for the contact. In the second tier (TOPS-2), tracers searched for additional sample member information or other contacts that could provide a potential lead to the sample member. Tracing staff conducted a thorough review of each case and determined the appropriate next steps based on the leads developed from prior tracing and contacting activities. Tracers utilized consumer databases, such as Quality experts. Quality experts monitored interviewer performance via the Quality Evaluation System to ensure that the data collected met the highest possible quality standards. During each session, quality experts used special audiovisual stations to silently observe a percentage of all calls made by B&B interviewers. Quality experts attended interviewer project training and met regularly with project staff throughout data collection to discuss interviewing protocols and their concerns. Tracing staff. Tracing staff responsibilities are described in section 3.2.2. Tracers completed a 16-hour training program on general tracing techniques, led by tracing managers within RTI's CCS. Tracers also completed two additional hours of project-specific training, including an overview of B&B:08/12, review of frequently asked questions, and tracing techniques most appropriate for locating B&B:08/12 sample members. Additional training. Concepts from training sessions were reinforced in bi-weekly quality circle meetings, which provided a forum for brief trainings on specific topics, such as case management techniques, updates to project procedures, or strategies for administering complex interview forms. Telephone interviewers were also encouraged to ask questions, which helped identify needs for training topics for subsequent quality circle meetings. Selected staff received additional trainings on specific topics, including refusal conversion techniques. Telephone interviewers, quality control supervisors, and quality experts all received a booklet outlining best practices in managing cases and administering the B&B instrument."}, {"section_title": "Data Collection", "text": "B&B:08/12 student data collection included a single web-based interview in two modes: selfadministered web and telephone. The data collection effort included training for telephone interviewers and supervisors; tracing and locating sample members; mail, e-mail, and telephone prompting; and refusal conversion. Telephone interviewing, help desk, and tracing activities were conducted by RTI's Call Center Services (CCS) group, housed in the Research Operations Center. Telephone interviewers assisted sample members who needed assistance with a password, or had questions about the interview, or who called in to complete the interview by phone. The interviewers also made outbound calls to sample members, prompting them to complete the interview by phone or web. The procedures are detailed in this section."}, {"section_title": "Study Website and Help Desk", "text": "B&B:08/12 sample members were provided with a link to the study website in all mail and e-mail communications and were referred to the website to obtain information about the study. The website provided detailed information about the study, including the study sponsor, answers to frequently asked questions (FAQs), information on confidentiality and data security procedures, findings from previous studies, links to the NCES and RTI websites, and contact information for the data collection staff. Sample members used their login credentials to access the secure website and complete the interview. Figure 3 shows the home page for the B&B:08/12 study website. The study website was designed in compliance with NCES web policies, which incorporate a three-tier security approach to protect all data. The first tier requires a secure login, including a unique study ID and strong password provided to sample members. The second tier protects any information entered on the website with SSL technology, which transmits only encrypted data. The third tier of security requires collected data to be stored on a secured SQL Server database that is physically separate from the web server. The B&B:08/12 help desk opened in conjunction with the start of data collection. A toll-free help desk number was established and staffed 7 days a week during the day and evening. Help desk staff were trained to answer questions about the study, provide technical support, and conduct telephone interviews. The help desk is further described in section 3.2.4."}, {"section_title": "Locating, Tracing, and Contacting Sample Members", "text": "B&B:08/12 used a multistep approach to locating, tracing, and contacting sample members, in which the most cost-effective steps were taken first to minimize the number of cases who required more expensive tracing efforts (figure 4). Prior to the start of data collection, database searches and contact information update mailings were conducted. Throughout data collection, follow-up locating methods were employed, including CATI locating and additional tracing. Sample members who were not successfully located in batch tracing were sent to TOPS, as described in section 3.1. Batch tracing. Batch database searches were conducted to update sample member contact information prior to the start of data collection. These searches were conducted for all sample members with information sufficient for matching (such as sample member name and Social Security number [SSN]), regardless of whether they already had contact information on file. The database searches used for B&B:08/12 were as follows: \u2022 The Central Processing System (CPS) contains locating information for students who have applied for financial aid using the Free Application for Federal Student Aid (FAFSA). \u2022 The National Change of Address Database (NCOA) contains 160 million change-of-address records submitted to the U.S. Postal Service. Data are maintained for 4 years and updated weekly. \u2022 Lexis Nexis (formerly FirstData) Phone Append offers a residential telephone number search of over 170 million listings, including 6 million listings for recent relocates. Phone Append returns a telephone number based on a search by name, street address, and ZIP code. The information obtained from these sources was merged with the information previously available from the NPSAS:08 and B&B:08/09 locator databases. An additional batch search using Lexis Nexis Premium Phone was conducted during data collection for cases in which all existing numbers resulted in no contact with the sample member. Premium Phone searches over 475 million landline, Voice over Internet Protocol (VoIP), and wireless numbers in the United States, Puerto Rico, and Canada. Initial contact mailings. In July 2012, about 2 weeks before the start of the B&B:08/12 data collection, an initial contact mailing was sent to sample members and their parents to request up-to-date contact information. This mailing included a study brochure, a letter with detailed information about B&B:08/12 signed by the associate commissioner of NCES, a contact information update sheet, a business reply envelope, and instructions for updating information online. If either the sample member or parent updated or confirmed contact information, sample members were sent a $10 check. Data collection mailings. At the start of data collection on August 21, 2012, sample members were sent a data collection announcement mailing and e-mail. The mailing included a study brochure and a letter that announced the start of data collection. The letter, signed by both the B&B project director and the NCES project officer, informed sample members of the incentive for completing the interview, provided the study website and login credentials for accessing the web interview, and provided the study's toll-free help desk number and e-mail address. The e-mail included equivalent information, along with a direct link to the interview. Additional reminders were sent periodically to nonrespondents throughout data collection, including reminder e-mails, postcards, and letters sent in FedEx packages. (See appendix D for data collection materials.) CATI locating. In addition to interviewing and prompting sample members to complete the interview, telephone interviewers helped locate sample members. When making outbound calls, the telephone interviewers attempted to gather locating information from parents or other contacts who answered the call. Interviewers then followed up on the leads generated by those contact attempts. If this approach was not successful, the interviewer used other information-such as previous study call records-available for the sample member and other contacts to try to locate the sample member. When the interviewer had exhausted all tracing leads available, the case was sent to intensive tracing."}, {"section_title": "Interviewing", "text": "B&B:08/12 interviews were conducted between August 21, 2012, and April 15, 2013. Data collection for the B&B:08/12 interview consisted of two phases: the early response phase, during which sample members were invited to complete the web interview, and the production phase, during which sample members were prompted to complete either the web or telephone interview with regular outbound phone calls, mailings, and e-mails (figure 5). This approach allowed the most responsive cases to complete the interview before beginning more expensive outbound prompting efforts. Help desk. Throughout data collection, telephone interviewers and quality control supervisors also answered calls to the toll-free help desk number. They were trained to assist sample members with a variety of issues, such as verifying the sample member's identity, looking up web interview login information for the sample member, immediately sending login credentials to sample members by e-mail, scheduling text message reminders, and updating the sample members' contact information. Because the help desk was staffed by telephone interviewers and supervisors, the help desk staff were also able to immediately conduct the telephone interview with inbound callers. Help desk staff logged each call, recording a description of the issue and its resolution. During B&B:08/12, the help desk responded to 511 inbound calls and 92 voicemail messages-86 in English and 6 in Spanish. The most common types of help desk calls included requests for more information about the study, information on incentive checks not yet received, and requests to retrieve login credentials. To reduce the need for telephone assistance, the B&B:08/12 website featured a Need your study ID or Password? link that allowed sample members to retrieve their own password or study ID immediately."}, {"section_title": "Data Collection Quality Control", "text": "A number of quality control procedures were implemented throughout the B&B:08/12 interview data collection. These procedures included frequent monitoring of telephone interviews, a help desk that tracked and resolved difficulties encountered by sample members attempting to complete the web interview, quality circle feedback meetings, and help desk agent and interviewer debriefings at the conclusion of the study."}, {"section_title": "Interview Monitoring", "text": "Regular monitoring of telephone interviews during B&B:08/12 data collection was conducted to meet the following data quality objectives: \u2022 identification of problem items in the interview; \u2022 improvement in interviewer performance through reinforcement of effective interviewing strategies; \u2022 reduction in the number of interviewer errors; and \u2022 assessment of the quality of the data collected. Quality experts and project staff monitored live and recorded interviews throughout data collection, using remote monitoring telephones and computer equipment. They recorded observations on standardized monitoring forms that covered such topics as interviewer professionalism, question administration, and knowledge of the instrument. After each monitoring session, interviewers received feedback based on observations from the session. Issues identified during monitoring were frequently incorporated into quality circle meetings to improve the quality of telephone interviews. Segments of recorded interviews were also employed as training aids during project trainings and quality circle meetings."}, {"section_title": "Quality Circle Meetings", "text": "Quality circle meetings were held regularly throughout data collection to maintain strong communication between data collection staff and telephone interviewers, to communicate the goals and progress of the study, and to address challenges encountered along the way. These meetings provided staff with the opportunity to discuss the survey instrument, to share sample member cooperation strategies, to motivate staff toward response rate goals, and to obtain insight on various data collection issues. Quality circle meeting topics included \u2022 updates on data collection and overall study progress; \u2022 review and discussion of interview questions, response options, and help text; \u2022 guidance on interviewing techniques (such as active listening); \u2022 guidance on strategies for gaining cooperation from sample members and other contacts and general refusal conversion and aversion techniques; \u2022 data security protocols; and \u2022 guidance on case review activities. After each quality circle meeting, data collection staff prepared a detailed newsletter summarizing the meeting discussion. The newsletter included updated counts of interview responses, answers to interviewers' questions, discussion of issues specific to the interview instrument, and special topics. All interviewing, tracing, and supervisory staff were required to read the newsletter."}, {"section_title": "Debriefing", "text": "At the end of data collection, telephone interviewers completed a debriefing questionnaire and participated in group debriefing meetings to discuss the information provided in the questionnaire. Questionnaire topics included training and communication, the interview instrument, the CATI-CMS and case management systems, and the techniques and tools for locating and contacting sample members. The results obtained from the debriefing questionnaire were discussed and successes and areas for improvement in project training and data collection were identified. For example, telephone interviewers suggested improving the training module on managing inbound calls and using more challenging practice interview scenarios in training. They also reported that the tools and support they received throughout data collection helped them to overcome the challenges associated with gaining cooperation from sample members. A debriefing report was prepared to summarize results of the debriefing and document considerations for planning future studies."}, {"section_title": "Data Collection Outcomes", "text": "This section provides the results of the B&B:08/12 interview data collection. Details of the overall interview response rate of 85 percent are included, as is a description of the success of various locating methods. A timing analysis shows that the interview, on average, took about 35 minutes to complete."}, {"section_title": "Student Locating Results", "text": "Overall locating and response rates for B&B:08/12 varied by several factors, including the response status in the two prior studies and the NPSAS:08 base-year institution type. Located and response rate results, by first follow-up interview response status and institution type, are shown in table 11. # Rounds to zero. 1 Through the course of data collection, approximately 20 sample members were found to be deceased. The deceased cases have been excluded from the final eligible sample (17,110) but are included in this analysis of data collection results. 2 Sample members were counted as \"located\" if they were ever located at some point during data collection. 3 Interviewed count includes eligible sample members who met the criteria for qualification as an interview respondent, which required completing at least a partial interview. NOTE: Percentages were unweighted. Detail may not sum to total because of rounding. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2008/12 Baccalaureate and Beyond Longitudinal Study (B&B:08/12). First follow-up (B&B:08/09) interview respondents were more likely than B&B:08/09 interview nonrespondents to be located and to complete the second follow-up (B&B:08/12) interview. B&B:08/09 respondents had a located rate of 98 percent in B&B:08/12, compared with only 87 percent for B&B:08/09 nonrespondents (\u03c7 2 = 646.26, p < .001). Ninety percent of all B&B:08/09 respondents completed B&B:08/12, compared with only 53 percent of B&B:08/09 nonrespondents (\u03c7 2 = 1976.51, p < .001). Located rates for B&B:08/12 ranged from 94 percent (for sample members from for-profit 2-year or more institutions) to 100 percent (for those from public 2-year institutions). Response rates ranged from 81 percent (for sample members from for-profit 2-year or more institutions) to 100 percent (for those from private nonprofit 2-year or less institutions). Located rates by source of locating data. Contact information for the B&B:08/12 sample was obtained and confirmed by matching with various sources of locating data. This process successfully confirmed contact information or provided new contact information for 32,240 records (36 percent). Prior to data collection, the sample was matched with CPS, NCOA, Phone Append, FAFSA, and the National Student Loan Data System (NSLDS). Premium Phone and the SSN Search were used only as needed for cases who required additional tracing efforts. Depending on the source, a matched record could have been a new or confirmed address, e-mail, or phone number. Locating results by tracing source are shown in table 12. Among the cases assigned to intensive tracing, approximately 89 percent were located (table 15). Of those cases located through intensive tracing, about 39 percent completed the interview. "}, {"section_title": "Interview Response Rates", "text": "Completed interviews by first follow-up (B&B:08/09) response status and interview type. About 95 percent of B&B:08/09 interview respondents completed a full interview, compared with 80 percent of B&B:08/09 interview nonrespondents. Eighteen percent of B&B:08/09 interview nonrespondents completed an abbreviated interview, compared with 4 percent of B&B:08/09 interview respondents (table 16). Completed interviews by mode of administration. B&B:08/12 interviews were completed by web or by telephone. Most of the interviews-about 9,440 (65 percent)-were completed by web without telephone contact; 3,960 (27 percent) were completed by web with telephone contact; and 1,200 (8 percent) were completed by telephone (figure 6). Completed interviews by data collection phase. The B&B:08/12 data collection design consisted of two operational phases. The early response phase included the first 4 weeks of data collection during which there were no outbound calling activities. The majority (56 percent) of B&B:08/12 interviews were completed during the early response phase (table 17). The remaining 44 percent were completed during the production phase, which comprised the remainder of the study and was marked by the start of outbound calling. The early response phase of data collection yielded a 48 percent response rate, with 8,150 completed interviews out of 17,140 cases. The next phase of data collection, the production phase, yielded a 70 percent response rate, with 6,300 completed interviews out of the remaining 8,990 cases. "}, {"section_title": "Interview Timing Burden", "text": "To assess the burden associated with completing the B&B:08/12 interview, the time required for each respondent to complete the full-scale interview was collected and analyzed. Special attention was paid to differences by mode, items with consistently high administration times, and the time required to navigate particular interview paths. To record the time it took to complete the interview, whether administered online or by telephone, a time stamp was embedded on each web screen, or form, of the interview. A start timer recorded the clock time on a respondent's or interviewer's computer when a form was first loaded, and an end timer recorded the clock time when the Next button was clicked. For each form administered to a respondent, time was calculated by subtracting the start time from the end time. Total instrument time was calculated by summing the times recorded for all forms. Only cases who completed the entire interview in a single session were included in the analysis. Outliers, defined as form response times exceeding 2 standard deviations from the interview-level mean, were also excluded. Overall, respondents took an average of 35.3 minutes to complete the B&B:08/12 interview (table 18). Web interviews averaged 34.6 minutes, while telephone interviews took significantly longer, at 43.2 minutes (t(1,023) = 20.94, p < .001). 10 Telephone interviews required more time than web interviews for all sections, and all differences were significant: Eligibility (t(1,080) = 20.38, p < .001); Undergraduate Education (t(639) = 2.73, p < .01); Postbaccalaureate Education (t(10,028) = 6.83, p < .001); Postbaccalaureate Employment (t(10,028) = 11.58, p < .001); Teaching (t(914) = 6.09, p < .001); Background (t(1,177) = 16.88, p < .001); Locating (t(10,028) = 18.67, p < .001). Interview time by path. The path respondents had followed to complete the B&B:08/12 interview varied by whether respondents had been enrolled in a degree or certificate program, whether they had any postbaccalaureate employment, and whether they had taught since receiving a bachelor's degree. It also varied by their previous interview response status. Depending on their prior study response status, some respondents were asked questions related to study eligibility and undergraduate experiences to capture information that had been collected in earlier interviews. NPSAS:08 nonrespondents completed the interview in an average of 36.5 minutes, while NPSAS:08 respondents took an average of 35.3 minutes (table 19). B&B:08/09 nonrespondents completed the interview in an average of 37.9 minutes, while B&B:08/09 respondents took an average of 35.1 minutes. In the Postbaccalaureate Education section, respondents were asked about any additional postsecondary attendance since their last interview. The questions in this section covered both degree and certificate programs as well as nondegree coursework. Respondents who had not attended a postsecondary institution as part of a postbaccalaureate degree program or for nondegree coursework were asked about any plans for future enrollment. Respondents who had attended as part of a postbaccalaureate degree or certificate program took an average of 9.4 minutes to complete the section, while respondents who had not attended as part of a postbaccalaureate degree or certificate program since the last interview took an average of 3.5 minutes (table 20). Respondents who were not currently attending to obtain a postbaccalaureate degree or certificate but planned to attend in the 2012-13 school year took an average of 2.7 minutes to complete the section. The Postbaccalaureate Employment section focused on the job(s) that respondents held in the years after completing a bachelor's degree. This section collected information regarding all periods of employment and unemployment since graduation, focusing on job duties, earnings, benefits, job satisfaction, and job search activities. On average, the Postbaccalaureate Employment section took 15.2 minutes to complete (table 21). Respondents who had been employed since completing their bachelor's degree (about 97 percent) spent 15.6 minutes on the Postbaccalaureate Employment section. Those who had no employment to report spent 1.3 minutes. Respondents who reported only one job took 10.2 minutes to complete, those with two or three jobs required an average of 17.9 minutes, and respondents with four or more jobs took 25.1 minutes to complete the section. The Teaching section collected information about the respondent's experiences with, or interest in, teaching at the Kindergarten through 12th-grade level (K-12). The K-12 Teaching section took an average of 1.6 minutes to complete (table 22). Respondents who had never been a teacher and were not planning or preparing to become one took an average of 30 seconds to complete the section. Respondents who were current or former teachers and those who were either considering or preparing to teach took 4.3 minutes. The latter group can be divided among current teachers (6.6 minutes), former teachers (4.7 minutes), respondents preparing to become teachers (2.1 minutes), and respondents considering to become teachers (2.0 minutes). Average form times were compared across all forms in the instrument. 11 Of the forms in the B&B:08/12 interview, the current occupation coder (B12DOCC01) had the highest average observed administration time at 1.5 minutes. The time required to complete the occupation coder (and other coders in the interview) was expected, given that coders required the respondent or telephone interviewer to (1) enter text strings on the form; (2) hit Enter to conduct a keyword search on an underlying database; and (3) select a response from the returned list of possible matches. Forms requiring respondents to provide specific details about their job (including starting and ending job title, salary, hours per week, and full-time vs. part-time status [B12DEMPLOY201]), and about their employer (including employer name and location [B12DEMPLOY01]), also had relatively high administration times, at 63.3 seconds and 40.3 seconds, respectively. A calendar-style form that collected months of employment after completing a bachelor's degree through the present month (B12DWKMON01) also had high administration times. This form requested information from previous years, which required respondents to take time to recall information going back as far as July 2007 and record the responses. Forms that collected several pieces of information on the same screen by utilizing a radio button grid also had high administration times (B12CFINAIDG01, B12CFACS, and B12ETHNKINFL). These forms required additional response time because respondents provided answers to several questions on one screen (table 23). NOTE: Variables that end in \"01\" are items that were asked in multiple iterations, depending on the number of jobs held or schools attended. The form-level time presented here is the average for a single iteration. The timing analysis included only cases who completed or partially completed the interview in one session; outliers were excluded. Forms in the locating section were excluded. Detail may not sum to totals because of rounding. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2008/12 Baccalaureate and Beyond Longitudinal Study (B&B:08/12). Timing of abbreviated interview. The abbreviated version of the B&B:08/12 interview included the Eligibility section and key questions from the other sections of the interview. Only cases who completed the entire abbreviated interview in a single session were included in the timing analysis. Outliers, defined as form response times exceeding 2 standard deviations from the interview-level mean, were also excluded. On average, the abbreviated interview took 20.2 minutes (table 24). Overall, web interviews took 19.8 minutes and were significantly shorter than telephone interviews, at 23.1 minutes (t(87) = 3.74, p < .001 ). Respondents spent the most time on the Postbaccalaureate Employment section of the abbreviated interview, as was the case with the full interview. Telephone interviews required more time than web interviews for all sections, and the following sections had significantly longer average interview lengths for telephone interviews: Eligibility (t(117) = 2.56, p < .05); Teaching (t(653) = 5.49, p < .001); Background (t(200) = 7.78, p < .001); and Locating (t(653) = 5.31, p < .001). "}, {"section_title": "Telephone Interviewer Hours", "text": "B&B:08/12 telephone interviewers logged about 5,827 hours, with 897 telephone interviews completed. An average of 7 calls were made to each case overall, with prior round respondents receiving an average of 14 calls per case. Telephone interviewer hours were spent on interviewing and case management activities, which included locating and contacting sample members, prompting sample members to complete interviews, reviewing case events from current as well as prior data collections with this cohort, scheduling appointments for callbacks, recording events in the case management system, and responding to incoming calls to the help desk."}, {"section_title": "Number of Calls to Sample Members", "text": "The average number of calls placed per sample member varied by B&B:08/12 interview response status, prior round (B&B: 08/09) response status, and interview mode (web or telephone). B&B:08/09 interview respondents received an average of 6 calls per case, compared with 14 calls per case for B&B:08/09 nonrespondents (t(2,493) = 32.19, p < 0.001). B&B:08/12 respondents required an average of 5 calls per case, with web respondents requiring an average of 4 calls and telephone respondents requiring an average of 11 calls (table 25). However, when excluding web respondents who required no calls (such as cases completed during the early response phase), web respondents required an average of 14 calls, 3 more calls on average than their telephone interview counterparts (t(2,036) = 9.3, p < .001). Sample members who did not respond or were ultimately excluded or determined ineligible for the B&B:08/12 interview received an average of 20 calls per case. "}, {"section_title": "Refusal Conversion", "text": "Strategies for converting and averting refusals were integrated into the telephone interviewer training activities and were reinforced in quality circle meetings throughout data collection. In quality circle meetings, telephone interviewers and project staff often discussed the most effective strategies for gaining and maintaining cooperation from sample members as well as the best practices for averting and converting refusals. Sample members who refused to participate were assigned to a special queue and handled only by interviewers who received specialized training on refusal conversion techniques. A total of 7 percent of all eligible cases ever refused; 33 percent of those cases eventually completed an interview (table 26). "}, {"section_title": "Responsive Design Experiment", "text": "B&B:08/12 tested a responsive data collection design based on a multivariate distancing measure known as Mahalanobis distance. A key requirement of a responsive data collection design was the ability to identify nonrespondents who were most likely to contribute to nonresponse bias. The likelihood for nonresponse bias can then be reduced by increasing response among the identified cases. The Mahalanobis distance value measures the extent to which a nonrespondent differs from the average across the respondents and therefore assists in identifying target cases. A Mahalanobis calculation based on the multivariate distance between the baseline respondent average and an individual nonrespondent was calculated at several points during the B&B:08/12 data collection. The calculation was based on the following covariates that were known or imputed for both respondents and nonrespondents: \u2022 race/ethnicity; \u2022 age; \u2022 parents' education; \u2022 marital status; \u2022 immigrant generational status; \u2022 disability; \u2022 Carnegie code; B&B:08/12 Data File Documentation \u2022 selectivity of institution; \u2022 baccalaureate field of study; \u2022 enrollment in degree program; \u2022 expected family contribution; \u2022 SAT score; \u2022 earned income; \u2022 employment history; \u2022 grade point average; and \u2022 time to receive bachelor's degree. Prior to the start of data collection, all sample cases were randomly assigned to control and treatment groups. Treatment group cases with a high Mahalanobis value were targeted during data collection at three points in time. At each of the three points: \u2022 Mahalanobis values were evaluated for all remaining nonrespondents; \u2022 a cut point was determined to distinguish low-and high-distance cases based on the Mahalanobis distribution and sample size; \u2022 cases were assigned to low-and high-distance groups on the basis of the cut point; and \u2022 treatment cases within the high-distance group were eligible for interventions as defined below. Phase 1 -additional incentive. The first 3 months of data collection included web data collection and CATI-light, which involved a minimal number of phone calls, mainly to prompt web response. After the first 3 months, Mahalanobis values were evaluated for the remaining nonrespondents, and treatment cases above the cut point were offered a $15 incentive in addition to their original of $20, $35, or $55. Once a case became eligible for the additional $15, it remained eligible for that amount even if it moved into the low-distance group later. Phase 2 -$5 prepay incentive via FedEx. After an additional month of data collection (4 months overall), Mahalanobis values were evaluated again for the remaining nonrespondents, and those treatment cases above the new cut point received a $5 prepay incentive via FedEx. Phase 3 -abbreviated interview. After an additional 2 months of data collection (6 months overall), Mahalanobis values were evaluated again for the remaining nonrespondents, and those treatment cases above the cut point were offered an early abbreviated interview. The experiment was closed 2 weeks before the end of data collection, and at that time all remaining nonrespondents were offered an abbreviated interview. Several questions were used to outline the analytical framework for the full-scale experiment. The questions and the results of the related analyses are described below. Were response rates improved among high-distance cases with the use of increased incentive amounts, a $5 prepay via FedEx, and earlier abbreviated interviews? Response rates were examined for the high-distance control and treatment groups to determine whether the overall response rates for the treatment and control groups differ significantly. While the goal of this approach was to minimize bias-and not necessarily to increase response rates-a higher response rate among the treatment group was a necessary requirement for a reduction in bias. Table 27 shows the number of respondents and nonrespondents, the weighted response rate, and whether the difference between the treatment group and the control group was significant. The weighted response rates for the treatment group were significantly higher than those for the control group during Phase 2 (13 percent vs. 5 percent) and Phase 3 (10 percent vs. 5 percent), which accounted for an overall significant difference during all three phases (30 percent vs. 21 percent). Chapter 3. Institution Data Collection Design,Outcomes,and Evaluation 44 B&B:08/12 Data File Documentation  1 Exclusions had a final disposition code (e.g., Out of the country), which meant they were no longer being worked in the field. NOTE: Detail may not sum to totals because of rounding. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2008/12 Baccalaureate and Beyond Longitudinal Study (B&B:08/12)."}, {"section_title": "Did outcome measures differ for high-and low-distance cases?", "text": "The Mahalanobis distance measure was used to identify cases believed to be important in terms of bias reduction. One way to assess the effectiveness of this approach was to compare outcomes between high-and low-distance cases to determine whether they were in fact different. A significant difference in outcome measures between high-and low-distance cases would indicate that the Mahalanobis model, which used the proxy variables described earlier in this section, correctly identified cases who were different from each other. Two types of outcome measures were assessed: (1) outcomes based on survey responses (available only for survey respondents) and (2) outcomes based on administrative data (available for the entire sample). Among the survey-based outcome measures (using the bookend weight), there was a significant difference between high-and low-distance respondents for 9 out of 16 interview outcome measures (table 28). For survey respondents, high-distance cases held more jobs but spent more time unemployed since earning a bachelor's degree than lowdistance cases. High-distance cases were older when they received their bachelor's degree and were more likely to have dependent children than low-distance cases. Outcomes available for the entire sample included indicators of postbaccalaureate enrollment from NSC and federal financial aid information from the NSLDS. There were no significant differences between high-and low-distance sample members for federal aid estimates (table 29), but high-distance sample members were less likely than low-distance sample members to have enrolled in another postsecondary program or attained another postsecondary degree since their 2007-08 bachelor's degree. "}, {"section_title": "Was nonresponse bias reduced by targeting and converting high-distance cases?", "text": "The effect of the responsive design approach on unit nonresponse bias was measured by comparing nonresponse bias estimates for the treatment group with nonresponse bias estimates for the control groups. The null hypothesis is that there is no difference in unit nonresponse bias between respondents assigned to the treatment group-including high-and low-distance cases-and respondents assigned to the control groupincluding high-and low-distance cases. Nonresponse bias was estimated and tested to determine if bias was significant at the p < .05 level for the variables in the Mahalanobis model listed earlier in this section and for the following frame variables: The mean and median absolute bias were computed across the categories of these variables. Results showed that the mean absolute bias was not significantly different between the treatment group and the control group for any comparisons, indicating that the inclusion of the treated high-distance cases did not significantly reduce nonresponse bias. This result may have been due to the homogeneous population and the relatively high response rate. However, the percentage of variable categories that was significantly biased was lower in the treatment group than in the control group (table 30). For example, using the bookend weight with frame and Mahalanobis model variables, the control group had significant bias in 31 of 65 variables tested (48 percent). In comparison, the treatment group had 18 of 66 variables with significant bias (27 percent). This suggests that, while the overall measure of bias-mean absolute bias-was not significantly different between the treatment and control groups, bias may have been reduced for some individual variables. "}, {"section_title": "Evaluation of Interview Data Quality", "text": "Evaluation of the B&B:08/12 interview items included analyses of the data collected in the instrument coders and a review of help text access rates, success rates for conversion text, and item nonresponse."}, {"section_title": "Instrument Coding Systems", "text": "Assisted coding systems were used to standardize data collection and code postsecondary schools attended, majors or fields of studies, occupations, and any elementary or secondary schools where the respondent may have taught. Text strings were collected from the respondent, and then a keyword search of an underlying database was conducted, allowing the respondent to select the best option from a list of possible options returned. For a description of each coder, see section 3.1.1. Recoding. Ten percent of the major and occupation codes chosen in the interviews were randomly selected for recoding, a process in which expert coding staff reviewed codes. To review data and provide information for improving instrument coders in future studies, expert coding staff assessed the accuracy of codes chosen in the interview based on the text string provided by the respondent. Across modes of administration and coders, expert coding staff generally agreed with the codes chosen for text strings in the interview. Overall, expert coding staff agreed with major and occupation codes chosen in the interview 93 percent of the time and agreed with recoded codes chosen to a new value about 4 percent of the time. They were unable to choose a code due to vague text strings about 2 percent of the time. Neither the major nor occupation coder showed significant differences in recode rates between modes of administration. The rate of recoded values-same as original code, recoded to different value, or text string too vague to code-chosen by the expert coding staff for the major and occupation coders in the interview are shown in table 31. Upcoding. In a process known as upcoding, project staff chose an appropriate code for any text strings provided by respondents or interviewers for which a code was not selected in the postsecondary, high school, major or field of study, and occupation coders. Text strings from web interviews generally required more upcoding than text strings from telephone interviews because interviewers received special training on coders. The major and occupation coders showed significant differences in upcoding rates between modes of administration. The upcoding rate among web interviews (6 percent) was higher than that for telephone interviews (2 percent) (\u03c7 2 (1, N = 8,426)= 16.36, p < .001) for the major coder (table 32). Three percent of web interviews required upcoding, compared with 1 percent of telephone interviews (\u03c7 2 (1, N = 24,114) = 46.82, p < .001) for the occupation coder. "}, {"section_title": "Help Text Use", "text": "Question-specific help was provided on each B&B:08/12 interview screen. The questionspecific help provided definitions of key terms and phrases used in the question wording and response options and provided any other explanations thought to help clarify and standardize the meaning of questions for respondents. The number of times that respondents accessed help text for the first time on each screen relative to the number of respondents who were administered the question determined the rate of help text access for that screen. 12 The screen-level rate of help text access was analyzed overall and by mode of interview administration to identify questions that may have been problematic for respondents. The overall mean rate of help text hits per screen was less than 1 percent on forms administered to at least 50 respondents. The mean rate of help text hits per screen was significantly higher for telephone interviews compared with web interviews, with rates of approximately 3 percent and less than 1 percent, respectively (\u03c7 2 (1, N = 881,642) = 3,818.26, p < 0.05). Twelve interview questions administered to at least 50 respondents had an overall help text access rate of 2 percent or greater (table 33). The interview question asking Aware of TEACH Grant Program (B12ETCHGRT) had the highest observed overall rate, at approximately 7 percent, and did not differ by mode of administration (\u03c7 2 (1, N = 1,785) = 1.00, p = 0.32). The interview question with the second highest observed overall rate asked respondents to report their Current activity since not employed (B12DEMPOTH); this interview question had an overall rate of approximately 6 percent and did not differ by mode of administration (\u03c7 2 (1, N = 1,169) = 2.19, p = 0.14). Four interview questions with 2 percent or greater overall rates of help text access showed significantly higher rates of access during telephone interviews than during web interviews. These questions include the following: Able to complete bachelor's degree without 2-year college (B12BNP2YR) (x 2 (1, N = 299) = 21.54, p < 0.001); Had vocational or technical certificate or industry certification or occupational license (B12CLICFILT) (x 2 (1, N = 13,641) = 317.0, p < 0.001); Private student loan interest rate (B12CPRIVRT) (x 2 (1, N = 2,424) = 20.69, p < 0.001); and Retirement account types (B12FRETIR) (x 2 (1, N = 13,641) = 224.21, p < 0.001). The help text was accessible on every question through a help text button; however, some questions also offered an additional hyperlink within the question or response option text to allow access to the help text (figure 7). Whether accessed through the help button or through the hyperlink, respondents and interviewers saw the same help text that provided definitions and explanations to clarify and standardize the meaning of questions. Help text hyperlinks were only available on strategically selected questions that had potentially complex or unfamiliar terms. On some questions, the hyperlink appeared in question wording; while in other questions, the help text appeared in response option wording. (For more information on the impact of hyperlinked item text on help text use, see the field test help text experiment in appendix C.) The method through which respondents accessed help text was analyzed to identify the impact of help text hyperlinks. Because interviewers received training on help text use, this analysis included only web respondents. The data were further restricted to items that provided both a help button and a help text hyperlink, and interview questions that had an overall help text access rate of at least 2 percent overall. Of the 56 interview questions that provided both a help text button and hyperlink for respondents to access help text, only nine of these questions had a help text access of at least 2 percent overall. Eight of the nine questions showed significantly higher rates of help text access through the help text hyperlink than through the help text button, and one showed no difference between the methods of help text access (table 34). The question with the highest observed rate of help text access via a help text hyperlink was Estimated amount of private student loans (B12CPRIVEST). Every web respondent who accessed help text on this question used the hyperlink available on the question wording alternative or private loans. The item Total amount borrowed in private student loans (B12CPRIVAMT) also provided a hyperlink on the question wording alternative or private loans; it showed a significantly higher rate of help text access through the hyperlink 84 percent of the time vs. the help text button, with a 16 percent rate of access (x 2 (1, N = 275) = 129.89, p < 0.001). Five other questions provided a help text hyperlink within question wording and showed a significant difference between hyperlink and button access rates: Shares financial responsibilities with household adult (B12AFINCON) (x 2 (1, N = 216) = 156.74, p < 0.001); Postbaccalaureate school 1: average hours worked while attending (B12CENEMP01) (x 2 (1, N = 136) = 92.24, p < 0.001); Job 1: start of career (B12DCURL01) (x 2 (1, N = 221) = 154.86, p < 0.001); Aware of teacher loan forgiveness programs (B12ELNFRGV) (x 2 (1, N = 62) = 58.06, p < 0.001); and Aware of TEACH Grant Program (B12ETCHGRT) (x 2 (1, N = 118) = 114.03, p < 0.001). For these questions, respondents used the hyperlink to access help text from 91 to 99 percent of the time. Finally, two questions provided the help text hyperlink in the response option wording. A question asking about respondent's Retirement account types (B12FRETIR) (x 2 (1, N = 350) = 307.38, p < 0.001) and the item Had vocational or technical certificate or industry certification or occupational license (B12CLICFILT) (x 2 (1, N = 389) = 327.63, p < 0.001) provided help text hyperlinks on each available response option. Of respondents who accessed help text, approximately 95 percent used one of the available hyperlinks to access help text on each of these questions."}, {"section_title": "Item-Level Nonresponse", "text": "The rate of nonresponse was used to identify troublesome interview items and to better understand the experiences of sample members in completing the interview. Total nonresponse rates were calculated for items with missing data (including don't know responses) that were administered to at least 100 respondents. Overall, the item-level nonresponse analysis yielded 38 out of 602 interview items with more than 15 percent missing data. 13 The interview items with the overall highest observed nonresponse rates were Estimated federal student loan monthly payment (B12CELNMEST), Estimated private student loan interest rate (B12CPRIVREST), and Job 1: level of education industry (B12DEDIND01). The two estimate items (B12CELNMEST and B12CPRIVREST) appeared only to respondents who earlier refused to provide a response to an original question and may explain the high nonresponse on these items. Of the 140 respondents who received the estimate item Estimated federal student loan monthly payment (B12CELNMEST), approximately 79 percent did not provide a response. Likewise, of the 234 respondents who received the estimate item Estimated private student loan interest rate (B12CPRIVREST), approximately 59 percent either answered don't know or refused to provide a response. Approximately 1,604 respondents saw the item Job 1: level of education industry (B12DEDIND01), of whom 61 percent provided no response. Twenty-five of the 38 interview items with nonresponse rates of 15 percent or greater were presented in groups as matrix questions. The number of items presented in each matrix varied. One matrix question displayed nine items including an other category and collected information on postbaccalaureate school 1 aid type. All matrix questions presented items except the other category and yielded levels of nonresponse that ranged from 17 to 48 percent. In addition, two other matrix questions collected information on current activity since not employed and activity while not working. These item sets displayed five and six items, respectively, and yielded nonresponse rates ranging from 18 to 29 percent. Item-level nonresponse rates were also examined by mode of administration. There were significant differences in nonresponse rates between the web and telephone modes for 26 of the 38 interview items with more than 15 percent of data missing. All 26 items had a higher rate of nonresponse among web interviews. The item-level nonresponses for items administered to at least 100 respondents with a rate of more than 15 percent of data missing are summarized in table 35. The table also identifies itemlevel nonresponse differences between the web and telephone interview modes.  "}, {"section_title": "Conversion Text", "text": "Eleven questions in the interview were considered critical-responses to these questions were especially important to the study. When respondents did not provide an answer to these questions and they clicked the Next button on the interview screen, the question was redisplayed with conversion text explaining the importance of that particular question to the study and emphasizing the confidential nature of responses. The text was displayed in both web and telephone modes. In telephone mode, telephone interviewers were asked to read the conversion text to respondents and then to reread the interview question. Once the conversion text was triggered, some critical questions also displayed a don't know response option in the event the respondent could not reliably answer. A response conversion rate was calculated by dividing the total number of responses to the critical questions after the conversion text was displayed by the total number of cases where the conversion text was triggered. The conversion rates for the 11 critical items by mode of administration are shown in table 36. Eighty-six percent of nonresponses to the critical items were converted to responses after the conversion text was displayed. Web interviews accounted for 880 of the 1,020 cases where conversion text was triggered and for 800 of the 880 converted cases. Telephone interviews accounted for the remaining cases where conversion text was triggered; approximately 70 of these cases were converted. Critical items administered on the web were converted at a significantly higher rate than the telephone conversion rate (88 percent vs. 70 percent) (x 2 (1, N = 1,018) = 36.95, p < .001). Conversion text was triggered more than 100 times for two interview questions. Respondent's income in 2011 (B12FINCOM) triggered conversion text in 470 cases and yielded a conversion rate of 90 percent with no significant difference in mode of administration. Monthly rent or mortgage payment amount (B12FMTGAMT) triggered conversion text in 170 cases and produced a conversion rate of 69 percent. Web interviews for this item were converted at a rate of 77 percent, compared with telephone interviews that were converted at a rate of 30 percent (x 2 (1, N = 170) = 25.58, p < .001)."}, {"section_title": "Summary of Interview Design and Data Collection Findings", "text": "B&B:08/12 full-scale interviews were conducted from August 2012 through April 2013. Of the 17,160 sample members, 14,600 (or 85 percent) successfully completed a full or abbreviated interview or were considered to be final partial interviews completed through the grid loop of the Employment section (160 final partial interviews). Successful locating methods included matches with locating data sources such as Telematch and CPS and address update information provided by both sample members and parents of sample members. Overall, only about 5 percent of the B&B:08/12 sample required intensive tracing. Attempts to increase participation during B&B:08/12 data collection included text message reminders, paper mailout reminders, and frequent e-mail contacts. Fifty-six percent (8,150 cases) of the surveys were completed in the early response phase (begun in August 2012 and lasting approximately 4 weeks) and 44 percent (6,300 cases) were completed in the production phase. Ninety-two percent of interviews were completed by web and 8 percent of interviews were completed by telephone. Sixty-five percent of completed web interviews required no telephone contact to encourage participation. Eighty-nine percent of first follow-up respondents completed the B&B:08/12 interview, compared with 51 percent of first follow-up nonrespondents. The B&B:08/12 full-scale study included a data collection experiment that utilized a monetary incentive structure that ranged from $20 to $70 and was based on results of an experiment conducted in the field test. Sample members who completed the interview received $20, $35, $50, $55, or $70 based on the predicted likelihood of a respondent's participation and his or her status in the responsive design experiment. The full-scale experiment was designed to evaluate RTI's ability to decrease nonresponse bias by identifying a nonrespondent's likelihood of contributing to bias should he or she remain a nonrespondent. This experiment had several key steps: identify those cases who were least like the current nonrespondents; offer targeted cases additional incentives or other treatment to increase their response propensities; and evaluate the impacts of the intervention. Utilizing the responsive design methodology, at three points throughout the data collection, individuals who were most likely to contribute to nonresponse bias were identified and offered higher monetary incentives or alternative data collection efforts. The responsive design experiment produced mixed results. Results of the experiment showed that the $5 prepaid incentive and the abbreviated interview offer increased response rates. Some outcome measures differed for high-and low-distance cases and between treatment and control groups, but nonresponse bias analyses did not show significant reductions in nonresponse bias between the treatment and control groups across sampling frame variables or variables included in the Mahalanobis model. Although overall nonresponse bias was not reduced by targeting and converting high-distance cases, there were fewer variable categories with significant bias in the treatment group than in the control group. The B&B:08/12 interview was based in part on core data elements used in previous B&B interviews and was tested prior to data collection through cognitive interviewing. On average, the survey took 35.3 minutes to complete. Overall, web interviews were significantly shorter (34.6 minutes) than telephone interviews (42.6 minutes). The time required to complete the interview varied by the respondent's status as a K-12 teacher, whether the respondent had participated in additional education, and the number of jobs held after obtaining a bachelor's degree. Teachers (current, former, and those who prepared for or were interested in teaching) had a longer path through the interview, requiring, on average, 39.2 minutes to complete it. Those who had continued their education after receiving a bachelor's degree took an average of 37.0 minutes to complete the interview. Those who held four or more jobs took an average of 45.5 minutes to complete the interview, compared with 30.1 minutes for those with only one job. The abbreviated interview, administered to 730 respondents, took 20.6 minutes, on average. Evaluation of the quality of the data provided by the B&B:08/12 survey showed that methodological features built into the instrument, such as the design of assisted coding systems, conversion text, and help text aided in the successful administration of the interview. Ninety-three percent of the codes selected by the respondent or interviewer matched the codes selected by expert coders. Eighty-six percent of the cases where conversion text was triggered in the interview were converted to a response. Help text on individual interview screens was accessed less than 1 percent of the time. The item-level nonresponse analysis yielded 38 out of 602 interview items with more than 15 percent of data missing. Debriefing of interviewers at the end of data collection indicated that frequent monitoring of telephone interviewers, quality circle training, and feedback meetings were all useful as data collection quality control procedures. Most telephone interviewers indicated that they had all the tools necessary to successfully administer the B&B:08/12 survey and provided recommendations for future training topics, particularly focusing on gaining sample member cooperation to complete interviews."}, {"section_title": "Chapter 4. Administrative Data Sources", "text": "The data files for B&B:08/12 contain student-level data collected from administrative databases, interviews, and transcripts. These data are available to users in two ways. For restricted data licensees, a set of fully documented restricted research files are available on a CD from NCES. For any user, tables and regression analyses can be run through the public NCES online application PowerStats, via the DataLab site at http://nces.ed.gov/datalab/index.aspx. PowerStats also contains variable documentation. This chapter describes the administrative databases used and details the record matching processes applied to each."}, {"section_title": "Overview of Data Sources", "text": "In addition to the interview, data collection for B&B:08/12 included record matching to the Central Processing System (CPS), the National Student Loan Data System (NSLDS), and the National Student Clearinghouse (NSC). This section provides a discussion of the observed match rates for these databases."}, {"section_title": "Central Processing System", "text": "The CPS files contain data provided to the U.S. Department of Education by students and their families when they complete the Free Application for Federal Student Aid (FAFSA). Successful record matching to CPS occurred only for sample members who were federal student financial aid applicants for the years requested. Matching for B&B:08/12 was to CPS data for the 2010-11, 2011-12, and 2012-13 financial aid years, using a sample member's SSN concatenated with the first two letters of the last name to form the CPS ID. The percentage of sample members who matched to CPS for the 2010-11 academic year was about 23 percent (table 37). The rate was approximately 19 percent for 2011-12 and about 13 percent for 2012-13. This decrease in match rates year-overyear was expected, as fewer members of this cohort continue to be enrolled in postsecondary education and to apply for federal aid. "}, {"section_title": "National Student Loan Data System", "text": "At the request of the U.S. Department of Education, the NSLDS contractor performed NSLDS matching using names, SSNs, and dates of birth provided by RTI. Successful NSLDS matching could occur only for sample members who were awarded federal loans or Pell Grants. NSLDS files are historical, so information about a student's receipt of such loans and grants was available not only for the current academic year but also for any applicable prior years. Consequently, historical match rates reported for B&B:08/12 sample members do not necessarily reflect just the match rates for the 2012-13 academic year. The federal loan match rate was about 77 percent, and the Pell Grant match rate was about 51 percent (table 38). "}, {"section_title": "National Student Clearinghouse", "text": "In addition to the CPS and NSLDS file matching, the B&B:08/12 interview nonrespondents were matched to the NSC StudentTracker database, which provides information on postsecondary enrollment, degree, and certificate records on behalf of participating postsecondary institutions. In order to perform the match, RTI supplied the NSC with SSNs, names, and dates of birth for sample members who were interview nonrespondents. Overall, a record match for a student's enrollment at any NSC-participating institution was obtained for about 45.5 percent of the B&B:08/12 nonrespondents. Match results shown in table 39 were based on enrollment and degree records from all participating institutions for the 2007-08 academic year through the 2012-13 academic year. "}, {"section_title": "Chapter 5. Data File Processing and Preparation", "text": "As described in chapter 4, the B&B:08/12 data files contain individual-level data collected from interviews and administrative databases. These files are available as restricted research files for restricted-use licensees or as publically available data through PowerStats. This chapter describes the main study data files and details the editing and data preparation process."}, {"section_title": "Main Study Data Files", "text": "The primary analysis file, from which PowerStats was constructed, contains data for 14,570 respondents. The primary analysis file contains over 2,600 variables, developed from multiple sources. Throughout the data collection period, data were processed and examined for quality control purposes. Editing of student data began shortly after the start of web data collection, when procedures and programs for this purpose were first developed. Anomalous values were investigated and resolved, where appropriate, through the use of data corrections and logical recodes. Interim files were delivered to NCES for review throughout the data collection period. Complete data for B&B:08/12 are located in the restricted-access files and were documented by detailed codebooks. The restricted files are available to researchers who have applied for and received authorization from NCES to access the restricted-data use files. Researchers may obtain authorization by contacting the NCES Data Security Office. The restricted-use B&B:08/12 files are listed below:"}, {"section_title": "NPSAS:08 and B&B:08/12 Files", "text": "\u2022 B&B:08/12 analysis file. Contains analytic variables derived from all B&B data sources and selected direct interview variables available as of the release of B&B:08/12 PowerStats. /DATA/DERIVED/B12DERIVED/B12DERIVED_DATAFILE.CSV \u2022 NPSAS:08 analysis file. Contains analytic variables derived from the NPSAS:08 study for the B&B:08/12 sample members. /DATA/DERIVED/B12N8DERIVED/B12N8DERIVED_DATAFILE.CSV \u2022 AYP data file. Contains Annual Yearly Progress (AYP) records from 2009-2011 for schools whose NCES School ID's were reported by respondents as schools in which they worked. There is a separate record for each school. /DATA/SOURCE/AYP0911/B12AYP0911_DATAFILE.CSV \u2022 NPSAS:08 ACG/SMART Grant data file. Contains raw grant-level data from the National Student Loan Data System for the B&B:08/12 sample members who received Academic Competitiveness Grants during the 2009-10 academic year or prior years. This is a history file with separate records for each transaction in the file. /DATA/SOURCE/B12N8ACGSMART/B12N8ACGSMART_DATAFILE.CSV \u2022 NPSAS:08 ACT data file. Contains ACT survey and score data for B&B12 sample members who matched to the 2001-02 through 2006-07ACT files. /DATA/SOURCE/B12N8ACT/B12N8ACT_DATAFILE.CSV \u2022 NPSAS:08 institution data file. Contains institution-level data for the B&B:08/12 sample members collected during 2007-08. /DATA/SOURCE/B12N8INSTITUTION/B12N8INSTITUTION_DATAFILE.CSV \u2022 NPSAS:08 school info data file. Contains institution data obtained from the NPSAS:08 student interview for the B&B:08/12 sample members. It is a student-level file with at least one record for each respondent; however, a student can have more than one record in the file. There is a separate record for each postsecondary institution students reported in the interview as somewhere they had attended during the study year (up to 5 institutions). /DATA/SOURCE/B12N8SCHINFO/B12N8SCHINFO_DATAFILE.CSV \u2022 2008 National Postsecondary Student Aid Study (NPSAS:08) file. Contains the base-year data included in the NPSAS:08 data file collected for the B&B:08/12 sample members from institutional records and from student interviews. There is a separate record for each study respondent. /DATA/SOURCE/B12N8STUDBASE/B12N8STUDBASE_DATAFILE.CSV \u2022 Common Core of Data (CCD) data files. Contains Common Core of Data (CCD) records from the academic year for schools whose NCES School ID's were reported by respondents as schools in which they worked. There is a separate record for each school. /DATA/SOURCE/B12N8STUDBASE/CCD* 2007-08: file contains about 1,420 matched respondents 2008-09: file contains about 1,420 matched respondents 2009-10: file contains about 1,410 matched respondents 2010-11: file contains about 1,410 matched respondents 2011-12: file contains about 1,400 matched respondents \u2022 CPS data files. Contains data received from the CPS for the eligible sample members who matched to the financial aid application files. /DATA/SOURCE/CPS* 2010-11: file contains about 3,930 matched respondents 2011-12: file contains about 3,280 matched respondents 2012-13: file contains about 2,240 matched respondents \u2022 B&B:08/12 student interview data file. Contains interview data collected from 14,570 B&B:08/12 respondents. Topics include eligibility, undergraduate and graduate education, employment, teaching, and student background. /DATA/SOURCE/INTERVIEW/B12INTERVIEW_DATAFILE.CSV \u2022 NSLDS loan data file. Contains raw loan-level data received from the National Student Loan Data System for respondents who received federal loans as of November 2012. This is a history file with separate records for each transaction in the loan files. /DATA/SOURCE/NSLDS_LOAN/B12NSLDSLOAN_DATAFILE.CSV \u2022 NSLDS delinquency data file. Contains raw delinquent loan-level data received from the National Student Loan Data System for respondents who received federal loans as of November 2012. This is a history file with separate records for each transaction in the loan files. /DATA/SOURCE/NSLDS_LOAN_DELINQ/B12NSLDSDELINQ_DATAFILE.CSV \u2022 NSLDS deferment data file. Contains raw loan-level deferment data received from the National Student Loan Data System for respondents who received federal loans as of November 2012. This is a history file with separate records for each transaction in the loan files. /DATA/SOURCE/NSLDS_LOAN_DFR/B12NSLDSDFR_DATAFILE.CSV \u2022 NSLDS consolidation data file. Contains raw loan-level consolidation data received from the National Student Loan Data System for respondents who received federal loans as of November 2012. This is a history file with separate records for each transaction in the loan files. /DATA/SOURCE/NSLDS_LOAN_GP/B12NSLDSGP_DATAFILE.CSV \u2022 NSLDS maturation history data file. Contains raw loan-level maturation data received from the National Student Loan Data System for respondents who received federal loans as of November 2012. This is a history file with separate records for each transaction in the loan files. /DATA/SOURCE/NSLDS_LOAN_MAT/B12NSLDSMAT_DATAFILE.CSV \u2022 NSLDS outstanding interest balance data file. Contains raw loan-level outstanding interest balance data received from the National Student Loan Data System for respondents who received federal loans as of November 2012. This is a history file with separate records for each transaction in the loan files. /DATA/SOURCE/NSLDS_LOAN_OIB/B12NSLDSOIB_DATAFILE.CSV \u2022 NSLDS outstanding principal balance data file. Contains raw loan-level outstanding principal balance data received from the National Student Loan Data System for respondents who received federal loans as of November 2012. This is a history file with separate records for each transaction in the loan files. /DATA/SOURCE/NSLDS_LOAN_OPB/B12NSLDSOPB_DATAFILE.CSV \u2022 NSLDS loan repayment data file. Contains raw loan-level repayment data received from the National Student Loan Data System for respondents who received federal loans as of November 2012. This is a history file with separate records for each transaction in the loan files. /DATA/SOURCE/NSLDS_LOAN_RPMT/B12NSLDSRPMT_DATAFILE.CSV \u2022 Pell grants data file. Contains raw Pell grant data received from the National Student Loan Data System for respondents who received federal loans as of November 2012. This is a history file with separate records for each transaction in the loan files. /DATA/SOURCE/NSLDS_PELL/B12NSLDSPELL_DATAFILE.CSV \u2022 Private School Survey (PSS) data file. Contains the most recent PSS records for schools whose NCES ID was reported by B&B:08/09 respondents as schools in which they taught. There is a separate record for each school. /DATA/SOURCE/PSS* 2007-08: file contains about 150 matched respondents 2009-10: file contains about 150 matched respondents \u2022 B&B:08/12 weights file. Contains all of the sampling and analysis weights created for B&B:08/12, including transcripts (containing a separate record for each study member). This file also has all weight history variables containing intermediate weight and adjustment factors, as well as the final institution and student weights created for B&B:08/12 (containing a separate record for each study member). /DATA/SOURCE/WEIGHTS/B12WEIGHTS_DATAFILE.CSV"}, {"section_title": "B&B:08/09 Files", "text": "These files were provided as part of the B&B:08/09 ECB restricted data release, and are being re-released as part of the B&B:08/12 Restricted Use Files. \u2022 B&B:08/09 analysis file. Contains analytic variables derived from all B&B data sources and selected direct interview variables available as of the initial release of B&B:08/09 PowerStats. /B&B 2009 with PETS/datafiles/SAS/b9derived.sas7bdat \u2022 B&B:08/09 transcript file. Contains analytic variables derived from B&B:08/09 transcripts. /B&B 2009 with PETS/datafiles/SAS/trander.sas7bdat \u2022 B&B:08/09 weights file. Contains all weight and variance estimation variables for B&B:08/09. /B&B 2009 with PETS/datafiles/SAS/weights.sas7bdat \u2022 ACG/SMART Grant data file. Contains raw grant-level data received from the NSLDS for the 3,440 respondents who were awarded Academic Competitiveness Grants (ACGs) or National Science and Mathematics Access to Retain Talent (SMART) Grants through 2009-10. This is a history file with separate records for each transaction in the database; therefore, there can be multiple records per case. /B&B 2009 with PETS/datafiles/SAS/Source_Files/BB09/acg_smart.sas7bdat \u2022 Common Core of Data (CCD) data file. Contains the most recent CCD records (from the 2006-07 through 2008-09 academic years) for schools whose NCES ID was reported by B&B:08/09 respondents as schools in which they studied. /B&B 2009 with PETS/datafiles/SAS/Source_Files/BB09/ccd*.sas7bdat \u2022 B&B:08/09 coding data file. Contains major/field of study, industry, and occupation strings collected in the B&B interview and the associated codes. /B&B 2009 with PETS/datafiles/SAS/Source_Files/BB09/coding.sas7bdat \u2022 CPS data files. Contains data received from the CPS for the eligible sample members who matched to the financial aid application files. 2008-09: file contains about 4,400 matched respondents 2009-10: file contains about 4,150 matched respondents /B&B 2009 with PETS/datafiles/SAS/Source_Files/BB09/cps*.sas7bdat \u2022 B&B:08/09 graduate institution data file. Contains postbaccalaureate institution and degree data obtained from the B&B:08/09 interview for all respondents. This was a studentlevel file; however, a student can have more than one record in the file. There was a separate record for each degree obtained from each postsecondary institution that the student attended since earning a bachelor's degree. /B&B 2009 with PETS/datafiles/SAS/Source_Files/BB09/gschinfo.sas7bdat \u2022 NSLDS data file. Contains raw loan-level data received from the NSLDS for the 13,800 respondents who were awarded loans through 2009-10. This is a history file with separate records for each transaction in the loan files; therefore, there can be multiple records per case spanning several academic years. /B&B 2009 with PETS/datafiles/SAS/Source_Files/BB09/loansbb.sas7bdat \u2022 Pell Grants data file. Contains raw grant-level data received from the NSLDS for the 9,550 respondents who were awarded Pell Grants through 2009-10. This is a history file with separate records for each transaction in the Pell Grant system; therefore, there can be multiple records per case. /B&B 2009 with PETS/datafiles/SAS/Source_Files/BB09/pell.sas7bdat \u2022 Private School Survey (PSS) data file. Contains the most recent PSS records (from the 2005-06 through 2007-08 academic years) for schools whose NCES ID was reported by B&B:08/09 respondents as schools in which they taught. /B&B 2009 with PETS/datafiles/SAS/Source_Files/BB09/pss*.sas7bdat \u2022 B&B:08/09 student data file. Contains interview data collected from 15,050 respondents. Topics include eligibility, undergraduate and graduate education, employment, teaching, and student background. /B&B 2009 with PETS/datafiles/SAS/Source_Files/BB09/studinfo.sas7bdat \u2022 Teacher data file. Contains raw grant-level data received from the NSLDS for the 30 respondents who were awarded Teacher Education Assistance for College and Higher Education (TEACH) Grants through 2009-10. This is a history file with separate records for each transaction in the database; therefore, there can be multiple records per case. /B&B 2009 with PETS/datafiles/SAS/Source_Files/BB09/teacher.sas7bdat \u2022 B&B:08/09 undergraduate institution data file. Contains undergraduate institution and degree data obtained from the B&B:08/09 interview for all respondents. This was a studentlevel file; however, a student can have more than one record in the file. There was a separate record for each degree obtained from each postsecondary institution that the student attended between the time the student graduated from high school and the time the student graduated with a bachelor's degree. /B&B 2009 with PETS/datafiles/SAS/Source_Files/BB09/ugschinfo.sas7bdat \u2022 B&B:08/09 and NPSAS:08 analysis file. Contains analytic variables derived from all B&B:08/09 and NPSAS:08 data sources and selected direct interview variables available as of the initial release of B&B:08/12 PowerStats. /B&B 2009 with PETS/datafiles/SAS/Source_Files/NPSAS08/b9n8der.sas7bdat \u2022 ACT data file. Contains data received from ACT for the 5,390 respondents who matched to the 2001-02 through 2006-07 ACT files. /B&B 2009 with PETS/datafiles/SAS/Source_Files/NPSAS08/B9N8ACT.sas7bdat \u2022 CPS data files. Contains data received from the CPS for the eligible sample members who matched to the financial aid application files. 2007-08: file contains about 10,490 matched respondents. /B&B 2009 with PETS/datafiles/SAS/Source_Files/NPSAS08/cps0708.sas7bdat \u2022 NPSAS:08 institution data file. Contains selected institution-level variables for sampled institutions. /B&B 2009 with PETS/datafiles/SAS/Source_Files/NPAS08/np08inst.sas7bdat \u2022 NPSAS:08 school info file. Contains institution data obtained from the student interview. It is a student-level file with at least one record for each respondent; however, a student can have more than one record in the file. There is a separate record for each postsecondary institution students reported in the interview as somewhere they had attended during the study year (up to 5 institutions). Perturbation procedures were applied to this file to protect against disclosure of individual information. /B&B 2009 with PETS/datafiles/SAS/Source_Files/NPSAS08/schinfo.sas7bdat \u2022 NPSAS:08 studbase data file. Contains raw data collected from institutional records and from student interviews. /B&B 2009 with PETS/datafiles/SAS/Source_Files/NPSAS08/studbase.sas7bdat \u2022 B&B:08/09 transcript courses file. Contains course-level transcript data collected during B&B:08/09. There is a separate record for each course reported on the transcripts. /B&B 2009 with PETS/datafiles/SAS/Source_files/Transcripts/trander.sas7bdat \u2022 B&B:08/09 transcript degree file. Contains degree-level transcript data collected during B&B:08/09. There is a separate record for each degree reported on the transcripts. /B&B 2009 with PETS/datafiles/SAS/Source_files/Transcripts/degree.sas7bdat \u2022 B&B:08/09 transcript institution file. Contains institution-level transcript data collected during B&B:08/09. /B&B 2009 with PETS/datafiles/SAS/Source_files/Transcripts/institution.sas7bdat \u2022 B&B:08/09 transcript student school file. Contains student-level transcript data collected during B&B:08/09. This file contains one record per pairing of student and school. There is a separate record for each postsecondary institution reported on the transcripts as somewhere the student had attended. /B&B 2009 with PETS/datafiles/SAS/Source_files/Transcripts/studschools.sas7bdat \u2022 B&B:08/09 transcript terms file. Contains term-level transcript data collected during B&B:08/09. There is a separate record for each term reported on the transcripts. /B&B 2009 with PETS/datafiles/SAS/Source_files/Transcripts/terms.sas7bdat The web-based Instrument Development and Documentation System (IDADS) module of the Integrated Management System (see section 3.1.2) contains the finalized version of all interview items, their question wording, and variable and value labels, most of which were imported directly from the interview development system, Hatteras. B&B staff used IDADS for compiling all documentation for the interview. Also included were the more technical descriptions of items such as variable types (alpha or numeric), respondents to whom the item applied, and frequency distributions for response categories based on completed interview data. B&B staff used the IDADS documentation module to facilitate the generation of the final deliverable documentation for the codebooks."}, {"section_title": "Postdata Collection Editing", "text": "B&B staff edited the B&B:08/12 data using procedures developed and implemented for previous studies sponsored by NCES, including B&B:08/09. Following data collection, staff subjected the information collected in the interview to various quality control checks and examinations. For example, staff conducted these checks to confirm that the collected data reflected appropriate item routing (skip patterns). Another evaluation involved examination of all variables with missing data and substitution of specific values to indicate the reason for the missing data. The most commonly used missing codes are listed in table 40; however, for some items the missing values have different meanings. Users should refer to the codebook for information on missing values for specific variables. Additionally, reserve codes can be redefined as needed using the included data definition files. B&B staff examined skip-pattern relationships in the interview database by methodically cross-tabulating gate items and their associated nested items. In many instances, gate-nest relationships spanned multiple levels within the interview. Items nested within a gate question may themselves have been gate items for additional items. Consequently, validating the gate-nest relationships often required several iterations and many multiway cross-tabulations to ensure that the survey captured the proper data. The data cleaning and editing process for the full-scale data files involved a multistage process that consisted of the following steps: 1. B&B staff replaced blank or missing data with -9 for all variables in the interview database. Staff reviewed a one-way frequency distribution of every variable to confirm that no missing or blank values remained. SAS formats were created from expected values, and the associated value labels revealed any categorical outliers. Staff provided descriptive statistics for all continuous variables. Staff temporarily recoded all values that were less than zero to missing and examined the minimum, median, maximum, and mean values to assess reasonableness of responses. Staff also investigated anomalous data patterns and corrected them as necessary. 2. B&B staff identified legitimate skips for the interview items using interview source codes and flowcharts. Staff defined gate-nest relationships to replace -9s (data missing, reason unknown) with -3s (not applicable), as appropriate. For example, if a student was single and had never been married, then staff assigned a -3 code to a variable that collected a spouse's income. Staff evaluated two-way cross-tabulations between each gate-nest combination; they investigated high numbers of nonreplaced -9 codes to ensure skippattern integrity. They further checked nested values to reveal instances in which the legitimate skip code overwrote valid data, which typically occurred if a respondent answered a gate question and the appropriate nested items but then reverted to change the value of the gate to one that opened up an alternate path of nested items. Because responses to the first nested items remained in the database, they required editing. 3. B&B staff formatted variables (e.g., they formatted dates as YYYYMM). In addition, they merged back into the data file any new codes assigned by expert coders reviewing IPEDS, high school, occupation, and major codes (including those strings that interviewers or respondents could not code during the interview). Staff reviewing string data reviewed several variables, including occupation title and duties as well as major field of study, and sanitized strings by removing any inappropriate or revealing information. At this stage, they performed logical recodes in the data when someone could determine the value of missing items from answers to previous questions or preloaded values. For example, if the interview preloaded a student's date of birth from another source (NPSAS:08, B&B:08/09, or CPS), then the interview skipped the date of birth question and copied the preloaded value into the variable. 4. B&B staff examined descriptive statistics for all continuous variables for out-of-range, or outlier, values and replaced them with the value -6 (i.e., out-of-range data). Concurrently with data cleaning, documentation was developed to detail question text, response options, logical recoding, and the applies to text for each delivered variable (for documentation information, see the interview facsimile in appendix F)."}, {"section_title": "Data Perturbation", "text": "To protect the confidentiality of NCES data that contain information about specific individuals and to minimize disclosure risks, B&B:08/12 data were subject to perturbation procedures. Perturbation procedures, which have been approved by the NCES Disclosure Review Board, preserve the central tendency estimates but may result in slight increases in nonsampling errors. All respondents were given a positive probability of being selected for swapping. Perturbation was carried out under specific targeted, but undisclosed, swap rates. In data swapping, the values of the variables being swapped were exchanged between carefully selected pairs of records: a target record and a donor record. Swapping variables were selected from all questionnaire items. An extensive data quality check was carried out to assess and limit the impact of swapping. For example, the distribution of variables was evaluated pre-and posttreatment to verify that the swapping did not greatly affect the distribution."}, {"section_title": "Statistical Imputations", "text": "B&B staff imputed missing data in many variables included in the restricted-use derived file (also used in PowerStats) in accordance with mass imputation procedures described by Krotki, Black, and Creel (2005). After replacing missing data in those cases where values could be deduced with certainty based upon logical relationships among observed variables, the weighted sequential hot deck (WSHD) method was used to replace missing data by imputing plausible values from statistically selected donor cases (Cox 1980;Iannacchione 1982). The first stage in the imputation procedure was the identification of vectors of variables that, due to their substantive similarity or shared pattern of missingness, could be imputed simultaneously. Then, variables/vectors of variables were prioritized for imputation based upon their level of missing data, imputing those variables/vectors of variables with low levels of missingness prior to imputing variables where the rate of missingness was greater. For each variable/vector of variables, B&B staff identified imputation classes from which donor cases for the hot deck procedure would be selected. To develop those classes, nonparametric classification or regression trees were used to identify homogeneous subgroups of item respondents (Breiman et al. 1984), using complete response variables and any previously imputed variables as possible predictor variables. Finally, missing data were replaced using the WSHD procedure within each of the imputation classes. In the second stage of imputation, missing data were replaced using the WSHD procedure. To improve imputation quality, this previously described procedure using trees and WSHD was combined and implemented with the cyclic n-partition hot deck technique (Marker, Judkins, and Winglee 2002), as discussed in Judkins (1997). 14 This technique begins by identifying and replacing initial imputations for each missing variable (ordered from least to most missingness), based upon variables with complete responses and any imputed variables as possible predictors to form the imputation classes, within which the WSHD was used. The result was a complete dataset containing the variable/vector of variables being reimputed and variables related to the development of imputation classes. Then, in each of n iterations, imputed data in each variable (ordered from least to most missingness) were erased and a new response was imputed based upon the otherwise complete dataset. This approach reinforces existing patterns within the data, avoiding the need to make strong assumptions about distribution shapes or about prior distributions for parameters. Instead, B&B staff members were able to make deliberate choices about which features of the covariance structure deserve the best preservation efforts (Marker, Judkins, and Winglee 2002, p. 334). Typically, the result of cycling was a convergence to plausible values, maintaining relationships that already exist. Rarely, cycling fails to converge, introducing errors because of the missing data pattern and the random nature of the imputations. To reduce error due to imputation, B&B staff performed quality checks throughout the imputation process. In particular, staff compared the distribution of variable values pre-and postimputation, examining the data as needed to resolve apparent anomalies. Selected results from the imputation process are shown in appendix G, which shows the item response and nonresponse rates and pre-and postimputation distributions for each variable subject to imputation for all students."}, {"section_title": "Derived Variable Construction", "text": "Analysts created the main study analytic variables by examining the data available for each student from the various data sources, prioritizing the data sources on an item-by-item basis, and reconciling discrepancies within and between sources. In some cases, staff created derived variables by simple assignment of a value from the available source with the highest priority. In other cases, they recoded interview items or otherwise summarized them to create a derived variable (for a listing of the set of analysis variables derived for B&B:08/12, see appendix H). Details about the creation of each variable appear in the variable descriptions contained in the PowerStats documentation and codebooks for the restricted files."}, {"section_title": "Chapter 6. Weighting and Variance Estimation", "text": "This chapter provides information pertaining to the weighting procedures used for B&B:08/12. The development of statistical analysis weights for the B&B:08/12 sample is discussed in section 6.1. Analysis procedures that can be used to produce design-unbiased estimates of sampling variances are discussed in section 6.2, including variances computed using Taylor series and bootstrap replication techniques. Section 6.2 also describes how the Taylor series strata and primary sampling unit (PSU) variables were constructed and how the bootstrap replicate weights were constructed. Section 6.3 provides weighted and unweighted response rates. Section 6.4 discusses the accuracy of B&B:08/12 estimates for precision and the potential for nonresponse bias."}, {"section_title": "Analysis Weights", "text": "The weights for analyzing the B&B:08/12 data were derived from the NPSAS:08 weight, because the B&B:08/12 sample members were a subset of the NPSAS:08 sample. 15 Three weights were developed for analyzing data from the B&B:08/12 data collection. A cross-sectional weight was not included because analysis of the B&B:08/12 interview items would almost always be analyzed along with NPSAS:08 items, especially demographic items. One weight (bookend) was developed for analyzing NPSAS items in combination with B&B:08/12 items directly from or derived from the 2012 interview. A second weight (panel) was developed for analyzing NPSAS items in combination with B&B:08/09 items directly from or derived from the 2009 interview and B&B:08/12 items directly from or derived from the 2012 interview. A third weight (panel transcript) was developed for analyzing items from all three previous rounds and items directly from or derived from the transcript. The weights were adjusted for nonresponse and were also adjusted to IPEDS and NPSAS:08 control totals. This section describes the steps that were followed in order to develop each weight. The 2007-08 National Postsecondary Student Aid Study (NPSAS:08) Full-scale Methodology Report (Cominole et al. 2010) (hereinafter referred to as the NPSAS:08 full-scale methodology report) describes the development of the NPSAS study weight. The statistical analysis weight compensated for the unequal probability of selection of institutions and students in the NPSAS:08 sample. The weight also adjusted for multiplicity at the institution and student levels, unknown student eligibility, nonresponse, and poststratification. The institution weight was computed and then used as a component of the student weight. A weight was computed for NPSAS:08 respondents as the product of the following 10 weight components: \u2022 institution sampling weight (WT1); \u2022 institution multiplicity adjustment (WT2); \u2022 institution poststratification adjustment (WT3); \u2022 institution nonresponse adjustment (WT4); \u2022 student sampling weight (WT5); \u2022 student multiplicity adjustment (WT6); \u2022 student unknown eligibility adjustment (WT7); \u2022 student not located adjustment (WT8); \u2022 student other nonresponse adjustment (WT9); and \u2022 student poststratification adjustment (WT10). The B&B:08/12 sample contains both NPSAS study respondents and nonrespondents. Therefore, the B&B:08/12 base weight was formed as the product of the first seven of these adjustment factors. The subsample of 500 NPSAS:08 interview nonrespondents was selected with probabilities proportional to the NPSAS:08 student weight. The B&B:08/12 base weight was multiplied by the inverse of this selection probability for the subsampled cases to obtain the weight for cases in the sample. An adjustment was made for nonresponse, as defined for each analysis weight (see subsections below), using a model-based constrained logistic weighting procedure. The weights were then calibrated to IPEDS and weight sums from NPSAS:08, which had been calibrated to IPEDS and external control totals as described in the NPSAS:08 full-scale methodology report. 16 The procedure WTADJUST in SUDAAN (RTI 2012) was used to implement the nonresponse and calibration adjustments. This weighting methodology is described by Folsom and Singh (2000). The distribution of sample characteristics for refusals and other nonrespondents was reviewed to determine if a two-stage nonresponse adjustment was warranted to better reduce nonresponse bias. However, there was not a discernible difference in distributions, so one stage of nonresponse adjustment was conducted for all of the analysis weights. The adjustment for the nonresponse model included the eligible cases who were not deceased; the response indicator was set to 1 for the respondents and to 0 for nonrespondents. Independent variables were chosen that were considered to be predictive of response status and were nonmissing for respondents and nonrespondents. Variables for the model include the frame and survey design variables that were used for the NPSAS:08 weight adjustments and other data known for both the respondents and nonrespondents. Candidate predictor variables include the following: 17 \u2022 institution control; \u2022 region; \u2022 institution enrollment from IPEDS file (categorical); \u2022 Pell Grant receipt (yes/no); \u2022 Pell Grant amount (categorical); \u2022 Stafford Loan receipt (yes/no); \u2022 Stafford Loan amount (categorical); \u2022 Parent Loan for Undergraduate Students (PLUS) amount (categorical); \u2022 federal aid receipt (yes/no); 16 Calibration in this chapter generally refers to adjusting the weights to weight sums, and poststratification generally refers to adjusting the weights to external totals. However, these terms are sometimes used interchangeably when referring to both types of adjustments at the same time. 17 The enrollment and financial aid amount categories were formed based on quartiles. \u2022 institution aid receipt (yes/no); \u2022 state aid receipt (yes/no); \u2022 any aid receipt (yes/no); \u2022 Social Security number indicator (yes/no); \u2022 number of times answering machine was encountered (three levels); \u2022 count of phone numbers available for a student; \u2022 count of e-mail addresses available for a student; and \u2022 count of mailing addresses available for a student. Variables initially included in the nonresponse modeling included all of the candidate predictor variables, as well as certain important two-way and three-way interactions. To identify these interactions, a chi-square automatic interaction detection (CHAID) analysis was performed on the predictor variables (Kass 1980). The CHAID analysis divided the data into segments that differed with respect to the response variable. The segmentation process first divided the sample into groups based on categories of the most significant predictor of response. It then split each of these groups into smaller subgroups based on other predictor variables. It also merged categories of a variable that were found to be nonsignificant predictors of response propensity. To ensure population coverage and consistency with NPSAS:08 and B&B:08/09, the analysis weights were further adjusted to control totals. Variables used to define the control totals were similar to those used for the poststratification adjustments for B&B:08/09. The control totals for the B&B:08/12 bookend weight were obtained using the weighted sums from NPSAS:08 (using the NPSAS:08 study weights 18 ) for the following variables for the full B&B cohort (including ineligible and deceased students): \u2022 number of Stafford Loan recipients by institution control; 19 \u2022 total amount of Pell Grants awarded; 20 "}, {"section_title": "and", "text": "\u2022 amount of PLUS Loans awarded by institution control. The control totals for the B&B:08/12 panel and transcript panel weights were obtained using the weighted sums from the bookend weight for the same variables. Additionally, control totals were formed from IPEDS counts of bachelor's degree recipients for institution control, sex, and major. The following variables were used in defining control totals from IPEDs: 21 \u2022 2007-08 recipients of baccalaureate degree by sex; \u2022 2007-08 recipients of baccalaureate degree by institution control; and \u2022 2007-08 recipients of baccalaureate degree by major (12 categories). 18 NPSAS:08 weights were revised in August 2013. These revised weights were used to create control totals. 19 NPSAS:08 weights attempted to control to total Stafford Loan amounts disbursed in addition to the number of Stafford Loan recipients, but the B&B calibration model would not converge with both of them included. As a result, they were controlled only to the number of recipients. 20 The calibration model would not converge with amount of Pell Grants awarded by institution control, so total amount of Pell Grants awarded was used instead. 21 The IPEDS control totals are from the HD2008 and C2008_A files and can be downloaded from the online IPEDS data center at: http://nces.ed.gov/ipeds/datacenter/DataFiles.aspx. The control totals from NPSAS include cases who became ineligible or were deceased. Thus, the ineligible and deceased cases were included in the calibration adjustment to the NPSAS totals but not the IPEDS totals. After the adjustment, the ineligible and deceased cases were dropped from the file; the sum of the final weights estimates the number of the NPSAS:08 population who were eligible for B&B and were still alive at the time of the B&B:08/12 interview. As part of the calibration process, students with extreme (outlier) weights had different bounds on their adjustment factors to accomplish weight trimming and smoothing in the same step as calibration. Extreme weights were identified as weights greater than the median weight plus 3 times the interquartile range or less than the median weight minus 3 times the interquartile range. Weight values outside of these bounds were trimmed to fall within the bounds. To assess the overall predictive ability of the nonresponse model, a Receiver Operating Characteristic (ROC) curve was used (Hanley and McNeil 1982). The ROC provides a measure of how well the model correctly classified individuals of known response type-in other words, how well the model predicts a student's response propensity. 22 The ROC curve was developed in the following manner. For any specified probability, c, two proportions were calculated: \u2022 the proportion of respondents with a predicted probability of response greater than c; and \u2022 the proportion of nonrespondents with a predicted probability of response greater than c. The plot of the first probability against the second, for c ranging from 0 to 1, resulted in the ROC curve shown in figure 8. The area under the curve equals the probability that a randomly chosen pair of observations-one respondent and one nonrespondent-will be correctly ranked. The probability of a correct pairwise ranking was the same quantity that was estimated by the nonparametric Wilcoxon statistic. The null hypothesis associated with the Wilcoxon statistic was that the variable was not a useful discriminator between the respondent and nonrespondent populations. This corresponds to the null hypothesis that the predicted response probability of a respondent was just as likely to be smaller than the predicted response probability of a nonrespondent as it was to be greater. Thus, if the null hypothesis is true, the ROC curve will be a diagonal line that reflects the equally likely chance of making a correct or incorrect decision, and the area under the curve will be 0.5. If the null hypothesis is not true, the ROC curve will rise above the diagonal, and the area under the curve will be significantly greater than 0.5. Figure 8 in section 6.1 shows that the area under the ROC curve is 0.83, such that 83 percent of the time (about 8 of 10 pairings) the predicted probabilities of response give the correct classification. The ROC area of 0.83 equals the value of the Wilcoxon test statistic; based on this result, the null hypothesis of no predictive ability (p < 0.05) is rejected. This level of discrimination implies that the variables used in the model were highly informative, but not definite, predictors of a sample student's overall response propensity. The weight adjustments summarized in table 41 and the following sections provide the results of the weighting process for each of the three analysis weights. Bookend poststratification adjustment Adjust the student weights-match NPSAS:08 weight sums and known population totals from IPEDS-ensure population coverage. Includes trimming and smoothing of the weights-reduce unequal weighting."}, {"section_title": "Panel analysis weight", "text": "Panel nonresponse adjustment Adjust the weights-compensate for B&B:08/12 students who did not respond-NPSAS:08 or the 2009 or 2012 interview Panel poststratification adjustment Adjust the student weights-match NPSAS:08 weight sums and known population totals from IPEDS-ensure population coverage. Includes trimming and smoothing of the weights-reduce unequal weighting."}, {"section_title": "Transcript panel analysis weight", "text": "Transcript panel nonresponse adjustment Adjust the weights-compensate for B&B:08/12 students who did not respond-NPSAS:08 or the 2009 or 2012 interview or for whom a transcript was not collected"}, {"section_title": "Transcript panel poststratification adjustment", "text": "Adjust the student weights-match NPSAS:08 weight sums and known population totals from IPEDS-ensure population coverage. Includes trimming and smoothing of the weights-reduce unequal weighting. NOTE: All adjustments in the bookend, panel, and transcript panel weights were B&B:08/12 adjustments. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2008/12 Baccalaureate and Beyond Longitudinal Study (B&B:08/12)."}, {"section_title": "Bookend Analysis Weight", "text": "A B&B:08/12 bookend respondent was defined as any sample member who was determined to be eligible for the study, was not deceased at the time of the B&B:08/12 data collection, and had a completed, partial, or abbreviated interview in 2012 and was considered a study respondent for NPSAS:08. Among the eligible B&B:08 cohort, about 14,560 were classified as bookend respondents, yielding a weighted response rate of 77 percent. The B&B:08/12 sample consisted of 17,110 eligible students. At the conclusion of the B&B:08/12 data collection, 14,560 students were initially determined to be eligible respondents, 2,560 were nonrespondents, 30 were ineligible, and 20 were deceased. Variables that made up the CHAID interaction terms were NPSAS sector, institution enrollment, geographic region, number of times an answering machine was encountered, counts of phone numbers and e-mail addresses available for a student, Stafford Loan receipt, and PLUS amount. The initial model including all variables did not converge, requiring some variables to be dropped. Retained variables are listed in table 42. The predictor variables used in the model to adjust the weight for nonrespondents and the average weight adjustment factors resulting from these variables are shown in table 42. The nonrespondent weight adjustment factors have the following characteristics: \u2022 minimum: 1.00; \u2022 median: 1.00; and \u2022 maximum: 2.50.  The variables used for the calibration, the values of the control totals, and the average weight adjustment factors for each variable are shown in table 43. The last column of table 43 shows the sum of the weights after removing the cases who were ineligible or deceased at the time of the B&B:08/12 data collection. Statistics for the weight adjustment factors are the following: \u2022 minimum: 0.08; \u2022 median: 1.55; and \u2022 maximum: 9.32. The response-adjusted, calibrated interview weight is the variable WTD000 on the data file. The weight distributions and the variance inflation due to unequal weighting by institution control are summarized in table 44. The median bookend weight ranges from 25 for students whose base-year institution was for-profit to 86 for students whose base-year institution was public. The mean bookend weight ranges from 100 for students whose base-year institution was private nonprofit to 124 for students whose base-year institution was public. The unequal weighting effect overall was 2.44 and ranged from 2.33 for students whose base-year institution was public to 3.04 for students whose base-year institution was for-profit. "}, {"section_title": "Panel Analysis Weight", "text": "A B&B:08/12 panel respondent was defined as any sample member who met the requirements to be classified as a bookend respondent (described above) who also had a completed, partial, or abbreviated interview in 2009. Among the eligible B&B:08 cohort, 13,490 cases met the requirements to be classified as a panel respondent with a weighted response rate of 68 percent. Variables that made up the CHAID interaction terms were institution enrollment, geographic region, number of times an answering machine was encountered, counts of phone numbers, and e-mail and mailing addresses available for a student. The initial model including all variables did not converge, requiring some variables to be dropped. Retained variables are listed in table 45. Table 45 shows the predictor variables used in the model to adjust the weight for nonrespondents and the average weight adjustment factors resulting from these variables. The nonrespondent weight adjustment factors have the following characteristics: \u2022 minimum: 1.00; \u2022 median: 1.00; and \u2022 maximum: 3.00.  The variables used for the calibration, the values of the control totals, and the average weight adjustment factors for each variable are shown in table 46. Statistics for the weight adjustment factors are the following: \u2022 minimum: 0.07; \u2022 median: 1.53; and \u2022 maximum: 8.79. The response-adjusted, calibrated interview weight is the variable WTE000 on the data file. The weight distributions and the variance inflation due to unequal weighting by institution control are summarized in table 47. The median panel weight ranges from 27 for students whose base-year institution was for-profit to 89 for students whose base-year institution was public. The mean panel weight ranges from 108 for students whose base-year institution was private nonprofit to 134 for students whose base-year institution was public. The unequal weighting effect overall was 2.48 and ranged from 2.37 for students whose base-year institution was public to 3.04 for students whose base-year institution was for-profit. The area under the ROC curve is 0.77, such that 77 percent of the time (or about 8 of 10 pairings) the predicted probabilities give the correct classification (figure 9). The ROC area of 0.77 equals the value of the Wilcoxon test statistic; based on the result, the null hypothesis of no predictive ability (p < 0.05) is rejected. This level of discrimination implies that the variables used in the model were highly informative, but not definite, predictors of a sample student's overall response propensity.  "}, {"section_title": "Transcript Panel Analysis Weight", "text": "A B&B:08/12 transcript panel respondent was defined as any sample member who was a panel respondent who also had a transcript provided by the NPSAS:08 institution. The B&B:08/12 sample consisted of 17,070 eligible students. 23 Using the transcript panel weight, 12,570 students were classified as eligible respondents, yielding a weighted response rate of 64 percent. Variables that made up the CHAID interaction terms were NPSAS sector, institution financial aid, state financial aid, any financial aid, geographic region, number of times an answering machine was encountered, counts of phone numbers, and e-mail addresses available for a student. The initial model including all variables did not converge, requiring some variables to be dropped. Retained variables are listed in table 48. The predictor variables used in the model to adjust the weight for nonrespondents and the average weight adjustment factors resulting from these variables are shown in table 48. The nonrespondent weight adjustment factors have the following characteristics: \u2022 minimum: 1.00; \u2022 median: 1.04; and \u2022 maximum: 3.00.  The variables used for the calibration, the values of the control totals, and the average weight adjustment factors for each variable are provided in table 49. Statistics for the weight adjustment factors are the following: \u2022 minimum: 0.08; \u2022 median: 1.56; and \u2022 maximum: 7.39. The response-adjusted, calibrated interview weight is the variable WTF000 on the data file. The weight distributions and the variance inflation due to unequal weighting by institution control are summarized in table 50. The median panel weight ranges from 28 for students whose base-year institution was for-profit to 92 for students whose base-year institution was public. The mean panel weight ranges from 118 for students whose base-year institution was private nonprofit to 142 for students whose base-year institution was public. The unequal weighting effect overall was 2.50 and ranged from 2.42 for students whose base-year institution was public to 3.08 for students whose base-year institution was for-profit. The area under the ROC curve is 0.68, such that 68 percent of the time (or about 7 of 10 pairings) the predicted probabilities give the correct classification (figure 10). The ROC area of 0.68 equals the value of the Wilcoxon test statistic; based on the result, the null hypothesis of no predictive ability (p < 0.05) is rejected. This level of discrimination implies that the variables used in the model were highly informative, but not definite, predictors of a sample student's overall response propensity.  "}, {"section_title": "Variance Estimation", "text": "For probability-based sample surveys, most estimates are nonlinear statistics. For example, a mean or proportion, which is expressed as is nonlinear because the denominator is a survey estimate of the (unknown) population total. In this situation, the variances of the estimates cannot be expressed in closed form. Two procedures for estimating variances of survey statistics are the Taylor series linearization procedure and the bootstrap replication procedure. Variables used for both of these variance estimation procedures are available on the B&B:08/12 data files. The analysis strata and PSUs created for the Taylor series procedure are discussed in section 6.2.1, and section 6.2.2 contains a discussion of the replicate weights created for the bootstrap procedure."}, {"section_title": "Taylor Series", "text": "The Taylor series variance estimation procedure is a well-known technique used to estimate the variances of nonlinear statistics. The procedure takes the first-order Taylor series approximation of the nonlinear statistic and then substitutes the linear representation into the appropriate variance formula based on the sample design. Woodruff (1971) presented the mathematical formulation of this procedure. For stratified multistage surveys, the Taylor series procedure requires variance estimation (analysis) strata and variance estimation (analysis) PSUs defined from the sampling strata and PSUs used in the first stage of sampling. Because B&B:08/12 is the second follow-up study of NPSAS:08, the variance estimation strata and PSUs for B&B:08/12 were derived from the variance estimation strata and PSUs that were originally developed for NPSAS:08 and then modified for B&B:08/09. The steps involved in the construction of the NPSAS:08 strata and PSU variables are described in chapter 6 of the NPSAS:08 full-scale methodology report (Cominole et al. 2010). The process for the construction of the B&B:08/09 strata and PSU variables was similar to the process used for B&B:08/12, which is described below. The variance estimation formulas require at least two PSUs in each stratum. The B&B:08/09 variance estimation strata and PSUs were examined for the B&B:08/12 sample, and strata with only one PSU were combined with other strata to obtain at least two PSUs. The following three rules were used: variance estimation strata were combined with other variance estimation strata within the original NPSAS:08 sampling strata; certainty schools were combined with other certainty schools; and noncertainty schools were combined with other noncertainty schools. In addition, the original sort order that was used for constructing the NPSAS:08 variance estimation strata and PSUs was used. If the stratum was the first in the sorted list, then it was combined with the next stratum in the list; otherwise, it was combined with either the previous or next stratum. The single PSU then became an additional PSU in the new variance estimation stratum. The resulting variance estimation strata and PSUs for B&B:08/12 are the variables ANALSTR and ANALPSU. Note that these strata and PSUs were formed such that they were applicable to use with any of the three analysis weights described in section 6.1. The procedure described above may overestimate the variance because it does not always account for the finite population correction (FPC) at the institution stage of sampling. Alternatively, the Taylor series procedure can account for the FPC if the variance estimation (analysis) secondary sampling units (SSUs) and PSU counts were considered in addition to the analysis strata and analysis PSUs. The variables used to implement this procedure were FANALSTR, FANALPSU, FANALSSU, and PSUCOUNT for the analysis strata, PSUs, SSUs and PSU counts, respectively. An alternate variance estimation method using replicate weights to account for the FPC is also provided for users of the B&B:08/12 data, as described below."}, {"section_title": "Bootstrap Replicate Weights", "text": "The variance estimation strategy that was chosen for B&B:08/12 was the same as that used for B&B:08/09 and NPSAS:08, and it satisfies the following requirements: \u2022 recognition of variance reduction due to stratification at all stages of sampling; \u2022 recognition of effects of unequal weighting; \u2022 recognition of possible increased variance due to sample clustering; \u2022 recognition of effects of weight adjustments for nonresponse and for calibration of selected total estimates to known external totals or weight sums; \u2022 satisfactory properties for estimating variances of nonlinear statistics and quantiles (such as the median) as well as for linear statistics; \u2022 ability to apply finite population corrections at the institution stage of sampling and reflect the reduction in variance due to the high sampling rates in some first-stage sampling strata; and \u2022 ability to test hypotheses about students on the basis of normal distribution theory by ignoring the finite population corrections at the student level of sampling. The replicate weights were produced using a methodology combining approaches developed by Flyer (1987) and Kott (1988). The Flyer-Kott approach is described in the NPSAS:08 full-scale methodology report (Cominole et al. 2010). The Flyer-Kott methodology was used to develop a vector of bootstrap sample weights that was added to the analysis file. These weights are zero for units not selected in a particular bootstrap sample; weights for other units are inflated for the bootstrap subsampling. The initial analytic weights for the complete sample are also included to compute the desired estimates. The vector of replicate weights allows for computing additional estimates for the sole purpose of estimating a variance. Assuming B sets of replicate weights, the variance of any estimate, \u03b8\u02c6, can be estimated by replicating the estimation procedure for each replicate and computing a simple variance of the replicate estimates; that, The number of replicate weights was set at 200 for NPSAS:08. For the three sets of 200 replicate weights included on the analysis file, both the nonresponse adjustment and calibration process were repeated so that the variance of survey estimates would include the variability due to the weight adjustments. For some of the replicates, the adjustment factor bounds were loosened and not all of the control totals could be met because of model convergence problems; i.e., there was no solution to satisfy all model equations simultaneously. The analysis and replicate weights that are available on the analysis file for B&B:08/12 are the following:"}, {"section_title": "Type of respondents", "text": "Analysis weight Replicate weights Bookend respondents WTD000 WTD001-WTD200 Panel respondents WTE000 WTE001-WTE200 Transcript panel respondents WTF000 WTF001-WTF200 The weight and variance estimation variables and how they are used in selected software packages that allow for Taylor series variance estimation (SUDAAN, Stata, the SAS survey data analysis procedures, IBM SPSS Complex Samples, and R) and bootstrap variance estimation (SUDAAN, Stata, the SAS survey data analysis procedures, WesVar, and R) are summarized in table 51. Variance estimates and design effects shown in appendix I were produced using the bootstrap replicate weights.  Replicates: WTD001 -WTD200 Replicates: WTE001 -WTE200 Replicates: WTF001 -WTF200 R survey package 1 mydesign<-svrepdesign( type=\"BRR\", weights=~WTD000, repweights= \"WTD00 \", combined.weights=FALSE) mydesign<-svrepdesign (type=\"BRR\", weights=~WTE000, repweights= \"WTE00 \", combined.weights=FALSE) mydesign<-svrepdesign( type=\"BRR\", weights=~WTF000, repweights= \"WTF00 \", combined.weights=FALSE) 1 For the R survey package, \"mydesign\" can be renamed to any name for an R object to hold the specification of the survey design. For the without replacement design, the R survey package does not account for the second stage of sampling. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2008/12 Baccalaureate and Beyond Longitudinal Study (B&B:08/12)."}, {"section_title": "Overall Weighted and Unweighted Response Rates", "text": "The overall B&B:08/12 response rate was an estimate of the proportion of the study population directly represented by the respondents. Because the B&B:08/12 study includes a subsample of NPSAS:08 nonrespondents, the overall B&B:08/12 response rate was the product of the NPSAS:08 institution-level response rate times the B&B:08/12 student-level response rate. Therefore, the overall B&B:08/12 response rates can only be estimated directly for defined institution characteristics. The unweighted and weighted NPSAS:08 base-year institution and B&B:08/12 student response rate components by institution control are shown in table 52. Only the weighted response rates can be interpreted as estimates of the proportion of the B&B:08/12 population that was directly represented by the respondents. The types of student respondents were the following: 24 \u2022 B&B:08/12 bookend respondents (i.e., NPSAS:08 study respondent and B&B:08/12 interview respondent); \u2022 B&B:08/12 panel respondents (i.e., NPSAS:08 study respondent and B&B:08/09 and B&B:08/12 interview respondents); and 24 See section 6.1 for respondent definitions. \u2022 B&B:08/12 transcript panel respondents (i.e., NPSAS:08 study respondent, B&B:08/09 and B&B:08/12 interview respondents, and transcript respondent).  (Cominole et al. 2010, p. 50). Overall response rates are the product of the NPSAS:08 and BPS:08/12 response rates. The eligible student counts for the transcript panel differ from the counts for the bookend and panel due to perturbation. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2008/12 Baccalaureate and Beyond Longitudinal Study (B&B:08/12). The institution-level response rates were the percentage of institutions that provided sufficient data to select the NPSAS:08 student-level sample; these rates are presented and discussed in the NPSAS:08 full-scale methodology report (Cominole et al. 2010, p. 50). Approximately 77 percent of the eligible sample were NPSAS:08 study respondents and B&B:08/12 interview respondents. The rate varied from 75 percent to 78 percent, by institution control. The overall weighted response rate, incorporating the NPSAS:08 base-year institution response rate, was 69 percent. The bookend analysis weight described in section 6.1 (WTD000) was developed to compensate for the potentially biasing effects of bookend nonresponse. Approximately 68 percent of the eligible sample were NPSAS:08 study respondents and B&B:08/09 and B&B:08/12 interview respondents. The rate varied from 61 percent to 70 percent, by institution control. The overall weighted response rate, incorporating the NPSAS:08 base-year institution response rate, was 61 percent. The panel analysis weight described in section 6.1 (WTE000) was developed to compensate for the potentially biasing effects of panel nonresponse. Approximately 64 percent of the eligible sample were NPSAS:08 study respondents, B&B:08/09 and B&B:08/12 interview respondents, and transcript respondents. The rate varied from 60 percent to 66 percent, by institution control. The overall weighted response rate, incorporating the NPSAS:08 base-year institution response rate, was 58 percent. The transcript panel analysis weight described in section 6.1 (WTF000) was developed to compensate for the potentially biasing effects of transcript panel nonresponse. Section 6.4.2 analyzes the potential bias due to unit nonresponse and the effect the weight adjustments had in reducing the bias."}, {"section_title": "Accuracy of Estimates", "text": "The accuracy of survey statistics is affected by both random and nonrandom errors. Random errors reduce the precision of survey estimates, while nonrandom errors result in bias (i.e., estimates that do not converge to the true population parameter as the sample size increases without limit). The sources of error in a survey are often dichotomized as sampling and nonsampling errors. Sampling error refers to the error that occurs because the survey was based on a sample of population members rather than the entire population. All other types of errors are nonsampling errors, including survey nonresponse (because of inability to contact sampling members, their refusal to participate in the study, etc.) and measurement errors, such as the errors that occur because the intent of survey questions was not clear to the respondent, because the respondent had insufficient knowledge to answer correctly, or because the data were not captured correctly (e.g., because of recording, editing, or data entry errors). The sampling errors are primarily random errors for well-designed surveys such as NPSAS:08 and B&B:08/12. However, nonrandom errors can occur if the sampling frame does not provide complete coverage of the target population. The B&B:08/12 survey instrument and data collection procedures were subjected to thorough development and testing to minimize nonsampling errors, because these errors are difficult to quantify and are likely to be nonrandom errors. In this section, sampling errors and design effects for some B&B:08/12 estimates are presented for a variety of domains; these sampling errors and design effects are computed using the three analysis weights that were constructed for analyzing the B&B:08/12 student data in conjunction with NPSAS:08, B&B:08/09, and transcript data. Next, the results of analyses comparing B&B:08/12 nonrespondents with respondents and with the full sample using characteristics known for both nonrespondents and respondents are presented. An analysis of nonresponse bias is presented at both the student level and the item level."}, {"section_title": "Measures of Precision: Standard Errors and Design Effects", "text": "The survey design effect for a statistic was defined as the ratio of the design-based variance estimate divided by the variance estimate that would have been obtained from a simple random sample of the same size (if that were practical). It is often used to measure the effects that sample design features have on the precision of survey estimates. For example, stratification tends to decrease the variance, but multistage sampling and unequal sampling rates usually increase the variance. In addition, weight adjustments for nonresponse (performed to reduce nonresponse bias) and calibration often increase the variance by increasing the weight variation. Because of these effects, most complex multistage sampling designs, such as B&B:08/12, result in design effects greater than 1.0. That is, the design-based variance is larger than the simple random sample variance. Specifically, the survey design effect for a given estimate, \u03b8 , was defined as The square root of the design effect is another measure and can be expressed as the ratio of the standard errors, or In appendix I, design effect estimates are presented for important survey domains and estimates in order to summarize the effects of stratification, multistage sampling, unequal probabilities of selection, and the weight adjustments. These design effects were estimated for B&B:08/12 using SUDAAN and the bootstrap variance estimation procedure described in section 6.2.2. While not recommended, if an analysis of B&B:08/12 data must be performed without using one of the software packages for analysis of complex survey data, the design effect tables in appendix I can be used to make approximate adjustments to the standard errors of survey statistics computed with the standard software packages that assume simple random sampling designs. However, one cannot be confident about the actual design-based standard errors without performing the analysis with one of the software packages specifically designed for analysis of data from complex sample surveys. Large design effects imply large standard errors and relatively poor precision. Small design effects imply small standard errors and good precision. In general terms, a design effect less than 2.0 is low, 2.0 to 3.0 is moderate, and greater than 3.0 is high. Moderate and high design effects often occur in complex surveys such as B&B:08/12, and the design effects presented in appendix I are consistent with those in past B&B studies. Unequal weighting causes large design effects and is often due to nonresponse and calibration adjustments. However, in B&B:08/12 (as in NPSAS:08), the unequal weighting is also due to the sample design, different sampling rates between institution strata, different sampling rates between student strata, and subsampling of the nonrespondents who were included in B&B:08/09 and B&B:08/12."}, {"section_title": "Measure of Bias", "text": "The bias in an estimated mean based on respondents, R y , is the difference between this mean and the target parameter, \u03c0, that is, the mean that would be estimated if a complete census of the target population was conducted and everyone responded. This bias can be expressed as follows, is the expected value of the mean based on respondents over repeated samples: The estimated mean based on nonrespondents, NR y , can be computed if data for the particular variable are available for most of the nonrespondents. The true target parameter, \u03c0, can be estimated for these variables as follows: where \u03b7 is the weighted unit (or item) nonresponse rate. For the variables that are from the frame, rather than from the sample, \u03c0 can be estimated without sampling error. The bias can then be estimated as follows: This formula shows that the estimate of the nonresponse bias is the difference between the mean for respondents and nonrespondents multiplied by the weighted nonresponse rate. The relative bias estimate was defined as the ratio of the estimated bias divided by the sample mean, using the base weight, as follows: This definition of relative bias provides a measure of the magnitude of the bias relative to the estimated sample mean. Nonresponse bias analysis was conducted when the response rate at any level (institutions, students, items) was below 85 percent. 25 Institution nonresponse bias was performed as a part of NPSAS:08 and is described in the NPSAS:08 full-scale methodology report (Cominole et al. 2010). A student nonresponse bias analysis was performed for the bookend, panel, and transcript panel analysis weights, and an item nonresponse bias analysis was also performed. The remainder of this section summarizes the unit and item nonresponse bias analyses that were conducted for B&B:08/12. Unit nonresponse bias analysis. The bookend, panel, and transcript panel weighted response rates were below 85 percent overall and for all three institution controls, so a nonresponse bias analysis was conducted overall and within each institution control. Unit nonresponse bias analyses were conducted for the following sets of respondents: \u2022 B&B:08/12 bookend respondents versus the full set of cases eligible for B&B:08/12 (bookend respondents and bookend nonrespondents), before and after the nonresponse weight adjustment; \u2022 B&B:08/12 bookend respondents versus B&B:08/12 bookend nonrespondents, before the nonresponse weight adjustment; \u2022 B&B:08/12 panel respondents versus the full set of cases eligible for B&B:08/12 (panel respondents and panel nonrespondents), before and after the nonresponse weight adjustment; \u2022 B&B:08/12 panel respondents versus B&B:08/12 panel nonrespondents, before the nonresponse weight adjustment; \u2022 B&B:08/12 transcript panel respondents versus the full set of cases eligible for B&B:08/12 (transcript panel respondents and transcript panel nonrespondents), before and after the nonresponse weight adjustment; and \u2022 B&B:08/12 transcript panel respondents versus B&B:08/12 transcript panel nonrespondents, before the nonresponse weight adjustment. The nonresponse bias was estimated for variables known-that is, nonmissing-for most respondents and nonrespondents, and some variables were added that were not included in the 25 See Seastrom (2014) for a discussion of nonresponse bias analysis. nonresponse weight adjustment. Bias estimates were suppressed for variable categories with fewer than 30 nonrespondents. The following variables were used in the analyses: 26 \u2022 institution control; \u2022 region; \u2022 institution enrollment from IPEDS file (categorical); \u2022 Pell Grant receipt (yes/no); \u2022 Pell Grant amount (categorical); \u2022 Stafford Loan receipt (yes/no); \u2022 Stafford Loan amount (categorical); \u2022 PLUS (categorical); \u2022 federal aid receipt (yes/no); \u2022 institution aid receipt (yes/no); \u2022 state aid receipt (yes/no); and \u2022 any aid receipt (yes/no); \u2022 major (categorical); \u2022 sex; \u2022 age as of NPSAS:08 (categorical); and \u2022 CPS match as of NPSAS:08 (yes/no). First, for the variables listed above, the nonresponse bias was estimated by comparing estimates from base-weighted respondents with those of both nonrespondents and the full sample to determine if the differences were statistically significant at the 5 percent level. The two comparisons yield identical bias estimates but not always the same significance-testing results. Second, nonresponse adjustments were computed to reduce or eliminate nonresponse bias for key variables included in the models. Third, using base weights adjusted for nonresponse, bias was reestimated and statistical tests were performed to check for any remaining significant nonresponse bias. Finally, to better understand the effect of poststratification on efforts to reduce nonresponse bias, two additional sets of estimates were created. The first, the difference between respondent means before and after poststratification, represents the effect of poststratification on nonresponse adjustments. The second, the difference between base-weighted full sample means and the poststratified respondent means, represents the cumulative effect of all weighting and adjustment steps. The nonresponse weighting adjustment eliminated some, but not all, significant bias on these characteristics (table 53). 27 Before weighting, the percentage of variable categories that were significantly biased ranged from 2.9 percent among for-profit institutions with the panel and transcript panel weights to 49.0 percent among public institutions with the panel weight. The percentage of variable categories that remained significantly biased after the nonresponse weight adjustment ranged from 0.0 percent among private nonprofit institutions with the bookend and transcript panel weights to 13.8 percent among for-profit institutions with the bookend weight. As shown in appendix J, overall, significant bias remained for two or fewer categories of major and both categories of sex for each analysis weight. Significant bias estimates ranged from -2.0 to 2.0 percent. Among public institutions, two or fewer categories of institution enrollment and three or fewer categories of major had remaining significant bias for each analysis weight. Additionally, both categories of sex were significantly biased for the transcript panel weight. The significant bias estimates ranged from -1.9 to 2.3 percent. Among private nonprofit institutions, one category of region had significant bias of -1.5 percent for the panel weight, and no variables had significant bias remaining for the other two weights. Among for-profit institutions, two or fewer categories of enrollment had remaining significant bias for each analysis weight, and both categories of state aid status had significant bias for the bookend weight. The bias estimates ranged from -6.1 to 5.0 percent. Across all three weights, the mean and median absolute differences between means for respondents before and after poststratification adjustment are less than 2.0 for the overall, public, and nonprofit institutions and greater than 4.4 for the for-profit institutions (table 54). The differences between means for the full sample and respondents after poststratification adjustment are less than 2.0 for the overall, public, and nonprofit institutions and greater than 3.4 for the forprofit institutions. See appendix J, tables J-1 through J-24, for detailed nonresponse bias analysis tables. Item nonresponse bias analysis. NCES Statistical Standard 4-4-3A states: \"For an item with a low total response rate, respondents and nonrespondents can be compared on sampling frame and/or questionnaire variables for which data on respondents and nonrespondents are available. Base weights must be used in such analysis. Comparison items should have very high response rates. A full range of available items should be used for these comparisons. This approach may be limited to the extent that items available for respondents and nonrespondents may not be related to the low response rate item being analyzed\" (Seastrom 2014). Moreover, NCES Statistical Standard 1-3-5 states: \"Item response rates (RRI) are calculated as the ratio of the number of respondents for whom an in-scope response was obtained (I x for item x) to the number of respondents who are asked to answer that item. The number asked to answer an item is the number of unit level respondents (I) minus the number of respondents with a valid skip for item x (V x ). When an abbreviated questionnaire is used to convert refusals, the eliminated questions are treated as item nonresponse. In longitudinal analyses, the numerator of an item response rate includes cases that have data available for all waves included in the analysis and the denominator includes the number of respondents eligible to respond in all waves included in the analysis. In the case of constructed variables, the numerator includes cases that have available data for the full set of items required to construct the variable, and the denominator includes all respondents eligible to respond to all items in the constructed variable\" (Seastrom 2014). The item response rate is calculated as Item response rates were computed using nonimputed data. Valid skips were later logically imputed for the follow-up items after the gate question was imputed (but these imputed skips count as missing for computing the response rate). Table G-1 in appendix G lists the items from the B&B:08/12 interview along with the number of cases who were eligible to answer each item and the weighted item response rates and nonresponse rates. The B&B:08/12 bookend weight (WTD000) was used to calculate the response rates. The nonresponse rate was also the same as the percentage of cases for which the item was imputed. Cases who did not respond to a gate item were treated as missing for the items within the gate. Of the 176 items listed in table G-1, 47 had an item response rate less than 85 percent. A nonresponse bias analysis was conducted for items with a weighted response rate less than 85 percent for B&B:08/12 bookend respondents. The possibility of estimating the degree of bias depends on having some variables that reflect key characteristics of respondents and for which there are little or no missing data. The variables that were used (a subset of the bulleted list above 28 ) are known for all B&B:08/12 interview respondents and nonrespondents. These variables are important to the study and are related to many of the items being analyzed for low item response rates. For the items with a weighted response rate less than 85 percent, the nonresponse bias prior to imputation was estimated for each of these characteristics that are known for respondents and nonrespondents. Table J-25 in appendix J is included as an example of the bias analyses conducted for each item. It presents the estimated bias (prior to item imputation) for Private student loan status in 2012 (B2PRIVSTAT) for B&B:08/12 bookend respondents. Similar computations were performed for each of the items. Table J-26 summarizes the results of the item nonresponse bias analysis for each of the items from the student interview with a response rate of less than 85 percent, and gives the mean and median relative bias and the percentage of the variable categories with statistically significant bias. Across the items, the percentage of variables with statistically significant bias ranged from 18 to 45 percent. Item imputation was used to fill in missing data for B&B:08/12 interview respondents, as described in chapter 5. Item imputation was expected to reduce the bias due to item nonresponse and was used instead of a separate weight adjustment for nonresponse for each item. All of the questionnaire items that are listed in table G-1 were imputed using the imputation process described in chapter 5. A by-product of imputation was the reduction or elimination of item-level nonresponse bias. While item-level bias before imputation was measurable, after imputation it was not. As a result, how well an imputation procedure worked in reducing bias could not be directly evaluated. Instead, the before-and after-imputation item estimates were compared to determine whether the imputation significantly changed the biased estimates, thus suggesting a reduction in bias. Weighted estimates were computed using the nonimputed data (including only those cases who responded to the item) and also using the imputed data (including cases who responded to the item and also cases with imputed data for the item). Table G-2 shows the means before and after imputation for the continuous variables, and table G-3 shows the distributions before and after imputation for the categorical variables. These tables also show the difference between the preimputation and postimputation estimates. The difference between the pre-and postimputation estimates was statistically significant for 10 percent of the variables and variable categories included in the nonresponse bias analysis (see table J-26). 29 This suggests that imputation was only slightly successful in reducing the bias due to item nonresponse. 29 Statistical testing of differences was not conducted for variable categories with fewer than 30 item-level nonrespondents. The data elements for the B&B:08/12 full-scale interview cover general topics which pertain to all students (e.g., postbaccalaureate degree enrollment, employment, demographic characteristics); however, many items are only relevant to certain categories of students, such as prior round nonrespondents, those with a postbaccalaureate education, or teachers. The core data elements are listed in table B-1.  This report describes the methodology and findings of the B&B:08/12 field test, which took place during the 2010-11 school year. The field test was used to plan, implement, and evaluate methodological procedures, instruments, and systems proposed for use in the full-scale study scheduled for the 2011-12 school year."}, {"section_title": "Instrumentation", "text": "The content of the interview was based on previous interviews created for the 1993/03 Baccalaureate and Beyond Longitudinal Study (B&B:93/03) and 2000/01 Baccalaureate and Beyond Longitudinal Study (B&B:2000/01) cohorts and on the B&B:08/09 interview, building on data elements developed with input from the study's Technical Review Panel (TRP) and from NCES. Core data elements maintained in the B&B:08/12 field test interview included such items as degree attainment, continuing or graduate education, employment, debt and finances, family formation, civic engagement and volunteerism, and interest in or preparation for K-12 teaching. Additionally, information on teachers' professional experiences was collected so as to permit a comparison of the teaching profession to other occupations as well as a general study of labor market outcomes for the cohort as a whole. The B&B:08/12 field test interview consisted of eight sections, grouped by topic: Eligibility, Undergraduate Education, Postbaccalaureate Education and Training, Postbaccalaureate Employment, K-12 Teaching, Student Background, Locating, and Opinions. Development of the B&B:08/12 interview included conducting cognitive interviews to provide feedback on the survey. A total of 24 cognitive interviews were conducted in March and April 2011. The cognitive interview process was designed to elicit respondent feedback to broad questions about terminology, experiences, and decisions related to their continued education, employment, and interest in and preparation for K-12 teaching. The 24 respondents were divided into three groups of particular interest: (1) currently enrolled students; (2) those who graduated with a bachelor's degree in a science, technology, engineering, and mathematics (STEM) field; and 3those who had prepared for or who were interested in teaching at the K-12 level. The feedback helped to construct new items and refine current survey items. The field test instrument was designed as a mixed-mode instrument. A single web-based instrument was used for both self-administered interviews and interviewer-administered interviews. Although the use of mixed-mode data collection provides many benefits, it also introduces certain issues that must be considered to minimize mode effects. Several methodological features were built into the instrument to minimize mode effects, including help text on every form, telephone interviewer instructions on every form, pop-up messages when a response was entered in an incorrect format, and conversion text to encourage responses to critical items when sample members did not provide a response. The field test interview employed five interactive coding systems to standardize the collection of data on postsecondary institutions attended, major or field of study, occupation, employer industry, and any elementary or secondary schools where the respondent had taught. The reliability of responses to interview questions was evaluated through a reinterview containing 29 selected items from the main interview. Reinterviews were conducted approximately 3 to 4 weeks after the initial interview and in the same administration mode as the initial interview. A subsample of approximately 320 interview respondents was randomly selected for reinterview to permit analysis of the reliability of the 29 items."}, {"section_title": "Experiments", "text": "The B&B:08/12 field test included two instrumentation experiments. The first examined differences in the use of vertical and horizontal visual analog scales (VASs) in conjunction with realtime feedback displays (RTFDs) compared to radio buttons without RTFDs. Results showed that item nonresponse did not vary significantly across the item types; however, the radio button version took the least time to complete. A second experiment compared radio buttons and VASs, without feedback displays. Four different item versions were tested: a 5-point slider with midpoints, a 10-point slider, 10-point radio button scales, and 5-point radio buttons with midpoints. There were no significant differences in response distributions or nonresponse across any of the layout options. Two data collection experiments were conducted during the B&B:08/12 field test. The first tested whether viewing a short informational video describing the study had any impact on response rates. Field test results indicated that those who received the video were not more likely to complete the survey instrument. The second data collection experiment evaluated the use of response propensity modeling to target cases with low likelihood of response, with the goal of improving weighted response rates and thereby minimizing nonresponse bias. Results showed that additional monetary incentives did increase response rates for the majority of the sample members, but not for those individuals near the highest and lowest propensity scores. Field test results did not show a reduction in bias as a result of the additional incentives offered to treatment cases.\nSeveral experiments were conducted as part of the B&B:08/12 field test. The purpose of these experiments was to inform the instrument design and data collection procedures for the fullscale data collection. The following section discusses the design, implementation, and results of the instrumentation; data collection; and help text experiments conducted in the B&B:08/12 field test."}, {"section_title": "Planned Changes for the Full-Scale Study", "text": "The final chapter of this report summarizes the changes planned for the B&B:08/12 fullscale study, based on the results of the field test. Key changes for the full-scale study include the following: \u2022 The employment section will be revised to prioritize completeness of job history over detail for specific jobs. \u2022 Occupation coder results will be supplemented by external data matching. \u2022 In contrast to the a priori, propensity-based bias reduction approach tested in the field test, the full scale study will include an experiment that tests one group-level and one individual-level responsive design methodology in an effort to reduce nonresponse bias. \u2022 Incentive amounts will be restructured so that those most likely to respond will be offered $20, those least likely to respond will be offered $55, and all others will be offered $35. Additionally, incentives may increase during the survey, based on the responsive design measures. This chapter provides detail about the field test sampling design and procedures, interview design, data collection procedures, and the outcomes of the data collection. Unless otherwise indicated, a criterion probability level of .05 was used for all tests of significance conducted for the B&B:08/12 evaluations. Throughout this document, reported numbers of sample institutions and sample members have been rounded to ensure the confidentiality of the individual. As a result, row and column entries in tables may not sum to their respective totals, and reported percentages may differ somewhat from those that would result from these rounded numbers."}, {"section_title": "Chapter 2. Design and Outcomes", "text": ""}, {"section_title": "Sampling", "text": "This chapter describes the design of the B&B:08/12 field test sample, which has four stages. The first two stages occurred during The 2007-08 National Postsecondary Student Aid Study (NPSAS:08) field test study, when samples of NPSAS eligible institutions and students within institutions were selected. The third stage was in the first follow-up, when all confirmed and potential baccalaureate recipients from NPSAS:08 were included in the B&B:08/09 field test sample. The fourth stage is for the second follow-up, when all eligible sample members from B&B:08/09 (as determined by the B&B:08/09 interview and the transcripts) are included in the B&B:08/12 field test. This chapter describes the institution and student respondent universes. The steps used to select the institution and student samples for the base-year (NPSAS:08) field test, as well as the steps used in the NPSAS:08 and B&B:08/09 field tests to determine the B&B:08/09 and B&B:08/12 field test samples, respectively, are presented."}, {"section_title": "Respondent Universe", "text": "The respondent universe for the B&B:08/12 field test was identified using the same criteria and processes outlined in section 2.1 of the main report, with the exceptions that field test sample members \u2022 consisted of students who completed requirements for a bachelor's degree during the 2006-07 academic year; and \u2022 were identified from the NPSAS:08 and B&B:08/09 field tests. To be eligible for the NPSAS:08 field test, students had to be enrolled in a NPSAS eligible institution in any term or course of instruction at any time from July 1, 2006, through June 30, 2007 Students also had to meet the requirements identified in section 2.1 of the main report."}, {"section_title": "Samples", "text": "The B&B:08/12 field test student sample consisted of all B&B:08/09 field test eligible interview respondents, including those who were not NPSAS:08 interview respondents. 1 All B&B:08/09 field test interview nonrespondents were also included in the B&B:08/12 field test sample, including those who were not NPSAS:08 interview respondents. 2 The NPSAS:08 field test institution and student samples, and B&B:08/09 and B&B:08/12 field test student samples, are described in this section. To be eligible for B&B, students had to complete requirements for a bachelor's degree between July 1, 2006, andJune 30, 2007. Given that institutions were asked to identify potential bachelor's degree recipients before degree completion, the number of those who would actually complete their degree was expected to be lower. Therefore, NPSAS sampling rates for those identified by the sample institutions as potential baccalaureate recipients and other undergraduate students were adjusted to determine the expected sample sizes after accounting for expected false positives. 3 The false positives experienced in NPSAS:2000 (the most recent NPSAS to include a B&B base-year cohort) were used to set appropriate sampling rates for the NPSAS:08 field test. 4 B&B:08/09 field test student sample. The total B&B:08/09 field test sample size was 1,820. The NPSAS:08 field test yielded about 1,220 interview respondents who were confirmed to be bachelor's degree recipients. The base-year sample also included about 600 interview nonrespondents who were either classified as potential bachelor's degree recipients in the student institution records obtained through computer-assisted data entry (CADE) or were identified as such according to the initial classification by the NPSAS sample institution at the time of student sampling (prior to base-year data collection). Table 1 shows the distribution of the B&B sample, by NPSAS:08 interview response status and B&B eligibility. B&B:08/12 field test student sample. The B&B:08/12 field test sample included 1,590 sample members. To determine this sample size, the B&B:08/09 field test sample was the starting 1 The term \"student\" is used to refer to sample members, although at the time of the B&B:08/12 study, many of the sample members were not students. 2 The nonrespondents exclude students who were determined to be ineligible based on the interview or transcript. 3 False positives are students sampled as bachelor's degree recipients who are not actually bachelor's degree recipients. 4 In NPSAS:2000, 13 percent of students identified by the sample institution as potential baccalaureate recipients at the time of sampling were later determined during the interview to be other undergraduate or graduate students. The false negative rate (percentage of students not sampled as bachelor's degree recipients who are actually bachelor's degree recipients) was 3 percent for those identified at the time of sampling as other undergraduate or graduate students but determined during the interview to be baccalaureate degree students. Given that potential baccalaureates were identified earlier in NPSAS:08 than in NPSAS:2000, a false positive rate of 15 percent was assumed for sampling purposes, and the false negative rate was ignored because it was expected to be minimal. point, and ineligible and deceased cases were excluded. Ineligibles were determined in the B&B:08/09 interview or from the transcript. Table 2 shows the determination of the sample size. The distribution of this sample by prior response status is shown in table 3. Response rates among sample members who responded to the previous survey are generally expected to be high. However, the B&B:08/12 field test sample included some sample members who were nonrespondents to the first follow-up or the base-year study, or both, and experience suggested that the response rates among these sample members would be very low. Due to the limited amount of time to pursue difficult cases in the field test, the yield was expected to be at least 900 interviews (a response rate of about 57 percent). The field test experiments (described in section 3.2.5) provided an opportunity to evaluate whether nonresponse among prior-round nonrespondents, and the resulting bias, can be minimized.  "}, {"section_title": "Interview Design and Reinterview Analysis", "text": "The purpose of the B&B:08/12 field test was to fully test all procedures, methods, and systems of the study in a realistic operational environment prior to implementing the full-scale study. This chapter provides an overview of the interview design and systems, data collection and outcomes, and an evaluation of the interview."}, {"section_title": "Reinterview Analysis", "text": "An important element of data quality in survey research is the reliability of self-reported responses to interview questions. One method of measuring reliability involves temporal stability, or how constant responses remain over time. That is, do the survey questions evoke the same responses from study members at time one and time two? To evaluate the reliability of responses collected in the B&B: 08/12 field test interview, a reinterview containing items selected from the main field test interview was developed. Items were selected for the reinterview because they were assumed to be temporally stable and most were newly designed or critical main interview items, or both. A random sample of about 320 respondents who had completed a full main interview was selected for reinterview. The reinterview sample was contacted beginning 3 weeks after completion of the main interview and asked to complete the reinterview. The interview was completed by 220 respondents (69 percent). Sample members selected for reinterview were allowed to complete the reinterview either by web or by telephone. Of the 220 respondents who completed the reinterview, 150 (68 percent) completed by web and 70 (32 percent) completed by telephone. The majority of respondents who completed the initial interview by telephone also completed the reinterview by telephone (97 percent). However, of those who completed the initial interview by web, approximately 19 percent switched modes and completed the reinterview by telephone (table 4).  Table 5 shows reliability estimates for the items included in the reinterview. For each item, the number of cases, percentage agreement between the interview and reinterview, and relational statistic are shown. For discrete items, percentage agreement was based on the extent to which responses to the initial interview matched exactly to the reinterview responses. For continuous items, responses were considered in agreement if the initial interview responses were within one standard deviation of the reinterview responses. The relational statistics quantified the strength of association between the pairs of items being compared; for each statistic, 1.00 was indicative of a perfect correlation (i.e., an exact match between the item on the initial interview and the same item on the reinterview for all respondents). The relational statistic, Cramer's V, was used for items with discrete, unordered response categories (e.g., yes/no). Kendall's tau-b (\u03c4b) estimated the relationship between items with ordered categories (e.g., excellent, fair, poor). Lastly, the Pearson product-moment correlation coefficient (r) was used for items with interval responses (e.g., salary). Percentage agreement was generally high between the main interview and the reinterview. The five highest agreement items were in the Postbaccalaureate Education section, with agreement ranging from 95 percent to 98 percent. Three of the five items with the highest agreement were related to the types of financial aid the respondent received. The item with the lowest percentage agreement was in the Postbaccalaureate Education section, Stress from education related debt (43 percent). The low level of agreement for this item may indicate that the temporal component of the concept was vague; that is, respondents were not sure whether to answer based on their current level of stress or their overall level of stress. Asking about the respondent's overall level of stress in the full-scale interview may increase the reliability of this item. The remaining items all have a percentage agreement higher than 80 percent. The highest agreement item in the Postbaccalaureate Employment section was Type of employer training in past year -remedial training (95 percent). The lowest agreement item was Type of employer training in past year -communication training (74 percent). Only four items in this section had a percentage agreement less than 80 percent. The item with the second lowest percentage agreement was Result of the sale of all major possessions (72 percent). The low relational statistic for this item (.52) also suggested that improvements to this question should be explored in the remaining cognitive interview sessions. The only other item in the Student Background section, Respondent has retirement account, had a percentage agreement of 91 percent. Overall, the results of the reinterview analysis indicate the survey was very reliable. The majority of items have a percentage agreement of 80 percent or higher. Only one item had a percentage agreement less than 70 percent."}, {"section_title": "Instrumentation Experiments", "text": "Multiple instrumentation experiments were conducted in the B&B:08/12 field test. The first experiment examined differences in the use of vertical and horizontal visual analog scales (VASs) in conjunction with real-time feedback displays (RTFDs), compared to radio buttons without an RTFD. For these items the RTFD was a \"bucket\" that filled or emptied as the respondent moved a slider (figure 1). A second experiment compared radio buttons and VASs, both without feedback displays. Visual Analog Scales (VAS) Experiment. The use of horizontal and vertical VASs with real-time feedback was compared to radio buttons without real-time feedback for an item that collected data on the percentage of out-of-pocket educational expenses. There were two versions of the feedback display, one vertical and one horizontal. The results obtained from the VASs were compared to the results from the radio button version of the same question. The three question versions are shown in figure 1. Item nonresponse does not vary significantly across the layout types, but the three versions produced significantly different distributions (table 6). Rank sums were significantly higher for the vertical slider and radio button versions compared to the horizontal slider, indicating that those who saw the horizontal layout reported a smaller percentage of out-of-pocket expenses (\u03c7 2 = 11.30, p < .001, \u03c7 2 = 8.12, p = .01). Response times for the vertical, horizontal, and radio button layouts were 29.3 seconds, 34.5 seconds, and 23.2 seconds, respectively. The radio button version took the least time to complete and was significantly faster than the horizontal bucket slider (t(297) = 3.79, p < .001). Radio button experiment. A more general comparison of VASs and radio button layouts was also conducted. Four experimental item versions were evaluated in terms of response rates, response distributions, and time to complete: a 5-point slider with midpoints, a 10-point slider, a 10-point radio button, and a 5-point radio button with midpoints. Figure 2 shows examples of the slider layouts, and figure 3 shows examples of radio button layouts.  "}, {"section_title": "C-16", "text": "B&B:08/12 Data File Documentation There were no significant differences in the distribution of responses or item nonresponse when comparing the 5-point radio button to the 5-point slider. Similarly, there were no significant differences in distribution of responses when comparing the 10-point radio button to the 10-point slider. There were no significant differences in the amount of time it took to complete questions using a 5-point slider versus a 5-point radio button with midpoints; however, there were significant differences between the 10-point slider and the 10-point radio button, with the slider taking significantly longer than the radio button on the following forms: The average times for the 10-point slider version and the 10-point radio button versions of B12DJOBSAT01 were 36.7 seconds and 29.1 seconds, respectively (t(553) = 3.36, p< .001). The average times for the 10-point slider version and the 10-point radio button versions of B12DNJYA01 were 26.8 seconds and 19.2 seconds, respectively (t(553) = 3.66, p < .001). The average times for the 10-point slider version and the 10-point radio button versions of B12DPARSUP01 were 14.3 seconds and 11.8 seconds, respectively (t(461) = 2.16, p < .05). Timing information for each of the response layouts is shown in table 7. "}, {"section_title": "Data Collection Experiments", "text": "As part of the B&B:08/12 field test, the benefits of using a priori propensity scoring and incentive targeting to reduce nonresponse bias were evaluated. The method to estimate nonresponse bias and the variables used are described in appendix E. Nonresponse bias was computed once for all cases and again for all cases with low propensity cases treated as nonrespondents. Additionally, nonresponse bias was computed separately for \u2022 high propensity cases; \u2022 low propensity cases; \u2022 low propensity cases in the control group; \u2022 low propensity cases in the experimental group; \u2022 high propensity cases combined with low propensity control group cases; and B&B:08/12 Data File Documentation C-17 \u2022 high propensity cases combined with low propensity experimental group cases. The first two comparisons were to show if there was nonresponse bias due to low propensity cases, and the last two comparisons were to show if the response propensity experiment successfully reduced nonresponse bias among low propensity cases. Table 8 shows the results of the comparisons. None were statistically significant. A second data collection experiment focused on a short YouTube video aimed at increasing response rates was also conducted. No difference was found in response rates between sample members who did and did not receive the informational video."}, {"section_title": "Help Text Experiments", "text": "Two experiments were conducted with the B&B:08/12 field test help text: (1) the impact of inline hyperlinks on help text use and (2) the impact of item-specific wording on the help text button versus generic wording. Item-specific wording included the phrase \"Help with this question,\" and the generic wording was \"Help.\" Respondents saw four possible combinations of the help text button experiment treatments: \u2022 item-specific wording with a hyperlink; \u2022 item-specific wording without a hyperlink; \u2022 generic wording with a hyperlink; and \u2022 generic wording without a hyperlink. Figure 4 shows the question with and without hyperlinks and the help text button with itemspecific and generic wording. When hyperlinks were present, respondents were significantly more likely to access help text than when there were no hyperlinks. Table 9 shows the percentage of respondents accessing help text when a hyperlink was present and when it was not, within the two help text button formats. Within the item-specific wording treatment, the percentage of respondents accessing help text was 35 percent when hyperlinks were present and 15 percent when they were not (\u03c7 2 = 29.77, p < .0001). Within the generic wording treatment, the percent of respondents accessing help text was 36 percent when hyperlinks were present and 9 percent when they were not (\u03c7 2 = 54.21, p < .0001). When hyperlinks were not present, respondents were significantly more likely to access help when they were presented with the item-specific wording versus the generic wording (\u03c7 2 = 4.44, p = .0351). "}, {"section_title": "Recommendations for the Full-Scale Study", "text": "The 2008/12 Baccalaureate and Beyond Longitudinal Study (B&B:08/12) field test was conducted to test and evaluate study methods and results prior to implementation of the full-scale study. The following recommendations are based on the field test experience. B&B:08/12 full-scale sample. The B&B:08/12 full-scale sample will consist of all B&B:08/09 eligible respondents and all B&B:08/09 nonrespondents. The sample size will be 17,160. The expected yield is about 14,500, given an expected eligibility rate of 95 percent among the B&B:08/09 nonrespondents and an expected 85 percent response rate. The B&B:08/09 full-scale study included 18,500 sample members and consisted of those who were confirmed to be baccalaureate recipients in the 2007-08 National Postsecondary Student Aid Survey (NPSAS:08) interview and also a subsample of potential baccalaureate recipients who were not interviewed in NPSAS:08. There were three types of nonrespondents in B&B:08/09: \u2022 a sample member who responded to the NPSAS:08 interview but did not respond to the B&B:08/09 interview (referred to henceforth as a first follow-up nonrespondent); \u2022 a sample member who did not respond to the NPSAS:08 interview but did respond to the B&B:08/09 interview (referred to henceforth as a base-year nonrespondent); and \u2022 a sample member who did not respond to either the NPSAS:08 or B&B:08/09 interviews (referred to henceforth as a double nonrespondent). Table 10 shows the distribution of the B&B:08/09 full-scale sample by prior response status. Alternative sample designs were considered for the full-scale study, including subsampling first follow-up and double nonrespondents or excluding double nonrespondents. The decision to include all B&B:08/09 nonrespondents in the B&B:08/12 sample was reached after evaluating the implications of including, subsampling, or excluding nonrespondents. It was determined that including all nonrespondents may do the following: \u2022 Be feasible, given the 35 percent response rate for double nonrespondents and the 49 percent response rate for first follow-up nonrespondents in the B&B:08/12 field test; \u2022 Help reduce the nonresponse bias that exists in the B&B:08/09 data; \u2022 Improve the donor pool for imputation; \u2022 Improve the power and precision for analyses; \u2022 Be useful for future analyses using transcript data because there are transcript data for some of the interview nonrespondents; \u2022 Be beneficial for possible future follow-ups of the B&B:08/09 cohort; \u2022 Improve the variance over subsampling due to a potential increase in unequal weighting if subsampling were to be done; \u2022 Affect the time, effort, and cost necessary to obtain interviews; and \u2022 Reduce the overall response rate. Interview data collection. The field test data collection experience is used to inform plans for the full-scale study. Based on feedback from interviewing staff, the full-scale training protocol will be adjusted to include \u2022 additional practice on the CATI-CMS; \u2022 additional review of frequently asked questions, \u2022 expanded training on conversational interviewing, and \u2022 more detailed coverage of interviewer instructions. The full-scale study will continue to use previously proven tracing and locating methods and, possibly, a monetary incentive for participants who update their address. Electronic communications have proved to be an effective way to contact sample members. For the full-scale study, sample members will continue to receive postcards, e-mails, and text messages to remind them that they have been selected for the study, to provide easy access to the web survey, and to encourage their participation. Several experiments were conducted in the field test. A response propensity approach was evaluated to assess its ability to minimize nonresponse bias. Results demonstrated that the response propensity approach was not successful in reducing bias; however, higher incentives were significantly related to increased response for medium-propensity cases (i.e., those at the high end of the low-propensity range and those at the low end of the high-propensity range). These results suggest a three-tiered incentive structure for the full-scale study. Proposed incentive amounts are as follows: \u2022 $20 to the highest-propensity cases (the top 30 percent); \u2022 $35 to the medium-propensity group (the middle 40 percent); and \u2022 $55 to the lowest-propensity cases (the bottom 30 percent). Due to the lack of success in reducing nonresponse bias in the field test, a revised nonresponse experiment will be carried out in the full-scale study. The revised experiment will evaluate the effectiveness of responsive design methodologies based on group and individual metrics that are intended to identify and strategically target the cases most likely to contribute to nonresponse bias. A field test experiment examined the effectiveness of an informational video describing the study on response rates. Results showed that the YouTube video did not increase response rates among those who viewed it; however, the video will be used in full-scale data collection to help capitalize on any effect that repeated exposures may have to positively influence response to future follow-up studies with this cohort. The experiment comparing radio button to slider response options revealed no observable differences in response rates or response distributions; however, the radio button version took significantly less time to complete than the slider option. When comparing 10-point whole number scales to 5-point scales which allowed decimals (e.g., 2.5 points), once again, no differences in response distribution or response rates were seen, but whole number scales took less time to complete. Drawing from these results, the full-scale instrument scales will be 5-point radio buttons, with no decimals. An experiment designed to evaluate help text use compared help text access methods to identify those that resulted in the highest use. Respondents saw either a general or an item-specific help button at the bottom of the screen and, within those conditions, half of the respondents were provided with a hyperlink to the help text in the question wording. When help text was presented as a hyperlink within the question wording, respondents were significantly more likely to access the help text compared to when the hyperlinks were absent. In addition, when help text hyperlinks were not present, the item-specific button significantly increased the use of help text. Based on these results, the full-scale study will incorporate question wording with hyperlinks on selected terms and the item-specific button into the instrument. Generally, field test data indicated no substantial problems with the B&B:08/12 interview during data collection. However, revisions to specific questions in the field test interview will be made for the full-scale interview. These revisions are based on analysis of field test data, feedback from a Technical Review Panel meeting conducted in November 2011, and feedback from cognitive interviews with approximately 30 respondents. A major change to the full-scale instrument will involve the way employment history is collected. Employment data will be collected by employer, rather than by job. Respondents will be asked for their employer(s) and the starting and ending (or current) date(s), along with any periods of leave and the reason for the leave. The full-scale instrument will also collect the date of birth for all dependent children and dates of all marriages and divorces occurring since graduating from college. These changes are intended to capture a timeline of life events to help track changes that may correspond to educational and employment decisions in the years following graduation. Several forms were added in the K-12 Teaching section and others were reorganized or reworded in order to clarify the definition of a \"teaching position\" for respondents. With these changes, it is possible to include both components of a teaching position-the teacher type (e.g., regular classroom teacher) and school or district name (e.g., Smith High School)-in question wording throughout the teaching position loop. Administrative records matching recommendations. Administrative records matching procedures for the full-scale study will be very similar to procedures used in the field test for both the Central Processing System (CPS) and the National Student Loan Data System (NSLDS). A match with the CPS database for the Free Application for Federal Student Aid (FAFSA) data will occur for the 2010-11, 2011-12, and 2012-13 academic years. Students will likely be matched with the NSLDS database for federal loan and Pell Grant data at three different times during data collection. Two interim matches to obtain preliminary data and a final match prior to data delivery will be performed. In addition to matching with CPS and NSLDS, the full-scale study will include administrative records matches with National Student Clearinghouse (NSC) for enrollment and degree data. The match with NSC will occur near the end of data collection. Data file editing and preparation recommendations. Full-scale survey data will be edited, processed, and prepared for delivery in a manner similar to the procedures described for the field test. There are no revisions required for the full-scale plans based on the field test experience. Alternative coding methods are being evaluated in an attempt to reduce respondent burden and improve accuracy. The full-scale data collection may include components that permit the evaluation of those alternatives in more detail."}, {"section_title": "Thank You and Reminder Postcard", "text": "Recently, we sent you information about the Baccalaureate and Beyond Longitudinal Study (B&B). If you have already completed the survey, we would like to thank you. Your assistance is very much appreciated. If you have not yet completed the survey, we would like to remind you that if you complete the survey, you will receive $\u00abincentive\u00bb as a token of our appreciation. To complete the online survey over our secure website, log on to https://surveys.nces.ed.gov/bb/. When B&B data collection begins in August 2012, you will receive a letter in a large white envelope that will provide specific information about how to participate. The letter will explain that if you complete the approximately 35-minute survey on the Web by the date indicated, you will receive $\u00abincamt\u00bb as a token of our appreciation. In the meantime, we need to update our contact information for you. Please help us now by providing your mailing address, telephone number(s), and e-mail address(es) online at https://surveys.nces.ed.gov/bb/. You will also find out more about B&B at this website. As a token of our appreciation for providing your contact information, you will receive $10. NCES has contracted with RTI International to conduct B&B on its behalf. The enclosed brochure provides a brief description of B&B, findings from past studies, and a summary of our strict confidentiality procedures. If you have additional questions or concerns about the study after reviewing this material, please call the RTI study director, Melissa Cominole at 1-866-662-8227. We thank you in advance for your participation in this important study. Your cooperation is greatly appreciated. We will be contacting \u00abfname\u00bb and other study participants in \u00abstart_month\u00bb to ask questions about \u00abpronoun1\u00bb education and work experiences after graduation. We are asking for your help in updating our records so that we will be able to get in touch with \u00abpronoun1\u00bb. Only a limited number of people are selected for this study so it is extremely important that we be able to contact \u00abpronoun1\u00bb. If \u00abfname\u00bb completes the survey by the date provided in the announcement letter we will be sending in \u00abstart month\u00bb, \u00abpronoun1\u00bb will receive a \u00abpronoun2\u00bb incentive as a token of our appreciation. Before data collection can begin, we need your help to update our records for \u00abfname\u00bb \u00ablname\u00bb."}, {"section_title": "Please take a few minutes right now to update the enclosed Address Update Information sheet and return it in the enclosed postage-paid envelope. As a token of our appreciation for providing \u00abfname\u00bb's contact information, \u00abpronoun1\u00bb will receive <<UPDATE_INC_AMT>>.", "text": "NCES has contracted with RTI International to conduct the B&B study on its behalf. Please be assured that both NCES and RTI follow strict confidentiality procedures to protect the privacy of study participants. If you would like more information about the B&B study, please visit http://surveys.nces.ed.gov/bb/ or call the RTI study director, Melissa Cominole at 1-866-662-8227. We sincerely appreciate your assistance and thank you in advance for helping us conduct this important study. Surveys for the Baccalaureate and Beyond Longitudinal Study (B&B) are now being conducted. The survey will take about 35 minutes to complete. As a token of our appreciation, once you complete the survey, we will mail you $\u00abIncAmt\u00bb check. You may access the survey by logging on to our secure website at https://surveys.nces.ed.gov/bb/ using the Study ID and password provided below. The password is case sensitive; you will need to enter it exactly as it appears here. Enclosed you will find a brochure with a brief description of findings from prior B&B studies and our strict security procedures. Federal law requires that we protect your privacy. Your responses will be secured behind firewalls and will be encrypted during internet transmission. Your responses will be used only for statistical purposes and may not be disclosed, or used, in identifiable form for any other purpose, except as required by law (20 U.S.C. \u00a7 9573). If you have questions, problems completing your survey online, or prefer to complete the survey over the telephone, simply call the B&B Help Desk at 1-877-262-4440. Your participation, while voluntary, is critical to the study's success. You can learn more about B&B by watching a brief informational video, featuring ED, on our study website, https://surveys.nces.ed.gov/bb/. On behalf of the National Center for Education Statistics (NCES) in the U.S. Department of Education's Institute of Education Sciences and the staff of B&B, I would like to thank you for updating your contact information for the B&B survey. Enclosed you will find a check for $10 as a token of our appreciation. Once you complete the survey, we will mail you an additional $\u00abIncAmt -promised\u00bb check. Your participation in B&B is very important in helping to ensure the success of the study. You may access the survey by logging on to our secure website at https://surveys.nces.ed.gov/bb/ using the Study ID and password provided below. The password is case sensitive; you will need to enter it exactly as it appears here. B&B collects information on how earning a bachelor's degree affects the lives of college graduates as it pertains to their transitions to the workforce. In addition to employment questions, the interview covers topics such as any experiences in additional education, earnings and expenses, family formation, and personal and professional goals. To ensure that B&B is representative of all 2007-08 graduates, we need your participation."}, {"section_title": "Study", "text": "You will receive a <<$IncAmt>> check as a token of our appreciation<< in addition to the $5 bill included in this letter>>. The interview takes approximately 35 minutes to complete. To complete the interview by telephone with a professional interviewer, call the B&B Help Desk toll-free at 1-877-262-4440. If you wish to complete the interview over the Web, simply log onto our secure website: https://surveys.nces.ed.gov/bb/ Study ID = \u00abcaseid\u00bb Password = \u00abpassword\u00bbr Please note that the survey is not designed for mobile handheld browsers and that the password is case sensitive and must be entered exactly as it appears here. Your responses will be used only for statistical purposes and may not be disclosed, or used, in identifiable form for any other purpose, except as required by law. If you have any questions or concerns about the study itself, please contact the B&B Project Director, Melissa Cominole, toll-free at 1-877-225-8470 (email: mcominole@rti.org) or me at 202-502-7383 (e-mail: ted.socha@ed.gov). Thank you in advance for your participation in this important study. Your cooperation is greatly appreciated and needed to make this study a success. "}, {"section_title": "SUMSTFLG", "text": "Interview completion flag SUMSTFLG indicates the completion status. Respondents with SUMSTFLG = 3 were selected to participate in the abbreviated interview (ABBREV = 1) and completed their interview. Respondents with SUMSTFLG = 2 failed to complete their interview, either abbreviated or full. The respondent must have completed the Eligibility (B12A*), Undergraduate Education (B12B*), Post-Bachelor's Education (B12C*), and the first employer loop in Employment (B12D*) to be considered a final partial interview. 1 = Full complete student interview 2 = Partial student interview (full or abbreviated) 3 = Completed abbreviated student interview Applies To: All respondents. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "ABBREV", "text": "Abbreviate interview status ABBREV is a flag to indicate whether the respondent was selected to complete the abbreviated interview. The abbreviated interview consisted of select forms in each section of the interview. 0 = Did not participate in the abbreviated interview 1 = Yes, participated in the abbreviated interview Applies To: All respondents. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12AELIG", "text": "NPSAS enrollment between July 1, 2007 andJune 30, 2008 Were you enrolled at [NPSAS] "}, {"section_title": "B12AMULTDG", "text": "Working on bachelor's at NPSAS between July 1, 2007 andJune 30, 2008 Were you working on a bachelor's degree at [NPSAS] at any time during the 2007-08 school year (July 1, 20071, -June 30, 2008 At what other school have you been enrolled between the time you graduated from high school and the time you graduated from [NPSAS], and in what city and state is it located? (If you attended more than one other school between high school and before your graduation from [NPSAS] tell us about the most recent school first. You will have an opportunity to tell us about all schools later.) (Hints: Do not use abbreviations or acronyms such as ASU for Arizona State University. Entering a school name with the city and state will help to limit the number of schools displayed.) [else] What is the name of that school? (If you attended more than one other school between high school and before your graduation from [NPSAS] tell us about the most recent school first. You will have an opportunity to tell us about all schools later.) (Hints: Do not use abbreviations or acronyms such as ASU for Arizona State University. Entering a school name with the city and state will help to limit the number of schools displayed.) School name. NOTE: If the institution is successfully coded using the IPEDS coder in the B&B:08/12 student interview, this information comes from IPEDS; otherwise, this information is provided by the respondent. At what other school have you been enrolled between the time you graduated from high school and the time you graduated from [NPSAS], and in what city and state is it located? (If you attended more than one other school between high school and before your graduation from [NPSAS] tell us about the most recent school first. You will have an opportunity to tell us about all schools later.) (Hints: Do not use abbreviations or acronyms such as ASU for Arizona State University. Entering a school name with the city and state will help to limit the number of schools displayed.) [else] What is the name of that school? (If you attended more than one other school between high school and before your graduation from [NPSAS] tell us about the most recent school first. You will have an opportunity to tell us about all schools later.) (Hints: Do not use abbreviations or acronyms such as ASU for Arizona State University. Entering a school name with the city and state will help to limit the number of schools displayed.) City. NOTE: City where the institution is located. If the institution is successfully coded using the IPEDS coder in the B&B:08/12 student interview, this information comes from IPEDS; otherwise, this information is provided by the respondent.   \nWorking on bachelor's at NPSAS between July 1, 2007 andJune 30, 2008 Were you working on a bachelor's degree at [NPSAS] at any time during the 2007-08 school year (July 1, 20071, -June 30, 2008? 0 = Did not work on bachelor's 1 = Yes, worked on bachelor's Applies To: Respondents who did not complete the BB:08/09 interview and indicated that they did not work on a bachelor's degree at their NPSAS school between July 1, 2007-June 30, 2008 and B12ADGBA = 0. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12BLEVL01", "text": "Other prebachelor's school 1: level Is that school... Level. NOTE: Indicates the length of time it takes to complete the highest level of program offered by the institution. If the institution is successfully coded using the IPEDS coder in the B&B:08/12 student interview, this information comes from IPEDS; otherwise, this information is provided by the respondent. 1 = 4-year 2 = 2-year 3 = Less-than-2-year Applies To: B&B:08/09 nonrespondents who reported attending at least one other college, university, or "}, {"section_title": "B12BCTRL01", "text": "Other prebachelor's school 1: control Is that school..."}, {"section_title": "Control. NOTE:", "text": "The institution control is a classification of whether an institution is operated by publicly elected/appointed officials or by privately elected/appointed officials and derives its major source of funds from private sources. If the institution is successfully coded using the IPEDS coder in the B&B:08/12 student interview, this information comes from IPEDS; otherwise, this information is provided by the respondent. Public institution -An educational institution whose programs and activities are operated by publicly elected or appointed school officials and which is supported primarily by public funds. Private nonprofit institution -A private institution in which the individual(s) or agency in control receives no compensation, other than wages, rent, or other expenses for the assumption of risk. These include both independent nonprofit schools and those affiliated with a religious organization. Private for-profit institution -A private institution in which the individual(s) or agency in control receives compensation other than wages, rent, or other expenses for the assumption of risk. \nThe institution control is a classification of whether an institution is operated by publicly elected/appointed officials or by privately elected/appointed officials and derives its major source of funds from private sources. If the institution is successfully coded using the IPEDS coder in the B&B:08/12 student interview, this information comes from IPEDS; otherwise, this information is provided by the respondent. Public institution -An educational institution whose programs and activities are operated by publicly elected or appointed school officials and which is supported primarily by public funds. Private nonprofit institution -A private institution in which the individual(s) or agency in control receives no compensation, other than wages, rent, or other expenses for the assumption of risk. These include both independent nonprofit schools and those affiliated with a religious organization. Private for-profit institution -A private institution in which the individual(s) or agency in control receives compensation other than wages, rent, or other expenses for the assumption of risk. \nThe institution control is a classification of whether an institution is operated by publicly elected/appointed officials or by privately elected/appointed officials and derives its major source of funds from private sources. If the institution is successfully coded using the IPEDS coder in the B&B:08/12 student interview, this information comes from IPEDS; otherwise, this information is provided by the respondent. Public institution -An educational institution whose programs and activities are operated by publicly elected or appointed school officials and which is supported primarily by public funds. Private nonprofit institution -A private institution in which the individual(s) or agency in control receives no compensation, other than wages, rent, or other expenses for the assumption of risk. These include both independent nonprofit schools and those affiliated with a religious organization. Private for-profit institution -A private institution in which the individual(s) or agency in control receives compensation other than wages, rent, or other expenses for the assumption of risk. 1 = Public 2 = Private nonprofit 3 = Private for-profit Applies To: Respondents who reported attending at least one school for a postbaccalaureate degree or certificate. Where: B12CPSTGRD = 1. Source: B&B:08/12 Full-scale Student Interview, IPEDS 2011"}, {"section_title": "B12BIPDUC01", "text": "Other prebachelor's school 1: school coded after interview Respondents entered text strings for school name, city and state, then selected a corresponding school using the IPEDS coder built into the instrument. In some cases, respondents did not select a school in the coder. Survey contractor staff attempted to code these schools and when they were able to do so, this flag was set to 1. If the flag was set to 1, the following variables were changed to reflect the new code selected: B12BIPED01, B12BSCH01, B12BCT01, B12BST01, B12BLEVL01, B12BCTRL01. 0 = Not upcoded 1 = Yes, upcoded What is the name of that school, and in what city and state is it located? Please bear with me as I code this -IT SHOULD JUST TAKE A SECOND. City. NOTE: City where the institution is located. If the institution is successfully coded using the IPEDS coder in the B&B:08/12 student interview, this information comes from IPEDS; otherwise, this information is provided by the respondent.  "}, {"section_title": "B12CLEVL01", "text": "Postbaccalaureate school 1: level Is this school... Level. NOTE: Indicates the length of time it takes to complete the highest level of program offered by the institution. If the institution is successfully coded using the IPEDS coder in the B&B:08/12 student interview, this information comes from IPEDS; otherwise, this information is provided by the respondent. 1 = 4-year 2 = 2-year 3 = Less-than-2-year Applies To: Respondents who reported attending at least one school for a postbaccalaureate degree or certificate. Where: B12CPSTGRD = 1. Source: B&B:08/12 Full-scale Student Interview, IPEDS 2011\nPostbaccalaureate school 1: level Is this school... Level. NOTE: Indicates the length of time it takes to complete the highest level of program offered by the institution. If the institution is successfully coded using the IPEDS coder in the B&B:08/12 student interview, this information comes from IPEDS; otherwise, this information is provided by the respondent. 1 = 4-year 2 = 2-year 3 = Less-than-2-year Applies To: Respondents who reported attending at least one school for a postbaccalaureate degree or certificate. Where: B12CPSTGRD = 1. Source: B&B:08/12 Full-scale Student Interview, IPEDS 2011"}, {"section_title": "B12CCTRL01", "text": "Postbaccalaureate school 1: control Is this school...\nPostbaccalaureate school 1: control Is this school..."}, {"section_title": "B12CIPDUC01", "text": "Postbaccalaureate school 1: school coded after interview NOTE: Respondents entered text strings for school name, city and state, then selected a corresponding school using the IPEDS coder built into the instrument. In some cases, respondents did not select a school in the coder. Survey contractor staff attempted to code these schools and when they were able to do so, this flag was set to 1. If the flag was set to 1, the following variables were changed to reflect the new code selected: 01.0000 = Agriculture, general 01.0101 = Agricultural business and mgmt, general 01.0102 = Agribusiness/agricultural bus operations 01.0103 = Agricultural economics 01.0104 = Farm/farm and ranch management 01.0105 = Agricultur/farm supplies retail/wholesal 01.0106 = Agricultural business technology 01.0199 = Agricultural business and mgmt, other 01.0201 = Agricultural mechanization, general 01.0204 = Agricultural power machinery operation 01.0205 = Agricultural mech and equip/machine tech 01.0299 = Agricultural mechanization, other 01.0301 = Agricultural production ops, general 01.0302 = Animal/livestock husbandry/production 01.0303 = Aquaculture 01.0304 = Crop production 01.0306 = Dairy husbandry and production 01.0307 = Horse husbandry/equine science and mgmt 01.0308 = Agroecology and sustainable agriculture 01.0309 = Viticulture and enology 01.0399 = Agricultural production ops, other 01.0401 = Agricultural and food product processing 01.0504 = Dog/pet/animal grooming 01.0505 = Animal training 01.0507 = Equestrian/equine studies 01.0508 = Taxidermy 13.0406 = Higher education/higher education admin 13.0407 = Community college education 13.0408 = Elementary/middle school admin/principal 13.0409 = Secondary school admin/principalship 13.0410 = Urban education and leadership 13.0411 = Superintendency/educational system admin 13.0499 = Educational admin and supervision, other 13.0501 = Educational/instructional technology 13.0601 = Educational evaluation and research 13.0603 = Educational statistics/research methods 13.0604 = Educational assess/testing/measurement 13.0607 = Learning sciences 13.0699 = Educational assess/eval/research, other 13.0701 = International and comparative education 13.0901 = Social/philosophical foundations of ed 13.1001 = Special education and teaching, general 13.1003 = Ed/teaching indiv with hearing impair 13.1004 = Ed/teaching gifted and talented 13.1005 = Ed/teaching indiv with emotional disturb 13.1006 = Ed/teaching indiv w mental retardation 13.1007 = Ed/teaching indiv with mult disabilities 13.1008 = Ed/teaching indiv w physical impairments 13.1009 = Ed/teaching indiv w vision impairment 13.1011 = Ed/teaching indiv w learning disab 13.1012 = Ed/teaching indiv w speech/lang impair 13.1013 = Ed/teaching indiv with autism 13.1014 = Ed/teaching indiv develop delayed 13.1015 = Ed/teaching early childhood special ed 13.1016 = Ed/teaching indiv w traum brain injury 13.1017 = Ed/teaching elementary special ed 13.1018 = Ed/teaching middle school special ed 13.1019 = Ed/teaching secondary special ed 13.1099 = Special education and teaching, other 13.1101 = Counselor ed/school counseling/guidance 13.1102 = College student counsel/personnel srvcs 13.1199 = Student counseling/personnel srvcs other 13.1201 = Adult/continuing education and teaching 13.1202 = Elementary education and teaching 13.1203 = Junior high/middle school ed/teaching 13.1205 = Secondary education and teaching 13.1206 = Teacher education, multiple levels 13.1207 = Montessori teacher education 13.1208 = Waldorf/Steiner teacher education 13.1209 = Kindergarten/preschool ed/teaching   admin/research/clinic 51.3901 = Licensed practical/vocat nurse training 51.3902 = Nurse/nursing aide/patient care asst 51.3999 = Practical/vocational nursing/assts other 51.9999 = Health professions/related science other 52.0101 = Business/commerce, general 52.0201 = Business admin and management, general 52.0202 = Purchasing, procurement/contracts mgmt 52.0203 = Logistics materials/supply chain mgmt 52.0204 = Office management and supervision 52.0205 = Operations management and supervision 52.0206 = Non-profit/public/organizational mgmt 52.0207 = Customer service management 52.0208 = E-commerce/electronic commerce 52.0209 = Transportation/mobility management 52.0210 = Research and development management 52.0211 = Project management 52.0212 = Retail management 52.0213 = Organizational leadership 52.0299 = Business/managerial operations, other 52.0301 = Accounting 52.0302 = Accounting tech/technician/bookkeeping 52.0303 = Auditing 52.0304 = Accounting and finance 52.0305 = Accounting and business/management 52.0399 = Accounting and related services, other 52.0401 = Admin asst/secretarial sciences, general 52.0402 = Executive assistant/executive secretary 52.0406 = Receptionist 52.0407 = Business/office automation/data entry 52.0408 = General office occupations/clerical srvc 52.0409 = Parts, warehousing, inventory mgmt ops 52.0410 = Traffic/customs/transport clerk/tech 52.0411 = Customer service support/call center 52.0499 = Business ops support services, other 52.0501 = Business/corporate communications 52.0601 = Business/managerial economics 52.0701 = Entrepreneurship/entrepreneurial studies 52.0702 = Franchising and franchise operations 52.0703 = Small business administration/mgmt 52.0799 = Entrepreneurial and small bus ops, other 52.0801 = Finance, general 52.0803 = Banking and financial support services 52.0804 = Financial planning and services 52.0806 = International finance 52.0807 = Investments and securities 52.0808 = Public finance 52.0809 = Credit management 52.0899 = Finance/financial mgmt services, other 52.0901 = Hospitality administration/mgmt, general 52.0903 = Tourism and travel services management 52.0904 = Hotel/motel administration/management 52.0905 = Restaurant/food services management 52.0906 = Resort management 52.0907 = Meeting and event planning 52.0908 = Casino management 52.0909 = Hotel, motel, and restaurant management 52.0999 = Hospitality administration/mgmt, other 52.1001 = Human resources mgmt/pers admin, general 52.1002 = Labor and industrial relations 52.1003 = Organizational behavior studies 52.1004 = Labor studies 52.  \nPostbaccalaureate school 1: school coded after interview NOTE: Respondents entered text strings for school name, city and state, then selected a corresponding school using the IPEDS coder built into the instrument. In some cases, respondents did not select a school in the coder. Survey contractor staff attempted to code these schools and when they were able to do so, this flag was set to 1. If the flag was set to 1, the following variables were changed to reflect the new code selected: B12CIPED01, B12CSCH01, B12CCT01, B12CST01, B12CLEVL01, B12CCTRL01. 0 = Not upcoded 1 = Yes, upcoded This variable was one of a series of items presented to respondents in a loop in the B&B:12 Fullscale interview. For each employer, beginning with the first employer respondents reported working for after receiving their bachelor's degrees (including jobs continued since before the bachelor's degree), respondents were asked to report basic information for that employer. The variables included in this loop are: (where xx = loop iteration) B12DWKyymmxx (where yy = year and mm = month), B12DEMPSMYxx, B12DWKPRIxx, This variable was one of a series of items presented to respondents in a loop in the B&B:12 Fullscale interview. For each employer, beginning with the first employer respondents reported working for after receiving their bachelor's degrees (including jobs continued since before the bachelor's degree), respondents were asked to report basic information for that employer. The variables included in this loop are: (where xx = loop iteration) B12DWKyymmxx (where yy = year and mm = month), B12DEMPSMYxx, B12DWKPRIxx, This variable was one of a series of items presented to respondents in a loop in the B&B:12 Fullscale interview. For each employer, beginning with the first employer respondents reported working for after receiving their bachelor's degrees (including jobs continued since before the bachelor's degree), respondents were asked to report basic information for that employer. The variables included in this loop are: (where xx = loop iteration) B12DWKyymmxx (where yy = year and mm = month), B12DEMPSMYxx, B12DWKPRIxx, January 2013 NOTE: This variable was one of a series of items presented to respondents in a loop in the B&B:12 Fullscale interview. For each employer, beginning with the first employer respondents reported working for after receiving their bachelor's degrees (including jobs continued since before the bachelor's degree), respondents were asked to report basic information for that employer. The variables included in this loop are: (where xx = loop iteration) B12DWKyymmxx (where yy = year and mm = month), B12DEMPSMYxx,B12DWKPRIxx,B12DEMPCURxx,B12DBKTMPxx,B12DBKRESxx,B12DBKLEVxx,B12DBKOTHxx B12DEMPAMTxx,B12DEMPAT2xx B12DEMPTIMxx,B12DEMPTM2xx,B12DEMPHRSxx,B12DEMPHR2xx,B12DEMPFPTxx,and "}, {"section_title": "B12CMAJRC01", "text": "Postbaccalaureate school 1: B12CMGEN01/B12CMSPE01 recode/upcode flag NOTE: Respondents entered a text string describing a major, then selected a major using the major coder built into the instrument. In some cases, respondents did not select a major in the coder. Survey contractor staff attempted to code these text strings and when they were able to do so, this flag was set to 1. Some majors that were coded by respondents were randomly selected for a recoding review by expert coders. In the event that the major was recoded to a new value, this flag was set to 1. If the flag was set to 1, the following variables were changed to reflect the new code selected: B12CMGEN01, B12CMSPE01.   Recode note: If B12CFED01 = 0 and B12CPRIV01 = 0 and B12CGRANT01 = 0 and B12CASST01 = 0 and B12CWRKSDY01 = 0 and B12CEMPAID01 = 0 and B12CGIFT01 = 0 and B12COTHAID01 = 0 and B12CPOCKET01 in 1 2 3 "}, {"section_title": "B12CELNMOS", "text": "Monthly federal student loan payment How much do you typically pay each month on your federal student loans? (Please provide the amount you pay each month, even if it is different from your minimum monthly payment.) (Please answer based on any federal student loans you have, including loans for your bachelor's degree and for any education since your bachelor's degree. If the amount changes, please report the most recent amount.) Applies To: Respondents who were actively repaying federal student loans and did not participate in the abbreviated interview. Where: B12CELNSTAT in (-8 1 2 3) and ABBREV ne 1. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12CELNMEST", "text": "Estimated monthly federal student loan payment Please indicate the range that best represents the total current monthly payment for your federal student loans. Would you say it was... 0 = $0.00 1 = $0.01 -$49.99 2 = $50.00 -$99.99 3 = $100.00 -$149.99 4 = $150.00 -$199.99 5 = $200.00 -$249.99 6 = $250.00 -$499.99 7 = $500.00 -$749.99 8 = $750.00 -$999.99 9 = $1000.00 or more Applies To: Respondents who did not provide their monthly federal loan payment amount and did not participate in the abbreviated interview, or respondents who were actively repaying federal student loans and participated in the abbreviated interview. Where: (B12CELNMOS = -9 and ABBREV ne 1) or (B12CELNSTAT in 1 2 3 1 2 3 4 "}, {"section_title": "B12CSOC", "text": "Importance in choosing field of study: contribution to society There may be several factors that influence how people choose a field of graduate-level study. Please indicate how important each of the following is to you in choosing your field. Would you say not at all important, somewhat important, important, or very important? Ability to contribute to society via the field (e.g., cure or prevent disease, improve education) 1 = Not at all important 2 = Somewhat important 3 = Important 4 = Very important Applies To: Respondents who did not report attending a postbaccalaureate school, did not indicate postbaccalaureate enrollment in 2012-13 school year, reported they were likely to enroll in a program in the future, and did not participate in the abbreviated interview. Where: B12CPSTGRD not in 1 2 "}, {"section_title": "B12DBKTMP01", "text": "Employer grid 1: employment break: seasonal or temporary Based on the dates you provided, it appears that there was a break in your employment with [employer 1] (i.e., it was not one continuous period). Why were you not employed during the time you indicated? Employment was seasonal or temporary NOTE: This variable was one of a series of items presented to respondents in a loop in the B&B:12 Fullscale interview. For each employer, beginning with the first employer respondents reported working for after receiving their bachelor's degrees (including jobs continued since before the bachelor's degree), respondents were asked to report basic information for "}, {"section_title": "B12DBKLEV01", "text": "Employer grid 1: employment break: medical, personal, family leave Based on the dates you provided, it appears that there was a break in your employment with [employer 1] (i.e., it was not one continuous period). Why were you not employed during the time you indicated? Took a medical, personal, or family leave NOTE: This variable was one of a series of items presented to respondents in a loop in the B&B:12 Fullscale interview. For each employer, beginning with the first employer respondents reported working for after receiving their bachelor's degrees (including jobs continued since before the bachelor's degree), respondents were asked to report basic information for that employer. The variables included in this loop are: (where xx = loop iteration) B12DWKyymmxx (where yy = year and mm What was your starting salary including bonuses, tips, and commissions? NOTE: This question was asked in two parts: respondents were asked to provide a salary dollar amount (B12DEMPAMT01) and then indicate the time frame for that salary (B12DEMPTIM01). Therefore, this variable includes salary amounts reported in hourly, weekly, monthly, and yearly amounts. These data must be combined with the accompanying time frame data for this job in order to be analyzed. This variable was one of a series of items presented to respondents in a loop in the B&B:12 Full-scale interview. For each employer, beginning with the first employer respondents reported working for after receiving their bachelor's degrees (including jobs continued since before the bachelor's degree), respondents were asked to report basic information for that employer. The variables included in this loop are: (where xx = loop iteration) B12DWKyymmxx (where yy = year and mm What is/was your current/ending salary? Amount NOTE: This question was asked in two parts: respondents were asked to provide a salary dollar amount (B12DEMPAT201) and then indicate the time frame for that salary (B12DEMPTM201). Therefore, this variable includes salary amounts reported in hourly, weekly, monthly, and yearly amounts. These data must be combined with the accompanying time frame data for this job in order to be analyzed. This variable was one of a series of items presented to respondents in a loop in the B&B:12 Full-scale interview. For each employer, beginning with the first employer respondents reported working for after receiving their bachelor's degrees (including jobs continued since before the bachelor's degree), respondents were asked to report basic information for that employer. \nEmployer grid 1: employment break: medical, personal, family leave Based on the dates you provided, it appears that there was a break in your employment with [employer 1] (i.e., it was not one continuous period). Why were you not employed during the time you indicated? Took a medical, personal, or family leave NOTE: This variable was one of a series of items presented to respondents in a loop in the B&B:12 Fullscale interview. For each employer, beginning with the first employer respondents reported working for after receiving their bachelor's degrees (including jobs continued since before the bachelor's degree), respondents were asked to report basic information for that employer. The variables included in this loop are: (where xx = loop iteration) B12DWKyymmxx (where yy = year and mm = month), B12DEMPSMYxx,B12DWKPRIxx,B12DEMPCURxx,B12DBKTMPxx,B12DBKRESxx,B12DBKLEVxx,B12DBKOTHxx B12DEMPAMTxx,B12DEMPAT2xx B12DEMPTIMxx,B12DEMPTM2xx,B12DEMPHRSxx,B12DEMPHR2xx,B12DEMPFPTxx,and B12DEMPFT2xx. 0  What was your starting salary including bonuses, tips, and commissions? NOTE: This question was asked in two parts: respondents were asked to provide a salary dollar amount (B12DEMPAMT01) and then indicate the time frame for that salary (B12DEMPTIM01). Therefore, this variable includes salary amounts reported in hourly, weekly, monthly, and yearly amounts. These data must be combined with the accompanying time frame data for this job in order to be analyzed. This variable was one of a series of items presented to respondents in a loop in the B&B:12 Full-scale interview. For each employer, beginning with the first employer respondents reported working for after receiving their bachelor's degrees (including jobs continued since before the bachelor's degree), respondents were asked to report basic information for that employer. The variables included in this loop are: (where xx = loop iteration) B12DWKyymmxx (where yy = year and mm = month), B12DEMPSMYxx,B12DWKPRIxx,B12DEMPCURxx,B12DBKTMPxx,B12DBKRESxx,B12DBKLEVxx,B12DBKOTHxx B12DEMPAMTxx,B12DEMPAT2xx B12DEMPTIMxx,B12DEMPTM2xx,B12DEMPHRSxx,B12DEMPHR2xx,B12DEMPFPTxx,and  Did you consider your starting job full-time or parttime? NOTE: This variable was one of a series of items presented to respondents in a loop in the B&B:12 Fullscale interview. For each employer, beginning with the first employer respondents reported working for after receiving their bachelor's degrees (including jobs continued since before the bachelor's degree), respondents were asked to report basic information for that employer. The variables included in this loop are: (where xx = loop iteration) B12DWKyymmxx (where yy = year and mm = month), B12DEMPSMYxx,B12DWKPRIxx,B12DEMPCURxx,B12DBKTMPxx,B12DBKRESxx,B12DBKLEVxx,B12DBKOTHxx B12DEMPAMTxx,B12DEMPAT2xx B12DEMPTIMxx,B12DEMPTM2xx,B12DEMPHRSxx,B12DEMPHR2xx,B12DEMPFPTxx,and  What is/was your current/ending salary? Amount NOTE: This question was asked in two parts: respondents were asked to provide a salary dollar amount (B12DEMPAT201) and then indicate the time frame for that salary (B12DEMPTM201). Therefore, this variable includes salary amounts reported in hourly, weekly, monthly, and yearly amounts. These data must be combined with the accompanying time frame data for this job in order to be analyzed. This variable was one of a series of items presented to respondents in a loop in the B&B:12 Full-scale interview. For each employer, beginning with the first employer respondents reported working for after receiving their bachelor's degrees (including jobs continued since before the bachelor's degree), respondents were asked to report basic information for that employer. The variables included in this loop are: (where xx = loop iteration) B12DWKyymmxx (where yy = year and mm = month), B12DEMPSMYxx,B12DWKPRIxx,B12DEMPCURxx,B12DBKTMPxx,B12DBKRESxx,B12DBKLEVxx,B12DBKOTHxx B12DEMPAMTxx,B12DEMPAT2xx B12DEMPTIMxx,B12DEMPTM2xx,B12DEMPHRSxx,B12DEMPHR2xx,B12DEMPFPTxx,and  Do/did you consider your current/ending job full-time or part-time? NOTE: This variable was one of a series of items presented to respondents in a loop in the B&B:12 Fullscale interview. For each employer, beginning with the first employer respondents reported working for after receiving their bachelor's degrees (including jobs continued since before the bachelor's degree), respondents were asked to report basic information for that employer. The variables included in this loop are: (where xx = loop iteration) B12DWKyymmxx (where yy = year and mm = month), B12DEMPSMYxx,B12DWKPRIxx,B12DEMPCURxx,B12DBKTMPxx,B12DBKRESxx,B12DBKLEVxx,B12DBKOTHxx B12DEMPAMTxx,B12DEMPAT2xx B12DEMPTIMxx,B12DEMPTM2xx,B12DEMPHRSxx,B12DEMPHR2xx,B12DEMPFPTxx,and "}, {"section_title": "SAWJOBLOOP01", "text": "Job 1: saw job loop This variable indicates whether the respondent received the first job loop in the B&B:08/12 interview. Questions in this loop relate to the most recent job at employer 1. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. the closest match from the options returned from our database based on your information. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. the closest match from the options returned from our database based on your information. NOTE: 2010 Occupational Information Network (O*NET) occupation codes were used to code these data. See http://www.onetcenter.org/ for more information on O*NET. When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 111=  the closest match from the options returned from our database based on your information. NOTE: 2010 Occupational Information Network (O*NET) occupation codes were used to code these data. See http://www.onetcenter.org/ for more information on O*NET. When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. \nJob 1: saw job loop This variable indicates whether the respondent received the first job loop in the B&B:08/12 interview. Questions in this loop relate to the most recent job at employer 1. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. To earn extra money NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007 2008. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 0 = Did not work more than 40 hours to earn more money 1 = Yes, worked more than 40 hours to earn more money Applies To: Respondents for whom employer 1 was selected for additional questions based on duration and recency of employment and (if needed) random selection and worked at employer 1  In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 0 = Did not work more than 40 hours due to job demands 1 = Yes, worked more than 40 hours due to job demands Applies To: Respondents for whom employer 1 was selected for additional questions based on duration and recency of employment and (if needed) random selection and worked at employer 1  Other reason not listed. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. When you were employed, did you consider this job to be part of a career you were pursuing in your occupation or industry? NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 0 = Job is not the start of career 1 = Yes, job is the start of career   B&B:08/12 Data File Documentation G-3  B&B:08/12 Data File Documentation G-5  B&B:08/12 Data File Documentation G-7  B&B:08/12 Data File Documentation G-9        B&B:08/12 Data File Documentation G-17                                        1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight adjusted for subsampling) and the mean of all sample cases (using the base weight adjusted for subsampling). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the full sample. 3 Bias in the sample mean is estimated as the product of the base weighted nonresponse rate and the difference between the mean of respondent cases (using base weight adjusted for subsampling) and the mean of nonrespondent cases (using the base weight adjusted for subsampling). 4 Base weight, adjusted for subsampling and nonresponse. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington.   1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight adjusted for subsampling) and the mean of all sample cases (using the base weight adjusted for subsampling). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the full sample. 3 Bias in the sample mean is estimated as the product of the base weighted nonresponse rate and the difference between the mean of respondent cases (using base weight adjusted for subsampling) and the mean of nonrespondent cases (using the base weight adjusted for subsampling). 4 Base weight, adjusted for subsampling and nonresponse. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight adjusted for subsampling) and the mean of all sample cases (using the base weight adjusted for subsampling). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the full sample. 3 Bias in the sample mean is estimated as the product of the base weighted nonresponse rate and the difference between the mean of respondent cases (using base weight adjusted for subsampling) and the mean of nonrespondent cases (using the base weight adjusted for subsampling). 4 Base weight, adjusted for subsampling and nonresponse. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 Categories were defined by quartiles. NOTE: PLUS = Parent Loan for Undergraduate Students. Detail may not sum to totals because of rounding. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2008/12 Baccalaureate and Beyond Longitudinal Study (B&B:08/12)."}, {"section_title": "B12DOCCRC01", "text": "Job 1: recode flag for B12DOCC201, B12DOCC301, B12DOCC601 NOTE: Respondents entered a text string describing the job title and duties of the job held prior to the 2011-12 school year, then selected an occupation using the occupation coder built into the instrument. In some cases, respondents did not select an occupation in the coder. Survey contractor staff attempted to code these text strings and when they were able to do so, this flag was set to 1. Some occupations that were coded by respondents were randomly selected for a recoding review by survey contractor staff. In the event that the occupation was recoded to a new value, this flag was set to 1. If the flag was set to 1, the following variables B&B:08/12 Data File Documentation F-83 were changed to reflect the new code selected: B12DOCC201, B12DOCC301, B12DOCC601. When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. [{If B12DEMPCUR01 = 1} Is {else} Was] it... NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. What is the primary business or industry for [B12DEMPNAM01]? Please bear with me while I code this. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above.  "}, {"section_title": "B12DINDTX01", "text": "Job 1: industry text string NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above.  In which level of the education industry [{if B12DEMPCUR01 = 1} is {else} was] this job? NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 0 = Did not work more than 40 hours to earn more money 1 = Yes, worked more than 40 hours to earn more money Applies To: Respondents for whom employer 1 was selected for additional questions based on duration and recency of employment and (if needed) random selection and worked at employer 1  Responsibilities of your position demand more than 40 hours per week NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 0 = Did not work more than 40 hours due to job demands 1 = Yes, worked more than 40 hours due to job demands Other reason not listed. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 0 = Did not work more than 40 hours for other reasons 1 = Yes, worked more than 40 hours for other reasons Applies To: Respondents for whom employer 1 was selected for additional questions based on duration and recency of employment and (if needed) random selection and worked at employer 1 for In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 0 = Would not have preferred full-time employment 1 = Yes, would have preferred full-time employment Applies To: Respondents for whom employer 1 was selected for additional questions based on duration and recency of employment and (if needed) random selection and worked at part-time or fewer than 30  Working while attending school. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 0 = Did not work less than full-time to attend school 1 = Yes, worked less than full-time to attend school Applies To: Respondents for whom employer 1 "}, {"section_title": "B12DWYFR01", "text": "Job 1: why worked less than full-time: family responsibilities Job title: [job title at employer 1] Why did you work fewer than 35 hours per week? Family responsibilities. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 0 = Did not work less than full-time due to family 1 = Yes, worked less than full-time due to family Applies To: Respondents for whom employer 1 was selected for additional questions based on duration and recency of employment and (if needed) random selection, worked at employer 1 "}, {"section_title": "B12DWYNJA01", "text": "Job 1: why worked less than full-time: full-time job not available Job title: [job title at employer 1] Why did you work fewer than 35 hours per week? Full-time job not available. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. "}, {"section_title": "B12DWYMLJ01", "text": "Job 1: why worked less than full-time: held more than one job Job title: [job title at employer 1] Why did you work fewer than 35 hours per week? Held more than one job. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 0 = Not because held more than one job 1 = Yes, because held more than one job Applies To: Respondents for whom employer 1  Did not need or want to work more hours. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. Other. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers."}, {"section_title": "F-88", "text": "B&B:08/12 Data File Documentation In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. Overtime. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. Commission. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. ? NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. Did your duties in this job require a bachelor's degree or higher? NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. Was your license related to the work you did at your job? NOTE: This question was only asked as part of the first employment loop. Subsequent employment loops do not contain this item. When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. When you were employed, did you consider this job to be part of a career you were pursuing in your occupation or industry? NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 0 = Job is not the start of career 1 = Yes, job is the start of career In which of the following ways would you describe this job, since it [{if B12DEMPCUR01 = 1} is {else} was] not part of your career? Working to obtain job experience. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 0 = Not working to obtain job experience 1 = Yes, working to obtain job experience Applies To: Respondents for whom employer 1 was selected for additional questions based on duration and recency of employment and (if needed) random selection, whose job at employer 1 [NPSAS]. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. In which of the following ways would you describe this job, since it [{if B12DEMPCUR01 = 1} is {else} was] not part of your career? Working to receive benefits. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 0 = Not working to receive benefits 1 = Yes, working to receive benefits Applies To: Respondents for whom employer 1 was selected for additional questions based on duration and recency of employment and (if needed) random selection, whose job at employer 1  In which of the following ways would you describe this job, since it [{if B12DEMPCUR01 = 1} is {else} was] not part of your career? Working to pay the bills (e.g., best job available). NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 0 = Not working to pay the bills 1 = Yes, working to pay the bills Applies To: Respondents for whom employer 1 was selected for additional questions based on duration and recency of employment and (if needed) random selection, whose job at employer 1 was not part of their career path and who did not participate in the abbreviated interview. Where: [Job 1 selected for additional questions] and B12DCURL01 = 0 and ABBREV ne 1. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12DCUREDU01", "text": "Job 1: job description: providing experience for education Job title: [job title at employer 1] In which of the following ways would you describe this job, since it [{if B12DEMPCUR01 = 1} is {else} was] not part of your career? Working to prepare for future education. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since B&B:08/12 Data File Documentation F-93 completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 0 = Not working to prepare for future education 1 = Yes, working to prepare for future education Applies To: Respondents for whom employer 1 was selected for additional questions based on duration and recency of employment and (if needed) random selection, whose job at employer 1 was not part of their career path and who did not participate in the abbreviated interview. Where: [Job 1 selected for additional questions] and B12DCURL01 = 0 and ABBREV ne 1. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12DCURSCH01", "text": "Job 1: job description: job while in school Job title: [job title at employer 1] In which of the following ways would you describe this job, since it [{if B12DEMPCUR01 = 1} is {else} was] not part of your career? Job while in school. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. In which of the following ways would you describe this job, since it [{if B12DEMPCUR01 = 1} is {else} was] not part of your career? Job while pursuing other interests. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 0 = Not a job while pursuing other interests 1 = Yes, job while pursuing other interests Applies To: Respondents for whom employer 1 was selected for additional questions based on duration and recency of employment and (if needed) random selection, whose job at employer 1 was not part of their career path and who did not participate in the abbreviated interview. Where: [Job 1 selected for additional questions] and B12DCURL01 = 0 and ABBREV ne 1. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12DCURFUT01", "text": "Job 1: job description: work while exploring education/career options Job title: [job title at employer 1] In which of the following ways would you describe this job, since it [{if B12DEMPCUR01 = 1} is {else} was] not part of your career? Working while exploring future education and/or career options. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three Any other description. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 0 = Not another type of job 1 = Yes, other type of job Wanted better salary or benefits. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 0 = Did not want better salary/benefits 1 = Yes, wanted better salary/benefits Applies To: Respondents for whom employer 1 was selected for additional questions based on duration and recency of employment and (if needed) random selection, were not currently working for employer 1  Wanted a different job in the same or similar field. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 0 = Did not want a job in a similar field 1 = Yes, wanted a job in a similar field Applies To: Respondents for whom employer 1 was selected for additional questions based on duration and recency of employment and (if needed) random selection, were not currently working for employer 1  Wanted a job in a different field. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 0 = Did not want a job in a different field 1 = Yes, wanted a job in a different field Applies To: Respondents for whom employer 1 was selected for additional questions based on duration and recency of employment and (if needed) random selection, were not currently working for employer 1  Wanted different working conditions (such as work hours, commute, colleagues, etc.). NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 0 = Did not want different working conditions 1 = Yes, wanted different working conditions Applies To: Respondents for whom employer 1 was selected for additional questions based on duration and recency of employment and (if needed) random selection, were not currently working for employer 1  Wanted better opportunities for career advancement. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 0 = Did not want better opportunities for career adv 1 = Yes, wanted better opportunities for career adv Applies To: Respondents for whom employer 1 was selected for additional questions based on duration and recency of employment and (if needed) random selection, were not currently working for employer 1  completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. . NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 0 = Not because disliked job 1 = Yes, did not like job Applies To: Respondents for whom employer 1 was selected for additional questions based on duration and recency of employment and (if needed) random selection, were not currently working for employer 1  Position was temporary or seasonal. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 0 = Not because position was temporary or seasonal 1 = Yes, because position was temporary or seasonal Applies To: Respondents for whom employer 1 was selected for additional questions based on duration and recency of employment and (if needed) random selection, were not currently working for employer 1 and who did not participate in the abbreviated interview. Where: [Job 1 selected for additional questions] and B12DEMPCUR01 = 0 and ABBREV ne 1. Recode note: If B12DEMPCUR01 = 0 and B12DBKTMP01 = 1 then B12DCGTP01 = 1. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12DCGTM01", "text": "Job 1: why no longer working for emp: laid off/terminated Why are you no longer employed by [B12DEMPNAM01]? Laid off, terminated, or contract not renewed. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible B&B:08/12 Data File Documentation F-97 employers, then one was selected based on the criteria discussed above. 0 = Not because laid off or terminated 1 = Yes, because laid off or terminated Applies To: Respondents for whom employer was selected for additional questions based on duration and recency of employment and (if needed) random selection, were not currently working for employer 1  Enrolled in school. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 0 = Not because enrolled in school 1 = Yes, because enrolled in school Applies To: Respondents for whom employer 1 was selected for additional questions based on duration and recency of employment and (if needed) random selection, were not currently working for employer 1  Relocated to another area. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. Care for children. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 0 = Did not leave to care for children 1 = Yes, left to care for children Applies To: Respondents for whom employer 1 was selected for additional questions based on duration and recency of employment and (if needed) random selection, were not currently working for employer 1  Health reasons. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. Other reason(s). NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 0 = Did not leave because of other reasons 1 = Yes, left because of other reasons In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 1 = Wanted better salary or benefits 2 = Wanted a different job in the same/similar field 3 = Wanted a job in a different field 4 = Wanted different working conditions 5 = Wanted better opportunities for career advancement 6 = Wanted better job security 7 = Did not like job at [B12DEMPNAM01] 8 = Position was temporary or seasonal 9 = Laid off, terminated, or contract not renewed 10 = Enrolled in school 11 = Relocated to another area 12 = Care for children 13 = Health reasons 14 = Other reason(s) Applies To: Respondents for whom employer 1 was selected for additional questions based on duration and recency of employment and (if needed) random selection, were not currently working for employer 1 and who did not participate in the abbreviated interview. Where: [Job 1 selected for additional questions] and B12DEMPCUR01 = 0 and ABBREV ne 1. Recode note: If B12DCGSL01 = 1 then B12DSINGLE01 = 1; If B12DCGSF01 = 1 then B12DSINGLE01 = 2; If B12DCGDF01 = 1 then B12DSINGLE01 = 3; If B12DCGWC01 = 1 then B12DSINGLE01 = 4; If B12DCGCA01 = 1 then B&B:08/12 Data File Documentation F-99 B12DSINGLE01 = 5; If B12DCGJS01 = then B12DSINGLE01 = 6; If B12DCGDL01 = then B12DSINGLE01 = 7; If B12DCGTP01 = then B12DSINGLE01 = 8; If B12DCGTM01 = 1 then B12DSINGLE01 = 9; If B12DCGES01 = then B12DSINGLE01 = 10; If B12DCGRA01 = 1 then B12DSINGLE01 = 11; If B12DCGCC01 = 1 then B12DSINGLE01 = 12; If B12DCGHR01 = 1 then B12DSINGLE01 = 13; If B12DCGOT01 = 1 then B12DSINGLE01 = 14. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12DPAY01", "text": "Job 1: level of satisfaction: wages and bonuses Job title: [job title at employer 1] Indicate your level of satisfaction, from very dissatisfied to very satisfied, with each of the following areas of this job: Wages and bonuses. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 1 = Very dissatisfied 2 = Dissatisfied 3 = Neither satisfied nor dissatisfied 4 = Satisfied 5 = Very satisfied Applies To: Respondents for whom employer 1 was selected for additional questions based on duration and recency of employment and (if needed) random selection and who did not participate in the abbreviated interview. Where: [Job 1 selected for additional questions] and ABBREV ne 1. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12DBEN01", "text": "Job 1: level of satisfaction: benefits Job title: [job title at employer 1] Indicate your level of satisfaction, from very dissatisfied to very satisfied, with each of the following areas of this job: Benefits. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above.  Indicate your level of satisfaction, from very dissatisfied to very satisfied, with each of the following areas of this job: Importance of your work. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. 1 = Very dissatisfied 2 = Dissatisfied 3 = Neither satisfied nor dissatisfied 4 = Satisfied 5 = Very satisfied Applies To: Respondents for whom employer 1 was selected for additional questions based on duration and recency of employment and (if needed) random selection and who did not participate in the abbreviated interview. Where: [Job 1 selected for additional questions] and ABBREV ne 1. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12DSEC01", "text": "Job 1: level of satisfaction: job security Job title: [job title at employer 1] Indicate your level of satisfaction, from very dissatisfied to very satisfied, with each of the following areas of this job: Job security. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above. Indicate your level of satisfaction, from very dissatisfied to very satisfied, with each of the following areas of this job: Ability to balance work and family obligations. NOTE: When going through the B&B:08/12 Full-scale instrument, respondents were able to enter basic information about all employers they had since completing their Bachelor's degree requirements in 2007-08. In the full interview, respondents were asked additional questions for up to three of these employers. In the abbreviated interview, respondents were asked additional questions for one of these employers. These additional questions only applied to employers for which the respondent had worked more than three consecutive months. If the full interview respondent had four or more eligible employers three were selected based on recency, duration of employment, and (if necessary) random assignment. If the abbreviated interview respondent had two or more eligible employers, then one was selected based on the criteria discussed above.  "}, {"section_title": "B12DWRKS", "text": "Primarily student or employee while enrolled Since you are currently enrolled as a student and also working, would you say you are primarily... 1 = A student working to meet expenses, or 2 = An employee who decided to enroll in school  4 5 7and B12ESMSC01 in (1 3)   Recode note: If B12EJBVER01 in (1 2) and B12EJBTP01 = 2 and B12DEMPST01 > 0 then B12ESTE01 = B12DEMPST01; else if B12EJBVER01 in (3 4) and B12EJBTP01 = 2 and B12DEMPST02 > 0 then B12ESTE01 = B12DEMPST02; else if B12EJBVER01 in (5 6) "}, {"section_title": "B12ECODUP01", "text": "K-12 job 1: school coded after interview Respondents entered text strings for school name, city and state, then attempted to select the corresponding school using the elementary/secondary school coder built into the instrument. In some cases, respondents did not select a school in the coder. Survey contractor staff attempted to code these schools and when they were able to do so, this flag was set to 1.  1 2) and B12EJBTP01 not in 4 5 7and ABBREV ne 1. This question was asked in two parts: respondents were asked to provide a salary dollar amount (B12EJBSAMT01) and then indicate the time frame for that salary (B12EJBSTIM01). Therefore, this variable includes salary amounts reported in hourly, weekly, monthly, and yearly amounts. These data must be combined with the accompanying time frame data for this job in order to be analyzed. Applies To: Respondents whose first reported K-12 teaching position in the B&B:08/12 interview was not as a teacher's aide, short-term substitute, or student teacher; and who did not participate in the abbreviated interview. Where: ([Selected a teaching occupation code for a current or former position] or B12EEVRTCH in (1 2)) and B12EJBTP01 not in 4 5 7and ABBREV ne 1. Recode note: If B12EJBVER01 in (1 2) and B12EJBTP01 not in (4 5 7) then B12EJBSAMT01 = B12DEMPAMT01; else if B12EJBVER01 in (3 4) and B12EJBTP01 not in (4 5 7) then B12EJBSAMT01 = B12DEMPAMT02; else if B12EJBVER01 in (5 6) and B12EJBTP01 not in 4 5 7then B12EJBSAMT01 = B12DEMPAMT03...else B12EJBVER01 in (19 20) and B12EJBTP01 not in 4 5 7 4 5 7and ABBREV ne 1. Recode note: If B12EJBVER01 in (1 2) and B12EJBTP01 not in 4 5 7then B12EJBSTIM01 = B12DEMPTIM01; else if B12EJBVER01 in 3 4and B12EJBTP01 not in 4 5 7then B12EJBSTIM01 = B12DEMPTIM02; else if B12EJBVER01 in (5 6) and B12EJBTP01 not in 4 5 7then B12EJBSTIM01 = B12DEMPTIM03...else B12EJBVER01 in 19 20and B12EJBTP01 not in 4 5 7 This question was asked in two parts: respondents were asked to provide a salary dollar amount (B12EJBEAMT01) and then indicate the time frame for that salary (B12EJBETIM01). Therefore, this variable includes salary amounts reported in hourly, weekly, monthly, and yearly amounts. These data must be combined with the accompanying time frame data for this job in order to be analyzed.  4 5 7and ABBREV ne 1. Recode note: If B12EJBVER01 in (1 2) and B12EJBTP01 not in 4 5 7then B12EJBEAMT01 = B12DEMPAMT201; else if B12EJBVER01 in (3 4) and B12EJBTP01 not in 4 5 7then B12EJBEAMT01 = B12DEMPAMT202; else if B12EJBVER01 in (5 6) 1 2) and B12EJBTP01 not in 4 5 7and ABBREV ne 1. Recode note: If B12EJBVER01 in (1 2) and B12EJBTP01 not in 4 5 7then B12EJBETIM01 = B12DEMPTIM201; else if B12EJBVER01 in 3 4and B12EJBTP01 not in 4 5 7then B12EJBETIM01 = B12DEMPTIM202; else if B12EJBVER01 in 5 6and B12EJBTP01 not in 4 5 7then B12EJBETIM01 = B12DEMPTIM203...else B12EJBVER01 in 19 20and B12EJBTP01 not in 4 5 7 4 5 7and [not self-employed in the associated occupation] and ABBREV ne 1. Recode note: If B12EJBVER01 in (1 2) and B12EJBTP01 not in 4 5 7then B12EBNGT01 = B12DBENANY01; else if B12EJBVER01 in 3 4 1 2) and B12EJBTP01 not in 4 5 7and ABBREV ne 1 1 2) and B12EJBTP01 not in 4 5 7and ABBREV ne 1  In your most recent teaching position, were you satisfied with each of the following... Your relationships with supervisors? 0 = Not satisfied with relationships with supervisors 1 = Yes, satisfied with relationships with supervisors Applies To: Respondents who had held a K-12 teaching position at any time since completing their bachelor's degree requirements; did not exclusively report K-12 teaching positions in the B&B:08/12 interview that were as a teacher's aide, short-term substitute, or student teacher; and who did not participate in the abbreviated interview. Where: ([Selected a teaching occupation code for a current or former position] or B12EEVRTCH in (12) or [identified as a teacher in B&B:08/09 student interview]) and [no iteration where B12EJBTP** in 4 5 7] and ABBREV ne 1. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12ETCHEFF", "text": "Teacher satisfaction: effectiveness as a teacher [If current teacher] In your current teaching position, are you satisfied with each of the following..."}, {"section_title": "[else]", "text": "In your most recent teaching position, were you satisfied with each of the following... Your effectiveness as a teacher? 0 = Not satisfied with effectiveness as a teacher 1 = Yes, satisfied with effectiveness as a teacher Applies To: Respondents who had held a K-12 teaching position at any time since completing their bachelor's degree requirements; did not exclusively report K-12 teaching positions in the B&B:08/12 interview that were as a teacher's aide, short-term substitute, or student teacher; and who did not participate in the abbreviated interview. Where: ([Selected a teaching occupation code for a current or former position] or B12EEVRTCH in (1 2) 4 5 7] and ABBREV ne 1. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12ELNFRGV", "text": "Aware of teacher loan forgiveness programs Are you aware of loan forgiveness programs which allow you to cancel all or part of your student loans in return for service to the community through teaching? 0 = Not aware of loan forgiveness programs 1 = Yes, aware of loan forgiveness programs Applies To: Respondents who had held a K-12 teaching position at any time since completing their bachelor's degree requirements; did not exclusively report K-12 4 5 7] and B12ELNFRGV = 1 and ABBREV ne 1. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12EPLNTCH", "text": "Plan to teach K-12 at some time in the future Do you plan to teach in a K-12 classroom at some time in the future? 0 = Did not plan to teach in the future 1 = Yes, planned to teach in the future Applies To: Respondents who had held a K-12 teaching position at any time since completing their bachelor's degree requirements but did not currently hold a teaching position or had prepared for teaching or considered teaching, who did not currently hold a teaching position, and who did not participate in the abbreviated interview. Where: [Former teacher  We would like to know when your child became financially dependent upon you. If he or she became dependent upon you at a time other than his or her birth (through adoption, foster care, etc.) please indicate the month and year he or she became your dependent. [else] For each dependent child, we would like to know when he or she became financially dependent upon you. If he or she became dependent upon you at a time other than his or her birth (through adoption, foster care, etc.) please indicate the month and year he or she became your dependent. B12FDPDATMY1 is presented in YYYYMM format. "}, {"section_title": "B12FDPDATMY2", "text": "Date of dependency: dependent child 2 For each dependent child, we would like to know when he or she became financially dependent upon you. If he or she became dependent upon you at a time other than his or her birth (through adoption, foster care, etc.) please indicate the month and year he or she became your dependent. B12FDPDATMY2 is presented in YYYYMM format. "}, {"section_title": "B12FDPDATMY4", "text": "Date of dependency: dependent child 4 For each dependent child, we would like to know when he or she became financially dependent upon you. If he or she became dependent upon you at a time other than his or her birth (through adoption, foster care, etc.) please indicate the month and year he or she became your dependent. B12FDPDATMY4 is presented in YYYYMM format. "}, {"section_title": "B12FDPDATMY7", "text": "Date of dependency: dependent child 7 For each dependent child, we would like to know when he or she became financially dependent upon you. If he or she became dependent upon you at a time other than his or her birth (through adoption, foster care, etc.) please indicate the month and year he or she became your dependent. B12FDPDATMY7 is presented in YYYYMM format. "}, {"section_title": "B12FOTDEPMY1", "text": ""}, {"section_title": "Date of dependency of other dependent 1", "text": "In what month and year did you begin providing financial support or did you become the primary caregiver to your other dependent(s)? B12FOTDEPMY1 is presented in YYYYMM format. Month values of 00 indicate a missing month. Applies To: Respondents who financially supported other dependents and did not participate in the abbreviated interview. Where: B12FOTHER = 1 and ABBREV ne 1. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12FOTDEPMY2", "text": ""}, {"section_title": "Date of dependency of other dependent 2", "text": "In what month and year did you begin providing financial support or did you become the primary caregiver to your other dependent(s)? B12FOTDEPMY2 is presented in YYYYMM format. Month values of 00 indicate a missing month. Applies To: Respondents who financially supported at least two other dependents and did not participate in the abbreviated interview. Where: B12FOTDEPMY2 > 0 and ABBREV ne 1. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12FOTDEPMY3", "text": ""}, {"section_title": "Date of dependency of other dependent 3", "text": "In what month and year did you begin providing financial support or did you become the primary caregiver to your other dependent(s)? B12FOTDEPMY3 is presented in YYYYMM format. "}, {"section_title": "B12FHOMVAL", "text": ""}, {"section_title": "Current value of primary residence", "text": "What is the approximate current value of your home(s)? (If you do not know the exact amount, please provide your best guess.) Values between $0 and $1,000 or greater than $1,500,000 were replaced with a -6 to indicate the value was out of range. "}, {"section_title": "B12FINCOM", "text": "Respondent's income in 2011 What was your income for calendar year 2011, prior to taxes and deductions? (Calendar year 2011 includes January 1, 2011 through December 31, 2011. Include all income you paid taxes on, including work, investment income, or alimony. Do not include [{if B12AMARR = 2} your spouse's income, {else if B12AFINWHO = 1} partner's income,] any grants or loans you may have used to pay for school, or any money given to you by your family.) (If you are unsure of the exact amount, provide your best estimate.) Values between $0 and $100 or greater than $1,000,000 were replaced with a -6 to indicate the value was out of range. Applies To: Respondents who did not participate in the abbreviated interview. Where: ABBREV ne 1. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12FINEST", "text": "Respondent's income range in 2011 [If B12AMARR = 2] This question about your income is critical to understanding the financial benefits and labor market outcomes of people who have recently earned a bachelor's degree. Excluding your spouse's income, please indicate the range that best estimates your income from all sources (including income from work, investments, alimony, etc.), prior to taxes and deductions, for calendar year 2011 (January 1, 2011 through December 31, 2011). [else if B12AFINWHO = 1] This question about your income is critical to understanding the financial benefits and labor market outcomes of people who have recently earned a bachelor's degree. Excluding your domestic partner's income, please indicate the range that best estimates your income from all sources (including income from work, investments, alimony, etc.), prior to taxes and deductions, for calendar year 2011 (January 1, 2011 through December 31, 2011). [else] This question about your income is critical to understanding the financial benefits and labor market outcomes of people who have recently earned a bachelor's degree. Please indicate the range that best estimates your income from all sources (including income from work, investments, alimony, etc.) prior to taxes and deductions for calendar year 2011 (January 1, 2011 through December 31, 2011). 1 = Less than $20,000 2 = $20,000-$29,999 3 = $30,000-$39,999 4 = $40,000-$49,999 5 = $50,000-$59,999 6 = $60,000-$69,999 7 = $70,000-$79,999 8 = $80,000-$89,999 9 = $90,000-$99,999 10 = $100,000-$149,999 11= $150,000 or more Applies To: Respondents who did not provide an income and did not participate in the abbreviated interview, or respondents who participated in the abbreviated interview. Where:"}, {"section_title": "B12FSPEMP", "text": "Spouse worked for pay in 2011 [If B12AFINWHO = 1] Did your partner work for pay in calendar year 2011 (January 1, 2011 through December 31, 2011)? [else] Did your spouse work for pay in calendar year 2011 (January 1, 2011 through December 31, 2011)? 0 = Spouse did not work for pay in 2011 1 = Yes, spouse worked for pay in 2011 Applies To: Respondents who were married or had a domestic partner and did not participate in the abbreviated interview. Where: (B12AMARR = 2 or B12AFINWHO = 1) and ABBREV ne 1. Recode note: If B12FINCSP = 0 then B12FSPEMP = 0. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12FINCSP", "text": "Spouse's income in 2011 [If B12AFINWHO = 1] What was your partner's income for calendar year 2011, prior to taxes and deductions? (Calendar year 2011 includes January 1, 2011 through December 31, 2011. Include all income your partner paid taxes on, including work, investment income, or alimony. Do not include any grants or loans your partner may have used to pay for school, or any money given to your spouse by family.) [else] What was your spouse's income for calendar year 2011, prior to taxes and deductions? (Calendar year 2011 includes January 1, 2011 through December 31, 2011. Include all income your spouse paid taxes on, including work, investment income, or alimony. Do not include any grants or loans your spouse may have used to pay for school, or any money given to your spouse by family.) Values between $0 and $100 or greater than $1,000,000 were replaced with a -6 to indicate the Applies To: Respondents who were married or had a domestic partner and did not participate in the abbreviated interview. Where: (B12AMARR = 2 or B12AFINWHO = 1) and ABBREV ne 1. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12FSPNOT", "text": "Not married to spouse in 2011 [If B12AFINWHO = 1] What was your partner's income for calendar year 2011, prior to taxes and deductions? (Calendar year 2011 includes January 1, 2011 through December 31, 2011. Include all income your partner paid taxes on, including work, investment income, or alimony. Do not include any grants or loans your partner may have used to pay for school, or any money given to your spouse by family.) [else] What was your spouse's income for calendar year 2011, prior to taxes and deductions? (Calendar year 2011 includes January 1, 2011 through December 31, 2011. Include all income your spouse paid taxes on, including work, investment income, or alimony. Do not include any grants or loans your spouse may have used to pay for school, or any money given to your spouse by family.) Check here if you were not living with your [{if B12AFINWHO = 1} partner {else} spouse] in 2011 0 = Living with/married to spouse in 2011 1 = Not living with/married to spouse in 2011 Applies To: Respondents who were married or had a domestic partner and did not participate in the abbreviated interview. Where: (B12AMARR = 2 or B12AFINWHO = 1) and ABBREV ne 1. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12FINSRA", "text": "Spouse's income range in 2011 [If B12AFINWHO = 1] This question about your partner's income is critical to understanding the financial benefits and labor market outcomes of people who have recently earned a bachelor's degree. Please indicate the range that best estimates your partner's income from all sources (including income from work, investments, alimony, etc.), prior to taxes and deductions, in calendar year 2011 (January 1, 2011 through December 31, 2011). [else] This question about your spouse's income is critical to understanding the financial benefits and labor market outcomes of people who have recently earned a bachelor's degree. Please indicate the range that best estimates your spouse's income from all sources (including income from work, investments, alimony, etc.), prior to taxes and deductions, in calendar year 2011 (January 1, 2011 through December 31, 2011). 1 = Less than $20,000 2 = $20,000-$29,999 3 = $30,000-$39,999 4 = $40,000-$49,999 5 = $50,000-$59,999 6 = $60,000-$69,999 7 = $70,000-$79,999 8 = $80,000-$89,999 9 = $90,000-$99,999 10 = $100,000-$149,999 11 = $150,000 or more Applies To: Respondents who were married or had a domestic partner, did not provide an income for their spouse or partner or who were not married or living with their spouse of partner in B&B:08/12 Data File Documentation F-153 2011, and did not participate in the abbreviated interview. Where: (B12AMARR = 2 or B12AFINWHO = 1) and   Did your spouse attend college or graduate school during the 2012-13 school year? 0 = Not in college or graduate school 1 = Yes, full time in college or graduate school 2 = Yes, part time in college or graduate school Applies To: Respondents whose spouse or domestic partner had attended a college, university, or trade school and who did not participate in the abbreviated interview. Where: (B12AMARR = 2 or B12AFINWHO = 1) and B12FSPLV not in (1 2) and ABBREV ne 1. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12FSPLN", "text": "Spouse took out student loans [If B12AFINWHO = 1 and B12FSPLV in (2 3 4 5 6)] Did your partner ever take out any student loans for his or her undergraduate education? [else if B12AFINWHO = 1] Did your partner ever take out any student loans for his or her undergraduate and/or graduate education? [else if B12FSPLV in 2 3 4 5 6] Did your spouse ever take out any student loans for his or her undergraduate education? [else] Did your spouse ever take out any student loans for his or her undergraduate and/or graduate education? 0 = Spouse did not take out student loans 1 = Yes, spouse took out student loans Applies To: Respondents whose spouse or domestic partner had attended a college, university, or trade school and who did not participate in the abbreviated interview. Where: (B12AMARR = 2 or B12AFINWHO = 1) and B12FSPLV not in (1 2) and ABBREV ne 1. Recode note: If B12FSPAMT = 0 then B12FSPLN = 0. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12FSPAMT", "text": "Spouse's student loans: amount borrowed [If B12AFINWHO = 1] What is the total amount your partner has borrowed in student loans?(If you are unsure of the amount, please provide your best estimate.) [else] What is the total amount your spouse has borrowed in student loans?(If you are unsure of the amount, please provide your best estimate.) Applies To: Respondents whose spouse or domestic partner had attended a college, university, or trade school and took out student loans and who did not participate in the abbreviated interview. Where: (B12AMARR = 2 or B12AFINWHO = 1) and B12FSPLV not in (1 2) and B12FSPLN = 1 and ABBREV ne 1. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12FSPOWE", "text": "Spouse's student loans: amount owed [If B12FSPAMT = -9 and B12AFINWHO = 1] How much of your partner's student loans are still owed? [else if B12FSPAMT ne -9 and B12AFINWHO = 1] How much of the $[B12FSPAMT] in total student loans does your partner still owe? [else if B12FSPAMT = -9] How much of your spouse's student loans are still owed? [else] How much of the $[B12FSPAMT] in total student loans does your spouse still owe? 1 = All loans still owed 2 = Some loans still owed 3 = No loans owed Applies To: Respondents whose spouse or domestic partner had attended a college, university, or trade school and took out student loans and who did not participate in the abbreviated interview. Where: (B12AMARR = 2 or B12AFINWHO = 1) and B12FSPLV not in (1 2)  How much does your spouse pay each month for his or her student loans? Not yet in repayment 0 = Spouse loans in repayment 1 = Spouse loans not in repayment Applies To: Respondents whose spouse or domestic partner had attended a college, university, or trade school and had not paid off their student loans and who did not participate in the abbreviated interview. Where: (B12AMARR = 2 or B12AFINWHO = 1) and B12FSPLV not in (1 2) and B12FSPLN = 1 and B12FSPOWE not in (-9 3) and ABBREV ne 1. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12FAFFWKMR", "text": "Result of financial cost of education: worked more than desired All students experience some financial costs as a result of their undergraduate [{if B12CDEG* in (5 6 7 8 9)} and graduate] education, whether they take out loans, pay for their education in other ways, or spend time on coursework that could have been spent working for pay. As a result of your financial costs for undergraduate [{if B12CDEG* in (5 6 7 8 9)} and graduate] education, have you... Had to work more than desired (e.g., work more than one job or work more hours)? 0 = Did not work more than desired 1 = Yes, worked more than desired Applies To: Respondents who did not indicate they did not have paid employment since completing their bachelor's degree requirements. Where: B12DANYJOBS ne 0. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12FAFFLESS", "text": "Result of financial cost of education: took job outside of field All students experience some financial costs as a result of their undergraduate [{if B12CDEG* in (5 6 7 8 9)} and graduate] education, whether they take out loans, pay for their education in other ways, or spend time on coursework that could have been spent working for pay. As a result of your financial costs for undergraduate [{if B12CDEG* in 5 6 7 8 9 "}, {"section_title": "B12FAFFEDJB", "text": "Result of financial cost of education: took job instead of enrolling All students experience some financial costs as a result of their undergraduate [{if B12CDEG* in 5 6 7 8 9} and graduate] education, whether they take out loans, pay for their education in other ways, or spend time on coursework that could have been spent working for pay. As a result of your financial costs for undergraduate [{if B12CDEG* in 5 6 7 8 9 "}, {"section_title": "B12FAFFHOME", "text": "Result of financial cost of education: delayed buying a home All students experience some financial costs as a result of their undergraduate [{if B12CDEG* in (5 6 7 8 9)} and graduate] education, whether they take out loans, pay for their education in other ways, or spend time on coursework that could have been spent working for pay. As a result of your financial costs for undergraduate [{if B12CDEG* in ( "}, {"section_title": "B12FAFFMARR", "text": "Result of financial cost of education: delayed getting married All students experience some financial costs as a result of their undergraduate [{if B12CDEG* in (5 6 7 8 9)} and graduate] education, whether they take out loans, pay for their education in other ways, or spend time on coursework that could have been spent working for pay. As a result of your financial costs for undergraduate [{if B12CDEG* in 5 6 7 8 9 "}, {"section_title": "B12FAFFCHLD", "text": "Result of financial cost of education: delayed having children All students experience some financial costs as a result of their undergraduate [{if B12CDEG* in 5 6 7 8 9} and graduate] education, whether they take out loans, pay for their education in other ways, or spend time on coursework that could have been spent working for pay. As a result of your financial costs for undergraduate [{if B12CDEG* in 5 6 7 8 9 "}, {"section_title": "B12FWORTH", "text": "Undergraduate education was worth the financial cost Do you think your undergraduate education was worth its financial cost? 0 = Undergraduate education not worth its cost 1 = Yes, undergraduate education worth its cost Applies To: All respondents. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12FWORTHG", "text": "Graduate education was worth the financial cost Do you think your graduate education was worth its financial cost? 0 = Graduate education not worth its cost 1 = Yes, graduate education worth its cost Applies To: Respondents who had a graduate degree. Where: B12CDEG01 in (5 6 7 8 9) or B12CDEG02 in (5 6 7 8 9) or B12CDEG03 in (5 6 7 8 9) or B12CDEG04 in (5 6 7 8 9) or B12CDEG05 in (5 6 7 8 9) or B12CDEG06 in (5 6 7 8 9) or B12CDEG07 in (5 6 7 8 9). Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12FSELLPO", "text": "Result of the sale of all major possessions [If B12FMORTG = 1 or B12FOWNHM = 1] Suppose you [{if B12AMARR = 2} and your spouse {else if B12AFINWHO = 1} and your partner] were to sell all your major possessions, including your home, turn all of your investments and other assets into cash, and pay off all your debts. Do you think you would have something left over, break even, or be in debt? [else] Suppose you [{if B12AMARR = 2} and your spouse {else if B12AFINWHO = 1} and your partner] were to sell all your major possessions, turn all of your investments and other assets into cash, and pay off all your debts. Do you think you would have something left over, break even, or be in debt? 1 = Have something left over 2 = Break even 3 = Be in debt Applies To: Respondents who did not participate in the abbreviated interview. Where: ABBREV ne 1. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12FSTRESS", "text": "Financial difficulty in past 12 months During the past 12 months, has there been a time when you did not meet all of your essential expenses, such as mortgage or rent payments, utility bills, or important medical care? 0 = Met all essential expenses 1 = Yes, did not meet all essential expenses Applies To: Respondents who did not participate in the abbreviated interview. Where: ABBREV ne 1. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12FMOMED", "text": "Mother's level of education What is the highest level of education your mother (or female guardian) completed? 1 = Did not complete high school 2 = High school diploma or equivalent 3 = Vocational or technical training 4 = Less than 2 years of college 5 = Associate's degree 6 = 2 or more years of college but no degree 7 = Bachelor's degree 8 = Master's degree or equivalent 9 = Professional degree 10 = Doctoral degree (PhD, EdD, etc.) 11 = Don't know "}, {"section_title": "B12FCOMSRV", "text": "Performed community service or volunteered in last year Have you performed any community service or volunteer work in the last 12 months? Please do not include paid community service, court-ordered service, or charitable donations (such as food, clothing, money, etc.). 0 = Did not perform community service 1 = Yes, performed community service Applies To: Respondents who did not participate in the abbreviated interview. Where: ABBREV ne 1. Recode note: If B12FVLHRS = 0 then B12FCOMSRV = 0. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12FVLHRS", "text": "Hours volunteered: amount About how many hours did you volunteer during the last year? Values greater than 2,080 when B12FVLAMT = 1, greater than 160 when B12FVLAMT = 2, or greater than 40 when B12FVLAMT = 3, were replaced with a -6 to indicate the value was out of range. Each calculation was based on the respondent reporting more than 2,080 hours per year. Applies To: Respondents who volunteered in the last 12 months and did not participate in the abbreviated interview. Where: B12FCOMSRV = 1 and ABBREV ne 1. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12FVLAMT", "text": "Hours volunteered: time frame About how many hours did you volunteer during the last year? 1 = Per year 2 = Per month 3 = Per week Applies To: Respondents who volunteered in the last 12 months for something other than a one-time event and did not participate in the abbreviated interview. Where: B12FCOMSRV = 1 and B12FVLONE ne 1 and ABBREV ne 1. Source: B&B:08/12 Full-scale Student Interview"}, {"section_title": "B12FVLONE", "text": "One time volunteer event About how many hours did you volunteer during the last year? One time event 0 = Not a one-time volunteer event 1 = Yes, one time volunteer event  "}, {"section_title": "Y_NPLEVL", "text": "NPSAS level 1 = 4-year 2 = 2-year 3 = Less-than-2-year Applies To: All respondents. "}, {"section_title": "B12AREQ", "text": "Completed bachelor's requirements b/w July 1, 2007 andJune 30, 2008 Did you complete the requirements for your bachelor's degree while you were enrolled at [NPSAS]   What is the name of that school, and in what city and state is it located? Please bear with me as I code this -IT SHOULD JUST TAKE A SECOND. City. NOTE: City where the institution is located. If the institution is successfully coded using the IPEDS coder in the B&B:08/12 student interview, this information comes from IPEDS; otherwise, this information is provided by the respondent. What is the name of that school, and in what city and state is it located? Please bear with me as I code this -IT SHOULD JUST TAKE A SECOND. State."}, {"section_title": "F-166", "text": "B&B:08/12 Data File Documentation NOTE: A numeric code for the state in which the institution is located. If the institution is successfully coded using the IPEDS coder in the B&B:08/12 student interview, this information comes from IPEDS; otherwise, this information is provided by the respondent. "}, {"section_title": "B12DBKRES01", "text": "Employer grid 1: employment break: resigned Based on the dates you provided, it appears that there was a break in your employment with [employer 1] (i.e., it was not one continuous period). Why were you not employed during the time you indicated? Resigned or left [B12DEMPNAM01] NOTE: This variable was one of a series of items presented to respondents in a loop in the B&B:12 Fullscale interview. For each employer, beginning with the first employer respondents reported working for after receiving their bachelor's degrees (including jobs continued since before the bachelor's degree), respondents were asked to report basic information for that employer. The variables included in this loop are: (where xx = loop iteration) B12DWKyymmxx (where yy = year and mm = month), B12DEMPSMYxx,B12DWKPRIxx,B12DEMPCURxx,B12DBKTMPxx,B12DBKRESxx,B12DBKLEVxx,B12DBKOTHxx B12DEMPAMTxx,B12DEMPAT2xx B12DEMPTIMxx,B12DEMPTM2xx,B12DEMPHRSxx,B12DEMPHR2xx,B12DEMPFPTxx,and B12DEMPFT2xx. 0 "}, {"section_title": "Appendix J. Nonresponse Bias Analysis", "text": "B&B:08/12 Data File Documentation J-15   1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight adjusted for subsampling) and the mean of all sample cases (using the base weight adjusted for subsampling). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the full sample. 3 Bias in the sample mean is estimated as the product of the base weighted nonresponse rate and the difference between the mean of respondent cases (using base weight adjusted for subsampling) and the mean of nonrespondent cases (using the base weight adjusted for subsampling). 4 Base weight, adjusted for subsampling and nonresponse. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington.   1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight adjusted for subsampling) and the mean of all sample cases (using the base weight adjusted for subsampling). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the full sample. 3 Bias in the sample mean is estimated as the product of the base weighted nonresponse rate and the difference between the mean of respondent cases (using base weight adjusted for subsampling) and the mean of nonrespondent cases (using the base weight adjusted for subsampling). 4 Base weight, adjusted for subsampling and nonresponse. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington.    1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight adjusted for subsampling) and the mean of all sample cases (using the base weight adjusted for subsampling). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the full sample. 3 Bias in the sample mean is estimated as the product of the base weighted nonresponse rate and the difference between the mean of respondent cases (using base weight adjusted for subsampling) and the mean of nonrespondent cases (using the base weight adjusted for subsampling). 4 Base weight, adjusted for subsampling and nonresponse. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. 7 1 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using base weight adjusted for subsampling) and the mean of all sample cases (using the base weight adjusted for subsampling). 2 Relative bias is defined as the ratio of estimated bias to the weighted mean of the full sample. 3 Bias in the sample mean is estimated as the product of the base weighted nonresponse rate and the difference between the mean of respondent cases (using base weight adjusted for subsampling) and the mean of nonrespondent cases (using the base weight adjusted for subsampling). 4 Base weight, adjusted for subsampling and nonresponse. 5 Bias in the sample mean is estimated as the difference between the mean of respondent cases (using the specified weight) and the mean of all sample cases (using the specified weight). 6 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington.   2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 B&B:08/12 Data File Documentation J-39  2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington.   2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington.   2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 B&B:08/12 Data File Documentation J-47  2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington.   2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 B&B:08/12 Data File Documentation J-51  2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington.   2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 B&B:08/12 Data File Documentation J-55  2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington.   2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 B&B:08/12 Data File Documentation J-57  2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington.   2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 B&B:08/12 Data File Documentation J-59  2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington.   2Respondents, adjusted for nonresponse and poststratified 3Mean 1-Mean 3Mean 2 2 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington.  B&B:08/12 Data File Documentation J-63 "}]