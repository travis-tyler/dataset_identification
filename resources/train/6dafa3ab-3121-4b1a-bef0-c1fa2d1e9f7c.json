[{"section_title": "", "text": "However, ascertaining the quality of education is not an easy task, particularly because quality education can be understood differently by different stakeholders (Fitz-Gibbon, 1996) . When asked to describe quality education, many would use their terms, rather than standardised descriptions. Therefore, although the definition of quality education varies, it includes aspects of excellence, fitness for use, conforming to requirements, or meeting expectations. In South Africa, when analysing and understanding student performance, the inclusion of contextual factors is of particular importance, given the apartheid past and the disparities that exist in schooling (Scherman, 2007) . The poor performance of South African students in international assessments underscores the importance of contextual data (Howie, 2002) . South Africa students have underperformed in international comparative assessments such as the Trends in International Mathematics and Science Study (TIMSS) studies in 2015, 2003, 1999, and 1995 ; and the Progress in International Reading Literacy Study (PIRLS) in 2006 (PIRLS) in , 2011 (PIRLS) in , and 2015 ; in addition to the Southern and Eastern Africa Consortium for Monitoring Educational Quality (SACMEQ) studies, where students performed below the international average and below many countries. Likewise, students performed well below expectation in national assessment programmes such as the Systemic Evaluation in Grades 3 and 6, in addition to the Annual National Assessment (ANAs)-both programmes that are now discontinued as the Government works on developing a new monitoring system. These disappointing results could be due to students being ill prepared (for some reasons) regarding the content areas and, therefore, being unable to achieve the expected assessment standards. Although the concept of quality may take on different meanings depending on the context, for the purposes of this article, quality is viewed as the extent to which education systems can achieve the identified goals of education as embodied by knowledge and skill development (Chapman, Weidman, Cohen, & Mercer, 2005; Scheerens, Glas, & Thomas, 2003) .\nThe present research involved the use of an adapted monitoring system developed in the United Kingdom for the South African context. Further, the research involved an exploration of a schools-based system that would provide information for planning purposes. Thus, one monitoring system was selected, the Middle Year Information System (MidYIS), developed by the Centre for Evaluation and Monitoring (CEM) at the University of Durham. The researcher investigated the extent to which the system could yield valid inferences regarding the performance of secondary school students.\nTo explore quality-related issues, the Centre for Evaluation and Assessment (CEA) at the University of Pretoria, in collaboration with the CEM at the University of Durham, embarked on a joint research project to investigate the feasibility of using MidYIS as a school-based monitoring system. The National Research Foundation (NRF), a national funding body in South Africa, funded this project. The aim of adapting the school-based monitoring system is to provide information about the quality of education in addition to the extent of academic gains made with the purpose of intervening effectively in the students' development.\nThe MidYIS project was selected because there were no systems, in the South African system, at the beginning of secondary schools. The project was seen as being essential because there are two exit points in the education system, namely, Grade 9 and Grade 12 (schooling in South Africa is compulsory to the end of Grade 9). MidYIS was developed to offer schools information on how students would perform at the end of Key Stage 3 and the end of their General Certificate Secondary Education (GCSE). The Key Stage 3 and the GCSE are national assessments in the United Kingdom education system. The MidYIS project predicts how students would perform in Key Stage 3 and GCSE. These predictions are based on results obtained from a baseline assessment, and the schools are provided with information on the future achievement of enrolled students, the CEM center developed assessments that can be used for prediction and monitoring purposes."}, {"section_title": "Monitoring the Quality of Education", "text": "The purpose of the monitoring education is to assess achievement progress across various subjects due to global calls for the improvement of the quality of education for all (Howie & Scherman, 2017) . After all, to assure the quality of education, it has to be monitored (Scherman & Bosker, 2017) . Monitoring is continuously cited in school effectiveness research (SER) and is often linked to the achievement of students (Scheerens et al., 2003) . Scheerens et al. (2003) are of the opinion that monitoring can be defined as a systematic collection of information to make judgments regarding the effectiveness of schooling. Monitoring stresses the ongoing collection of information so that decisions can be made concerning improving learning. Raffan and Ruthen (2003) links this activity to learning and observing learning, regarding difficulties experienced and progress made.\nMonitoring is essential, providing mechanisms for formally regulating the desired level of quality (Scheerens et al., 2003) . It is seen as a tool that focuses students, educators, and the principal on set goals (Sammons, 1999) . Monitoring of students also has the potential to inform planning, teaching, and assessment, but, most importantly, monitoring sends the message that the educator and the school are interested in the student and in progress being made (Sammons, 1999) . Monitoring is essential to the learning process and assists children who are not performing to reach their potential (Lockheed & Murphy,1996) . Achievement, within a monitoring framework, is measured over time (Lockheed, 1996) . In the context of this research study, monitoring is seen as collecting information about student performance, at various stages, to establish whether academic gains have been made. The monitoring function is also needed to identify strategies where necessary.\nMonitoring can be formal or informal and can, therefore, take various forms (Sammons, 1999) . Formally, monitoring could refer to student monitoring systems that represent a set of educational achievement tests that help to identify students who have fallen behind and the subject matter or skills in which difficulties are experienced. Alternatively, informal monitoring can take the form of assessment-based self-evaluation (Scheerens et al., 2003) . Regardless of which form of monitoring is applied, certain tools are required to track progress (Lockheed, 1996) . International school effectiveness literature may be modified to serve as measurement criteria that would be appropriate in the intended context.\nThe benefit of monitoring is that the levels of quality in education can be formally regulated and this provides the impetus for on-going improvement in education. The improvement is measured as information is regularly collected, the information is evaluated, which, in turn, leads to action. This action could take place at various levels within the education system-for example, national, regional (provinces), and local levels (school and student). The aim of the monitoring efforts is supporting learning or reaching a judgement with regard to levels of achievement (Willms, 1992) . The information gleaned can be used to map changes in performance over time (Nuttall, 1994) . This on-going monitoring allows schools to learn from their own experiences, taking the educational context into account (Fullan & Watson, 1999) ."}, {"section_title": "Method", "text": ""}, {"section_title": "Research Design", "text": "For this research, it was necessary to obtain information on some levels within the system using a variety of data collection tools. Thus, pragmatism was used as the underpinning paradigm (Biesta, 2010) . Pragmatism is pluralist, is critical in nature, focuses on solving problems, and is action oriented in that the focus is on identifying theories that work locally (Johnson & Onwuegbuzie, 2004) , and this was seen as necessary for the research problem that this research addressed. Pragmatism is practical and can be considered as an applied research philosophy, thereby making the investigation of the perceived problem possible (Biesta, 2010) .\nWithin a pragmatic framework, mixed methods can be used, thereby adequately addressing the research question (Teddlie & Tashakkori, 2003) . Using mixed methods as a framework provides the opportunity to present results that are more credible and less biased because the one method compensates for the weaknesses of the other method. Using a mixed methods framework, therefore, makes knowledge claims stronger (Greene, 2005) . Mixed methods research helps researchers to answer questions that researchers using other approaches in isolation cannot. An example of this is the investigation of validity issues, which in itself is a complex task (Kline, 1993) , where a combination of methods can be used to provide different perspectives on the same issue, thereby making inferences stronger. Thus, the issue of validity can be addressed quantitatively using inferential statistics to investigate construct-related validity and to undertake reliability analysis. Inferences about validity are addressed qualitatively by undertaking an analysis of curriculum documents, follow-up interviews with the Provincial Department of Basic Education officials, and interviews with National Department of Basic Education officials.\nA concurrent nested strategy was used in this research study. The quantitative approach was the dominant method in the research. The qualitative component was given lesser priority, nested within the quantitative approach. Although the quantitative phase involved the use of information at the school, classroom, and student level, the qualitative phase involved the collection of information at the provincial and national levels. The use of a concurrent nested strategy provided a broader perspective by studying different groups and different levels simultaneously (Creswell, 2003) . In both instances, information from one level built on information from the other levels (Creswell, Plano Clark, Gutman, & Hanson, 2003) . "}, {"section_title": "Sample", "text": "The design called for the selection of participants at different levels within the education system (Creswell, 2015) . Two General Education and Training officials specializing in the areas of mathematics and language were asked to participate. A representative from the Gauteng Department of Basic Education Office for Standards in Education (OFSTED), national government officials in the fields of curriculum and assessment (two participants), and experts in the field of psychology and education were also asked to participate. Specialists were purposefully selected. In total, 11 schools were purposefully selected in the Gauteng Province to participate in this project; the schools were selected via maximum variation sampling (Patton, 2002) . Diversity was regarded as important and, thus, schools from a variety of environments, including demographic variations in students, educators, surrounding communities, and access to funding, were included. The whole Grade 8 classes were randomly selected. In total, 794 students from the two classes were selected from each of the 11 participating schools."}, {"section_title": "Instruments", "text": "An interview schedule, used for the National Department of Basic Education, served as the instrument to collect information on assessment and curriculum issues. The information sought included policy, making use of developed assessments, strategies facilitating the ensurance of curricular or curriculum validity, and issues about monitoring. The interview schedule was semi-structured in nature. A provincial-level questionnaire was also used to explore assessment practices and the use of developed assessments, issues related to curriculum validity, items related to skills development regarding the curriculum, and background information such as age, gender, qualifications, previous, and current employment. The questionnaire consisted of both closed-ended items and open-ended items.\nSeven sub-sections were included in the assessment instrument. The sub-sections were combined into four different scales (vocabulary scale, mathematics scale, skills scale, and non-verbal scale). The seven sub-sections were timed and consisted of multiple-choice items except for the mathematics subtest, which included both constructed-response items and multiple-choice items over and above the assessment instrument."}, {"section_title": "Data Collection", "text": "Document analysis was undertaken to establish the test curriculum overlap. Thus, the skills tested in the assessment and the skills that were taught in the curriculum were compared. The documents included in the analyses were the South African language curriculum policy document as well as the mathematics curriculum policy document. The documents were imported into ATLAS.Ti (Smit, 2005) and analyzed using identifying themes of the skills that students were meant to be taught according to the curriculum; the results were used in conjunction with the evaluation reports to make inferences about the test curriculum overlap.\nThe evaluation report was intended to explore issues of content-related validity, and specialists in the field of psychology and education were approached for this purpose, a meeting was scheduled to discuss the results of the evaluation, and process notes were taken. Two national government officials were visited for face-to-face interviews to obtain additional information about curriculum validity and monitoring systems more generally. The participants were contacted telephonically to participate in the research project. The interviews were completed in 30 minutes on average. The interview, which was semi-structured in nature, was recorded with the permission of the participants (Kajornboon, 2005) .\nThe provincial-level education officials were contacted telephonically to request their participation in this research. Once the officials had agreed to participate in the study, the questionnaires were emailed and faxed to them. Upon completion, the questionnaires were either emailed or faxed back to the researcher. The OFSTED representative was contacted telephonically to ascertain whether the official would be willing to participate in this research study. The questionnaire was emailed to the OFSTED official once he agreed to participate. That was followed up with a telephone interview using the questionnaire as an interview schedule. The telephonic interview lasted approximately 15 minutes.\nEach school was visited on a separate day; a fieldworker oversaw the standardized administration procedure for each classroom. The fieldworker read a script explaining the assessment procedure as well as the time limits that applied for each sub-section. Reading the script ensured that the administration procedures were standardized across the schools and that each student received the same information. The English script was translated into Sepedi and Afrikaans. Two groups of translators were used to translate the administration script. The first group translated the English script into Sepedi and Afrikaans, whereas the second group of translators checked the Sepedi and Afrikaans translations against the English version. Corrections were made in discussion with the specialists before the scripts were finalized. The fieldworkers also completed an administration questionnaire detailing the administration process, including problems experienced, comments made by students, and general impressions and time taken for the majority of students to complete the sub-sections."}, {"section_title": "Data Analysis", "text": "A parallel mixed methods analysis was utilised in this research. Thus, the interpretation and writing up of the qualitative and quantitative data took place separately (Onwuegbuzie & Teddlie, 2003) . The data were analyzed using the Statistical Package for the Social Sciences (SPSS), WINSTPS (Linacre, 2005) , and ATLAS.ti (Smit, 2005) .\nFor the qualitative data, curriculum documents and semi-structured interviews were analysed using thematic content analysis. Through the process of analysis, interviews were captured verbatim and coded according to different units of meaning (Henning, Van Rensburg, & Smit, 2004) . Codes were assigned to pieces of data to attach meaning, as well as to index the data. Open coding was initially used for breaking up the data to generate theoretical possibilities within the data. The codes were grouped into themes (Punch, 1998) .\nA number of strategies were used to analyse the quantitative data, namely, descriptive statistics, Rasch analysis, reliability analysis, and correlation analysis. These procedures were chosen because they aligned with the research questions. Descriptive statistics were used to summarise data using central tendency as well as measures of dispersion (Coolican, 1999) . For the Rasch analysis, the real person, used to classify people, and real item separation, used to verify the item hierarchy, were evaluated (Linacre, 2005) . The real person and real item separation reliabilities were also scrutinised (Smith, 2003) . The separation reliabilities are synonymous with measures of internal consistency (in that a value between 0 and 1 is obtained) (Andrich, 1982) . The INFIT and OUTFIT mean squares were scrutinized. The INFIT means square (MNSQ) is associated with the response patterns, whereas the OUTFIT mean square (MNSQ) is related to response patterns that are not expected. Its expected value is 1. In the evaluation, values lower than 1 indicate a lack of fit, whereas values greater than 1 indicate noise within the measurement (Smith, 2003) . Values between 0.7 and 1.3 for the mean squares are considered adequate for measurement purposes (Bond & Fox, 2015) . More weight was attached to the mean square (MNSQ) interpretation because Z-values derived from more than 300 observations tend to be very sensitive (Linacre, 2005) . As such, the items that should not misfit, do (Linacre, 2005) . Items with an absolute Zvalue greater than 2.0 were also identified. Any person or item misfitting the above criteria was removed from the analysis.\nInternal consistency is viewed as a pre-requisite for inferences drawn about construct validity; high item-total correlations measure the same construct (Kline, 1993) . Kuder-Richardson 21 (KR-21) was used because the assessment data were recoded as dichotomous items. Reliabilities for assessment data should be high, preferably around 0.9, but should never drop below 0.7 (Kline, 1993) .\nCorrelation analysis was undertaken to explore inferences about predictive validity. A positive correlation of 0.3 or 0.4 was considered sufficient for the ability-academic relationship (Kline, 1993) ."}, {"section_title": "Ethical Issues", "text": "The ethics requirements were adhered to as prescribed by the Faculty of Education of the University of Pretoria and, second, the researcher's integrity. Permission was sought from the Faculty of Education. The Faculty was satisfied with the procedures suggested and granted leave to continue with the research. With regard to professional integrity, it was important to be transparent about the research. The Gauteng Department of Basic Education in addition to the schools were contacted to obtain informed consent. Also, parents of every student were sent a letter of assent explaining the project and requesting permission for their child to participate. The research was explained to the students who were selected to participate. Schools and educators were approached and asked whether they would be willing to participate by completing a background questionnaire. Confidentiality was promised to parents, students, educators, and school administrators as well as National and Provincial Department of Basic Education officials. Furthermore, participants could withdraw at any time."}, {"section_title": "Results", "text": "Comparisons were made between MidYIS and several other monitoring systems as gleaned from the literature. The comparison is illustrated in Table 1 . Rationale underpinning the project Developing sound self-evaluation tools based on research and theory.\nAssist schools to monitor the effectiveness of their teaching and learning.\nTo provide process information that schools can use for improvement plans.\nTo provide schools with value-added information.\nStakeholder input Schools evaluate themselves. Component evaluated to ascertain efficiency effectiveness and use of information.\nSchools interpret the data based on training received. School management teams primarily responsible. However, the process is participative and the stakeholders work together.\nStakeholders decide which elements should be monitored and who will collect the data. Participative in nature.\nSchools interpret the data based on training received.\nEffect on behavioural aspects Information used by schools to draw up self-improvement plans in line with legislation.\nInformation used by schools to develop strategies for improvement including personnel management strategies.\nInformation used to develop school improvement strategies and plans.\nSchools decide whether they want information on behavioural aspects.\nImplementation of the project School-based minimum interference with school activities.\nMinimum interference with school activities because this forms part of the VCE assessment programme.\nThe model is time-consuming and labour intensive. However, data collected are not collected by outcomes-driven indicator systems.\nSchool-based minimum interference with school activities.\nThree monitoring systems from the literature were reviewed in addition to the MidYIS project. These systems were the ZEBO-project, the VCE data project, and the ABC+ model. MidYIS only includes the student level, whereas the other systems include at least two levels (student and classroom or student and school). The MidYIS involves performing a monitoring function with the aim to provide schools with value-added information on performance. The schools then decide how to use the information. CEM provides the information, but schools are responsible for interpreting the data or undertaking the additional analysis. A deviating point is the use of behavioural components such as student attitudes. With MidYIS, this is available, but schools decide whether they would like to have the additional information. The other monitoring systems reviewed include behavioural information as an integral part of the monitoring system and not as an additional component as is the case with MidYIS. The objective of the interviews with the National Department of Basic Education was to elicit views about assessment, the curriculum, and the monitoring of learning. The ultimate goal was to propose a monitoring system that could be implemented nationally. As such, it was important to understand what would be acceptable and what would be endorsed by the National Department of Basic Education. From the interviews, it appears as if the more holistic approach to monitoring was preferred. The issue of quality was of importance, and student performance might have been used as an indicator to determine the quality of education, as exemplified by \"\u2026 student performance \u2026 can [be] used as indicator for quality or determining quality of the system\" (Interviewee 2) and \"I think we need to move beyond assessment you know especially student assessment as the only tool of monitoring performance \u2026 [rather] a system that will operate at all levels of education, all levels starting from the classroom\" (Interviewee 2).\nWhichever monitoring system is used, it has to be multilayered, and able to provide information on some levels, namely school, district, province, and national. This is supported by the following statement: \"\u2026 schools and teachers respond best when accountability demands accompany support, and that capacity, therefore, needs to be built at district, school and classroom levels so as to strengthen systems for both monitoring and supporting learning\" (Interviewee 2).\nAnother important consideration was the types of schools across the country. Thus, a monitoring system would have to apply to the whole spectrum of contexts. This need was expressed as a \"system that needs to talk to different contexts in our country\" (Interviewee 2).\nThe implications for the current research were that if the MidYIS is to be accepted on a national level, it should include in its framework some levels-namely, classroom, school, and provincial levels-and be appropriate in a variety of contexts, taking into account the diversity of resources and people. Apart from the monitoring system having to apply to different contexts, it should also make use of assessment practices that are in line with the continuous assessment model advocated by the Department of Basic Education, as stated by one of the participants: \"different ways of collecting evidence are encouraged and assessment, which is linked \u2026 the curriculum\" (Interviewee 1) and \"assessment should be used formatively ... . If you even ask the teacher, what do you do with the results of the assessment? Nothing I just record them, and that is it\" (Interviewee 1).\nThe reiteration of the importance of skills in conjunction with knowledge of content appeared to be highlighted, as \"I think there needs to be a relationship between what is taught and what is assessed. However, this relationship goes beyond the content. It has to also focus on \u2026 skills \u2026 content \u2026 the two definitely need to go together\" (Interviewee 2) and \"you need to understand that there is a relationship between the teaching process and the assessment process\" (Interviewee 1).\nAlthough it would appear that the National Department officials might accept the use of already developed assessments, as long as the assessment is clearly aligned with the curriculum, the three Provincial Education officials who completed the questionnaire were not in favour of using already developed assessments or that were not developed by the Educator him/herself. The official who works for OFSTED was contacted telephonically to clarify some issues that emerged from the questionnaire that he completed. When asked why he was not in favour of the developed assessment, the respondent indicated that continuous assessment practices are new. Furthermore, the respondent revealed that he had not seen assessments that are related to the curriculum. Curriculum-related assessments are important because the curriculum was open to interpretation and customisation by schools. If, however, the school had a programme or curriculum in place and the assessment was related to the programme, he believed that then it might work. The OFSTED official also indicated that it might be good to have a standardised assessment in place, because some schools might want to have a benchmark from which to evaluate their performance, specifically against similar schools, as well as against international standards. The statements from the OFSTED official reinforces the ideas that emerged from the interviews with the National Department officials. If a monitoring system is to be acceptable to the government, the inferences based on the tools used in the monitoring system should be valid for the school's curriculum. Furthermore, if student progress is to be tracked, the assessment tool should take place at intervals.\nIn an attempt to ascertain whether the skills assessed in the MidYIS were present in the intended curriculum, the Provincial Department of Basic Education officials were asked to indicate whether the skills were indeed present. A skills list was compiled drawing from the list of competencies assessed by the MidYIS instrument. Based on the analysis, there was overlap between the skills represented in the instrument and that of the curriculum. Furthermore, many of the skills were introduced to students during primary school. Thus, these skills can be considered as the core competencies underpinning the secondary school curricula. The fundamental competencies assessed by MidYIS seem to exist in the primary school curriculum and should be in place upon entry to secondary school (see Table 2 ). As can be seen in Table 2 , the majority of the skills are taught at the primary school level. From the document analysis, it appeared that there is overlap between the MidYIS assessment and the intended policy documents. To confirm the document analysis, specialists in the field of language were asked to evaluate MidYIS (see Table 3 ). From the specialists' perspective, the instructions, vocabulary sub-test, and proofreading subtest were of relevance for the language learning area. The skills needed to succeed in these areas are taught in the curriculum. Furthermore, one of the specialists indicated that the items in the MidYIS assessment were not biased regarding gender or race and that the language used was age appropriate. However, the other specialist indicated that although the core competencies were present in the curriculum, certain items could be difficult for second language students and that these items should be modified or replaced. Initial analysis indicated some agreement between the MidYIS assessment and the mathematics curriculum document (see Table 4 ). Out of all the subtests included in the MidYIS assessment, the mathematics subtest is the most curriculum-bound. A mathematics specialist was consulted to support the results from the document analysis. The document analyses were presented to the mathematics specialist. The evaluation of the mathematics specialist concurred with the results of the document analysis. There appears to be overlap between the MidYIS assessment and the curriculum. However, data handling was not present in the assessment. Not only are the instructions read to the students, the instructions are also printed on the first page of each sub-test as well as throughout the sub-test. This implies that the student can read with the administrator or can read independently for meaning. In order to complete the proof reading section of MidYIS, students would have to read the passage in order to make sense of the passage and to rectify mistakes in terms of spelling, grammar, and punctuation. Spelling, grammar, and punctuation are skills in which students should be proficient by the time they enter Grade 8 as emphasis is placed on these skills in preceding grade levels. Language structure and use Vocabulary Proof reading\nVocabulary and proof reading contains elements of the structures used in language. Vocabulary has been included because students should be able to recognise the meaning of words and their synonyms and be able to match words on this basis. Proof reading requires students to identify mistakes in terms of spelling, grammar, and punctuation. Both perceptual speed and pictures include elements of finding or completing the pattern given, while the mathematics section includes algebraic equations, all of which are reasonable for Grade 8.\n3) Space and shape Block counting Cross-sections Block counting and cross-sections are a measure of spatial ability. Spatial ability requires certain skills in 2D and 3D manipulation. These two sub-tests are in line with the basic skills that are taught in this learning area in order to prepare students to be successful in geometry. 4) Measurement Mathematics In the mathematics sub-test, various measuring units and formulae are used. The type of items included is grade appropriate in that students should have been exposed to the skill in preceding grade levels. (Kline, 2000) , as measured by tests of vocabulary and reading comprehension (Sternberg, 1985) , using words in context: understanding proverbs, verbal analogies and vocabulary (Cooper, 1999 Measured by means of identifying poor grammar and correcting errors (Hunt, 1985) ."}, {"section_title": "Washington-Pre-College Test Battery Differential Aptitude Test (DAT) Middle Years Information System (MidYIS) Spelling", "text": "Denotes the recognition of misspelled words (Kline, 1993) ."}, {"section_title": "Differential Aptitude Test (DAT) Middle Years Information System (MidYIS) Numerical ability", "text": "Facility in the manipulation of numbers but does not include arithmetic reasoning (Kline, 2000) ."}, {"section_title": "General Scholastic Aptitude Test Battery (GSAT) Senior South African Individual Scale (SSAIS) Junior Aptitude Test (JAT) Differential Aptitude Test (DAT) Middle Years Information System (MidYIS) Numerical facility", "text": "Denotes the ability to use algebra and other forms of mathematical operation (Cooper, 1999 (Kline, 2000; Sternberg, 1985) ."}, {"section_title": "Junior Aptitude Test (JAT) Senior Aptitude Test (SAT) Washington-Pre-College Test Battery Differential Aptitude Test (DAT) Middle Years Information System (MidYIS) Perceptual speed and accuracy", "text": "Denotes the ability to rapidly assess differences between stimuli (Kline, 2000) and measured by the rapid recognition of symbols (Sternberg, 1985 Table 5 illustrates the types of ability factors prominent in aptitude assessments (or abilities assessment). The psychology specialists were requested to appraise the MidYIS assessment for content-related validity, specifically regarding intelligence or ability theory. Thus, the similarity between the MidYIS assessment and other developed abilities assessment (e.g., the Wechsler Intelligence Scale for Children [WISC] , Differential Aptitude Test [DAT]) was evaluated. The language was judged as well as any bias regarding gender or race. The analysis indicated that the domains in the MidYIS assessment corresponded with the domain of items typically included in an ability assessment. The domains include verbal ability, comprehension and relations, spatial ability, grammar or language usage, perceptual speed and accuracy, and numerical ability and facility. The items appeared to be free of bias regarding language or gender.\nRegarding the construct-related validity of the assessment instrument, the seven sub-tests were analysed separately. With regard to vocabulary, for both the persons and items, the INFIT and OUTFIT mean squares (MNSQ) were close to 1. The person separation reliability was .83 and the item separation reliability was 0.99. The separation reliability indicated that the scale discriminates among persons. Thus, the separation reliability indicated that the items do create a well-defined variable. Of the 40 items included for the language subtest, nine items misfitted (almost 25%), namely Items 1, 3, 4, 17, 20, 27, 36, 38, and 39 (based on the criteria for OUTFIT or INFIT MNSQ). The items could have misfitted due to unusual response patterns across all persons. The items, therefore, might not have tapped the same ability as did the other items in the sub-test, or they might have been biased regarding gender or subgroups (Barnard, 2004) . Misfitting persons were also identified, Of the 794 persons included in the analysis, 72 persons were identified as misfitting persons. Thus, the persons who misfitted did not meet the specifications of the Rasch model, as explained in the beginning of the section, and were removed from further analysis (Bond & Fox, 2015) . The analysis was undertaken again, the INFIT and OUTFIT mean squares (MNSQ) were explored and were close to 1 for both persons and items. The separation reliabilities for persons and items were 0.83 and 0.99, respectively, indicating both adequate discrimination between persons and a well-defined construct (see Table 6 ). Of the 40 items included, only five items remained problematic, namely Items 17, 20, 27, 38, and 39. Seventy-four items were included for the mathematics sub-tests. For both person and item, the INFIT and OUTFIT mean squares (MNSQ) were close to 1, indicating an excellent fit and lack of noise. The separation reliabilities for both persons and items were high at 0.89 and 1.00, respectively, indicating that there was sufficient discrimination between persons and that the items did form a well-defined construct (see Table 7 ). Of the 74 items included initially, 24 items misfitted (this equates to approximately one third) possibly due to an inability to tap the same ability level as did the other items or some form of bias (as was discussed earlier). One hundred and two persons (1 out of 7 persons) were identified as misfitting due to the unexpected response patterns of these individuals. With the reanalysis, the INFIT and OUTFIT mean squares (MNSQ) were again close to 1, indicating relatively good fit between the theoretical model and the data. The separation reliability for persons and items was 0.89 and 0.99, respectively, indicating discrimination between individuals and forming of a distinct construct. Twenty-five items did not meet the stipulated criteria (namely, OUTFIT or INFIT mean squares of 0.7 to 1.3). It was found that even after the misfitting persons were removed, the same items misfitted. Upon inspection, the misfitting items included identification of the largest or smallest number, percentages, simple multiplication and division, fractions, area, co-ordinates, and manipulation of three different sizes of cogs.\nThe proofreading section has two components. The first component consists of a passage that participants had to read while identifying spelling or punctuation mistakes. The second element, however, asks participants to identify mistakes by comparing sentences. The components were kept separate. Only the items containing actual errors that had to be identified were included. The reasoning behind including items with actual mistakes was the coding procedure used when initially preparing for data capturing. For the first section of the proofreading sub-test, the INFIT and OUTFIT mean squares were 1 or close to 1. The OUTFIT mean square (MNSQ) for both persons and items were slightly more than 1, possibility indicating noise. The separation reliabilities were high, 0.89 for persons and 0.99 for items (see Table 8 ). Fifty-eight items were initially included for analysis. However, 18 items misfitted due to inconsistent response patterns because of bias or inability to tap the same ability level as the other items. Of the 794 persons included in the initial analysis, 104 misfitted and would need to be removed from the analysis. The INFIT and OUTFIT mean squares (MNSQ) were close to 1, indicating relatively good fit between the data and the theoretical model. The fit statistics for the reanalysis were much the same as for the initial analysis. Once the misfitting persons were removed, of the items, only 17 misfitted. They included errors in punctuation such as a full stop and spelling errors, for example, \"there\" instead of \"their\" and \"lead\" instead of \"led.\"\nFor the second section of the proofreading sub-test, participants identify mistakes by comparing a master list to a copy list. The INFIT mean square (MNSQ) for both persons and items was 1, whereas the OUTFIT mean square (MNSQ) for both persons and items was slightly lower than 1. The separation reliability for persons and items were 0.90 and 0.98, respectively. Thirty-four items were included in the initial analysis, of which 15 items misfitted (almost 50%), and of the 794 persons, 57 persons misfitted. The analysis was repeated with the misfitting persons removed. The INFIT mean square (MNSQ) reanalysis was similar to the initial analysis; however, the OUTFIT mean square (MNSQ) of 0.97 was slightly lower than the original analysis, which indicated a slight lack of fit between the data and the theoretical model. The separation reliabilities for both persons and items were 0.91 and 0.98, respectively (see Table 9 ). Seventeen items misfitted in the reanalysis, which was more than the original 15 items (exactly 50%). Upon inspection, it was found that the misfitting items included words in which letters were switched around or omitted when transferred from the master to the copy list, words like \"Sandels\" and \"Sandles\" or \"Alexandra\" and \"Alexandria.\" Twenty-six items were included in the initial analysis for the perceptual speed and accuracy sub-test. Both the INFIT and OUTFIT mean squares for persons and items were acceptable, although INFIT and OUTFIT mean squares (MNSQ) for the items (0.96 and 0.94, respectively) were slightly below 1, indicating a slight lack of fit. What was cause for concern was the relatively low separation reliability for persons (0.67), an indication that discrimination between persons was not desirable. The item separation reliability was 0.96, which indicated that the items did form a welldefined construct. Eight items of the 26 items included for analysis misfitted (30% of the items). Of the 794 participants, 57 misfitted (7% of the persons) due to unexpected response patterns. The analysis was undertaken again with the misfitting persons excluded. The INFIT mean square (MNSQ) for persons and items were 1.01 and 0.97, respectively, indicating fit between the data and the theoretical model. The OUTFIT mean square (MNSQ) for persons and items were 0.86 and 0.87, showing a slight of lack of fit. The separation reliabilities were 0.66 and 0.96 (see Table 10 ). This indicated a lack of discrimination between students but did suggest that a distinct construct was present. Twenty-six items were included in this reanalysis, of which nine misfitted (35% of the items). Participants visually compare and find matches between two columns in this sub-test. Due to the nature of the task, it is possible that some symbols were confusing to the participants or that, due to time constraints, mistakes were made. The cross-sections sub-test consists of 16 items. As with the other sub-tests, the INFIT and OUTFIT mean squares (MNSQ) were close to 1, indicating good fit and lack of noise. The separation reliability for items was excellent, 0.99, showing a well-defined construct. The separation reliability for persons was relatively low at 0.54, especially in comparison with other sub-tests. Students did not fare well in this particular sub-test, and it is likely that defined ability groups would be difficult to identify. Participants are requested to identify the 2D shape that would result if a 3D shape were cut through. Of the 16 items, two items misfitted. Due to unexpected response patterns, 81 persons misfitted. The analysis was undertaken again without these individuals. The INFIT and OUTFIT mean squares (MNSQ) were close to 1. The separation reliabilities for persons and items were 0.50 and 0.99, respectively, indicating a lack of discrimination among participants (see Table 11 ). It is possible that the person separation was affected, because 10% of the total sample was removed due to unexpected response patterns. The results of the reanalysis indicated that three items misfitted (Items 7, 10 and 12). Items 10 and 12 are very similar in nature and could have been easily confused because the instruction was to find the shape that was used to create the cross section. Twenty items are included in the block counting sub-test. Participants were also requested to determine the minimum number, and a maximum number of small blocks in the figure presented. The INFIT mean square (MNSQ) for both persons and items were below 1, indicating a slight lack of fit. The OUTFIT means square (MNSQ) values for both persons and items were well above 1, indicating noise within the data. The person separation reliability was 0.74, and the items reparation reliability was 1.00 (see Table 12 ). Both values were acceptable. Of the 20 items, 15 items initially misfitted (75% of the items, which is very high). With the reanalysis, the INFIT mean squares (MNSQ) and OUTFIT mean squares (MNSQ) were around 1. Eight items misfitted after reanalysis (40% of the sub-test). The number of misfitting items was substantially better than for the initial 75% of the items. Four of the items cited the minimum (two items) and maximum (two items) number of small blocks possible. The remaining four items denoted the number of small blocks (2 items) and a number of larger blocks (2 items). Three sections are included in the pictures sub-test (i.e., adding pictures, subtracting pictures, and picture sequences). Six items are included per section, 18 items in total. The INFIT mean square for both persons and items was 0.99, which is very close to 1. The OUTFIT mean square was 1.21 and 1.28 for persons and items, respectively, which was slightly elevated. The person separation reliability was 0.73, and the item separation reliability was 1.00. For both the individual and item, the separation reliability was acceptable. The value of 0.73 was an indication of discrimination among persons, although lower than for some of the other sub-tests. The separation reliability for items (1.00) was an indication of a well-defined construct. Of the 18 items included in the initial analysis, eight misfitted. One hundred and forty-one persons misfitted. It is possible that the students did not listen to the instructions given and, as a result, did not answer the items correctly. The INFIT mean square (MNSQ) for both persons and items was 0.99, whereas the OUTFIT mean square (MNSQ) reanalysis for both persons and items was 1.06. The separation reliability for persons and items was 0.76 and 1.00, respectively (see Table 13 ). Nine items misfitted, amounting to 50% of the sub-test. Four items are located in the subtracting pictures sections, three items are located in the adding pictures section, whereas the remaining two items are in the sequences section. Sections are sequenced and it is possible that students did not read the instructions at the top of the subtracting pictures, thereby treating the section as adding instead of subtracting. Furthermore, this is the last sub-test in the assessment and participant fatigue could have been a contributing factor. Internal consistency reliability inferences about the content-related validity of the assessment are strengthened (Suen, 1990) . This form of reliability is also a prerequisite for construct-related validity (Kline, 1993) . The reliability coefficients for the MidYIS scales are provided in Table 14 . The reliabilities for all four scales were high, three of the four scales had reliability coefficients of 0.90 or higher, whereas the non-verbal scale had a reliability coefficient of 0.84. The purpose of the correlation analyses was to establish whether relationships exist between the MidYIS scores and academic achievement (specifically language and mathematics achievement). Correlations above 0.3 (Kline, 1993) for the MidYIS scales and the mathematics and English marks are considered indicative of a positive relationship, but, in addition to the positive correlations, the variance explained also has to be considered. The variance explained is calculated by squaring the correlation (Kline, 2000) . From the results, it seems as if positive relationships exist between the MidYIS scales and the mathematics marks (see Table 15 ). The exception would be for vocabulary, in which weak relationships existed for Schools 5, 6, and 9. This might be explained by the difference in mathematics and vocabulary (language) as well as that, very often, students are more proficient in one subject than the other. What is noteworthy is the percentage of variance that MidYIS explained regarding mathematics academic achievement. For the former Model C schools, the percentage of variance explained ranged from 26% to 33% on the vocabulary scale. However, percentages as low as 4% (School 5), 7% (School 9), and 9% (School 6) were recorded. A similar result was obtained for the skills and the non-verbal scales as with the vocabulary scale. The percentage of variance explained regarding academic success for mathematics was better than for the other scales. However, in School 9, as little as 4% of the variance could be accounted for. This means that abilities alone explained little in school's variation concerning performance, even though the scales can be related to the domain of mathematics. Thus, other factors possibly at a student, classroom, or school level must be considered, for instance, language spoken in the home of the student, age of the student, socio-economic status of the student, gender of the student or educator, language of teaching and learning, teaching style of the educator, or principal management style. As might have been expected, the correlations between vocabulary and the English mark in most of the schools exceeded 0.3 and were statistically significant at the 0.01 level (Onwuegbuzie & Daniel, 2005) . Strong correlations were found between the mathematics scale and the English marks (see Table 16 ). Less substantial correlations were found between non-verbal and the English mark, with weak correlations for School 3 and School 9. One might have expected a slightly higher correlation between the skills scale and the English mark because proofreading is language-bound. Although the majority of the correlations for the schools were above 0.3, the correlations were lower than was the correlation between vocabulary and the English mark. "}, {"section_title": "Discussion", "text": "A curriculum perspective and a psychometric perspective were explored to strengthen inferences regarding contentrelated validity. From a psychometric perspective, MidYIS is a developed abilities assessment, where ability is seen as competence or skill. The curriculum has its roots in competency-based education (Kraak, 1998) . In this context, competence refers to general intelligence or aptitude, as motivation or as a set of key competencies or skills. The message from the National and Provincial Departments of Basic Education was that assessment used in a school setting must be aligned with the curriculum. To explore the alignment of the MidYIS assessment with the South African curriculum, a document analysis was undertaken, and specialists consulted. Two specialists were selected, namely, one language and one mathematics specialist, in order to evaluate the fundamental skills assessed in MidYIS.\nRegarding content-related validity, for language, several content areas were represented in the MidYIS assessment, indicating a moderate alignment between MidYIS and the curriculum. However, the skills assessed in the MidYIS assessment that can be found in the curriculum refer to the basic skills needed-for example vocabulary. Teal (2003) is of the opinion that vocabulary knowledge is one of the best predictors of reading comprehension. Vocabulary knowledge provides a source of prior knowledge and word meaning that can be used to enhance reading comprehension. Word recognition and comprehension are key because if a student becomes better at reading, s/he will be able to read more challenging texts, resulting in a larger vocabulary and syntactic knowledge that, in turn, positively affects language ability (Aarnoutse, Brand-Gruwel, & Oduber, 1997) -word recognition and comprehension forms the basic competence that is needed to master the language curriculum. The act of reading helps to increase the student's vocabulary and also his/her awareness of language and structure of the text (McFarlane, 1997) . By including an additional section, student reading skills and comprehension can be directly assessed.\nInferences regarding curriculum validity for the mathematics curriculum are substantially stronger because four of the five areas are represented in MidYIS. The acquisition of mathematical problem-solving and reasoning skills in addition to the ability to apply the skills to mathematical situations and real-life situations constitute a major goal or objective of mathematics education (Verschaffel, 1999) . A primary goal of mathematics education is to enable students to apply their knowledge of facts, concepts, formulas, and procedures to solve problems in a variety of learning situations (Muth, 1997) . Solving mathematics problems requires learning of domain-specific knowledge that is well structured and flexible, including content, procedures, and reflective knowledge, to be able to solve the given problem (Nelissen, 1999) . In order to solve problems, students need to have basic mathematical skills and be able to observe, to relate, to question, and to infer. To solve mathematical problems, students must be able to reason about ideas, to see the relationships and connections, and to be able to make sense of mathematics. Students should be able to draw conclusions, to induce patterns, and to deduce ideas, resulting in students having the ability to use models and mathematical ideas to explain thinking (Holmes, 1995) . It would appear from the document analysis and specialist evaluation that MidYIS has a reasonable degree of curriculum validity. However, the proposal for additional items about the outcome currently not represented would make the inferences drawn that much stronger. Furthermore, the evaluations from the psychologists indicate that the items in the MidYIS are in agreement with the ability domain. Also, MidYIS is comparable to other ability assessments currently used in South Africa such as the DAT, and is not biased regarding gender or race.\nWhat emerges from the Rasch analyses is that there are core items associated with sub-tests and that the subtests can be combined into scales, as was originally designed by the CEM. However, there are items that seem to measure constructs other than the constructs that they were designed to measure, and these were removed from further analyses. Thus, the items that were identified as misfitting should be revised or rewritten, based on an assessment framework for the assessment as a whole. The assessment framework should be developed both from a curriculum and psychometric perspective, thereby satisfying conditions for conceptual forms of validity. The results of the reliability analyses indicate internal consistency for the set of items per scale. It is suggested that in the future, larger samples for sub-population groups should be included if inferences per population group are to be made with confidence.\nThe analysis was undertaken per school and not across schools. The results indicated that the scales do correlate with the results obtained from schools for mathematics and English. Therefore, MidYIS could be used for prediction purposes, although more analytic work is needed in this area before definitive statements can be made (including a larger sample from other provinces and contexts). What does seem to emerge is that MidYIS on its own can only 268 V. SCHERMAN account for so much variation in performance. Other factors on the student, classroom, and school level have to be taken into account."}, {"section_title": "Conclusion", "text": "Monitoring in education is necessary because it is a way in which to regulate formally levels of quality in education, it provides a mechanism to hold stakeholders accountable, and it provides the impetus for ongoing improvement in education (Scheerens et al., 2003) . Furthermore, given the amount of financial and human resources that is put into education, the effects of education should be considered (Sammons, 2006) . If it is stated that education should prepare students for the world of work and if the resources are allocated to this, then this implies a holistic view regarding personal development, citizenship, and, indeed, the necessary skills needed to succeed in the labour market (Luyten, Visscher, & Witziers, 2005) . Thus, the resources allocated by the government would show some return. If this is the case, as in many countries around the world, then assessing the extent to which these goals are met is essential. The use of and distribution of resources are also linked to the relevance of educational objectives and whether these objectives are in reality attained. Also, the fair allocation of resources, especially in South Africa, is paramount, likewise how these resources are translated into an economic benefit (Scheerens et al., 2003) .\nIn this research, a mixed methods study was conducted. A continuing issue in the mixed methods discourse, however, is the manner in which paradigms are used in the development of the mixed methods as a field (Teddlie & Tashakkori, 2003) . Based on the pragmatist paradigm in its various forms (Biesta, 2010) , the use of mixed methods is appropriate in the case of this research, and the use of the methods is seen as being complementary (Morse, 2003) , because one method only yields a partial snapshot of the phenomenon and the use of both methods provides a more complete picture (Greene & Caracelli, 2003) . The current research study has shown that a value-added monitoring system can yield valid and reliable information. However, there is room for improvement. For this study, only urban and peri-urban schools from one province were included. Urban, peri-urban, and rural schools in other provinces were not included. Similar studies based on this research should be undertaken in the other provinces. Furthermore, the evaluation process, in which specialists representing the field of education and psychology are consulted, could be extended to suggestions on what could be included to make inferences regarding curriculum validity and content-related validity stronger. Here, the selection of the specialists or the way in which the specialists are sampled will have to be undertaken with care and with a specific purpose in mind. The qualitative component of this mixed methods research design could also be extended to include follow-up interviews with National Department of Basic Education officials to ensure that the specifications of the monitoring envisaged would comply with policies on a national level. Furthermore, units directly involved in monitoring the quality of education should be involved. In addition to officials in the Provincial Department of Basic Education, it might have been beneficial to include officials working on the district level because these officials would have more grass roots knowledge and potentially could provide valuable insights into how the envisaged monitoring system could be used in the varying school's contexts. Finally, the Rasch analyses could be extended to include equating items from different assessments and exploring the differential item functioning. Essentially, equating draws on item response theory whereby equating items from various grade assessments means that the items are linked. The difficulty of the items and the ability of the students can then be put on the same scale. Using this analysis, potentially weak items can be identified and the ability of the students ascertained. The results could then feed directly into topics for intervention programmes. The differential item functioning, on the other hand, would detect bias in the items, specifically about gender and cultural groups. The analysis of how items are performing for boys and girls and across racial groups would strengthen claims of cultural validity and would identify items that are not working well for the different groups. If these items were identified, changed, or removed, then the assessment would be better in the end and would result in an assessment that could be used in a wide variety of contexts."}, {"section_title": "Notes", "text": ""}]