[{"section_title": "Abstract", "text": "Linear models are widely used in various data mining and machine learning algorithms. One major limitation of such models is the lack of capability to capture predictive information from interactions between features. While introducing high-order feature interaction terms can overcome this limitation, this approach dramatically increases the model complexity and imposes significant challenges in the learning against overfitting. When there are multiple related learning tasks, feature interactions from these tasks are usually related and modeling such relatedness is the key to improve their generalization. In this paper, we propose a novel Multi-Task feature Interaction Learning (MTIL) framework to exploit the task relatedness from high-order feature interactions. Specifically, we collectively represent the feature interactions from multiple tasks as a tensor, and prior knowledge of task relatedness can be incorporated into different structured regularizations on this tensor. We formulate two concrete approaches under this framework, namely the shared interaction approach and the embedded interaction approach. The former assumes tasks share the same set of interactions, and the latter assumes feature interactions from multiple tasks share a common subspace. We have provided efficient algorithms for solving the two formulations. Extensive empirical studies on both synthetic and real datasets have demonstrated the effectiveness of the proposed framework."}, {"section_title": "INTRODUCTION", "text": "Linear models are simple yet powerful machine learning and data mining models that are widely used in many applications. Due to the additive nature of the linear models, it can fully unleash the power of feature engineering, allowing crafted features to be easily integrated into the learning system. This is a desired property in many practical applications, in which high-quality features are the key to predictive performance. Moreover, efficient parallel algorithms are readily available to learn linear models from large-scale datasets. Despite its attractive properties, one apparent limitation of such models is that they can only learn a set of individual effects of features contributing to the response, due to its linear additive property. Thus when a part of the response is derived from interactions between features, such models would not be able to detect such non-linear predictive information, thereby leading to poor predictive performance.\nIn practice, high-order feature interactions are common in many domains. For example, in genetics studies, environmental effects and genetic-environmental interaction are found to have strong relationship with the variability in adoptee aggressivity, conduct disorder and adult antisocial behavior [7] . Similarly, the interaction effects between continuance commitment and affective commitment was found in predicting annexed absences [28] . Also, a recent study of depression found that genotype, sex, environmental risk and their interaction have combined influence on depression symptoms [12] . It is also reported that the interaction of brain-derived neurotrophic factor and early life stress exposure are identified in predicting syndromal depression and anxiety, and associated alterations in cognition [16] . In biomedical studies, many human diseases are a result of complicated interactions among genetic variants and environmental factors [19] . One intuitive solution to overcome this limitation is to augment interaction terms into linear models, explicitly modeling the effects from the interactions. However, this will dramatically increase the model complexity and lead to poor generalization performance when there is limited amount of data [9, 11, 23, 26, 35] .\nOn the other hand, when there are multiple related learning tasks, the multi-task learning (MTL) paradigm [1, 4, 8] has offered a principled way to improve the generalization performance of such learning tasks by leveraging the relatedness among tasks and performing inductive transfer among them. The past decade has witnessed a great amount of success in applying MTL to tackle problems where large amount of labeled data are not available or creating such datasets incurs prohibitive cost. Such problems are especially prevalent in biological and medical domains, where MTL has achieved significant success, including data analysis on genotype and gene expression [21] , breast cancer diagnosis [37] and progression modeling of Alzheimer's Disease [18] , etc. The MTL improves generalization performance by learning a shared representation from all tasks, which serves as the agent for knowledge transfer. Structured regularization has provided an effective means of modeling such shared representation and encoding various types of domain knowledge on tasks [1, 20, 24, 33] . The attractive benefits provided by MTL make it an ideal scheme when learning problems involve multiple related tasks with feature interactions, because tasks may be related with each other by shared structures on feature interactions. For example, predicting various cognitive functions may involve a shared set of interactions among brain regions.\nHowever, many existing MTL frameworks are based on linear models [1] in the original input space. Thus they cannot be directly applied to explore task relatedness in the form of high-order feature interactions. On the other hand, although traditional nonlinear MTL methods based on neural networks (e.g., [2] ) can exploit non-linear feature interactions to some extends, it is generally difficult to encode prior knowledge on task relatedness to such models. In this paper, we propose a novel multi-task feature interaction learning framework, which learns a set of related tasks by exploiting task relatedness in the form of shared representations in both the original input space and the interaction space among features. We study two concrete approaches under this framework, according to different prior knowledge about the relatedness via feature interactions. The shared interaction approach assumes that there are only a small number of interactions that are relevant to the predictions, and all tasks share the same set of interactions; the embedded interaction approach assumes that, for each task, the feature interactions are derived from a low-dimensional subspace that is shared across different tasks. We have provided formulations and efficient algorithms for both approaches. We conduct empirical studies on both synthetic and real datasets to demonstrate the effectiveness of the proposed framework on leveraging feature interactions from tasks. The contributions of this paper are three folds:\n\u2022 Our novel framework has extended the MTL paradigm, for the first time, to allow high-order representations to be shared among tasks, by exploiting predictive information from feature interactions.\n\u2022 We proposed two novel approaches under our framework to model different task relatedness over feature interactions.\n\u2022 Our comprehensive empirical studies on both synthetic and real data have led to practical insights of the proposed framework.\nThe remainder of this paper is organized as follows: Section 2 reviews related work of MTL and models involving feature interactions. Section 3 introduces the framework for MTIL. The two approaches under MTIL have been given in 4. Section 5 presents the experimental results on both synthetic and real datasets. Section 6 concludes the paper."}, {"section_title": "RELATED WORK", "text": "The proposed research is related to existing work on MTL and feature interaction learning. In this section, we briefly summarize the these related work and show how our work advances these areas."}, {"section_title": "Multi-Task Learning", "text": "MTL has been extensive studied over the last two decades. In the center of most MTL algorithms is how task relationships are assumed and encoded into the learning formulations. The concept of learning multiple related tasks in parallel was first introduced in [8] . It was demonstrated in multiple real-world applications that adding a shared representation in neural network tasks can help others get better models. Such discovery had inspired many subsequent research efforts in the community and applications in diverse application domains. Among these studies, the regularized MTL framework has been pioneered by [13] . The regularization scheme can easily integrate various task relationship into existing learning formulations to couple MTL, thus providing a flexible multi-task extension to existing algorithms. It is well adopted and is soon generalized to a rich family of MTL algorithms. MTL via Regularization. Among the work in the regularization based MTL scheme, there are many different assumptions about how tasks are related, leading to different regularization terms in the formulation. For example, one common assumption is that the tasks share a subset of features, and the task relatedness can be captured by imposing a group sparsity penalty on the models to achieve simultaneous feature selection across tasks [33, 24] . Another common assumption is that the models of tasks come from the same subspace, leading to a low-rank structure within the model matrix. Directly penalizing the rank function leads to NPhard problems, and one convex alternative is to penalize the convex envelop of the rank function, i.e., trace norm. This encourages low-rank by introducing sparsity to the singular values of the model matrix [20] . In [1] , the authors studied a MTL formulation that learns a common feature mapping for the tasks and assumed all tasks share the same features after the mapping. The authors have shown that this assumption can also be equivalently expressed by a low-rank regularization on the model. There are many more formulations that fall into this category of formulation to capture task relatedness by designing different shared representation and regularization terms, such as cluster structures [38] , tree/graph structures [21, 10] , etc. However, to the best of our knowledge, all of these formulations do not consider feature interactions in the model, and extensions to consider interactions are not straightforward. In this work, we will extend the MTL framework to enable knowledge transfer not only in the original input space, but also in higher-order feature interaction space. Multilinear MTL. The use of tensor in MTL has shown to be very effective in representing structural information underlying in MTL problems. In [27] , Romera-Paredes et al. proposed a multilinear multitask (MTMTL) framework that arranges parameters of linear effects from all tasks into a tensor W, by which they are able to represent the multi-modal relationships among tasks. In a dataset containing multimodal relationships, tasks can be referenced by multiple indices. In MTMTL, the authors employed a regularizer on W to induce a low-rank structure to transfer knowledge among tasks. The optimization problem contains the minimization of tensor's rank, which leads to solving a non-convex problem. Thus the authors develop an alternating algorithm, employing the Tucker decomposition and convex relaxation using tensor trace norm. Although the authors also used a tensor representation in MTL, the learning formulations, implications, as well as the meaning of such the tensor is fundamentally different from those in our work. The proposed MTIL framework utilizes tensor to capture the relatedness among tasks and transfer knowledge through high-order feature interactions, which cannot be achieved by any existing MTL formulations. Note that the tensor in MTMTL is indexed by multi-modal tasks. In MTIL, the tensor is indexed by features and tasks, which is clearly different from the aforementioned work. In the proposed embedded interaction approach for MTIL, however, we face a similar challenge in MTMTL to seek a solution involving a low-rank tensor."}, {"section_title": "Feature Interaction", "text": "In many machine learning tasks, we are interested in learning a linear predictive model. Given the input feature vector of a sample, the response is given by a linear combination of these features, i.e., a weighted sum of the features. Because of this reason we call them linear effects. There are strong evidences found in many complex applications that, in addition to the linear effects, there are also effects from high-order interactions between such features. As a result, there are considerable efforts from both academia and industry aiming at addressing this limitation by removing the additive assumption and including interaction effects.\nTo overcome the dimensionality issues introduced by interaction effects, two types of heredity constraints have been studied [5] ; namely strong hierarchy in which an interaction effect can be selected into the model only if both of its corresponding linear effects have been selected, and weak hierarchy, in which an interaction effect can be selected if at least one of its corresponding linear effects has been selected. In [11] , the authors proposed an approach known as SHIM to identify the important interaction effects. SHIM extends the classical Lasso [29] and enforces a strong hierarchy. An iterative algorithm was proposed based on Lasso, which may not scale to problems with high dimensional feature space. Radchenko et. al proposed the VANISH method to address the problem [26] . They developed a convex formulation with a refined penalty that can not only learn the sparse solution, but also treat the linear and interaction effects using different weights. This way, the main effect could have more influence on the prediction. In [5] , a hierarchical lasso was proposed to search for interactions with large main effects instead of considering all possible interactions. The authors proposed an algorithm based on ADMM for strong hierarchy lasso and a generalized gradient descent for weak hierarchical lasso. More recently, Liu et al. [23] proposed an efficient algorithm for solving the non-convex weak hierarchical Lasso directly, based on the framework of general iterative shrinkage and thresholding (GIST) [17] . The authors proposed a closed form solution of proximal operator and further improved the efficiency of solving the subproblem of proximal operator from quadratic to linearithmic time complexity.\nIn many real work applications there are multiple related tasks. When those these tasks involve interaction effects, the tasks could be related via the high order feature interactions. In our paper, we propose to address the model complexity issue from interaction effects using a new perspective, by leveraging such relatedness."}, {"section_title": "TASK RELATEDNESS IN HIGH ORDER FEATURE INTERACTIONS", "text": "In this section, we present the framework of Multi-Task feature Interaction Learning (MTIL). For completeness, we give a self-contained introduction of our work. We will derive concrete learning algorithms under this framework in Section 4. Linear and Interaction Effects. Consider the traditional linear models. For an input feature vector x \u2208 R d and a scalar response y, we have assumed the following underlying linear generative model:\nwhere w \u2208 R d is the weight vector for linear effects, and \u223c N (0, \u03c3 2 ) is a Gaussian noise. A linear model f (x; w) = x T w can be a quite effective prediction function. However, if the underlying generative model includes effects from feature interactions, i.e.,\nwhere xixjQi,j is the joint effect between the ith feature and the jth feature, and Qi,j is the weight for this joint effect. This type of feature interactions have been commonly found in many applications. If the training data follow this distribution then the linear model is not enough to capture the relationship between input features and output responses.\nOne of the approaches is to introduce non-linear feature interaction terms into the linear model. That is, we can denote it as a quadratic function:\nwhere w \u2208 R d and Q \u2208 R d\u00d7d collectively represent the parameters for linear effects and interaction effects, respectively. We note that Q is typically symmetric because this representation includes two terms involving feature i and j: xixj(Qi,j + Qj,i) and it also includes second-order feature transformations of the original features x 2 i Qi,i. Discussions on Feature Interactions. In supervised learning, we seek a predictive function that maps an input vector\nThe goal of learning is to find the best predictor f \u2208 H so that the predicted value\u0177i for the input data xi is as close as possible to the ground truth yi, \u2200(xi, yi) \u2208 (X, y), given a loss function L(., .). We hope that the predictor f learned in this way is close to the optimal model that minimizes the expected loss according to the \u03bc:\nSuch predictor is given by the minimum of the empirical risk:f\nThe error caused by learning the best predictor in the training dataset is called the estimation error. The error caused by using a restricted H is called the approximation error. For a fixed data size, the smaller the hypothesis space H, the larger the approximation error, and vice versa. The trade-off between approximation error and estimation error is controlled by selecting the size of H. By including feature interactions we would enlarge the hypothesis space, and we may be able to dramatically minimize the approximation error compared to the traditional hypothesis space for linear models. On the other hand, we note that given a limited amount of data, a large hypothesis space may result in models with poor generalization performance. We will need to either increase our training data, or provide effective regularizations to narrow down the hypothesis space. Multi-task Feature Interactions. We consider the setting that there are multiple learning tasks which are related not only in the original feature space, but also in terms of feature interactions. The propose framework simultaneously learns all related tasks and provides an effective regularization on the hypothesis space using relatedness on the interactions. Let D = (X1, y1) , . . . , (XT , yT ) be the training data for the T learning tasks, and the i.i.d. training samples for task t is drawn from (\u03bct) m t , where mt is the number of data points available for task t. We collectively denote the distribution as\nThe corresponding features are homogeneous and have the same semantic meaning. The total training data points are:\nThe goal of MTL is to learn T functions for the tasks such that f t(xit) = yit, based on the assumption that all task functions are related to some extent.\nIn order to consider interactions for each task, we use the quadratic predictive function in Eq. 1 for all tasks. We collectively represent the linear effects from all tasks as a matrix W = [w1, . . . , wT ] \u2208 R d\u00d7T , wi \u2208 R d and the interaction effects as a tensor Q \u2208 R d\u00d7d\u00d7T , in which the t-th frontal slice Qt \u2208 R d\u00d7d represents the interaction effects for task t. We illustrate this interaction tensor in Figure 1(a) .\nGiven specific loss functions\u02c6 for samples from one task, (e.g., square loss for regression and logistic loss for classification, see Table 1 ), the loss function for each task is t(f, w, Q; X, y) = m t i=1\u02c6 (f (xi; w, Q), yi). Our multi-task feature interaction loss function is given by:\nNote that it is not necessary for all tasks to have the same loss function. In MTL, the learning of each task benefits from the knowledge from other tasks, which effectively reduces the hypothesis space for all tasks. In order to achieve knowledge transfer among tasks, we would like to impose shared representations via designing regularization terms on both W and Q, which specify how tasks are related in the original feature space and features interactions, respectively. The MTIL Framework. The proposed Multi-Task feature Interaction Learning (MTIL) framework is then given by the following learning objective:\nwhere RF (W) is the regularization providing task relatedness in the original feature space, RI (Q) is the regularization encoding our knowledge about how feature interactions are related among tasks, \u03bbR and \u03bbI are the corresponding regularization coefficients. For \u03bbI \u2192 \u221e, the problem reduces to traditional MTL, when RI is chosen properly. In this paper, we formulate two concrete approaches to capture the feature interaction patterns:\n\u2022 Shared Interaction Approach. In many applications, even though we have a large number of feature interactions, only a few interactions may be related to the response [5, 11] . When learning with multiple tasks, different tasks may share exactly the same set of feature interactions, but with different effects. As such, we can design MTIL formulations that learns a set of common feature interactions, which could effectively reduce the hypothesis space. During the learning process the selected feature interactions for one task will be task's knowledge, contributing to the share representation: a set of indices of common interactions. An analogy in traditional MTL is the joint feature learning approach [24, 33] , in which tasks share the same set of features. One way to achieve this approach is by using the structured sparsity to induce the same sparsity patterns on the interaction effects. An illustration of this approach is given in Figure 1 (b).\n\u2022 Embedded Interaction Approach. When the response from one task is related to complicated feature interactions, the patterns of such interactions may be captured by a low-dimensional space, resulting in a low-rank interaction matrix. When there are multiple related tasks, they could have a shared low-dimensional space, i.e., different interaction matrices may share the same set of rank-1 basis matrices, but have different weights associated with these basis matrices. When collectively represented by a tensor, we end up with a low-rank tensor. During the learning process, each task contributes their subspace information to facilitate learning of the share low-dimensional subspace, which in turn, improves the feature space. The analogy in traditional MTL is the low-rank based models [1, 20] . However, there are challenging questions such as: How to define a proper rank function for tensor? Are there tractable algorithms to induce low-rank structure in tensor? In the next section we will discuss these important questions and propose efficient algorithms. We illustrate this approach in Figure 1 (c).\nWe note that even though we only provided two specific approaches in this paper, the proposed MTIL framework could offer broader class of formulations. The proposed framework allows many other possible ways to define task relatedness on feature interactions, leading to a brand-new research area of MTL."}, {"section_title": "FORMULATIONS AND ALGORITHGMS OF THE TWO MTIL APPROACHES", "text": "In this section, we will study how the formulations and algorithms of the shared interaction approach and embedded interaction approach under the proposed MITL framework. We note that extension of multi-task learning to feature interactions is not trivial because of the involvement of tensors. We start with formulating the shared interaction approach by incorporating a group Lasso penalty to introduce structured sparsity on the tensor, which would select only a set of common feature interactions across different tasks that are relevant to the prediction. For the embedded interaction approach, we propose both a convex formulation and a non-convex formulation. While the convex formulation leads to efficient optimization algorithms and global solutions, the non-convex formulation provides reduced storage complexity for large-scale problems."}, {"section_title": "Preliminary", "text": "In this paper, we use the following basic definition of tensor: Mode-n fiber is a vector defined by fixing every index but one. We may see it as the higher order analogue of matrix rows (mode-2 fibers) and columns (mode-1 fibers). For example, in a three-way tensor Q \u2208 R n 1 \u00d7n 2 \u00d7n 3 , the mode-3 fiber is Qi,j,: \u2208 R n 3 . Mode-n unfolding is the process of reordering the elements of an N-way tensor Q \u2208 R n 1 \u00d7n 2 \u00d7,..,\u00d7n N into a matrix. The mode-k unfolding of tensor Q is denoted by\nThe matrix is arranged by concatenating all mode-k fibers of the tensor. Rank-n in our paper denotes the rank of tensor's mode-n unfolding. It's actually the dimension of the space spanned by the mode-n fibers of tensor. Specifically, rankn(Q) = rank(Q (n) ). When Q is a matrix (i.e. 2-way tensor), this becomes the regular definition of rank, since rank1(Q) = rank2(Q) = rank(Q)."}, {"section_title": "Shared Interaction Approach", "text": "The goal of the shared interaction approach is to identify a set of common and relevant feature interactions across different tasks. The interaction tensor Q in our framework has provided a convenient representation to encode such information, and we are able to incorporating a group Lasso penalty [14] to induce a special type of structured sparsity on the tensor, coupling the same interactions for all tasks. Recall that the sparsity implies that only the significant interaction effects are captured in the model. For the purpose of shared interaction, a sparse tensor norm is defined as:\nNote that this norm enforces a symmetric sparsity by over the tensor, so that the one group is defined to include coefficients of one interaction between feature i and feature j, from all tasks. Penalizing the tensor sparse norm leads to the following formulation:\nwhere the parameter \u03bbI control the sparsity of tensor Q, a larger \u03bc will end up with a more sparse Q. The solution to formulation delivers a tensor such that the mode-3 fibers are either all zeros vectors or non zero vectors, i.e., interaction effects between 2 features xi, xj either exists on all tasks, or irrelevant for all tasks. Note that even the sparsity patterns is same for all tasks, their interactions may have different weights. It is easy to see that, this approach subsumes the traditional multi-task learning as a special case: when \u03bbI \u2192 \u221e by setting regularization parameter on tensor Q to infinity, all the elements in of Q in the solution will be zeros, and the model only considers linear effects. When the loss function L chosen is convex and continuously differentiable with Lipschitz continuous gradient [26] , then we can use proximal based gradient methods, such as first order FISTA [3] , SpaRSA [34] or second order Proximal Newton [22] to solve it efficiently. Because that the linear effects and interaction effects are decoupled in the predictive function, a major class of loss functions belong to this category, and we give a few examples of common loss functions in Table 1 . Note that even when L is non-convex, a local optimal solution can be efficiently obtained using the GIST framework [17] . The key to apply these algorithms is to efficiently compute the proximal operator that associates to the problem (refer to [25] for more details about proximal):\nwhere\u0174 andQ are intermediate solutions at each step, \u03c11 and \u03c12 are regularization parameters augmented with step size. Note that we have extend the Forbenius norm from matrix to tensor. We see that the problem is decoupled for W and Q. And the tensor proximal:\ncan be solved in the same way as the group Lasso proximal operator [36] . Moreover, we find that when the gradient is symmetric, we don't need to enforce a symmetric tensor "}, {"section_title": "Loss with Interaction Loss function", "text": "is the sigmoid function defined as g(\nsparse norm, and we could simply use a simple alternative:\nand initialize the algorithm with a symmetric tensor as the starting point. The reason that symmetry holds can be explained by two parts. First, the gradient of Q is symmetric, therefore the gradient descent step won't change the symmetry of tensor Q. Second, the proximal operator associated to sparse tensor norm won't change the symmetry of matrix. To see this, the proximal operation is performed by vectorizing the matrix into a vector and shrink each element of the vector with respect to a input vector, which is obtained by the last gradient descent step. Since the input vector represents an symmetric matrix, the element and its symmetric element will always shrink to the same new value. Therefore, the symmetry of Q holds. The sparse tensor norm is equivalent to perform the l1 projection of vectors where each element is the l2 norm of mode-3 fiber in tensor Q."}, {"section_title": "Embedded Interaction Approach", "text": "The share interaction approach has enforced a very restrictive form of how tasks are supposed to relate to each other. In many applications, the prediction may be a result of complicated feature interactions, instead only involves a few interactions. Even though the prediction may involve all feature interactions, it is usually a reasonable assumption that there are patterns among these interactions. Numerically, existence of patterns imply a low-dimensional subspace, which is reflected by a low-rank structure in the matrix. When there are multiple related learning tasks, one way for these tasks relate to others via a shared lowdimensional subspace, which gives us a low-rank tensor. As such, we may design a structured regularization to encourage the matrix Q to be a low-rank tensor. In this paper we describe one convex formulation that encourages low-rank structure by penalizing a tensor norm and one non-convex formulation that directly learns a low-rank representation."}, {"section_title": "Convex Formulation", "text": "One way to obtain a low-rank tensor is to augment our formulation with a rank penalty. One problem associates to tensor is that there is no consistent way to define the rank of a tensor. One way is to use the average rank of unfolding on different mode [15] :\nwhere N is the total number of mode of the tensor (N = 3 when only pair-wise interactions), and Q (n) is unfold on n mode. Since minimizing the rank function is proven to be NP-hard, we could penalize the trace norm instead, which is the convex envelope of the rank function. The trace norm is defined as the sum of singular values of the matrix variable [20] . We then obtain the following convex formulation:\nwhere . * denotes the trace norm. However, this convex formulation penalizes every mode of tensor Q to be jointly low rank, which may be too restricted in practice, which may lead to suboptimal performance. Moreover, the practical way to solve the formulation in Eq. (7) is to use the alternating direction methods of multipliers (ADMM) [6] , which introduces auxiliary variables and equality constraints, in order to decouple the three tensor trace norm terms. However, ADMM algorithm in practice is shown to have a slow convergence rate, and less preferred when composite proximal methods such as FISTA can be applied.\nOne alternative way to address these issues is to use the latent trace norm [30, 31] , which is defined as following for a N \u2212way tensor:\nwhere Q (1) . . . Q (N ) are a set of low-rank auxiliary tensors, which states that the original tensor can be decomposed into the sum of a set of tensors that are low-rank in different modes. Finally, we proposed to drop the equality constraint that each auxiliary tensor equal to the original one, but we directly use the mixture of tensors to represent the original tensor, so the problem becomes a unconstrained optimization problem. The predictive function of task t with such mixture is given by:\nwhere Q (j) \u2208 R d\u00d7d\u00d7K , \u2200j = 1, 2, 3 are the auxiliary tensors for replacing the original tensor Q, matrix Q\n\u2208 R d\u00d7d is the tth frontal slice of tensor Q (j) . Finally, our convex formulation under embedded interaction approach is given by:\nThe convexity of this formulation holds since both the loss function and the penalty are convex. We note that this formulation can be solved in the same way as the formulation in Eq. (7), and the model is much more flexible to model the complicated interactions among the features, leveraging the advantages of such auxiliary tensors."}, {"section_title": "Non-Convex Formulation", "text": "Although using proximal gradient methods we are able to secure an optimal solution for the convex formulation, the time complexity and storage cost are unacceptable in practice as the dimension of data increase. To see this, we note that the proximal operator associated to a trace norm regularized objective requires singular projections [20] , which requires cubic-complexity singular value decomposition. Recall in each iteration of the gradient methods could involve more than one computation of proximal operator [3] , and thus the computation may be prohibitive when dimension grows larger. On the other hand, we have to maintain 3 dense tensors of size d \u00d7 d \u00d7 T which means the storage cost is at O(d 2 ), where T is the number of tasks and typically we have T d. Also the mixture of three low-rank auxiliary tensors may lead to some difficulty when it comes to analyzing the predictive model itself.\nTo this end, we propose to use a tensor with a explicit lowrank structure. Consider the interaction effects matrix Q \u2208 R d\u00d7d for one task, we assume the low-rank decomposition Q = BQB T , where B \u2208 R d\u00d7r is a basis matrix,Q \u2208 R r\u00d7r is a small matrix, capturing the information of the original tensor under the set of bases (columns) in B. To see this, we can expand Q = r i,j=1Q (i,j) BiB T j , meaning the matrix Q is a result of interactions among bases in B and also spanned by the columns of B. We thus can use a predictive function that explicitly considers this low-rank structure:\nWhen there are multiple tasks, our assumption for embedded interaction approach is the shared basis, meaning B is restricted to be same as all other tasks. The multi-task loss function is thus given by:\nt(fnvc, wt, B,Qt; Xt, Yt), whereQ \u2208 R r\u00d7r\u00d7T collective denotes the set of matricesQ from all tasks. This loss function is not convex because of the multiplication of variables in x T BQB T x. This loss function leads to our final non-convex formulation for embedded:\nwhere the regularization RI ({B},Q) can be Forbenius norm or other structural information (e.g. 1 norm). The dimension r of B can be chosen according to the need of specific application demands, and can be selected by cross-validation. In general, we choose r d. We note that the storage complexity for the feature interaction effects (e.g., tensor Q) is\n, which is dramatically smaller than the full tensor, especially in the high dimensional settings. We could use the family of block coordinate descent algorithms [32] to alternatively solve the variables W, {B}, andQ, to get a local optimal solution."}, {"section_title": "EXPERIMENTS", "text": "In this section, we perform experiments on both synthetic datasets and two real world datasets to evaluate the effectiveness of our proposed MTIL framework. "}, {"section_title": "Synthetic Dataset", "text": "In order to justify the effectiveness of modeling the feature interactions and MTIL framework, we test our methods on synthetic datasets."}, {"section_title": "Effectiveness of modeling feature interactions", "text": "In this subsection, we test whether the interactions between features can be properly handled by adding the interaction term Q. To do so, we create a single task synthetic dataset by assuming:\nwhere X \u2208 R n\u00d7d is the feature matrix, y \u2208 R n\u00d71 is the responses, w \u2208 R d\u00d71 is the weight vector, Q \u2208 R d\u00d7d is a symmetric, low-rank sparse matrix, which represents the feature interactions in the dataset, and \u223c N (0, 0.01In) is the additive noise term. We generate 20 synthetic datasets with different sizes (1000 or 1k and 5000 or 5k) and different feature dimensions (varying from 10 to 100, stepped by 10) by randomly selecting X, w, and Q and computing y according to Eq.(8).\nWe use single task feature interaction learning model (STIL) to evaluate the effectiveness of the interaction term Q:\nwhere w \u2208 R d\u00d71 is the weight vector, Q \u2208 R d\u00d7d is the feature interaction matrix, and Q 1,1 = i j |Qi,j| denotes the 1,1 norm.\nWe compared the Root Mean Square Error (RMSE) between the Ridge Regression(RR) and STIL on both of the synthetic datasets. As the results show in Figure 2 , STIL outperforms RR on both of the datasets, which shows the effectiveness of modeling the feature interaction in the data. Besides, STIL-5k (RR-5k) performs better than STIL-1k (RR-1k), which demonstrates that the learning models will capture the underlining models of the data better with larger training size. Also note that with the number of dimensions increases, STIL will gradually overfit the data, because of the dramatic increase of the interactions between features. "}, {"section_title": "Effectiveness of MTIL", "text": "In order to test the effectiveness of MTIL, we generate a multi-task synthetic data by assuming:\nwhere Xt \u2208 R n\u00d7d is the feature matrix of task t, yt \u2208 R n\u00d71 is the responses of task t, W \u2208 R d\u00d7T = [w1, w2, w3, ..., wT ] is the weights for tasks. As described in Section 4.3, we generate feature interaction matrix Qt = BqtB T and project it into a sparse, symmetric space.\nIn this experiment, we generate 5 datasets with different feature dimensions from 10 to 50, stepped by 10, by randomly selecting Xt, wt, B and qt.\nThe predictive performance of the methods outlined below are examined on the synthetic multi-task datasets:\n\u2022 Ridge Regression (RR): We choose this model as the baseline and make neither assumptions of feature interaction nor the relation among all the tasks.\n\u2022 STIL: We perform STIL on each of the task independently.\n\u2022 MTL-L: This approach refers to the traditional MTL method regularized by the trace norm of the weight matrix W[1]. It does not make assumptions on feature interactions.\n\u2022 MTIL-L-S: This approach, refers to multi-task feature interaction learning regularized by the trace norm of the weight matrix W and the tensor group lasso norm of tensor Q (see section 4.2).\n\u2022 MTIL-S-S: This approach is similar to MTIL-L-S except that the regularization term on W is 2,1 norm.\n\u2022 MTIL-L-Lc: This approach refers to multi-task feature interaction learning regularized by the trace norm of the weight matrix W and latent trace norm of tensor Q (see section 4.3).\n\u2022 MTIL-S-Lc: This approach is similar to MTIL-L-Lc except for that the regularization term on W is 2,1 norm.\n\u2022 MTIL-L-Ln: This approach refer to multi-task feature interaction learning regularized by the low rank norm of tensor Q and the trace norm of the weight matrix W (see section 4.3.2).\n\u2022 MTIL-S-Ln: This approach is similar to MTIL-L-Ln except for that the regularization term on W is 2,1 norm. Figure 3 compares the RMSE of the above methods on the 5 synthetic datasets. We can see that MTIL-L-Ln and MTIL-S-Ln are not that sensitive to the change of feature dimensions, thanks to the low-rank assumption on the feature interaction. Also, RR and MTL-L share a similar performance, which is consistent with the fact that we did not assume any low-rank structure in this synthetic dataset. Note that although STIL performs almost the best on low dimensional data, its performance deteriorates rapidly compared with other MTIL methods, due to the incapability of learning the feature interactions across tasks."}, {"section_title": "School Dataset", "text": "This dataset contains the examination records of 15362 students with 28 features from 139 schools in years of 1985, 1986 and 1987 , provided by the Inner London Education Authority(ILEA). In this dataset, each task is to predict exam scores for students in one out of the 139 schools. We perform 4 sets of experiments by varying the amount of training size, from 20% to 50% of the total sample size. We test the approaches summarized in section 5. Table 2 . First, for most of the methods, RMSE will decrease when the training size increases. This means that providing more data in the training set will help overcome the overfitting problem. Also, we found that the performance of embedded feature approaches (i.e. MTIL-L-Lc, MTIL-L-Ln, MTIL-SLn) are worse than the single task learning approach. The reason behind this is that embedded feature approaches do not have sparse constraints on the interaction term, which will severely overfit the data when there is not sufficient training samples. Additionally, the MTL-L and MTIL-L-S obtain better performance than single task learning, which indicates that the low-rank structure shared by tasks are effectively captured by the low-rank assumption in these two methods. Moreover, MTIL-L-S method outperforms all other methods, which empirically proves the effectiveness of learning the shared interactions with sparse constraints."}, {"section_title": "Modeling Alzheimer's Disease", "text": "The Alzheimer's Disease Neuroimaging Initiative (ADNI) database(adni.loni.ucla.edu), which was launched in 2003 as a 5-year public-private partnership, is aimed to test whether the positron emission tomography (PET), serial magnetic resonance imaging (MRI), other biological markers, and clinical and neuropsychological assessments can be combined to measure the progression of mild cognitive impairment (MCI) and early Alzheimer's disease (AD). We follow the procedure of preprocessing mentioned in [39] and obtain 648 subjects and 305 MRI features. The parameters are tuned in the same way as we described in 5.2. The RMSE comparison result is shown in Table 3 . First, we found that all of the MTLs outperform the single task learning approaches (RR and STIL), which demonstrates the effectiveness of learning multiple tasks jointly by exploring the relatedness between tasks, as well as the existence of the underlying relatedness between tasks in the ADNI dataset. Second, the RMSE results of MTIL-L-S and MTL-L are comparable with each other, which indicates that the multiple tasks in this dataset do not share the same feature interaction structure. Finally, the result of MTIL-S-Lc method outperforms all other methods, which shows superiority of our feature interaction framework. Through a mixture of 3 low-rank tensor, we are able to learn the feature interaction pattern in this dataset."}, {"section_title": "Discussion", "text": "The proposed multi-task feature interaction learning framework has provided us a way to bridge related tasks using interaction effects. By employing different types of regularizations on the interaction effects tensor, the formulations under this framework have very different characteristics.\nFor the shared interaction approach: we utilize Group Lasso on the interaction tensor to control the model complexity. The proximal operator admits a closed form solution, and thus the overall computational cost is very low. We are able to obtain interpretable results from the model, showing what are important interactions that are relevant to the prediction tasks. The main drawback is that we assume all tasks share the same set of interaction effects, which may not be the case for many data sets. One way to further improve the formulation is by extending the strong or weak heredity properties [5, 23] to the proposed MTIL framework.\nFor the embedded interaction approach: we can easily obtain the global optimal for the convex formulation. Though we are able to tune the regularization parameter on the trace norms to control the rank of the interaction tensor, it is usually very hard to decide the value unless cross-validation is used. A rank larger than necessary may lead to over-fitting when training samples are insufficient. On the other hand, the obtained mixture of 3 tensor is hard to interpret. The non-convex formulation provides a better model decomposition, from which we can see the combination of basis for different tasks and identify embedded bases that are shared among the set of tasks. The drawback of this formulation is that we may easily trapped in a bad local optimal unless we carefully choose the initial value (e.g., using the solution from the convex formulation).\nIn general, this framework can be generalized into many other possible relatedness on feature interactions by incorporating different regularization terms. Different approaches of this framework should be carefully chosen according to the application domain. In the future work we plan to study the statistical properties of the proposed model, which may lead to deeper understanding of these interaction models."}, {"section_title": "CONCLUSIONS", "text": "One major limitation of linear models is the lack of capability to capture predictive information from interactions between features. While introducing high-order feature interaction terms can overcome this limitation, this approach tremendously increases the model complexity and imposes significant challenges in the learning against overfitting. In this paper, we proposed a novel Multi-Task feature Interaction Learning (MTIL) framework to exploit the task relatedness from high-order feature interactions, which provides better generalization performance by inductive transfer among tasks via shared representations of feature interactions. We formulate two concrete approaches under this framework and provide efficient algorithms: the shared interaction approach and the embedded interaction approach. The former assumes tasks share the same set of interactions, and the latter assumes feature interactions from multiple tasks come from a shared subspace. We have provided efficient algorithms for solving the two approaches. Extensive empirical studies on both synthetic and real datasets have demonstrated the effectiveness of the proposed framework."}]