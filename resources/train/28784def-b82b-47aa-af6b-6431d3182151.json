[{"section_title": "Abstract", "text": "[1] We update the Goddard Institute for Space Studies (GISS) analysis of global surface temperature change, compare alternative analyses, and address questions about perception and reality of global warming. Satellite-observed night lights are used to identify measurement stations located in extreme darkness and adjust temperature trends of urban and periurban stations for nonclimatic factors, verifying that urban effects on analyzed global change are small. Because the GISS analysis combines available sea surface temperature records with meteorological station measurements, we test alternative choices for the ocean data, showing that global temperature change is sensitive to estimated temperature change in polar regions where observations are limited. We use simple 12 month (and n \u00d7 12) running means to improve the information content in our temperature graphs. Contrary to a popular misconception, the rate of warming has not declined. Global temperature is rising as fast in the past decade as in the prior 2 decades, despite year-to-year fluctuations associated with the El Ni\u00f1o-La Ni\u00f1a cycle of tropical ocean temperature. Record high global 12 month running mean temperature for the period with instrumental data was reached in 2010."}, {"section_title": "INTRODUCTION", "text": "[2] Analyses of global surface temperature change are routinely carried out by several groups, including the NASA Goddard Institute for Space Studies, the NOAA National Climatic Data Center (NCDC), and a joint effort of the UK Met Office Hadley Centre and the University of East Anglia Climatic Research Unit (HadCRUT). These analyses are not independent as they must use much the same input observations. However, the multiple analyses provide useful checks because they employ different ways of handling data problems such as incomplete spatial and temporal coverage and nonclimatic influences on measurement station environment.\n[3] Here we describe the current Goddard Institute for Space Studies (GISS) analysis of global surface temperature change. We first provide background on why and how the GISS method was developed and then describe the input data that go into our analysis. We discuss sources of uncertainty in the temperature records and provide some insight about the magnitude of the problems via alternative choices for input data and adjustments to the data. We discuss a few of the salient features in the resulting temperature reconstruction and compare our global mean temperature change with those obtained in the NCDC and HadCRUT analyses. Given our conclusion that global warming is continuing unabated and that this conclusion differs from some popular perceptions, we discuss reasons for such misperceptions including the influence of short-term weather and climate fluctuations."}, {"section_title": "BACKGROUND OF GISS ANALYSIS METHOD", "text": "[4] GISS analyses of global surface temperature change were initiated by one of us (J.H.) in the late 1970s and first published in 1981 [Hansen et al., 1981] . The objective was an estimate of global temperature change that could be compared with expected global climate change in response to known or suspected climate forcing mechanisms such as atmospheric carbon dioxide, volcanic aerosols, and solar irradiance changes. There was a history of prior analyses of temperature change, as discussed by Jones et al. [1982] and summarized by Intergovernmental Panel on Climate Change (IPCC) [2007, Figure 1.3] , with most of the studies covering large but less than global regions.\n[5] A principal question at the time of the first GISS analysis was whether there were sufficient stations in the Southern Hemisphere to allow a meaningful evaluation of global temperature change. The supposition in the GISS analysis was that an estimate of global temperature change with useful accuracy should be possible because seasonal and annual temperature anomalies, relative to a long-term average (climatology), present a much smoother geographical field than temperature itself. For example, when New York City has an unusually cold winter, it is likely that Philadelphia is also colder than normal.\n[6] One consequence of working only with temperature change is that our analysis does not produce estimates of absolute temperature. For the sake of users who require an absolute global mean temperature, we have estimated the 1951-1980 global mean surface air temperature as 14\u00b0C with uncertainty several tenths of a degree Celsius. That value was obtained by using a global climate model [Hansen et al., 2007] to fill in temperatures at grid points without observations, but it is consistent with results of Jones et al. [1999] based on observational data. The review paper of Jones et al. [1999] includes maps of absolute temperature as well as extensive background information on studies of both absolute temperature and surface temperature change.\n[7] The correlation of temperature anomaly time series for neighboring stations was illustrated by Hansen and Lebedeff [1987] as a function of station separation for different latitude bands. The average correlation coefficient was shown to remain above 50% to distances of about 1200 km at most latitudes, but in the tropics the correlation falls to about 35% at station separation of 1200 km. The GISS analysis specifies the temperature anomaly at a given location as the weighted average of the anomalies for all stations located within 1200 km of that point, with the weight decreasing linearly from unity for a station located at that point to zero for stations located 1200 km or farther from the point in question.\n[8] Hansen and Lebedeff [1987] found that the correlation of neighboring station temperature records had no significant dependence on direction between the stations. They also examined the sensitivity of analyzed global temperature to the chosen limit for station radius of influence (1200 km). The global mean temperature anomaly was insensitive to this choice for the range from 250 to 2000 km. The main effect is to make the global temperature anomaly map smoother as the radius of influence increases. However, global maps of temperature anomalies using a small radius of influence are useful for detecting stations with a temperature record that is inconsistent with stations in neighboring regions. Thus, the GISS Web page provides results for 250 km as well as 1200 km.\n[9] The standard GISS analysis thus interpolates among station measurements and extrapolates anomalies as far as 1200 km into regions without measurement stations. Resulting regions with defined temperature anomalies are used to calculate a temperature anomaly history for large latitude zones. Early versions of the GISS analysis, including Hansen and Lebedeff [1987] , calculated the global temperature anomaly time series as the average for these several latitude zones, with each zone weighted by the area with defined temperature anomaly. That definition can result in the global temperature anomaly differing from the average anomaly for the two hemispheres by as much as several hundredths of a degree during the early decades when spatial coverage was especially poor. This characteristic caused confusion and questions from users of the GISS data set. Our present analysis weights the temperature anomalies of zones by the zone's full area, thus obtaining consistent zonal and global results. Differences among these arbitrary choices for definition of the global average anomaly are within the uncertainty in the global result discussed in section 7.\n[10] Hansen and Lebedeff [1987] calculated an error estimate due to incomplete spatial coverage of stations using a global climate model that was shown to have realistic spatial and temporal variations of temperature anomalies. The average error was found by comparing global temperature variations from the spatially and temporally complete model fields with the results when the model was sampled only at locations and times with measurements. Calculated errors increased toward earlier times as the area covered by stations diminished, with the errors becoming comparable in magnitude to estimated global temperature changes at about 1880. Thus, the GISS temperature anomaly estimates are restricted to post-1880. Independent analyses [Karl et al., 1994; Brohan et al., 2006; Smith et al., 2008] of hemispheric temperature errors using observational data yield similar results, supporting the date at which meaningful global analyses become possible.\n[11] The GISS analysis uses 1951-1980 as the base period. The United States National Weather Service uses a 3 decade period to define \"normal\" or average temperature. When we began our global temperature analyses and comparisons with climate models, that climatology period was 1951-1980. There is considerable merit in keeping the base period fixed, including the fact that many graphs have been published with that choice for climatology. Besides, a different base period only alters the zero point for anomalies, without changing the magnitude of the temperature change over any given period. Note also that many of today's adults grew up during that period, so they can remember what climate was like then. Finally, the data for a base period must have good global coverage, which eliminates periods prior to the 1950s.\n[12] GISS analyses beginning with Hansen et al. [1999] include a homogeneity adjustment to minimize local (nonclimatic) anthropogenic effects on measured temperature change. Such effects are usually largest in urban locations where buildings and energy use often cause a warming bias. Local anthropogenic cooling can also occur, for example, from irrigation and planting of vegetation [Oke, 1989] , but on average, these effects are probably outweighed by urban warming. The homogeneity adjustment procedure [Hansen et al., 1999, Figure 3] changes the long-term temperature trend of an urban station to make it agree with the mean trend of nearby rural stations. The effect of this adjustment on global temperature change was found to be small, less than 0.1\u00b0C for the past century. Discrimination between urban and rural areas was based on the population of the city associated with the meteorological station. Location of stations relative to population centers varies, however, so in the present paper we use the intensity of high-resolution satellite night light measurements to specify which stations are in population centers and which stations should be relatively free of urban influence.\n[13] The GISS temperature analysis has been available for many years on the GISS Web site (http://www.giss. nasa.gov), including maps, graphs, and tables of the results. The analysis is updated monthly using several data sets compiled by other groups from measurements at meteorological stations and satellite measurements of ocean surface temperature. Ocean data in the presatellite era are based on measurements by ships and buoys. The computer program that integrates these data sets into a global analysis is freely available on the GISS Web site.\n[14] Here we describe the current GISS analysis and present several updated graphs and maps of global surface temperature change. We compare our results with those of HadCRUT and NCDC, the main purpose being to investigate differences in recent global temperature trends and the ranking of annual temperatures among different years."}, {"section_title": "INPUT DATA", "text": "[15] The current GISS analysis employs several independent input data streams that are publicly available on the Internet and updated monthly. In addition, the analysis requires a data set for ocean surface temperature measurements in the presatellite era. In this paper and in the monthly updates on our Web site, we now show results using alternative choices for presatellite ocean data and alternative procedures for concatenating satellite and presatellite data."}, {"section_title": "Meteorological Station Measurements", "text": "[16] The source of monthly mean station measurements for our current analysis is the Global Historical Climatology Network (GHCN) version 2 of Peterson and Vose [1997] , which is available monthly from NCDC. GHCN includes data from about 7000 stations. We use only those stations that have a period of overlap with neighboring stations (within 1200 km) of at least 20 years [see Hansen et al., 1999, Figure 2] , which reduces the number of stations used in our analysis to about 6300. When GHCN version 3 becomes available, expected in late 2010, we will make results of our analysis available on our Web site for both versions 2 and 3 for a period that is at least long enough to assess the effect of differences between the two versions.\n[17] The GISS, NCDC, and HadCRUT global temperature results all make use of the GHCN data collection in their analyses. This is a matter of practicality, given the magnitude of the task of assembling and checking observations made in many different nations. However, a separate data collection effort that originated in the old Soviet Union led to another global temperature analysis, which does not use GHCN. This alternative analysis leads to a global temperature record in good agreement with that found by the groups using GHCN data [Lugina et al., 2006] .\n[18] We use the unadjusted version of GHCN. However, note that a subset of GHCN, the United States Historical Climatology Network (USHCN), has been adjusted via a homogenization intended to remove urban warming and other artifacts Peterson and Vose, 1997] . Also, bad data in GHCN were minimized at NCDC [Peterson and Vose, 1997; Peterson et al., 1998b] via checks of all monthly mean outliers that differed from their climatology by more than 2.5 standard deviations. About 15% of these outliers were eliminated for being incompatible with neighboring stations, with the remaining 85% being retained.\n[19] The current GISS analysis adjusts the long-term temperature trends of urban stations on the basis of neighboring rural stations, and we correct discontinuities in the records of two specific stations as described below. Our standard urban adjustment now utilizes satellite observations of night lights to identify whether stations are located in rural or urban areas. The urban adjustment, described in section 4, is carried out via our published computer program and the publicly available night light data set.\n[20] Our analysis also continues to include specific adjustments for two stations, as described by Hansen et al. [1999] [21] Our procedure in the current monthly updates of the GISS analysis when we find what seems to be a likely error in a station record is to report the problem to NCDC for their consideration and possible correction of the GHCN record. Our rationale is that verification of correct data entry from the original meteorological source is a person-intensive activity that is best handled by NCDC with its existing communications network. Also, it seems better not to have multiple versions of the GHCN data set in the scientific community.\n[22] The contiguous United States presents special homogeneity problems. One problem is the bias introduced by change in the time of daily temperature recording [Karl et al., 1986] , a problem that does not exist in the temperature records from most other nations. High energy use, built up local environments, and land use changes also cause homogeneity problems . The adjustments included by NCDC in the current USHCN data set (version 2 [Menne et al., 2009] ) should reduce these problems. As a test of how well urban influences have been minimized, we illustrate in section 4 the effect of our night light-based urban adjustment on the current USHCN data set."}, {"section_title": "Antarctic Research Station Measurements", "text": "[23] Measurements at Antarctic research stations help fill in what would otherwise be a large hole in the GHCN land-based temperature record. Substantial continuous data coverage in Antarctica did not begin until the International Geophysical Year (1957) . However, the period since 1957 includes the time of rapid global temperature change that began in about 1980.\n[24] The GISS analysis uses Scientific Committee on Antarctic Research (SCAR) monthly data [Turner et al., 2004] , which are publicly available. Specifically, the data are from the SCAR Reference Antarctic Data for Environmental Research project (http://www.antarctica.ac.uk/met/ READER/)."}, {"section_title": "Ocean Surface Temperature Measurements", "text": "[25] Our standard global land-ocean temperature index uses a concatenation of the Met Office Hadley Centre analysis of sea surface temperatures (HadISST1) [Rayner et al., 2003 ] for 1880-1981, which is ship-based during that interval, and satellite measurements of sea surface temperature for 1982 to the present (Optimum Interpolation Sea Surface Temperature version 2 (OISST.v2)) . The satellite measurements are calibrated with the help of ship and buoy data .\n[26] Ocean surface temperatures have their own homogeneity issues. Measurement methods changed over time as ships changed, most notably with a change from measurements of bucket water to engine intake water. Homogeneity adjustments have been made to the ship-based record Parker et al., 1995; Rayner et al., 2003 ], but these are necessarily imperfect. The spatial coverage of ship data is poor in the early 20th century and before. Also, although the land-based data confirm the reality of an early 1940s peak in global temperature, it has been suggested that changes in ship measurements during and after World War II cause an instrumental artifact that contributes to the magnitude of the peak of ocean temperature in the HadISST1 data set in the early 1940s [Thompson et al., 2008] . Ocean coverage and the quality of sea surface temperature data have been better since 1950 and especially during the era of satellite ocean data, i.e., since 1982. Satellite data, however, also have their own sources of uncertainty, despite their high spatial resolution and broad geographical coverage.\n[27] Thus, we compare the global temperature change obtained in our standard analysis, which concatenates HadISST1 and OISST.v2, with results from our analysis program using alternative ocean data sets. Specifically, we compare our standard case with results when the ocean data are replaced by the Extended Reconstructed Sea Surface Temperature, version 3b (ERSST.v3b) [Smith et al., 2008] , for the full period 1880 to the present and also when HadISST1 is used as the only source of ocean data for the full period 1880 to the present. In Appendix A we compare the alternative ocean data sets themselves over regions of common data coverage to help isolate differences among input SSTs as opposed to differences in the area covered by ocean data.\n[28] We use ocean temperature change only in regions that are ice free all year (a map of this area is included in Appendix A) because our data set is intended to be temperature change of surface air. Surface air temperature (SAT), measured at heights of 1.25-2 m at meteorological stations, is of most practical significance to humans, and it is usually SAT change that is reported in climate model studies. Change of sea surface temperature (SST) should be a good approximation to change of SAT in ice-free ocean areas; climate model simulations [Hansen et al., 2007] suggest that long-term SAT change over ice-free ocean is only slightly larger than SST change. However, ocean water temperature does not go below the freezing point of water, while surface air temperature over sea ice can be much colder. As a result, SST change underestimates SAT change when sea ice cover changes. Indeed, most climate models find that the largest SAT changes with global warming occur in regions of sea ice [IPCC, 2007] . Thus, we estimate SAT changes in sea ice regions by extrapolating actual SAT measurements on nearby land or islands; if there are no stations within 1200 km, we leave the temperature change undefined."}, {"section_title": "URBAN ADJUSTMENTS", "text": "[29] A major concern about the accuracy of analyses of global temperature change has long been the fact that many of the stations are located in or near urban areas. Humanmade structures and energy sources can cause a substantial local warming that affects measurements in the urban environment. This local warming must be eliminated to obtain a valid measure of global climate change. Global temperature analyses now routinely either omit urban stations or adjust their long-term trends to try to eliminate or minimize the urban effect. A comprehensive review of the topic is provided by Parker [2010] .\n[30] The urban influence on long-term global temperature change is generally found to be small. It is possible that the overall small urban effect is, in part, a consequence of partial cancellation of urban warming and urban cooling effects. A significant urban cooling can occur, for example, if a station is moved from central city to an airport and if the new station continues to be reported with the same station number and is not treated properly as a separate station in the global analysis.\n[31] Global satellite measurements of night lights allow the possibility for an additional check on the magnitude of the urban influence on global temperature analyses. We describe in this section a procedure in which all stations located in areas with night light brightness exceeding a value (32 mW m \u22122 sr \u22121 mm \u22121 ) that approximately divides the stations into two categories: rural and urban or periurban [Imhoff et al., 1997] . The standard GISS global temperature analysis now adjusts the long-term trends of stations located in regions with night light brightness exceeding this limit to agree with the long-term trend of nearby rural stations.\nIf there are not a sufficient number of nearby rural stations, the \"bright\" station is excluded from the analysis.\n[32] We present evidence here that the urban warming has little effect on our standard global temperature analysis. However, in Appendix A we carry out an even more rigorous test. We show there that there are a sufficient number of stations located in \"pitch black\" regions, i.e., regions with brightness below the satellite's detectability limit (\u223c1 mW m \u22122 sr \u22121 mm \u22121 ), to allow global analysis with only the stations in pitch black regions defining long-term trends. The effect of this more stringent definition of rural areas on analyzed global temperature change is immeasurably small (<0.01\u00b0C century \u22121 ). The finding of a negligible effect in this test (using only stations in pitch black areas) also addresses, to a substantial degree, the question of whether movement of weather stations to airports has an important effect on analyzed global temperature change. The pitch black requirement eliminates not only urban and periurban stations but also three quarters of the stations in the more than 500 GHCN records that are identified as airports in the station name. (The fact that one quarter of the airports are pitch black suggests that they are in extreme rural areas and are shut down during the night.)\n[33] Station location in the meteorological data records is provided with a resolution of 0.01 degrees of latitude and longitude, corresponding to a distance of about 1 km. This resolution is useful for investigating urban effects on regional atmospheric temperature. Much higher resolution would be needed to check for local problems with the placement of thermometers relative to possible building obstructions, for example. In many cases such local problems are handled via site inspections and reported in the \"metadata\" that accompanies station records, as discussed by Karl ; and (c) a region shown at the data resolution of 0.0083\u00b0\u00d7 0.0083\u00b0 [Imhoff et al., 1997] . Light blue area is that seen as \"pitch dark\" from the satellite, i.e., with radiance less than 1 mW m \u22122 sr \u22121 mm \u22121 ; the green area is \"dark,\" between 1 and 32 mW m \u22122 sr \u22121 mm \u22121 . Blue dots, green triangles, and red asterisks indicate meteorological stations associated with towns having populations less than 10,000, between 10,000 and 50,000, and greater than 50,000, respectively. In our new standard night light treatment stations in the yellow and pink regions are adjusted for urban effects.\n[34] Of course, the thousands of meteorological station records include many with uncorrected problems. The effect of these problems tends to be reduced by the fact that they include errors of both signs. Also, problems are usually greater in urban environments or associated with movement of stations from urban areas. An urban adjustment based on night lights should tend to reduce the effect of otherwise unnoticed nonclimatic effects, and our check that the use of only pitch black stations to set long-term temperature trends (see Appendix A) yields a practically identical result to that of our standard analysis adds further confidence in the result.\n[35] We use a night light radiance data set [Imhoff et al., 1997] that is publicly available (http://data.giss.nasa. gov/gistemp/sources/wrld-radiance.tar (measurements made between March 1996 and February 1997)) at a resolution of 30\u2033 \u00d7 30\u2033 (0.0083\u00b0\u00d7 0.0083\u00b0), which is a linear scale of about 1 km. In Figure 1a the global radiances are shown after averaging to 0.5\u00b0\u00d7 0.5\u00b0resolution; that is, a pixel (resolution element) in Figure 1a is an average over 3600 high-resolution pixels. This averaging reduces the maximum radiance from about 3000 mW m \u22122 sr et al. [1997] investigated the relation between night light radiances and population density in the United States. We find that radiances <32 mW m \u22122 sr\ncorrespond well with the \"unlit\" or \"dark\" category of Imhoff et al. [1997] within the United States, as can be seen by comparing Figure 1b here with Hansen et al. [2001, Plate 1] . The \"unlit\" regions, according to Imhoff et al. [1997] , correspond to population densities of about 0.1 persons ha \u22121 or less in the United States.\n[37] The relation between population and night light radiance in the United States is not valid in the rest of the world as energy use per capita is higher in the United States than in most countries. However, energy use is probably a better metric than population for estimating urban influence, so we employ night light radiance of 32 mW m \u22122 sr\nas the dividing point between rural and urban areas in our global night light test of urban effects. Below we show, using data for regions with very dense station coverage, that use of a much more stringent criterion for darkness does not significantly alter the results.\n[38] We first compare two alternatives for the urban correction. One case uses the definition of rural stations used by Hansen et al. [1999] , i.e., stations associated with towns of population less than 10,000 (population data available at ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/v2). The second case defines rural stations as those located in a region with night light radiance less than 32 mW m \u22122 sr \u22121 mm \u22121 . This night light criterion is stricter than the population criterion in the United States; that is, many sites classified as rural on the basis of population below 10,000 are classified as urban on the basis of night light brightness, as shown by the fact that some of the blue dots (towns of population below 10,000) in Figure 1c fall within the urban areas defined by night lights (yellow area). However, as we will see, the opposite is true in places such as Africa; that is, a population criterion of less than 10,000 results in fewer rural stations than the night light criterion.\n[39] Our adjustment of urban station records uses nearby rural stations to define the long-term trends while allowing the local urban station to define high-frequency variations, nominally as described by Hansen et al. [1999] but with details as follows. The reason to retain the urban record is to increase the sample for high-frequency data and because urban stations often extend a bit further toward earlier times. The adjustment to the urban record is a two-segment broken line that approximates the difference between the averaged regional rural record and the urban record. It is computed as by Hansen et al. [1999] , but the knee of the broken line is variable (rather than being fixed at 1950), chosen so as to optimize the approximation. If there are not at least three rural stations to define the regional trend, the urban station is dropped.\n[40] Figure 2 shows the resulting temperature change over the period 1900-2009 for urban adjustment based on night lights, urban adjustment based on population, and no urban adjustment. The effect of urban adjustment on global temperature change is only of the order of 0.01\u00b0C for either night light or population adjustment. The small magnitude of the urban effect is consistent with results found by others [Karl et al., 1988; Jones et al., 1990; Peterson et al., 1999; Peterson, 2003; Parker, 2004] . Our additional check is useful, however, because of the simple reproducible way that night lights define rural areas. We previously used this night light method [Hansen et al., 2001] but only for the contiguous United States.\n[41] The most noticeable effect of the urban adjustment in Figure 2 is in Africa and includes changes of both signs. The large local changes are due not to addition of an urban correction to specific stations but rather to the deletion of urban stations because of the absence of three rural neighbors. African station records are especially sparse and unreliable [Peterson et al., 1998b; Christy et al., 2009] . Thus, the large local temperature changes between one adjustment and another may have more to do with variations in station reliability rather than urban warming. Given the small number of long station records in Africa and South America [Peterson and Vose, 1997] , a single bad station record can affect a noticeable area in a map of temperature change. However, even though the paucity of station records in Africa and South America makes the results less reliable there, the general consistency between land and nearby ocean temperature patterns suggests that the principal trends in Africa and South America are real.\n[42] Figure 3 compares the global mean temperature versus time for the two alternative urban adjustments and no urban adjustment. The main conclusion to be drawn is that the differences among the three curves are small. Nevertheless, we know that the adjustment is substantial for some urban stations, so it is appropriate to include an urban adjustment.\n[43] How can we judge whether the night light or population adjustment is better in the sense of yielding the most realistic result? One criterion might be based on which one yields more realistic continuous meteorological patterns for temperature anomaly patterns. Night lights arguably do very slightly better on the basis of that criterion ( Figure 2 ). Independently, we expect night light intensity to be a better indication than population of urban heat generation. Night lights also preserve a greater area with defined temperature anomalies ( Figure 2 ). Finally, stations can be more accurately associated with night light intensity (within about 1 km) than with population, and the night light data are easily accessible, so anyone can check our analysis.\n[44] For these reasons, beginning in January 2010 the standard GISS analysis employs global night lights in choosing stations to be adjusted for urban effects. Use of night lights is a well-defined objective approach for urban adjustment, and the night light data set that we use (http://data.giss.nasa.gov/ gistemp/sources/wrld-radiance.tar) is readily available. In the future, as additional urban areas develop, it will be useful to employ newer satellite measurements.\n[45] The small urban correction is somewhat surprising, even though it is consistent with prior studies. We know, for example, that urban effects of several degrees exist in some cities such as Tokyo, Japan, and Phoenix, Arizona, as illustrated by Hansen et al. [1999, Figure 3 ]. Although such stations are adjusted in the GISS analysis, is it possible that our \"rural\" stations themselves contain substantial humanmade warming? There is at least one region, the United States, where we can do a stricter test of urban warming because of the high density of meteorological stations. The United States is a good place to search for greater urban effects because of its high energy use and a consequent expectation of large urban effects.\n[46] In Figures 4 and 5 we compare no adjustment, population adjustment, and two night light adjustments. The standard night light adjustment defines rural stations as those with night light radiance < 32 mW m \u22122 sr \u22121 mm \u22121 . The strict night light adjustment defines rural stations as those that are pitch black (radiance < 1 mW m \u22122 sr \u22121 mm \u22121 ). There are about 300 pitch black stations in the contiguous United States (Figure 1b) , sufficient to yield a filled in United States temperature anomaly map even with the station radius of influence set at 250 km. We use the 250 km radius of influence to provide higher resolution in Figure 4 , compared with 1200 km radius of influence, and we exclude smoothing from the plotting package so that results are shown at the 2\u00b0\u00d7 2\u00b0r esolution of the calculation, thus allowing more quantitative inspection.\n[47] The largest urban adjustment is in the southwest United States, where a warming bias is removed. In a few locations the adjustment yields greater warming, which can result from either the spatial smoothing inherent in adjusting local trends to match several neighboring stations or neighboring rural stations that have greater warming than the urban station. The standard night light adjustment removes [48] In Appendix A we show that pitch dark stations have sufficient global distribution to allow comparison of temperature change analysis pegged to dark stations (our standard analysis) with analysis pegged to pitch dark stations. Results are practically identical, offering further evidence that urban effects have a negligible effect on our global analysis.\n[49] Global temperature change in this paper, unless indicated otherwise, is based on the standard night light adjustment. We conclude, on the basis of results reported here and the other papers we referenced, that unaccounted for urban effects on global temperature change are small in comparison to the \u223c0.8\u00b0C global warming of the past century. Extensive confirmatory evidence (such as glacier retreat and borehole temperature profiles) is provided by IPCC [2007] ."}, {"section_title": "ALTERNATIVE OCEAN DATA SETS", "text": "[50] Yasunaka and Hanawa [2010] compare several data sets for sea surface temperature. They find reasonably good agreement among the data sets, ascribing differences to the use of different methods to interpolate between measurements, instrumental bias correction methods, treatments of satellite-derived temperatures, and other factors. The Global Climate Observing System SST Working Group has an intercomparison Web site (http://ghrsst.nodc.noaa. gov/intercomp.html) that allows comparison of SST data sets. A peer-reviewed Community White Paper (N. A. Rayner et al., available at http://www.oceanobs09.net/blog/?p=121) discusses changing biases in the SST data sets and some reasons for differences among them. A paper describing the differences in greater detail and recommending adjustments is in preparation (J. Kennedy, manuscript in preparation, 2010) .\n[51] Here we illustrate how our analysis of global temperature change is altered by the choice of SST data set. 2002] for 1982 to the present, (2) ERSST.v3b [Smith et al., 2008] for the full period 1880 to the present, and (3) HadISST1 for . In all three cases the land data are based on the GISS analysis of GHCN and Antarctic (SCAR) data.\n[52] The first of the ocean data sets, the combination HadISST1 plus OISST.v2, is used in the GISS analysis for our standard land-ocean temperature index. Results based on HadISST1 alone and HadISST1 plus OISST.v2 are in close agreement in the 1982 to the present period during which they might differ (Figure 6b ). We use HadISST1 with concatenated OISST.v2 for our monthly updates because OISST is available in near real time.\n[53] ERSST, a newer SST analysis covering the period 1880 to the present, is also available in near real time. SST values in data-sparse regions in ERSST are filled in by NCDC using statistical methods, dividing the SST anomaly patterns into low-frequency (decadal-scale) anomalies and high-frequency residual anomalies by averaging and filtering available data points [Smith et al., 2008] . The concept is that the SST reconstruction may be improved by constraining temperature anomaly fields toward realistic modes of variability. (HadISST1 also uses a reconstruction approach, while HadSST2 defines temperature only in grid boxes with actual observations.)\n[54] A newer Met Office Hadley Centre SST data set, HadSST2 [Rayner et al., 2006] , has cooler SSTs in 1908-1912, comparable to the lower temperatures in ERSST. Both HadSST2 and ERSST use newer versions of the International Comprehensive Ocean-Atmosphere Data Set (ICOADS [Worley et al., 2005] ). Presumably, the newer ICOADS data set is superior as it is based on more data with better geographical coverage. However, the HadSST2 resolution (5\u00b0\u00d7 5\u00b0) is too crude for our purposes. HadISST1 and ERSST resolutions are 1\u00b0\u00d7 1\u00b0and 2\u00b0\u00d7 2\u00b0.\n[ [56] Figure 7 illustrates the geographical distribution of the differences between ERSST and the ocean data used in the current standard GISS analysis (HadISST1 plus OISST). It is apparent that the greater warming in ERSST on the century time scale occurs primarily in the South Pacific and South Atlantic oceans. The difference between the two reconstructions is large enough in some parts of the Pacific Ocean, of the order of 1\u00b0C, that it may be possible to discriminate between them on the basis of focused examination of all available data for those regions. Even proxy temperature measures, e.g., from corals [Beck et al., 1992] , have the potential to resolve differences of that magnitude.\n[57] In recent years, as shown for 2000-2008 in Figure 7 (bottom right), the greater warming in ERSST occurs especially in the eastern Pacific Ocean. Both data sets include use of in situ data, so it is unclear why the difference is large in this region. HadISST1 plus OISST has been used in the standard GISS global temperature analysis for the past decade. However, we now also make a second analysis, ERSST plus optimum interpolation (OI), using a new procedure for concatenating satellite data with presatellite era data. This new procedure is designed to incorporate (in the satellite era) the merits of both the high-resolution satellite data and the greater stability of modern in situ measurements.\n[58] The new concatenation procedure allows the in situ data to appropriately constrain the long-term temperature trend, but the full information from the high spatial resolution satellite data is also incorporated. The merit of these characteristics is suggested by the following facts: (1) the satellite SST measurements have a bias in their trend because of effects of aerosols and clouds that cannot be fully removed [Reynolds et al., , 2010 and (2) found that HadISST1 plus OISST provides much better agreement than ERSST with high temporal resolution in situ temperature measurements made at research sites in the North Atlantic Ocean.\n[59] Our new concatenation procedure compares the temperature anomalies in ERSST and OISST each month at all grid boxes containing in situ measurements (as judged by the HadSST2 uninterpolated gridded data). The OISST anomaly is then adjusted by the difference between the ERSST and OISST anomalies averaged over the area containing in situ measurements (the adjustment is a single global number each month). This adjusted OISST data thus provides a high spatial resolution SST data set from which the small cold bias described by Reynolds et al. [2002 Reynolds et al. [ , 2010 has been removed in global mean.\n[60] We anticipate that this new concatenation procedure yields a better data product for the satellite era (1992 to the present) than either unadjusted OISST data or SST data sets reconstructed from only in situ measurements (reconstruction refers to methods, such as employed by ERSST and HadISST1, in which a complete ocean SST data set is defined from irregularly spaced in situ data by constraining temperature anomaly fields toward realistic modes of variability). Validation with high-frequency in situ observations at many locations, such as the data at research stations in North Atlantic Ocean [Hughes et al., 2009] , is needed to judge the merits of the resulting data set.\n[61] Concerning the presatellite era, it is not clear which data set is most useful for our purposes. None of the available data sets corrects as yet for the possible data inhomogeneity at the end of World War II [Thompson et al., 2008] , a matter that will need assessment when proposed adjustments have been made available. Until there is a demonstrably superior ocean data set, we will retain HadISST1 plus OISST (concatenated as in our analyses for the past several years) in our standard analysis. However, we also make available ERSST plus OI with the new concatenation procedure. As expected, the global mean temperature trend using ERSST plus OI with the new concatenation is practically the same as obtained with ERSST employed for the entire period 1880 to the present (Figure 6 ).\n[62] We compare alternative ocean data sets in more detail in Appendix A. Note that differences among the data sets are less than uncertainties estimated by the data providers. Also note that the differences are small enough that the choice of ocean data set does not alter the conclusions drawn in this paper about global temperature change."}, {"section_title": "CURRENT GISS SURFACE TEMPERATURE ANALYSIS", "text": "[63] The results in this section use HadISST1 plus OISST with the switch to OI in 1982. A smooth concatenation is achieved by making the 1982-1992 OISST mean anomaly at each grid box identical to the HadISST1 1982-1992 mean for that grid box. Results with our ERSST plus OI analysis are very similar but with slightly larger global warming. Both results are available on our Web site.\n[64] Figure 8 shows the global surface temperature anomalies for the past 4 decades, relative to the 1951-1980 base period. On average, successive decades warmed by 0.17\u00b0C. The warming of the 1990s (0.13\u00b0C relative to the 1980s) was reduced by the temporary effect of the 1991 Mount Pinatubo volcanic eruption. (El Chichon, in the prior decade, produced a global average aerosol optical depth only about half as large as Pinatubo.)\n[65] Warming in these recent decades is larger over land than over ocean, as expected for a forced climate change Figure 7 . Temperature change in the GISS global analysis using ERSST and HadISST1 plus OISST and differences in specific periods. Numbers in the top right corners are global means.\n[ Hansen et al., 2007; IPCC, 2007; Sutton et al., 2007] , in part because the ocean responds more slowly than the land because of the ocean's large thermal inertia. Warming during the past decade is enhanced, relative to the global mean warming, by about 50% in the United States, a factor of 2-3 in Eurasia, and a factor of 3-4 in the Arctic and the Antarctic Peninsula.\n[66] Warming of the ocean surface has been largest over the Arctic Ocean, second largest over the Indian and western Pacific oceans, and third largest over most of the Atlantic Ocean. Temperature changes have been small and variable in sign over the North Pacific Ocean, the Southern Ocean, and the regions of upwelling off the west coast of South America.\n[67] Figure 9a updates the GISS global annual and 5 year mean temperatures through 2009. Results differ slightly from our prior papers because of our present use of the global night lights to adjust for urban effects, but the changes are practically imperceptible. The night light adjustment reduces the 1880-2009 global temperature change by an insignificant 0.004\u00b0C relative to the prior population-based urban adjustment. Global temperature in the past decade was about 0.8\u00b0C warmer than at the beginning of the 20th century (1880-1920 mean) . Two thirds of the warming has occurred since 1975.\n[68] Figure 9a has become popular, eagerly awaited by some members of the public and the media. An analogous graph, often in the form of a histogram, is made available each year by the Met Office Hadley Centre/University of [69] We suggest, however, that a more informative and convenient graph is the simple 12 month running mean global temperature (Figure 9b) . From a climate standpoint there is nothing special about the time of year at which the calendar begins. The 12 month running mean removes the seasonal cycle just as well at any time of year. Note that the use of temperature anomalies itself does not fully remove the seasonal cycle: substantial seasonality remains, with global temperature variability and average temperature trends larger in Northern Hemisphere winter than in Northern Hemisphere summer. This residual seasonality is removed via the 12 month (or n \u00d7 12 month) running mean.\n[70] The 12 month running mean temperature anomaly (Figure 9b ) provides an improved measure of the strength and duration of El Ni\u00f1os, La Ni\u00f1as, and the response to volcanic eruptions. In contrast, use of the calendar year, as in Figure 9a , can be misleading because one El Ni\u00f1o may coincide well with a calendar year while another is split between two calendar years. The 12 month running mean also provides a better measure of the longevity of an event (a positive or negative temperature excursion).\n[71] We note that more sophisticated filters are useful for specific purposes. Also, if our flat (equal weight) running mean were used for frequency analyses it could have undesirable effects. However, there is no need for it to be used for that purpose as full resolution data are available from our Web site. We suggest only that the simple 12 month running mean provides a powerful and intuitive visual presentation that usefully complements the usual annual mean graph, and it is probably the best filter for use in communication with the public. But we do not discourage others from using our full resolution data with more sophisticated filters and spectral analyses, despite the shortness of the record, its nonuniformity over time, and the sorry record of spectral studies for such a short period in revealing anything new about the physical system.\n[72] A clearer view of temporal variations is provided by Figure 10 , which covers the shorter period 1950-2010. Figure 10a shows the monthly mean global temperature anomaly, and Figure 10b shows the 12 month running mean. The red-blue Ni\u00f1o index in Figure 10c is the 12 month running mean of the temperature anomaly (relative to averaged over the Ni\u00f1o 3.4 area (longitudes 120\u00b0W-170\u00b0W and latitudes 5\u00b0N-5\u00b0S) in the eastern Pacific Ocean [Philander, 2006] . Because the monthly Ni\u00f1o index is much smoother than monthly global temperature, we can usefully extend the Ni\u00f1o index to the present. The final five points in the Ni\u00f1o 3.4 curve are 10, 8, 6, 4, and 2 month means.\n[73] The well-known strong correlation of global surface temperature with the Ni\u00f1o index [Foster et al., 2010; Thompson et al., 2009 ] is apparent in Figure 10 . The correlation is maximum (at 30%) with 12 month running mean global temperature lagging the Ni\u00f1o index by 4 months. Global cooling due to large volcanoes in 1963 (Agung), 1982 (El Chichon), and 1991 (Pinatubo) is also apparent.\n[74] The data in [75] As for calendar year 2010, the first half of the year is warm enough that the 2010 global surface temperature in the GISS analysis likely will be a record for the period of instrumental data, or at least so close to the prior warmest year (2005) that it must be declared in a statistical dead heat. It is still conceivable that record global temperature for the calendar year will not occur, if tropical SSTs deteriorate rapidly into a very deep La Ni\u00f1a in the latter half of 2010."}, {"section_title": "COMPARISON OF GISS, NCDC, AND HadCRUT ANALYSES", "text": "[76] The likelihood of a record global temperature in the GISS analysis for 2010 raises the question about differences among the several global surface temperature analyses. For example, GISS and NCDC have 2005 as the warmest year in their analyses, while HadCRUT has 1998 as the warmest year. Here we investigate differences arising from two factors that are likely to be important: (1) the way that temperature anomalies are extrapolated, or not extrapolated, into regions without observing stations and (2) the ocean data sets that are employed.\n[77] Figure 11 compares the GISS, NCDC, and HadCRUT analyses. The characteristic causing most interest and concern in the media and with the public is their different results for the warmest year in the record, as noted above. A likely explanation for discrepancy in identification of the warmest year is the fact that the HadCRUT analysis excludes much of the Arctic, where warming has been especially large in the past decade, while GISS and NCDC estimate temperature anomalies throughout most of the Arctic. The difference between GISS and HadCRUT results can be investigated quantitatively using available data defining the area that is included in the HadCRUT analysis.\n[78] Figure 12 shows maps of GISS and HadCRUT 1998 and 2005 temperature anomalies relative to base period (the base period used by HadCRUT). The temperature anomalies are at a 5\u00b0\u00d7 5\u00b0(latitude-longitude) resolution in Figure 12 for the GISS data to match the resolution of the HadCRUT analysis. In Figure 12 (bottom) we display the GISS data masked to the same area (and resolution) as the HadCRUT analysis.\n[79] Figure 13 shows time series of global temperature for the GISS and HadCRUT analyses, as well as for the GISS analysis masked to the HadCRUT data region. With the analyses limited to the same area, the GISS and HadCRUT results are similar. The GISS analysis finds 1998 as the warmest year, if analysis is limited to the masked area. Figure 13 reveals that the differences that have developed between the GISS and HadCRUT global temperatures during the past decade are due primarily to the extension of the GISS analysis into regions excluded from the HadCRUT analysis.\n[80] The question is then, how valid are the extrapolations and interpolations in the GISS analysis? The GISS analysis assigns a temperature anomaly to many grid boxes that do not contain measurement data, specifically all grid boxes located within 1200 km of one or more stations that do have defined temperature anomalies. The rationale for this aspect of the GISS analysis is based on the fact that temperature anomaly patterns tend to be large scale, especially at middle and high latitudes.\n[81] The HadCRUT analysis also makes an (implicit) assumption about temperature anomalies in regions remote from meteorological stations, if the HadCRUT result is taken as a global analysis. The HadCRUT approach areaweights temperature anomalies of the regions in each hemisphere that have observations; then the means in each hemisphere are weighted equally to define the global result . Thus, HadCRUT implicitly assumes that areas without observations have a temperature anomaly equal to the hemispheric mean anomaly. Given the pattern of large temperature anomalies in the fringe Arctic areas with data (Figure 12 ), this implicit estimate surely understates the effect of Arctic temperature anomalies.\n[82] Qualitative support for the greater Arctic anomaly of the GISS analysis is provided by Arctic temperature anomaly patterns in the GISS analysis: regions warmer or cooler than average when the mean anomaly is adjusted to zero are realistic-looking meteorological patterns. More quantitative support is provided by satellite observations of infrared radiation from the Arctic [Comiso, 2006 [Comiso, , 2010 . Although we have not yet attempted to integrate this infrared data record, which begins in 1981, into our temperature record, the temperature anomaly maps of Comiso [2006 Comiso [ , 2010 have the largest positive temperature anomalies (several degrees Celsius) during the first decade of this century over the interior of Greenland and over the Arctic Ocean in regions where sea ice cover has decreased. Because there are no weather stations in central Greenland with long records or within the sea ice region, our analysis may understate warming in these regions. An exception is the station on Sakhalin Island, which is located in a region of decreasing sea ice cover and which does show relatively large warming in the past decade.\n[83] Comiso [2010] uses advanced very high resolution radiometer data because they provide the longest available satellite record. However, ongoing satellite observations with the Moderate Resolution Infrared Spectrometer are also available (http://neo.sci.gsfc.nasa.gov/Search.html?datasetId= MOD_LSTAD_M) and provide another opportunity to check the validity of our extrapolations into regions without observing stations. Cursory preliminary examination of data for Greenland suggests that Greenland temperature anomalies may be larger on that continent than those that we obtain via extrapolation from surface observation sites.\n[84] An independent method to extrapolate into regions without measurement stations is to use a global model in \"reanalysis\" mode, i.e., to insert available meteorological observations into the model, which then produces complete global \"data\" fields. Simmons et al. [2010] have carried out such a reanalysis study, concluding that 2005 was likely the warmest year in the record.\n[85] Another analysis aimed specifically at evaluating the magnitude of Arctic warming in the past decade has been carried out by P. Chylek et al. (How large is recent Arctic warming?, manuscript in preparation, 2010) using only surface air temperature measurements at meteorological stations. They find about 0.25\u00b0C less Arctic warming during the past decade than in the GISS analysis, a difference that they attribute to our method of interpolating and extrapolating data, especially into the Arctic Ocean regions where no station data are available. We agree with Chylek et al. (manuscript in preparation, 2010 ) that the Arctic temperature change is uncertain and must be regarded with caution, but we make two observations. First, the Arctic covers about 4% of the global area, so an overestimate of its warming by 0.25\u00b0C would cause an overestimate of global temperature by only about 0.01\u00b0C, and it would affect neighboring years as well as 2005, so it would not alter our conclusion about 2005 being the warmest year. Second, regions in the Arctic Ocean that changed from sea ice covered to open water in the past decade likely experienced a large increase in surface air temperature that would not be captured fully in an analysis using only land-based meteorological stations. Also, greater warming in the interior of Greenland would not be captured because there are no long-record interior stations in the analyses. Thus, although it is possible that the GISS analysis overstates the magnitude of Arctic warming in regions where data are extrapolated, it is also possible that the Chylek et al. (manuscript in preparation, 2010) analysis underestimates the warming in the Arctic Ocean.\n[86] Inadequate spatial coverage of stations is just one of the significant sources of uncertainty in global temperature change [Karl et al., 1994; Jones and Wigley, 2010] , but it is especially important in comparing nearby years, in contrast with slowly changing factors such as urban warming. We obtain a quantitative estimate of uncertainty (likely error) in the GISS global analysis due to incomplete spatial coverage of stations using a time series of global surface temperature generated by a long run of the GISS climate model runs [Hansen et al., 2007] . We sample this data set at meteorological station locations that existed at several times during the past century. We then find the average error when the model's data for each of these station distributions are used as input to the GISS surface temperature analysis program.\n[87] Table 1 shows the derived error. As expected, the error is larger at early dates when station coverage was poorer. Also, the error is much larger when data are available only from meteorological stations, without ship or satellite measurements for ocean areas. In recent decades the 2s uncertainty (95% confidence of being within that range, thus \u223c2%-3% chance of being outside that range in a specific direction) has been about 0.05\u00b0C. Incomplete coverage of stations is the primary cause of uncertainty in comparing nearby years, for which the effect of more systematic errors such as urban warming is small.\n[88] Additional sources of error, including urban effects, become important when comparing temperature anomalies separated by longer periods Folland et al., 2001; Smith and Reynolds, 2002] . Hansen et al. [2006] estimated the additional error, by factors other than incomplete spatial coverage, as being s \u2248 0.1\u00b0C on time scales of several decades to a century, but that estimate is necessarily partly subjective. If that estimate is realistic, the total uncertainty in global mean temperature anomaly with land and ocean data included is similar to the error estimate in the first row of Table 1 , i.e., the error due to limited spatial coverage when only meteorological stations are available. However, biases due to changing practices in ocean measurements may cause even greater uncertainty on a century time scale [Rayner et al., 2006] .\n[89] Occasionally, measurement biases might change rapidly, as has been suggested for SSTs right after World War II. However, in most cases the error in comparing nearby years to each other is probably dominated by the substantial error from incomplete spatial coverage of measurements, rather than changes of measurement practices or urban warming. Under that assumption, let us consider whether we can specify a rank among the recent global annual temperatures, i.e., which year is warmest, second warmest, etc. Figure 9a shows 2009 as the second warmest year, but it is so close to 1998, 2002, 2003, 2006, and 2007 that we must declare these years as being in a virtual tie as the second warmest year. The maximum difference among these years in the GISS analysis is \u223c0.03\u00b0C (2009 being the warmest among those years and 2006 being the coolest). This total range is approximately equal to our 1s uncertainty of \u223c0.025\u00b0C.\n[90] Year 2005 is 0.06\u00b0C warmer than 1998 in the GISS analysis. How certain is it that 2005 was warmer than 1998? Given s \u2248 0.025\u00b0C for nearby years, we estimate the chance that 1998 was warmer than 2005 as follows. Actual 1998 and 2005 temperatures are specified by normal probability distributions about our calculated values. For each value of actual 1998 temperature there is a portion of the probability function for the 2005 temperature that has 2005 cooler than 1998. Integrating successively over the two distributions we find that the chance that 1998 was warmer than 2005 is 0.05; that is, there is 95% confidence that 2005 was warmer than 1998.\n[91] The NCDC analysis finds 2005 to be the warmest year but by a smaller amount. Thus, a similar probability calculation for their results would estimate a greater chance that 1998 was actually warmer than 2005. NCDC reports 2009 as being the fifth warmest year (http://www.ncdc.noaa. gov/sotc/?report=global). Although the latter result seems to disagree with the GISS conclusion that 2009 tied for the second warmest year, this is mainly a consequence of the GISS preference to describe as statistical ties those years with global temperature differing by only about one standard deviation or less."}, {"section_title": "WEATHER VARIABILITY VERSUS CLIMATE TRENDS", "text": "[92] Public opinion about climate change is affected by recent and ongoing weather. North America had a cool summer in 2009, perhaps the largest negative temperature anomaly on the planet (Figure 14a ). Northern Hemisphere winter (December, January, and February) of 2009-2010 was unusually cool in the United States and northern Eurasia (Figure 14b ). The cool weather contributed to increased public skepticism about the concept of \"global warming,\" especially in the United States. These regional extremes occurred despite the fact that June, July, and August 2009 were second warmest (behind June, July, and August 1998) and December, January, and February 2009-2010 were second warmest (behind December, January, and February 2006-2007) .\n[93] Northern Hemisphere winter of 2009-2010 was characterized by an unusual exchange of polar and midlatitude air. Arctic air rushed into both North America and Eurasia and, of course, was replaced in the polar region by air from middle latitudes. Penetration of Arctic air into middle latitudes is related to the Arctic Oscillation (AO) index [Thompson and Wallace, 2000] , which is defined by surface atmospheric pressure. When the AO index (Figure 15a) is positive, surface pressure is low in the polar region. This helps the middle-latitude jet stream blow strongly and consistently from west to east, thus keeping cold Arctic air locked in the polar region. A negative AO index indicates relatively high pressure in the polar region, which favors weaker zonal winds and greater movement of frigid polar air into middle latitudes.\n[94] December 2009 had the most extreme negative Arctic Oscillation since the 1970s. There were ten cases between the early 1960s and mid 1980s with negative AO index more extreme than \u22122.5 but no such extreme cases since then until December 2009. It is no wonder that the public had become accustomed to a reduction in extreme winter cold air blasts. Then, on the heels of the December 2009 anomaly, February 2010 had an even more extreme AO (Figure 15 ), the most negative AO index in the record that extends back to 1821 (http://www.cru.uea.ac.uk/\u223ctimo/ datapages/naoi.htm).\n[95] The AO index and United States surface temperature anomalies are shown with monthly resolution in Figure 15b for two 6 year periods: the most recent years (2005) (2006) (2007) (2008) (2009) (2010) and the period (1975) (1976) (1977) (1978) (1979) (1980) just prior to the rapid global warming of the past 3 decades. Extreme negative temperature anomalies, when they occur, are usually in a winter month. Note that winter cold anomalies in the late 1970s were more extreme than the recent winter cold anomalies.\n[96] Maps of monthly temperature anomalies are shown in Figure 16 . Figure 16a is a polar projection for 24\u00b0N-90\u00b0N with 1200 km smoothing to provide full coverage, while 250 km resolution without further smoothing is possible for the contiguous United States (Figure 16b ) because of high station density there. Monthly mean negative anomalies exceeded 5\u00b0C over large areas in the cold months of the late 1970s, while negative temperature excursions are more limited in 2009-2010. [97] Monthly temperature anomalies in the United States and Europe in winter are positively correlated with the AO index (Figures 15 and 17) , with any lag between the index and temperature less than the monthly temporal resolution (Figure 15 ). The correlation of temperature anomalies and the AO index is especially large (correlation coefficient 61%) for Europe in winter. Winter correlation for the United States is 41%. The correlation in summer is 29% for the United States and 25% for Europe.\n[98] Thompson and Wallace [2000] , Shindell et al. [2001] , and others point out that increasing carbon dioxide causes the stratosphere to cool, in turn causing on average a stronger polar jet stream and thus a tendency for a more positive Arctic Oscillation. There is an AO tendency of the expected sense (Figure 15 ), but the change is too weak to account for the temperature trend. Indeed, Figure 17 shows that the warming trend of the past few decades has led to mostly positive seasonal temperature anomalies, even when the AO is negative. In the United States 16 of the past 20 winters and 15 of the past 20 summers were warmer than the 1951-1980 climatology, a frequency consistent with the expected \"loading of the climate dice\" [Hansen, 1997] due to global warming. Notable change of these probabilities is a result of the fact that local seasonal mean temperature change due to long-term trends (Figure 18 ) is now comparable to the magnitude of local interannual variability of seasonal mean temperature (Figure 14b) .\n[99] Such change of probability of a warm season is typical of the rest of the world. Figures 17c and 17d compare the AO index with temperature anomalies over \"Europe,\" where, for convenience, we have approximated European borders by [100] Monthly temperature anomalies are typically 1.5-2 times greater than seasonal anomalies. So loading of the climate dice is not as easy to notice in monthly mean temperature. Daily weather fluctuations are even much larger than global mean warming. Yet it is already possible for an astute observer to detect the effect of global warming in daily data by comparing the frequency of days with record warm temperature to days with record cold temperature. The number of days with record high temperature now exceed the number of days with record cold by about a 2:1 ratio [Meehl et al., 2009] ."}, {"section_title": "DATA AND ANALYSIS FLAWS", "text": "[101] Figure 19 shows the effect of an error that came into the GISS analysis with the changes in the analysis described by Hansen et al. [2001] . One of the changes was use of an improved version of the USHCN station data records including adjustments developed by NCDC to correct for station moves and other discontinuities [Easterling et al., 1996] . Our error was failure to recognize that the updates to the records for these stations obtained from NCDC electronically each month did not contain these adjustments. Thus, there was a discontinuity in 2000 in those station records as prior years contained the adjustment while later years did not.\n[102] The error was readily corrected, once it was recognized and we were notified, which occurred in 2007. Figure 19 shows the global and United States temperatures with and without the error. The error averaged 0.15\u00b0C over the contiguous 48 states, which is detectable in Figure 19b but much smaller than the 1\u00b0C-2\u00b0C interannual and interdecadal variations of United States temperature. Because the 48 states cover only about 1 1/2 percent of the globe, the error in global temperature was about 0.003\u00b0C, which is insignificant and undetectable in Figure 19a .\n[103] This error was widely reported in the media, frequently with the assertion that NASA had intentionally exaggerated the magnitude of global warming and with a further assertion that correction of the error made 1934 the warmest year in the record rather than 1998 (http:// www.rushlimbaugh.com/home/daily/site_080907/content/ 01125106.guest.html). Initial confusion between global and United States temperature was conceivably inadver- tent, but attempts to correct this misstatement were ineffectual. As Figure 19b shows, the reality is that United States temperatures in 1934 and 1998 (and 2006) were and continue to be too similar to conclude that one year was warmer than the other.\n[104] Uncertainty in comparing United States temperatures for years separated by more than half a century is larger than 0.1\u00b0C. As shown in section 4, the GISS adjustment for urban effects in the United States by itself approaches that magnitude, even when the adjustment is made on USHCN records that are already adjusted by NCDC to account for several sources of bias (inhomogeneity). The NCDC pairwise comparison of urban and rural stations is expected to remove much of the urban effect [Menne et al., 2009] .\n[105] Temperature records in the United States are especially prone to uncertainty, not only because of high energy use in the United States but also because of other unique problems such as the bias due to systematic change in the time at which observers read 24 h maximum-minimum thermometers. These problems and adjustments to minimize their effect have been described in numerous papers by NCDC researchers [Karl et al., 1986; Karl and Williams, 1987; Karl et al., 1988 Karl et al., , 1989 Quayle et al., 1991; Easterling et al., 1996; Peterson and Vose, 1997; Peterson et al., 1998a Peterson et al., , 1998b .\n[106] When alterations, improvements, or adjustments occur in any of the input data streams (from meteorological stations, ocean measurements, or Antarctic research stations), the results of the GISS global temperature analysis change accordingly. Monthly updates of the GHCN (including USHCN) data records include not only an additional month of data but late station reports for previous months and sometimes corrections of earlier data. Thus, slight changes in the GISS analysis can occur every month, but these changes are small in comparison with the global and United States temperature changes of the past century or the past 3 decades.\n[107] Occasionally, changes of input data occur that are detectable in graphs of the data. This has occurred especially for United States meteorological station data as NCDC has worked to improve the quality and homogeneity of these records. Figure 20 illustrates the effect of changes in the USHCN data that occurred when we switched (in November 2009) from USHCN version 1 to USHCN version 2, the latter being a NCDC update of their homogenization of USHCN stations. The effect of this revision of USHCN data is noticeable in the United States temperature (Figure 20 ), but it is small compared to century time scale changes. The effect on global temperature is imperceptible, of the order of a thousandth of a degree.\n[108] On the basis of our experience with the data flaw illustrated in Figure 19 , we made two changes to our procedure. First, we now (since April 2008) save the complete input data records that we receive from the three near-real-time data sources every month. Although the three input data streams are publicly available from their sources and a record is presumably maintained by the providing organizations, they are now also available from GISS. Second, we published the computer program used for our temperature analysis, making it available on our Web site.\n[109] An additional data flaw occurred in November 2008. Although the flaw was only present in our data set for a few days (10-13 November), it resulted in additional lessons learned. The GHCN records for many Russian stations for November 2008 were inadvertently a repeat of October 2008 data. The GHCN records are not our data, but we properly had to accept the blame for the error because the data were used in our analysis. Occasional flaws in input data are normal, and the flaws are eventually noticed and corrected if they are substantial. We have an effective working relationship with NCDC, reporting to them questionable data that we or our data users discover.\n[110] This specific data flaw was a case in point. The quality control program that NCDC runs on the data from global meteorological stations includes a check for repetition of data: if two consecutive months have identical data, the data are compared with those at the nearest stations. If it appears that the repetition is likely to be an error, the data are eliminated until the original data source has verified the data. The problem in the November 2008 data evaded this quality check because a change in their computer program inadvertently bypassed that quality check.\n[111] This data flaw led to another round of fraud accusations on talk shows and other media, which was another lesson learned. Since then, to minimize misinformation, we first put our monthly analyzed data up on a site that is not visible to the public. This allows several scientists to examine graphs of the data for potential flaws. If anything seems questionable, we report it back to the data providers for their resolution. This process can delay availability of our data analysis to users for up to several days and has resulted in a criticism that we now \"hide\" our data.\n[112] It should be noted that the data flaws discussed here are all either minor or temporary or both and they do not compromise the integrity of the overall data product. The flaws are important, though, and efforts must be made to minimize them because they can be used to cast doubt on the entire scientific enterprise.\n[113] It is impossible to entirely eliminate data flaws or satisfy all conflicting user demands. We believe that the steps we take now to check the data are a good compromise between assurance of data integrity and prompt availability and also reasonable from the standpoint of the use of our time and resources. But we continue to seek ways to improve the data, we welcome user suggestions, and we appreciate it when problems are brought to our attention directly."}, {"section_title": "SUMMARY DISCUSSION", "text": "[114] Human-made climate change has become an issue of surpassing importance to humanity [Hansen, 2009] , and global warming is the first-order manifestation of increasing greenhouse gases that are predicted to drive climate change.\nThus, it is understandable that analyses of ongoing global temperature change are now subject to increasing scrutiny and criticisms that are different than would occur for a purely scientific problem.\n[115] Our comments here about communication of this climate change science to the public are our opinion. Other people may have quite different opinions. We offer our opinion because it seems inappropriate to ignore the vast range of claims appearing in the media and in hopes that open discussion of these matters may help people distinguish the reality of global change sooner than would otherwise be the case. However, these comments, even though based on experience over a few decades, are only opinion. Our primary contribution is quantitative results discussed in the numbered paragraphs below.\n[116] Communication of the status of global warming to the public has always been hampered by weather variability. Laypeople's perception tends to be strongly influenced by the latest local fluctuation. This difficulty can be alleviated by stressing the need to focus on the frequency and magnitude of warm and cold anomalies, which change noticeably on decadal time scales as global warming increases.\n[117] Other obstacles to public communication include the media's difficulty in framing long-term problems as \"news,\" a preference for sensationalism, a generally low level of familiarity with basic science, and a preference for \"balance\" in every story. The difficulties are compounded by the politicization of reporting of global warming, a perhaps inevitable consequence of economic and social implications of efforts required to alter the course of human-made climate change.\n[118] The task of alleviating the communication obstacle posed by politicization is formidable, and it is made more difficult by attacks on the character and credibility of scientists that the attacks have been effective in causing many members of the public to doubt the reality or seriousness of global warming (http://www.gallup.com/poll/126560/americansglobal-warming-concerns-continue-drop.aspx).\n[119] Given this situation, the best hope may be repeated clear description of the science and passage of sufficient time to confirm validity of the description. A problem with that prescription is the danger that the climate system could pass tipping points that cause major climate changes to proceed largely out of humanity's control [Hansen et al., 2008] . Yet continuation of careful scientific description of ongoing climate change seems to be essential for the sake of minimizing the degree of future climate change, even while other ways are sought to draw attention to the dangers of continued greenhouse gas increases.\n[120] One lesson we have learned is that making our global data analysis immediately available, with data use by ourselves and others helping to reveal flaws in the input data, has a practical disadvantage: it allows any data flaws to be interpreted and misrepresented as machinations. Yet the data are too useful for scientific studies to be kept under wraps, so we will continue to make the data available on a monthly basis. But we are making special efforts to make the process as transparent as possible, including availability of the computer program that does the analysis, the data that go into the analysis (also available from original sources), and detailed definition of urban adjustment of meteorological station data.\n[121] Our principal task remains the scientific one: describing ongoing global temperature change with as much clarity and insight as we can. Contributions of the present paper include the following:\n[122] 1. The paper provides insight into why the GISS analysis yields 2005 as the warmest calendar year, while the HadCRUT analysis has 1998 as the warmest year. The main factor is our inclusion of estimated temperature change for the Arctic region. We note that SST change cannot be used as a measure of surface air temperature change in regions of sea ice and that surface air temperature change is the quantity of interest both for its practical importance to humans and for comparison with the results that are usually reported in global climate model studies.\n[123] 2. Twelve-month (and n \u00d7 12 month) running mean temperatures provide more information than the usual graphs with calendar year mean temperature. The magnitude and duration of global temperature effects of volcanoes and the Southern Oscillation can be seen much more clearly in a 12 month running mean graph such as Figure 10 . The simplicity of the running mean, compared to filtered time series, is helpful for public communications.\n[124] 3. The 12 month running mean global temperature in the GISS analysis has reached a new record in 2010. The new record temperature in 2010 is particularly meaningful because it occurs when the recent minimum of solar irradiance [Fr\u00f6hlich, 2006] (data at http://www.pmodwrc.ch/ pmod.php?topic=tsi/composite/SolarConstant) is having its maximum cooling effect. At the time of this writing (July 2010) the tropical Pacific Ocean is changing from El Ni\u00f1o to La Ni\u00f1a conditions in the tropical Pacific Ocean. It is likely that global temperature for calendar year 2010 will reach a record level for the period of instrumental data, but that is not certain if La Ni\u00f1a conditions deepen rapidly.\n[125] 4. The cool weather anomalies in the United States in June, July, and August 2009 and in both the United States and northern Eurasia in the following December, January, and February are close to the cool extreme of the range of seasonal temperatures that are now expected (Figure 17) given the warming of the past few decades. Although comparably cool conditions could occur again sometime during the next several years, the likelihood of such event is low in any given year, and it will continue to decrease as global warming continues to increase.\n[126] 5. We suggest a new procedure for use of satellite SST data that takes advantage of the high spatial resolution and broad coverage of satellite observations but avoids the bias in the temperature trend in satellite data [Reynolds et al., , 2010 . We adjust the satellite data by a small constant such that the monthly temperature anomalies of satellite and Figure 21 . The 60 month and 132 month running means using data through June 2010 for two alternative choices for the ocean data set.\nin situ data are equal over their common area. This procedure is used in our current ERSST plus OI analysis. We continue to also provide our HadISST1 plus OI analysis, without such adjustment, as our standard data product. Because of a cold bias in unadjusted OI data, global warming in ERSST plus OI exceeds that in HadISST1 plus OI by about 0.04\u00b0C in 2010. Further study is needed to verify which of these data products is superior. Other improvements of the ocean data sets may become available in the near future. For example, none of the publicly available global data sets corrects as yet for a discontinuity in ocean data that has been suggested to exist near the end of World War II [Thompson et al., 2008] . However, note that none of these adjustments or uncertainties is large enough to alter any of our major conclusions in this paper.\n[127] 6. Global warming on decadal time scales is continuing without letup. Figure 8 , showing decadal mean temperature anomalies, effectively illustrates the monotonic and substantial warming that is occurring on decadal time scales. But because it is important to draw attention to change as soon as possible, we need ways to make the data trends clear without waiting for additional decades to pass. Figure 21 shows the 60 month (5 year) and 132 month (11 year) running means of global temperature. The 5 year mean is sufficient to minimize ENSO variability, while the 11 year mean also minimizes the effect of solar variability. Figure 21 gives the lie to the frequent assertion that \"global warming stopped in 1998.\" Of course it is possible to find almost any trend for a limited period via judicious choice of start and end dates of a data set that has high temporal resolution, but that is not a meaningful exercise. Even a more moderate assessment [Solomon et al., 2010 [Solomon et al., , p. 1219 , \"the trend in global surface temperature has been nearly flat since the late 1990s despite continuing increases in the forcing due to the sum of the well-mixed greenhouse gases,\" is not supported by our data. On the contrary, we conclude that there has been no reduction in the global warming trend of 0.15\u00b0C-0.20\u00b0C per decade that began in the late 1970s."}, {"section_title": "APPENDIX A", "text": "[128] Figure A1a shows the global distribution of pitch dark stations (night light radiance less than 1 mW m \u22122 sr \u22121 mm ). Figure A1b compares the analyzed global temperature change for the case of our standard night light adjustment and the case in which pitch dark stations are used to adjust the long-term trend of all other stations. As Figure A1b shows, adjustment using only pitch dark stations has very little effect on the result. Indeed, the global mean warming is slightly larger (by 0.01\u00b0C) using the stricter night light adjustment. Conceivably, the slight warming is a result of the fact that the pitch dark requirement removes about three quarters of the airport stations from those used. However, we have not investigated which specific stations cause the slight change in Figure A1 because the change is negligible in comparison with the total temperature change and its uncertainty.\n[129] The global temperature change obtained in our analysis depends on the ocean surface temperature data set (s) that we employ. We compare here the HadISST1, ERSST, and HadSST2 data sets, as well as the data sets that result when OI satellite data are concatenated with HadISST1 or ERSST. Here the OI concatenation procedure is the same Figure A2 . (left) Ocean surface temperatures for alternative data sets, the temperature zero point in all cases being the 1951-1980 (base period) mean. (right) OI is concatenated by equating its mean for 1982-1992 with the mean of the appended data set for the same period. (top) Only areas that have data in all data sets are used; the temperature anomaly is averaged over this area, so it is not a true global ocean mean. (bottom) The global ocean mean obtained for each of these data sets, as described in the text.\nfor all data sets, the OI mean for 1982-1992 being equated to the mean of the other data set for the same period.\n[130] Figure A2 compares the HadISST1, HadSST2, and ERSST data sets after spatial averaging. Figure A2 (top) shows the temperature anomaly averaged over only those regions where all three records have data. Figure A2 (bottom) gives estimates of the global ocean mean anomaly, obtained as follows: monthly SSTs are interpolated to 5\u00b0\u00d7 5\u00b0grid (HadSST2 grid), mean anomalies are computed for the permanently ice-free ocean area with defined SST within each of four latitude zones (90\u00b0S-25\u00b0S, 25\u00b0S-0\u00b0S, 0\u00b0N-25\u00b0N, and 25\u00b0N-90\u00b0N), and the global ocean mean is computed as the average of these four zones with each zone weighted by the open ocean area of that zone.\n[131] Variations among the resulting data sets are within the expected uncertainty ranges [Folland et al., 2001; Rayner et al., 2006; Reynolds et al., 2002; Smith et al., 2008] . It is notable that the largest differences occur in the past decade, when the most comprehensive observations exist. Figure A3 shows the geographical distribution of the differences between the data sets.\n[132] The largest difference between HadSST2 and HadISST1, as well as the largest difference between ERSST and HadISST1, occurs in the upwelling region just west of South America. Presumably, this change from the earlier HadISST1 is a result of the more comprehensive data available from ICOADS release 2.1 [Worley et al., 2005] . However, HadSST2 and ERSST are also warmer than HadISST1 during the past decade throughout most of the global ocean.\nThe ubiquity of the recent differences suggests that they may be related to calibration of satellite measurements, which are a major data source in the past decade.\n[133] The effect on the GISS analysis of global temperature change caused by alternative choices for the ocean data record is reduced by the fixed contributions from meteorological stations on continents and islands, including extrapolation into the Arctic. Figure 6 compares the global temperature records that result with HadISST1 plus OI and ERSST plus OI, the differences being as much as several hundredths of a degree Celsius.\n[134] The standard GISS global analysis uses the concatenated HadISST1 plus OISST data set, as described in the main text. Any of the alternative ocean data sets described here would yield slightly greater global warming, both in recent decades and on the century time scale.\n[135] Until improved assessments of the alternative SST data sets exist, the GISS global analysis will be made available for both HadISST1 plus OISST and ERSST plus OISST. The HadISST1 concatenation with OISST will continue to be based on equating means for 1982-1992, as it always has been in the standard GISS analysis. The ERSST plus OISST concatenation will be as described in the main text of this paper. HadISST1 plus OISST will continue to be our standard product unless and until verifications show ERSST plus OISST to be superior.\n[136] Figure A4 investigates a charge that has been bruited about frequently in the past year, specifically the claim that GISS has systematically reduced the number of stations used in its temperature analysis so as to introduce an artificial global warming. GISS uses all of the GHCN stations that are available, but the number of reporting meteorological stations in 2009 was only 2490, compared to \u223c6300 usable stations in the entire 130 year GHCN record. The reduced number of stations is, in part, a consequence of the fact that not all stations report in near real time, so their data may be included in future GHCN records. However, Figure A4 shows that the additional stations will not make much difference. When the GISS analysis uses only stations reporting in 2009, the temperature curve obtained for recent years is almost identical to curve obtained using all 6300 stations.\n[137] ACKNOWLEDGMENTS. We thank Wayne Hamilton, Phil Jones, Tom Karl, Nick Rayner, Dick Reynolds, Alan Robock, and Gavin Schmidt for helpful suggestions on a draft version of the paper.\n[138] The Editor responsible for this paper was Alan Robock.\nHe thanks three reviewers, Phil Jones, Mark Moldwin, and Thomas Karl. "}]