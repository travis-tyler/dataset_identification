[{"section_title": "Abstract", "text": "In medical studies, endpoints are often measured for each patient longitudinally. The mixed-effects model has been a useful tool for the analysis of such data. There are situations in which the parameters of the model are subject to some restrictions or constraints. For example, in hearing loss studies, we expect hearing to deteriorate with time. This means that hearing thresholds which reflect hearing acuity will, on average, increase over time. Therefore, the regression coefficients associated with the mean effect of time on hearing ability will be constrained. Such constraints should be accounted for in the analysis. We propose maximum likelihood estimation procedures, based on the expectation-conditional maximization either algorithm, to estimate the parameters of the model while accounting for the constraints on them. The proposed methods improve, in terms of mean square error, on the unconstrained estimators. In some settings, the improvement may be substantial. Hypotheses testing procedures that incorporate the constraints are developed. Specifically, likelihood ratio, Wald, and score tests are proposed and investigated. Their empirical significance levels and power are studied using simulations. It is shown that incorporating the constraints improves the mean squared error of the estimates and the power of the tests. These improvements may be substantial. The methodology is used to analyze a hearing loss study."}, {"section_title": "INTRODUCTION", "text": "Occupational hearing loss is known to be a leading cause of work related disability with a broad negative impact on the quality of life. Moreover, hearing loss often impedes communication and thus further contributes to other safety hazards in, and outside of, the workplace. A deeper understanding of the factors influencing the development of noise-induced hearing loss and its time course will hopefully aid the public health community in designing more effective interventions (Carel and Brinker, 2005) .\nHearing loss research focuses on the way in which hearing thresholds evolve over time, that is, longitudinally. Hearing acuity is modeled, for each frequency under consideration, as a function of occupational exposure, age, gender, and other covariates. Since hearing thresholds are recorded at occasions that may 328 O. DAVIDOV AND S. ROSEN not be common to all study participants the resulting data is often unbalanced with unequal time intervals between exams. It is also important to note that hearing loss patterns vary among individuals (Morrell and Brant, 1991) , and that hearing thresholds are measured with considerable noise (Carel and Brinker, 2005) . Data of this kind has often been analyzed using linear mixed-effects models (Laird and Ware, 1982) . Within this framework, individual variability is modeled by incorporating subject-specific random effects. Examples of this approach in the current application are Morrell and others (1997) , Verbeke and others (2001) , Seixas and others (2005) , and Skotgstad and others (2009) . In particular, Morrell and Brant (1991) fit a linear mixed model and studied the effect of both age at entry and time in the study for a cohort of 268 males participating in the Baltimore Longitudinal Study of Aging (BLSA). The subjects in this study were all over the age of 70 and had between 1-12 visits. Both cross-sectional as well as longitudinal differences were analyzed. Pearson and others (1995) also used data from the BLSA. Their study population consisted of 681 males ages 20-90 who were followed for up to 23 years, and 416 females followed for up to 13 years. A series of, rather complex, linear mixed models with higher-order effects were fit to this data. The results show that in general hearing sensitivity declines with time among both men and women. Surprisingly, they found that the mean hearing sensitivity improved among women under age 60 for frequencies between 1 and 4 kHz. This finding is somewhat controversial because it is believed that hearing loss is an irreversible biological process. Therefore, it is natural to assume that the mean hearing threshold increases over time. In practice, this amounts to imposing constraints on the parameters of the model. Such constraints will likely improve the fit of the model. However, placing such restriction complicates the associated inferential procedures. In this communication, we develop the methodology for constrained estimation and testing in longitudinal mixed models.\nEstimation and testing problems for longitudinal data with inequality constraints have been investigated by several authors. Shin and others (1996) proposed a likelihood ratio (LR) test for ordered group effects for correlated normal data. Peddada and others (2005) address a similar problem using different methods. Both, however, model group means and do not incorporate covariates. Pilla and others (2006) proposed a quadratic score test to detect order among treatment effects for general correlated data in a model that includes covariates but not random effects. Silvapulle (1997) showed that some mixed linear models can be reduced to fixed-effect models where the usual one-sided tests for ordered hypothesis can be applied. When this approach is applicable, the asymptotic null distribution of the test statistic is chi-bar-squared with weights that do not depend on the unknown variance components. Tan and others (2005) develop a method to analyze dose-response relations for repeated measures data arising in tumor xenograft experiments. They fit a linear regression model assuming the components of the intercept \u03b1 \u03b1 \u03b1 = (\u03b1 1 , . . . , \u03b1 m ) T are ordered by the simple ordering, that is, \u03b1 1 \u03b1 2 \u2022 \u2022 \u2022 \u03b1 m , and showed that ignoring this restriction may result in misleading inferences, usually an underestimate of the treatment effect. Fang and others (2006) proposed a modified expectation-maximization (EM) algorithm for deriving the maximum likelihood (ML) estimator in a multivariate random-effects model which imposed constraints on the intercepts. This model was used to assess radiation therapy for pediatric brain tumors. Other papers dealing with multivariate normal observations with restricted means are Perlman (1969) , Wang and Mcdermott (1998a,b) , Perlman and Wu (2002) , and Silvapulle (1996) . These papers, however, do not explicitly discuss random effects. Finally, we mention the papers by Neelon and Dunson (2004) and Cai and Dunson (2007) that fit flexible nonlinear and nonparametric, regression models with order restriction within the Bayesian paradigm.\nIn this paper, we propose 3 algorithms based on the expectation-conditional maximization either (ECME) algorithm to estimate the regression parameters of a mixed-effects model while accounting for the constraints on them. The constrained estimators are shown to be consistent. Simulations show that they improve on the unconstrained estimators in terms of the mean square error (MSE). In some situations, the improvement is significant. In addition LR, Wald, and score tests for the constrained hypotheses are considered. Their empirical significance levels and power are studied using simulations. It is shown Constrained inference in mixed-effects models 329 that accounting for the constraint improves the power of the tests. The methodology we propose is general and can be used in any longitudinal setting and not only in the context of hearing loss data.\nThe rest of this paper is organized as follows. Section 2 introduces the mixed-effects model with linear inequality constraint. In Section 3, we propose 3 algorithms for ML estimation of the model parameters using the ECME algorithm. In Section 4, LR, Wald, and score tests are considered. In Section 5, simulation results are presented. Section 6 applies the proposed methods to a real hearing loss data set. We conclude, in Section 7, with a short discussion."}, {"section_title": "FORMULATION AND SOME NOTATION", "text": "Let y T i = (y i1 , . . . , y in i ) denote the n i measurements taken on the ith individual, i = 1, . . . , N . It is often assumed (e.g. Diggle and others, 1994 or Verbeke and Molenberghs, 2000 ) that the linear model\nholds. Note that X i and Z i are n i \u00d7 p and n i \u00d7 d known design matrices. The vector \u03b2 \u03b2 \u03b2 is a p \u00d7 1 vector of unknown population parameters called the fixed effects. The vector b i is a d\u00d71 vector of unknown subjectspecific effects called the random effects. The vector e i is an n i \u00d7 1 vector representing the random error. It is assumed that e i are independent of each other and are distributed as N (0, \u03c3 2 I n i ). The random effects are distributed as N (0, D), independently of each other and of the e i . Here, D is d \u00d7 d positive definite covariance matrix. The model (2.1) is referred to as a mixed-effects model. It is worth mentioning that longitudinal data can also be analyzed using alternative methods such as generalized estimating equations or more generally quasi likelihood methods (Pilla and others, 2006) . These methodologies are especially effective for nonnormal data. In many practical settings, the fixed effects in (2.1) may be subject to constraints. The constraints typically reflect prior information about the value of the parameters. For example, in hearing loss studies, hearing acuity is known to deteriorate with time. This translates to having positive regression coefficients associated with the fixed effect of time on hearing thresholds. As noted by an associate editor, some individuals may have trajectories which differ from the average behavior in the population. In fact, there are individuals in our sample whose hearing seems to improve over time. This phenomena has been observed by other researchers (e.g. Pearson and others, 1995; Morrell and others, 1997) . The reasons for this phenomena is not clear. It is known from the medical literature that some exposures to noise lead to what is called \"temporary threshold shift,\" that is, a temporary but recoverable loss of hearing. This type of hearing loss may be what some subject in our sample is experiencing. Another possibility proposed by several authors is that some \"learning\" is taking place (Pearson and others, 1995) . Finally, it is likely that large measurement errors and high variability among individuals are driving the observed phenomena. It is, however, important to emphasize that the linear mixed-effects model is consistent with the observed data.\nFormally, we incorporate the constraints by assuming that A\u03b2 \u03b2 \u03b2 0, where A is a k \u00d7 p matrix of full row rank, and the inequality x 0 is interpreted coordinate-wise, that is, x 1 0, x 2 0, . . . , x k 0. The main goals of this paper is to develop estimation and testing procedures for the model (2.1) assuming the parameters satisfy some linear constraints as described above."}, {"section_title": "CONSTRAINED ESTIMATION", "text": "The likelihood function for the mixed-effects model (2.1) is where V i = Z i D Z T i + \u03c3 2 I n i and \u03c3 2 and D are usually assumed to be unknown. Let \u03b2 \u03b2 \u03b2, \u03c3 2 , and D denote the unconstrained ML estimators which are often computed using an ECME algorithm. The ECME algorithm is a variant of the EM algorithm (Mclachlan and Krishnan, 1997) in which the M-step is decomposed into a series of conditional maximizations (CMs) steps. A brief description follows. Let y = (y T 1 , . . . , y T N ) T be the observed data. We take the unobserved data to be b\nT , that is, we are thinking of the random effects as the missing data.\nThe likelihood for the observed data, L o (\u03b8 \u03b8 \u03b8), is given by (3.1). The likelihood for the complete data is given, up to a constant, by\nInitialize D = D 0 and \u03c3 2 = \u03c3 2 0 and maximize (3.1) to obtain \u03b2 \u03b2 \u03b2 0 . The ECME iterates between its E and M steps. In the (k + 1)th iteration of the E-step, the expectation\nis the estimated parameter after the kth iteration. This reduces to computing\n, the expected values of the sufficient statistics of the missing data. The maximization of Q(\u03b8 \u03b8 \u03b8 |\u03b8 \u03b8 \u03b8 k ) in (k + 1)th iteration is carried out using 2 CM steps. In the first CM-step, D k+1 and \u03c3 2 k+1 are found by maximizing Q(\u03c3 2 , D, \u03b2 \u03b2 \u03b2 k |\u03b8 \u03b8 \u03b8 k ) with respect to \u03c3 2 and D. In the second CM-step, \u03b2 \u03b2 \u03b2 k+1 is found by maximizing the observed data likelihood, L o (\u03c3 2 k+1 , D k+1 , \u03b2 \u03b2 \u03b2), see (3.1), with respect to \u03b2 \u03b2 \u03b2. The E-step as well as both CM-steps have closed-form solutions. The algorithm iterates until some stopping rule is attained. For more details and explicit formulas, see Laird and Ware (1982) or Liu and Rubin (1994) .\nConstrained estimation is, of course, more complicated compared with unconstrained estimation. Let \u03b2 \u03b2 \u03b2 denote the constrained MLE. First note that if \u03c3 2 and D were known then V i , i = 1, . . . , N , would also be known. In this situation, maximizing (3.1) subject to A\u03b2 \u03b2 \u03b2 0 is equivalent to finding\nThe minimization in (3.3) is a quadratic programing problem which can be readily solved using the quadprog package in R. However, in most applications, \u03c3 2 and D are unknown and therefore need to be estimated along with the regression coefficients. In the following, we propose 3, increasingly complicated, algorithms for constrained ML estimation of the model parameters.\nALGORITHM 3.1 First maximize the likelihood function (3.1) using the usual ECME to obtain MLs of \u03b2 \u03b2 \u03b2, \u03c3 2 , and D. Calculate V i for i = 1, . . . , N . Solve the quadratic optimization program in (3.3) assuming V i = V i . Denote the solution by \u03b2 \u03b2 \u03b2.\nAlgorithm 3.1 amounts to solving a constrained estimation problem using variance components estimated by the standard ECME algorithm. We note that the Algorithm 3.1 does not yield the MLE because the estimated variance components do not incorporate the constraints. Constrained inference in mixed-effects models 331 V i,2 . Resolve the quadratic optimization program in (3.3) assuming V i = V i,2 and denote the solution by \u03b2 \u03b2 \u03b2 2 . Repeat the step above. The process is stopped at the (k + 1)th iteration if \u03b2 \u03b2 \u03b2 k+1 \u2212 \u03b2 \u03b2 \u03b2 k < \u03b5 for some prespecified \u03b5 > 0 and norm \u2022 .\nIn contrast with Algorithm 3.1, Algorithm 3.2 outputs estimators of the variance components which incorporate the constraints. However, it is not an EM-type algorithm because the likelihood is not guaranteed to increase between iterations. ALGORITHM 3.3 Apply the usual ECME algorithm but perform the maximization in the second CM-step, that is, the maximization of\nsubject to the constraint that A\u03b2 \u03b2 \u03b2 0. Note that this maximization is again a quadratic programing problem.\nAlgorithm 3.3 is an EM-type algorithm. EM-type algorithms for constrained estimation problems have been investigated by several authors. Kim and Taylor (1995) considered equality constraints; Nettleton (1999) and Shi and others (2005) considered inequality constraints and proposed algorithms in the spirit of Algorithm 3.3, that is, their algorithms perform a constrained maximization within the M-step. They called these algorithms restricted EM algorithms. However, their algorithms are not directly designed to deal with random-effects models as are Algorithms 3.1-3.3 above. As noted by the associate editor, the MLEs can be found by direct maximization of (3.1) without resorting to the ECME. However, the ECME is particularly useful in this context because it allows for a relatively simple maximization of the likelihood in the presence of constraints. This is a great advantage.\nIt can be shown that for random-effect models discussed in this paper, all 3 algorithms above yield consistent estimators, THEOREM 3.4 Suppose that the model (2.1) and the assumptions in Section 2 hold. Then the estimators obtained by using the Algorithms 3.1-3.3 are all consistent.\nFor a proof see the Appendix. For some recent related results see Xu and Wang (2008) . The numerical performance of these methods in terms of the mean squared error are reported in Section 5. Performance is compared in terms of estimation of the fixed effects as well as that of the nuisance parameters which are also consistently estimated."}, {"section_title": "HYPOTHESIS TESTING", "text": "We consider testing hypothesis of the type H 0 : C\u03b2 \u03b2 \u03b2 = 0 versus H 1 : A\u03b2 \u03b2 \u03b2 0, (4.1)\nwhere C is k \u00d7 p matrix of full row rank that is partitioned as C = [A T B T ] T , where A is of order q \u00d7 p and B is of order (k \u2212 q) \u00d7 p and k q and q takes values in {0, . . . , p}. It is assumed that under H 1 at least one inequality in (4.1) is strict. For example, if C = A = I p , where I p is the p \u00d7 p identity matrix, then (4.1) may be rewritten as H 0 : \u03b2 \u03b2 \u03b2 = 0 versus H 1 : \u03b2 \u03b2 \u03b2 0, that is, under the null all fixed effects are zero, whereas under the alternative at least one fixed effect is strictly positive. This is the most common form of (4.1) in applications."}, {"section_title": "332", "text": "O. DAVIDOV AND S. ROSEN\nThe log-likelihood based on (3.1) may be rewritten as a function of \u03b2 \u03b2 \u03b2 and V and denoted l(\u03b2 \u03b2 \u03b2, V ).\nHere, y and X are obtained by stacking the vectors y i and matrices X i underneath each other and V is a block-diagonal matrix with blocks V i . Let V and \u03b2 \u03b2 \u03b2 be the MLEs under the null, and let V and \u03b2 \u03b2 \u03b2 be the MLE's under the alternative. Several test statistics can be used for testing ordered hypothesis of the form (4.1). The LR statistic is given by\n( 4.2) The global score test is given by\nwhere S(\u03b2 \u03b2 \u03b2,V ) = \u2207 \u03b2 l(\u03b2 \u03b2 \u03b2, V ) \u03b2 \u03b2 \u03b2,V and the Wald-type test is given by\nTheir asymptotic distribution is given in the following theorem proved in the Appendix, THEOREM 4.1 Suppose that the model (2.1) and the assumptions in Section 2 hold. Then under H 0 , the tests (4.2)-(4.4) are asymptotically equivalent, that is,\nwhere T is any of the above tests. In addition, the following bounds on (4.5) hold: The quantities w i = w i (q, , A), where = (X T V \u22121 X ) \u22121 is the true asymptotic variance of the unconstrained MLE, are known as the chi-bar weights or level probabilities. The weights can be computed exactly whenever q 3. For large values of q, the weights can be computed only approximately. For a thorough discussion on how to calculate the weights and the distribution of the above tests see Silvapulle and Sen (2004) or Davidov and others (2010) . It is important to note that all 3 tests are valid regardless of which of the Algorithms 3.1-3.3 are used.\nNote that the weights are functions of the unknown nuisance parameter V = V (\u03c3 2 , D). The bounds in (4.6) can be used to test (4.1), that is, reject H 0 if the upper bound is \u03b1; and do not reject H 0 if the lower bound is >\u03b1. Otherwise, the test is not conclusive. We have found that, at least in this application, the naive approach of substituting V = V works well even in small to moderate sample sizes (this is also true for V and V ). It is worth mentioning that several general approaches to dealing with unknown nuisance parameters in the context of order-restricted inference are reviewed in Silvapulle and Sen (2004) ."}, {"section_title": "SIMULATIONS RESULTS", "text": "All simulations are conducted using the model where \u03b2 \u03b2 \u03b2 T = (\u03b2 0 , \u03b2 1 , \u03b2 2 ) and b T i = (b i0 , b i1 ), i = 1, . . . , N , where N = 20 or 50. y T i = (y i1 , . . . , y i5 ) denote the 5 measurements taken on the ith individual and e T i = (e i1 , . . . , e i5 ). The jth row, j = 1, . . . , 5, in the X i and Z i matrices is X i j = (1, x 1i j , x 2i j ) and Z i j = (1, x 2i j ). We take x 1 to be a time-independent covariate, while x 2 is assumed to be time dependent. More specifically, we set x T 1i = (i, i, i, i, i) (e.g. x T 11 = (1, 1, 1, 1, 1 ), x T 12 = (2, 2, 2, 2, 2), and so on) and x T 2i = (0, 1, 2, 3, 4) for all i = 1, . . . , N . The errors e i are independent of each other and are distributed as N (0, I 5 ). The b i are distributed as N (0, D), independently of each other and of the e i , and D is 2\u00d72 positive definite covariance matrix with elements d kl = 1 if k = l and d kl = 1/2 if k = l (k, l = 1, 2). The regression coefficients were restricted to satisfy a positivity constraint, that is, \u03b2 1 0 and \u03b2 2 0.\nWe note that other values for \u03c3 2 and D as well as other covariate configurations were also explored with similar results."}, {"section_title": "Estimation", "text": "Let MSE 0 denote the MSE of the unconstrained estimators, and let MSE i , where i = 1, 2, 3 denote the MSE of the constrained estimators obtained by the Algorithms 3.1-3.3, respectively. The relative MSE of the ith algorithm for the jth parameter in the model is defined by\nfor i = 1, 2, 3 and j = 1, . . . , 7 (we include all regression parameters and variance components). The performance of the Algorithms 3.1-3.3 in terms of the relative MSE is presented in Table 1 .\nOur simulation results show that the proposed methods improve, in terms of the MSE, on the unconstrained estimators. The improvement is dramatic (up to 50%) for small values of the regression parameters. Note that parameter estimates of all 3 regression coefficients are improved, that is, even the estimate for the unconstrained intercept is improved but to a lesser degree (MSE reduction of up to 60%). Estimation of the variance components \u03c3 2 and D is hardly changed. Note that according to the relative MSE criterion the Algorithms 3.1-3.3 perform similarly.\nWe found, by tracking CPU times, that the 3 algorithms have comparable running times, at least for our unoptimized R code. In fact, since in the R system quadratic programs are solved very quickly We therefore recommend Algorithm 3.3 which yield the MLE for both the regression parameters and the variance components and converges monotonically. Simulations for larger sample sizes and different values for the parameters showed qualitatively similar results."}, {"section_title": "Testing", "text": "For simplicity set, \u03b2 0 = 0 in (5.1) and consider testing where \u03b2 \u03b2 \u03b2 T = (\u03b2 0 , \u03b2 1 , \u03b2 2 ) and A = 0 1 0 0 0 1 .\nWe use Algorithm 3.3 for estimation of the model parameters. Table 2 presents empirical significance levels, estimated using 10 5 replications, for testing (5.2) for different sample sizes. When calculating the weights in (4.5), we substitute V = V . Our simulation shows that the empirical significance levels of all tests approach the nominal levels as the sample size grows. This, of course, is consistent with the fact that the tests are asymptotically equivalent. The LR and Wald tests have empirical significance levels which are much higher than the nominal ones. This is true both for the unconstrained and the constrained tests. Note that the constrained tests seems to be performing a bit better (they are closer to the nominal levels). The empirical significance levels for the global score tests (unconstrained and constrained) are lower than the nominal level.\nFinally, we compare by simulation the power of LR, Wald, and score tests. It is clear from Table 2 that the empirical and nominal significance levels may differ substantially. To make the comparisons meaningful, we adjusted the nominal significance level to ensure that the empirical significance level will be exactly 0.05 for all n. Results, estimated using 10 5 replications, are presented in Table 3 .\nSimulation results show that incorporating the constraints improves the power of the tests, especially in small and medium sample sizes. Improvement is substantial of up to 50% in situations where the power is low. constrained LR, Wald, and global score tests have comparable power.\nRecall that in the unconstrained settings one can use the score test by fitting the model only under the null. This is not the case in constrained models. Nevertheless, we still recommend using the score test. The score test is slightly conservative but its (actual) level is closer to the nominal one compared with the other tests investigated. This seems to be true especially for small sample sizes where differences may be large. In addition, its power is comparable to the anti-conservative Wald and LR tests. In this section, we analyze a small data set collected by the Israeli Ministry of Health as part of a larger study. The data consist of 45 males without prior hearing impairments who were exposed, occupationally, to noise levels above 85 dB. The subjects, aged 22-58 at entry to the study, were followed between 2.15 to 24.7 years. The number of repeated hearing tests ranges between 3 and 10. The hearing threshold level was recorded in dB as measured by a pure-tone audiogram at 7 different frequency levels (varying from 0.5 to 8 kHz) for both right and left ears, yielding a maximum of 14 observations per visit. Hearing threshold data for the right ear at 1 kHz is displayed in Figure 1 . Note that the measured hearing thresholds increases for most but not all subjects in the study.\nThe following mixed-effects model is fitted\nwhere \u03b2 \u03b2 \u03b2 T = (\u03b2 0 , \u03b2 1 , \u03b2 2 ) denote the vector of fixed effects and b T i = (b i0 , b i1 ), i = 1, . . . , 45, denote the vector of random effects. y T i = (y i1 , . . . , y in i ) denote the n i hearing thresholds taken on the ith individual and e T i = (e i1 , . . . , e in i ) is the random error. The jth row, j = 1, . . . , n i , in the X i and Z i matrices is X i j = (1, age i1 , t i j ) and Z i j = (1, t i j ), where age i1 is the age (in years) of the individual at entry into the study and t i j is the time the ith individual has been in study when the jth test is performed. Note that age 1 models the cross-sectional differences in hearing thresholds of individuals of different ages, while the time term models the longitudinal changes in the thresholds of each individual. Since we expect the hearing thresholds to increase on average over time, we constraint the regression coefficient which is associated with time \u03b2 2 0. We implement Algorithm (3.3) to obtain regression parameters and variance components estimates and test all hypothesis using a score test. Let \u03b2 \u03b2 \u03b2 T = (\u03b2 0 , \u03b2 1 , \u03b2 2 ). We start by testing the hypothesis that H 0 : \u03b2 \u03b2 \u03b2 = 0 versus H 1 : (0, 0, 1)\u03b2 \u03b2 \u03b2 0 and then test for every parameter separately, that is,\nThe results are displayed in Table 4 .\nNote that the components of \u03b2 \u03b2 \u03b2 are all positive, hence, the constrained and unconstrained estimates agree. The estimates for the variance components are \u03c3 2 = 23.29, d 11 = 31.52, d 12 = \u22121.54, and d 22 = 0.91. Although the restricted estimate equals the unrestricted ones, the p-values obtained are different. Using an unconstrained test, one would conclude that time does not affect hearing thresholds, whereas the constrained model detects a significant time effect. The results suggest that the hearing thresholds increase over time with an average rate of 0.51 dB per year."}, {"section_title": "DISCUSSION", "text": "Situations in which the parameters of a model are subject to some restrictions or constraints arise naturally in applied statistical research. For example, in hearing loss studies, we expect hearing to deteriorate on average with time and therefore, the regression coefficients associated with the effect of time on hearing ability will be constrained. Such constraints should be accounted for in the analysis.\nHere, methodology for constrained inference in linear mixed-effects models for longitudinal data is developed. Three Algorithms 3.1-3.3, based on an ECME algorithm, for estimating the regression parameters assuming linear constraints are proposed and investigated. It is shown that all 3 algorithms lead to consistent estimators of the true value of the regression parameter provided the constraint holds. We recommend using Algorithms 3.3 because it is an EM-type algorithm (i.e. it converges monotonically to the true value) and is not much more difficult to implement relative to the simpler Algorithms 3.1-3.2. The proposed methods improve in terms of MSE on the unconstrained estimators. In some settings, the improvement may be substantial. For example, for values of the parameters close to the boundary of the parameter space up to 50% improvement was observed. We also found that constraints improved the estimation of the unconstrained parameters with MSE reduction of up to 60%. Hypotheses testing procedures that incorporate the constraints are also developed. Specifically, LR, Wald, and score tests are proposed and investigated. The tests are shown to be asymptotically equivalent. In practice, we find that the score test (4.3) performs best. Its level is the closest to the nominal level and its power is similar to the LR and Wald tests. It is shown that incorporating the constraints improves the mean squared error of the estimates and the power of the tests. These improvements may be substantial, of up to 50% in some settings. The methodology is used to analyze a small hearing loss study, where the effect of time is shown to be significant only if one accounts for the constraint.\nFinally, we note that the methodology we propose is general and can be used in any longitudinal setting and not only in the context of hearing loss data. Moreover, it can be extended readily to restricted maximum likelihood estimation (REML) (the difference between ML and REML comes in the E-step, for details see Laird and Ware, 1982; Laird and others, 1987) and nonlinear constraints. where \u03b2 \u03b2 \u03b2 is the unconstrained estimator. Hence, T LR follows what is known as a chi-bar distribution with tail probabilities given by (4.5), see Shapiro (1988) . Consider now the Wald statistic and note that by the continuous mapping theorem, we have\nwhere we emphasize that \u03b2 \u03b2 \u03b2 = V ( \u03b2 \u03b2 \u03b2|A) ) ) is the V \u2212projection of \u03b2 \u03b2 \u03b2. Thus, T LR = T W + o p (1) follows from Gourieroux and others (1982) . Finally, note that the global score test is A bit of algebra and the consistency of \u03b2 \u03b2 \u03b2 show that T GS = T GS + o p (1) as required. The bounds given in (4.6) follow from Silvapulle and Sen (2004) , see page 80. The lower bound is attained when w 0 = w 1 = 1/2 and the upper bound when w q\u22121 = w q = 1/2."}]