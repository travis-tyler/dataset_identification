[{"section_title": "Abstract", "text": "Early diagnosis, playing an important role in preventing progress and treating the Alzheimer's disease (AD), is based on classification of features extracted from brain images. The features have to accurately capture main AD-related variations of anatomical brain structures, such as, e.g., ventricles size, hippocampus shape, cortical thickness, and brain volume. This paper proposed to predict the AD with a deep 3D convolutional neural network (3D-CNN), which can learn generic features capturing AD biomarkers and adapt to different domain datasets. The 3D-CNN is built upon a 3D convolutional autoencoder, which is pre-trained to capture anatomical shape variations in structural brain MRI scans. Fully connected upper layers of the 3D-CNN are then fine-tuned for each task-specific AD classification. Experiments on the CADDementia MRI dataset with no skull-stripping preprocessing have shown our 3D-CNN outperforms several conventional classifiers by accuracy. Abilities of the 3D-CNN to generalize the features learnt and adapt to other domains have been validated on the ADNI dataset.\nIndex Terms-Alzheimer's disease, deep learning, 3D convolutional neural network, autoencoder, brain MRI"}, {"section_title": "INTRODUCTION", "text": "Alzheimer's disease (AD) is a progressive brain disorder and the most common case of dementia in the late life. AD leads to the death of nerve cells and tissue loss throughout the brain, thus reducing the brain volume in size dramatically through time and affecting most of its functions [1] . The estimated number of affected people will double for the next two decades, so that one out of 85 persons will have the AD by 2050 [2] . Because the cost of caring the AD patients is expected to rise dramatically, the necessity of having a computer-aided system for early and accurate AD diagnosis becomes critical [3] .\nSeveral popular non-invasive neuroimaging tools, such as structural MRI (sMRI), functional MRI (fMRI), and positron emission tomography (PET), have been investigated for developing such a system [4, 5] . The latter extracts features from the available images, and a classifier is trained to distinguish between different groups of subjects, e.g., AD, mild cognitive impairment (MCI), and normal control (NC) groups [3, [6] [7] [8] . The sMRI has been recognized as a promising indicator of the AD progression [3, 9] .\nVarious machine learning techniques were employed to leverage multi-view MRI, PET, and CSM data to predict the AD. Liu et al. [10] extracted multi-view features using several selected templates in the subjects' MRI dataset. Tissue density maps of each template were used then for clustering subjects within each class in order to extract an encoding feature of each subject. Finally, an ensemble of support vector machine (SVM) was used to classify the subject. Deep networks were also used for diagnozing the AD with different image modalities and clinical data. Suk et al. [11] used a stacked autoencoder to separately extract features from MRI, PET, and cerebrospinal fluid (CSF) images; compared combinations of these features with due account of their clinical mini-mental state examination (MMSE) and AD assessment scale-cognitive (ADAScog) scores, and classified the AD on the basis of three selected MRI, PET, and CSF features with a multi-kernel SVM. Later on, a multimodal deep Boltzmann machine (BM) was used [12] to extract one feature from each selected patch of the MRI and PET scans and predict the AD with an ensemble of SVMs. Liu et al. [13] extracted 83 regions-of-interest (ROI) from the MRI and PET scans and used multimodal fusion to create a set of features to train stacked layers of denoising autoencoders. Li et al. [14] developed a multi-task deep learning for both AD classification and MMSE and ADAScog scoring by multimodal fusion of MRI and PET features into a deep restricted BM, which was pre-trained by leveraging the available MMSE and ADAS-cog scores.\nVoxel-wise, cortical thickness, and hippocampus shape-volume features of the sMRI are used to diagnose the AD [3] . The voxelwise features are extracted after co-aligning (registering) all the brain image data to associate each brain voxel with a vector (signature) of multiple scalar measurements. Kl\u00f6ppel et al. [15] used the gray matter (GM) voxels as features and trained an SVM to discriminate between the AD and NC subjects. The brain volume in [16] is segmented to GM, white matter (WM), and CSF parts, followed by calculating their voxel-wise densities and associating each voxel with a vector of GM, WM, and CSF densities for classification. For extracting cortical thickness features, Lerch et al. [17] segmented the registered brain MRI into the GM, WM, and CSF; fitted the GM and WM surfaces using deformable models; deform and expand the WM surface to the GM-CSF intersection; calculate distances between corresponding points at the WM and GM surfaces to measure the cortical thickness, and use these features for classification. To quantify the hippocampus shape for feature extraction, Gerardin et al. [18] segmented and spatially aligned the hippocampus regions for various subjects and modeled their shape with a series of spherical harmonics. Coefficients of the series were then normalized to eliminate rotation-translation effects and used as features for training an SVM based classifier.\nComparative evaluations [3, [6] [7] [8] revealed several limitations of the above feature extraction techniques for classifying the AD. The voxel-wise feature vectors obtained from the brain sMRI are very noisy and can be used for classification only after smoothing and clustering to reduce their dimensionality [16] ations of the whole brain structure affected by the AD in other ROIs, e.g., the ventricle's volume. Appropriateness of the extracted feature vectors highly depend on image preprocessing due to registration errors and noise, so that feature engineering requires the domain expert knowledge. Most of the trainable classifiers are biased toward a particular dataset, which was used for training and testing (i.e., the classification features extracted at the learning stage are datasetspecific).\nWe propose a new deep 3D convolutional neural network (3D-CNN), for unsupervised generic and transferable feature extraction based on 3D extension of convolutional autoencoder (3D-CAE) [19] to overcome the aforementioned limitations in feature extraction from brain sMRI for AD classification. The proposed network combines a pretrained 3D-CAE in the source domain, e.g. CADDementia, with upper task-specific layers to be fine-tuned in the target domain, e.g. ADNI dataset, [20, 21] . Such adaptation of prelearned generic features allows for calling the proposed classifier a 3D Adaptable CNN (3D-ACNN)."}, {"section_title": "MODEL", "text": "The proposed AD diagnostic framework extracts features of a brain MRI with a source-domain-trained 3D-CAE and performs taskspecific classification with a target-domain-adaptable 3D-CNN. The 3D-CAE architecture and the AD diagnosis framework using the 3D-ACNN are detailed in Sections 2.1 and 2.2, respectively."}, {"section_title": "Feature Extraction using 3D Convolutional Autoencoder:", "text": "Conventional unsupervised autoencoder extracts a few co-aligned scalar feature maps for a set of input 3D images with scalar or vectorial voxel-wise signals by combining data encoding and decoding. The input image is encoded by mapping each fixed voxel neighborhood to a vectorial feature space in the hidden layer and is reconstructed back in the output layer to the original image space. To extract features that capture characteristic patterns of input data variations, training of the autoencoder employs back-propagation and constraints on properties of the feature space to reduce the reconstruction error.\nExtracting global features from 3D images with vectorial voxelwise signals is computationally expensive and requires too large training data sets, due to growing fast numbers of parameters to be evaluated in the input (encoding) and output (decoding) layers [22] . Moreover, local features are more suitable for extracting patterns from high-dimensional images, although autoencoders with full connections between all nodes of the layers try to learn global features. To overcome this problem, we use a stack of unsupervised CAE with locally connected nodes and shared convolutional weights to extract local features from 3D images with possibly long voxel-wise signal vectors [19, 23, 24] . Each input image is reduced hierarchically, using the hidden feature (activation) map of each CAE for training the next-layer CAE.\nOur 3D extension of a hierarchical CAE proposed in [19] is sketched in Fig. 1(a) . To capture characteristic variations of an input 3D image, x, each voxel-wise feature, h i:j:k , associated with the i-th 3D lattice node, j-th component of the input voxel-wise signal vector, and k-th feature map; k = [1, . . . , K], is extracted by a moving-window convolution (denoted below * ) of a fixed n \u00d7 n \u00d7 n neighborhood, x i:neib , of this node with a linear encoding filter, specified by its weights, W k = [W j:k : j = 1, . . . , J] for each relative neighboring location with respect to the node i and each voxel-wise signal component j, followed by feature-specific biases, b k = [b j,k : j = 1, . . . , J] and non-linear transformations with a certain activation function, f (\u00b7):\nThe latter function is selected from a rich set of constraining differentiable one, including, in particular, the sigmoid, f (u) = (1 + exp(\u2212u)) \u22121 and rectified linear unit (ReLU), f (u) = max (0, u) [25] . Since the 3D image x in Eq. (1) has the Jvectorial voxel-wise signals, actually, the weights W k define a 3D moving-window filter convolving the union of J-dimensional signal spaces for each voxel within the window.\nTo simplify notation, let h k = T (x : W k , b k , f (\u00b7)) denote the entire encoding of the input 3D image with J-vectorial voxel-wise signals with the k-th 3D feature map, h k , such that its scalar components are obtained with Eq. (1) using the weights W k and bias vectors b k for a given voxel neighborhood. The like inverse transformation, Tinv(. . .), with the same voxel neighborhood, but generally with the different convolutional weights, P k , biases, b inv:k , and, possibly, activation function, g(\u00b7), decodes, or reconstructs the initial 3D image:\nGiven L encoding layers, each layer l generates an output feature image, h The 3D-CAE of Eqs. (1) and (2) is trained by minimizing the mean squared reconstruction error for T ; T \u2265 1, given training input images, x\n[t] ; t = 1, . . . , T ,\nwhere \u03b8 = [W k ; P k ; b k ; b inv:k : k = 1, . . . , K], and . . . denote all free parameters and the average vectorial 2-norm over the T training images, respectively. To reduce the number of the free parameters, the decoding, P k , and encoding, W k , weights were tied by flipping over all their dimensions as proposed in [19] . The cost of Eq. (3) was minimized in the parameter space by using the stochastic gradient descent search, combined with error back-propagation."}, {"section_title": "AD Classification by 3D Adaptive CNN (3D-ACNN)", "text": "While the lower layers of a goal predictive 3D-CNN extract generalized features, the upper layers have to facilitate task-specific classification using those features [21] . The proposed classifier extracts the generalized features by using a stack of locally connected bottom convolutional layers, while performing task-specific fine-tuning of parameters of the fully connected upper layers. Training the proposed hierarchical 3D-CNN consists of pre-training, initial training of the lower convolutional layers, and final task-specific fine-tuning.\nAt the pre-training stage, the convolutional layers for generic feature extraction are formed as a stack of 3D-CAEs, which were pre-trained in the source domain. Then these layers are initialized by encoding the 3D-CAE weights [26] , and, finally, the deep-supervisionbased [27] fine-tuning of the upper fully-connected layers is performed for each task-specific binary or multi-class sMRI classification. Due to pre-training on the source domain data, the bottom convolutional layers can extract generic features related to the AD biomarkers, such as the ventricular size, hippocampus shape, and cortical thickness, as shown in Fig. 1(b) . We use the Net2Net initialization [26] , which allows for different convolutional kernel and pool sizes of the 3D-CNN layers, comparing to those in the 3D-CAE, based on the target-domain image size and imaging specifications, and facilitates adapting the 3D-CNN across the different domains. To classify the extracted features in a task-specific way, weights of the upper fully-connected 3D-CNN layers are fine-tuned on the target-domain data by minimizing a specific loss function. The loss depends explicitly on the weights and is proportional to a negated log-likelihood of the true output classes, given the input features extracted from the training target-domain images by the pre-trained bottom part of the network.\nOur implementation of the 3D-CNN uses the ReLU activation functions at each inner layer and the fully connected upper layers with a softmax top-most output layer (Fig. 2) , predicting the probability of belonging an input brain sMRI to the AD, MCI, or NC group. The Adadelta gradient descent [28] was used to update the pre-trained 3D-CAE and fine-tune the entire 3D-ACNN. [11] PET+MRI+CSF n/a n/a 95.9 1.1 n/a 85.0 1.2 Suk et al. [12] PET+MRI n/a n/a 95.4 5.2 n/a 85.7 5.2 Zhu et al. [29] PET+MRI+CSF n/a n/a 95.9 n/a n/a 82.0 n/a Zu et al. [30] PET+MRI n/a n/a 96.0 n/a n/a 80.3 n/a Liu et al. [13] PET+MRI 53.8 4.8 n/a 91.4 5.6 n/a 82.1 4.9 Liu et al. [10] MRI n/a n/a 93.8 n/a n/a 89.1 n/a Li et al. [14] PET+MRI+CSF n/a n/a 91.4 1.8 70.1 2.3 77.4 1.7 Sarraf et al. [31] fMRI n/a n/a 96.8 n/a n/a n/a Our 3D-ACNN MRI "}, {"section_title": "EXPERIMENTAL RESULTS", "text": "To performance of the proposed 3D-CAES is evaluated on CADDementia 1 as source domain, for generalized feature extraction. The data set contains structural T1-weighted MRI (T1w) scans of patients with the diagnosis of probable AD, patients with the diagnosis of MCI, and NC without a dementia syndrome [3] . To pretrain 3D-CAES on CADDementia, sMRI are preproceesed by spatially normalizing using rigid registration approach. Then skull is removed and image intensities are normalized to [0, 1], resulting in sMRI of size (200 \u00d7 150 \u00d7 150).\nThe classification performance of proposed 3D-ACNN is evaluated on Alzheimers Disease Neuroimaging Initiative (ADNI) database, as target domain, for five classification tasks: four binary ones (AD vs. NC, AD+MCI vs. NC, AD vs. MCI, MCI vs NC) and the ternary classification (AD vs. MCI vs. NC). Classification accuracy was evaluated for each test by ten-fold cross-validation. ADNI dataset without using any preprocessing and skull stripping is used AD classification, compared to preprocessed CADDementia dataset. The Theano library [32] was used to develop the deep CNN implemented for our experiments on the Amazon EC2 g2.8xlarge instances with GPU GRID K520.\nTo pretrain 3D-CAE for feature extraction of brain sMRI, we use ReLu nonlinear function as the encoder and decoder layer's activation. The 3D-CAE contains eight encoding and decoding filters of size (3 \u00d7 3 \u00d7 3). Three hierarchical 3D-CAE's are trained to extract the low-dimensional feature maps. The extracted feature maps are of dimension (102 \u00d7 76 \u00d7 76), (52 \u00d7 40 \u00d7 40), and (28 \u00d7 22 \u00d7 22), subsequently. Each 3D-CAE extracts eight feature maps, according to the number of their encoding filters ( Fig. 1(b) ). Selected slices of the three feature maps from each layer of our stacked 3D-CAE (abbreviated 3D-CAES below) in Fig. 1(b) , show that the learnt generic convolutional filters can really capture features related to the AD biomarkers, e.g., the ventricle size, cortex thickness, and hippocampus model. These feature maps were generated by the pre-trained 3D-CAES for the CADementia database. According to these projections, the first layer of the 3D-CAES extracts the cortex thickness as a discriminative AD feature of AD, whereas the brain size (related to the patient gender), size of ventricles, and hippocampus model are represented by the subsequent layers. Each 3D-CAES layer combines the extracted lower-layer feature maps in order to train the higher level for describing more in detail the anatomical variations of the brain sMRI . Both the ventricle size and cortex thickness features are combined to extract conceptually higher-level features at the next layers. Performance of the proposed 3D-ACNN classifier, in terms of accuracy (ACC), for each task-specific classification was evaluated and compared to competing approaches [10] [11] [12] [13] [14] [29] [30] [31] . Table 2 presents the average results of ten-fold cross-validation of our classifier. According to these experiments, the proposed 3D-ACNN outperforms the other approaches in all five task-specific cases, in spite of employing only a single imaging modality (sMRI) and performing no prior skull-stripping."}, {"section_title": "CONCLUSION AND FUTURE WORK", "text": "This paper proposed a 3D-ACNN classifier, which can more accurately predict the AD on structural brain MRI scans, than several other state-of-the-art predictors. The pretraining and layer freezing were used to enhances generality of features in capturing the AD biomarkers. Three stacked 3D CAE network were pretrained on CADDementia Dataset. Then the learnt features are extracted and used as AD biomarkers detection in bottom layers of a 3D CNN network. Then three fully connected layers are stacked on top of the bottom layers to perform AD classification on 210 subjects of ADNI dataset. Classification performance were measured using ten-fold crossvalidation, and were compared to the state-of-the-art models, demonstrated the out-performance of the proposed 3D CNN. The future application of proposed 3D-ACNN include detection of lung cancer [33] [34] [35] [36] , heart failure [37] , and autism detection [38] ."}]