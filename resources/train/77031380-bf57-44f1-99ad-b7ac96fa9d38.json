[{"section_title": "Abstract", "text": "We propose nonparametric methods for functional linear regression which are designed for sparse longitudinal data, where both the predictor and response are functions of a covariate such as time. Predictor and response processes have smooth random trajectories, and the data consist of a small number of noisy repeated measurements made at irregular times for a sample of subjects. In longitudinal studies, the number of repeated measurements per subject is often small and may be modeled as a discrete random number and, accordingly, only a finite and asymptotically nonincreasing number of measurements are available for each subject or experimental unit. We propose a functional regression approach for this situation, using functional principal component analysis, where we estimate the functional principal component scores through conditional expectations. This allows the prediction of an unobserved response trajectory from sparse measurements of a predictor trajectory. The resulting technique is flexible and allows for different patterns regarding the timing of the measurements obtained for predictor and response trajectories. Asymptotic properties for a sample of n subjects are investigated under mild conditions, as n \u2192 \u221e, and we obtain consistent estimation for the regression function. Besides convergence results for the components of functional linear regression, such as the regression parameter function, we construct asymptotic pointwise confidence bands for the predicted trajectories. A functional coefficient of determination as a measure of the variance explained by the functional regression model is introduced, extending the standard R 2 to the functional case. The proposed methods are illustrated with a simulation study, longitudinal primary biliary liver cirrhosis data and an analysis of 1. Introduction. We develop a version of functional linear regression analysis in which both the predictor and response variables are functions of some covariate which usually but not necessarily is time. Our approach extends the applicability of functional regression to typical longitudinal data where only very few and irregularly spaced measurements for predictor and response functions are available for most of the subjects. Examples of such data are discussed in Section 5 (see Figures 1 and 6) .\nSince a parametric approach only captures features contained in the preconceived class of functions, nonparametric methods of functional data analysis are needed for the detection of new features and for the modeling of highly complex relationships. Functional principal component analysis (FPCA) is a basic methodology that has been studied in early work by Grenander In the recent literature there has been increased interest in regression models for functional data, where both the predictor and response are random functions. Our aim is to extend the applicability of such models to longitudinal data with their typically irregular designs, and to develop asymptotics for functional regression in sparse data situations. Practically all investigations to date are for the case of completely observed trajectories, where one assumes either entire trajectories or densely spaced measurements taken along each trajectory are observed; recent work includes Cardot, Ferraty, Mas and Sarda [3], Cardot, Ferraty and Sarda [5], Chiou, M\u00fcller, Wang and Carey [7] and Ferraty and Vieu [16] .\nIn this paper we illustrate the potential of functional regression for complex longitudinal data. In functional data settings, Cardot, Ferraty and Sarda [4] provided consistency results for the case of linear regression with a functional predictor and scalar response, where the predictor functions are sampled at a regular grid for each subject, and Cardot, Ferraty and Sarda [4] discussed inference for the regression function. The case of a functional response was introduced by Ramsay and Dalzell [25], and for a summary of this and related work we refer to Ramsay and Silverman ([26], Chapter 11) and to Faraway [15] for a discussion of relevant practical aspects. The theory"}, {"section_title": "", "text": "1. Introduction. We develop a version of functional linear regression analysis in which both the predictor and response variables are functions of some covariate which usually but not necessarily is time. Our approach extends the applicability of functional regression to typical longitudinal data where only very few and irregularly spaced measurements for predictor and response functions are available for most of the subjects. Examples of such data are discussed in Section 5 (see Figures 1 and 6) .\nSince a parametric approach only captures features contained in the preconceived class of functions, nonparametric methods of functional data analysis are needed for the detection of new features and for the modeling of highly complex relationships. Functional principal component analysis (FPCA) is a basic methodology that has been studied in early work by Grenander [18] and, more recently, by Rice and Silverman [27] , Ramsay and Silverman [26] and many others. Background in probability on function spaces can be found in [19] . James, Hastie and Sugar [21] emphasized the case of sparse data by proposing a reduced rank mixed-effects model using B-spline functions. Nonparametric methods for unbalanced longitudinal data were studied by Boularan, Ferr\u00e9 and Vieu [2] and Besse, Cardot and Ferraty [1] . Yao, M\u00fcller and Wang [31] proposed an FPCA procedure through a conditional expectation method, aiming at estimating functional principal component scores for sparse longitudinal data.\nIn the recent literature there has been increased interest in regression models for functional data, where both the predictor and response are random functions. Our aim is to extend the applicability of such models to longitudinal data with their typically irregular designs, and to develop asymptotics for functional regression in sparse data situations. Practically all investigations to date are for the case of completely observed trajectories, where one assumes either entire trajectories or densely spaced measurements taken along each trajectory are observed; recent work includes Cardot, Ferraty, Mas and Sarda [3] , Cardot, Ferraty and Sarda [5] , Chiou, M\u00fcller, Wang and Carey [7] and Ferraty and Vieu [16] .\nIn this paper we illustrate the potential of functional regression for complex longitudinal data. In functional data settings, Cardot, Ferraty and Sarda [4] provided consistency results for the case of linear regression with a functional predictor and scalar response, where the predictor functions are sampled at a regular grid for each subject, and Cardot, Ferraty and Sarda [4] discussed inference for the regression function. The case of a functional response was introduced by Ramsay and Dalzell [25] , and for a summary of this and related work we refer to Ramsay and Silverman ( [26] , Chapter 11) and to Faraway [15] for a discussion of relevant practical aspects. The theory for the case of fixed design and functional response in the densely sampled case was investigated by Cuevas, Febrero and Fraiman [8] . Chiou, M\u00fcller and Wang [6] studied functional regression models where the predictors are finitedimensional vectors and the response is a function, using a quasi-likelihood approach. Applications of varying-coefficient modeling to functional data, including asymptotic inference, were presented in [13] and [14] .\nThe proposed functional regression approach is flexible, and allows for varying patterns of timing in regard to the measurements of predictor and response functions. This is relevant since it is a common occurrence in longitudinal data settings that the measurement of either the predictor or response is missing. The contributions of this paper are as follows: First, we extend the functional regression approach to longitudinal data, using a conditioning idea. This leads to improved prediction of the response trajectories, given sparse measurements of the predictor trajectories. Second, we provide a complete practical implementation of the proposed functional regression procedure and illustrate its utility for two longitudinal studies. Third, we obtain the asymptotic consistency of the estimated regression function of the functional linear regression model for the case of sparse and irregular data, including rates. Fourth, we construct asymptotic pointwise confidence bands for predicted response trajectories based on asymptotic distribution results. Fifth, we introduce a consistent estimator for a proposed measure of association between the predictor and response functions in functional regression models that provides an extension of the coefficient of determination R 2 in standard linear model theory to the functional case. The proposed functional coefficient of determination provides a useful quantification of the strength of the relationship between response and predictor functions, as it can be interpreted in a well-defined sense as the fraction of variance explained by the functional linear regression model, in analogy to the situation for the standard linear regression model. The paper is organized as follows. In Section 2 we introduce basic notions, the functional linear regression model, and describe the estimation of the regression function. In Section 3 we discuss the extension of the conditioning approach to the prediction of response trajectories in functional regression under irregular and sparse data. Pointwise confidence bands and the functional coefficient of determination R 2 are also presented in Section 3. Simulation results that illustrate the usefulness of the proposed method can be found in Section 4. This is followed by applications of the proposed functional regression approach to longitudinal PBC liver cirrhosis data and an analysis of the longitudinal relationship between blood pressure and body mass index, using data from the Baltimore Longitudinal Study on Aging in Section 5. Asymptotic consistency and distribution results are provided in Section 6, while proofs and auxiliary results are compiled in the Appendix. "}, {"section_title": "2.1.", "text": "Representing predictor and response functions through functional principal components. The underlying but unobservable sample consists of pairs of random trajectories (X i , Y i ), i = 1, . . . , n, with square integrable predictor trajectories X i and response trajectories Y i . These are realizations of smooth random processes (X, Y ), with unknown smooth mean functions EY (t) = \u00b5 Y (t), EX(s) = \u00b5 X (s), and covariance functions cov(Y (s), Y (t)) = G Y (s, t), cov(X(s), X(t)) = G X (s, t). We usually refer to the arguments of X(\u00b7) and Y (\u00b7) as time, with finite and closed intervals S and T as domains. We assume the existence of orthogonal expansions of G X and G Y (in the L 2 sense) in terms of eigenfunctions \u03c8 m and \u03c6 k with nonincreasing eigenvalues \u03c1 m and \u03bb k , that is,\nWe model the actually observed data which consist of sparse and irregular repeated measurements of the predictor and response trajectories X i and Y i , contaminated with additional measurement errors (see [28, 30] ). To adequately reflect the irregular and sparse measurements, we assume that there is a random number of L i (resp. N i ) random measurement times for X i (resp. Y i ) for the ith subject, which are denoted as S i1 , . . . , S iL i (resp. T i1 , . . . , T iN i ). The random variables L i and N i are assumed to be i.i.d. as L and N respectively, where L and N may be correlated but are independent of all other random variables. Let U il (resp. V ij ) denote the observation of the random trajectory X i (resp. Y i ) at a random time S il (resp. T ij ), contaminated with measurement errors \u03b5 il (resp.\n, and independent of functional principal component scores \u03b6 im (resp. \u03be ik ) that satisfy\nThen we may represent predictor and response measurements as follows:\nWe note that the response and predictor functions do not need to be sampled simultaneously, extending the applicability of the proposed functional regression model. \nHere the bivariate regression function \u03b2(s, t) is smooth and square integrable, that is, T S \u03b2 2 (s, t) ds dt < \u221e. Centralizing X(t) by X c (s) = X(s) \u2212 \u00b5 X (s), and observing E[Y (t)] = \u00b5 Y (t) = \u03b1(t) + S \u03b2(s, t)\u00b5 X (s) ds, the functional linear regression model becomes\nOur aim is to predict an unknown response trajectory based on sparse and noisy observations of a new predictor function. This is the functional version of the classical prediction problem in a linear model where, given a set of predictors X, one aims to predict the mean response Y by estimating E(Y |X) (see [12] , page 81). An important step is to estimate the regression function \u03b2(s, t). We use the following basis representation of \u03b2(s, t), which is a consequence of the population least squares property of conditional expectation and the fact that the predictors are uncorrelated, generalizing the representation \u03b2 1 = cov(X, Y )/ var(X) of the slope parameter in the simple linear regression model E(Y |X) = \u03b2 0 + \u03b2 1 X to the functional case. This representation holds under certain regularity conditions which are outlined in [20] and is given by\nThe convergence of the right-hand side of (5) is discussed in Lemma A.2 (Appendix A.3). When referring to \u03b2, we always assume that the limit (5) exists in an appropriate sense. In a first step, smooth estimates of the mean and covariance functions for the predictor and response functions are obtained by scatterplot smoothing; see (30) and (31) in Appendix A.2. Then a nonparametric FPCA step yields estimates\u03c8 m ,\u03c6 k for the eigenfunctions, and\u03c1 m ,\u03bb k for the eigenvalues of predictor and response functions; see (33) below.\nWe use two-dimensional scatterplot smoothing to obtain an estimate C(s, t) of the cross-covariance surface C(s, t), s \u2208 S, t \u2208 T ,\n) be \"raw\" cross-covariances that serve as input for the two-dimensional smoothing step; see (36) in Appendix A.2. The smoothing parameters in the two coordinate directions can be chosen independently by one-curve-leave-out cross-validation procedures [27] . From (6) we obtain estimates for\nWith estimates (33), the resulting estimate for \u03b2(s, t) i\u015d\nIn practice, the numbers M and K of included eigenfunctions can be chosen by one-curve-leave-out cross-validation (34), or by an AIC type criterion (35). For the asymptotic analysis, we consider M (n), K(n) \u2192 \u221e as the sample size n \u2192 \u221e. Corresponding convergence results can be found in Theorem 1."}, {"section_title": "Prediction and inference.", "text": "3.1. Predicting response trajectories. One of our central aims is to predict the trajectory Y * of the response for a new subject from sparse and irregular measurements of the predictor trajectory X * . In view of (4), the basis representation of \u03b2(s, t) in (5) and the orthonormality of the {\u03c8 m } m\u22651 , the prediction of the response function would be obtained via the conditional expectation\nwhere \u03b6 * m = S (X * (s) \u2212 \u00b5 X (s))\u03c8 m (s) ds is the mth functional principal component score of the predictor trajectory X * . The quantities \u00b5 Y , \u03c6 k , \u03c3 km and \u03c1 m can be estimated from the data, as described above. It remains to discuss the estimation of \u03b6 * m , and for this step we invoke Gaussian assumptions in order to handle the sparsity of the data.\nLet U * l be the lth measurement made for the predictor function X * at time S * l , according to (1) , where l = 1, . . . , L * , with L * a random number. Assume that the functional principal component scores \u03b6 * m and the measurement errors \u03b5 * l for the predictor trajectories are jointly Gaussian. Following Yao, M\u00fcller and Wang [31] , the best prediction of the scores \u03b6 * m is then obtained through the best linear prediction, given the observations U * = (U * 1 , . . . , U * L * ), and the number and locations of these observations, L * and\nwhere\nX \u03b4 jl with \u03b4 jl = 1 if j = l and 0 if j = l. According to (10) , estimates for the functional principal component scores \u03b6 * m are obtained by substituting estimates of \u00b5 * X , \u03c1 m and \u03c8 * m that are based on the entire data collection, leading t\u00f4 (11) where\nThe predicted trajectory is then obtained as\nIn the sparse situation, the Gaussian assumption is crucial. It allows us to obtain the best linear predictors in (10)- (12) through conditional expectations, borrowing strength from the entire sample and thus compensating for the sparseness of data for individual trajectories. Simulation results, reported in Section 4, indicate that the proposed method is quite robust regarding the Gaussian assumption. Theoretical results for predicted trajectories (12) are given in Theorem 2.\n3.2. Asymptotic pointwise confidence bands for response trajectories. We construct asymptotic confidence bands for the response trajectory Y * (\u00b7) of a new subject, conditional on the sparse and noisy measurements that are available for the underlying predictor function. (10), and define the\nis the projection of \u03b6 * M on the space spanned by the linear functions of U * given L * and S * , cov(\nUnder the Gaussian assumption and conditioning on L * and S * , then\u03b6 \n} conditional on L * and S * can be approximated by N (0,\u03c6 T tK P KM \u2126 M P T KM\u03c6 tK ). As a consequence, the (1 \u2212 \u03b1) asymptotic pointwise interval for E[Y * (t)|X * ], the mean response at predictor level X * , is given by\nwhere \u03a6 is the standard Gaussian c.d.f."}, {"section_title": "3.3.", "text": "Coefficients of determination for functional linear regression. In standard linear regression, a measure to quantify the \"degree of linear association\" between predictor and response variables is the coefficient of determination R 2 (e.g., [12] , page 138). The coefficient of determination plays an important role in applications of regression analysis, as it may be interpreted as the fraction of variation of the response that is explained by the regression relation. Accordingly, R 2 is commonly used as a measure of the strength of the regression relationship.\nThe proposed extension to functional linear regression can be motivated by the standard linear regression model with a response Y and a predictor X, where the coefficient of determination R 2 is defined by (9) and the orthonormality properties of {\u03c6 k } and {\u03c8 m }, one obtains\nThe analogous notion of total variation of\nAnother interpretation of the functional coefficient of determination R 2 (15) is as follows: Denoting by R 2 km the coefficients of determination of the \nkm is the coefficient of determination of regressing \u03be k on all \u03b6 m , m = 1, 2, . . . , simultaneously, by successively adding predictors \u03b6 m into the regression equation and observing that R 2 k is obtained as the sum of the R 2 km , as the predictors \u03b6 m are uncorrelated. Then\nis seen to be a weighted average of these R 2 k , with weights provided by the \u03bb k . According to (15) , a natural estimate R 2 for the functional coefficient of determination R 2 is\nwhere\u03c3 km are as in (7) .\nBesides the functional R 2 (15) that provides a global measure of the linear association between processes X and Y , we also propose a local version of a functional coefficient of determination. The corresponding function R 2 (t) may be considered a functional extension of the local R 2 measure that has been introduced in [10] and [11] . As shown above, for fixed t \u2208 T , the variation of Y (t) explained by the predictor process X is determined by"}, {"section_title": "var(E[Y (t)|X])/ var(Y (t))", "text": ". This motivates the following definition of a pointwise functional coefficient of determination R 2 (t):\nNote that R 2 (t) satisfies 0 \u2264 R 2 (t) \u2264 1 for all t \u2208 T .\nA second option to obtain an overall R 2 value is to extend the pointwise measure R 2 (t) to a global measure by taking its integral, which leads to an alternative definition of the global functional coefficient of determination, denoted by R 2 ,\nwhere |T | denotes the length of the time domain T . Natural estimates of R 2 (t) and R 2 are given by\nWe refer to Section 5 for further discussion of R 2 , R 2 (t) and R 2 in applications and to Theorem 4 in Section 6 regarding the asymptotic convergence of these estimates."}, {"section_title": "Simulation", "text": "The predictor trajectories X i and associated sparse and noisy measurements U il (1) were generated as follows. The simulated processes X had the mean function \u00b5 X (s) = s + sin(s), with covariance function constructed from two eigenfunctions, \u03c8 1 (s) = \u2212 cos(\u03c0s/10)/ \u221a 5 and \u03c8 2 (s) = sin(\u03c0s/10)/ \u221a 5, 0 \u2264 s \u2264 10. We chose \u03c1 1 = 2, \u03c1 2 = 1 and \u03c1 m = 0, m \u2265 3, as eigenvalues, and \u03c3 2 X = 0.25 as the variance of the additional measurement errors \u03b5 il in (1), which were assumed to be normal with mean 0. For the 500 normal samples, the FPC scores \u03b6 im (m = 1, 2) were generated from N (0, \u03c1 m ), while the \u03b6 im for the nonnormal samples were generated from a mixture of two normals, N ( \u03c1 m /2, \u03c1 m /2) with probability 1/2 and N (\u2212 \u03c1 m /2, \u03c1 m /2), also with probability 1/2.\nFor the response trajectories, letting b 11 = 2, b 12 = 2, b 21 = 1, b 22 = 2, the regression function was \u03b2(s, t) = We investigated predicting response curves for new subjects. For each Monte Carlo simulation run, we generated 100 new predictor curves X * i , with noisy measurements taken at the same random time points as X i , and 100 associated response curves E[Y * i |X * i ]. Relative mean squared prediction error was used as an evaluation criterion, given by\nwhere predicted trajectories Y * i,KM were obtained according to (11) and (12) . This method is denoted as CE in Table 1 , and was compared with a \"classical\" functional regression approach that was also based on (12) , but with the conditional expectation replaced by the integral approximation\u03b6 * I im = Table 1 . The numbers of eigenfunctions K and M were chosen by the AIC criterion (35), separately for each simulation. We also included the case of irregular but nonsparse data, where the random numbers of the repeated measurements were chosen from {20, . . . , 30} for both X i and Y i with equal probability. From the results in Table 1 , we see that, for sparse data, the CE method improves the prediction errors by 57%/60% for normal/mixture distributions, while the gains for nonsparse data are not as dramatic, but nevertheless present. The CE method emerges as superior for the sparse data case. [23] is a rare but fatal chronic liver disease of unknown cause, with a prevalence of about 50 cases per million population. The data were collected between January 1974 and May 1984 by the Mayo Clinic (see also Appendix D of [17] ). The patients were scheduled to have measurements of blood characteristics at six months, one year and annually thereafter post diagnosis. However, since many individuals missed some of their scheduled visits, the data are sparse and irregular with unequal numbers of repeated measurements per subject and also different measurement times T ij per individual."}, {"section_title": "Applications.", "text": ""}, {"section_title": "Primary biliary cirrhosis data. Primary biliary cirrhosis", "text": "To demonstrate the usefulness of the proposed methods, we explore the dynamic relationship between albumin in mg/dl (predictor) and prothrombin time in seconds (response), which are both longitudinally measured. We include 137 female patients, and the measurements of albumin level and prothrombin time before 2500 days. For both albumin and prothrombin time, the number of observations ranged from 1 to 10, with a median of 5 measurements per subject. Individual trajectories of albumin and prothrombin time are shown in Figure 1 . The smooth estimates of the mean function for both albumin and prothrombin time are also displayed in Figure 1 , indicating opposite trends. The AIC criterion leads to the choice of two eigenfunctions for both predictor and response functions, and smooth eigenfunction estimates are presented in Figure 2 . For both albumin and prothrombin time, the first eigenfunction reflects an overall level, and the second eigenfunction a contrast between early and late times. The estimate of the regression function \u03b2 is displayed in Figure 3 . Its shape implies that, for the prediction of early prothrombin times, late albumin levels contribute positively, while early levels contribute negatively, whereas the prediction of late prothrombin times is based on a sharper contrast with highly positive weighting of early albumin levels and negative weighting of later levels.\nWe randomly selected four patients from the sample for which the trajectory of prothrombin time was to be predicted solely from the sparse and noisy albumin measurements. For this prediction, the data of each subject to be predicted were omitted, the functional regression model was fitted from the remaining subjects, and then the predictor measurements were entered into the fitted model to obtain the predicted response trajectory, thus leading to genuine predictions. Predicted curves and 95% pointwise confidence bands are shown in Figure 4 . Note that these predictions of longitudinal trajectories are based on just a few albumin measurements, and the prothrombin time response measurements shown in the figures are not used.\nRegarding the functional coefficients of determination R 2 (15) and R 2 (18), we obtain very similar estimates, R 2 = 0.37 and R 2 = 0.36, which we would interpret to mean that about 36% of the total functional variation "}, {"section_title": ". Left panel: Observed individual trajectories (solid ) and the smooth estimate of the mean function for albumin (thick solid ). Right panel: Corresponding observed individual trajectories (solid ) and the smooth estimate of the mean function for prothrombin time (thick solid ), for the primary biliary cirrhosis (PBC) data.", "text": ""}, {"section_title": "Fig. 2. Left panel: Smooth estimates of the first (solid ) and second (dashed ) eigenfunctions for albumin, accounting for 87% and 8% of total variation. Right panel: Smooth estimates of the first (solid ) and second (dashed ) eigenfunctions for prothrombin time,", "text": "accounting for 54% and 33% of total variation, for the PBC data."}, {"section_title": "Fig. 3. Estimated regression function (8), where the predictor (albumin) time is s (in days), and the response ( prothrombin) time is t (in days), for the PBC data.", "text": "of the prothrombin time trajectories is explained by the albumin data, indicating a reasonably strong functional regression relationship. The curve of estimated pointwise coefficients of determination R 2 (t) (19) is shown in Figure 5 , left panel, which describes the trend of the proportion of the variation of the prothrombin time, at each argument value, that is explained by the entire albumin trajectories. We find that the observations in the second half are better determined by the albumin trajectories than the values in the first half of the domain of prothrombin time."}, {"section_title": "Functional regression of systolic blood pressure on body mass index.", "text": "As a second example, we discuss a functional regression analysis of systolic blood pressure trajectories (responses) on body mass index trajectories (predictor), using anonymous data from the Baltimore Longitudinal Study of Aging (BLSA), a major longitudinal study of human aging [29] . The data consist of 1590 male volunteers who were scheduled to visit the Gerontology "}, {"section_title": "t) (19) as a function of time t for the PBC data (left panel, time is duration of study in days) and for the BLSA data (right panel, time is age in years).", "text": "Research Center bi-annually. Time corresponds to age of each subject and is measured in years. On each visit, systolic blood pressure (in mm Hg) and body mass index (BMI = weight in kg/height in m 2 ) were assessed. Since both measurements are highly variable, the data are noisy, and as many participants missed scheduled visits, or were seen at other than the scheduled times, the data are sparse with largely unequal numbers of repeated measurements and widely differing measurement times per subject. More details about the study and data can be found in [24] , and for previous statistical approaches, we refer to [22] .\nWe included the participants with measurements within the age range [60, 80], and first checked for outliers based on standardized residuals of body mass index (BMI) and systolic blood pressure (SBP), respectively. The standardized residuals are defined as residuals divided by the pooled sample standard deviation, where residuals are the differences between measurements and the estimated mean function obtained by scatterplot smoothing, using the local linear smoother. We excluded subjects with standardized residuals larger (or less) than \u00b13, for either BMI or SBP. Individual trajectories of BMI and SBP for the included 812 subjects are shown in Figure 6 , along with the smooth estimated mean functions of BMI and SBP.\nWhile average BMI decreases after age 64, SBP throughout shows an increasing trend. Based on the AIC criterion, three eigenfunctions are used for the predictor (BMI) function, and four for the response (SBP) function; these are displayed in Figure 7 . The first eigenfunctions of both processes correspond to an overall mean effect, and the second eigenfunctions to a contrast between early and late ages, with further oscillations reflected in third and fourth eigenfunctions.\nThe estimated regression function in Figure 8 indicates that a contrast between late and early BMI levels forms the prediction of SBP levels at later ages, where late BMI levels are weighted positive and early levels negative. When predicting SBP at age 80, the entire BMI trajectory matters, and rapid overall declines in BMI lead to the lowest SBPs, where speed of de- cline between 60 and 65 and between 75 and 80 is critical. Similar patterns can be identified for predicting SBP at other ages. As in the previous example, we randomly select four study participants and obtain predictions and 95% pointwise bands for each of these, based on one-leave-out functional regression analysis (Figure 9 ). The predicted trajectories are found to be reasonably close to the observations, which are not used in the analysis.\nThe functional coefficients of determination R 2 (15) and R 2 (18) were both estimated as 0.13, indicating that the dynamics of body mass index explains 13% of the total variation of systolic blood pressure trajectories; the functional regression relationship is seen to be weaker than in the previous example. The curve of estimated pointwise coefficients of determination R 2 (t) (17) is displayed in Figure 5 , right panel, indicating generally weaker linear association at older ages (beyond 70 years) as compared to the earlier ages (60 to 70 years). The minimal linear association between predictor trajectories and the functional response is seen to occur around age 75.7.\n6. Asymptotic properties. In this section we present the consistency of the regression function estimate (8) In what follows, we only consider the case that the processes X and Y are infinite-dimensional. If they are finite-dimensional and there exist true finite K and M , such that G X and G Y are finite-dimensional surfaces, then, as n tends to infinity, remainder terms such as \u03b8 n and \u03d1 n in (41) will disappear. Appropriately modified versions of the following theoretical results still hold. Since in this case the true K and M are finite, most of the technical assumptions that are needed to handle the infinite case would not be needed, such as (A1)-(A3), (A6)-(A7) and (B5), and K(n), M (n) would be assumed to converge to the true K and M instead of infinity, as n \u2192 \u221e.\nTo define the convergence of the right-hand side of (5) in the L 2 sense, we require that\nFurthermore, the right-hand side of (5) converges uniformly on S \u00d7 T , provided that\nis continuous in s and t, and the function \u03b2 KM (s, t) = K k=1 M m=1 \u03c3 km \u03c8 m (s)\u03c6 k (t)/\u03c1 m absolutely converges to \u03b2(s, t) for all s \u2208 S, t \u2208 T as M, K \u2192 \u221e.\nThe numbers M = M (n) and K = K(n) of included eigenfunctions are integer-valued sequences that depend on the sample size n; see assumption The rate of convergence in (22) and (23) depends on specific properties of processes X and Y in the following way: If \u03c4 n , \u03c5 n and \u03c2 n are defined as in (41) in Appendix A.2, then, for (22) we obtain the rate\nand for (23) the rate is\nas n \u2192 \u221e. Here \u03c2 n depends on bandwidths h 1 and h 2 that are used in the smoothing step (36) for the cross-covariance function C(s, t) = cov(X(s), Y (t)). These rates depend on specific properties of the processes X and Y , such as the spacings of the eigenvalues of their autocovariance operators. We note that, due to the sparsity of the data (at most, finitely many observations are made per random trajectory), fast rates of convergence cannot be expected in this situation, in contrast to the case where entire trajectories are observed or are densely sampled. (A4) The number and locations of measurements for a given subject or cluster remain unaltered as the sample size n \u2192 \u221e.\nTheorem 2. Under (A3), (A4) and the assumptions of Lemma A.1 and (B5) (see Appendix A.1), given L * and S * , for all t \u2208 T ,\nThis provides the consistency of the prediction Y * KM for the target trajectory Y * .\nFor the following results, we require Gaussian assumptions.\n(A5) For all 1 \u2264 i \u2264 n, m \u2265 1 and 1 \u2264 l \u2264 L i , the functional principal component scores \u03b6 im and the measurement errors \u03b5 il in (1) are jointly Gaussian.\n. We require existence of a limiting function and, therefore, the following analytical condition:\n(A6) There exists a continuous positive definite function \u03c9(t 1 , t 2 ) such that\nWe obtain the asymptotic distribution of { Y * KM (t)\u2212E[Y * (t)|X * ]} as follows, providing inference for predicted trajectories. \nConsidering the measure R 2 (15), R 2 is well defined since\nFurthermore, the right-hand side of (17) uniformly converges on t \u2208 T , provided that\nThe consistency of R 2 (16), R 2 (t) (19) and R in probability, (26) and if (A7) is assumed,\nWe note that the rate of convergence in (26) is the same as in the remark after Theorem 1, and that the rate in (27) and (28) is given by O p (\u03c4 n + \u03c5 n + \u03c2 n + \u03c0 n ), where \u03c0 n is as in (29) . The approach to functional regression we have proposed is quite flexible, and in simulations is seen to be robust to violations of assumptions such as the Gaussian assumption. It is a useful tool for data where both predictors and responses are contaminated by errors. We refer to [9] for another approach and discussion of de-noising of single pairs of curves observed with measurement errors. Besides varying coefficient models, the available methodology for situations where one has a sample of predictor and response functions is quite limited. Common methods in longitudinal data analysis such as Generalized Estimating Equations and Generalized Linear Mixed Models are not suitable for this task.\nThe proposed methodology may prove generally useful for longitudinal data with missing measurements, where missingness would be assumed to be totally unrelated to the random trajectories and errors. Extensions to situations where missingness is correlated with the time courses would be of interest in many practical applications. There are also some limitations to the functional regression approach under sparse data. Our prediction methods target the trajectory conditional on the available data, while the response trajectory given the entire but unobservable predictor trajectory is not accessible. While in theory it is enough that the probability that one observes more than one measurement per random trajectory is positive, in practice there needs to be a substantial number of subjects with two or more observations. Sometimes a prediction for a response trajectory may be desired even if there is no observation at all available for that subject. In this case we predict the estimated mean response function as the response trajectory. This is not unreasonable, since borrowing strength from other subjects to predict the response trajectory for a given subject is a key feature of the proposed method that will come more into play for subjects with very few measurements. Predictions for these subjects will often be relatively closer to the mean response than for subjects with many measurements.\nWe conclude by remarking that extensions to cases with more than one predictor function are of interest in a number of applications, and would be analogous to the extension of simple linear regression to multiple linear regression. Functional regression is only at its initial stages and much more work needs to be done."}, {"section_title": "FUNCTIONAL REGRESSION ANALYSIS", "text": ""}, {"section_title": "APPENDIX", "text": "A.1. Assumptions and notation. The data (S il , U il ) and (T ij , V ij ), i = 1, . . . , n, l = 1, . . . , L i , j = 1, . . . , N i , as described in (1) and (2), are assumed to have the same distribution as (S, U ) and (T, V ), with joint densities g 1 (s, u) and g 2 (t, v). Assume that the observation times S il are i.i.d. with marginal density f S (s). Dependence is allowed for observations U il 1 and U il 2 made for the same subject or cluster, and analogous properties hold for V ij , where T ij are i.i.d. with marginal density f T (t). We make the following assumptions for the number of observations L i and N i that are available for the ith subject:\n(B1.1) The number of observations L i and N i made for the ith subject or cluster are random variables such that L i\n\u223c N , where L and N are positive discrete random variables, with P (L > 1) > 0 and P (N > 1) > 0.\nThe observation times and measurements are assumed to be independent of the number of measurements, that is, for any subsets L i \u2286 {1, . . . , L i } and J i \u2286 {1, . . . , N i }, and for all i = 1, . . . , n, (B1.2) ({S il : l \u2208 L i }, {U il : l \u2208 L i }) is independent of L i , and ({T ij : j \u2208 J i }, {Y ij : j \u2208 J i }) is independent of N i .\nLet K 1 (\u00b7) and K 2 (\u00b7, \u00b7) be nonnegative univariate and bivariate kernel functions that are used in the smoothing steps for the mean functions \u00b5 X , \u00b5 Y , covariance surfaces G X , G Y , and cross-covariance structure C. Assume that K 1 and K 2 are compactly supported densities with zero means and finite variances. Let b X = b X (n), b Y = b Y (n), h X = h X (n), h Y = h Y (n) be the bandwidths for estimating\u03bc X and\u03bc Y (30), G X and G Y (31) , and h 1 = h 1 (n), h 2 = h 2 (n) be the bandwidths for obtaining C (36). We develop asymptotics as the number of subjects n \u2192 \u221e, and require the following: 6 1 \u2192 \u221e and nh 8 1 < \u221e. Define the Fourier transformations of K 1 (u), K 2 (u, v) by \u03ba 1 (t) = e \u2212iut K 1 (u) du and \u03ba 2 (t, s) = e \u2212(iut+ivs) K 2 (u, v) du dv. They satisfy the following: (B3.1) \u03ba 1 (t) is absolutely integrable, that is, |\u03ba 1 (t)| dt < \u221e. (B3.2) \u03ba 2 (t, s) is absolutely integrable, that is, |\u03ba 2 (t, s)| dt ds < \u221e.\nAssume that the fourth moments of Y and U , centered at \u00b5 Y (T ) and \u00b5 X (S), are finite, that is, Let S 1 and S 2 be i.i.d. as S, and U 1 and U 2 be the repeated measurements of X made on the same subject, taken at S 1 and S 2 separately. Assume (S il 1 , S il 2 , U il 1 , U il 2 ), 1 \u2264 l 1 = l 2 \u2264 L i , is identically distributed as (S 1 , S 2 , U 1 , U 2 ) with joint density function g X (s 1 , s 2 , u 1 , u 2 ), and analogously for (T ij 1 , T ij 2 , V ij 1 , V ij 2 ) with identical joint density function g Y (t 1 , t 2 , v 1 , v 2 ). Appropriate regularity assumptions are imposed for the marginal and joint densities, f S (s), f T (t), g 1 (s, u), g 2 (t, v), g X (s 1 , s 2 , u 1 , u 2 ) and g Y (t 1 , t 2 , v 1 , v 2 ) .\nDefine the rank one operator f \u2297 g = f, h y, for f, h \u2208 H, and denote the separable Hilbert space of Hilbert-Schmidt operators on H by F \u2261 \u03c3 2 (H), endowed by T 1 , T 2 F = tr(T 1 T * 2 ) = j T 1 u j , T 2 u j H and T 2 F = T, T F , where T 1 , T 2 , T \u2208 F , and {u j : j \u2265 1} is any complete orthonormal system in H. The covariance operator G X (resp. G X ) is generated by the kernel G X (resp. G X ), that is, G X (f ) = S G X (s, t)f (s) ds, G(f ) = S G X (s, t)f (s) ds. Then G X and G X are Hilbert-Schmidt operators, and Theorem 1 in [31] implies that G X \u2212 G X F = O p (1/( \u221a nh 2 X )). Let I i = {j : \u03c1 j = \u03c1 i }, I \u2032 = {i : |I i | = 1}, where |I i | denotes the number of elements in I i . Let P X j = k\u2208I j \u03c8 k \u2297 \u03c8 k , and P X j = m\u2208I j\u03c8 m \u2297\u03c8 m denote the true and estimated orthogonal projection operators from H to the subspace spanned by {\u03c8 m : m \u2208 I j }. For fixed j, let = {z \u2208 C : |z \u2212 \u03c1 j | = \u03b4 X j }, where C stands for the set of complex numbers. The resolvent of G X (resp. G X ) is denoted by R X (resp. R X ), that is, R X (z) = (G X \u2212 zI) \u22121 [resp. R X (z) = ( G X \u2212 zI) \u22121 ]. Let Y . We assume that the numbers M = M (n) and K = M (n) of included eigenfunctions depend on the sample size n, such that as n \u2192 \u221e, if M (n) \u2192 \u221e and K(n) \u2192 \u221e,\nThe main effect of this assumption is to impose certain constraints on the rate of K and M in relation to n and the bandwidths."}]