[{"section_title": "Abstract", "text": "Remediation is the most common policy designed to prepare students academically and socially during their early stages of college. But despite its profound importance and its significant costs, there is very little rigorous research analyzing its effectiveness. The goal of this paper is to provide a conceptual framework for evaluation of remedial education programs. Based on previous literature, we review a list of ingredients for successful interventions, present a number of approaches to remediation that make use of these ingredients, discuss alternative research designs for systematic evaluations, and enumerate basic data requirements."}, {"section_title": "Introduction", "text": "The \"remediation\" crisis has surely become one of the most controversial issues in higher education in recent times.\n1 Large numbers of students are accepted into colleges and universities who are underprepared for the content and rigor of coursework at this level. And much of the underpreparation involves academic skills that are foundational to learning, such as those used in mathematics, reading, and writing. The demand for remedial courses has increased rapidly in recent decades, especially at community colleges, as their doors began to open to all students whatever their level of academic preparedness (Dougherty, 1994; . Over 60 percent of first-time community college students in the National Education Longitudinal Study (NELS: 88) took at least one remedial course, compared to 29 percent of first-time students in public fouryear institutions (Bailey, Jenkins, & Leinbach, 2005) .\nDespite the need, providing remedial courses is costly to students, institutions, and governments. These courses are costly to students because they usually do not confer college credit; thus students must pay fees and tuition and support themselves without making progress toward a degree. As a result, some students are discouraged from enrolling in the first place and others fail to complete remedial courses in which they enroll (Levin & Koski, 1998; Deil-Amen & Rosenbaum, 2002; Rosenbaum, 2001) . As for institutions, they spend large amounts of resources on remediation and other programs designed to make up for the deficiencies of their diverse entering students. A decade ago Breneman and Haarlow (1998) estimated that public colleges spent between one and two billion dollars annually on remedial education programs.\nMore recently, a report from the Florida Legislature found that remediation at Florida legislatures, which often pay for remediation, question the need to pay twice for academic preparation of the same skills (Merisotis & Phipps, 2000) . 1 In many quarters the term remedial education has been replaced with the term developmental education. We have chosen to use the more traditional term to avoid confusion. Although developmental education is certainly more euphemistic, we view it as somewhat ambiguous because virtually all education is developmental. The ongoing debate about remediation lacks a useful knowledge-base about the effectiveness of different approaches to remediation that could inform policymakers, educators, scholars, and students. As many researchers have already pointed out, the majority of evaluations of remedial education have serious methodological flaws (Grubb, 2001; Bailey & Alfonso, 2005) . The goal of this paper is to provide a conceptual framework for the evaluation of various remedial education programs. Based on previous literature, we review a list of ingredients for successful interventions, present a number of approaches to remediation that make use of these ingredients, discuss alternative research designs for systematic quantitative evaluations, and enumerate basic data requirements."}, {"section_title": "Situating Postsecondary Remediation", "text": "Grubb and Associates (1999, p. 174) defined remediation as \"a class or activity intended to meet the needs of students who initially do not have the skills, experience or orientation necessary to perform at a level that the institutions or instructors recognize as 'regular' for those students.\" These courses have been a prominent feature in community colleges since these institutions first appeared in postsecondary education in the early years of the twentieth century (Cohen & Brawer, 2003) . But community college students who are referred to remedial coursework comprise a very diverse group. They vary from students who have done poorly in high school in all subjects to ones who are deficient in just a single subject. Many are older students who performed satisfactorily in their high school studies, but who have rusty skills because of disuse. Others have very poor study habits or have mild to serious learning problems that must be addressed. Finally many colleges have significant immigrant populations who may possess the underlying academic skills for college level work, but have difficulty with English (English as a Second Language [ESL] students). This tremendous variety of student types suggests that long-term solutions must be diverse.\nInstitutions identify such students either by administering placement tests in basic skills or by noting deficiencies in course completion or grades from high school transcripts. It is especially noteworthy that the placement tests that are used to identify students for remediation are usually calibrated to select students who have severe deficiencies, typically those lacking the skills required at eighth grade. For example, many states contract with the College Board to implement ACCUPLACER tests, a computer adaptive placement testing system designed to facilitate the evaluation and placement of college students in three basic skills areas: reading, writing, and mathematics. Students with milder academic deficiencies are unlikely to be diverted into remedial classes even though they may be underprepared for the academic demands of college. In most cases, remedial courses are mandatory for students identified by placement tests; in other cases, advisors and counselors recommend or encourage students to attend remedial courses, but the courses are not required. Even where there is a presumed requirement, however, students and faculty may still find many ways to avoid remediation."}, {"section_title": "3", "text": "What are the costs of remediation? First there are the direct costs of providing remedial instruction and the duplication of effort in using higher educational institutions to provide instruction on subjects that should have presumably been learned earlier.\nAs noted, Breneman\nand Haarlow (1998) estimated the total annual cost of remedial courses across all types of higher education to be between one and two billion dollars some ten years ago. However, a recent report by the Ohio Board of Regents found that remedial courses are less expensive than collegelevel courses (Glenn & Wagner, 2006) . Moreover, remediation at two-year institutions is less costly than at four-year colleges because many remedial courses are taught by low-paid adjunct (part-time) faculty and have larger class sizes (Bettinger & Long, 2007) .\nCritics of remediation point out, however, that there are many hidden costs as well, such as dilution of rigor in regular college-level courses as more and more remedial students must be accommodated (Bennett, 1994; MacDonald, 1998) . Steinberg (1998) cited examples of regular course offerings that were once viewed as being remedial as well as full-year courses that cover what was once included in one semester. Costrell (1998) also pointed to the fact that the presence of large numbers of remedial students places pressure on instructors to reduce course content and raise grades, diluting the quality of instruction for non-remedial students. What is more, in terms again of monetary costs, Breneman and Haarlow's (1998) estimate does not take account of the foregone earnings that students lose during the period of remediation, even though this is considered a major cost of higher education by economists. Finally, others have argued that remediation carries substantial social costs due to lower completion rates of remedial students (Glenn & Wagner, 2006) .\nThe costs and the large numbers of students enrolled in remedial courses have generated a debate about where remediation should take place. At least ten states prevent or discourage public four-year institutions from offering remedial education (Jenkins & Boswell, 2002) , while many states restrict remediation to two-year institutions. As mentioned above, it is often argued that shifting courses to community colleges can result in substantial savings because it is less costly to remediate students in 2-year institutions than in 4-year colleges. However, opponents argue that community colleges are poorly equipped and funded to deal with the least well prepared students (Astin, 2000) . Moreover, it is argued that if community colleges unwittingly serve to hinder the chances of receiving a bachelor's degree, then this trend toward basing remedial education at two-year colleges will reduce the educational opportunities of minorities, immigrant, and low-income students who are disproportionately less well prepared for postsecondary education (Shaw, 1997) .\nGiven the costs of these programs and the controversies discussed above, there is also a question of whether successful completion of remedial courses assures that students can succeed in the college mainstream. Using NELS data, Attewell et al. (2006) found that about 70 percent of students pass the reading and writing remedial courses they enroll in, but only 30 percent pass all their remedial math courses. 4 However, the degree to which remedial courses improve students' chances of academic success is almost unknown because of a lack of rigorous followup studies. It is not possible to evaluate the effectiveness of remedial courses and practices without a rigorous evaluation design that takes account of student proficiencies and other characteristics. There is in fact little definitive evidence on the effectiveness of remedial courses and practices on persistence to graduation, quality of performance in subsequent courses, grade point average, and so on in the relevant literature."}, {"section_title": "Choosing Remedial Interventions", "text": "If there is any consensus among educators concerning remediation, it is that so-called drill-and-skill approaches are falling out of favor. Yet, while there is no reliable national survey on teaching techniques for remedial courses in community colleges, casual observation at many sites suggests that drill-and-skill approaches are still dominant (e.g. Grubb and Associates, 1999) . Such courses are based upon the presentation of concepts, operations, or classification schemes and repetitive practice to master them, and they are often combined with learning laboratories. This style of pedagogy has many drawbacks, including the fact that many remedial students have serious attitudinal obstacles to learning in this way. Often it is the same style that the students were exposed to in high school, which may have contributed to their difficulties in the first place. Beyond that, its abstract and isolated nature may prevent students from seeing its usefulness in real-world situations and from applying the skills that are learned to later academic and vocational coursework.\nBased on previous literature on remediation in higher education and adult learning, Levin and Koski (1998) found the following ingredients to be central for designing successful interventions for underprepared students in higher education:"}, {"section_title": "5", "text": "-Motivation: building on the interests and goals of the students and providing institutional credit toward degrees or certificates.\n-Substance: building skills within a substantive or real-world context as opposed to a more abstract approach.\n-Inquiry: developing students' inquiry and research skills to help them learn about other subjects and areas about which they might be curious.\n-Independence: encouraging students to do independent meandering within the course structure to develop their own ideas, applications, and understandings.\n-Multiple Approaches: using collaboration and teamwork, technology, tutoring, and independent investigation as suited to student needs. 5 Many of these ingredients have been applied successfully in the Accelerated Schools Project in which so-called atrisk students are provided with enrichment experiences, a pedagogy associated with gifted and talented instruction instead of remediation (Levin, 1997; Finnan & Swanson, 2000) . See Bloom et al. (2001) for evaluation results.\n-High Standards: setting high standards and expectations that all students will meet if they make adequate efforts and are given appropriate resources to support their learning.\n-Problem Solving: Viewing learning less as an encyclopedic endeavor and more as a way of determining what needs to be learned and how, and then implementing \"the how.\"\n-Connectiveness: emphasizing the links among different subjects and experiences and how they can contribute to learning rather than seeing each subject and learning experience as isolated and independent.\n-Supportive Context: recognizing that to a large degree learning is a social activity that thrives on healthy social interaction, encouragement, and support.\nA number of approaches that build on these ingredients have shown some success relative to isolated courses or study skills. We have grouped these practices into three categories:\n(1) restructuring college or remedial curriculum; (2) developing new institutional structures; and (3) employing specific instructional strategies or technologies designed to enhance learning.\nThese categories are not mutually exclusive, and many examples of each one can be (and are) used to supplement and complement another."}, {"section_title": "Restructured Curriculum", "text": "A large number of remedial education experts and practitioners recommend that basic skills be taught in conjunction with content course materials so that students gain experience in transferring these skills to tasks that are perceived to be \"real\" (Blanc, DeBuhr, & Martin, 1983; Commander, & Smith, 1995; Wilcox, delMas, Stewart, Johnson, & Ghere, 1997; Stone & Jacobs, 2006) . They believe that skills taught in isolation are less likely to be applied productively to further coursework. The obvious solution is to tie basic skills development to concrete applications in academic or vocational courses.\nThere exist many models for doing this, including adjunct courses, tandem classes, paired courses, packaged courses, linked courses, and, in a variant form, supplemental instruction and learning communities. As an overall description, we will call all these linked courses. At their very essence, linked courses offer a basic skills subject such as writing or study skills to students who are simultaneously registered in a credit-bearing content course, such as history or science.\nMaterials in the content course are used for instruction in the remedial course. In such content-based remedial courses, the content course may be one of the student's own selection or tied to a specific lower-division course.\nBoth Supplemental Instruction (SI) and the learning community are variants of the linked courses model. Although SI is decidedly not a remedial program and does not link a specific skill-building course to the remedial course, it embeds effective learning and study strategies in a course that supplements the remedial course (Martin & Arendale, 1994; Arendale, 2005) . 6 This supplemental course is most often taught by a trained student who has already succeeded in the remedial course. The learning community links courses for a group of co-registrants and provides other supportive academic and social services to students. Thus it combines a new approach to curriculum with larger institutional changes, as discussed below."}, {"section_title": "Developing New Institutional Structures", "text": "The learning community is one approach to remediation that shows particular promise based on results from experimental (Tinto, 1997; Brock & LeBlanc, 2005) and quasiexperimental (Wilcox et al., 1997) evaluations. 7 It is built upon the well-established finding that persistence and success in higher education depends not only on the quality of instruction, but also on the integration of students into the social and academic life of the institution (Tinto, 1993) . In this approach, groups of courses are offered that are taken together by the same cohort of students with the purpose of establishing student communities around learning. At the same time, these communities are provided with other supports that integrate the social and academic sides of college participation and engage students more fully in the life of the institution.\nExamples of such supports are college orientation courses or \"student success\" classes that focus on learning styles, study skills, time management, and successful habits. 8 Learning communities usually also include a restructured curriculum as well as other features, such as student-faculty collaboration-usually a sharing of knowledge about courses among students, faculty, and staff.\nIn addition to the remedial courses themselves, many community colleges have created learning assistance centers, which are quite diverse in the types of services they offer. In general, 6 In a seminal article, Blanc et al. (1983) described an early SI program and reported the evaluation results. 7 For reviews of learning communities, see Taylor, K. & Associates (2003) and Price (2005) . 8 See Florida Department of Education (2006) and Zeidenberg, Jenkins, & Calcagno (2007) for non-experimental evaluation results of the effects of \"student life skills\" (SLS) programs on community college student success.\nthese centers are independent of traditional academic departments and holistic in their approach to student development. Such services are open to all students, but they tend to focus particular attention on the needs of remedial students (Perin, 2004) . Nevertheless, because they are designed to meet the needs of all students who desire some extra assistance, the impact of such support services on remedial students is unclear. Typical services include career counseling, peer and faculty tutoring, group tutoring, self-guided computer-based instruction, study skills classes, and additional diagnostic testing."}, {"section_title": "Modified Classroom Strategies", "text": "Modifying what goes on in the classroom, that is, adopting alternative instructional strategies and technologies, is another approach in remediation. These strategies include meaning-centered methods in reading remediation and collaborative learning. Such strategies are based on knowledge of how young adults and adult learners are best taught. Some strategies also use technology and computers to deliver basic skills. Many institutions have implemented selfdirected computer-assisted modules that instruct underprepared students in their areas of weakness and also provide diagnostic feedback and monitoring of progress on a highly individualized basis.\nFinally, some educators advocate instruction in higher-order thinking skills across the curriculum, including the remedial curriculum (Chaffee, 1992; . 9 Critical thinking, complex problem solving, and abstract reasoning have long been the hallmarks of programs aimed at the academically gifted, but such skills are traditionally considered not within the realm of immediate possibility for most remedial students, who are assumed to need to acquire basic skills first. Some educators have challenged this notion, however, and have asserted that instruction in critical thinking can benefit all students, including remedial students.\nThis limited discussion of approaches to remediation illustrates the plethora of possibilities available in the education of remedial students. Often different approaches to remediation are utilized within the same institution, varying from subject to subject and 9 The Critical Thinking Program at LaGuardia Community College was evaluated using a variety of strategies by Dr. Garlie Forehand of the Educational Testing Service (ETS). See Chaffee (1985) .\nclassroom to classroom with the inventiveness of individual staff. Although costs and effectiveness can be compared among interventions to see if there are clear distinctions among them in terms of productivity, rarely are these different approaches subject to rigorous evaluation to ascertain their effects. The fact that there are so many possibilities and, as discussed below, little comparative data on the efficacy of different approaches raises a dilemma for community colleges about which strategies to adopt. In order to improve performance in preparing remedial students for college-level courses and to compare outcomes across interventions, an institution might wish to answer a number of questions:\n1-What are the background characteristics of students taking remedial courses?\n2-What proportion of the students who are required to take remediation courses actually enroll and pass the courses and with how many attempts?\n3-What levels of proficiency are exhibited by students who pass courses in each remedial subject relative to non-remedial students? What are the best approaches for addressing the needs of an institution's underprepared students? And how can its faculty and staff understand the impacts of specific interventions?\nAnswering these questions will require a systematic evaluation of each intervention and background data on students to control statistically for other differences between remedial and non-remedial students. Both these components are analyzed below."}, {"section_title": "Evaluating Interventions: Methodological Problems and a Hierarchy of Solutions", "text": "We begin with the truism that the consequences of an intervention cannot be known without systematic evaluation. Both the costs and educational outcomes of alternative interventions need to be ascertained in order to choose wisely among different approaches. Light, Singer, and Willett (1990) have shown that many outcomes can be assessed through systematic evaluation. This section discusses the empirical problems associated with evaluating the effectiveness of remedial interventions and suggests a range of rigorous evaluation designs.\nThe estimation of costs is not discussed here but can be determined through a careful cost analysis as discussed in Levin and McEwan (2001) .\nThe lack of high-quality research evidence on the effectiveness of remediation is due The main statistical problem in estimating the effectiveness of remedial courses is that it is difficult to identify a causal relationship between remediation and educational attainment.\nStudents are not randomly assigned to remedial education; therefore, factors unobserved by the statistician may also influence future outcomes of remedial students. Therefore, if we simply compare the performances of remedial versus non-remedial students in terms of educational outcomes, the former group will perform far worse than the latter group due mainly to precollege differences rather than to the program itself (Bettinger & Long, 2005a; Grubb, 2001 ). We should, instead, compare only those remedial and non-remedial students who actually share similar backgrounds and academic preparedness. By doing so, the effects of an intervention can be attributed to the program rather than to pre-college differences.\nA related problem is that evaluations on later academic success typically do not take into account the fact that those who successfully complete remediation courses will do better than those who drop out. A generally accepted finding in the literature is that students who successfully complete remedial courses have better educational outcomes than similar non-remedial students (e.g. Attewell et al., 2006) . However, the group of completers of remedial coursework is in fact a self-selected sample of all remedial students. Therefore, a comparison between successful remediation completers and non-remedial students biases the results of the intervention upward because the group of low-performing students who dropped out without completing the remedial sequences are excluded from the analysis. If students are discouraged from enrolling in remediation in the first place, or enroll but fail to complete remedial courses, this discouragement effect should be taken into account in the evaluation.\nMethodological flaws abound in previous studies. Even so, the hierarchy of methods to obtain causal inferences is widely known by quantitatively-oriented applied researchers. The \"gold standard\" for evaluation is random assignment, followed by quasi-experiments, and, finally, non-experimental designs (Shadish, Cook, & Campbell, 2002 In the next sections, we discuss each of these research designs with examples of applications used to evaluate remedial education programs found in recent research contributions. Depending on resources and data available, these designs can be used for evaluation at the institutional, state, or national level. Likewise, all these designs can be used either to estimate the efficacy of broadly-defined remedial programs or to evaluate specific practices, such as those discussed in the previous section."}, {"section_title": "Experimental Designs", "text": "Experimental designs based upon random assignment of students to treatments provide the most credible evidence of the consequences of an intervention. 10 In a typical experiment, a substantial pool of eligible students are randomly assigned to a treatment or a control group, and this property guarantees no systematic differences in measured and unmeasured characteristics of the two groups at the start. A control group generated through random assignment provides the best counterfactual to describe what would have happened to students in a treatment group if they had not been exposed to the treatment (Rubin, 1974; Holland, 1986) . Hence, any differences 10 A good source on random assignment and experimental design is Riecken and Boruch (1974). in outcomes between treatment and comparison groups can be attributed to the program.\nBaseline measures of student proficiency in the subject as well as attitudinal variables are taken to assure that randomization has produced equivalent groups. The equivalency of the two groups can be confirmed by measuring and comparing motivation, attitude toward the subject, high school courses and grades, demographic variables (age, gender, race, socioeconomic status), English language proficiency, living conditions, and family and work obligations.\nAt the beginning and end of the intervention, proficiency tests in the subject and attitudinal measures toward it are administered to compare gains in proficiency and attitudinal orientations. In addition, post-intervention measures are made of student completions, attendance, and pass rates. Comparisons are also made of attrition rates and their timing and whether attrition is experienced by similar types of students. Statistical adjustments are sometimes made for differential attrition if the experimental group (those students exposed to the intervention) and control group (those students who proceed in the traditional fashion) end up with different compositions of students. Presumably, the difference in performance on educational outcome measures can be attributed to the intervention relative to the traditional treatment. These are short-term results in that there is no assurance that they will be maintained over the long run, so longer-run outcomes such as persistence toward degree completion and performance in higher level courses must be examined at a later time.\nOf course, the internal validity of this design is enhanced if, when one (experimental) version of a remedial course is being compared to its traditional counterpart, the same instructor is used for both. This may be impractical, however, if a single instructor is not equally enthusiastic or adept at both approaches. Alternatively, when two different instructors are selected, it is desirable to assign instructors with comparable past success in teaching similar students. Past student evaluations might be used to make such a selection. Also, an attempt must be made to avoid both Hawthorne and John Henry effects. Hawthorne effects are created by students being identified as receiving special treatment or status by virtue of being chosen for the intervention. The conferment of special status in the experimental period may stimulate higher motivation and achievement, but the same effects may not be present when the intervention is replicated and routinized under more conventional conditions. The John Henry effect refers to the initiation of a special effort on the part of the instructor or students in the traditional course to show that they can beat the alternative intervention, even though this effort is unlikely to be marshaled or sustained under routine implementation. Presence of the Hawthorne effect overstates the efficacy of the intervention when replicated routinely; presence of the John Henry effect understates it.\nIt is also wise to document the intervention carefully. This can be accomplished by using qualitative methods that provide classroom observations describing activities and interactions among students and between instructor and students. Reading materials and assignments can be evaluated for their content and the demands that they place on students. Periodic measures of student attitudes toward the course, attendance, and hours devoted to preparing for class are valuable indicators of student behavior. This information can be used later to analyze the actual differences (as opposed to just the differences in design) between the experimental and the traditional courses with a possibility of identifying salient features that contribute to success.\nRandom assignment evaluations of remediation are scarce. When they do occur, they are often carried out to evaluate learning communities' interventions. For example, a comprehensive evaluation of a learning communities program and its cost is being conducted at Kingsborough Community College as part of the Opening Doors Demonstration (Brock & LeBlanc, 2005) .\nIncoming first-year students who meet certain eligibility criteria were randomly assigned either to the learning communities program or to a control group that participates in the standard course format. 11 The intervention for the experimental group consisted of three classes (a college orientation, an English, and a standard college course) that groups of approximately 25 students take together as a block during their first semester. Opening Doors also includes a voucher that covers the cost of the books, and students have access to a dedicated tutor who can assist them with English and other course assignments (Bloom & Sommo, 2005) . Preliminary findings suggest that incoming first-year students in the learning communities program passed remedial English courses at higher rates and earned more course credits overall after one year of followup than students in the control group (Bloom & Sommo, 2005) . Importantly, students assigned to the learning communities were also more than twice as likely to pass the English examination required by the City University of New York for students to graduate or transfer to a four-year school.\nTrue experiments such as the one just described are costly and very uncommon in education. Among other reasons for this, Cook (2002) argues that philosophical and practical reasons undermine the potential benefits of experiments in education. Besides the fact that institutions rarely have the capacity to undertake experimentation of this sort, there is also the practical problem of having enough students taking particular courses at precisely the same time to assign them randomly. However, the latter challenge can be overcome in many community colleges because of the very large number of students who must take remediation. Multiple sections can be established that meet at precisely the same time, and students who register for the course can be assigned randomly to specific sections. It can be argued that there is no ethical dilemma here in terms of assigning some students to \"better\" instruction and others to \"poorer\" instruction as long as there is no history of definitive advantages for either alternative. "}, {"section_title": "Quasi-Experimental Designs", "text": "Random assignment is considered the \"gold standard\" for evaluation, but random assignment must be carried out correctly and with adequate numbers of students in order to assure the similarity of comparison groups and the power to detect policy-relevant impacts (Light, Singer, & Willett, 1990, pp. 106-114; Riecken & Boruch, 1974) . Also, while random assignment can help in improving internal validity, there is frequently a tradeoff involved: a reduction in the generalizability of results to other settings (also known as external validity).\nOften the only realistic alternative to random assignment is a quasi-experimental design, which attempts to simulate experimental studies through statistical controls (Shadish et al., 2002) . By definition, quasi-experiments lack random assignment, but researchers often have control over design elements, such as the covariates used to assign to treatment that allow them to construct useful counterfactual inference. Shadish et al. consider that regression-discontinuity, interrupted time-series, and matched cohort designs are the strongest alternatives to randomized experiments. Nevertheless, such quasi-experimental designs ought to include a careful analysis to identify and reduce the possibility of alternative causal explanations.\nResearch on remedial education can take advantage of assignment policies as sources of quasi-experimental design. As examples, we review here four quasi-experimental strategies using assignment policies to evaluate effectiveness of remedial programs that can be used at the state or institutional level: (1) regression-discontinuity design using cutoff assignment policies;\n(2) instrumental variable approach when institutions within a state use different cutoff policies but the same standardized test; (3) matched cohorts when assignment is referred, but not mandatory; (4) changes in assignment policies. All these designs can be used either to estimate the efficacy of broadly-defined remedial programs or to evaluate specific practices such as those discussed above.\nOne common strategy at the institutional or state level is to use standardized placement tests and cutoff policies to assign students to remedial classes. For example, in the State of Florida all first-time-in-college degree-seeking students entering a community college or state university must demonstrate certain basic skills before beginning college-level courses. Basic skills are measured using standardized test scores, and minimum score requirements are set statewide and established by the State Board of Education. Incoming students who do not achieve minimum scores in each subject area on the exams must take remedial classes before they begin college-level work in each subject. In other words, students are assigned to either remedial or college-level courses, depending on their scores on the standardized tests, not by coin toss or lottery as in a randomized experiment.\nMandatory cutoffs create a sharp discontinuity in the probability of receiving remedial classes that can be exploited for evaluation. Students with test scores above a cutoff are not assigned to remediation and those scoring below are referred to remedial classes. A regressiondiscontinuity design uses a basic linear regression framework for a given outcome (e.g., passing first college-level course) and a remediation indicator and the test score, or any other variable used for assignment, as independent variables. 13 This natural experiment allows a comparison between two observationally-alike students, one receiving remediation and the other not, but only differing in that one scored just below the cutoff score and the other just above it. After including the test score in the statistical model, assignment to remediation is not correlated with the error term (such correlation is the fundamental cause of distortions due to selection bias discussed above). The regression coefficient can be interpreted as the causal impact of the intervention for students on the margin of passing the cutoff, comparable to a randomized experiment (Rubin, 1977; Trochim, 1984) .\n14 One example of this regression-discontinuity evaluation design is provided by Calcagno (2007), who took advantage of this statewide cutoff policy in Florida to estimate the causal impact of remedial education on students' educational outcomes. Results suggest a positive effect of remediation on the likelihood of enrolling in the following fall term for students on the margin of passing the cutoff. The author found no significant difference in terms of passing first college-level courses, associate degree completion rates, or chances of transferring to a state four-year college between remedial and non-remedial students on the margin of the cutoff. The program appears to increase the time to observe these educational outcomes for math remedial students as compared with academically equivalent peers. However, there is no statistical evidence that remediation extends time-to-degree for academically equivalent students who received an associate degree in Florida. This same research design could be used in other states using cutoff policies, such as Texas and Maryland, or in individual institutions when cutoffs are known."}, {"section_title": "15", "text": "In other states, such as Ohio, Connecticut, and Virginia, the state authorities encourage institutions to use particular standardized placement tests, but the assignment rules are left to the individual institutions. For these cases, researchers can combine the different institutional cutoff policies and the distance from students' homes to higher education institutions to estimate effectiveness of programs at the state level. Previous research has shown that students are more likely to attend one college over another depending on how close the colleges are to their homes (Rouse, 1995) , but given between-college variation in remediation placement policies, the distance from home also affects the probability of receiving remediation for different students.\n14 A straightforward instrumental variable solution is available when institutions do not comply with statewide cutoff policies. The same approach is used in experimental designs when participation is voluntary and some noncompliers are expected. See Brock & LeBlanc (2005) , Appendix A; Lesik (2006); . 15 For examples of applications at the institutional level, see Lesik (2006) and Moss & Yeaton (2006) . Therefore, this situation generates a natural experiment where researchers can compare two observationally-alike students, one assigned to remedial courses and the other not.\nIn practice, the analytical technique for estimating the program effect in this setting is an instrumental variable approach. 16 The \"instrument\" is created by combining two-step regressions. In the first step, researchers estimate the probability of assignment to remediation given the test scores and the different cutoff policies used by different institutions. The second stage estimates the likelihood of attending college X given the distance from student's home to each college. The output of these regressions generates a combined variable as the product between the likelihood of a student choosing a given institution and the likelihood of being placed into remediation at that college. This instrument does influence the assignment to remediation but it does not have an impact on outcomes (a condition also known as the exclusion restriction).\nBettinger and Long (2005a) used this strategy on data from public four-year institutions in Ohio. They found that math and English remediation reduced the likelihood of dropping out and increased the likelihood of completing a degree. Remedial students taking English classes were also less likely to transfer to a less selective or lower level college in comparison to similar non-remedial students. The same authors extended their research to compare the outcomes of similar students taking the ACT, but who enrolled in two-year institutions in Ohio (Bettinger & Long, 2005b) and found that students placed in math remediation were 15% more likely to transfer to a four-year college than students with similar test scores and high school preparation who attended colleges with policies that did not require placement in remedial classes. These students were also more likely than their natural comparison group to take ten more credits hours. Results for English remediation were not statistically significant across all of the outcomes.\nSome states require institutions to assess the academic preparedness of students and place them accordingly, but without specifying a standardized placement test or cutoff scores. Under this policy incentive, institutions rely heavily on faculty perceptions of need for remediation. Jepsen (2006) exploits this flexibility to estimate the impact of remedial classes in California community colleges (CCC). Basic-skills enrollment is not mandatory in the CCC system. 16 A good source on instrumental variables techniques is Wooldridge (2003) .\nFaculty members encourage students to enroll in remedial classes based on multiple assessments, but students can decide whether or not to enroll. Hence, the author matched students into pairs that were considered similar in terms of preparation for college-level work by faculty or staff, one who decided to enroll in remedial classes with another who did not. Data problems prevented the author from calculating statewide effects, but preliminary results for twelve institutions suggest that remedial classes are positively associated with second-term continuation in college and completing a transfer-level class. For younger students, the classes are negatively related to the probability of transferring, whereas for older students the classes are positively related to the probability of receiving a degree or certificate.\nFinally, other sources for quasi-experimental designs are policy changes at the institutional, state, or national level. Lavin, Alba, and Silberstein (1981) underprepared students decided to enroll at the university, but the institution was not able to provide remediation for all of the students who scored low on the placement test or to introduce mandatory remedial classes for them. As a result, the authors were able to identify a group of comparable students identified as candidates for remediation, where some elected to take the courses and some did not. After controlling statistically for differences in high school background and level of need for remediation, they found that success in remedial courses increased by about seven to eight percent the probability that students would return for a second year, and two to three percent the probability of graduation or transfer to a senior institution, all results relative to similar students not taking the courses. Paradoxically, students taking remedial courses did not have better grade-point averages than similar students who did not take remedial work. However, students not taking remedial courses may have had other means (e.g., tutoring) that provided academic help, something not measured in this study."}, {"section_title": "Non-Experimental Designs and Mixed Methods", "text": "A non-experimental design does not include random assignment components or any \"controllable\" design elements that could allow researchers to generate natural counterfactuals.\nThis approach is the weakest of the evaluation designs, with wide variance in application and potential validity. When employing this design, researchers generally use statistical techniques to select a comparison group on the basis of observable pre-treatment differences. Researchers often try to eliminate selection bias through a statistical assumption-that by including control variables (e.g., prior academic results and socioeconomic status), selection to remedial courses can be ignored. 17 Unfortunately, the assumption becomes difficult to defend in the context of higher education because key factors such as motivation or ability are difficult to measure. The internal validity of the results depends on the quality of the techniques and their capacity to control for unobservable characteristics.\nNon-experimental designs to evaluate the effectiveness of remedial courses should be conducted only when key student characteristics such as socioeconomic background, test scores, and data on previous educational experiences are well-measured. 18 A careful non-experimental evaluation at the national level was conducted by Attewell et al. (2006) using NELS:88. The authors used propensity score matching and key student background characteristics to provide a detailed picture of the effectiveness of remedial courses. 19 Their results suggest that, on average, remediation has no negative effect on degree completion at two-year colleges (although results depend on the subject of the course), but a six to seven percent negative effect on degree completion at four-year colleges (especially for reading). Yet, they also found evidence that community college students who successfully complete remedial courses have better educational outcomes than similar non-remedial students, a result that is consistently supported by previous research (Lavin et al., 1981; Moss & Yeaton, 2006) . Finally, it is also possible to use mixed methods in conducting research by combining quantitative and qualitative approaches. For example, researchers may first use one of the previously discussed quantitative methods to rank institutions according to the effectiveness of remedial classes. Then, an arbitrary number of top-and low-ranking institutions could be selected for field research. Qualitative methods might then be used for an in-depth study of 17 This statistical assumption is called \"selection on observables\" or the \"ignorability condition.\" 18 The best known non-experimental evaluation of remedial education programs and their program characteristics was conducted in the early 1990s by the National Center for Developmental Education (Boylan, Bliss, & Bonham, 1992; . However, only brief summaries have been reported for this major national study, and the reports exclude information that would be helpful for evaluating the effectiveness of remediation (Bailey & Alfonso, 2005) . 19 See Bloom (2005) for a good summary on properties of propensity score matching and its controversies.\npolicies and practices that could help explain the differences in educational outcomes of remedial and non-remedial students at different institutions."}, {"section_title": "Data Sources and Requirements", "text": "Sources of data for experimental designs generally include both surveys and academic records over several years, and these data are usually collected specifically for a particular study.\nMost of the quantitative information used in quasi-and non-experimental designs, however, is regularly collected by states or institutions through administrative datasets, as illustrated in the cases of Florida, Ohio, and California. Both the educational outcomes and control variables that are chosen for use in such a study depend upon the specifics of the student populations and the goals of the intervention. For example, in the case of a course that is designed to improve writing and reasoning skills, the following might be viewed as potential educational short-term-outcome variables: course completion, grade, writing performance, reasoning performance, attendance, and course satisfaction. Long-term outcomes might include first college-level course enrollment and completion, fall-to-fall retention, certificate or degree completion, and baccalaureate transfer.\nIf randomization is properly achieved and the experimental design minimizes the problems discussed above, then comparing average outcomes for treatment and control groups using standard statistical tests should suffice to estimate program impacts. In other cases, an attempt should be made to gather not only baseline data on each student's performance, but also demographic data and previous educational performance of students to control statistically for pre-intervention differences between treatment and control groups. For experimental designs, and certainly for quasi-and non-experimental designs, researchers might gather information for each student using the following variables: (1) baseline writing performance; (2) baseline reasoning performance; (3) gender; (4) race; (5) whether student is a native English speaker; (6) age; (7) socioeconomic status; (8) high school courses (specific courses or GED); (9) grade point average in high school in pertinent courses; (10) work hours per week; (11) hours devoted to household demands (e.g., care of others); (12) word processing skills and access to computer.\nThese control variables can be used to adjust outcomes statistically for differences in the students who are included in each intervention by using a multivariate analysis (Shadish et al., 2002) .\nData on costs are also important. Interventions have different costs attached to them, and these need to be compared along with effectiveness to ascertain if an intervention under study is \"worth it.\" Levin and McEwan (2001) provide the analytical tools for a careful cost analysis of remedial education interventions. Different costs may be associated with various interventions because of differences in the use of adjunct versus full-time faculty or because of limits on class size or other resources, such as the use of counselors or extra faculty time, that are required for some interventions. These can be taken into account when comparing intervention alternatives.\nIn some cases, more expensive interventions may be more cost-effective when one considers that costs associated with students who leave community college without completing even the most basic requirements represent squandered resources for the college, the student, and society. If a higher-cost remedial intervention increases substantially the number of students who complete basic requirements and graduate or transfer to four year institutions, the additional cost may be more than compensated by a reduction in poorly-spent resources."}, {"section_title": "Discussion", "text": "The view taken in this paper is that the number of underprepared students at both community and four-year colleges has become alarmingly high. In the long run, vast improvements in elementary and secondary education will be needed to raise the quality of preparation for postsecondary education. But for the foreseeable future, U.S. higher education institutions will be faced with the formidable challenge of assisting large numbers of underprepared students so they can succeed in postsecondary education.\nIn this respect, we make two central assertions. First, the traditional drill-and-skill approach currently used to raise the performance of remedial students at community colleges is not as productive as other available alternatives. A number of remedial approaches discussed here have shown some success, especially in terms of reducing dropout and failures rates.\nSecond, community colleges need to carry out formal evaluations of different remedial approaches to test their efficacy and cost-effectiveness in order to pursue a wise remediation strategy. This paper's contribution to the literature is a comprehensive discussion of research design for quantitative evaluations of effectiveness that can be used by institutions.\nPreviously, Levin (1991) contended that higher education institutions need to become \"experimenting institutions\" if they are to continually improve their productivity. They need to constantly seek better ways of meeting their objectives and to test and replicate them with an eye toward the cost-effectiveness of alternatives. Why does this ideal seem far-removed from the actual practices of many such institutions, and, in particular, community colleges?\nExperimenting institutions that build \"local\" knowledge which they can use for policy decisions must have clear goals, incentives to reach those goals, information on present performance, and information on the likely consequences of alternatives. Yet institutional research carried out at most community colleges is not aligned with these elements. Instead, institutional research largely entails the gathering of information for simple reports of cross-sectional data required by federal and state governments and accreditation agencies. In most cases the institutional research function is staffed by only a single professional with a small amount of clerical support, and in some cases, that professional lacks training and experience in evaluation (Morest & Jenkins, 2007) . In short, there exists little real capacity to carry out rigorous research at most community colleges.\nThese institutional conditions do not augur well for experimentation and systematic\nevaluation. Yet some steps could be taken that would assist community colleges. It probably makes sense to establish a central resource at the state level to assist colleges and individual faculty members in creating experimental interventions and to provide support for evaluating them. Standard intervention designs and data collection centers could be established as well as methods for analyzing data on outcomes and costs. Faculty and administrators could collaborate with the evaluation staff inside or outside of their institutions in order to specify the appropriate outcomes and control variables, to help administer the data instruments, and to assist in the interpretation of results. Implementing steps such as these could encourage improvements in the evaluation of various educational practices and programs, including remedial interventions."}]