[{"section_title": "Abstract", "text": "Abstract. We propose a model of brain atrophy as a function of high dimensional genetic information and low dimensional covariates such as gender, age, APOE gene, and disease status. A nonparametric single index Bayesian model of high dimension is proposed to study the data with B-spline series prior on the unknown functions and Dirichlet process scale mixture of centered normal prior on the distributions of the random effects. The posterior rate of contraction without the random effect is established for a fixed number of regions and time points with increasing sample size. The performance of the proposed Bayesian method is compared with the corresponding least square estimator in the linear model with LASSO and SCAD penalization on the high dimensional covariates. The proposed Bayesian method is applied to a dataset of 748 individuals with 620,901 SNPs and 6 other covariates for each individual, to identify factors associated with significant variation."}, {"section_title": "Introduction", "text": "Alzheimer's disease (AD) is a progressive neurodegenerative disease that affects approximately 5.5 million people in the United States and about 30 million people worldwide. It is believed to have a prolonged preclinical phase initially characterized by the development of silent pathologic changes when patients appear to be clinically normal, followed by mild cognitive impairment (MCI) and then dementia (AD) (Petrella (2013) ). Apart from its manifestation in the impairment of cognitive abilities, disease progression also produces a number of structural changes in the human brain, which includes the deposition of amyloid protein and the shrinkage or atrophy for certain regions of the brain over time (Thompson et al. (2003) ). Previous studies have shown that the rate of brain atrophy is significantly modulated by a number of factors, such as gender, age, baseline cognitive status and most markedly, allelic variants in the Apolipoprotein E (APOE) gene (Hostage et al. (2014) ). In this paper, we examine if any other genes are also implicated in modulating the rate of brain atrophy along with examining effects of the low dimensional covariate on the rate of atrophy using the data, collected by Alzheimer's Disease Neuroimaging Initiative (ADNI).\nAging has a significant effect on cerebral atrophy (Nagata et al., 1987) . There are some research works on the association of brain atrophy with Alzheimer's disease (Devanand et al., 2007; Kopelman, 1989; Sabuncu et al., 2011) . McDonald et al. (2009) studied longitudinal magnetic resonance imaging (MRI) data to evaluate the effect of age and Alzheimer's disease on the brain atrophy. Apart from age, genetic variations also have significant effect on brain atrophy (Schott et al., 2016; Gregory et al., 2006; Hostage et al., 2014) .\nThe data, we are using here, have not been extracted directly from the magnetic resonance (MR) images by us. We collect this data directly from ADNI. We have volumetric measurements over six visits of thirteen disjoint brain regions and a total brain measure which is a summary measure of total brain parenchyma, including the Cerebral-Cortex, Cerebellum-Cortex, Thalamus-Proper, CaudatePutamen, Pallidum, Hippocampus, Amygdala, Accumbens-area, VentralDC, Cerebral-White-Matter, Cerebellum-White-Matter, and WM-hypointensities. These visits are roughly around six months apart. Thus the subjects are scanned roughly for three years. The anatomic structures of the brain also differ across different individuals and are assumed to be dependent on subject-specific covariates like genetic variations, gender, age etc. In this paper, we propose a model to study the effects of these different covariates along with Alzheimer's disease state on brain atrophy in different brain regions. This analysis represents a technical challenge because the genomic data is high dimensional and needs to be incorporated in a model for longitudinal progression of brain volumes measured in multiple parts of the brain in a non-parametric setup. A schematic of the regions, we studied here, are depicted in the Figure 1 . This image is obtained from Ahveninen et al. (2012) . Although our analysis does not have any association with this paper, an image from it is used to show the brain regions used for the analysis in our paper. We have not collected the data directly from the magnetic resonance (MR) images according to some brain parcellation atlas. We got the data in a preprocessed form from ADNI itself. (Hostage et al., 2014) had also used a similar set of regions of interest.\nWe consider two separate sets of unknown functions of covariates X and Z to model the volumetric measurements of the first visit and rates of changes for different regions. These functions have two inputs. The first X consists of high dimensional single-nucleotide polymorphism (SNP) of each individual, and the other Z consists of low-dimensional covariates like gender, age, disease state, APOE gene status of each individual. These functions {a 0,j (X \u03b2, Z \u03b7) : j = 1, . . . , 14} for the volumetric measurements of the first visit and {a 1,j (X \u03b2, Z \u03b7) : j = 1, . . . , 14} for the rate of change in the jth region and the coefficients \u03b2 and \u03b7 are unit vectors of appropriate dimensions. A finite random series prior is put on the functions based on tensor products of B-splines with appropriate prior distribution on the coefficients. To take care of the high dimensionality of X, the coefficient \u03b2 is assumed to be sparse. We reparametrize \u03b2 in polar coordinates to incorporate sparsity in its prior. We incorporate the effect of time in the modeling by an increasing function which is estimated nonparametrically also using a finite random series of on B-splines with appropriate prior on the coefficients.\nApart from proposing a sophisticated model for studying brain atrophy, the proposed method develops new estimation scheme for a general high dimensional single index model. Estimation for high dimensional single index model is addressed in Zhu (2009), Yu and Ruppert (2002) , Wang et al. (2012) , Peng and Huang (2011), Radchenko (2015) and Luo and Ghosal (2016) . All of them used the 1 -penalty and used optimization techniques to get the estimates. In a Bayesian framework, Antoniadis et al. (2004) used the Fisher-von Mises prior to the directional vector. This cannot be easily modified for a high dimensional covariate as then we shall need a prior which favors many zeros in the unit vector. Another paper addressing sparse Bayesian single index model estimation is Alquier and Biau (2013) . Even though their method is theoretically attractive, it is difficult to implement for high dimensional covariate due to its high computational complexity. Wang (2009) developed a Bayesian method for sparse single index model using reversible jump Markov chain Monte Carlo (MCMC) technique which is computationally expensive, especially in the high dimensional situation. We introduce a new way of incorporating sparsity on a unit vector by a spike and slab prior to the polar form. The computation scheme is based on a stochastic search variable selection technique. To make our method more accessible to users, we provide an R package for estimation in single index model with inputs both in high and low dimensional setup.\nThe rest of the paper is organized in the following manner. The next section discusses the dataset and modeling in more detail. In Section 3, we describe the prior on different parameters of the model. Section 4 describes posterior computation in this setup. We study the posterior rate of contraction in the model in Section 5 under the asymptotic regime that the number of individuals goes to infinity but the number of time points where measurements are taken and the number of regions is fixed. We present a simulation study comparing the proposed Bayesian procedure with its linear counterpart in Section 6. The concentration of the posterior justifies the use of the proposed Bayesian procedure from a frequentist perspective. In Section 7, we present conclusions from the ADNI data on brain atrophy described above using our proposed method. Section 8, concludes the paper with some further remarks."}, {"section_title": "Data description and modeling", "text": "Data used in the preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). ADNI was launched in 2003 as a public-private partnership, led by Principal Investigator Michael W. Weiner, MD. The primary goal of ADNI has been to test whether serial magnetic resonance imaging (MRI), positron emission tomography (PET), other biological markers, and clinical and neuropsychological assessment can be combined to predict the progression of mild cognitive impairment (MCI) and early Alzheimer's disease (AD). In the ADNI dataset, the grey matter part of the brain is divided into thirteen disjoint regions. The volume of these regions and the summary measure of the whole brain are recorded over time for n = 748 individuals. The number of visits is not uniform across individuals but varies between 1 to 6. The volume data of J = 14 regions which include thirteen brain regions and the summary measure of the whole brain over T i many time points for the ith individual, for i = 1, . . . , 748 is collected where 1 \u2264 T i \u2264 6.\nIn ADNI, the subjects were genotyped using the Human 610-Quad BeadChip (Illumina, Inc., San Diego, CA), yielding a set of 620,901 SNP and copy number variation (CNV) markers. The APOE gene has been the most significant gene in GWAS of Alzheimer's disease. The corresponding SNPs, rs429358 and rs7412, are not on the Human 610-Quad Bead-Chip. At the time of participant enrollment, APOE genotyping was performed and included in the ADNI database. The two SNPs (rs429358, rs7412) define the epsilon 2, 3, and 4 alleles and therefore were not genotyped using DNA extracted by Cogenics from a 3 mL aliquot of EDTA blood. These alleles are considered separately in the study.\nThus apart from the volumetric measurements, we also have high dimensional SNP data and data on some other covariates for each individual. The other covariates are gender, disease state, age and allele 2 and 4 of the APOE gene. Except for the covariate age, all other low-dimensional covariates are categorical. To represent the categories, we use binary dummy variables. Since the disease status has three states-NC (no cognitive impairment), MCI (mild cognitive impairment) and AD (Alzheimer's disease), we consider two dummy variables Z AD and Z MCI respectively standing for the onset of MCI and AD, setting NC at the baseline. Similarly, the dummy variable Z M indicating male gender is introduced setting females at the baseline. Also, we introduce Z APOE,2 , Z APOE,4 standing for Alleles 2 and 4 for the two alleles APOEallele2 and APOEallele4 together setting Allele 3 as a baseline for each of the two cases. We consider the age corresponding to the initial visit as a covariate as well. Let Z = (Z MCI , Z AD , Z M , Age, Z APOE,2 , Z APOE,4 ) stand for the whole vector of covariates. The continuous variable, Age, is standardized.\nWith time, different brain regions change differently. We study the effects of different attributes to these changes. For every individual, the volume of a brain region on a particular visit should primarily depend on the volume of that region at the zeroth visit and the rate of change of volume for that region with time. Neither of these is uniform across individuals or regions. Hence, it is logical to consider the baseline volume and the rate of change as functions on the subject and brain region. As the geometry of brain structure is complicated, we do not assume a form of standard spatial dependence between measurements across brain regions. Thus we need two sets {a 0,j (\u00b7), a 1,j (\u00b7) : j = 1, . . . , 14} of functions for modeling volume at the initial visit and the rate of change for the jth region. These functions are unknown and are modeled nonparametrically. For nonparametric regression problems, single-index models provide a lot of flexibility in estimation and interpretation of the results. Hence we adopt the bivariate single-index model with two inputs for high-dimensional and low-dimensional covariates separately for easy interpretation and computational efficiency. The effect of time is captured through an unknown increasing function F 0 (\u00b7), which is bounded in [0, 1] . This is also modeled nonparametrically.\nLet Y ijt is the volume of the jth brain region for the ith individual at the tth time point in the logarithmic scale, X i is high-dimensional SNP expression of length p for the ith individual, t = 1, . . . , T i , i = 1, . . . , m and j = 1, . . . , 14. Then the data generating process can be represented through the following specification\nwhere a 0,j (\u00b7), a 1,j (\u00b7), F 0 (\u00b7) are all unknown continuous, monotone increasing functions from [0, 1] to [0, 1] and F 0 (0) = 0, F 0 (1) = 1, and N stands for a normal distribution. For identifiability of the functions along with the parameters \u03b2 and \u03b7, we assume that \u03b2 = 1, \u03b7 = 1; here \u00b7 denotes L 2 -norm of a vector. We also normalize the covariates for each individual i.e. X i and Z i for each i such that X i and Z i are one. This is to ensure that X i \u03b2 and Z i \u03b7 are bounded between [\u22121, 1] for each i. For nonparametric function estimation, bounded domain is important for uniform approximation using the basis expansion. The biggest challenge for estimation in this model is the high dimensionality of \u03b2. To identify important SNPs from X, we need to perform variable selection. To do that, we propose a sparse estimation scheme for \u03b2. First we reparametrize the two unit vector \u03b2 = (\u03b2 1 , . . . , \u03b2 p ) and \u03b7 = (\u03b7 1 , . . . , \u03b7 k ) to their respective polar forms which allow us to work with Euclidean spaces. In this setup, only the directions of \u03b2 and \u03b7 are identifiable. Note that \u03b2 and \u2212\u03b2 have the same directions. In the polar setup, for s = 1, . . . , p \u2212 1, \u03b2 s = s\u22121 l=1 sin \u03b8 l cos \u03b8 s , and \u03b2 p = p\u22121 l=1 sin \u03b8 l where {\u03b8 = (\u03b8 1 , . . . , \u03b8 p\u22121 )} is the polar angle corresponding to the unit vector \u03b2. Here \u03b8 s \u2208 [0, \u03c0] for s = 1, . . . , (p \u2212 2) and \u03b8 p\u22121 \u2208 [0, 2\u03c0]. Similarly, let \u03b1 be the polar angle corresponding to \u03b7. Then for"}, {"section_title": "Prior specification", "text": "In the nonparametric Bayesian setup described above, we induce prior distributions on the smooth functions a 0,j and a 1,j in (2.1) through basis expansions in tensor products of B-splines and suitable prior for the corresponding coefficients. Given other parameters in this setup, a normal prior distribution on the coefficients of the tensor products of B-splines will lead to conjugacy and faster sampling via Gibbs sampling scheme. An inverse gamma prior on \u03c3 2 is an obvious choice due to conjugacy and faster sampling. We also put a B-spline series prior on the smooth increasing function of time F 0 (\u00b7). The coefficients for this function would be increasing in the index of the basis functions and lie in (0,1]. To put a prior on an increasing sequence, we introduce a set of latent variables of size equal to one less than the number of B-spline coefficients. Then the Bspline coefficients would be normalized cumulative sum of those latent variables. Other two parameters \u03b2 and \u03b7 are reparametrized into their polar coordinate system. The parameter space of the polar angles will be a hyper-rectangle. It will be easier to put prior on the polar angles. To estimate using the sparsity of \u03b2, we need to carefully put a shrinkage prior to the polar angles. A polar angle of \u03c0/2 will ensure that the corresponding coordinate in the unit vector equals to zero. When there is sparsity in the unit vector, most of the polar angles will be \u03c0/2. Thus a spike and slab prior to polar angle with a spike of \u03c0/2 should be able to capture sparsity in the corresponding unit vector. The last polar angle has spike both at 0 and \u03c0/2, due to the special structure of the last and the penultimate co-ordinates of a unit vector in the polar form. Since only the directions of \u03b2 and \u03b7 are identifiable, the intercept and slope functions are modeled as even functions i.e. symmetric around zero. Now we describe the prior in details. Let x denote the lowest integer greater than or equal to x. (ii) The function of time:\nWe put a prior on (\u03bb 2 , . . . , \u03bb K \u22121 ) by reparameterizing as \u03bb l+1 = (\n, and putting the prior \u03b4 i \u223c Un(0, 1) for l = 1, . . . , K \u2212 1, where Un stands for the uniform distribution. Further, K is given a prior with probability mass function of the form \u03a0(\n, where Ga stands for the gamma distribution.\n(iv) Polar angles \u03b1 of \u03b7: We let \u03b1 r \u223c Un(0, \u03c0), r = 1, . . . , (k\u22122), and \u03b1 k\u22121 \u223c Un(0, 2\u03c0), independently.\n(v) Polar angles \u03b8 of \u03b2: We put a spike and slab prior on the polar angles that has spike at \u03c0/2 for the first polar angle and all the multiples of \u03c0/2 for the last polar angle. Then the spike distribution would look like Figure 2 . The spike and slab prior for \u03b8 r , r \u2264 (p \u2212 2), has density given by\nfor 0 < \u03b8 r < \u03c0 and the distribution of \u03b8 p\u22121 is given by\nhere Be(M 1 , M 2 ) [a,b] denotes the beta distribution with shape parameters M 1 and M 2 , supported within the interval [a, b] , and\nThe spike distribution on \u03b8 looks like Figure 2 . The first plot is for the first (p \u2212 2) angles and the second plot is for the last polar angle.\nNote that either geometric or Poisson distribution (respectively b 3 = 0 and 1) can be chosen as prior on K , the number of terms to be used in the B-spline series for the growth function F 0 . For K, the square, which is the number of terms in the tensor product series representation, can have a geometric or Poisson-like tail.\nModel selection: Polar angles with a posterior probability of selection in the model more than 0.5 are considered in the model."}, {"section_title": "Computation", "text": "Introduce a latent variable I l for the indicator of the Un(0,1) component of the distribution of \u03b8 l , l = 1, . . . , p \u2212 1. Now the conditional log-likelihood is given by \nwhere C involves only the hyperparameters a, M 2 , M 1 , K, d 1 , d 2 and the observations but not parameter of the model.\nAll the B-spline coefficients and \u03c3 are updated using the conjugacy structure. All the posterior updates are discussed in the supplementary materials.\nIn our computation of the model, we are not using the priors on K and K i.e. the number of B-spline basis functions as it will require reversible jump MCMC strategy which is computationally expensive. Given the data, we choose these values by following a particular strategy and are kept fixed in the MCMC scheme. These are chosen by minimizing the Akaike Information Criterion (AIC) after fitting the model over a grid of a number of B-spline basis functions for randomly generated 10 different choices of \u03b2 and \u03b7. We generate 10 different choices for \u03b2 and \u03b7 and then fit the non-linear model for different choices of a number of B-spline basis functions. After taking an average overall 10 AIC values for each case, we take the number as optimal that has the least AIC value."}, {"section_title": "Large-sample Properties", "text": "In this section, we examine large sample properties of the proposed Bayesian procedure for the model (2.1). We have observations for fixed J number of regions and T many time points. We show posterior consistency in the asymptotic regime of increasing sample size and increasing dimension p of SNPs.\nWe study posterior contraction rate with respect to the empirical 2 -distance on the regression function, which is defined as follows. For two sets of parameters (F, a, \u03b2, \u03b7) and (F * , a * , \u03b2 * , \u03b7 * ), the empirical 2 -distance is given by\nand a = (a 0,j , a 1,j : j = 1, . . . , J), a * = (a * 0,j , a * 1,j : j = 1, . . . , J). Since \u03b2 is a high-dimensional parameter, sensible estimation is possible only if it has sparsity, which must be picked up by the prior. In the setting of a spike-and-slab prior for polar coordinates described in Section 3, we need to ensure sufficient concentration near \u03c0/2 by choosing a large value of the second parameter M 2 in the beta spike distribution (depending on the sample size n and the dimension p) and a small value of the probability of slab \u03b3. More precisely, we choose M 1 \u2264 1 fixed, M 2 > \u221a np log p and \u03b3 = o(p \u22121 ). Then the contraction rate will be determined by the smoothnesses of the underlying true functions a 0,0 , a 0,1 and F 0,0 , the sparsity s 0 of true high dimensional regression coefficient \u03b2 0 and mildly on the parameter b 3 , b 3 in the prior distribution for the number of basis elements K, K used in the B-spline bases, as shown by the following result. \nIn the above result, since the observation points are not dense over the domain, posterior contraction on the function is based on its distance with the true function only at the observation points.\nThe proof of the theorem uses the general theory of posterior contraction (see Ghosal and van der Vaart (2017) ) for independent non-identically distributed observations and some estimates for finite random series based on B-splines. The proof is given in the supplementary materials part."}, {"section_title": "Simulation", "text": "We compare our method with some common penalization methods based on the following simplified linear model\n. . , n, j = 1, . . . , 13. In the above model, \u03b2 is a sparse vector and all other parameters are unpenalized. The performance of these methods is compared based on MSE values on a test set under the scenarios the linear model is correct and the linear model is false. For sample sizes 200, 500 and 1000, we gather the mean squared error (MSE) values corresponding to those models both for well-specified and misspecified cases. We use half of the sample for training and the remaining half for testing. Among other parameters, we consider thirteen regions in total, five time points and vary the value of p as 5000, 10000 and 20000. We include one case for ultra high dimension of p = 100000 and sample size 200. We set M 1 = 0.1 and tune M 2 for different cases to ensure a good acceptance rate and desired model size (sum of q i 's) across MCMC samples. The results are summarized below for 30 replications and 1000 post burn-in samples. The number of basis functions for spline is different across different sample sizes. For the ultra-high dimensional case with p = 100000 and n = 200, we consider 10 replications."}, {"section_title": "Data generation for the non-linear case:", "text": "\u2022 Generate a data matrix X with elements coming from Bernoulli distribution with success probability of the ith row as p i .\n\u2022 Generate p i , i = 1, . . . , n, from the standard uniform distribution.\n\u2022 Generate all the elements of the matrix Z from N(0, 1).\n\u2022 Generate the sparse vector \u03b2 with 5% elements non-zero. Positions for non-zero elements are chosen first at random by sampling p/20 elements from total p positions, where p is the length of \u03b2. The non zero elements are generated from mixture distribution of two normals N(2, 1) and N(\u22121, 1).\n\u2022 Set the value of \u03b7 to (1, \u22122, 4.3, 10, \u22128).\n\u2022 Normalize each row of X and Z along with \u03b2 are \u03b7 to the unit norm.\nWe let the true functions be a 0,0,j (x, y) = exp((j/13)x) + exp((1 \u2212 j/13)y), a 0,1,j (x, y) = 3 exp(\u2212(j/13)y) + 4 exp(\u2212(1 \u2212 j/13)x), j = 1, . . . , 13, and F 0,0 (t) = t 2 . After generating the true functional values, the data Y ijt is generated from N(a 0,j (X i \u03b2, Z i \u03b7) \u2212 a 1,j (X i \u03b2, Z i \u03b7)F 0 (t), 1)."}, {"section_title": "Data generation for the linear case:", "text": "In this case, the true model is the one given in (6.1). All the steps for generating X, Z, \u03b2 and \u03b7 are similar as above. The coefficients \u03b3 1,j and \u03b3 2,j , j = 1, . . . , 13, are generated from N(0, 1). After generating the design matrix, the data Y ijt is generated from N(\nFor the data generation, we set the error standard deviation \u03c3 0 = 1.\nWe compare the prediction MSEs across all the methods. We split the whole data into two equal parts for training and testing. We compare the performance of our method with LASSO (Tibshirani (1996) ) and SCAD (Fan and Li (2001) ) on testing dataset based estimates of parameters from the training dataset. We use the R package glmnet for LASSO and ncvreg for SCAD. To fit our model we vary the the number of B-spline basis function as 8 for n = 200, 11 for n = 500, and 14 for n = 1000. These numbers are chosen according to the strategy, described in the Section 4. From Table 1 , we infer that the performance of the proposed Bayesian method based on the high dimensional single index model is always much better than the LASSO and the SCAD for the non-linear case. For the linear case in Table 2 , it is competitive with linearity based methods like the LASSO or the SCAD. This is natural as the LASSO or the SCAD use more precise modeling information which the semiparametric methods cannot use.\n7 Real-data analysis 7.1 Modification of the model for real data application Incorporating random effect and region wise varying effect As the data are longitudinal time series, it is reasonable to add subject specific random effect (\u03c4 i ) in the model and vary the effect of the low dimensional covariates region-wise. The new modified model will then become "}, {"section_title": "Prior on the random effects", "text": "We put a Dirichlet process scale mixture of normal prior on the random effect distribution."}, {"section_title": "Region-wise varying effect with no SNP", "text": "To compare the nonlinear model with the linear model of Hostage et al. (2014) , we also fit the following model without the SNPs,\n2) t = 1, . . . , T i with 1 \u2264 T i \u2264 6, j = 1, . . . , 14, i = 1, . . . , 748."}, {"section_title": "Corresponding Linear Model", "text": "We compare the performance of our above non-linear models with following linear model,\nwhere t = 1, . . . , T i with 1 \u2264 T i \u2264 6, j = 1, . . . , 14, i = 1, . . . , 748, and We have volumetric measurement data for the total thirteen brain regions along with the summary measure of the whole brain over time for 748 individuals. For each individual, we have covariate information which is summarized in Table 3 . The standard deviations for age in each group are mentioned in the bracket. The baseline subject for our analysis is a female individual with average age and no cognitive impairment. We first fit the model in (7.2) and the following linear model in (7.3) in accordance with Hostage et al. (2014) with same set of covariates and interactions between APOE and disease states. Then we compare the prediction MSE. Prediction error gives us the predictive performance and fitted relative MSE helps to judge the reliability of inference. We consider 17 basis B-spline functions for univariate and 17 2 basis functions for bivariate cases.\nLinear model gives the prediction error 3.83 whereas that in our non-linear model hugely improves to 0.07. The model in (7.1) with SNPs betters the prediction error to 0.06 which is around 14% improvement. For the prediction error, we divide the whole dataset into training (Tr) and testing (Te). We use stratified sampling using each subject-region pair as stratum so that training will have all the individuals that belong to the testing set. This is important for prediction with a random effect in the model. The formula for prediction error will be |Te| \u22121\n2 ; here |Te| denotes total number of elements in test set Te.\nAfter selecting the significant genes, we calculate the Bayesian information criterion (BIC) of models leaving out one of the low dimensional covariates every time with all the genes to compare the significance. The table below gives an ordered list of the significance of low dimensional covariates for different regions in Tables 4 to 6 (broken down into three tables). In Table 7 , we show the estimates from the linear model in (7.3) for the whole brain.\nWe map the significant SNPs from our analysis to corresponding genes using the R package rsnps. We tune the parameter \u03b3 in the model to select 20 most significant SNPs. Among those, we identify 11 of those. The significant genes from our analysis are mentioned in Section 8 along with some previous studies that found the corresponding gene significant for the AD and/or cerebral atrophy. "}, {"section_title": "Conclusions and discussion", "text": "We fit a bivariate single index model to capture the volumetric change of different cortical regions in the human brain. There are both high and low-dimensional covariates as input to the unknown functions determining initial configuration and rate of change of different regions. To tackle the high dimensional covariate within a single index model, we provide a new technique to assign sparse prior in this paper. Posterior consistency results are also established. An 'R' package is attached with this paper as a supplementary material (https://github.com/royarkaprava/SIMBayes). This can be used to fit high and low dimensional single index models. This package can fit a single index model with only one input or two inputs with at most one in shrinkage consideration. The function betaupdate of this package is used to generate MCMC samples for polar angles of \u03b2 and \u03b7 of our model.\nIn our results on the real dataset, we find that allele 4 of APOE gene is always among the top three significant covariates for almost all the cases in Table 4 to 6. The fact that allele 4 of the APOE gene is significant was established in Hostage et al. (2014) . They used a linear model, similar to the model in (7.3). But in our estimates of parameters for the linear case in Table 7 , APOE allele 4 is not significant. Such is the case for several other regions in the linear case. Since allele 4 is not found to be significant here in the linear model, for this dataset linear model is not suitable. We identify 11 significant genes. There are some previous studies that also noted the significant genes from our analysis as possible candidates for the AD and/or cerebral atrophy. The genes along with associated future study citing that gene in connection with AD and/or cerebral atrophy are mentioned here SLC6A1 (Carvill et al., 2015) , KCNIP4 (Himes et al., 2013) , ADGRL3 (Orsini et al., 2016) , SORBS2 (Zhang et al., 2016; Lee et al., 2014; Niceta et al., 2015) , LPAR3 (Yung et al., 2015) , SHROOM3 (Dickson et al., 2015; FreudenbergHua et al., 2016) , SORCS3 (Breiderhoff et al., 2013; Lane et al., 2012) , NPY2R (Lin (Lin et al., 2013) , PALLD (Nho et al., 2015) and KCNMA1 (Burns et al., 2011; Tabarki et al., 2016) tutes of Health (www.fnih.org). The grantee organization is the Northern California Institute for Research and Education, and the study is coordinated by the Alzheimer's Therapeutic Research Institute at the University of Southern California. ADNI data are disseminated by the Laboratory for Neuro Imaging at the University of Southern California.\nThe first author would like to thank Mr. Kushal Kumar Dey for helping him how to build an 'R' package.\nWe are grateful to the anonymous reviewer and the editor for their valuable comments that have greatly helped improve the manuscript."}]