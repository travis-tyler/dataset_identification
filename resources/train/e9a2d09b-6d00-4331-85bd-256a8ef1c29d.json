[{"section_title": "Abstract", "text": "Motivated by the analysis of imaging data, we propose a novel functional varying-coefficient single index model (FVCSIM) to carry out the regression analysis of functional response data on a set of covariates of interest. FVCSIM represents a new extension of varying-coefficient single index models for scalar responses collected from cross-sectional and longitudinal studies. An efficient estimation procedure is developed to iteratively estimate varying coefficient functions, link functions, index parameter vectors, and the covariance function of individual functions. We systematically examine the asymptotic properties of all estimators including the weak convergence of the estimated varying coefficient functions, the asymptotic distribution of the estimated index parameter vectors, and the uniform convergence rate of the estimated covariance function and their spectrum. Simulation studies are carried out to assess the finite-sample performance of the proposed procedure. We apply FVCSIM to investigating the development of white matter diffusivities along the corpus callosum skeleton obtained from Alzheimer's Disease Neuroimaging Initiative (ADNI) study."}, {"section_title": "Introduction", "text": "The aim of this paper is to develop a new functional regression model for the regression analysis of imaging (or functional) data collected over space and/or time, such as diffusion tensor imaging (DTI) and positron emission tomography (PET) (Towle et al. (1993) ; Niedereyer & da Silva (2005) ; Buzsaki (2011); Heywood et al. (2006) ; Zhu et al. (2007) ; Friston (2009)) . A common feature of many imaging techniques is that massive functional data are observed/calculated at the same grid points, such as voxels in a three-dimensional space. Let y i (s) = (y i1 (s), \u2026, y iJ (s)) T be a J-dimensional functional response vector measured at a set of the same M location points, denoted as = {s 1 , \u2026, s M }, for subject i, i = 1, \u2026, n. We propose a novel functional varying-coefficient single index model given by (1) where X i is a p\u00d71 covariate vector with varying coefficient functions \u03b1 j (s) = (\u03b1 j1 (s), \u22ef, \u03b1 jp (s)) T , g j (\u00b7) is an unknown link function, Z i is a q \u00d7 1 vector of covariates with its index parameter vector \u03b2 j \u2208 \u211d q , \u03b5 ij (s) is the measurement error, and \u03b7 ij (s) characterizes individual curve variations. The random function {\u03b7 ij (s) : s \u2208 [0, 1]} is assumed to be a stochastic process with mean zero and covariance function R(s, t) = cov{\u03b7 i (s), \u03b7 i (t)}, where \u03b7 i (s) = (\u03b7 i1 (s), \u22ef, \u03b7 iJ (s)) T . The error terms are mean zero process with covariance \u2130(s, t) = cov{\u03b5 i (s), \u03b5 i (t)}, where \u03b5 i (s) = (\u03b5 i1 (s), \u22ef, \u03b5 iJ (s)) T . Moreover, \u03b5 i (s) and \u03b5 i (t) are assumed to be independent for s \u2260 t and \u2130(s, t) takes the form of \u2130(s, s)I(s = t), where I(\u00b7) is an indicator function. Model (1) allows a joint model of J different measures obtained from a single imaging modality or multiple modalties. For instance, for DTI, standard imaging measures include fractional anisotropy (FA) and mean diffusivity (MD).\nModel (1) can be regarded as a novel integation of varying-coefficient models and single index models. When g j (\u00b7) \u2261 0, model (1) reduces to varying coefficient models widely adopted in the existing literature. This class of models has been thoroughly studied and developed for longitudinal, time series, and functional data. For varying coefficient models, it is of particular interest in data analysis to construct simultaneous confidence bands for \u03b1 j (\u00b7) and to develop global test statistics for the hypotheses regarding \u03b1 j (\u00b7). See Hoover et al. (1998) , Fan & Zhang (1999) , Wu & Chiang (2000) , Ramsay & Silverman (2005) , Fan et al. (2003) , Morris & Carroll (2006) , Fan & Zhang (2008) , Wang et al. (2008) , Cheng et al. (2009 ), Zhang (2011 and Zhu et al. (2012) for various statistical procedures proposed for different varying coefficient models. In particular, Morris & Carroll (2006) developed a Bayesian wavelet-based approach for a functional mixed effects modeling framework. Zhu et al. (2012) developed several statistical inference procedures for multivariate varying coefficient models for functional responses and systematically studied their theoretical properties.\nIn the absence of \u03b1 j (s), that is, \u03b1 j (s) \u2261 0, model (1) reduces to single index models for functional responses with unknown link functions g j (\u00b7). For identifiability, it is often assumed that and the first component of \u03b2 j is positive. Single index models have been widely studied and developed for cross-sectional, longitudinal, and functional data. For single index models, one of the most important objectives in data analysis is to estimate and test the model index coefficients \u03b2 j . See Horowitz (2009) for a comprehensive review of single index models. There is also a great interest in developing partial-linear single-index models via the integration of single-index models with linear regression models (Carroll et al., 1997; Wang et al., 2010) . Most earlier references focus on univariate response observed from cross-sectional studies (Xia et al., 2002; H\u00e4rdle et al., 1993; Xia, 2006; Wang et al., 2010; Cui et al., 2011; Ma & Zhu, 2013) . Recently, Jiang & Wang (2010) developed functional single index models for functional/longitudinal response data and derived their associated estimation method and asymptotic theory.\nOur FVCSIM can also be regarded as a novel extension of varying-coefficient single-index model (VCSIM) for scalar response data (Wong et al., 2008; Wang & Xue, 2011) . There are at least three key differences between FVCSIM and VCSIM. (i) Our FVCSIM (1) is developed for multi-dimensional functional responses and explicitly incorporates long-range spatial correlation among functional data, whereas VCSIMs were developed for scalar responses without spatial correlation.\n(ii) Our FVCSIM requires a more sophisticated estimation method in order to account for both spatial correlation and spatial smoothness of imaging data. In contrast, in Wong et al. (2008) , a bivariate kernel smoother was used to estimate the two types of functions, whereas in Wang & Xue (2011), a mean difference approach was proposed to reduce VCSIM to a varying-coefficient model. Bivariate smoothing usually produces less stable solutions and the mean difference approach relies on smoothing y and X first and may introduce additional biases. These two estimation methods are not directly applicable to correlated imaging data. (iii) The theoretical properties of various estimates in FVCSIM differ significantly from those in VCSIM. For FVCSIM, one must deal with the long-range spatial correlation of imaging data. First, such spatial correlation introduces a non-negligible effect into the asymptotic covariance of estimates, which leads to a different convergence rate for semiparametric modeling compared with the corresponding results for VCSIM. Second, the asymptotic behavior of estimated functional random effects and their covariance decomposition needs to be carefully evaluated and uniform consistency results must be established. Third, one can obtain much stronger theoretical properties associated with the estimated coefficients and functions, especially after we incorporate the estimated covariance of correlated image data. This property is very relevant to the facilitation of efficient inference.\nCompared with the existing literature on functional data, our FVCSIM is a novel integration of single index models and varying-coefficient models for functional response data. At each grid point s, model (1) reduces to a partial-linear single-index model, whereas the functional single index model considered in Jiang & Wang (2010) reduces to a standard single-index model. Therefore, our model (1) differs from the functional single index model in Jiang & Wang (2010) . Compared with Zhu et al. (2012) and Morris & Carroll (2006) , model (1) is much more flexible in accommodating both the dynamic effects of X i and the stationary effects of Z i on functional data. Our estimation and inference procedures differ substantially from those in Jiang & Wang (2010) , Zhu et al. (2012) , and Morris & Carroll (2006) . For instance, we can establish the weak convergence of the estimated varying co-efficient functions and apply such a result to construct their confidence bands. Compared with the existing literature, we also establish the asymptotic distribution of single-index estimate in a totally different setting, while such extension requires non-trivial development in numerical implementation and theoretical derivation.\nThe rest of this article is organized as follows. In Section 2, an efficient estimation procedure is developed to iteratively estimate all unknown parameters and functions in model (1). In Section 3, we develop test statistics to test hypothesis of interest associated with varyingcoefficient functions and/or index parameter vectors. Section 4 systematically investigates the asymptotic properties of various estimates and test statistics. In Section 5, simulation studies are used to examine the finite-sample performance of the proposed estimates and test statistics. In Section 6, we apply FVCSIM to investigate the development of white matter diffusivities along the corpus callosum skeleton obtained from ADNI. Section 7 concludes the paper with some discussions. Technical details are given in the supplementary document."}, {"section_title": "The Estimation Procedure", "text": "We develop an estimation procedure to estimate the varying coefficient functions \u03b1 j (s), the single index functions g j (\u00b7), the index parameter vectors \u03b2 j , and the covariance function R(s, s\u2032). For notational simplicity, we assume s m \u2208 [0, 1] such that s 1 \u2264 \u22ef \u2264 s M , but our results can be easily extended to a compact subset of Euclidean space."}, {"section_title": "Varying-Coefficient Functions and Index Parameter Vectors", "text": "We adopt the local linear approximation to estimate the varying coefficient functions and the single index functions. Specifically, we use the Taylor's expansion to obtain\nwhere h j1 and h j2 are bandwidths, \u03b1j(s) = d\u03b1 j (s)/ds and \u0121 j (t) = dg j (t)/dt, s m is in a small neighborhood of s, and z T \u03b2 j is in a small neighborhood of . Moreover, we set a j = \u03b1 j (s), b j = \u03b1j(s)h j1 , c j = g j (z T \u03b2 j ), and . One may directly minimize an objective function given by where K h 1 ,h 2 (x, y) = K(x/h 1 , y/h 2 )/(h 1 h 2 ) is a scaled kernel with two bandwidths h 1 and h 2 .\nThe kernel that we choose in this paper is the product of two one-dimensional kernels such that K(x, y) = K(x)K(y). However, since it is computationally difficult to directly minimize (3), we consider an iterative estimation approach as follows.\nStep 0. Initialization step: At each s m (m = 1, \u22ef, M), we fit a partly linear single index model (4) using data available at s m . We then take and as the initial single index function and coefficient estimates, where . Write\n. To obtain initial varyingcoefficient estimates \u03b1\u0135(s), we then fit a varying-coefficient model (Zhu et al. (2012)) given by (5) Step 1. Let ji (z) = (1, (Z i \u2212z) T \u03b2\u0135/h j2 ) T . For a given \u03b2\u0135 and \u03b1\u0135(s), we can solve (6) to obtain (7) where .\nStep 2. Let . Given \u011d j (z T \u03b2 j ), we update the estimate of \u03b1 j (s) by minimizing (8) which leads to (9) where .\nStep 3. Let \u03b4 Z,i,i\u2032 = Z i \u2212 Z i\u2032 . Given \u03b1\u0135(s) = \u00e2 j , and the estimate for \u03b2 j at the preceding step, denoted by , we update the estimate of \u03b2 j by minimizing (10) where , which leads to\nwhere .\nStep 4. Normalize the updated \u03b2\u0135 such that ||\u03b2|| = 1 and then repeat Steps 1 to 3 until convergence.\nThe two bandwidths h j1 and h j2 involved in the above procedure are selected by using the cross-validation in our implementation. Since only univariate local linear regression is used in the estimation procedure, our procedure is numerically much more stable than those used in Wong et al. (2008) and Wang & Xue (2011) . In our simulations, it usually takes less than 10 iterations for a stringent convergence criterion, in which the average of the squared differences between two consecutive estimates is less than 10 \u22125 ."}, {"section_title": "Individual Functions and Covariance Function", "text": "For clustered data with fixed M locations, we may use empirical estimates for the random effects and the covariance matrix. In this paper we consider a more general approach that can meet this estimation goal and further adapt to functional data. Our methods may be appealing when M is large (as in our brain image example) and it is reasonable to consider the covariance to be a smooth function of the locations.\nWe employ the local linear regression to estimate all individual functions. Consider the Taylor's expansion for \u03b7 ij (s m ) around s given by (12) where a j = \u03b7 ij (s) and , in which \u03b7i j (s) = d\u03b7 ij (s)/ds. Let , we minimize (13) The estimates of a j and b j are given by (14) The minimizer \u00e2 j gives the estimator \u03b7\u00ee j (s m ). We obtain \u03b7\u00ee(s) = (\u03b7\u0302i 1 (s), \u22ef, \u03b7\u00ee J (s)) T . The bandwidth can be selected by the usual cross-validation or the generalized crossvalidation.\nThe covariance matrix R(s, t) can thus be estimated by an empirical covariance estimator (15) Subsequently, we can calculate the spectral decomposition of R\u0135 j (s, t) for each j as follows: ( 16) where \u03bb\u0135 1 \u2265 \u03bb\u0135 2 \u2265 \u2026 \u2265 0 are estimated eigenvalues and the \u03c8\u0135 l (t)'s are the corresponding estimated principal components. Furthermore, the (j, l)-th functional principal component scores can be computed using for i = 1, \u2026, n.\nThe noise variance function \u2130(s, s) measures the variation of \u03b5(s) and its estimation can be based on the residuals by following Hall & Marron (1990) and Fan & Yao (1998) . The kernel estimator for \u2130 jj (s, s) with a bandwidth h ** is given by (17)"}, {"section_title": "Refined Function amd Parameter Estimation", "text": "The estimated covariance structure may be incorporated to account for the spatial dependence in the model and further improve the estimation results. In this section, we adopt the estimated covariance matrix to obtain more efficient estimates for regression coefficients. Let \u03a6 j = R jj (s k , s l ) + \u2130 jj (s k , s l ) k,l=1, \u2026, M be the covariance matrix for the j-th response observed at M grid points in the sample. When \u03a6 j is given, we may obtain the following refined estimates gj(\u00b7), \u03b1j(\u00b7) and \u03b2j for g j (\u00b7), \u03b1 j (\u00b7) and \u03b2 j , respectively. We modify the three iterative steps in Section 2.1 as follows.\nStep 1. Let 1 M be an M \u00d7 1 vector of ones, s = (s 1 , \u22ef, s M ), y ij (s) = (y ij (s 1 ), \u22ef, y ij (s M )) T , and \u03b1 j (s) = (\u03b1 j (s 1 ), \u22ef, \u03b1 j (s M )). Given \u03b2\u0135 and \u03b1\u0135(s), we can solve (18) which yields that is equal to Step 2. Let\n. Given gj(z T \u03b2 j ), we update the estimate of \u03b1 j (s) by minimizing (20) where Kj = diag(K h j 1 (s 1 \u2212s), \u22ef, K h j 1 (s 1 \u2212s)). This leads to (21) Step 3. Given \u03b1j(s) = \u00e3 j , and the initial estimate \u03b2\u0135, we update the estimate of \u03b2 j by minimizing (22) which leads to (23) The bandwidths h j1 and h j2 are also selected by the cross-validation. To differentiate from the initial bandwidths, we denote the selected bandwidths as and .\nWe provide the asymptotic properties of the refined estimates in Theorem 4 of this paper. Lemma 5 in the Appendix justifies their optimality for the estimation of Euclidean model parameters. In practice, the unknown \u03a6 j may be estimated from (R\u0302j j (s k , s l ) + \u2130\u0302j j (s k , s l ) k,l=1, \u2026, M using (15) and (17)."}, {"section_title": "Hypothesis Test and Inference", "text": "It is of interest to carry out hypothesis test and other inference procedures after estimation. We provide a general guideline on how to test hypothesis of interest and construct confidence intervals for parametric and nonparametric components. Let \u03b1(s) = (\u03b1 1 (s) T , \u2026, \u03b1 J (s) T ) T and . Let \u03b1j 0 (\u00b7), gj 0 (\u00b7) and \u03b2j 0 be the estimators of unknown functions and parameters under the null hypothesis and \u03b1j 1 (\u00b7), gj 1 (\u00b7) and \u03b2j 1 be the corresponding estimators under the alternative hypothesis.\nFirst, we consider the test of the hypotheses for the varying coefficients functions as follows: (24) where A is an l \u00d7 Jp full rank matrix and a 0 (s) is an l \u00d7 1 vector of known functions. Write d(s) = A \u00d7 vec(\u03b1\u0303( 1) (s)) \u2212 a 0 (s), where vec(\u03b1\u0303( 1) (s)) = (\u03b1\u03031 1 (s) T , \u2026, \u03b1J 1 (s) T ) T . We may construct a global test as follows: (25) where . Although we will prove the asymptotic null distribution of T \u03b1 in Theorem 4, such theoretical result does not produce a good approximation when the sample size n is relatively small. We thus propose the following bootstrap test procedure.\nStep 1. Fit model (1) under H 0 to obtain \u03b1j 0 (\u00b7), gj 0 (\u00b7), \u03b2j 0 , \u03b7\u0129 j0 (s m ), and \u03b5\u0129 j0 (s m ).\nStep 2. Draw and for i = 1, \u2026, n and m = 1, \u2026, M independently from the standard normal distribution and construct (26) Step 3. Refit model (1) using \u0177 ij (s m ) (b) as the response values and calculate T \u03b1,b using formula (25) from this bootstrap sample.\nStep 4. Repeat Steps 2 and 3 B times to obtain {T \u03b1,b : b = 1, \u22ef, B} and then approximate the test p-value as . Reject H 0 if the p-value is lower than a pre-specified significance level \u03b1, say 0.05.\nThe bootstrap test can be also applied to test the linear hypotheses for \u03b2 as follows: \nwhere C is a given l \u00d7 Jq full rank matrix and c 0 is a given l \u00d7 1 vector. An F-type test can be constructed by (28) where and are the residual sum of squares under H 0 and H 1 , respectively. The asymptotic distribution of (28) can be established by following Fan & Huang (2005) under some mild conditions. We omit it for simplicity.\nBesides hypothesis tests, one may also be interested in computing the confidence intervals of the estimated parameters and the confidence bands of the estimated functions. Although we will prove their theoretical distributions as n \u2192 \u221e, such asymptotic results may not be accurate enough in practice. We suggest the use of bootstrap methods. For instance, we may modify the semiparametric bootstrap procedure described in this section to generate bootstrapped samples and then calculate their corresponding estimates {( , \u011d j (\u00b7) (b) ) : b = 1, \u22ef, B}. Subsequently, we can compute the confidence intervals and bands.\nIn real applications, the specific assignment of variables of interest into model (1) is usually guided by subjective matter experts in order to achieve meaningful practical interpretations. One may objectively allocate variables of interest into X and Z. Specifically, Cheng et al.\n(2009) considered a Bayesian information criterion on structural selection, whereas Zhang (2011) used the cross-validation criterion. Moreover, group penalization for functional estimation can also be used for structural selection (Cheng et al. (2014) ). These ideas may be readily adopted for model (1)."}, {"section_title": "Asymptotic Properties", "text": "We first define some notations. Let u r (K) = \u222bt r K(t)dt and v r (K) = \u222bt r K 2 (t)dt for any integer r. For any smooth functions f(s) and g(s, t), define \u1e1f(s) = df(s)/ds, f(s) = d 2 f(s)/ds 2 , f\u20db (s) = d 3 f(s)/ds 3 , and g (a,b) (s, t) = \u2202 a+b g(s, t)/\u2202 a s\u2202 b t, where a and b are any nonnegative integers. The true values of \u03b1(s), \u03b2, and g j (\u00b7) are denoted by \u03b1 0 (\u00b7), \u03b2 0 and g j0 (\u00b7), respectively. Let \u03bc \u03b2 j0 (z) = E(Z|Z T \u03b2 j0 = z T \u03b2 j0 ), \u03bd \u03b2 j0 (z) = \u03bc \u03b2 j0 (z) \u2212 z, \u03c9 \u03b2 j0 (z) = E(ZZ T |Z T \u03b2 j0 = z T \u03b2 j0 ), \u03c9 1jj\u2032 (z 1 , z 2 ) = \u03bd \u03b2 j0 (z 1 )\u03bd \u03b2 j\u20320 (z 2 ) T , and \u03c9 2j (z) = \u03c9 \u03b2 j0 (z) \u2212 \u03bc \u03b2 j0 (z)\u03bc \u03b2 j0 (z) T for j = 1, \u2026, J. Let A + be the Moore-Penrose inverse of a symmetric matrix A and f j (\u00b7) be the density function of Z T \u03b2 j . It is assumed that the initial value for estimating \u03b2 j is in an neighbor of \u03b2 j0 , denoted as B j,n such that B j,n = {\u03b2 j : |\u03b2 j \u2212 \u03b2 j0 | \u2264 \u03b4 \u03b2 j = C 0 n \u22121/2 } (H\u00e4rdle et al. (1993); Xia (2006) ), where \u03b4 \u03b2 j is the radius of B j,n . We also assume that the initial estimate for \u03b1 j (s) is in an neighbor of \u03b1 j0 (s), which can be achieved by using the estimates in Fan & Huang (2005) and Zhu et al. (2012) .\nWe state the following theorems, whose detailed conditions and proofs can be found in the Appendix. The first theorem establishes the weak convergence of (\u03b2, \u011d j (z T \u03b2\u0135), \u03b1(s)), which characterizes the asymptotic behavior of our estimators and is essential for constructing valid inferences based on these estimators.\nTheorem 1-Suppose that Assumptions (1)-(9) hold. As n \u2192 \u221e, we have the following results:\ni.\n, where and the (j, j\u2032)th block in \u03a8 \u03b2 is in which .\nii. , where .\niii. converges weakly to a mean-zero Gaussian process with covariance matrix , where \u03b1(s)\n, and H 2 = diag(h 12 , \u2026, h J2 ).\nThe key challenge in proving Theorem 1 is to deal with within-subject dependence, the unknown link function, and the index parameter vector. In fact the covariance between \u03b7(s) and \u03b7(s\u2032) in the newly proposed multivariate varying coefficient model does not converge to zero under the within-curve dependence structure and thus requires special care in the derivation of the asymptotic distribution. Also, because of the iterative nature of the proposed estimation procedure, the proof of the single index component requires an explicit induction argument. In practice, the estimation of the unknown moment quantities involved in this theorem is highly non-trivial. We therefore recommend the bootstrap resampling method given in the previous section.\nasymptotic results for the estimation of the single index component in our model are more general since our estimation reduces to MAVE under simpler settings. Similar to Xia (2006) , we can show that \u03a8 \u03b2 achieves the semiparametric information lower bound under the exponential family when there is no within-subject dependence for the data. Although research interest is usually restricted on the coefficients, we also provide the asymptotic result for single index estimate \u011d j (\u00b7) for the sake of completeness. Such a result may be used to quantify the estimation variability from a theoretical point of view. Finally, the estimation for the varying-coefficients functions in model (1) has a similar distributional result to Zhu et al. (2012) for multivariate functional data. We note because of the within-curve dependence the convergence speed for the estimated varying-coefficients is at the order O(n \u22121/2 ) instead of O((nMh j1 ) \u22121/2 ), which is usually the expected rate for cross-sectional data.\nWe next study the asymptotic bias and covariance of \u03b7\u00ee j (s) as follows. We distinguish between two cases. The first case is to condition on the design points in , X, Z and \u03b7. The second case is to condition on the design points in , X and Z. For the first case the conditional events involve the subject specific random effects. The bias and variance of the estimated random effects thus reflect individual heterogeneity. On the other hand, the second case does not condition on the sets of random effects and thus the bias and variance of the estimated random effects are considered for subjects randomly selected from a homogeneous population. The two cases may both be of interest for practitioners. We define K # (t) = \u222b K(u)K(u + t)du. (1) and (3)- (9), the following results hold for all s \u2208 (0, 1)."}, {"section_title": "Theorem 2-Under Assumptions", "text": ""}, {"section_title": "i.", "text": "Conditioning on ( , X, Z, \u03b7), we have"}, {"section_title": "ii.", "text": "The asymptotic bias and covariance of \u03b7\u00ee j (s) conditional on , X and Z are given by"}, {"section_title": "iii.", "text": "The mean integrated squared error (MISE) of \u03b7\u00ee j is given by (29)"}, {"section_title": "iv.", "text": "The optimal bandwidth to minimize MISE (29) is given by (30) v.\nThe first order local polynomial kernel reconstructions \u03b7\u00ee j (s) using in (30) satisfy (31) for i = 1, \u22ef, n.\nThis theorem may be used to study the statistical property of the entity-specific effects in the functional regression analysis. Because the estimation of \u03b7 ij (s) succeeds the estimation of the regression coefficients, the MISE of \u03b7\u00ee j (s) appears to be a function of the three bandwidths h j1 , h j2 and . If the optimal bandwidth in Theorem 2 (iv) is used, the resulting MISE can achieve the order of . Practitioners may interpret the estimated random effects as how differently individual subjects behave from the population average.\nWe then present the asymptotic results for the refined estimates which fully acknowledge the dependence in the error process.\nTheorem 3-Suppose that Assumptions (1)- (10) and (12) hold and and are of the same order as h j1 and h j2 , respectively. As n \u2192 \u221e, we have the following results:\n, where , and the (j, j\u2032)th block in is where j (s, t) and jj\u2032 (s, t) are defined in Assumption (12).\nii. iii.\nconverges weakly to a mean-zero Gaussian process with covariance matrix , where\n, and\nAn important implication of Theorem 3 (i) is the efficiency gain in the use of the refined estimator \u03b2\u0303 compared with \u03b2. That is, the individual elements of \u03b2\u0303 have smaller asymptotic variance than the corresponding elements of \u03b2. See the proof of Lemma 5 in the Appendix.\nThe refined procedure is thus more efficient in practice, especially when the goal of interest is to achieve an accurate estimation of the covariate effect.\nFinally, we have the following asymptotic results for the test statistic T \u03b1 . By Theorem 1 (iii), converges weakly to a Gaussian process. The results in Theorem 4 follow directly from Theorem 1 and Fubini theorem. (1)- (10), we have that T \u03b1 converges weakly to as n \u2192 \u221e, where (s) is an l-dimensional Gaussian process. In particular, under H 0 , (s) is a zero-mean Gaussian process."}, {"section_title": "Theorem 4-Under Assumptions", "text": ""}, {"section_title": "Simulation Study", "text": "In the following simulation study, the data were generated from model (1) , and X i = (1, X i1 , X i2 ) for all i = 1, \u2026, n and m = 1, \u2026, M. Moreover, X i1 and X i2 were independently generated from N(0, 1) and \u03b7 ij (s) = \u03be ij1 \u03c8 j1 (s) + \u03be ij2 \u03c8 j2 (s), where \u03be ijl ~ N(0, \u03bb jl ) for j = 1, 2 and l = 1, 2. We generated Z i from a 4-dimensional standard normal distribution with the index parameter vectors and . Furthermore, s m , X i1 , X i2 , \u03be i11 , \u03be i12 , \u03be i21 , \u03be i22 , \u03b5 i1 (s m ), and \u03b5 i2 (s m ) are independent random variables. We set and the single index functions, eigenfunctions, and varying coefficient functions as follows:\nWe conducted extensive simulation studies under different settings and reported the estimation results for n = 200 and 400 and M = 50 after 500 simulations. We applied the estimation procedure in Section 2 to each simulated data set and calculated all unknown quantities. Table 1 summarizes the numerical performance of our estimators with sample size n = 200, where we report mean absolute error (MAE) and root mean square error (RMSE) for estimated parameters and mean integrated absolute error (MIAE) and mean integrated squared error (MISE) for estimated functions. The results indicate satisfactory performance of our estimators since all MAE, RMSE, MIAE and MISE values are quite small. We notice that the refined estimators achieve smaller estimation error compared with the initial estimators. Typical estimated functions with median performance are displayed in Li et al. Page 15 J Am Stat Assoc. Author manuscript; available in PMC 2017 December 01.\nFigures 1 and 2. The estimated curves (broken lines) closely resemble the corresponding true functions (solid lines) in these figures. We also carried out some additional experiments under different sample sizes and obtained similar findings. Table 2 presents the estimation results for n = 400. As expected, increasing sample size decreases estimation error.\nWe then assess the finite-sample performance of T \u03b1 proposed in Section 3. We are interested in testing whether any varying-coefficient function can be constant. We consider the same data generating mechanism as in the previous simulations except that the varying coefficients are generated as follows: (32) where \u03b1 Finally, we consider the other example on the test of varying-coefficient functions. Specifically, we generated the coefficient functions via the following alternative model (33) where \u03b1 0 (s) is the same as that in (32). Under the null hypothesis, all varying-coefficient functions are equal to zero as k = 0. We examine the test performance over a range of k values and also plot the power curve of the bootstrap test in the right panel of Figure 3 . As expected, the proposed test preserves the specified significance level under the null hypothesis and its power increases with k."}, {"section_title": "Real Data Analysis", "text": "We used model (1) to analyze a real DTI data set with n = 214 subjects collected from NIH Alzheimer's Disease Neuroimaging Initiative (ADNI) study 1 . The NIH ADNI is an ongoing public-private partnership to test whether genetic, structural and functional neuroimaging, and clinical data can be integrated to assess the progression of mild cognitive impairment (MCI) and early Alzheimer's disease (AD). The structural brain MRI data and corresponding clinical and genetic data from baseline and follow-up were downloaded from the ADNI publicly available database (https://ida.loni.usc.edu/). The demographic information about the data set in this paper is presented in Table 3 .\nThe DTI data were processed by two key steps including a weighted least squares estimation method (Basser et al., 1994; Zhu et al., 2007) to construct the diffusion tensors and a FSL TBSS pipeline (Smith et al., 2006) to register DTIs from multiple subjects to create a mean image and a mean skeleton. Specifically, maps of fractional anisotropy (FA) were computed for all subjects from the DTI after eddy current correction and automatic brain extraction using FMRIB software library. FA maps were then fed into the TBSS tool, which is also part of the FSL. In the TBSS analysis, the FA data of all the subjects were aligned into a common space by non-linear registration and the mean FA image were created and thinned to obtain a mean FA skeleton, which represents the centers of all WM tracts common to the group. Subsequently, each subjects aligned FA data were projected onto this skeleton.\nWe focus on the midsagittal corpus callosum skeleton and associated FA curves from all subjects at M = 83 location points as shown in Fig. 4 . The corpus callosum (CC) is the largest fiber tract in the human brain and is a topographically organized structure. It is responsible for much of the communication between the two hemispheres and connects homologous areas in the two cerebral hemispheres. It is important in the transfer of visual, motoric, somatosensory, and auditory information.\nWe are interested in testing the association between FA and diagnostic groups (MCI and AD) using model (1). Specifically, the X i vector includes an intercept term and the gender variable (coded by a dummy variable indicating for male) and the Z i vector includes the age of the subject (years), an indicator for handiness (coded by a dummy variable indicating for left-hand), the education level (years), an indicator for Alzheimer's disease (AD) status (19.6%) and an indicator for mild cognitive impairment (MCI) status (55.1%). We standardized all continuous variables to be mean zero and variance one.\nThe estimated varying-coefficients models are shown in Figure 5 , along with 95% bootstrap confidence bands. The intercept function characterizes the nonlinear trend of FA values. The estimated coefficient function for sex suggests that the difference of FA values between men and women is not a constant. Since the coefficient curve is positive at most of the grid points, it may indicate that men tend to have higher FA values than women. This finding is consistent with the previous analysis with functional varying-coefficient models (Zhu et al. (2012) ). On the other hand, when we include sex in Z for the single index, its estimated coefficient is of size 0.2236 and in the same direction.\n50 sites across the U.S. and Canada. The initial goal of ADNI was to recruit 800 subjects but ADNI has been followed by ADNI-GO and ADNI-2. To date these three protocols have recruited over 1500 adults, ages 55 to 90, to participate in the research, consisting of cognitively normal older individuals, people with early or late MCI, and people with early AD. The follow up duration of each group is specified in the protocols for ADNI-1, ADNI-2 and ADNI-GO. Subjects originally recruited for ADNI-1 and ADNI-GO had the option to be followed in ADNI-2. For up-to-date information, see www.adni-info.org. The estimated single index function is presented in Figure 5 . Six non-zero eigenvalues of the covariance were estimated in a descending order to be 0.2294, 0.0290, 0.0255, 0.0107, 0.0086, and 0.0041, respectively. The first three eigenvalues account for 92.3% of the total variability and the remaining eigenvalues rapidly drop to zero. The estimated eigenfunctions corresponding to these eigen-values are shown in Figure 6 . The first eigen-function, with a dominant eigenvalue accounting for 74.6% of the total variation, is simple in structure and resembles a single cycle of a sine wave. While the remaining eigen-functions are also quite simple and roughly sinusoidal, they contain more and more cycles."}, {"section_title": "Discussion", "text": "We have proposed a novel functional varying-coefficient single index model (FVCSIM) to carry out the regression analysis of functional responses on a set of covariates of interest (e.g., time). The varying-coefficient and single index components in FVCSIM allow us to accommodate the dynamic effect of X and the stationary effect of Z on functional data. We have developed an efficient estimation procedure to iteratively estimate varying coefficient functions, link functions, index parameter vectors, and the covariance function of individual functions. We have systematically examined the asymptotic properties of all estimators in FVCSIM. Through simulation studies and a real data example, we have shown that FVCSIM is a valuable statistical model for quantifying the complex relationships between imaging data and clinical variables of interest.\nSeveral important issues need to be addressed in future research. First, we will extend model (1) to FVCSIM with high-dimensional covariates X and/or Z. Such extension is extremely critical for quantifying the effects of a huge number of genetic markers on imaging phetotype data. Second, we will extend model (1) to functional single index models with varying index parameters \u03b2 j (s). Moreover, we will consider high-dimensional covariate vector Z under such models and include regularization terms to incorporate the spatial smoothness and sparsity. This allows us to achieve dimension reduction and variable selection for complicated functional responses. Third, it is meaningful to extend model (1) from cross-sectional functional data to longitudinal functional data. Many complexities and new statistical tools will definitely emerge from these new developments. Representative FA imaging data for ADNI study: FA template curves measured at 83 grid points along the midsagittal skeleton of the corpus callosum (black) of subjects in 3 groups (from left to right): AD (left), MCI (middle) and NC (right). Table 1 Estimation results for n = 200 obtained from 500 simulations: MAE is mean absolute error; RMSE is root mean square error; MIAE is mean integrated absolute error; and MISE is mean integrated square error. The values in bold-face are obtained from the refined estimates incorporating the estimated covariance. Table 2 Estimation results for n = 400 obtained from 500 simulations. MAE is mean absolute error; RMSE is root mean square error; MIAE is mean integrated absolute error; MISE is mean integrated square error. The values in bold-face are obtained from the refined estimates incorporating the estimated covariance. Table 3 Demographic information about ADNI data, including disease status, range of age (RA), gender, handiness and Range of education level (REL).\nDisease status Num."}, {"section_title": "RA(mean)", "text": "Gender ( Table 4 Estimation results of regression coefficients for ADNI data. 95% confidence intervals are based on 500 bootstrap resamples. The values in bold-face are obtained from the refined estimation incorporating the estimated covariance. "}]