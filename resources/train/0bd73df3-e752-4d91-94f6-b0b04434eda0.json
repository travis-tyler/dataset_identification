[{"section_title": "Foreword", "text": "This report describes and evaluates the methods and procedures used in the 2004/06 Beginning Postsecondary Students Longitudinal Study (BPS:04/06), the first follow-up of the cohort of first-time beginning students who were identified as part of the 2003-04 National Postsecondary Student Aid Study (NPSAS:04). This cohort was first interviewed in 2004 and identified as first-time beginners (FTBs). An FTB was defined as an individual who began his or her postsecondary education during the 2003-04 academic year. BPS:04/06 is the first of two scheduled follow-up studies that will follow these students through college and into the workforce. The second, and final, follow-up is scheduled to take place in 2009. The BPS study is unique in that it includes both traditional and nontraditional students, follows their path through postsecondary education over the course of 6 years, and is not limited to enrollment at a single institution. One important change new to the BPS project this cycle was the use of a single webbased interview for all administration modes. Students were given the opportunity to complete a self-administered interview online. Computer-assisted telephone interviewing (CATI) and computer-assisted personal interviewing (CAPI) were used to follow up with sample members who did not complete the self-administered interviews. We hope that the information provided here will be useful to a wide range of interested readers and will encourage others to use the 2004/06 Beginning Postsecondary Students Longitudinal Study data. Additional information can be found at http://nces.ed.gov/surveys/bps/."}, {"section_title": "C. Dennis Carroll Associate Commissioner Postsecondary Studies Division", "text": "This introductory chapter describes the background, purpose, schedule, and products of BPS:04/06. Chapter 2 describes study design and procedures. Chapter 3 presents data collection outcomes, while chapter 4 presents evaluations of the quality of the data collected. Chapter 5 reviews the data file development process. Finally, chapter 6 describes the weighting and variance estimation procedures and reports on the quality of the estimates. Further information about the study, including members of the Technical Review Panel, data elements, instrument facsimile, materials used during interviewer training and data collection, and additional technical details about the data, are provided as appendixes to the report and cited in the text where appropriate. Analyses conducted to evaluate the effectiveness of the BPS:04/06 procedures are presented here. Unless otherwise indicated, a criterion probability level of .05 was used for all tests of significance. Throughout this document, reported numbers of sample institutions and students have been rounded to further ensure the confidentiality of individual student data. As a result, row and column entries in tables may not sum to their respective totals, and reported percentages (based on unrounded numbers) may differ somewhat from those that would result from these rounded numbers."}, {"section_title": "Background and Objectives of BPS Series", "text": "Each academic year, several million students begin postsecondary education for the first time. The BPS series provides a unique opportunity to learn about the experiences of these students during their first year, and at two additional time points after their first year. As one of several studies sponsored by NCES to respond to the need for a national, comprehensive database on postsecondary education, the BPS series addresses issues related to enrollment, persistence, progress, attainment, continuation into graduate/professional school and employment, and to early rates of return to the individual and society. The BPS series of studies is uniquely able to identify students as first-time beginners (FTBs) through its base study-the National Postsecondary Student Aid Study (NPSAS), a recurring survey of nationally representative, cross-sectional samples of postsecondary students designed to determine how students and their families pay for postsecondary education. The BPS series follows FTBs over a period of 6 years to monitor their progress in the issues of postsecondary education described above. Figure 1 presents the timelines for the base-year and subsequent follow-up studies for each BPS in the series. BPS:04/06 consists of individuals who first began postsecondary education in the 2003-04 academic year, regardless of when they completed high school. BPS is unlike previous longitudinal studies of high school age cohorts in that its student sample includes nontraditional postsecondary students who delay continuing their education after high school for a variety of reasons. Through its unique design, the BPS series makes it possible to trace the paths of FTBs throughout the entire system of postsecondary education over a number of years. Consequently, whereas typical retention and attainment studies of entering freshmen provide data at a single institution, BPS allows for the study of student persistence and attainment at any U.S. institution in the United States and Puerto Rico. The purpose of the BPS:04/06 follow-up is to monitor the academic progress and persistence in postsecondary education of 2003-04 FTB students during the 3 years following their initial entry into a postsecondary institution. The data collection focused on degree completion (less than 4-year) and continued education and experience, education financing, entry into the workforce, the relationship between experiences during postsecondary education and various societal and personal outcomes, and returns to the individual and to society on the investment in postsecondary education. The second follow-up of the BPS:04 cohort, scheduled for 2009, will monitor students' academic progress in the 6 years following their first entry into postsecondary education and will be able to assess completion rates in 4-year programs. Data collected will continue to focus on education and employment, and the survey will include many of the questions used in the first follow-up. The second follow-up will also be enhanced to focus on graduate and professional school access issues, and to further explore rate of return issues for those who will have completed their education. By following a cohort of new entrants into postsecondary education, the BPS series of studies provides a unique perspective on what happens to persons as they enter and pursue education beyond high school. Because it includes both traditional and nontraditional students, BPS permits study of educational aspirations, progress, persistence, and attainment for both groups of students. By providing longitudinal data for a single cohort and trend data across cohorts, the BPS series contributes a comprehensive national database addressing policy issues at the postsecondary level."}, {"section_title": "Schedule and Products of BPS:04/06", "text": "Table 1 summarizes the schedule for the full-scale study in 2006. Electronically documented, restricted-access research files (with associated electronic codebooks) as well as NCES Data Analysis Systems (DASs) for public release have been constructed from the fullscale data collection and made available to a variety of organizations and researchers. In addition to this full-scale methodology report, BPS:04/06 has produced \u2022 special tabulations on issues of interest to the higher education community, as determined by NCES; and \u2022 a descriptive summary of significant findings for dissemination to a broad audience. Institution Universe for NPSAS:04. The institutions eligible for NPSAS:04 were required during the 2003-04 academic year to meet all the requirements for distributing federal Title IV aid, including: \u2022 offering an educational program designed for persons who have completed a high school education; \u2022 offering at least one academic, occupational, or vocational program of study lasting at least 3 months or 300 clock hours; \u2022 offering courses that are open to more than the employees or members of the company or group (e.g., union) that administers the institution; and \u2022 being located in the 50 states, the District of Columbia, or Puerto Rico. Institutions providing only vocational, recreational, or remedial courses or only in-house courses for their own employees were excluded. U.S. service academies were excluded because of their unique funding/tuition base. The above institutional eligibility conditions are consistent with previous NPSAS studies with two exceptions. First, the requirement of being eligible to distribute Title IV aid was implemented beginning with NPSAS:2000. 1 Second, the previous NPSAS studies excluded institutions that offered only correspondence courses. NPSAS:04 included such institutions if they were eligible to distribute Title IV student aid. Student Universe for NPSAS:04 and BPS:04/06. Students eligible for the BPS:04/06 full-scale study were those both eligible to participate in NPSAS:04 and identified as FTB students at NPSAS sample institutions in the 2003-04 academic year. Consistent with previous NPSAS studies, the students eligible for the NPSAS:04 full-scale study were those enrolled in eligible institutions who satisfied all the following eligibility requirements: \u2022 were enrolled in either (1) an academic program; (2) at least one course for credit that could be applied toward fulfilling the requirements for an academic degree; or (3) an occupational or vocational program that required at least 3 months or 300 clock hours of instruction to receive a degree, certificate, or other formal award; and \u2022 were not concurrently or solely enrolled in high school, or in a General Educational Development (GED) or other high school completion program. NPSAS-eligible students who enrolled in a postsecondary institution during the \"NPSAS year\" (July 1, 2003-June 30, 2004) for the first time after completing high school requirements were considered pure FTBs and were eligible for BPS:04/06. Those NPSAS-eligible students who had enrolled for at least one course after completing high school but had never completed a postsecondary course before the 2003-04 academic year were considered effective FTBs and were also eligible for the BPS:04/06 full-scale study. In the full-scale BPS data collection, students were sampled from both (1) NPSAS:04 respondents who were identified as (pure or effective) FTBs and (2) NPSAS:04 nonrespondents who were potential (pure or effective) FTBs."}, {"section_title": "Statistical Methodology", "text": "The NPSAS:04 sampling design was a two-stage design in which eligible institutions were selected at the first stage and eligible students were selected at the second stage within eligible, responding sample institutions (see appendix A for more information on the NPSAS:04 sampling details). The NPSAS:04 sample, the process of identifying and selecting FTBs for the BPS follow-up studies, and the BPS:04/06 subsampling procedures are described below. Institution Sample for NPSAS:04. The institutional sampling frame for NPSAS:04 was constructed from the 2000-01 and 2001-02 Integrated Postsecondary Education Data System (IPEDS) Institutional Characteristics (IC) file and header files, and the 2000 and 2001 Fall Enrollment files. The sample of institutions was freshened using the 2002-03 IPEDS, to include a sample of newly formed institutions. Records on the IPEDS files for NPSAS-ineligible institutions were deleted. NPSAS-ineligible institutions included U.S. service academies, institutions located outside the U.S. and Puerto Rico, and institutions offering no programs of study lasting at least 3 months or 300 clock hours. The IPEDS files were then cleaned to resolve the following types of problems: \u2022 missing enrollment data, 2 because these data are needed to compute measures of size for sample selection; and \u2022 unusually large or small enrollment, especially if imputed because, if incorrect, these data would result in inappropriate probabilities of selection and sample allocation. Table 2 presents the allocation of the NPSAS:04 institutional sample to the nine types of institutions. The number of sample institutions was 1,670, of which 1,630 were eligible. Table 2 also indicates that 1,360 institutions provided student enrollment lists. NOTE: Detail may not sum to totals because of rounding. Among the 30 ineligible institutions, 10 closed after the sampling frame was defined, and 10 failed to meet one or more of the criteria for institutional NPSAS eligibility. The remainder were treated as merged institutions because two or more campuses were included on one combined student list. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2003-04 National Postsecondary Student Aid Study (NPSAS:04). A direct, unclustered sample of institutions was selected, like the sample selected for NPSAS:2000 and NPSAS:96, rather than the clustered sample used for earlier NPSAS studies. In addition, to allow analysis of the effects of state tuition and student aid policies in individual states, representative samples were selected from three institution types-public 2-year institutions; public 4-year institutions; and private not-for-profit 4-year institutions-in each of the following 12 states: California, Connecticut, Delaware, Georgia, Illinois, Indiana, Minnesota, Nebraska, New York, Oregon, Tennessee, and Texas. Student Sample for NPSAS:04. The NPSAS:04 student sampling design was based on fixed type sampling rates, not fixed type sample sizes. The design used two student sampling types for undergraduates (FTB and other undergraduates), three student sampling types for graduate students (master's, doctoral, and other graduate students), and one type for firstprofessional students. Differential sampling rates were used for the three types of graduate students to get adequate representation of students pursuing doctoral degrees and to limit the sample size for \"other\" graduate students, who are of limited inferential interest. The NPSAS:04 student interview data collection procedures were expected to produce about a 70 percent student response rate based on historical experience. The sample sizes were determined using prior NPSAS experience regarding institutional CADE response rates and sample student eligibility rates. A total of 109,210 sample students were selected for NPSAS:04, including 49,410 potential FTBs; 47,680 other undergraduate students; and 12,120 graduate and first-professional students (see table 3). Table 3. Numbers of NPSAS:04 sampled and eligible students and response rates, by institutional characteristics and student type: 2004 1 A responding student is defined as any eligible student for whom sufficient data were obtained from one or more sources, including student interview, institutional records, and the U.S. Department of Education's Central Processing System (CPS). 2 Percentages are based on the eligible students within the row under consideration. 3 Institutional characteristics are based on data from the sampling frame that was formed from the 2000-01 and 2001-02 Integrated Postsecondary Education Data System (IPEDS). Student type is based on data from the sampling frames that were the enrollment lists received from participating institutions. 4 Ineligible students were identified during the student interview or from institutional records if student eligibility was not determined from a student interview. NOTE: Detail may not sum to totals because of rounding. FTB = first-time beginner. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2003-04 National Postsecondary Student Aid Study (NPSAS:04). Postsecondary institutions are sometimes unable to accurately identify their FTB students. Therefore, students classified as potential FTBs for sampling for NPSAS:04 included both true FTBs who began their postsecondary education for the first time during the NPSAS year (potential false positives) and effective FTBs who had not completed a postsecondary class prior to the NPSAS year (potential false negatives). The NPSAS sampling rates for students identified as FTBs and other undergraduate students by the sample institutions were adjusted to yield the desired sample sizes after accounting for expected false positive and false negative rates. The false positive and false negative FTB rates experienced in NPSAS:96 were used to set appropriate sampling rates for NPSAS:04. 3 A discussion of the rates is presented in chapter 4. NPSAS:04 data collection included an institution record abstraction (CADE), a webbased student interview, and record matching against several extant databases (e.g., the U.S. Department of Education's Central Processing System [CPS]). NPSAS:04 study respondents were those sample members for whom key pieces of data were obtained from one or more of these sources. Students could be NPSAS:04 study respondents without completing the student instrument. BPS:04/06 Full-Scale Sample. The BPS:04/06 student sample consisted of four groups according to their base-year response status: 1. NPSAS:04 study respondents who completed the student interview and were determined to be FTBs; 2. NPSAS:04 study respondents who completed the student interview but were initially determined to be non-FTB other undergraduates, and who were potential FTBs based on data from other sources; 3. a subsample of potential FTBs 4 who were NPSAS:04 study respondents but student interview nonrespondents; and 4. a subsample of potential FTBs who were NPSAS:04 study nonrespondents. Multiple data sources were used to provide information regarding a student's FTB status during the NPSAS year, including the NPSAS:04 student interview, records from the student's base-year institution via CADE, and federal financial aid sources. The data elements that were examined to estimate a student's likelihood of being an FTB and to construct the frame for the BPS:04/06 sample included the following: \u2022 indicator of FTB status from the institution enrollment lists used for NPSAS:04 student sampling; \u2022 indicator of FTB status from the Central Processing System (CPS); \u2022 indicator of FTB status from student-level data obtained from institutional records via CADE; \u2022 student reports (obtained during the NPSAS:04 interview) indicating that they were FTBs during the 2003-04 academic year; \u2022 year of high school graduation; \u2022 receipt of Stafford loan (date loan was first received and number of years loan was received); \u2022 receipt of Pell grant (date grant was first received and number of years grant was received); and \u2022 undergraduate class level. Using the above indicators, a set of decision rules was developed to identify which cases would be included or excluded from the follow-up sample, and which among those included would require additional eligibility screening. The NPSAS:04 sample yielded the numbers of students below who either indicated that they were FTBs during the interview and had other institutional records or federal financial aid sources that supported this, or were identified as potential FTBs based on institutional records or federal financial aid sources: 1. Approximately 24,990 students responding to the student interview indicated that they were FTBs during the 2003-04 academic year. Based on a review of the FTB status indicators above, approximately 21,170 of these were identified for inclusion in the follow-up sample. Of the approximately 21,170 included in the follow-up sample, approximately 19,800 had other data that strongly supported their FTB status, and approximately 1,370 of these students had some indications that they were not FTBs; these potential \"false positives\" were rescreened during the BPS:04/06 interview to confirm their status. The remaining approximately 3,820 of the original 24,990 were identified for exclusion from the follow-up when multiple data sources confirmed that they could not have been FTBs during the NPSAS year. 2. Approximately 1,420 students were not originally classified as FTBs, but were potential FTBs based on either CPS data or because they had a high school graduation date in 2003 or 2004; these potential \"false negatives\" were also screened during the BPS:04/06 interview to verify their status. 3. Approximately 8,860 students did not respond to the student interview but were classified as NPSAS:04 study respondents and were potential FTBs based on CADE or CPS data, more positive than negative indicators among the other variables, and any Stafford loans or Pell grants that began after 2003. 4. Approximately 720 NPSAS:04 sample members were potential FTBs based on information from CADE or CPS, but did not respond to the student interview and did not have sufficient data to be classified as study respondents. Table 4 summarizes the distribution of the sample. As noted above, approximately 9,580 student interview nonrespondents were classified as potential FTBs. Of these, approximately 8,860 were NPSAS:04 study respondents who did not respond to the student interview, and approximately 720 were NPSAS:04 study nonrespondents. NPSAS:04 student interview nonrespondents who were potential FTBs were subsampled for follow-up to improve the nonresponse bias reduction achieved through the nonresponse adjustments incorporated into the NPSAS:04 statistical analysis weights. For these students, sampling types were developed from the following characteristics: \u2022 likelihood of being an FTB (medium, high); and \u2022 tracing outcome (located, not located). 5 Two factors, stratification by tracing outcome and the likelihood of being an FTB, were used to oversample the students most likely to be located and eligible for the study. The frame was also sorted by institutional sector to ensure representativeness of the sample. A stratified sample of 500 was selected with probabilities proportional to their NPSAS:04 sampling weights. Table 5 summarizes the BPS:04/06 counts of students eligible for the sample and the sample sizes, including the allocation of the subsample of 500 cases to the two groups of 5 The results from the advance tracing for BPS:04/06 were used to determine whether a student had been located. The National Change of Address file (NCOA) was used to obtain updated addresses for the student, then Telematch was used to obtain an updated telephone number. The student was classified as \"located\" if Telematch either returned a new telephone number or confirmed the current telephone number. NPSAS:04 student interview nonrespondents. Given that the NPSAS:04 sampling weights were available for all student interview nonrespondents, they served as the basis for computing the BPS:04/06 analysis weights. Therefore, selection of the NPSAS:04 student interview nonrespondents with probabilities proportional to these weights was used to reduce the overall unequal weighting effects for the sample. "}, {"section_title": "Data Collection Design", "text": "The sections that follow provide an overview of the procedures implemented for the BPS:04/06 full-scale data collection, in particular, the development of the instrument and the procedures used to locate sample members."}, {"section_title": "Instrument Development", "text": "Beginning with the NPSAS:04 base-year interview, BPS:04/06 was the first in the BPS series to give respondents the option of completing a self-administered web interview. A single, web-based instrument was developed to use in three modes: self-administered interview, computer-assisted telephone interview (CATI), and computer-assisted personal interview (CAPI). Sample members could access the interview directly from the study website by entering a Study ID and password provided to them in a mailing. Telephone interviewers could access the interview via RTI's integrated case management system, while field interviewers accessed the interview through an independent case management system installed on each field laptop. The content of the first follow-up interview remained primarily the same as that used in prior BPS first follow-up interviews (BPS:90/92 and BPS:96/98), building upon data elements developed with input from the study's Technical Review Panel (TRP) as well as from the National Center for Education Statistics (NCES). (See appendix B for a list of TRP members and appendix C for a list of the final set of data elements.) The interview consisted of six sections, grouped by topic (see figure 2). Only base-year nonrespondents and base-year respondents with questionable eligibility were asked questions in the first section, which determined eligibility both for NPSAS and for BPS. This section gathered a subset of information already collected in the base-year interview-specifically, postsecondary enrollment prior to and during the NPSAS year (July 1, 2003-June 30, 2004), type of program, reasons for attending the sample institution, information on high school completion, and date of birth. All respondents were asked questions in the next section-education history-that focused on their enrollment after the first year, that is, between July 1, 2004, and June 30, 2006. Data were collected about all institutions attended, any degrees or certificates earned, and dates of enrollment. A Spanish translation, covering just these first two sections (along with the locating section described below), was developed for Spanish-speaking respondents. The third section, education characteristics, focused on the respondent's experiences while enrolled. Questions pertained to the most recent degree sought, major or field of study if declared, grade point average, education expenses, work while enrolled, student loan debt, and loan repayment, if applicable. The fourth section of the interview, on post-enrollment employment, was asked only of respondents who were no longer enrolled in postsecondary education, regardless of whether they had completed a degree/certificate. The fifth section collected and updated as needed student demographic characteristics including race/ethnicity, citizenship, voting behavior, marital status and family composition, volunteerism, disability status, goals, and effects of the 2005 hurricanes on enrollment (if applicable). The final section collected contact information that will be used in locating sample members for the final followup data collection in 2009 (as part of BPS:04/09). Respondents were guided through each section of the interview according to skip logic that took into account both their current interview answers and any preloaded data available from the base year. Respondents could access help text by clicking on the help text link on each interview page. Pop-up messages were used to clarify inconsistent and out-of-range values and to convert item nonresponse. Like past BPS interviews, coding systems for standardizing the collection of data on schools attended, major or field of study, occupation, industry, and licenses/certificates were included in the BPS:04/06 full-scale interview. Text strings were first collected for schools attended, major field of study, occupation, and industry before the strings were coded. Coding of schools, majors, and occupations was performed using an assisted coding system that returned a list of options based on the original text strings provided by the respondent. The correct choice could be selected from among the set of returned choices. Industry coding was a manual process in which respondents selected the best categorical description from among a set of 24 options. Licenses/certifications were coded simply by requiring respondents to select from among two drop-down lists of general and specific categories. The assisted coder for institutions was developed using the set of institutions contained in the IPEDS developed by NCES. Similarly, the assisted coder used for majors was constructed to parallel the Classification of Instructional Programs taxonomy also developed by NCES. Like the school coder, the field of study coder derived a list of possible matches in accordance with the text string provided. If no areas matched, respondents were offered dual drop-down boxes to manually code the general and specific areas corresponding to their major. The assisted coding system for occupation, built from the Occupational Information Network Online (O*NET) database (for more information on O*NET, see http://online.onetcenter.org), involved entering a job title and job activities. The assisted coder then returned a set of possible categories based on both the job title and activities provided. If none of the options based on the database search was an appropriate match, respondents were directed to a series of drop-down menus from which they could select a general category, a specific category, and finally a detailed category. For more information, please visit http://nces.ed.gov/surveys/bps/. Prior to the start of data collection, a study website was designed for use by BPS:04/06 field test and full-scale sample members for updating address information and accessing the selfadministered web interview. The website also provided general information about the BPS set of studies, previous findings, contact information for the study Help Desk and project staff at RTI, and links to the NCES and RTI websites. The website was made available to sample members at the time of the first mailing to them, prior to data collection. Figure 3 shows the home page for the BPS:04/06 website. Designed according to NCES web policies, it used a two-tier approach to security to protect any data collected. At the first tier, sample members could log onto the secure areas of the website using a unique Study ID and password that they were provided prior to the start of data collection. At the second tier, data entered on the website were protected with Secure Sockets Layer (SSL) technology, which allowed only encrypted data to be transmitted over the Internet. "}, {"section_title": "Locating", "text": "Advance Locating and Contacting. Tracing activities for all students selected for the BPS:04/06 full-scale study were conducted prior to the start of data collection and before any mailouts to students and their families occurred. Batch searches using the U.S. Department of Education's CPS and the U.S. Postal Service's National Change of Address (NCOA) database were conducted using contact information available for each sample member and his or her parents. In December 2005, an initial mailing was sent to the parents of dependent sample members. The mailing included a study leaflet (see appendix D), an address update sheet, and a business reply envelope, together with a letter introducing the BPS:04/06 study and requesting parents' cooperation and assistance in locating the sample member. All updated addresses produced by the parent mailing were noted in the receipt control system. In January 2006, a mailing to students was sent to the best known address. The accompanying letter announced the upcoming data collection and asked sample members to update their address information. The mailing included a study leaflet, address update sheet, and a business reply envelope. A link to the study website was provided so that sample members could update their address directly. Closer to the start of self-administered web interviewing, all address information for sample members was sent to Gannett Co., Inc.'s Telematch service to obtain new telephone numbers and/or update existing numbers. Immediately prior to the start of data collection on March 20, 2006, a postcard announcing the availability of the self-administered web interview was sent to each sample member's current address. The mailing provided a unique Study ID and password and informed sample members that they would receive $30 if they completed the interview by April 18, 2006. At the same time as this mailing, a comparable mailing was sent via electronic mail (e-mail) to those sample members for whom a working e-mail address was available (provided during the base-year interview by the student or the institution or in response to the student notification mailing via the address update sheet or the student website). Additional e-mail prompts were sent to nonrespondents throughout the course of data collection to encourage their participation. CATI Locating. Once telephone interviewing began, telephone interviewers conducted limited tracing and locating activities as needed. These included calling all telephone numbers and contacts for a sample member or talking to persons answering the telephone to determine how to contact the sample member. When a sample member could not be located at a known address during CATI, the case was batched and sent to Accurint for directory assistance services. Cases that could not be located using any of the existing address information were identified for intensive tracing in RTI's Call Center Services (CCS). Cases that failed to be located a second time were either sent to the field for locating and interviewing, or returned to CCS for additional intensive tracing. Intensive Tracing. The most difficult locating cases were sent to CCS for intensive tracing using a number of online tracing sources, beginning with the credit bureau services (Experian, TransUnion, and Equifax) for those cases with a Social Security number (SSN). Any new information obtained was processed immediately and the case returned to production interviewing. Remaining cases underwent a more intensive level of tracing, which included calls to directory assistance, alumni offices, contacts with neighbors and/or landlords, and other locating strategies. Each case was handled individually based on the amount of information already available, the age of the locating data, and the presence of an SSN."}, {"section_title": "Field Cluster Selection and Locating.", "text": "A subset of the unlocatable cases was sent to field interviewers for tracing and interviewing. Using the best available address for the nonresponding sample members, the cases were plotted on a map and, using a 50-mile radius, the top 75 high-density areas were selected as geographic clusters for possible field interviewing. Field interviewers were hired in clusters with the highest numbers of sample members (e.g., major metropolitan areas). A total of 48 field interviewers were hired to conduct field interviews. For each case assigned to them, field interviewers received all available address information, locating information obtained from any tracing activities conducted to date, and information provided by telephone interviewers who had attempted to reach the sample member. Field interviewers used any and all tracing resources available to them, including many local resources not otherwise known or available outside the geographic area, contacts with the U.S. Postal Service, and searches of public records."}, {"section_title": "Interviewing", "text": ""}, {"section_title": "Early Response Phase: Self-Administered Web Interview", "text": "The BPS:04/06 full-scale data collection began with an early response period of about 4 weeks (March 20-April 18, 2006), during which sample members could complete a selfadministered interview via the Internet. Given the effectiveness of prompting calls for base-year nonrespondents utilized in the field test, prompting calls to these sample members (n = 500) were placed about halfway through the early response period to encourage participation in the study. Additionally, sample members were offered a $30 incentive to participate in the first 4 weeks. A toll-free hotline to the study Help Desk was provided to assist those who had problems accessing the website or questions about the survey. If technical difficulties prevented a sample member from completing the interview, a Help Desk staff member encouraged him or her to complete a telephone interview rather than attempt the web interview. An application designed for the Help Desk documented all calls from sample members and provided \u2022 information needed to verify a sample member's identity; \u2022 login information allowing a sample member to access the web interview; \u2022 systematic documentation of each call; and \u2022 a method for tracking calls that could not be immediately resolved. Reports on the types and frequency of problems experienced by sample members as well as a way to monitor the resolution status of all Help Desk inquiries were available to project staff. Examples of Help Desk staff training materials are available in appendix E."}, {"section_title": "Production Phase: Computer-Assisted Telephone Interviewing (CATI)", "text": "At the end of the early response period, the production interviewing phase of data collection (outbound CATI) began on April 19, 2006. Interviewers received intensive training and were required to complete a certification process to ensure satisfactory interview performance (see field interviewer training materials in appendix E). Interviewers placed outgoing calls to sample members to complete a telephone interview. Sample members were initially offered $20 for their participation. After 20 call attempts, the incentive amount was raised to $30 to encourage participation among nonrespondents. The interviewer-administered interview was identical to the self-administered web interview, except that instructions to interviewers on how to administer each question were embedded at the top of each CATI screen. An automated call-scheduler assigned cases to interviewers and allowed calls to be scheduled by case priority and time of day. If a self-administered web interview was in progress or had recently been completed, the scheduler prevented a CATI call to that case. If a sample member told an interviewer that he or she preferred to complete the self-administered web interview, interviewers would set a call-back appointment for 2 weeks from the date of the original contact for follow-up in the event that a self-administered web interview had not yet been completed."}, {"section_title": "Nonresponse Phase: Computer-Assisted Personal Interviewing (CAPI)", "text": "CAPI, or field interviewing, began June 26, 2006, with sample members who had not yet completed either a self-administered or a CATI interview. Interviewers were trained and certified prior to their entry into the field (see training materials in appendix E). Field interviews were conducted either in person or by telephone by local field interviewers assigned to one of 75 geographic clusters in 29 states based on the last known address for the sample member. Cases assigned to the field could not be accessed by CATI interviewers but could still be completed as a self-administered web interview. Like the CATI interview, the CAPI interview presented interviewer instructions at the top of each screen. Sample members completing interviews in the field were offered a $30 incentive for their participation."}, {"section_title": "Data Collection Systems 2.4.1 Instrument Development and Documentation System (IDADS)", "text": "The Instrument Development and Documentation System (IDADS) was a combination web and Visual Basic (VB) environment in which project staff developed, reviewed, modified, and communicated changes to specifications, code, and documentation for the BPS:04/06 fullscale instrument. All information relating to the instrument was stored in a Structured Query Language (SQL) Server database and was made accessible through web browser and Windows VB interfaces. The IDADS contained three modules: specification, programming, and documentation. Specification Module. The IDADS specification module provided tools and graphical user interfaces for creating, searching, reviewing, commenting on, updating, importing, and exporting information associated with instrument development. A web interface provided access to the instrument specifications for project staff at MPR Associates, Inc. (MPR), and NCES. Programming Module. Once specifications were finalized, the programming module within IDADS produced hypertext transfer markup language (HTML), Active Server Pages (ASP), and JavaScript template program code for each screen based on the contents of the SQL Server database. This output included question wording, response options, and code to write the responses to a database, as well as code to automatically handle such web instrument functions as backing up and moving forward, recording timer data, and linking to context-specific help text. Programming staff edited the automatically generated code to customize screen appearance and to program response-based routing. Documentation Module. The documentation module contained the finalized version of all instrument items, their screen wording, and variable and value labels. Also included were the more technical descriptions of items such as variable types (alpha or numeric), to whom the item was administered, and frequency distributions for response categories. The documentation module was used to generate the instrument facsimiles and the deliverable electronic codebook (ECB) input files."}, {"section_title": "Integrated Management System (IMS)", "text": "All aspects of the study were controlled using an Integrated Management System (IMS). The IMS was a comprehensive set of desktop tools designed to give project staff and NCES access to a centralized, easily accessible repository for project data and documents. The BPS:04/06 IMS consisted of several components: the management module, the Receipt Control System (RCS) module, and the instrumentation module. Management Module. The IMS management module included tools and strategies to assist project staff and the NCES project officer in managing the full-scale data collection. All management information pertinent to the study was located in the management module, accessible via the Web, and protected by SSL encryption and a password-protected login. The IMS contained the current project schedule, monthly progress reports, daily data collection reports and status reports (generated by the RCS described below), project plans and specifications, project deliverables, instrument specifications, staff contacts, the project bibliography, and a document archive. The IMS also had a download area from which staff at MPR and NCES could retrieve files as necessary."}, {"section_title": "Receipt Control System (RCS).", "text": "The RCS was an integrated set of systems used to monitor all activities related to data collection, including tracing and locating. Through the RCS, project staff were able to perform stage-specific activities, track case statuses, identify problems early, and implement solutions effectively. The RCS's locator data were used for a number of daily tasks related to sample maintenance. Specifically, the mailout system produced mailings to sample members, the query system enabled administrators to review the locator information and status for a particular case, and the mail return system enabled project staff to update the locator database as mailings or address update sheets were returned or forwarding information was received. The RCS also interacted with the Tracing Services case management database within RTI's Call Center Services (CCS), sending locator data between the two systems as necessary. A subcomponent of the RCS, the Field Case Management System (FCMS), controlled field interviewing activities. The FCMS allowed field staff to conduct tracing and CAPI, communicate with RTI staff via e-mail, transmit completed cases, and receive new cases. Instrumentation Module. The instrumentation module managed development of the multimode web data collection instrument within IDADS. Developing the instrument with IDADS ensured that all variables were linked to their item/screen wording and thoroughly documented."}, {"section_title": "The Variable Tracking System (VTS)", "text": "The central mechanism for constructing input files for the NCES ECB was a software application called the Variable Tracking System (VTS). The VTS tracked and stored documentation for both interview and derived variables required for the ECB and NCES' Data Analysis System (DAS). This included weighted and unweighted variable distributions, variable labels and codes, value labels, and a text field describing the development of each variable and the programming code used to construct it. Input files for the ECB and DAS systems were automatically produced by the VTS according to NCES specifications."}, {"section_title": "Chapter Data Collection Outcomes", "text": "The data collection efforts for 2004/06 Beginning Postsecondary Students Longitudinal Study (BPS:04/06) involved several steps, including attempting to locate sample members, initiating intensive locating efforts for hard-to-locate sample members, evaluating the utility of incentives paid throughout the course of data collection, and completing a self-administered, computer-assisted telephone interview (CATI), or computer-assisted personal interview (CAPI). This chapter reports the data collection outcomes of the full-scale study. The response rates are reported first, including an overall summary of results, followed by a discussion of interviewing outcomes by prior response status, institution type, administration mode, and tracing status. This section also discusses procedures that were used to encourage response. The second section discusses the interview burden on respondents, including times to complete various sections and transmit data. Results are presented for the entire interview, overall, by section, and by mode of administration. The average number of calls made overall and by current and prior response status, mode of administration, incentive period, and institution type are also covered in this section."}, {"section_title": "Response Rates", "text": ""}, {"section_title": "Overall Summary of Interview Results", "text": "Locating, eligibility, and participation results of the BPS:04/06 data collection are presented in figure 4. The BPS:04/06 sample consisted of participants and a small number of nonrespondents from the base-year study, the 2003-04 National Postsecondary Student Aid Study (NPSAS:04). Of the total 23,090 sample members, 20,580 (89 percent) were located for the study. 6 Of the number located, 16,580 (81 percent) were considered eligible and 3,130 cases were ineligible, which was determined via multiple data sources. A total of 870 located cases were excluded from the sample (because they were unavailable for the duration of the study, out of the country, incapable, institutionalized, hearing impaired, or deceased). Of the entire sample, 18,640 (81 percent) made up the total number of eligible cases, which consisted of 16,580 located and 2,060 not located cases. The ability to successfully locate sample members largely affects participant response rates. Just over 2,500 cases were not located for BPS:04/06. Among eligible nonrespondents, 55 percent were not located and 28 percent were refusals. Time ran out for 16 percent (e.g., data collection ended before these cases could be fully worked), and cases with a language barrier constituted less than 1 percent. Considering all eligible sample members (located and not 6 Students were classified as located if case management system status codes at the end of data collection indicated that good contact information for the respondent had been obtained, and/or the respondent had actually been contacted. Actual contacts considered located included calls, e-mailings, or paper mailings resulting in partial or complete interviews, appointments for callbacks, refusals, exclusions, or determination of ineligibility. A noncontact status was classified as located when unsuccessful attempts at contact did not cast doubt on the accuracy of the address or phone used for the attempted contact. located), the overall unweighted response rate was 80 percent. In comparison, the response rate was 90 percent for eligible cases that were successfully located. Almost 90 percent of all BPS:04/06 respondents completed the full BPS:04/06 interview, with another 9 percent completing an abbreviated interview. As shown in table 6, this included 1,130 English abbreviated interviews and 240 Spanish abbreviated interviews. 7 Response rates for interview type by base-year response status are also provided in table 6. The full interview was completed by 90 percent of NPSAS:04 respondents and 83 percent of NPSAS:04 nonrespondents. Overall, the English abbreviated interview accounted for 8 percent of all interviews. Only 130 interviews were partially completed. All of the partials were completed by base-year respondents. "}, {"section_title": "Interview Outcomes by Prior Response Status and Institution Type", "text": "Interview outcomes by prior response status and institution type are presented in tables 7 and 8. As previously mentioned, the results of sample member locating have an impact on response rates. For each table, response rates are provided among the eligible sample and the located eligible sample. As discussed in section 3.1.1 and shown in table 7, the overall locate rate for eligible sample members was 89 percent. Of those located, 90 percent completed the BPS:04/06 interview. Across all eligible sample members (located and not located), the response rate was 80 percent. Eligible base-year respondents had a locate rate of 89 percent, in which 90 percent of those located were interview respondents. The locate rate for eligible base-year nonrespondents was 75 percent and, of those located, 52 percent completed the interview. Regardless of locate status, 81 percent of eligible base-year respondents and 39 percent of eligible base-year nonrespondents completed the interview.  Table 8 shows locate and response rates by institution type. Locate rates for eligible cases ranged from 80 to 94 percent. Participation rates based on eligible located cases ranged from 87 percent for private for-profit (both less-than-2-year and 2 year or more) schools to 94 percent for private not-for-profit 4-year doctorate-granting and non-doctorate-granting schools. Response rates based on total eligibility ranged from 69 to 88 percent across the nine institution types. "}, {"section_title": "Interview Outcomes by Mode", "text": "The BPS:04/06 interview was cross-modal, where three options for interview administration were utilized during the data collection period, March 20-September 10, 2006. Self-administered web interviews remained an option throughout data collection; however, the early self-administered interview completion period with increased monetary incentive occurred during the first 4 weeks only. Once the early option ended, CATI began on April 19, 2006, and continued through the beginning of September. Field interviewing, or computer-assisted personal interviewing (CAPI), began near the end of data collection when remaining cases were most difficult to locate or reach by telephone. Table 9 provides the distribution of interview completions by mode of administration. As expected, web-based self-administration was a more productive and attractive mode for respondents than interviews completed through CATI and CAPI (z = 19.52, p < .01). Specifically, 58 percent of completed interviews were self-administered, while 39 percent of completions were CATI and 3 percent were CAPI. The majority of self-administered respondents (77 percent) completed the survey during the first 4 weeks, providing support for the effectiveness of an early web completion option. As mentioned, the most difficult cases were sent to the field near the end of data collection. Table 10 shows locate and response rates among the 940 cases sent for field interviewing (CAPI). Results overall, by base-year response status and by institution type, are provided. Of all eligible cases sent, 58 percent were located, and, of those located, 92 percent participated. In comparison, 54 percent of all eligible (located and not located) cases that were assigned to CAPI completed an interview. Of the CAPI cases that were located, 93 percent of base-year respondents and 85 percent of base-year nonrespondents participated. For institution type, of the cases that were located, response rates ranged from 88 percent for public 4-year doctorate-granting schools to 96 percent for private for-profit less-than-2-year schools."}, {"section_title": "Procedures to Encourage Response (Prompting, Incentives, and Refusal Conversion)", "text": "On the basis of results from field test experiments, several techniques were employed during full-scale data collection to enhance response rates: prompting of base-year nonrespondents at the outset of data collection, provision of monetary incentives, and focused refusal conversion efforts for difficult cases. The results of each of these measures are provided in the sections that follow. Prompting. The BPS:04/06 field test implemented an experiment to evaluate the effectiveness of prompting calls in increasing self-administered web interview response rates during the early response period. Results from the field test showed that prompting calls did not have a significant effect on interview participation among base-year respondents. Prompting calls did, however, increase response rates among base-year nonrespondents. The most significant finding was that, among prompted cases, there was no difference in interview participation between base-year respondents and nonrespondents, suggesting that prompting calls increase the likelihood that nonrespondents participate at the same rate observed for base-year respondents. Based on the findings from the BPS:04/06 field test, prompting calls were made to baseyear nonrespondents in the full-scale interview. Of the 380 total eligible base-year nonrespondents, approximately 330 were eligible for prompting. 8 Table 11 compares response rates for base-year nonrespondents who were reached by the prompting calls versus those who were not reached (e.g., staff spoke with someone other than the sample member, an answering machine was reached, or the call was not answered). The results suggest that the prompting calls were more effective when staff spoke with the sample member directly; the response rate for this group was 56 percent, compared to 34 percent for sample members not reached by prompting calls (z = 2.8; p < .01). The overall response rate for prompted base-year nonrespondents was 36 percent. 1 Approximately 60 base-year nonrespondents were ineligible for prompting for miscellaneous reasons such as no valid telephone number to call or had already completed the interview when prompting began. 2 The sample member was considered \"not contacted\" when someone other than the sample member was reached, the call was directed to an answering machine, there was no answer, or the sample member refused the call. NOTE: Only base-year nonrespondents received prompting calls at the start of data collection. Incentives. Table 12 depicts interview response rates per incentive phase (early, production, and nonresponse phase). During the early response phase of data collection, sample members were offered $30 to complete the self-administered web interview. A response rate of 38 percent was obtained during this phase. The production phase, during which sample members received $20 for completing an interview, yielded the lowest rate of response at 30 percent. The nonresponse conversion phase provided respondents a $30 incentive, and the resulting response rate was 53 percent. When compared, response rates observed during the early response, production, and nonresponse conversion phases were significantly different from one another (early vs. production z = 14.2; production vs. nonresponse z = 32.6; early vs. nonresponse z = 22.9; p < .01). Approximately 6 percent of all incentive checks issued to respondents were never cashed. "}, {"section_title": "Refusal Conversion.", "text": "As shown in tables 13 and 14, about 10 percent of all eligible sample members refused to be interviewed at some point during data collection. Of these initial refusals, approximately 38 percent ultimately completed an interview. Table 13 compares number of refusals and completion rates given refusal by base-year response status. Twice as many base-year nonrespondents (25 percent) as base-year respondents (10 percent) refused to be interviewed (z = 9.9; p < .01). Likewise, of those who refused, base-year respondents were converted at a higher rate (39 percent) than base-year nonrespondents (17 percent) (z = 4.3; p < .01). These findings likely reflect base-year respondents' familiarity with the study and their willingness to participate in the past. Table 14 provides refusal and interview rates by institution type. Refusal rates by type ranged from 7 percent for sample members who attended private not-for-profit 4-year institutions (both non-doctorate-granting and doctorate-granting) to 13 percent for sample members who attended public less-than-2-year institutions. Moreover, interview rates given sample member refusal ranged from 28 percent for sample members who attended private not-for-profit lessthan-4-year institutions to 45 percent for sample members attending public less-than-2-year schools.  Locating and Interviewing Outcomes. For the BPS:04/06 full-scale study, tracing began in fall 2005 by updating address and other contact information collected during the NPSAS:04 interview. Several tracing resources were used, including the Central Processing System (CPS), which contains federal financial aid application information and databases from Telematch, Accurint, and the National Change of Address (NCOA) file. Table 15 shows the record match rate for each method of batch tracing employed. Match rates, which are based on the number of records either confirmed or updated with new information, ranged from 5 percent for the second round of NCOA tracing to 72 percent for Telematch. The overall match rate, which accounts for all tracing methods implemented, was 46 percent. In addition, as part of each mailing to sample members and their parents, sample members were asked to complete an address update form either on the study website or on a hardcopy form. Table 16 presents the located and interview rates for those who returned some form of address update sheet. Almost all sample members who provided updated address information were located (98 percent), and about 96 percent of those who updated their contact information completed an interview. Locating and interview rates by intensive tracing status are shown in table 17. Of cases that were sent to the first stage of intensive tracing, or CCS-1, 60 percent were located, and 76 percent of those completed an interview. Among cases sent to the second stage of intensive tracing, or CCS-2, 36 percent were located and 69 percent of those located were interviewed.   "}, {"section_title": "Interview Burden and Effort", "text": "The following section provides a detailed look at the burden and effort associated with conducting and participating in the BPS:04/06 interview. Respondent burden is presented in a series of tables depicting the time needed to complete the interview, including overall, by section, and by mode comparisons. Completion times in conjunction with specific respondent characteristics are also presented, along with the timing of coding systems. Efforts by interviewing staff are examined, including hours expended, call counts, and results of call screening."}, {"section_title": "Timing to Complete the Student Interview", "text": "Overall Interview Completion Time. To track the time needed to complete the interview, two time stamp variables were associated with each question. The start time stamp recorded the computer clock time at which a particular question was displayed on the respondent's or interviewer's screen. The second time stamp variable, the end stamp, recorded the clock time at which the respondent or interviewer clicked \"Continue\" on that same screen. These two time stamps enabled calculation of on-screen and transit time. On-screen time was calculated by subtracting the start time from the end time for each web page received. The end stamp of a preceding screen subtracted from the start stamp of a current screen provided transit time. Transit time, therefore, takes into account several processes including data transmission time to the server, server processing time, and loading time of the next screen. Total on-screen time and total transit time were calculated for all respondents by summing all of the on-screen times for each screen received and summing all of the transit times for each respondent. Total instrument time was then calculated by summing a respondent's total on-screen and total transit times. Table 18 displays average completion time for the BPS:04/06 completed student interviews both overall and by section. The average total interview time was 19.6 minutes; section completion times were 2.1 minutes for section A, 2.7 minutes for section B, 6.5 minutes for section C, 4.7 minutes for section D, 6 minutes for section E, and 3.2 minutes for section F. Section A, which determined eligibility for BPS:04/06, was mostly administered to base-year nonrespondents; however, base-year respondents with questionable eligibility received several questions that clarified their eligibility for the BPS:04 cohort. 1 The number of cases in each section may vary because not all sections were applicable to all sample members. NOTE: Detail may not sum to totals because of rounding. Interview times are based on full completed web and computer-assisted telephone interview (CATI) cases only. Computer-assisted personal interview (CAPI), Spanish and English abbreviated versions, and partial cases were excluded from analysis. Also excluded from this analysis were outlier cases. An outlier was defined as any case whose completion time exceeded two standard deviations above or below the average time for a given section or for the total interview. Outliers were identified separately for each section and for the total interview; therefore, individual section times do not sum to the total interview time. There were approximately 1,130 outliers excluded from total interview time. In addition to the full BPS:04/06 interview, an English abbreviated version was available via web, CATI, and CAPI, and a Spanish abbreviated version was available via CATI and CAPI. The abbreviated interviews included sections A, B, and F. The number of English abbreviated interview completions was 1,130. Excluding outliers, the average total interview time for sections A, B, and F was 6.1 minutes. Specific section completion times included 2.7 minutes for section A, 2.7 minutes for section B, and 3.4 minutes for section F. A total of 240 abbreviated interviews were conducted in Spanish. Excluding outliers, the average total interview time was 8.2 minutes, while section completion times were 3.9 minutes for section A, 3.3 minutes for section B, and 4.8 minutes for section F. 9 Timing by Respondent Characteristics. When specific respondent characteristics were taken into account, interview completion times varied. In particular, the respondent characteristics of base-year response status, postsecondary enrollment, and employment affected overall and section completion time. Such comparisons are presented in table 19. Regarding base-year response status, nonrespondents had a higher completion time overall (22.9 minutes) than base-year respondents (19.6 minutes) (t = 5.79, p < .0001). Because section A focused on eligibility determination, base-year nonrespondents were asked more questions than base-year respondents. On average, it took base-year nonrespondents 3.9 minutes to complete section A, compared to less than 1 minute for base-year respondents (t = 29.27, p < .0001). Respondents enrolled at the time of the interview were not administered the employment questions in section D. Consequently, enrolled respondents required less time overall to complete the interview (19.4 minutes) than respondents who were no longer enrolled (19.9 minutes) (t = 4.35, p < .0001). Enrolled respondents, however, required more time (3.1 minutes) to complete section B, the enrollment history section, than those not enrolled (2.1 minutes; t = -30.74, p < .0001). The final respondent characteristic considered was employment status. Employed respondents had a longer overall interview completion time (20.7 minutes) than respondents not in the workforce (16.8 minutes) (t = -16.80, p < .0001). The difference in completion times can be attributed almost entirely to differences in section D completion times for these two groups. Because section D focused primarily on characteristics of the current job, employed respondents were asked significantly more questions than those not currently employed. Employed respondents took an average of 5.9 minutes to complete section D, compared to an average of 1.2 minutes for respondents not currently employed (t = 123.09, p < .0001). Section A -Base year 0.5 3.9 2.4 1.9 1.8 1.9 Section B -History 2.7 2.5 3.1 2.1 1.8 1.9 Section C -Characteristics 6.5 6.3 6.4 6.7 6.6 6.4 Section D -Employment 4.7 4.3 \u2020 4.7 5.9 1.2 Section E -Background 6.0 6.2 5.9 6.1 6.1 6.4 Section F -Locating 3.2 3.5 3.1 3.4 3.4 3.6 \u2020 Not applicable. NOTE: Interview times are based on full completed web and computer-assisted telephone interviewing (CATI) cases only. Computer-assisted personal interviewing (CAPI), Spanish and English abbreviated, and partial cases were excluded from analysis. Also excluded from this analysis were outlier cases. An outlier was defined as any case whose completion time exceeded two standard deviations above or below the average time for a given section or for the total interview. Outliers were identified separately for each section and for the total interview; therefore, individual section times do not sum to the total interview time. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2004/06 Beginning Postsecondary Students Longitudinal Study (BPS:04/06). Timing for Coding Systems. Interview completion times can be affected not only by respondent characteristics, but also by the extent to which respondents need to code responses during the interview. Several coding systems were used in the BPS:04/06 instrument, and the onscreen time required for coding is presented below. The institutional coding system collected information on additional schools attended, and provided various ways to search for schools, including by city, state, and/or school name. The field of study and occupation coding systems used an assisted-coding approach, such that the entry of text strings interfaced with a database to identify the best match or provide a set of comparable matches. The industry coder presented a list of radio button choices. 10 Overall, it took self-administered respondents and CATI interviewers just over 1 minute to code occupation and less than 1 minute to code institutions, field of study, and industry. No mode differences were found. Timing by Completion Mode. Differences between web-based cases, self-administered and interviewer-administered (CATI), are presented in table 20. CATI respondents required approximately 1 minute longer to complete the BPS:04/06 interview than self-administered respondents, 20.4 minutes as opposed to 19.1 minutes (t = -12.16, p < .0001), largely due to the conversational nature of the CATI interviewer-respondent experience. Exchange and verification of information and the verbal administration of each question plus response options tend to increase completion time. As with overall times, section A had the shortest completion time, and section E had the longest completion time across mode. Interview times are based on full completed web and computer-assisted telephone interviewing (CATI) cases only. Computer-assisted personal interviewing (CAPI), Spanish and English abbreviated, and partial cases were excluded from analysis. Also excluded from the analysis were outlier cases. An outlier was defined as any case whose completion time exceeded two standard deviations above or below the average time for a given section or for the total interview. Outliers were identified separately for each section and for the total interview; therefore, individual section times do not sum to the total interview time. Detail may not sum to totals because of rounding Completion times tended to increase when interviews were conducted in the field. Table  21 shows the average interview completion times for the two CAPI modes, in-person and by phone. Table 21. Average time in minutes to complete interview, by section and CAPI type: 2006   Interview section   CAPI by phone  average time   CAPI in-person  average time  Total interview  24.3  23.8 Section A -Base year 4.3 7.5 Section B -History 2.4 2.6 Section C -Characteristics 7.9 8.1 Section D -Employment 4.9 5.7 Section E -Background 7.1 6.7 Section F -Locating 4.9 4.8 NOTE: Interview times are based on completed computer-assisted personal interviewing (CAPI) cases only. Full completed web, computer-assisted telephone interviewing (CATI), Spanish and English abbreviated, and partial cases were excluded from analysis. Also excluded from this analysis were outlier cases. An outlier was defined as any case whose completion time exceeded two standard deviations above or below the average time for a given section or for the total interview. Outliers were identified separately for each section and for the total interview; therefore, individual section times do not sum to the total interview time. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2004/06 Beginning Postsecondary Students Longitudinal Study (BPS:04/06). Table 22 displays the average on-screen and transit times for both web-based, self-administered and interviewer-administered (CATI) surveys. CATI respondents had longer on-screen time (18.7 minutes) than web-based, self-administered respondents (15.7 minutes) (t = -31.11, p < .0001). This finding is expected, given the conversational nature of the interviewer-respondent experience. Average transit time between screens, however, was longer for web respondents (3.4 minutes) than CATI respondents (1.7 minutes) (t = 61.00, p < .0001). This result is most likely due to the efficient high-speed connection provided by the call center facility. Average on-screen and transit time by mode of administration is presented in figure 5.  As mentioned above, internet connection type impacts transit time, thereby affecting overall completion time. Many self-administered respondents indicated completing the interview via fast connection, as shown in table 23. A random sample of respondents was asked to complete a small set of debriefing questions post-interview. Almost 90 percent of respondents answering a debriefing question concerning internet connection type indicated using a fast connection. As expected, respondents using dial-up modems experienced much longer transit times (7.8 minutes) than those with a fast connection (3.1 minutes) (t = -38.86, p < .0001). 1 Represents those from the random sample selected for debriefing that provided a response to connection type. NOTE: Detail may not sum to totals because of rounding. Interview times are based on full completed web and computer-assisted telephone interviewing (CATI) cases only. Computer-assisted personal interviewing (CAPI), Spanish and English abbreviated, and partial cases were excluded from analysis. Also excluded from this analysis were outlier cases. An outlier was defined as any case whose completion time exceeded two standard deviations above or below the average time for a given section or for the total interview. Outliers were identified separately for each section and for the total interview; therefore, individual section times do not sum to the total interview time. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2004/06 Beginning Postsecondary Students Longitudinal Study (BPS:04/06)."}, {"section_title": "Components of Timing.", "text": ""}, {"section_title": "Telephone Interviewer Hours", "text": "The CATI component of data collection required considerable effort on the part of telephone interviewers and call center supervisory staff. Telephone interviewer hours for BPS:04/06 totaled about 17,040 hours with an average of 2.4 hours spent per completed interview. Given the average telephone interview completion time of 20 minutes, the remaining 2 hours was spent in activities outside the actual interview. The majority of this time was dedicated to locating and contacting each sample member. Multiple interview attempts were made with each sample member for whom contact information was available. When necessary, contacts with all available locating sources were attempted in an effort to interview a sample member. The balance of interviewer time was spent on case maintenance, such as opening a case and reviewing its call history, scheduling callbacks, providing comments, and updating case statuses."}, {"section_title": "Number of Calls and Call Screening", "text": "The average number of calls required to obtain a completed interview varied according to prior response status and phase of data collection. Table 24 shows the average number of telephone calls overall and by current and prior response status, mode of administration, and incentive period. On average, 14 calls were made per sample member. Base-year respondents received 2 fewer calls, on average, than base-year nonrespondents (14 and 16, respectively) (t = 2.54, p > .05). As expected, significant call count differences were found between BPS:04/06 respondents and nonrespondents and between self-administered, and intervieweradministered modes. For example, BPS:04/06 participants were called on average 10 times, compared to an average of 33 calls to nonrespondents (t = 67.82, p < .0001). Self-administered web respondents were called less frequently than CATI/CAPI cases, an average of 5 calls as opposed to 15 calls (t = -44.10, p < .0001). While fewer calls would be expected for selfadministered web respondents, outbound Help Desk calls were made as needed to respond to sample member inquiries, such as password requests and technical assistance. Call counts also varied by incentive period. Respondents who received either the early response or production incentive were called an average of three times (an average of less than one for early response and eight for production incentive), while those who received or were eligible to receive the nonresponse incentive were called an average of 29 times (t = 135.25, p < .0001). As mentioned above, both BPS:04/06 nonrespondents and those eligible for the nonresponse incentive had high call counts. Interviewer-based studies increasingly have to work around call screening behaviors in order to reduce the rate of participant nonresponse. Sample members use various devices such as answering machines, caller ID, call blocking, call filtering, and privacy managers in order to be selective about incoming calls. Cell phones also provide onscreen identification and instant voicemail. While call screening provides privacy and selectivity to the individual, studies with an interviewer component can be adversely affected by way of reduced representativeness, lower response rates, and higher project costs. Just over one half of the cases (9,590 of the 18,640) in the BPS:04/06 sample had an answering machine event. An answering machine event is when an interviewer-initiated call to the sample member resulted in obtaining an answering machine message. Overall, 43 percent of BPS:04/06 respondents has at least one answering machine event compared to 87 percent of nonrespondents (t = 64.92, p < .0001). For sample members with at least one answering machine event occurred, 26 calls on average were made. Only two calls on average were made to sample members when no answering machine event occurred (t = -123.37, p < .0001)."}, {"section_title": "Conclusion", "text": "This chapter reported data collection outcomes for the BPS:04/06 full-scale study, including response rates and interview burden on respondents. Of the 23,090 sample members, 18,640 were eligible for the study. In addition, 20,580 (89 percent) of the 23,090 total sample members were able to be located. The unweighted response rate for all eligible cases (located and not located) was 80 percent, while the response rate for located eligible cases was 90 percent. Prompting calls, use of incentives, and refusal conversion techniques were employed to encourage high response rates. Prompting calls increased the response rate of base-year nonrespondents. In the full-scale study, prompting calls were made to base-year nonrespondents, and the results suggest that response rates were significantly higher when staff spoke with the sample member directly (56 percent), as opposed to when staff spoke to someone other than the sample member, reached an answering machine, or a call was not answered (34 percent). Response rates were higher in the early response phase and nonresponse phase when a $30 incentive was offered than in the production phase when a $20 incentive was offered. Finally, the refusal rates and refusal conversion results indicated that eligible base-year nonrespondents were more likely (25 percent) than eligible base-year respondents (10 percent) to refuse at some point to complete the interview. Further, base-year respondents were converted at a higher rate (39 percent) than base-year nonrespondents (17 percent). Regarding interview burden, it took, on average, 20 minutes to complete the BPS:04/06 student interview. The average number of calls made per sample members was 14, and it varied based on response status. The average number of calls to respondents was 10, and the average number of calls to nonrespondents was 33."}, {"section_title": "Chapter", "text": ""}, {"section_title": "Evaluation of Data Quality", "text": "This chapter includes summaries of evaluations conducted throughout the 2004/06 Beginning Postsecondary Students Longitudinal Study (BPS:04/06) data collection, as well as a detailed analysis of the quality of data collected. Analyses of quality control procedures, coding processes, and item-level nonresponse are also presented in this chapter."}, {"section_title": "Defining and Identifying First-Time Beginners (FTBs) for Cohort Membership", "text": "Identifying first-time beginners (FTBs) for membership in the BPS:04/06 cohort required an extensive process involving data collected across two studies, the 2003-04 National Postsecondary Student Aid Study (NPSAS:04) and the current study, BPS:04/06. Data were collected from a number of sources, including \u2022 lists of students enrolled during the 2003-04 academic year at the NPSAS-eligible institutions that provided the lists; \u2022 student-level data abstracted from the student's institutional record using a computerassisted data entry ( The following section describes the process by which sample members were identified and ultimately classified as FTBs across these multiple data sources and time periods."}, {"section_title": "FTB Identification During NPSAS:04", "text": "To begin the NPSAS:04 data collection, NPSAS-eligible institutions were asked to submit to RTI lists of all students enrolled at the institution at any time during the 2003-04 academic year. Students were classified by their institutions as being either FTBs, other undergraduates, or graduate and professional students. As discussed in chapter 2, students should be identified as FTBs if they were an undergraduate enrolled at some time between July 1, 2003 and June 30, 2004 and, prior to July 1, 2003, had not earned any postsecondary degrees or completed any postsecondary classes toward a degree or formal award since completing high school requirements. 11 Table 25 presents the number of NPSAS-eligible FTBs and other undergraduate and graduate/first-professional students sampled from institution lists according to how they were listed initially by the institutions. That students were identified by their institutions as FTBs was only the first step in confirming student eligibility for membership in the BPS cohort. Information was also extracted by matching the entire NPSAS:04 sample to two extant databases. The first, the CPS, contains the records of all students who applied for federal financial aid using the Free Application for Federal Student Aid (FAFSA) for the 2003-04 academic year. Question 24 (Q24) of the FAFSA asked applicants about their year in postsecondary education:"}, {"section_title": "What will be your grade level when you begin the 2003-2004 school year?", "text": "Sample members who answered Q24 as \"first year/never attended college before\" were considered FTBs according to CPS and, therefore, potentially eligible for membership in the BPS cohort. In addition to the CPS, the NPSAS:04 student sample was matched to the National Student Loan Data System (NSLDS), which, as the central data system for federal student aid, contains records for Pell grants and the Direct loan program (including subsidized and unsubsidized Stafford, Perkins, and PLUS loans). As a history file, the NSLDS contains aid records for all years of a student's funding, not just the current academic year. Although NSLDS records do not contain an FTB indicator, it was assumed that any student with a record of federal financial aid receipt prior to the 2003-04 academic year could not have been an FTB in 2003-04. Two additional sources of student data were involved in the determination of eligibility for the BPS cohort. First, as part of NPSAS:04, records at the NPSAS institutions were abstracted for the entire sample using a CADE methodology. The CADE abstraction instrument contained one item, Question 8, that could help identify a particular sample member as FTB according to the definition of FTB reported under the Integrated Postsecondary Education Data System (IPEDS)."}, {"section_title": "Is this student classified as a first-time, first-year degree-seeking student for IPEDS reporting purposes? [y/n]", "text": "The response provided for CADE Question 8 helped confirm a student's eligibility as an FTB but, given the additional requirement of full-time status under the IPEDS definition, could not be used to exclude sample members from the BPS cohort. In addition to CADE, attempts were made to interview students selected for NPSAS:04 using either a self-administered interview on the Web, or a computer-assisted telephone interview (CATI). Several items in the NPSAS:04 student interview helped to clarify a student's status as an FTB. Depending on whether or not the NPSAS institution was the first postsecondary institution attended in the 2003-04 academic year, students were asked either N4FSTSTR or N4SCHSTR (see table 26), which determined when the sample member first enrolled at any postsecondary institution after completing high school requirements. If the sample member reported enrollment prior to the 2003-04 academic year, N4CMPCLS determined whether or not credit was earned for the prior enrollment. As long as the student did not earn transferable credit for postsecondary enrollment between high school completion and July 1, 2003, he or she would still be considered potentially eligible for the BPS cohort. At the end of the NPSAS:04 data collection, all available information for sample members-classification on the institution lists, student interview, CADE, and CPS and NSLDS records matching-was reviewed to make a final determination of BPS eligibility. Since these sources were sometimes found to be contradictory, a judgment was made as to the likely eligibility of each sample member. The outcome of this analysis is shown in table 27, organized according to the institution's original classification of the student. Institution listings of students were found to be correct for about 86 percent 12 of the NPSAS:04 eligible sample. FTBs were falsely classified as such for 35 percent of the listed sample (false positives), while about 10 percent of the other undergraduate, graduate, and first-professional students were determined actually to be FTBs (false negatives). About 17 percent of those whose status as an FTB was unknown at the time of the listing ended up classified as FTBs.  "}, {"section_title": "FTB Identification During BPS:04/06", "text": "In preparation for BPS:04/06 full-scale data collection, the 32,170 FTBs identified during NPSAS:04 were subsampled (see chapter 2) to yield a starting sample of 23,090 sample members. NPSAS:04 interview nonrespondents were asked the same set of base-year interview items, described above, to determine eligibility for the BPS:04 cohort. In addition, a subset of the base-year respondents, whose eligibility as an FTB remained in question despite their interview responses and the results of the CPS and NSLDS record matching were rescreened. Table 28 presents the FTB status of the BPS:04/06 sample, according to the original classification of the sample member by the NPSAS institution, following the BPS:04/06 interview. As part of the BPS:04/06 data collection, the BPS:04 cohort was again matched to the CPS, for every application year since 2004, and NSLDS databases. In addition, in 2006, the cohort sample was matched to a new source, the National Student Clearinghouse (NSC) StudentTracker database, which contains enrollment and degree completion data for any students enrolled in NSC-participating institutions. 13 A record match for a student's enrollment at the NPSAS institution was obtained for about 60 percent of FTBs (for further discussion of the match rates obtained from the CPS, NSLDS, and NSC, see section 4.2). Since it is a history file, the NSC shows current and prior postsecondary enrollment, as well as degrees being attempted and earned at all known institutions for an individual student. Consequently, the NSC data provided another opportunity to evaluate whether sample members were appropriately classified as FTB students in the 2003-04 academic year in light of what was already known from, and was sometimes contradictory with, the CPS, NSLDS, and interviews. If the NSC data confirmed enrollment prior to July 2003, or indicated degrees earned prior to 2003, the sample member was concluded to be ineligible for the BPS:04 cohort. Table 29 presents the final determination of BPS:04 cohort eligibility as a result of records matching to the CPS, NSLDS, and NSC databases in 2006. Based on the combination of information known about sample members across sources, another 7 percent of the sample initially classified as FTB was determined to be ineligible for the BPS:04 cohort. Identification of FTBs for the BPS:04 cohort, therefore, began with the NPSAS institution's classification of students and ended with the completion of records matching following data collection for BPS:04/06. Table 30 shows the final false positive and false negative rates of the initial institutional classification. From among the students initially classified by their institutions as FTBs for the 2003-04 academic year, 48 percent were ultimately determined not to be FTBs (false positives). Among those classified by their NPSAS institution as other undergraduate, graduate, or first-professional students, about 4 percent were determined to be eligible for the BPS:04 cohort (false negatives), with another 13 percent of students with unknown classification determined to be FTBs as well. "}, {"section_title": "Record Matching Rates for the BPS:04 Cohort", "text": "As described above, the initial FTB sample for BPS:04 was selected from a cross-section of students enrolled at NPSAS-eligible postsecondary institutions between July 1, 2003, and June 30, 2004. From the sampled institutions, RTI received lists of enrolled students from which it selected a random sample of students for participation in NPSAS. Data collection for NPSAS:04 included an abstraction of a sample member's institution records, an interview with the sample member, and record matching to the NSLDS and CPS databases. Similarly, the BPS:04/06 data collection included a follow-up interview with FTBs and additional record matching to CPS, NSLDS, and the NSC StudentTracker database. This section provides a discussion of the observed match rates for these three databases within the BPS:04 cohort. Matching to the CPS. In order to determine an accurate match rate for the CPS, RTI identified respondents who were known to have received federal financial aid of any kind because receipt of federal aid necessarily implies that an application was made. Two sources of information were used to identify federal aid recipients: presence of a record in the NSLDS and information abstracted from each sample member's institution record (CADE data) as part of NPSAS:04. Matching to the CPS applicant database was performed by RTI several times over the course of the 2003-04 FAFSA application year, using the sample member's SSN concatenated with the first two letters of the last name as the \"CPS ID.\" SSNs were received for approximately 96 percent of the NPSAS:04 eligible potential FTBs. As shown in table 31, about 97 percent of students known to have received federal aid matched to records in the CPS. Matching to the NSLDS. The second major database for records matching in BPS:04/06 was the NSLDS. To match to the NSLDS, RTI provided student SSNs to staff at the U.S. Department of Education; actual matching was performed at the request of the Department by the NSLDS contractor. Match rates were calculated against the number of BPS:04 cohort members identified in NPSAS:04 who, based on abstracted CADE records, were known to have received either a Pell grant or a Direct loan during the 2003-04 academic year. As shown in table 32, about 94 percent of study respondents with known Pell grants and/or Direct loans matched to the NSLDS database. Matching to the NSC. In addition to the CPS and NSLDS file matching, the BPS:04 cohort sample was matched to the NSC StudentTracker database in September 2006. RTI provided SSNs and dates of birth to the NSC for those sample members for whom the information was available (about 97 percent). 14 Irrespective of institution, at least one record match was found in the database for 76 percent of FTBs. Results of the student-level file matching to the NSC for their NPSAS institution enrollment are shown in table 33. Overall, a record match for a student's enrollment at the NPSAS institution was obtained for about 60 percent of the FTBs with an SSN and/or date of birth. An individual student record would match to the NSC only if the student's NPSAS institution was a participant in the NSC. 15 Of the original NPSAS institutions, 94 percent enrolled FTBs. (The remaining institutions were professional schools that do not enroll first-time 14 If not already provided during NPSAS:04, additional SSNs and birth dates were obtained from student respondents during the first follow-up interview. 15 Institutional participation in the NSC was assumed if a record of enrollment at the NPSAS institution was located for at least one BPS sample member. postsecondary students.) Among those institutions enrolling FTBs, 65 percent participated in the NSC. As shown in table 34, an NSC record match was obtained for about 84 percent of students enrolled at NSC-participating institutions.  "}, {"section_title": "Online Coding", "text": "The online coding systems used to code institution, major, occupation, and industry were developed to standardize sample member responses into predetermined categories. As described in chapter 2, institution, major, and occupation coding systems involved an assisted coding mechanism that retrieved a list of possible codes following the entry of a text string. Industry was coded manually from a list of 24 industries. The reliability of the coding systems was assessed by expert coders. When institution codes were missing, expert coders used the text string (if provided) to determine the appropriate school code. The rate at which institution codes were upcoded is presented in the next section. A total of 25 percent of reported majors, occupations, and industries were subject to review by expert coders. Expert coders reviewed text strings collected by the three coding systems and selected the appropriate corresponding code. An application compared expert codes to original codes provided in the interview. Expert codes replaced original codes when they did not coincide. Recoding rates for major, occupation, and industry are presented in table 35 along with comparisons by mode."}, {"section_title": "Institution (IPEDS) Coding", "text": "Institution names and enrollment history were collected in section B of the student interview in a looping series of questions that repeated for each school attended. As a result, respondents were able to report enrollment dates and intensity for multiple postsecondary institutions (the maximum number of institutions attended by a single respondent was seven). The upcoding rates were calculated on the basis of whether any of the institution names for a given respondent were upcoded. A small number of respondents (3 percent) had at least one upcoded institution. No mode differences were found."}, {"section_title": "Major Coding", "text": "As described in chapter 2, the major coding system utilized an assisted coder that derived a list of possible matches from the text string the respondent provided. If no areas matched, double drop-down boxes were used to categorize the major. As shown in table 35, 83 percent of all majors were coded correctly according to expert coder evaluations, while 15 percent required recoding and 2 percent of strings were too vague to code accurately. Mode comparisons suggest that CATI/CAPI produced reliable major codes at a higher rate than the self-administered web interview (89 percent of CATI/CAPI codes were correct versus 81 percent of web codes; z = 5.7; p < .01). The mode difference in major coding reliability suggests that the specialized training modules designed to introduce interviewers to the coding systems were effective. That interviewers received this training and, over time, became experienced coders accounts for their greater facility navigating the coding systems compared to self-administered respondents."}, {"section_title": "Occupation Coding", "text": "Chapter 2 discussed how the occupation coding system utilized an assisted coder that derived a list of possible matches from the text string the respondent provided. If no areas matched, then triple drop-down boxes were used to categorize the occupation. Occupation codes, collected from employed respondents who were no longer enrolled in postsecondary education, were correct for 81 percent of cases reviewed (table 35). Of the remaining 19 percent of cases, 17 percent of codes were recoded, and 2 percent were too vague to code. The reliability of occupation coding did not vary significantly by administration mode (z = 0.2)."}, {"section_title": "Industry Coding", "text": "Also described in chapter 2, the industry coding was a manual process where an industry was selected, based on the text string provided by the respondent, from a list of 24 industries. Like occupation, industry data were collected from working respondents who were not enrolled in postsecondary education. Table 35 displays the expert coder results for industry coding. Of the cases subject to quality review by expert coders, 86 percent of codes were deemed correct, while 11 percent required recoding, and 3 percent were too vague to code. The reliability of industry coding did not vary significantly by interview mode (z = 0.7). "}, {"section_title": "Identifying Difficult Items: Help Text, Conversion Text, and Item Nonresponse", "text": ""}, {"section_title": "Help Text Analysis", "text": "The BPS:04/06 full-scale interview offered general and screen-specific help text on all instrument screens. The general help text provided answers to frequently asked questions about response types and browser settings for questionnaire completion. The screen-specific help text provided definitions of terms and phrases used in question wording and response options, and explained the type of information requested. Each help text screen also provided a toll-free telephone number so that sample members could call the BPS:04/06 Help Desk for additional assistance. The number of times respondents clicked the help text button for each screen was tallied to determine the rate of help text access per screen relative to the number of respondents to whom the screen was administered. The screen-level rate of help text access was analyzed overall and by mode of interview administration to identify screens that may have been problematic for users. Overall, the mean percentage of help text hits per screen was less than 1 percent. Across all interview forms, cases completed with an interviewer accessed help text more often than did self-administered cases (1 percent compared with less than 1 percent, respectively; t = -7.1, p < .0001). It should be noted that interviewers were trained and encouraged to use help text as needed. Table 36 presents the rates of help text access for the interview screens that were administered to 50 or more respondents and in which help text was accessed at a rate of 2 percent or more. The item with the highest rate of help text access was employer's primary industry, at a rate of 7 percent. This item asked respondents to provide their employer's primary industry or business. The majority of requests for help text on this screen was from interviewer-administered respondents. Approximately 11 percent of all interviewer-administered respondents used help text for this form, compared to 1 percent among self-administered respondents (z = 11.9; p < .01). The item with the second highest rate of help text access was confirm first-time enrollment, at a rate of 6 percent. No differences were found between modes of administration. The two items where help text was accessed at a rate of 2 percent or more were similar questions. One item, type of employer while enrolled in school, had an overall help text rate of 3 percent. Approximately 9 percent of all interviewer-administered respondents used help text for this form, compared to less than 1 percent among self-administered respondents (z = 19.8; p < .01). The other item, type of employer worked for in current job if no longer enrolled in school, had an overall help text rate of 5 percent. Help text for this form was utilized by 9 percent of interviewer-administered respondents, compared to 1 percent of self-administered respondents (z = 10.3; p < .01). The remaining two items where help text was accessed at a rate of 3 percent dealt with untaxed benefits and remedial courses. In one item, ever taken remedial courses after completing high school, the help text was never accessed by self-administered respondents, but was used by 6 percent (z = 1.9, p < .1) of CATI respondents. For the other item, received other untaxed benefits, the help text was never accessed by CATI respondents, but was used by 4 percent (z = 5.23, p < .1) of self-administered respondents. "}, {"section_title": "Conversion Text Analysis", "text": "To keep item-level nonresponse to a minimum, the BPS:04/06 instrument implemented conversion text for 20 items of critical importance to the study. These key items included such topics as postsecondary enrollment, grade point average (GPA), employment status, amount of undergraduate loans, earnings, race, citizenship status, rent or mortgage payments, and number of credit cards in the respondent's name. If respondents did not provide an answer before continuing to the next screen, the original screen was reloaded with conversion text-a brief statement intended to encourage item completion. This additional text emphasized the confidential nature of the study as well as the importance of individual responses and explained how the information was to be used in research. Table displays the rate of conversion for each item in which the conversion text was viewed by at least 10 respondents (13 of the 20 critical items). Overall conversion rates are shown, as well as rates by mode (self-or interviewer-administered). Rates of conversion ranged from 32 to 100 percent, suggesting that the conversion text was generally successful in encouraging item response. Conversion text was particularly advantageous for items pertaining to enrollment and employment, earnings, and undergraduate loans. Conversion text was least effective in improving responsiveness to the item on monthly rent or mortgage payments (32 percent conversion rate). There were no significant mode differences identified for several of the key items, including still enrolled at last known school, number of jobs, current employment status, race, earnings in 2005, spouse's earnings in 2005, and citizenship status. Conversion text results did vary by mode with critical items of NPSAS enrollment through June 2006 (z = 2.59; p < .01 ), cumulative GPA (z = 6.76; p < .01 ), amount borrowed for undergraduate loans (z = 5.96 ; p < .01 ), monthly rent or mortgage payments (z = 2.63; p < .01 ), and number of credit cards (z = 4.16; p < .01 ). The results indicate that, on these items, higher rates of conversion were obtained through the self-administered web interview. Conversely, a higher rate of conversion was observed for interviewer-administered interviews on the item requesting parents' income in 2005 (z = 2.49; p < .05). This could be confounded by the fact that interviewers chose the don't know option more often than self-administered respondents, thus resulting in a valid answer."}, {"section_title": "Item-Level Nonresponse", "text": "The rate of nonresponse is another data quality measure that can identify troublesome interview items and help to better understand the experiences of sample members in completing the interview. The purpose of the evaluation presented here is not to analyze potential bias, but to identify items that were administered yet have relatively high rates of nonresponse. 16 Missing data for items in the full-scale student interview were associated with a number of factors: (1) a true refusal, (2) an unknown answer, (3) an inappropriate question for the respondent that he or she could not answer, (4) confusion related to the question wording or response options, or (5) hesitation to provide a best guess response. This section discusses items with high rates of missing data (including \"don't know\" responses) to better understand which items may be sensitive or difficult to answer. 17 Total nonresponse rates were calculated for each of the items that were administered to at least 100 respondents. Of over 400 items, only 7 yielded a total nonresponse rate greater than 5 percent. Results of the item-level nonresponse analysis by item are presented in table 38. The item with the highest rate of nonresponse (11 percent) was spouse's total student loan amount. Another item that produced a high nonresponse rate, at 9 percent, was month degree will be awarded at School 2. This item was asked of respondents who indicated they were expecting to complete a degree by July 2006 and listed March, April, May, and June as options, depending on the month the interview was administered. The item for estimate of GPA for respondents who were not graded on a 4.0 scale produced an 8 percent nonresponse; furthermore, 7 percent of nonresponse on this item was due to respondents providing a \"don't know\" response. Other items with more than 5 percent nonresponse included respondent's monthly payment on education loans, parent's income in 2005, and spouse's monthly payment of student loans. These items may have probed matters that were perceived as personal to respondents, thus contributing to their reluctance to answer them or dealt with information in which the respondent truly did not know the answer. Item-level nonresponse rates were examined by mode. All items, except for month degree will be awarded at School 1 and 2 and estimate of GPA, exhibited statistically significant differences in nonresponse rates (p < .01) between self-administered and intervieweradministered modes. Overall, CATI item nonresponse for these items was higher than for selfadministered interviews. Results of the item-level nonresponse analysis by mode are presented in table 39, along with the corresponding z values. The items that exhibited statistically significant differences between modes all pertained to financial information, including income and loan amounts, so the personal nature of the questions could have contributed to the rate of nonresponse for interviewer-administered cases. Overall, the help text, conversion text, and item-level nonresponse rates did not show many significant differences between modes of administration. The help text analysis resulted in 3 out of 4 items showing statistically significant differences between modes. This is an expected result: interviewers were trained to use help text, whereas self-administered web respondents may have forgotten it was available. The conversion text analysis only resulted in 6 out of 13 items showing significant differences between modes. The item-level nonresponse analysis resulted in significant mode differences in 4 out of the 7 items. There was only one item, parent's income in 2005, which contained conversion text and still had a high level of nonresponse. Overall, the rate of nonresponse for the item of parent's income was 5 percent; furthermore, over 80 percent of this item's nonresponse rate reflects respondents selecting the don't know option, which appeared once the conversion text was displayed."}, {"section_title": "Question Delivery and Data Entry Error Rates", "text": "Monitoring telephone data collection accomplishes a number of goals, all aimed at maintaining a high level of data quality. Regular monitoring in BPS:04/06 helped to meet three important quality objectives: (1) reduction in the number of interviewer errors, (2) improvement in interviewer performance by reinforcement of good interviewing practices, and (3) assessment of the quality of the data being collected. Specially trained monitors simultaneously listened to and viewed CATI interviews using remote monitoring telephones and computer equipment. This system provided for sampling of interviewing and interview items during CATI operations. It also allowed monitors to observe live interviews without disturbing the interviewer or respondent. Monitors listened to up to 20 questions during an ongoing interview and, for each question, evaluated two aspects of interviewer performance: (1) correct delivery of questions (error in question delivery) and (2) accurate keying of the response (error in data entry). To ensure that sufficient monitoring occurred for BPS:04/06, monitoring sessions were conducted throughout all of CATI data collection, including day, evening, and weekend shifts. Daily, weekly, and cumulative question delivery and data entry outcomes were measured and displayed on the Integrated Management System (IMS). During CATI data collection, 9,109 items were monitored. During the initial weeks of data collection, the number of observations was lower because telephone interviews were slow to start. Likewise, monitoring efforts were scaled back during the final weeks of data collection due to lighter caseloads being worked by the telephone interviewers. Among the 9,109 items observed, 31 question delivery errors and 20 data entry errors were observed. Throughout the monitoring period, error rates for each 2-week period remained within acceptable limits, never exceeding 1 percent. Error rates in question delivery and data entry, by 2-week data collection periods, are shown in figure 6 and figure 7. Both presentations provide upper and lower control limits for these measures. 18 The error rate peaks are attributable to the addition of new interviewer staff, who were becoming familiar with the student instrument. 19    "}, {"section_title": "Data Collection Evaluations", "text": ""}, {"section_title": "Help Desk", "text": "As described in chapter 2, a Help Desk was available to assist respondents in completing the student interview. Help Desk staff were trained to answer any calls received from the Help Desk hotline, as well as conduct telephone interviews as needed. Help Desk staff assisted sample members with questions about the web instrument and provided technical assistance to sample members who experienced problems while completing the self-administered web interview. Help Desk agents also responded to voice-mail messages left by respondents when the call center was closed. To gain a better understanding of the problems encountered by students attempting to complete the interview over the Web, a software program was developed to record each Help Desk incident that occurred during data collection. For each occurrence, Help Desk staff confirmed contact information for the sample member and recorded the type of problem, a description of the problem and resolution, an incident status (pending or resolved), and the approximate time required to assist the caller. Table 40 provides a summary of Help Desk incidents encountered during BPS:04/06 data collection. Help Desk staff assisted 669 students (4 percent of the sample) with 748 total incidents. The most common type of incident recorded by the Help Desk was sample members that called in to complete the interview (46 percent). Calls from students requesting their Study ID and/or password were the second most common type of Help Desk incident (24 percent). Further, 14 percent of Help Desk incidents involved miscellaneous issues, and 9 percent were related to browser settings and computer problems. Questions about the study accounted for 5 percent of Help Desk incidents, while \"website down\" or \"unavailable\" and \"program error\" call-in incidents each accounted for 1 percent of Help Desk incidents. Questionnaire content questions and questions about the study each represented fewer than 1 percent of Help Desk incidents. "}, {"section_title": "CATI Quality Circle Meetings", "text": "Quality Circle (QC) meetings were vital components for ensuring that project staff, call center supervisory staff, and telephone interviewers were communicating on a regular basis about the goals of the study and addressing challenges encountered along the way. These meetings provided a forum for discussing elements of the CATI instrument, questionnaire design, and interview cooperation tactics; motivating the group toward the goals of the study; and acquiring feedback on data collection issues. Meetings were held biweekly at the Call Center, and an agenda was provided to those in attendance. For interviewing staff unable to attend the meeting, notes were distributed electronically to the Call Center supervisory staff and passed along accordingly. A summary of issues addressed in the meetings is outlined below: \u2022 clarification of questions and item responses; \u2022 BPS eligibility criteria; \u2022 submission of problem sheets; \u2022 the importance of providing detailed case comments; \u2022 data security protocols; \u2022 methods of gaining cooperation from sample members and gatekeepers; and \u2022 general morale boosting and reinforcement of positive interviewing techniques. Throughout the duration of the study, a variety of issues were addressed at the QC meetings that reinforced specific content from training and contributed to prompt problem solving. Some of the issues covered in QC meetings included the following: Writing Problem Sheets. Reporting problems when they occur is an important part of telephone interviewing. Interviewers were trained to report problems electronically and to provide specific detail, including but not limited to the problem that occurred, when it occurred, and the specific point in the interview in which it occurred. Problem sheets further delineated how the issue was addressed. Review of problem sheets in QC meetings was a critical means through which staff learned to recognize and manage the different problems they would encounter. Eligibility Criteria. Because of the considerable complexity of the eligibility criteria, interviewers were reminded to allow eligibility determination to be made by the programmed instrument. Gaining Cooperation. Discussions focused on the difficulty of gaining a sample member's trust during the initial phases of the call. Refusal avoidance strategies were revisited during QC meetings and adapted, as needed, for problems specific to the BPS:04/06 full-scale study data collection. For example, obtaining new contact information from parents (for students no longer living at home) was a focal point for many discussions. Interviewers shared tips for overcoming parent concerns and found ways to benefit and learn from each other's experiences. Questionnaire. Interviewers were given hard copies of the questionnaire and asked to read and review the questions to identify any items that seemed to be confusing or misleading. During QC meetings, particular problems with question wording and other aspects of the interview were discussed. Interviewer Debriefings. At the conclusion of the BPS:04/06 full-scale study, project staff held debriefing meetings with the telephone and field interviewers to learn more about the field test experience. Interviewer debriefings focused on what worked well and what could be improved with respect to the following: \u2022 interviewer training sessions; \u2022 tracing strategies; \u2022 refusal conversion; \u2022 interview questions and coding systems that were difficult for the respondents to answer or the interviewers to code; and \u2022 use of incentives and mailouts. A summary of the telephone and field interviewer debriefing meetings was prepared and will be considered when planning the next BPS follow-up interview in 2009."}, {"section_title": "Conclusions", "text": "This chapter evaluated the quality of data collected by the BPS:04/06 full study instrument and analyzed the quality control procedures, coding processes, and item-level nonresponse. Students eligible for BPS:04/06 are the students who were eligible to participate in the NPSAS:04 and who were determined to be FTB students in the 2003-04 academic year. Approximately 81 percent of BPS:04/06 sample members were determined to be eligible for the study, and 80 percent of the eligible students responded to the interview. Assessment of coding systems by mode was assessed in this chapter. Of the cases subject to quality review by expert coders, 86 percent of industry codes, 83 percent of major codes, and 81 percent of occupation codes coincided with expert coder evaluations. Mode comparisons suggested that the reliability of industry and occupation codes did not vary significantly by interview mode. With major coding, CATI/CAPI produced reliable major codes at a higher rate than the self-administered web interview, suggesting that interview training modules on coding systems were effective. The help text analysis indicated that, on average, help text was accessed per screen less than 1 percent of the time, and the item with the highest rate of help text access was employer's primary industry. Conversion text was implemented for 20 items of critical importance for the study. Rates of conversion ranged from 32 to 100 percent. Conversion text was especially effective for enrollment and employment items and least effective for monthly rent and mortgage payments. The item-level nonresponse analysis indicated that out of the 400 items, only 7 yielded a total nonresponse rate greater than 5 percent. The item with the highest rate of nonresponse was spouse's total student loan amount. The examination of question delivery and data entry error rates indicates that out of the 9,109 items monitored, 31 question delivery errors and 20 data entry errors were observed. In addition, for each 2-week time period, error rates remained within acceptable limits, never exceeding 1 percent. A total of 748 Help Desk incidents was reported; student calls to complete the interview were the most common incidents."}, {"section_title": "Chapter 5 Variable Construction and File Development", "text": "The data files for the 2004/06 Beginning Postsecondary Students Longitudinal Study (BPS:04/06) contain student-level data collected from student interviews and national databases. These files are available as a set of restricted research files, fully documented by an electronic codebook (ECB), and as a public release Data Analysis System (DAS), which also contains full documentation. 20 This chapter describes each file and details the editing and documentation process."}, {"section_title": "Overview of the BPS:04/06 Data Files", "text": "The primary analysis file, from which the study DAS was constructed, contains data for 18,640 eligible sample members. The primary analysis file contains over 800 variables, developed from multiple sources. Throughout the data collection period, data were processed and examined for quality control purposes. Editing of student data began shortly after the start of self-administered web data collection, when procedures and programs for this purpose were first developed. Anomalous values were investigated and resolved, where appropriate, through the use of data corrections and logical recodes. Interim files were delivered to the National Center for Education Statistics (NCES) for review throughout the data collection period. The first DAS was adjudicated and approved for public release in June 2007. Complete data for BPS:04/06 are located on the restricted access files and are documented by the ECB. The restricted files and the ECB are available to researchers who have applied for and received authorization from NCES to access restricted research files. Authorization may be obtained by contacting the NCES Data Security Office. The restricted-use BPS:04/06 ECB contains the following files, each linked by the student's study ID: \u2022 There is a separate record for each study respondent."}, {"section_title": "Data Coding and Editing", "text": "The BPS:04/06 student instrument data were coded and edited using procedures developed and implemented for previous NCES-sponsored studies, including the base-year study, NPSAS:04. The coding and editing procedures fell into two categories: (1) consistency checks and online coding performed within the instrument during data collection and (2) postdata collection data editing."}, {"section_title": "Range/Consistency Checks and Online Coding", "text": "Range and Consistency Checks. The web-based student instrument included edit checks to ensure that data collected were within valid ranges. Examples of some of the general online edit checks include the following: \u2022 Range checks were applied to all numerical entries such that only valid numeric responses could be entered. \u2022 A consistency check was triggered when a respondent provided a valid answer and then checked a \"none of the above\" option. Respondents and interviewers were advised to uncheck other options before checking the \"none of the above\" option. Conversely, if a respondent selected \"none of the above\" first and then checked a valid answer, the system unchecked the \"none of the above\" option automatically. \u2022 Consistency checks were also used for cross-item comparisons. For example, if respondents indicated that they were born in 1989 but graduated from high school in 2004, they were asked to verify this information. Online Coding. As noted in chapter 2, section 2.2.1, BPS:04/06 had a single data collection system for self-administered web interviews, computer-assisted telephone interviews (CATI), and computer-assisted personal interviews (CAPI): a web-based instrument. The web instrument included online coding systems used for the collection of industry, occupation, certificates, and major field of study data. The instrument also included a coding module used to obtain information for all postsecondary institutions that the student attended since the base-year study. These online coding systems greatly reduced the coding efforts and the amount of file merging necessary after data collection was over. They allow the data file user to have useful and familiar codes for analysis while ensuring that most codes are assigned during data collection rather than during the data editing phase."}, {"section_title": "Post-Data Collection Editing", "text": "The BPS:04/06 data were edited using procedures developed and implemented for previous studies sponsored by NCES, including the base-year study, NPSAS:04. Following data collection, the information collected in the student instrument was subjected to various quality control checks and examinations. These checks were to confirm that the collected data reflected appropriate skip patterns. Another evaluation examined all variables with missing data and substituted specific values to indicate the reason for the missing data. A variety of explanations are possible for missing data. Table 41 lists the set of consistency codes used to assist analysts in understanding the nature of missing data associated with BPS:04/06 interview items. Skip-pattern relationships in the database were examined by methodically running crosstabulations between gate items and their associated nested items. In many instances, gate-nest relationships had multiple levels within the instrument. That is, items nested within a gate question may themselves have been gate items for additional items. Therefore, validating the gate-nest relationships often required several iterations and many multiway cross-tabulations to ensure the proper data were captured. The data cleaning and editing process for the BPS:04/06 data files involved a multistage process that consisted of the following steps: 1. Blank or missing data were replaced with -9 for all variables in the instrument database. A one-way frequency distribution of every variable was reviewed to confirm that no missing or blank values remained. These same one-way frequencies revealed any out-of-range or outlier values, which were investigated and checked for reasonableness against other data values (for example, hourly wages of $0.10 rather than $10.00). Creating SAS formats from expected values and the associated value labels also revealed any categorical outliers. Descriptive statistics were produced for all continuous variables. All values less than zero were temporarily recoded to missing. Minimum, median, maximum, and mean values were examined to assess reasonableness of responses, and anomalous data patterns were investigated and corrected as necessary. 2. Legitimate skips were identified using instrument source code. Gate-nest relationships were defined to replace -9's (missing for unknown reason) with -3's (not applicable) as appropriate. Two-way cross-tabulations between each gate-nest combination were evaluated, and high numbers of nonreplaced -9 codes were investigated to ensure skip-pattern integrity. Nested values were further quality checked to reveal instances in which the legitimate skip code overwrote valid data. This typically occurred if a respondent answered a gate question and the appropriate nested item(s), but then backed up and changed the value of the gate, following an alternate path of nested item(s). Responses to the first nested item(s) remained in the database and, therefore, required editing. 3. Variable formatting (e.g., formatting dates as YYYYMM) and standardization of time units, for items that collected amount of time in multiple units, were performed during this step. In addition, any new codes assigned by expert coders reviewing IPEDS, industry, occupation, and major codes from the interview (including those strings that were unable to be coded during the interview) were merged back with the interview data files. Also at this step, some logical recodes were performed when the value of missing items could be determined from answers to previous questions or preloaded values. For instance, if a student is not currently repaying education loans, then the monthly payment amount was recoded to $0. 4. One-way frequency distributions for all categorical variables and descriptive statistics for all continuous variables were examined. Out-of-range or outlier values were replaced with the value of -6 (out of range). 5. One-way frequencies on all categorical variables were regenerated and examined. Variables with high counts of -9 values were investigated. Because self-administered web respondents could skip over most items without providing an answer, -9's did remain a valid value, especially for sensitive items, such as those asking for income information. Concurrent with the data cleaning process, detailed documentation was developed to describe question text, response options, recoding, and the \"applies to\" text for each delivered variable. The documentation information can be found in the student instrument facsimile in appendix F. Data Perturbation. To protect the confidentiality of NCES data that contain information about specific individuals, BPS:04/06 data were subject to perturbation procedures to minimize disclosure risk. Perturbation procedures, which have been approved by the NCES Disclosure Review Board, preserve the central tendency estimates but may result in slight increases in nonsampling errors. BPS:04/06 has multiple sources of data for some variables (CPS, NLSDS, student interview, etc.), and reporting differences can occur in each. Data swapping and other forms of perturbation, implemented to protect respondent confidentiality, can also lead to inconsistencies. Statistical Imputations. All variables in the DAS with missing data were imputed. Imputed data are available on both the DAS and restricted data files. The variables included in the remaining restricted files have not been imputed. The variables were split into six groups, and a consistent imputation methodology was employed for each group. The imputation methodology varied by variable, depending on the relationship between the variable requiring imputation and other variables and the rate and pattern of missing data for the variable requiring imputation. The general imputation methodology is described in Ault et al (2004) and consisted of two steps. The first step, if applicable, was logical or deterministic imputation. That is, if the imputed value could be deduced from the logical relationships with other variables, then that information was used to deterministically impute the value for the recipient. The second step was weighted hot-deck imputation. That is, a relatively homogenous group of observations was identified, and within this group a random donor's value was selected to impute a value for the recipient. Variables requiring imputation were imputed sequentially. However, some variables that were related substantively were grouped together into blocks, and the variables within a block were imputed simultaneously. The order in which variables, or blocks of variables, were imputed was primarily based on the level of missing data. The variables with lower levels of missing data were imputed before the variables with higher levels of missing data. When a variable was selected for imputation based on its level of missing data, three specific pieces of information were evaluated. First, logical consistency was checked to make sure that any known relationships were maintained throughout the imputation process. Second, the pattern of missing data was evaluated to determine whether other variables should be included to create a block of variables requiring imputation. Finally, the imputation class variables and sorting variables were identified. The imputations were categorized into four possible approaches identified by the level and pattern of missing data. There are two categories for the level of missing data for the variable requiring imputation: less than or equal to 5 percent and greater than 5 percent. There were two categories for the pattern of missing data for the variable requiring imputation: unique and similar to other variables requiring imputation. If the pattern of missing data was unique, then the variable requiring imputation was imputed individually. If the pattern of missing data was similar to other variables requiring imputation, then all of variables requiring imputation and having a similar pattern of missing data are imputed as a vector. Typically, the vector imputation contains variables that are logically related. Figure 8 summarizes the four possible categories for the imputations and how they are implemented. All stochastic imputations use the weighted sequential hot-deck (WSHD) methodology (Cox 1980;Iannacchione 1982). In the case where the level of missing data is greater than 5 percent, the stochastic imputations use nonparametric classification trees in conjunction with WSHD. The WSHD methodology replaces missing data with valid data from a donor record within an imputation class. The WSHD methodology also incorporates sorting within imputation class for additional control and uses the sample weight of each record in the donor selection process. The imputation classes in the application of the WSHD methodology were formed by identifying variables related to the variable requiring imputation. Data were sorted within each imputation class to increase the chance of obtaining a close match between donor and recipient. The hot-deck process searches for donors sequentially, starting with the recipient and progressing up and down the sorted file to find the set of eligible donors from which a random selection of one is made. The process is weighted since it incorporates the sample weight of each record in the search and selection routine. For variables with less than or equal to 5 percent missing data, the imputation classes were formed using variables identified by subject matter experts based on prior knowledge and known relationships among variables. For variables with more than 5 percent missing data, the imputation classes were formed using nonparametric classification trees (Breiman et al 1984; Kass 1980). The nonparametric classification trees form imputation classes based on the observations with valid values for the variable requiring imputation. The nonparametric classification tree splits the cases, which are used to define the imputation classes. The observations with missing values are assigned their imputation class based on the same variables used in the tree splits. Given the number of variables and the complexity of the relationships among them, it was virtually impossible to identify and eliminate all inconsistencies. The objective was to reduce inconsistencies as much as possible, especially for key analytic variables. The imputation program was designed to impute all missing data as precisely and efficiently as possible, such that the process could be completed within a very short timeframe after the end of data collection and still maintain the desired quality. The aim was to replace missing data with data that were valid in all cases, with only a few relatively minor and unimportant exceptions. Imputation diagnostics consisted of three checks: overall imputation checks, imputation checks by class variables, and multivariate consistency checks. The imputation checks compared the distributions and sum of the weights and unweighted counts for each level of the imputed variable before and after imputation. Differences greater than 5 percent were flagged and examined to see if changes should be made to the imputation sort of class variables. The imputation checks by class variables evaluated the number of times a given observation was used as a donor, and compared the sum of the weights and unweighted counts for each level of the imputed variable in the defined imputation classes before and after the imputation. Differences of 5 percent or more were flagged for further review. Finally, multivariate consistency checks ensured that relationships between variables were maintained and that any special instructions for the imputation were implemented properly. In any of the three aforementioned checks, if there was any evidence of substantial deviation from the weighted sums or any identified inconsistencies, the imputation process was revised and rerun. Some results of the imputation process are provided in appendix G, which presents the percentage missing for each variable subject to imputation, as well as pre-and postimputation distributions for eight key variables. Appendix G also contains means and percent distributions of the continuous and categorical variables that were imputed. Approximately 30 percent of the variables show statistically significant estimated bias between the pre-and postimputation means and distributions, but the percent relative biases are small and about half have relative bias less than 5 percent. As will be discussed in chapter 6, imputations were performed for BPS:04/06 unit nonrespondents as well as respondents and take the place of a nonresponse adjustment to the analysis weights. Composite and Derived Variable Construction. Analytic variables were created by examining the data available for each respondent from the various data sources, establishing relative priorities of the data sources-on an item-by-item basis-and reconciling discrepancies within and between sources. In some cases, the derived or composite variables were created by simply assigning a value from the available source of information given the highest priority. In other cases, raw interview items were recoded or otherwise summarized to create a derived variable. A listing of the set of analysis variables derived for BPS:04/06 appears in appendix H. Specific details regarding the creation of each variable appear in the variable descriptions contained in the ECB and DAS."}, {"section_title": "Chapter 6 Weighting and Variance Estimation", "text": "This chapter provides information pertaining to the weighting procedures for the 2004/06 Beginning Postsecondary Students Longitudinal Study (BPS:04/06). The development of statistical analysis weights for the BPS:04/06 sample is discussed in section 6.1. Analysis procedures that can be used to produce design-unbiased estimates of sampling variances are discussed in section 6.2, including variances computed using Taylor series and balanced repeated replications (BRR) techniques. Section 6.2 also describes how the Taylor series strata and primary sampling unit (PSU) variables were constructed, and how the bootstrap replicate weights were constructed. Section 6.3 discusses the accuracy of BPS:04/06 estimates for precision and the potential for nonresponse bias."}, {"section_title": "Analysis Weights", "text": "The weights for analyzing the BPS:04/06 data were derived from the 2003-04 National Postsecondary Student Aid Study (NPSAS:04) weights, because the BPS:04/06 sample members are a subset of the NPSAS:04 sample. As described in chapter 2, a stratified sample of 500 NPSAS:04 student interview nonrespondents was selected with probabilities proportional to their NPSAS:04 sampling weights. The weights for these cases were adjusted for the subsampling. The distribution of the weights was examined, and it was determined that they would benefit from trimming and smoothing. Weight sums were compared to estimates obtained from NPSAS and also to external estimates of the population that were obtained from the 2003 Integrated Postsecondary Education Data System (IPEDS) file, and the weights were then calibrated to these external totals. Nonresponse adjustments to the weights were not computed, however, because the BPS:04/06 data file contains the entire eligible BPS:04/06 sample (with imputed data for the BPS:04/06 nonrespondents). This section describes the base weight for BPS:04/06, the trimming and smoothing, and the calibration steps. The overall weighted and unweighted response rates are also provided."}, {"section_title": "Base Weight for BPS:04/06", "text": "The 2004 National Postsecondary Student Aid Study (NPSAS:04) Full-scale Methodology Report (Cominole et al. 2006) (hereinafter referred to as NPSAS:04 Full-scale Methodology Report) describes the development of the NPSAS weights. The statistical analysis weights compensated for the unequal probability of selection of institutions and students in the NPSAS:04 sample. The weights also adjusted for multiplicity at the institution and student levels, unknown student eligibility, nonresponse, and poststratification. The institution weight was computed and then used as a component of the student weight. Weights were computed for NPSAS:04 respondents as the product of the following 13 weight components: 1. institution sampling weight (WT1); 2. institution multiplicity adjustment (WT2); 3. institution poststratification adjustment (WT3); 4. institution nonresponse adjustment (WT4); 5. student sampling weight (WT5); 6. student subsampling weight (WT6); 7. first student multiplicity adjustment (WT7); 8. student unknown eligibility adjustment (WT8); 9. student not located adjustment (WT9); 10. student refusal adjustment (WT10); 11. student other nonresponse adjustment (WT11); 12. second student multiplicity adjustment (WT12); and 13. student poststratification adjustment (WT13). The BPS:04/06 sample contains both NPSAS respondents and nonrespondents. Therefore, the BPS:04/06 base weight was formed as the product of the first eight of these adjustment factors. Specifically, for each student, the BPS:04/06 base weight was computed as The subsample of 500 NPSAS:04 student interview nonrespondents was selected with probabilities proportional to the NPSAS:04 student weight. The BPS:04/06 base weight, W_BPS0, was multiplied by the inverse of this selection probability for the subsampled cases to obtain the weight for cases in the sample."}, {"section_title": "Trimming and Smoothing of BPS:04/06 Weights", "text": "The BPS:04/06 sample consisted of 23,090 students. At the conclusion of the BPS:04/06 data collection, 17,700 students were initially determined to be eligible respondents, 4,550 were nonrespondents, and 840 were ineligible. Logistic models were developed to predict which of the nonrespondents were eligible. As a result of this step, 4,480 nonrespondents were classified as eligible. The distribution of the base weight and unequal weighting effect was examined overall and within subgroups, such as institutional sector, for the 22,190 eligible cases. Some students had very large weights, primarily due to the subsample of NPSAS:04 student interview nonrespondents. Because these cases will appear on the BPS:04/06 data file, many with imputed data, the weights were trimmed and smoothed to reduce the variability of the weights and to prevent these records with mostly imputed data from having a large influence on the estimates derived from the survey. Distributions of the weights were examined within classes formed by the original NPSAS:04 strata. For each of the strata (denoted by h), a maximum value for the weights was computed as where median h is the median of the weights for the stratum, and IQR h is the interquartile range of the weights for the stratum. Weight values greater than this cutoff were trimmed to this value. Very small strata (fewer than 30 cases) and strata with low unequal weighting effects were examined to determine whether trimming was needed and whether this gave a reasonable value. After this trimming and smoothing step, the weights for many NPSAS:04 nonrespondents (who had very large weights due to the subsampling) were trimmed to smaller values. The trimmed weights were adjusted so that they summed to the weights prior to trimming within each of the classes. Table 42 provides the minimum, median, and maximum weights before and after the trimming and smoothing step. The trimming and smoothing reduced the design effect 21 from 4.09 to 1.73 caused by unequal weighting. "}, {"section_title": "Calibration of the BPS:04/06 Weights", "text": "Additional cases were determined to be ineligible after comparing the BPS:04/06 sample members with the National Student Clearinghouse data, resulting in 18,640 eligible sample members. A more detailed discussion of this process is provided in chapter 4. The weight sums for these eligible cases were compared to the NPSAS:04 weight sums and to counts obtained from IPEDS:03 (for categories of students where there is a match of definitions between BPS:04/06 and IPEDS:03). As noted in section 6.1.2, the BPS:04/06 base weight was derived from the NPSAS:04 weight components and did not include the adjustments for nonresponse and poststratification that were applied to obtain the NPSAS:04 student analysis weight. Most of the BPS:04/06 eligible sample members were classified as NPSAS:04 respondents and have a NPSAS:04 weight; therefore, the trimmed and smoothed weights from section 6.1.2 were adjusted to the sums of the NPSAS:04 weights (for the set of BPS:04/06 students who were NPSAS:04 respondents) within the set of classes used for the NPSAS:04 poststratification, and resulted in the NPSAS:04-adjusted weight variable BPSW_NP. Control totals were derived from the sums of the NPSAS:04 weight for the following: 21 The design effect is the ratio of the variance under the sample design divided by the variance under a simple random sample. The design effect has components due to unequal weighting (caused by unequal probabilities of selection and weight adjustments for nonresponse and undercoverage), clustering, and stratification. \u2022 amount of Stafford loans awarded by institution type; \u2022 amount of Pell grants awarded by institution type; \u2022 nonfall undergraduate enrollment by institution type; \u2022 fall enrollment by institution type; and \u2022 fall enrollment by student type. Table 43 presents the variables used for adjusting the trimmed and smoothed BPS:04/06 weights to NPSAS:04 weight sums and the average weight adjustment factors by these variables. The overall weight adjustment factors for the adjustment to the NPSAS:04 weight sums have the following characteristics: \u2022 minimum: 0.31; \u2022 median: 1.27; and \u2022 maximum: 2.44. The weight sums resulting from this adjustment were compared to estimates from IPEDS:03, and a second calibration was made to adjust the weights for the 15,380 BPS:04/06 sample students who were identified as freshmen in fall 2003. These totals from IPEDS:03 were used in the calibration to IPEDS:03 counts: \u2022 fall 2003 freshmen enrollment by institution type; \u2022 fall 2003 full-time freshmen enrollment by institution type; \u2022 number of federal grants for fall 2003 full-time freshmen by institution type; \u2022 number of loans for fall 2003 full-time freshmen by institution type; \u2022 amount of federal grants for fall 2003 full-time freshmen by institution type; and \u2022 amount of loans for fall 2003 full-time freshmen by institution type. Table 44 gives the average weight adjustments for each of these variables. The overall weight adjustment factors for the adjustment to IPEDS:03 weight sums have the following characteristics: \u2022 minimum: 0.10; \u2022 median: 1.03; and \u2022 maximum: 6.23. The BPS:04/06 analysis weight, BPS06_WT, is equal to the IPEDS-calibrated weight (BPSW_IP) for the eligible sample members who were enrolled in fall 2003, and is equal to the NPSAS:04-calibrated weight (BPSW_NP) for the students who were not enrolled in fall 2003. All poststratification and calibration adjustments were computed using RTI's generalized exponential models (GEM; Folsom and Singh 2000), which are similar to logistic models using bounds for adjustment factors and bounds on variance inflation. The GEM approach is a general version of weighting adjustments based on Deville and S\u00e4rndal's logit model (1992). As discussed earlier, no further adjustment was made for nonresponse to the BPS:04/06 interview because all of the eligible BPS:04/06 respondents and BPS:04/06 nonrespondents predicted to be eligible will appear on the data file, with imputed data for the nonrespondents. In BPS:04/06 unit nonresponse is handled through imputation, rather than weight adjustments. Table 45 summarizes the student weight distribution and the variance inflation due to unequal weighting effects (UWE) by type of institution for the BPS:04/06 analysis weight. The median student weight ranges from 51.4 for students in public less-than-2-year institutions to 216.5 for students in public 4-year doctorate-granting institutions. The mean student weight ranges from 88.9 for students in public less-than-2-year institutions to 264.2 for students in private for-profit 2-year-or-more institutions. The unequal weighting effect is 1.95 overall and ranges from 1.3 for students in private not-for-profit less-than-4-year institutions to 2.61 for students in public less-than-2-year institutions. Table 46 gives the control totals obtained from IPEDS:03 and the estimates using the final calibrated weight (BPS06_WT)."}, {"section_title": "Overall Weighted and Unweighted Response Rates", "text": "The overall BPS:04/06 response rate is an estimate of the proportion of the study population directly represented by the study respondents. Because the BPS:04/06 study includes a subsample of NPSAS:04 nonrespondents, the overall study response rate is the product of the NPSAS:04 institution-level response rate times the BPS:04/06 student-level response rate. Therefore, the overall BPS:04/06 response rates can only be estimated directly for defined institutional characteristics. Both weighted and unweighted overall study response rates are shown in table 47, along with their institution and student response rate components. The institution-level response rates shown in this table are the percentage of institutions that provided sufficient data to select the NPSAS:04 student-level sample; these rates were obtained from the NPSAS:04 Full-scale Methodology Report. Only the weighted response rates can be interpreted as estimates of the proportion of the BPS:04/06 population that is directly represented by the study respondents. Table 47 shows that the student response rate is 77 percent and that approximately 62 percent of the BPS:04/06 population is represented by the respondents. The rate of population coverage appears to vary by type of institution; the rate is higher for public institutions than for private institutions. See section 6.3.2 for a discussion of the results of the student and item-level nonresponse bias analyses. Each weighted student response rate was calculated as the weighted number of respondents divided by the weighted number of eligible students. The weight used in these calculations was the calibrated BPS:04/06 weight (BPS06_WT), which is defined for all of the eligible BPS:04/06 sample members. Each overall study response rate was calculated as the product of the NPSAS:04 institutional response rate times the student response rate.   2 Calculated as the product of the institutional response rate times the student response rate. 3 See appendix J, tables J-1 through J-10 for the student-level nonresponse bias analyses. "}, {"section_title": "Variance Estimation", "text": "For probability-based sample surveys, most estimates are nonlinear statistics. For example, a mean or proportion, which is expressed as \u03a3wy/\u03a3w, is nonlinear because the denominator is a survey estimate of the (unknown) population total. In this situation, the variances of the estimates cannot be expressed in closed form. Two procedures for estimating variances of survey statistics are the Taylor series linearization procedure and the bootstrap replication procedure, which are both available on the BPS:04/06 data files. The analysis strata and replicates created for the Taylor series procedure are discussed in section 6.2.1, and section 6.2.2 discusses the replicate weights created for the bootstrap procedure."}, {"section_title": "Taylor Series", "text": "The Taylor series variance estimation procedure is a well-known technique used to estimate the variances of nonlinear statistics. The procedure takes the first-order Taylor series approximation of the nonlinear statistic and then substitutes the linear representation into the appropriate variance formula based on the sample design. Woodruff (1971) presented the mathematical formulation of this procedure. For stratified multistage surveys, the Taylor series procedure requires variance estimation strata and variance estimation primary sampling units (PSUs), also called replicates, defined from the sampling strata and PSUs used in the first stage of sampling. Because BPS:04/06 is a follow-up study of NPSAS:04, the variance estimation strata and PSUs for BPS:04/06 were derived from the variance estimation strata and PSUs that were developed for NPSAS:04 (ANALSTR and ANALPSU). The steps in the construction of the NPSAS:04 stratum and PSU variables are described in chapter 6 of the NPSAS:04 Full-scale Methodology Report (Cominole et al. 2006). The variance estimation formulas require at least two PSUs in each stratum. The NPSAS:04 variance estimation strata and PSUs were examined for the BPS:04/06 sample, and strata with only one PSU were combined with other strata to obtain at least two PSUs. The rules used were the following: variance estimation strata were combined with other variance estimation strata within the original NPSAS:04 sampling strata, certainty schools were combined with other certainty schools, and noncertainty schools were combined with other noncertainty schools. In addition, the original sort order that was used for constructing the NPSAS:04 variance estimation strata and PSUs was used. An ANALSTR was combined with the next ANALSTR in the sorted list. If the stratum was the first in the sorted list, then it was combined with the next stratum in the list. The single PSU then became an additional PSU in the new variance estimation strata. The resulting variance estimation strata and PSUs for BPS:04/06 are the variables BPS06STR and BPS06PSU. The procedure described above may overestimate the variance because it does not always account for the finite population correction (FPC) at the institution stage of sampling. An alternate variance estimation method using replicate weights is also provided for users of the BPS:04/06 data, as described below."}, {"section_title": "Bootstrap Replicate Weights", "text": "The variance estimation strategy that was chosen for BPS:04/06 is the same as that used for NPSAS:04 and satisfies the following requirements: 1. recognition of variance reduction due to stratification at all stages of sampling; 2. recognition of effects of unequal weighting; 3. recognition of possible increased variance due to sample clustering; 4. recognition of effects of weight adjustments for nonresponse and for poststratification of selected total estimates to known external totals; 5. satisfactory properties for estimating variances of nonlinear statistics and quantiles (such as the median) as well as for linear statistics; 6. ability to apply finite population corrections at the institution stage of sampling and reflect the reduction in variance due to the high sampling rates in some first-stage sampling strata; and 7. ability to test hypotheses about students based on normal distribution theory by ignoring the finite population corrections at the student level of sampling. Commonly applied bootstrap variance estimation techniques satisfy requirements 1 through 5. To meet requirements 6 and 7 as well, a methodology developed by Kaufman (2004) was applied, allowing for finite population correction factors at two stages of sampling. The application of Kaufman's method, used for both NPSAS:04 and BPS:04/06, incorporated the finite population correction factor at the first stage only, where sampling fractions were generally high. At the second stage, where the sampling fractions were generally low, the finite population correction factor was set to 1.00. The Kaufman methodology was used to develop a vector of bootstrap sample weights that was added to the analysis file. These weights are zero for units not selected in a particular bootstrap sample; weights for other units are inflated for the bootstrap subsampling. The initial analytic weights for the complete sample are also included for the purposes of computing the desired estimates. The vector of replicate weights allows for computing additional estimates for the sole purpose of estimating a variance. Assuming B sets of replicate weights, the variance of any estimate, \u03b8\u02c6, can be estimated by replicating the estimation procedure for each replicate and computing a simple variance of the replicate estimates, as follows: \u03b8 is the estimate based on the b-th replicate weight (where b = 1 to the number of replicates) and B is the total number of sets of replicate weights. Once the replicate weights are provided, this estimate can be produced by most survey software packages (e.g., SUDAAN [RTI International 2004] computes this estimate by invoking the DESIGN=BRR option). The number of replicate weights was set at 200 based on work that showed that this number of replicates has desirable properties for variance estimation in regression analyses. For the 200 replicate weights included on the weights file (WTA001-WTA200), the calibration process (i.e., calibration to NPSAS:04 weight sums and calibration to IPEDS:03 totals) was repeated so that the variance of survey estimates would include the variability due to the weights adjustment."}, {"section_title": "Accuracy of Estimates", "text": "The accuracy of survey statistics is affected by both random and nonrandom errors. Random errors reduce the precision of survey estimates, while nonrandom errors result in bias (i.e., estimates that do not converge to the true population parameter as the sample size increases without limit). The sources of error in a survey are often dichotomized as sampling and nonsampling errors. Sampling error refers to the error that occurs because the survey is based on a sample of population members rather than the entire population. All other types of errors are nonsampling errors, including survey nonresponse (because of inability to contact sampling members, their refusal to participate in the study, etc.) and measurement errors, such as the errors that occur because the intent of survey questions was not clear to the respondent, because the respondent had insufficient knowledge to answer correctly, or because the data were not captured correctly (e.g., because of recording, editing, or data entry errors). Sampling errors are primarily random errors for well-designed surveys such as NPSAS:04 and BPS:04/06. However, nonrandom errors can occur if the sampling frame does not provide complete coverage of the target population. The BPS:04/06 survey instrument and data collection procedures were subjected to thorough development and testing to minimize nonsampling errors, because these errors are difficult to quantify and are likely to be nonrandom errors. In this section sampling errors and design effects for some BPS:04/06 estimates are presented for a variety of domains. Next, the results of analyses comparing BPS:04/06 nonrespondents and respondents using characteristics known for both nonrespondents and respondents are presented. An analysis of nonresponse bias is presented at both the student level and the item level."}, {"section_title": "Measures of Precision: Standard Errors and Design Effects", "text": "The survey design effect for a statistic is defined as the ratio of the design-based variance estimate divided by the variance estimate that would have been obtained from a simple random sample of the same size. The design effect is often used to measure the effects that sample design features have on the precision of survey estimates. For example, stratification tends to decrease the variance, but multistage sampling and unequal sampling rates usually increase the variance. Weight adjustments for nonresponse (performed to reduce nonresponse bias) and poststratification often increase the variance because they can increase the weight variation. Because of these factors, estimates from most complex multistage sampling designs such as BPS:04/06 have design effects greater than one. That is, the design-based variance is larger than the simple random sample variance. Specifically, the survey design effect for a given estimate, \u03b8 , is defined as The square root of the design effect can also be expressed as the ratio of the standard errors, or In appendix I, design effect estimates are presented for important survey domains to summarize the effects of stratification, multistage sampling, unequal probabilities of selection, and the weight adjustments. These design effects were estimated using SUDAAN and the bootstrap variance estimation procedure described in section 6.2.2. If an analysis of BPS:04/06 data must be performed without using one of the software packages for analysis of complex survey data, the design effect tables in appendix I can be used to make approximate adjustments to the standard errors of survey statistics computed using the standard software packages that assume simple random sampling designs. For example, in an analysis using the BPS dataset on students in public less-than-2-year institutions, the standard error using the simple random sample formulas is estimated to be 1.50. Table I-12 in appendix I gives a median design effect of 1.80 for students in public less-than-2-year universities. An estimate of the standard error, adjusting for the BPS sample design, is 1.50 \u00d7 1.80 = 2.70. Large design effects imply large standard errors and relatively poor precision. Small design effects imply small standard errors and good precision. In general terms, a design effect under 2.0 is low, 2.0 to 3.0 is moderate, and above 3.0 is high. Moderate and high design effects often occur in complex surveys such as BPS:04/06, and the design effects in appendix I are consistent with those in past BPS studies. Unequal weighting causes large design effects and is often as a result of nonresponse and poststratification adjustments. However, in BPS:04/06 (as in NPSAS:04), the unequal weighting is also due to the sample design, different sampling rates between institution strata, and different sampling rates between student strata."}, {"section_title": "Measure of Bias", "text": "The bias in an estimated mean based on respondents, y -R , is the difference between this mean and the target parameter, \u03c0, that is, the mean that would be estimated if a complete census of the target population was conducted and everyone responded. This bias can be expressed as follows: The estimated mean based on nonrespondents, y -NR , can be computed if data for the particular variable are available for most of the nonrespondents. The true target parameter, \u03c0, can be estimated for these variables as follows: where \u03b7 is the weighted unit (or item) nonresponse rate. For the variables that are from the frame, rather than from the sample, \u03c0 can be estimated without sampling error. The bias can then be estimated as follows: This formula shows that the estimate of the nonresponse bias is the difference between the mean for respondents and nonrespondents multiplied by the weighted nonresponse rate. Nonresponse bias analysis was conducted when the response rate at any level (institutions, students, items) was below 85 percent. 22 Institution nonresponse bias was performed as a part of NPSAS:04 and is described in the NPSAS:04 Full-scale Methodology Report (Cominole et al. 2006). A student nonresponse bias analysis and an item nonresponse bias analysis were performed for BPS:04/06."}, {"section_title": "Unit Nonresponse Bias Analysis and Relative Bias.", "text": "A student respondent was defined as any sample member who was determined to be eligible for the study and had valid data for a selected set of key analytic variables. As noted earlier in this chapter, the BPS:04/06 analysis file contains all of the eligible sample members; nonrespondents to the BPS:04/06 interview appear on the analysis file with imputed data. Of the 18,640 eligible sample students 14,900 responded. Approximately 10 percent of the completed interviews were either abbreviated or partial interviews. This gives an unweighted response rate of 80 percent, and a weighted response rate of 77 percent. Since these rates are less than 85 percent, a nonresponse bias analysis was conducted. The nonresponse bias was estimated for variables known for both respondents and nonrespondents. Some of these variables were known for all sample members, and the remaining were only known for federally aided students. These variables are listed below: For all sample members: \u2022 type of institution; \u2022 region; \u2022 institution total enrollment; \u2022 CPS match (yes/no); \u2022 Pell grant recipient (yes/no); and \u2022 Stafford loan recipient (yes/no). For federally aided students: \u2022 Pell grant amount and \u2022 Stafford loan amount. The nonresponse bias was estimated for the above variables and tested to determine if the bias was significant at the 5 percent level. The tests are reported to be statistically significant if the p value is less than 0.05/(k -1), where k is the number of levels of the variable, which adjusts the p value for multiple comparisons. Results are given in table J-1 in appendix J for all institutions combined and in tables J-2 through J-10 by type of institution. The bias was significant for almost half of the categories. However, the relative bias was generally very small: for 7 of the 18 significant variables, the relative bias was less than 5 percent; for another 5, the relative bias was less than 10 percent. This analysis of bias due to student nonresponse examines the difference between respondents and nonrespondents; however, a separate weight adjustment for unit nonresponse was not made because the data file contains both respondents and nonrespondents with imputed data. Including the nonrespondents in the data file and analyses reduces biases due to unit nonresponse. Tables J-2 through J-10 present the bias analyses separately for each of the institutional strata. Item Nonresponse Bias Analysis. When item response rates were less than 85 percent, a nonresponse bias analysis was conducted. This analysis was conducted on the data items collected in the BPS:04/06 interview based on those sample members who responded to the interview. As shown in the equation below, item response rates (RRI) are calculated as the ratio of the number of respondents for whom an in-scope response was obtained (I x for item x) to the number of respondents who are asked to answer that item. The number asked to answer an item is the number of unit-level respondents (I) minus the number of respondents with a valid skip for item x (V x ). When an abbreviated questionnaire is used to convert refusals, the eliminated questions are treated as item nonresponse (U.S. Department of Education 2003). Item response rates were computed using nonimputed data. Valid skips were later logically imputed to the follow-up items after the gate question was imputed. Table J-11 in appendix J lists the 116 variables from the BPS:04/06 interview that have weighted item response rates less than 85 percent, along with the number of cases who were eligible to answer the item. A nonresponse bias analysis was conducted for all items with a weighted response rate less than 85 percent for all students who responded to the BPS:04/06 interview. The possibility of estimating the degree of bias depends on having some variables that reflect key characteristics of respondents and for which there is little or no missing data. The variables that were used (from the bulleted list above) are known for all BPS interview respondents and include type of institution, region, institution total enrollment, CPS match (yes or no), Pell grant recipient (yes or no), and Stafford loan recipient (yes or no). For federally aided students, the variables Pell grant amount and Stafford loan amount (also known for all BPS interview respondents) were also used. These variables are important to the study and are related to many of the items being analyzed for low item response rates. For the items listed above with a weighted response rate less than 85 percent, the nonresponse bias prior to imputation was estimated for each of these characteristics that are known for respondents. Table J-12 in appendix J illustrates the estimated bias (prior to item imputation) for all students who responded to the BPS:04/06 interview for one variable, KCGPAEST-Estimate of GPA. Similar computations were done for all of the variables listed in table J-11 that have item response rates of less than 85 percent. Table J-13 summarizes these computations. To view the complete set of estimated bias tables see http://nces.ed.gov/pubsearch/pubsinfo.asp?pubid=2008184. The percentage of variable categories with statistically significant bias across all items analyzed prior to imputation displayed in this table ranges from 5 percent to 54 percent. A byproduct of the imputation (described in section 5.2.2) is the reduction or elimination of item-level nonresponse bias. Imputation reduces or eliminates nonresponse bias by replacing missing data with statistically plausible values. Missing data and the associated nonresponse bias for variables are usually not ignorable (i.e., the respondents' distribution patterns differ from those in the full population). Therefore, replacing missing data with reasonable values produces imputed sample distributions that resemble full population distributions, thus reducing if not eliminating nonresponse bias. The use of carefully constructed imputation classes, donor-imputee matching criteria, and random hot-deck searches within imputation cells are all designed to ensure that imputed data are possible values and that the nonresponse bias is ignorable within the imputation classes. Item imputation was used to fill in missing item data for BPS:04/06 interview respondents and nonrespondents, as described in chapter 5. Item imputation was used instead of an adjustment for unit nonresponse to the sampling weights. Items imputed included questionnaire items and derived variables; the imputation process is described in chapter 5, and the variables imputed are listed in appendix G. Appendix G also presents the percentage of variables that were imputed and analyses that compare pre-and post-imputation means and distributions. The tables in appendix G include all students who were determined to be eligible for the BPS:04/06 interview (respondents as well as nonrespondents), and are subset to those students eligible to answer each of the items. The nonresponse bias was estimated before and after imputation to evaluate how well the imputations reduced nonresponse bias (see tables G-3 and G-4 for continuous and categorical variables, respectively). Tables G-3 and G-4 present an evaluation of the item nonresponse bias for each individual item based on the number of BPS:04/06 sample members (both respondents and nonrespondents) who were eligible to receive that item. This is in contrast to tables J-11, J-12, and J-13, which present the item-level nonresponse bias analysis after imputation and include only the BPS interview respondents. For continuous variables (table G-3), the estimated bias equals the mean before imputation minus the mean after imputation. For categorical variables (table G-4), the estimated bias was computed for each category as the percentage of students in that category before imputation minus the percentage of students in that category after imputation. Tables G-3 and G-4 also present the percent relative bias, computed as 100 \u00d7 (before imputation mean \u2212 after imputation mean) / (after mean). The bias and the relative bias are generally very small. The estimated bias was also tested (adjusting for multiple comparisons) to determine if the bias was significant at the 5 percent level. A categorical variable was deemed to be significantly biased if any of the categories was significantly biased. As shown in tables in appendix G, about 30 percent of the items show statistically significant estimated bias between the pre-and post-imputation means or distributions, but the percent relative bias is small (less than 5 percent) for about half of these items. Because the overall sample size is fairly large, a large number of estimated biases are statistically significant. However, since the actual percent relative bias is small, the bias is generally small and not practically significant. A-3 To develop the mathematical foundation for the 2003-04 National Postsecondary Student Aid Study (NPSAS:04) institutional and student sampling design, the following notation is used to represent the institutional and student sampling strata: r = 1, 2, ..., 58 indexes the institutional strata, and s = 1, 2, ..., 11 indexes the student/faculty strata. Note that the NSOPF sample of institutions was a subset of the NPSAS institutions, so the institution strata were expanded to accommodate the selection of certain types of institutions for NSOPF. The strata also accounted for selection of institutions in the 12 states where there were representative samples. The institution measure of size (described below) accounted for student as well as for faculty counts and sampling rates. Further define the following notation: j = 1, 2, ..., J(r) indexes the institutions that belong to institutional stratum r, M rs (j) = number of students and faculty during the NPSAS year who belong to person stratum s at the j-th institution in stratum r based on the latest IPEDS data, and m rs = number of students and faculty to be selected from student stratum s within the r-th institutional stratum, referred to henceforth as person stratum rs. The overall population sampling rate for student stratum rs, is then given by The person sampling rates, f rs , were computed based on the final sample allocation and IPEDS data regarding the population sizes. The composite measure of size for the j-th institution in stratum r will then be defined as which is the number of persons that would be selected from the j-th institution if all institutions on the frame were to be sampled. An independent sample of institutions was selected for each institutional stratum using Chromy's sequential, probability minimum replacement (pmr) sampling algorithm to select institutions with probabilities proportional to their measures of size (Chromy 1979). However, rather than allow multiple selections of sample institutions, those institutions with expected frequencies of selection greater than unity (1.00) were selected with certainty, and the remainder of the institutional sample was selected from the remaining institutions in each stratum. This process made it unnecessary to select multiple second-stage samples of persons by precluding institutions with multiple selections at the first stage of sampling. Therefore, the expected frequency of selection for the j-th institution in institutional stratum r is given by Within each of the r institutional type, the type r sampling frame was implicitly stratified by sorting in a serpentine manner (see Williams and Chromy 1980) by the following variables: \u2022 historically black colleges and universities (HBCU); \u2022 Bureau of Economic Analysis Code (OBE) Region (from the IPEDS IC file) with Alaska and Hawaii moved to Region 9 with Puerto Rico; \u2022 state; and \u2022 the institution measure of size. The objectives of this additional, implicit stratification were to ensure some HBCUs, to ensure proportionate representation of all geographic regions and states, and to ensure representation of both large and small institutions. Procedures for obtaining and sampling from student lists included: \u2022 obtaining as many lists as possible in machine-readable form, including e-mails, uploads to the project website, and diskettes or CD-ROMs; \u2022 processing lists on a flow basis as they were received; \u2022 unduplicating samples selected when an institution provided only a hard-copy list for each term of enrollment; \u2022 ensuring that each sample institution received a sufficient sample allocation that at least 30 respondents would be expected; \u2022 implementing quality assurance checks against the latest IPEDS data; and \u2022 compiling a master sample file on a flow basis as sample students were selected, including student and institution sampling weight factors. Student samples were selected as stratified, systematic random samples for both hardcopy and electronic lists primarily because of ease of implementation with hard-copy lists. The student sampling rates were fixed for each sample institution rather than the student sample sizes: \u2022 to facilitate selecting the samples on a flow basis as the student lists were received from sample institutions; \u2022 to facilitate unduplicating the samples selected when an institution provided only hard-copy lists by term; and \u2022 because sampling at a fixed rate based on the overall stratum sampling rate and the institution probabilities of selection results in approximately equal overall probabilities of selection within student strata. Recall that the overall population sampling rate for student stratum rs is given by For the unconditional probability of selection to be a constant for all eligible students in stratum rs, the overall probability of selection should be the overall student sampling fraction, f rs ; i.e., it was ensured that Since it was necessary to set the student sampling rates, and because complete information on eligibility and response status was available, M r\u015d was calculated as follows: where S denotes the set of all sample institutions, E r = the institutional eligibility factor for institutional stratum r, R r = the institutional response factor for institutional stratum r, E rs = the student eligibility factor for student stratum rs. NPSAS is a multivariate survey with a p-dimensional parameter space, \u03b8 = {\u03b8 j }, j = 1, \u2026.., p, for which it is desired to estimate \u03b8 with \u03b8\u02c6 while minimizing cost (sample size) subject to a series of precision requirements. Consequently, optimal sampling rates can be obtained by solving the following nonlinear optimization problem: Minimize: C 0 = fixed cost not affected by changes in the numbers of institutions or students selected; C 1i = variable cost per institution, depending on the number of participating institutions in the i-th institutional stratum; n 1i = number of participating institutions in the i-th stratum; C 2if = variable cost per student, depending on the number of participating students in the f-th student stratum within the i-th institutional stratum; and n 2if = number of participating students in the f-th student stratum within the i-th institutional stratum. In the above, variance constraints correspond to precision requirements that have been specified by NCES for key survey estimates. Using data from the NPSAS:2000 and NPSAS:96 (and NSOPF:99 for faculty constraints), all of the required variance components and their associated precision constraints were developed. Subsequently, the resulting nonlinear optimization problem to determine the most effective sample allocation was solved using Chromy's algorithm (Chromy 1987) to obtain feasible solutions to the above problem. The large sample sizes for NPSAS:04 were required to achieve the many objectives of the study, including estimates for three domains-public 2-year, public 4-year, and private notfor-profit 4-year institutions-in each of 12 states. A baseline cohort of FTBs was also selected for the BPS studies. Moreover, many NPSAS:04 statistical analyses focus on relatively rare domains, thereby requiring large overall sample sizes and disparate sampling rates. Discussions with NCES were used to identify the domains of interest and the study was designed to ensure adequate sample sizes for those domains. Students who first began their postsecondary education in the 2003-04 school year were selected to participate in the Beginning Postsecondary Students (BPS) Longitudinal Study sponsored by the U.S. Department of Education. This study collects information, over time, on students' postsecondary experiences, work while enrolled, persistence in school, degree completion, and employment following enrollment. The enclosed pamphlet describes BPS in more detail and presents selected findings from prior BPS studies. \u00absPfname\u00bb \u00absPlname\u00bb has been randomly selected to participate in this cycle of BPS. We need your help to update our records for \u00abpronoun2\u00bb. \u00abtext1\u00bb. Please take a few minutes to update the enclosed Address Update Information sheet and return it in the enclosed postage paid envelope. We will be re-contacting \u00absPfname\u00bb and other study participants beginning in early spring 2006 to ask questions about their recent education and employment experiences. Your help in updating our records will ensure the success of the study. Only a limited number of people were selected for the study. Therefore, each person selected represents many others, and it is extremely important that we be able to contact them. If \u00absPfname\u00bb completes the interview on the Web by the date provided \u00abpronoun2\u00bb, \u00abpronoun1\u00bb will receive a $30 check as a token of our appreciation. NCES has contracted with RTI International to conduct this cycle of the BPS data collection. Please be assured that both NCES and RTI follow strict confidentiality procedures to protect the privacy of study participants and the confidentiality of the information collected. If you have any questions about the study, please call the RTI study director, Dr. Jennifer Wine, toll-free at 1-877-225-8470. We sincerely appreciate your assistance and thank you in advance for helping us conduct this important study. In 2004, you participated in an interview for the U.S. Department of Education that focused on your early experiences as a postsecondary student and how you paid for your school expenses that year. We are now seeking your help with a follow-up interview with you and students like you who began their education in 2003-04. This new interview, conducted as part of the Beginning Postsecondary Students (BPS) Longitudinal Study, will focus on your experiences since the first interview, as you continued in, completed, or left postsecondary education. Results from previous BPS studies have been used by educators and policymakers to better understand the rate at which beginning students are completing degree programs, the factors preventing them from completing degree programs, and the effects of financial aid and jobs on academic performance. The interview will take about 25 minutes to complete on the Web whenever it is convenient for you. When data collection begins in March, you will receive a postcard that will provide specific information on how to participate. If you complete the interview on the Web by the date indicated on the postcard, you will receive a $30 check as a token of our appreciation. Your participation, while voluntary, is critical to the study's success. By law, we are required to protect your privacy. Your responses will be secured behind firewalls and will be encrypted during Internet transmission. Your responses may be used only for statistical purposes and may not be disclosed, or used, in identifiable form for any other purpose, except as required by law. Enclosed you will find a pamphlet with a brief description of BPS, findings from prior BPS studies, and confidentiality procedures. If your contact information has changed, you may provide your new address and telephone number on the enclosed address update sheet and return it to us in the business reply envelope provided. To find out more about this BPS interview and to update your contact information online, visit the study's website at https://surveys.nces.ed.gov/bps/. The BPS study is being conducted for the U. S. Department of Education's National Center for Education Statistics by RTI International. If you have any questions about the study, please call the RTI study director, Dr. Jennifer Wine, toll-free at 1-877-225-8470. We thank you in advance for your participation in this important study. Your cooperation is greatly appreciated. You have been randomly selected to take part in the Beginning Postsecondary Students (BPS) Longitudinal Study sponsored by the U.S. Department of Education. I am writing to ask you to participate in this important study by completing an interview about your experiences as a postsecondary student at \u00abInst_name\u00bb in 2003-04, and your education and employment experiences since you first enrolled. Results from previous BPS studies have been used by educators and policymakers to better understand the rate at which beginning students are completing degree programs, the factors preventing them from completing degree programs, and the effects of financial aid and jobs on academic performance. The interview will take about 25 minutes to complete on the Web whenever it is convenient for you. When data collection begins in March, you will receive a postcard that will provide specific information on how to participate. If you complete the interview on the Web by the date indicated on the postcard, you will receive a $30 check as a token of our appreciation. Your participation, while voluntary, is critical to the study's success. By law, we are required to protect your privacy. Your responses will be secured behind firewalls and will be encrypted during Internet transmission. Your responses may be used only for statistical purposes and may not be disclosed, or used, in identifiable form for any other purpose, except as required by law. Enclosed you will find a pamphlet with a brief description of BPS, findings from prior BPS studies, and confidentiality procedures. If your contact information has changed, you may provide your new address and telephone number on the enclosed address update sheet and return it to us in the business reply envelope provided. To find out more about this BPS interview and to update your contact information online, visit the study's website at https://surveys.nces.ed.gov/bps/. The BPS study is being conducted for the U. S. Department of Education's National Center for Education Statistics by RTI International. If you have any questions about the study, please call the RTI study director, Dr. Jennifer Wine, toll-free at 1-877-225-8470. We thank you in advance for your participation in this important study. Your cooperation is greatly appreciated. Data collection for the Beginning Postsecondary Students (BPS) longitudinal study is coming to a close. Because the results from this study will help develop policy regarding participation in higher education, your experiences will help determine how future tax dollars are spent. You will also help represent the thousands of others like you who lead busy lives. If you complete your BPS interview soon, you will receive a $\u00abIncAmt\u00bb check as a token of our appreciation. You may access the web interview by logging on to our secure website at https://surveys.nces.ed.gov/bps/ using the Study ID and password provided below. Study ID = \u00abcaseid\u00bb Password = \u00abpassword\u00bb On average the interview takes less than 25 minutes to complete, and your responses may be used only for statistical purposes and may not be disclosed, or used, in identifiable form for any other purpose, except as required by law. If you have questions or problems completing your interview online, or would like to complete your interview over the telephone with a professional interviewer, simply call the BPS Help Desk at 1-800-334-2321. If you have any questions or concerns about the study itself, please contact the BPS Project Director, Dr. Jennifer Wine, toll free at 1-877-225-8470 (e-mail: jennifer@rti.org), or the NCES Project Officer, Dr. Tracy Hunt-White, at 202-502-7438 (e-mail: tracy.hunt-white@ed.gov). Thank you in advance for making BPS a success. The Beginning Postsecondary Students Longitudinal Study (BPS) is now being conducted. Sponsored by the US Department of Education, the results of this study will be used to better understand the rate at which beginning students are completing degree programs, the factors preventing them from completing degree programs, and the effects of financial aid and jobs on academic performance."}, {"section_title": "Computer-Assisted Personal Interview (CAPI) Lead Letter", "text": "Your participation is critical to the success of the study. We need you to complete a brief interview with our Field Interviewer, which can be arranged at a time convenient to your schedule. All of your responses will be kept confidential and will be protected to the fullest extent allowable under law. When you complete your interview, we will pay you $30 to reimburse you for your time. On behalf of the U.S. Department of Education, I am writing to ask for your participation in the Beginning Postsecondary Students Longitudinal Study (BPS). Because the results from this study will help develop policy regarding participation in higher education, your experiences and opinions will help determine how future tax dollars are spent. You will also receive a $30 check as a token of our appreciation for the time you took to complete the survey. Please call us at 1-800-334-2321 to complete a telephone interview or, if you wish to complete the interview yourself over our secure website, log on to https://surveys.nces.ed.gov/bps/. You will need the Study ID and password provided below to access the web interview. Be assured that all of your answers will be kept confidential and will be protected to the fullest extent allowable under law. Please do not hesitate to contact me directly at 1-877-225-8470 (toll-free) or by e-mail at jennifer@rti.org if I can provide any additional information about the study or your interview.   ............................................................................. "}, {"section_title": "KAFSTMY", "text": ""}, {"section_title": "Date attended first school", "text": "In what month and year did you first attend any college, university, or trade school after high school? Applies to: Base-year nonrespondents or (base-year respondents with questionable BPS eligibility from the base year) whose first school was not [NPSAS]. Instrument code: (Y_BASEYR = 0 and KANFST = 0) or (Y_BASEYR = 1 and Y_FTBQST = 1 and KAQST = 0) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Sources: BPS:04/06 full-scale student interview"}, {"section_title": "KADGBMY", "text": ""}, {"section_title": "Date first began [NPSAS] ever", "text": "In what month and year did you first attend [NPSAS] after completing high school requirements? Applies to: Base-year nonrespondents or (base-year respondents with questionable BPS eligibility from the base year whose first school was not [NPSAS]). Instrument code: Y_BASEYR = 0 or (Y_BASEYR = 1 and Y_FTBQST = 1 and KAQST = 0) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Sources: BPS:04/06 full-scale student interview"}, {"section_title": "KANPSTMY", "text": ""}, {"section_title": "Date started at [NPSAS] during the NPSAS year", "text": "In what month and year did you first attend [NPSAS] during the 2003-2004 school year (that is, between July 1, 2003 and June 30, 2004)? Applies to: Base-year nonrespondents or base-year respondents with questionable BPS eligibility from the base year who first attended [NPSAS]  1 = First year or freshman 2 = Second year or sophomore 3 = Third year or junior 4 = Fourth year or senior 5 = Fifth year or higher undergraduate 6 = Unclassified undergraduate 7 = Graduate student taking undergraduate classes Applies to: Base-year nonrespondents who were working on a degree during the 2003-2004 school year. Instrument code: Y_BASEYR = 0 and KATYPE ne 7 Recode note: IF KAUGYR < 1 and KATYPE = 1 then KAUGYR = 6 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Sources: BPS:04/06 full-scale student interview   else TAGE = 21; If TAGE = -9, this sample member did not make it through the instrument far enough for TAGE to be calculated. Usually, SUMSTFLG will be a -9 when TAGE = -9. Applies to: All respondents. Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Sources: BPS:04/06 full-scale student interview NPSAS:04 full-scale student interview "}, {"section_title": "KBEN301, KBEN401, KBEN501, KBEN601", "text": "Intensity    "}, {"section_title": "KBLEVL01-07", "text": "school 1 level (Note: The specs for this variable are the same throughout each additional loop. There was a total of 7 possible loops.) school 1 level 1 = 4-year 2 = 2-year 3 = Less-than-2-year 4 = Some other type of school Applies to: Respondents who were enrolled at [  Initially set to TCURENR = 0 If PRIMSCH = 1 then TCRSCH* = TCURENR Note: Some partial completes did not make it far enough in the interview for TCURENR to be determined, so their value of TCURENR will be a -9 by default. 0 = Not currently enrolled 1 = Currently enrolled Applies to: All respondents. Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Instrument code: (KBNEWEN = 1 or (KBNEWEN in (-9 0 2) and KBANY = 1)) and (DEGSTAT in (1 4) or (DEGSTAT in (2 3) and DEGTYPE in (-3 -9 3))) Recode note: If DEGSTAT in (2 3) (expect degree or already earned) and DEGTYPE = 3 THEN KCDGAS = 1. Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Sources: BPS:04/06 full-scale student interview Instrument code: (KBNEWEN = 1 or (KBNEWEN in (-9 0 2) and KBANY = 1)) and (DEGSTAT in (1 4) or (DEGSTAT in (2 3) and DEGTYPE in (-3 -9 2))) Recode note: If DEGSTAT in (2 3) (expect degree or already earned) and DEGTYPE = 2 THEN KCDGCE = 1. Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Sources: BPS:04/06 full-scale student interview         and -were not working on a professional degree or -were working on a specific professional degree in \"chiropractic, dentistry, law, medicine, optometry, osteopathic medicine, pharmacy, podiatry, theology, or veterinary medicine\" or -were working on professional degrees and no other degree(s) and did not describe their specific professional degree as \"chiropractic, dentistry, law, medicine, optometry, osteopathic medicine, pharmacy, podiatry, theology, or veterinary medicine.\" Instrument code: (KBNEWEN = 1 or (KBNEWEN in (-9 0 2) and KBANY = 1)) and (KCDGPR ne 1 or (KCDGPR = 1 and KCDGPRT not in (28 29)) or (KCDGPR = 1 and KCDGPRT in (28, 29) and (KCDGBA ne 1 and KCDG5BA ne 1 and KCDGAS ne 1 and KCDGCE ne 1 and KCDGUND ne 1 and KCDGDR ne 1 and KCDGPB ne 1 and KCDGMA ne 1 and KCDGPM ne 1 and KCDGGNG ne 1))) Recode note: 1) If KBERN* = 2 then KCDBLMAJ = 0. 2) If (KCDGUND = 1 and KCDGBA=0 and KCDGPM= 0 and KCDGAS = 0 and KCDGCE = 0 and KCDGPR = 0 and KCDGMA = 0 and KCDGDR =0 and KCDGPB = 0 and KCDG5BA = 0) or (KCDGGNG = 1 and KCDGBA=0 and KCDGPM= 0 and KCDGAS = 0 and KCDGCE = 0 and KCDGPR = 0 and KCDGMA = 0 and KCDGDR =0 and KCDGPB = 0 and KCDG5BA = 0) then KCDBLMAJ = 0) If KCDGPR = 1 and 19<=KCDGPRT<=27 then KCDBLMAJ = 1. and did not describe their specific professional degree as \"chiropractic, dentistry, law, medicine, optometry, osteopathic medicine, pharmacy, podiatry, theology, or veterinary medicine\")) Instrument code: ((KBNEWEN = 1 or (KBNEWEN in (-9 0 2) and KBANY = 1)) AND KCDGPR ne 1) OR (((KBNEWEN = 1 or (KBNEWEN in (-9 0 2) and KBANY = 1)) AND KCDBLMAJ in (-9 0 1 2 3) AND KCDGPR = 1 and KCDGPRT in (28 29) and (KCDGBA ne 1 or KCDGPM ne 1 or KCDGAS ne 1 or KCDGCE ne 1 or KCDGMA ne 1 or KCDGDR ne 1 or KCDGPB ne 1 or KCDG5BA ne 1 or KCDGUND ne 1 orKCDGGNG ne 1))) Recode note: 1) If KCDBLMAJ = 0 then KCMAJ1A = \"NOT IN A DEGREE PROGRAM\" 2) If KCDBLMAJ = 3 then KCMAJ1A = \"UNDECLARED\" Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Sources: BPS:04/06 full-scale student interview  and humanities Applies to: Respondents who have been enrolled at some point since the 2003-2004 school year and (not working on a professional degree or (who are not working on professional degrees and another degree and did not describe their specific professional degree as \"chiropractic, dentistry, law, medicine, optometry, osteopathic medicine, pharmacy, podiatry, theology, or veterinary medicine\")) INSTRUMENT CODE: ((KBNEWEN = 1 or (KBNEWEN in (-9 0 2) and KBANY = 1)) AND KCDGPR ne 1) OR (((KBNEWEN = 1 or (KBNEWEN in (-9 0 2) and KBANY = 1)) AND KCDBLMAJ in (-9 0 1 2 3) AND KCDGPR = 1 and KCDGPRT in (28 29) and (KCDGBA ne 1 or KCDGPM ne 1 or KCDGAS ne 1 or KCDGCE ne 1 or KCDGMA ne 1 or KCDGDR ne 1 or KCDGPB ne 1 or KCDG5BA ne 1 or KCDGUND ne 1 or KCDGGNG ne 1))) Recode note: 1) If KCDBLMAJ = 0 then KCMJ1C4 = 98. 2) If KCDBLMAJ = 3 then KCMJ1C4 = 99 Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Sources: BPS:04/06 full-scale student interview    worked while enrolled and -declared at least one major or were working on professional degrees and another degree(s) and did not describe their specific professional degree as \"chiropractic, dentistry, law, medicine, optometry, osteopathic medicine, pharmacy, podiatry, theology, or veterinary medicine.\" Instrument code: (KBNEWEN = 1 or (KBNEWEN in (-9 0 2) and KBANY = 1)) and KCNUMJOB > 0 and (KCDBLMAJ in (1 2) or (KCDBLMAJ = -3 and KCDGPR = 1)) Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Sources: BPS:04/06 full-scale student interview    to respondents who were not currently enrolled, as well as additional conditions that specifically apply to each variable. Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Sources: BPS:04/06 full-scale student interview"}, {"section_title": "KCMAJ2A", "text": ""}, {"section_title": "KDOC2C2", "text": "[If COMPMODE = 0] Please click on the entry in the list below that most closely describes [KDJBTL]. If an appropriate entry does not appear in the list, you may search again by changing the keywords in one of the textboxes above, and clicking on one of the \"Search\" buttons. If you are still unable to find your job by searching, click on the \"None of these\" button at the bottom of the screen. Instrument code: TCURENR = 0 and FUTENR = 0 and KDJSTAT = 1 Note: Prior to June 1, all variables in section D applied to respondents who were not currently enrolled, as well as additional conditions that specifically apply to each variable. Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Sources: BPS:04/06 full-scale student interview"}, {"section_title": "KDOC2C3", "text": "[If COMPMODE = 0] Please click on the entry in the list below that most closely describes [KDJBTL]. If an appropriate entry does not appear in the list, you may search again by changing the keywords in one of the textboxes above, and clicking on one of the \"Search\" buttons. If you are still unable to find your job by searching, click on the \"None of these\" button at the bottom of the screen. Note: Prior to June 1, all variables in section D applied to respondents who were not currently enrolled, as well as additional conditions that specifically apply to each variable. Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Sources: BPS:04/06 full-scale student interview"}, {"section_title": "KDOC2C6", "text": "[If COMPMODE = 0] Please click on the entry in the list below that most closely describes [KDJBTL]. If an appropriate entry does not appear in the list, you may search again by changing the keywords in one of the textboxes above, and clicking on one of the \"Search\" buttons. If you are still unable to find your job by searching, click on the \"None of these\" button at the bottom of the screen.    Instrument code: TCURENR = 0 and FUTENR = 0 and KDJSTAT = 1 and KCDBLMAJ in (1 2) and (KDSIMJOB ne 2 or (KDSIMJOB = 2 and KCNUMJOB ne (0 -3)) Recode note: If KDSIMJOB = 2 then KDRELMAJ = KCRELMAJ. Note: Prior to June 1, all variables in section D applied to respondents who were not currently enrolled, as well as additional conditions that specifically apply to each variable. Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Sources: BPS:04/06 full-scale student interview Instrument code: TCURENR = 0 and FUTENR = 0 and KDJSTAT = 1 Note: Prior to June 1, all variables in section D applied to respondents who were not currently enrolled, as well as additional conditions that specifically apply to each variable. Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Sources: BPS:04/06 full-scale student interview"}, {"section_title": "KDLICT4", "text": "Type of license/certificate required for job -specific Which type of license or certificate would be considered a minimum requirement for your current position? (Coding Directions: Please select a general area and then the specific discipline within the general area. Use the arrow at the right side of the first box to display the general areas. Click to select the desired general area, and then select the desired specific discipline within the area from the second box.      "}, {"section_title": "KEDISSEN", "text": "Disability: long-lasting sensory condition The next few questions will help us better understand the educational services available for people with disabilities. Do you have a sensory impairment, such as blindness, deafness, or a severe vision or hearing impairment, that has lasted for 6 months or more? 0 = No 1 = Yes Applies to: All respondents. Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Sources: BPS:04/06 full-scale student interview KEDISMOB Disability: condition that limits physical activities Do you have a mobility impairment that has substantially limited one or more basic physical activities, such as walking, climbing stairs, reaching, lifting, or carrying, for 6 months or more? 0 = No 1 = Yes Applies to: All respondents. Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Sources: BPS:04/06 full-scale student interview KEDISOTH Disability: other condition lasting six months or more Excluding any disabilities already mentioned, do you have any other physical, mental, emotional, or learning condition that has lasted 6 months or more? (By this we mean any condition that limits your ability to learn, remember, or concentrate; to dress, bathe, or get around the house; or to get to school, around campus, or to work.) 0 = No 1 = Yes Applies to: All respondents. Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Sources: BPS:04/06 full-scale student interview"}, {"section_title": "KEHIGHED", "text": "Highest education level expected Now we'd like to ask a few questions about your educational and personal goals. What is the highest level of education you ever expect to complete? 0 = No degree or certificate expected 2 = Undergraduate certificate or diploma (occupational or technical program) 3 = Associate's degree 4 = Bachelor's degree 6 = Post-baccalaureate certificate or program 8 = Master's degree 9 = Post-master's certificate 10 = Professional degree (only includes the following programs: chiropractic, dentistry, law, medicine, optometry, osteopathic medicine, pharmacy, podiatry, divinity, theology, or veterinary medicine) 11 = Doctoral degree Applies to: All respondents. Perturbation procedures were applied to this and other variables to protect against disclosure of individual information. Sources: BPS:04/06 full-scale student interview                                 "}]