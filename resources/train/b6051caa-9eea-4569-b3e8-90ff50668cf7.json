[{"section_title": "Abstract", "text": "This study investigates the interplay between general cognitive abilities and domain-specific mathematical ability by applying three different models. The first model is a g factor model, the second model reflects two correlated factors, and the third model is a nested-factor model that comprises a g factor, a domain-specific mathematical factor and cognitive ability. Furthermore, we analyzed the relation between student characteristics such as self-concept in mathematics, gender, SES, and grade in mathematics. Using data from six different German LSAs of the three cohorts, Grade 5, 9 and 13 (sample sizes range from N Grade5 = 730 to N Grade9 = 3893), and two different frameworks (i.e., literacy and curricular), we confirmed that LSAs do test mathematical ability beyond g. The two cognitive factors correlated differently with the student characteristics considered; this calls for future longitudinal approaches to the issue."}, {"section_title": "Introduction", "text": "The aim of international large-scale assessments (LSAs) such as the Programme for International Student Assessment (PISA; Organisation for Economic Co-operation and Development [OECD], 2016) or the Trends in International Mathematics and Science Study (TIMSS; Martin, Mullis, Foy, & Stanco, 2012; Mullis, Martin, Foy, & Hooper, 2016) is to provide information about \"educational contexts for learning\" (Martin, Mullis, Gonzalez, & Chrostowski, 2004, p. 3) in various domains. The increasing impact of LSAs and their relevance emphasize the importance of high-quality standardized measurement instruments for educational research and monitoring. The results of LSAs should offer valuable information about what students have learned at school. In the context of educational monitoring, the interpretation of results from LSAs is based on the assumption that LSA results mainly reflect domainspecific abilities. Recently, LSA instruments were criticized for mostly measuring a general cognitive ability instead of a domain-specific ability (Frey & Detterman, 2004; Koenig, Frey, & Detterman, 2008; Rindermann, 2006) . Some studies already showed that LSAs measure the interplay of general cognitive ability and domain-specific abilities (Baumert, L\u00fcdtke, Trautwein, & Brunner, 2009; Brunner, 2008; Winkelmann, Robitzsch, Stanat, & K\u00f6ller, 2012) . These studies demonstrated that, although both abilities share variance among students, mathematical ability and general cognitive ability are empirically separate constructs. Recent studies have made a first attempt to describe these separate constructs by including covariates in the models (Brunner, 2008) . We broaden the initial approach of these studies by answering two emerging questions. First, do LSAs succeed in measuring domain-specific abilities across different grades, and does the interplay between domain-specific abilities and general cognitive ability differ between literacy and curricular tests assessing mathematical ability? Second, are there differential relations between domain-specific abilities and student characteristics on the one hand and general cognitive ability and student characteristics on the other hand? We answer these questions by analyzing the interplay of general cognitive ability and mathematical ability as well as their relations to a wide variety of student characteristics in a multicohort study for multiple age groups and for the two prevalent LSA frameworks: curricular and literacy."}, {"section_title": "Mathematical ability -the curricular and the literacy approach", "text": "In LSAs, two well-known strands concerning the conceptualization of mathematical ability subsist. Frameworks for LSAs either take a literacy-oriented or a curriculum-oriented approach. Curricular tests focus on the content and abilities defined in national curricula, which are explicitly taught at school. These tests cover facts and concepts (e.g., numbers and algebra, measurement and geometry, as well as statistics and probability), which lay the foundation for developing useful strategies for mathematical problem solving (e.g., Assessment and Qualifications Alliance [AQA], 2012; National Council for Teachers of Mathematics [NCTM], 2000) . On the other hand, the tests also cover (cognitive) processes such as problem solving and mathematical reasoning and proving (e.g., Educational Excellence [Edecxel], 2012; Ontario Math Curriculum, 2005) .\nLiteracy tests focus on the skills that need to be acquired in order to meet the demands of modern societies outside the classroom. Taking the example of PISA, mathematical literacy is defined as \"an individual's capacity to formulate, employ, and interpret mathematics in a variety of contexts. It includes reasoning mathematically and using mathematical concepts, procedures, facts, and tools to describe, explain, and predict phenomena.\" (OECD, 2013a, p. 25) . Literacy tests cover different mathematical concepts and aspects. PISA, for example, covers four overarching ideas: quantity, space and shape, change and relationships, and uncertainty (OECD, 2013a).\nThe main difference between the frameworks of the literacy and curricular test instruments is the content covered, whereas the cognitive processes required to solve mathematics items are mostly identical (Neidorf, Binkley, Gattis, & Nohara, 2006; Wu, 2010) . Despite this disparity, the distinction between the two frameworks is often viewed critically because of the high congruities concerning conceptual and empirical issues (Klieme, Neubrand, & L\u00fcdtke, 2001 ). Specifically, Klieme et al. (2001) were able to show that correlations between mathematical ability scores in a literacy test and mathematical ability scores in a curricular test were rather high (r = .91) and, therefore, they questioned the independence of the two conceptual frameworks. Reasons for the high, but not perfect, correlation can be inferred from our description of both test types: In order to solve mathematical problems in both test types, students need to apply the same underlying cognitive processes, such as inductive reasoning. Our investigation of both test types will shed light on whether the two test types measure mathematical ability in different ways. Both frameworks are very common in LSA. Due to the high conceptual congruence between curricular and literacy frameworks, we assume both to have rather identical interplays of cognitive and math-specific factors."}, {"section_title": "Mathematical ability and general cognitive abilities", "text": "As mentioned in the framework description above, it is essential to use inductive and deductive reasoning processes to understand mathematical relations, draw conclusions, and apply mathematical knowledge. Reasoning ability is therefore strongly related to mathematical ability (e.g., Floyd, Evans, & McGrew, 2003; Fuchs et al., 2006) . Reasoning ability is a major part of fluid intelligence (Gf), which is defined as the use of mental operations to solve novel and complex problems (i.e., tasks that cannot be solved through learning experience and acculturation; Rost, 2009).\nHow both cognitive abilities and mathematical abilities influence or more specifically predict the measurement of mathematical abilities in LSAs has extensively been researched (e.g., Floyd et al., 2003; Taub, Floyd, Keith, & McGrew, 2008) . A synthesis of educational research and intelligence research leads to three conceptions of the interplay between cognitive abilities and mathematical abilities.\nThe first assumption is based on the existence of a general cognitive ability (g), ascribable to Spearman (1904) , who proposed that an underlying mental ability is necessary for successful performance across domains. Some researchers argue that a standardized assessment of abilities (not only in mathematics) does not succeed in representing a multidimensional structure of ability that differs from g (e.g., Cole, 1995; Rindermann, 2007) . They even state that achievement and intelligence are \"theoretically and statistically inseparable constructs\" (Ceci, 1991, p. 4) . One study promoting the g viewpoint was based on re-analyses of the PISA 2000 data and revealed high latent correlations between test scores in mathematics, reading, and science (above .84), as well as between the same test scores and problem solving (above .89 for PISA 2000 with PISA 2003 Rindermann, 2006) . Rindermann (2006) concludes that all scales, namely, science, mathematics, and reading as well as problem solving, measure the same overarching construct: A g factor. Factor analyses including these four constructs revealed a first unrotated factor that explained 95% of the variance for different ability domains and intelligence tests. In a different study intelligence and educational achievement data from 86 countries were correlated (Lynn & Meisenberg, 2010) . After correcting measurement errors, the correlation was 1.0, again suggesting that intelligence and educational achievement data reflect the same construct.\nThe second assumption relies on the Gf-Gc theory (Cattell, 1963) which assumes several inter-correlated first-order factors of cognitive abilities. A lot of studies were conducted in order to examine the impact of fluid intelligence and crystallized intelligence on academic abilities (e.g., Floyd et al., 2003; Fuchs et al., 2006; Williams, McCallum, & Reed, 1996) . However, in educational research the predominant assumption is that there are two correlating abilities, namely, a domain-general ability (fluid intelligence) and a domain-specific ability (e.g., mathematical ability as crystallized intelligence). Brunner (2008) introduced this model as the standard model in educational research.\nThe third assumption is based on recent intelligence research and assumes a hierarchical structure between cognitive factors on various levels with a common factor g and specific cognitive abilities on lower levels (Carroll, 1993; Gustafsson, 1984; McGrew, 2005) . Numerous studies demonstrated that g is the most important predictor of general academic achievement (e.g., Deary, Strand, Smith, & Fernandez, 2007; Laidra, Pullmann, & Allik, 2007; Kaufman, Reynolds, Liu, Kaufman, & McGrew, 2012; Rhode & Thompson, 2007 . However, in a study that regressed observed course grades on latent aptitude (cognitive and domain-specific ability factors) variables, a general cognitive factor g explained between 12% and 41% of the variance in school grades (Gustafsson & Balke, 1993) . Using an additional mathematical achievement as a predictor explained 10% of the variance in the mathematics school grade (Gustafsson & Balke, 1993) . Several recent studies referred to the approach of Gustafsson and Balke (1993) and used nested-factor models (also known as bi-factor models)\n1 to analyze the interplay of domain-specific abilities and general cognitive ability (e.g., Brunner, 2006; K\u00f6ller, Reiss, Stanat, & Pant, 2012; Winkelmann et al., 2012) . The advantage of a bi-factor model is that it can reflect the structure of cognitive abilities (in line with the Three-Stratum theory; Carroll, 1993) by incorporating a number of factors that exist beyond a g factor. Thereby, a bi-factor model assumes that ability measures in LSAs are influenced both by a domain-general factor and a domain-specific factor (which is independent of g). Brunner (2008) and Winkelmann et al. (2012) investigated the relationship between general cognitive ability (g) and domain-specific abilities by applying several confirmatory factor analyses (CFA) incorporating mathematical ability, general cognitive ability, verbal ability, and fluid intelligence in elementary school and at the end of high school. A bi-factor model comprising a general cognitive factor as well as a math-specific ability factor showed the best model fit for both age groups. Results confirmed that mathematical achievement can be explained by a general cognitive component and a domain-specific component. Hence, the nested-factor structure could be shown for at least two age-differentiated subsamples and for a literacy and a curricular test."}, {"section_title": "Relation of student characteristics and cognitive abilities", "text": "Researchers in education are sometimes criticized for ignoring the conceptualization of intelligence (e.g., Adey, Csap\u00f3, Demetriou, Hautam\u00e4ki, & Shayer, 2007) . However, a lot of studies have related specific student characteristics to cognitive abilities in the past, namely: socio-economic status (SES), school grades, domain-specific self-concept, and gender. Some studies focused on group differences in cognitive abilities (e.g., Keith, Reynolds, Patel, & Ridley, 2008; Ros\u00e9n, 1995) ; other studies targeted individual characteristics as covariates in order to expand the research on and assessment of cognitive abilities (e.g., Reynolds, Keith, Flanagan, & Alfonso, 2013) . We relied on two"}]