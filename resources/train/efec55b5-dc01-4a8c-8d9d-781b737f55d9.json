[{"section_title": "Abstract", "text": "Bio-imaging technologies allow scientists to collect large amounts of high-dimensional data from multiple 24 heterogeneous sources for many biomedical applications. In the study of Alzheimer's Disease (AD), neuroimaging 25 data, gene/protein expression data, etc., are often analyzed together to improve predictive power. Joint learning 26 from multiple complementary data sources is advantageous, but feature-pruning and data source selections are 27 critical to learn interpretable models from high-dimensional data. Often, the data collected has block-wise missing 28 entries. In the Alzheimer's Disease Neuroimaging Initiative (ADNI), most subjects have MRI and genetic informa-29 tion, but only half have cerebrospinal fluid (CSF) measures, a different half has FDG-PET; only some have proteomic 30 data. Here we propose how to effectively integrate information from multiple heterogeneous data sources when 31 data is block-wise missing. We present a unified \"bi-level\" learning model for complete multi-source data, and 32 extend it to incomplete data. Our major contributions are: (1) our proposed models unify feature-level and 33 source-level analysis, including several existing feature learning approaches as special cases; (2) the model for in-34 complete data avoids imputing missing data and offers superior performance; it generalizes to other applications 35 with block-wise missing data sources; (3) we present efficient optimization algorithms for modeling complete and 36 incomplete data. We comprehensively evaluate the proposed models including all ADNI subjects with at least 37 one of four data types at baseline: MRI, FDG-PET, CSF and proteomics. YNIMG-10752; No. of pages: 15; 4C: 3, 5, 8, 11, 12, 13, 14 \u204e "}, {"section_title": "", "text": "Multi-source learning is closely related to an area known as \"multi- "}, {"section_title": "134", "text": "In this paper, we will use the term \"bi-level analysis\", which was intro-135 duced in (Breheny and Huang, 2009) , to refer to feature-and source-136 level analysis, performed simultaneously."}, {"section_title": "137", "text": "Besides the multi-modality aspects and the high dimensionality of 138 the data, a further problem is very commonly encountered: the existence 139 of (block-wise) missing data is another major challenge encountered in 140 AD and other biomedical applications. Fig. 1 in all groups. This makes it difficult to do \"out-of-sample\" prediction, i.e., when the testing data involves a different data source combination 188 from the training data."}, {"section_title": "189", "text": "In this paper, we propose a novel bi-level learning model, which We also evaluate the effectiveness of the proposed models, "}, {"section_title": "Subjects", "text": ""}, {"section_title": "223", "text": "We use data from the Alzheimer's disease Neuroimaging Initiative In this paper, we use four types of data sources, e.g., MRI, PET, CSF,"}, {"section_title": "249", "text": "and proteomics, including a total of 780 subjects (i.e., anyone who had 250 at least one of these measures at baseline are shown in Table 1 ."}, {"section_title": "281", "text": "A unified feature learning model for multi-source complete data"}, {"section_title": "282", "text": "We first present a unified learning model for multi-source data with- "}, {"section_title": "286", "text": "Assume we are given a collection of m samples from S data sources:\n287 288 where X i \u2208 R m\u00c2p i is the data matrix of the ith source with each sample 289 being a p i -dimensional vector, and y is the corresponding outcome for 290 each sample. We consider the following linear model:\n291 292 where each column of X is normalized to be zero mean and to have a stan-293 dard deviation of 1, and represents the noise term. \u03b2 is the underlying 294 true model, and is usually unknown in real-world applications. Based on 295 (X,y), we want to learn an estimator of \u03b2, denoted as b \u03b2 , whose 296 non-zero elements F \u00bc j : b \u03b2 j \u2260 0 n o correspond to the relevant fea-297 tures. In other words, features corresponding to the zero elements of 298 b \u03b2 are discarded. We consider the following regularization framework:\n299 300 where L(\u00b7) represents the data-fitting term and \u03a9(\u00b7) is the regulariza-301 tion term which encodes our prior knowledge about \u03b2. Specifically, 302 the choice of \u03a9(\u00b7) should also enable us to perform both feature-level 303 and source-level analysis simultaneously. Towards this end, a natural 304 approach is a two-stage model. First we learn different models for 305 each data source and then combine these learned models appropriately. 306 The regularization should be imposed independently on each stage, to 307 provide a bi-level analysis. We formalize our intuition as follows:\nwhere the minimization is taken with respect to (\u03b1,\u03b3) jointly. According 310 to the intuition above, \u03b1 i denotes the model learned using the ith data 311 source and \u03b3 is the weight that combines those learned models together."}, {"section_title": "312", "text": "The regularization is taken independently over \u03b1 and \u03b3 and therefore \u03b3 i with \u03b2 ip \u03b1 ip , we can obtain an equivalent formulation:"}, {"section_title": "329 330", "text": "Taking the partial derivative with respect to \u03b1 i , and setting it to zero, 331 leads to: Assigning different values to p and q leads to various kinds of regulariza-339 tion and feature learning models. Next, we show several widely-used 340 convex models are actually special cases of our model."}, {"section_title": "341", "text": "Let p = 1 and q = \u221e. In this case, the regularization term in Eq. (3) 342 becomes the ' 1 -regularization, and the resulting model becomes Lasso 343 (Tibshirani, 1996) : \nOn the other hand, if both p and q are equal to 2, then the ' 2 - "}, {"section_title": "354 355", "text": "Similarly, if p = \u221e and q = 1, we obtain the ' 1;\u221e -regularization 356 model (Quattoni et al., 2009; Turlach et al., 2005) , which penalizes the 357 largest elements of \u03b2 i for each source:"}, {"section_title": "359 360", "text": "Besides these common convex formulations, our general model also 361 includes a family of non-convex formulations, which have not been fully 362 explored in the literature. In particular, setting p = 1 and q = 2 leads to 363 the following non-convex model:\n364 365 and if p = 2 and q = 1, model (3) reduces to:\n366 367 368\nFor the convex models such as lasso, both the optimization algo- However, due to the non-convexity nature, it is more difficult to com-372 pute the optimal solution of models (9) and (10). We present a differ- to the presence of missing data blocks. To address this, we partition the 408 whole data set into multiple groups according to the availability of data Fig. 2 . Illustration of the proposed learning model. The data set is partitioned into four groups according to the availability of data sources, as highlighted by the red boxes. The goal is to learn three models \u03b2 1 , \u03b2 2 and \u03b2 3 for each data source as well as the coefficient \u03b1 that combines them. Notice that, for the ith data source, \u03b2 i remains identical while \u03b1 may vary across different groups.\nsources, as illustrated in the red boxes in Fig. 2 Mathematically, the proposed model solves the following formulation:\n463 464 where "}, {"section_title": "490", "text": "Computing \u03b1 when \u03b2 is fixed"}, {"section_title": "491", "text": "As shown in Fig. 2 , we learn the weight \u03b1 for each source combination "}, {"section_title": "502", "text": "Computing \u03b2 when \u03b1 is fixed"}, {"section_title": "503", "text": "When we keep \u03b1 fixed and seek the optimal \u03b2, Eq. (11) becomes an 504 unconstrained regularization problem: can be computed efficiently. Indeed, this is the case for many widely 515 used regularization terms. In addition, in order to apply standard first-\n516 order lasso solvers, we only need to provide the gradient of \u03b2 at any 517 given point without knowing the explicit quadratic form. For each data 518 source i, we can compute the gradient of the g(\u03b2) w.r.t. \u03b2 i as follows: the ' 1 -norm penalty to achieve the simultaneous feature-and source-542 level selection. "}, {"section_title": "Results", "text": ""}, {"section_title": "562", "text": "To examine the efficacy of the proposed bi-level feature learning 563 models, we report the performance of the proposed models for com-564 plete and block-wise missing data, based on both synthetic data and 565 ADNI data. Specifically, the following aspects are evaluated: (i) models 566 (9) and (10) for complete data; (ii) model (11) for block-wise missing 567 data; (iii) the capability of source-level analysis; (iv) the benefit of uti-568 lizing incomplete data; and (v) model ensemble."}, {"section_title": "569", "text": "Comparison on complete data 570 We first evaluate the effectiveness of the complete models (9) and "}, {"section_title": "584", "text": "For each scenario, we partition the data set into a disjoint training set 585 and test set, and we compare models (9) and (10) others. In addition, in both cases, models (9) and (10) 633 However, their results are not stable, as revealed by the low sensitivity/ 634 specificity value in some tasks; (2) the ScoreComp methods deliver 635 superior performances in the classification of AD patients and normal 636 controls; (3) the feature learning models, such as iSFS and iMSF, 637 outperform the imputation methods and often achieve uniform im-638 provement across all the measurements. This coincides with our in-639 tuition that estimating the missing blocks directly is usually difficult 640 and unstable and approaches avoiding imputation are preferred. In 641 particular, iSFS clearly delivers the best performance among all 642 approaches. We can also observe from the results that when 10% of 643 the data is used for training, iSFS consistently outperforms iMSF. 644 However, iSFS and iMSF achieve comparable performance when 50% of 645 the data is used for training. This is consistent with our analysis in the 646 Incomplete Source-Feature Selection (ISFS) model section, in which we 647 show that the iSFS formulation can be considered as a constrained ver-648 sion of iMSF and it involves a much smaller number of model parameters 649 than iMSF. Thus, iSFS is expected to outperform iMSF especially when 650 the number of samples in the training set is small (Fig. 4) "}, {"section_title": "Q12", "text": "."}, {"section_title": "651", "text": "Capability of source selection Performance on complete, synthetic data. The MSE denotes the mean squared error of prediction on the test set, and Esti stands for the parameter estimation error. For the scenario of sparse t2:3 features, the underlying true model has 6 groups and 18 features, while for the situation of non-sparse features, the true model takes 6 groups and 60 features. All results are averaged over 10 t2:4 replications. \nweights for each algorithm, in the second approach. Therefore the 687 final prediction is based on a weighted-combination of the results"}, {"section_title": "688", "text": "obtained from each individual algorithm. Specifically, we include 689 two imputation models: mean-value imputation and KNN. In addition,"}, {"section_title": "690", "text": "for each of iMSF and iSFS, we select two parameters (0.001, 0.01),"}, {"section_title": "691", "text": "which results in 6 models in total. Fig. 7 \nSince \u03bd i and b t i are nonnegative, all constraints in Eq. (18) must be 807 active at the optimal points. Thus, Eq. (18) is equivalent to the following 808 group lasso problem:\n809 810 811\nAfter \u03b2 is obtained, we update b t i with \u03b2 i2 and continue the iteration 812 until convergence. Notice that b t i \u2212 1 3 can be very large if \u03b2 i2 is small. For nu-813 merical stability, we add a smoothing term \u03b8 to each b t i as suggested by 814 Remark 7. Model (9) can be solved in exactly the same way as above."}, {"section_title": "821", "text": "The only difference is that in each iteration we need to solve a weighted which is a less challenging problem. We thus denote this two-stage 874 scheme as the model score completion method (ScoreComp). . ROC curves given by iSFS (on both complete and incomplete data), lasso and group lasso. Except for iSFS, the classification is carried out on a subset of the ADNI data set, where all the samples have four data sources available. For iSFS, it is evaluated on the whole incomplete data set.\nFor notation simplicity, denote the set sc(s) \u2282 {1,2,\u2026,m} as the model M is learned using (A,y) so that the sources are integrated. We then combine b B with the previously obtained imputed matrix e A 904 such that missing data imputation is performed:\n! :"}, {"section_title": "906 907", "text": "Finally, by extracting the lower part of matrix C, we can obtain the 908 derived feature matrix e B for the unlabeled data set. We then apply the 909 final model M to e B to obtain the prediction of the unlabeled set. Fig. 10 . Illustration of the two-stage scheme for block-wise incomplete data. We first train a base model on each individual data source using the available samples, and the base model is applied to produce prediction scores for this data source; thus each data source is represented by a single column of (incomplete) scores. A missing value estimation method is applied to obtain a complete set of model scores, which are treated as newly derived features to train our final classifier.\nLanckriet, G.R.G., De Bie, T., Cristianini, N., Jordan, M.I., Noble, W.S., 2004. A statistical"}]