[{"section_title": "Abstract", "text": "Abstract. We present an algorithm for creating high resolution anatomically plausible images consistent with acquired clinical brain MRI scans with large inter-slice spacing. Although large databases of clinical images contain a wealth of information, medical acquisition constraints result in sparse scans that miss much of the anatomy. These characteristics often render computational analysis impractical as standard processing algorithms tend to fail when applied to such images. Highly specialized or application-specific algorithms that explicitly handle sparse slice spacing do not generalize well across problem domains. In contrast, our goal is to enable application of existing algorithms that were originally developed for high resolution research scans to significantly undersampled scans. We introduce a model that captures fine-scale anatomical similarity across subjects in clinical image collections and use it to fill in the missing data in scans with large slice spacing. Our experimental results demonstrate that the proposed method outperforms current upsampling methods and promises to facilitate subsequent analysis not previously possible with scans of this quality."}, {"section_title": "Introduction", "text": "Increasingly open acquisition efforts in clinical practice are driving dramatic increases in the number and size of patient cohorts in clinical archives. Unfortunately, clinical volumes are typically of dramatically lower resolution than the research scans that motivate most methodological development. Specifically, while individual slices in the scan can be of high resolution, slice spacing is often significantly larger, resulting in missing voxels, as illustrated in Fig. 1 . This presents significant challenges for even basic tasks, such as skull stripping and registration, which are often necessary for downstream analysis [4, 9] . We present a novel method for constructing high resolution anatomically plausible volumetric images consistent with the available slices in sparsely sampled clinical scans. The restored images promise to enable computational analysis of clinical scans with existing techniques originally developed for isotropic research scans. Importantly, our method does not require any high resolution scans or expert annotations, but instead imputes the missing structure by learning from the available collection of clinical scans.\nOur work is motivated by a study that includes brain MRI scans of thousands of stroke patients acquired within 48 h of stroke onset. The study aims to quantify white matter disease burden, necessitating skull stripping and deformable registration into a common coordinate frame [16] . The volumes are severely under-sampled (0.85 mm \u00d7 0.85 mm \u00d7 6 mm) due to clinical constraints of acute stroke care, as illustrated in Fig. 1 . Such undersampling is typical of modalities that aim to characterize tissue properties such as T2-FLAIR, even in research studies like ADNI2 [9] .\nSince clinically acquired scans violate many algorithms' underlying assumptions, even basic tasks present significant challenges [6, 20, 21] . In undersampled scans, the image is no longer smooth, and the anatomical structure may change substantially between consecutive slices (Fig. 1) . Application-specific algorithms promise to address these problems for certain clinical scans but do not generalize well across applications and imaging modalities. In contrast, we harness the data available in a given clinical image collection to reconstruct the high resolution images that represent plausible anatomy from the low resolution scans (Fig. 2 ). The resulting images can then be analyzed by widely used algorithms that require nearly isotropic high resolution input. The imputed data acts as a medium for improving analysis tasks. For example, although imputed data should not be used in the clinical evaluation, the brain mask obtained through skull stripping of the restored scans can be applied to the original clinical scan for further analysis.\nPrior Work. Traditional image restoration, or superresolution, techniques depend on having enough information in a single scan to synthesize data. Unfortunately, clinical slices are often sampled too sparsely for functional interpolation, such as linear, cubic or spline [19] , to succeed. Similarly, patch-based methods that rely on redundancy within a single scan to \"hallucinate\" missing fine scale structure [13] [14] [15] the same subject to improve a single scan [2, 10, 15] are unsuitable for clinical data where multiple similar acquisitions are not commonly available. Nonparametric upsampling methods proposed to tackle the problem of superresolution often rely on an external dataset of high resolution data or cannot handle extreme undersampling present in clinical scans. For example, some methods fill in missing data by matching a low resolution patch from the scan with a high resolution patch from the training dataset [3, 7, 10, 11, 18] . A recent approach to improve resolution from a collection of scans with sparse slices jointly upsamples all images using non-local means [17] . However this method has only been demonstrated on slice spacing of roughly three times the in-plane resolution, and in our experience similar non-parametric methods fail to upsample clinical scans with more significant undersampling.\nParametric methods and low dimensional embeddings have been used to model the common structure of image patches from full resolution images, but are typically not designed to handle missing data. Specifically, priors [22] and Gaussian Mixture Models [23] have been used in both medical and natural images for classification [1] and denoising [5, 23] . The procedures used for training these models rely on having full resolution patches with no missing data. Unfortunately, high (full) resolution training datasets are not readily available for many image contrasts and scanners, and may not adequately represent pathology or other properties of clinical populations. Acquiring the appropriate high resolution data is often infeasible, and here we explicitly focus on the realistic medical scenario where only low resolution image sets are available.\nOverview. Our method takes advantage of the fact that local fine scale structure is intrinsically shared in a population of medical images, and each scan with sparse slices captures some partial aspect of this structure. We borrow ideas from Gaussian Mixture Model (GMM) patch priors [23] , low dimensional Gaussian embeddings [8] , and missing data models [8, 12] to develop a probabilistic model that describes sparse 3D patches from all volumetric images in a collection around a particular location using a low-dimensional GMM with partial observations. We derive an iterative algorithm to learn the model parameters. We demonstrate our algorithm using scans from the ADNI cohort as well as the motivating stroke study, and also illustrate a preliminary illustration of potential improvements in the down-stream analysis using an example task of skull stripping. Finally, we discuss initialization tradeoffs and modelling choices."}, {"section_title": "Method", "text": "We employ a Gaussian Mixture Model (GMM) to capture local structure in sparse 3D patches in the vicinity of a particular location across the entire collection. We treat a patch as a high dimensional manifestation of a low dimensional representation, with the intuition that the covariation within image patches has small intrinsic dimensionality relative to the number of voxels in the patch. In this section, we describe the model, the resulting learning algorithm, and our image restoration procedure."}, {"section_title": "Generative Model", "text": "Let {Y 1 , . . . , Y N } be an image collection of scans with large slice separation, roughly aligned into a common atlas space (we use affine transformations in our experiments). For each image Y i in the collection, only a few slices are known, and we seek to restore the anatomically plausible high resolution volume by imputing the missing voxel values.\nWe capture local structure using image patches. We assume a constant patch shape, and use y i to denote a D-length vector that contains voxels of the image patch centered at a certain location of image Y i . We perform inference at each location independently. We model the set of patches Y = {y i } at a common location as drawn from a K-component multivariate GMM. If generated by cluster k, patch y i is a high dimensional observation of a low dimensional patch representation x i of length d:\nHere patch \u03bc k is the mean of cluster k, matrix W k shapes the covariance structure of the cluster, and \u03c3 2 k is the variance of image noise. The likelihood of all patches at this location under the mixture model is\nwhere \nwhere\nk comprises rows of W k that correspond to the observed voxel set O i . The likelihood of the observed data is therefore\nwhere\nextracts the rows and columns of C k that correspond to the observed voxel subset O i ."}, {"section_title": "Learning", "text": "We learn the maximum likelihood estimates of the parameters {\u03bc k }, {W k }, {\u03c3 2 k } and \u03c0 under the likelihood (4). As traditional Expectation Maximization for our model does not lead to closed form update equations, we employ the Expectation Conditional Maximization (ECM) [8, 12] variant of the Generalized Expectation Maximization, where parameter updates depend on the previous parameter estimates. Due to space limitations, we omit the derivations and provide the resulting updates along with their interpretations.\nThe expectation step updates the class memberships based on the observed voxels of the patches at this location:\nNext, we compute the statistics of the low dimensional representation x for each patch as \"explained\" by each cluster:\nThe maximization step uses the observed voxels to update the model parameters. We let P j be the set of patches in which voxel j is observed, y The covariance factors and residual variance are updated based on the statistics of low dimensional representation from (6) and (7):\nFinally, we update the cluster proportions:\nThroughout learning, we work in the atlas space, and approximate voxels as either observed or missing in this space by thresholding interpolation weights and ignoring interpolation effects due to affine alignment. Intuitively, learning such a model with sparse data is possible because each image patch provides a slightly different subset of voxel observations that contribute to the parameter estimation (Fig. 3) . The estimation can be extended to carry out the learning by appropriately transforming model parameters into the subject-specific space in order to optimally use the observed voxels, but this leads to computationally prohibitive updates.\nIn our experiments, all subject scans have the same acquisition direction. Despite different affine transformations to the atlas space for each subject, some voxel pairs are still never observed in the same patch, resulting in missing entries of the covariance. Representing covariance using a low-rank approximation helps to alleviate this lack of observations."}, {"section_title": "Restoration", "text": "To restore an individual patch y i , we first estimate the most likely cluster k for patch y i by selecting the cluster with the highest membership \u03b3 ik . We then estimate the low dimensional representation x i k given the observed voxels y O i i using (6) . Finally, we reconstruct the high resolution patch:\nusing the estimates of the model parameters \u03bc k and W k . We average overlapping restored patches using standard techniques and form the restored volume Z i [13] ."}, {"section_title": "Implementation", "text": "We stack together the affinely registered sparse images from the entire collection and split the stack into overlapping subvolumes of 18 \u00d7 18 \u00d7 18 voxels in the isotropically sampled common atlas space. Subvolumes are centered 9 voxels apart in each direction. Within each subvolume, we learn the mixture model parameters. Instead of selecting just one patch from each volume at a given location, we collect all overlapping patches within the subvolume centered at that location. This aggregation provides more data for each model, which is crucial when working with severely undersampled volumes. Moreover, it offers robustness in the face of image misalignment. Given the learned parameters at each location, we restore all overlapping patches within a subvolume. We use a cubic patch of size 9 \u00d7 9 \u00d7 9 voxels, and found K = 5 clusters and d = 21 to be sufficient. We use a hierarchical implementation, where the subvolume parameters are trained at three iterative scales. While learning is performed in the common atlas space, we restore each volume in its original subject space to limit effects of interpolation. We apply the inverse of the estimated subject-specific affine transformation to the cluster statistics, and use them to restore patches via (12) in the original subject space.\nOur implementation is freely available at https://github.com/adalca/papago."}, {"section_title": "Experiments", "text": "We demonstrate the proposed imputation algorithm on two datasets and evaluate the results both visually and quantitatively. We also include a preliminary example of how imputation can aid in a skull stripping task. Representative reconstruction by NLM, linear interpolation, and our method, and the original high resolution images for two representative subjects in the study. Our method reconstructs more anatomically plausible substructures as can be especially seen in the close-up panels, for example in the skull or temporal lobe."}, {"section_title": "Data", "text": "ADNI Dataset. We evaluate our algorithm using 326 T1-weighted brain MR images from ADNI [9] . We downsample the isotropic 1 mm 3 images to slice separation of 5 mm (1 mm \u00d7 1 mm in-plane), and use these low resolution images as input. All subjects are affinely registered to a T1 atlas. The original images serve as the ground truth for quantitative comparisons.\nClinical Dataset. We also demonstrate our algorithm on a clinical set of 127 T2-FLAIR brain MR scans in a stroke patient cohort. These scans are severely anisotropic (0.85 \u00d7 0.85 mm in-plane, slice separation of 6 mm). All subjects are affinely registered to a T2-FLAIR atlas and intensity normalized. The slices are resampled to 1.2 mm \u00d7 1.2 mm resolution."}, {"section_title": "Evaluation", "text": "We compare our algorithm to three upsampling methods: nearest neighbour (NN) interpolation, linear interpolation, and non-local means (NLM) upsampling [13] . For ADNI images, we found the hierarchical implementation was unnecessary and only ran the final scale of our algorithm. We compare the reconstructions to the original isotropic volumes both visually and quantitatively (ADNI images only). We use mean squared error MSE (Z, of the reconstructed image Z relative to the original high resolution scan Z o , and peak signal to noise ratio PSNR = log 10 max(Zo) MSE(Z,Zo) . Both metrics are commonly used in measuring the quality of reconstruction of compressed or noisy signals. Additionally, we illustrate a preliminary example application where skull stripping fails using the original scan and improves dramatically if an imputed image is used. Figure 4 illustrates representative restored images for typical subjects in the ADNI dataset. Our method produces more plausible structure both in coronal and saggital slices. The method restores anatomical structures that are almost entirely missing in the other reconstructions, such as the dura or the sulci of the temporal lobe. Figure 5 reports the error statistics in the ADNI data. Due to high variability among subject scans, we report improvements of each method over the nearest neighbor interpolation baseline in the same scan. Our algorithm offers significant improvement compared to the linear interpolation and NLM."}, {"section_title": "Results", "text": "We also show a preliminary result where imputed data facilitates downstream image analysis. Specifically, the first step in many analysis pipelines is brain extraction -isolating the brain from the rest of the anatomy. Typical algorithms assume that the brain consists of a single connected component separated from the skull and dura by cerebral spinal fluid [20] , and often fail on sparsely sampled scans that no longer have clear contrast between these regions. Figure 6 illustrates an example result where the brain extraction fails on the original subject scan but succeeds on our reconstructed image. Figure 7 illustrates representative restoration improvements in the clinical population. Our method produces more plausible structure, as can be especially seen in the close-up panels focusing on anatomical details."}, {"section_title": "Discussion and Conclusions", "text": "We propose an image imputation method that employs a large collection of low-resolution images to infer fine-scale anatomy of a particular subject. We introduce a model that captures structural similarity across subjects in large clinical image collections, and fills in the missing data in low resolution scans. The method produces anatomically plausible volumetric images consistent with sparsely sampled input scans.\nLatent Structure. In this paper, we explicitly model and estimate the latent low-dimensional embedding for each patch. Additionally, we also explored an alternative choice that instead models each missing voxel as a latent variable. The resulting ECM algorithm estimates the expected missing voxel statistics directly, and then updates the cluster parameters. The most notable difference between this formulation and simpler algorithms that iteratively fill in missing voxels and then estimate GMM model parameters is in the estimation of the expected data covariance, which captures the covariance of the missing and observed data (c.f. [12] , Chap. 8). We found that this variant often got stuck in local minima and had difficulty moving away from the initial missing voxel estimates, and was an order of magnitude slower than the presented method. We provide both implementations in our code.\nInitialization. In contrast to the classical EM algorithm, the M-step of the ECM algorithm employs previous parameter estimates to perform parameter updates. This makes the initialization more challenging compared to the classical GMM learning, where initializing cluster memberships is sufficient, and also leads to slower convergence than simpler GMMs. We experimented with several initialization schemes, and provide them in our implementation. The experimental results are initialized by first learning a simple GMM from the linearly interpolated volumes, and using the resulting parameter as initializations for our method. This leads to results that improve on the linear interpolation but still maintain slightly blocky effects caused by interpolation. More agnostic initializations, such as random parameter values, lead to more realistic anatomy but noisier final estimates. Different methods perform well in different regions of the brain. Future research will further investigate the effects of initialization on the resulting reconstruction.\nRestoration Method. Our restoration method, assumes that the observed voxels are noisy manifestations of low dimensional patch representations, and reconstructs the entire patch, including the observed voxels, leading to smoother images. We also explored an alternative reconstruction method of filling in just the missing voxels given the observed voxels (not shown). This formulation imputes the most likely missing voxels assuming the observed voxels are true observations, leading to sharper but noisier patches. The two restoration methods therefore yield images with different characteristics. This tradeoff is a function of the noise in the original acquisition: higher noise in the clinical acquisition leads to noisier reconstructions using the alternative method, whereas in the ADNI dataset the two methods perform similarly.\nVarying Resolution. The proposed method can be used for general image imputation using datasets of various resolutions. For example, while acquiring a large high resolution dataset for a clinical study is often infeasible, our algorithm will naturally make use of any additional image data available. Even a small number of acquisitions in different directions or higher resolution than the study scans promise to improve accuracy of the resulting reconstruction.\nSlice Thickness. In many clinical datasets the slice spacing is unknown or varies by site, scanner or acquisition. Therefore, throughout our model we simply treat the original data as high resolution planes. Explicitly modeling varying slice thickness is an interesting direction of future research.\nOur method does not require high volumetric resolution scans or expert annotations, but can instead build the missing structure by learning from collections of clinical scans of similar quality to that of the input image. This enables the use of untapped clinical data for large scale scientific studies, promising to facilitate novel clinical analyses."}]