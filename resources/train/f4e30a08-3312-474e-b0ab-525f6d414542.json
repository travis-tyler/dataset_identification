[{"section_title": "Abstract", "text": "Following recent technological revolutions, the investigation of massive biomedical data with growing scale, diversity, and complexity has taken a center stage in modern data analysis. Although complex, the underlying representations of many biomedical data are often sparse. For example, for a certain disease such as leukemia, even though humans have tens of thousands of genes, only a few genes are relevant to the disease; a gene network is sparse since a regulatory pathway involves only a small number of genes; many biomedical signals are sparse or compressible in the sense that they have concise representations when expressed in a proper basis. Therefore, finding sparse representations is fundamentally important for scientific discovery. Sparse methods based on the 1 norm have attracted a great amount of research efforts in the past decade due to its sparsity-inducing property, convenient convexity, and strong theoretical guarantees. They have achieved great success in various applications such as biomarker selection, biological network construction, and magnetic resonance imaging. In this paper, we review state-of-the-art sparse methods and their applications to biomedical data."}, {"section_title": "INTRODUCTION", "text": "Recent technological revolutions have unleashed a torrent of biomedical data with growing scale, diversity, and complexity [24; 27; 77; 86; 101] . The wealth of data confronts scientists with an urgent need for new methods and tools that can intelligently and automatically extract useful information from data and synthesize knowledge [17; 32; 56; 74] . Although complex, the underlying representations of many real-world data are often sparse [32; 38; 41] . For example, for a certain disease such as leukemia, even though humans have tens of thousands of genes, only a small number of them are relevant to the disease; a gene network is sparse since a regulatory pathway involves only a small number of genes; the neural representation of sounds in the auditory cortex of unanesthetized animals is sparse, since the fraction of neurons active at a given instant is small; many biomedical signals have sparse representations when expressed in a proper basis. Therefore, finding sparse representations is fundamentally important for scientific discovery. The last decade has witnessed a growing interest in the search for sparse representations of data. The quest for sparsity is further motivated for various reasons. First, sparse representations enhance the interpretability of the model. For example, in many biological applications, the selection of genes or proteins which are related to the study, is crucial to facilitate the biological interpretation [18; 38] . In addition, the resulting gene/protein selection might enable a feasible biological validation with a reduced experimental cost. Second, sparseness is one way to measure the complexity of the learning model [84] . Regularization is commonly employed to penalize the complexity of a learning model and alleviate overfitting. Regularization based on the 0 norm maximizes sparseness, which, however, leads to an NP-hard problem. As a computationally efficient alternative, the 1 norm regularization, which also leads to a sparse model, is widely used in many areas including signal processing, statistics, and machine learning [13; 23; 52; 93; 98; 124; 127] . Finally, finding sparse representations has recently received increasing attention due to the current burst of research in Compressed Sensing (CS) [4; 6; 16; 25; 26; 102] . CS is a technique for acquiring and reconstructing a signal utilizing the prior knowledge that it is sparse or compressible. It encodes a large sparse signal using a relatively small number of linear measurements, and minimizing the 1 norm in order to decode the signal. Recent theories [13; 14; 15; 16; 25] assert that one can recover certain signals and images from far fewer samples or measurements than traditional methods. In this paper, we review sparse methods for (1) incorporating a priori knowledge on feature structures for feature selection, (2) constructing undirected Gaussian graphical models, and (3) parallel magnetic resonance imaging.\nStructured Feature Selection. Although sparse learning models based on the 1 norm such as the Lasso [98] have achieved great success in many applications, they do not take the existing feature structure into consideration. Specifically, these models yield the same solution after randomly reshuffling the features. However, in many applications, the features exhibit certain intrinsic structures, e.g., spatial or temporal smoothness, disjoint/overlapping groups, trees, and graphs [42; 45; 51; 65; 116] . The a priori structure information may significantly improve the classification/regression performance and help identify the important features. For example, in the study of arrayCGH [99; 100] , the features-the DNA copy numbers along the genome-have the natural spatial order, and the fused Lasso, which incorporates the structure information using an extension of the 1-norm, outperforms the Lasso in both classification and feature selection. In this paper, we review various structured sparse learning models including group Lasso, sparse group Lasso, overlapping group Lasso, tree Lasso, fused Lasso, and graph Lasso.\nSparse Undirected Gaussian Graphical Models. Undirected graphical models explore the relationships among a set of random variables through their joint distribution. The estimation of undirected graphical models has applications in many domains, such as computer vision, biology, and medicine. An instance is the analysis of gene expression data. As shown in many biological studies, genes tend to work in groups based on their biological functions, and there exist some regulatory relationships between genes [19] . Such biological knowledge can be represented as a graph, where nodes are the genes, and edges describe the regulatory relationships. Graphical models provide a useful tool for modeling these relationships, and can be used to explore gene activities. One of the popular graphical models is the Gaussian graphical model (GGM), which assumes the variables to be Gaussian distributed [5] . In GGM, the problem of learning a graph is equivalent to estimating the inverse of the covariance matrix (precision matrix), since the nonzero off-diagonal elements of the precision matrix represent edges in the graph [5] . In some applications, we need to estimate multiple related precision matrices. For example, in the modeling of brain networks for Alzheimer's disease using neuroimaging data [43] , we want to estimate graphical models for three groups: normal controls (NC), patients of mild cognitive impairment (MCI), and Alzheimer's patients (AD). These graphs are expected to share some common connections, but they are not identical. It is thus desirable to jointly estimate the three graphs. In this paper, we review sparse methods for estimating a single undirected graphical model and for estimating multiple related undirected graphical models and discuss their properties.\nParallel Magnetic Resonance Imaging. Parallel imaging has been the single biggest innovation in magnetic resonance imaging in the last decade. It exploits the difference in sensitivities between individual coil elements in a receive array to reduce the number of gradient encodings required for imaging, and the increase in speed comes at a time when other approaches to acquisition time reduction were reaching engineering and human limits [59] . In the SENSE-type reconstruction approach, researchers have taken advantage of the sparsity promoting penalties (e.g., wavelets and total variations) to reduce the acquisition time while maintaining the image quality. Key components of sparse learning include the estimation of the coil sensitivity profiles, the design of the sparsity promoting regularization, the development of the sampling pattern that takes advantage of sparse learning, and the efficient optimization of the non-smooth inverse problem. In this paper, we review different components of sparse learning in magnetic resonance imaging.\nThe rest of the paper is organized as follows. We review structured sparse learning for feature selection in Section 2. The estimation of sparse undirected Gaussian graphical models is presented in Section 3. We discuss sparse learning in parallel magnetic resonance imaging in Section 4. Finally, we conclude the paper in Section 5."}, {"section_title": "STRUCTURED FEATURE SELECTION", "text": "We are given a set of training samples {ai, bi} n i=1 , where ai \u2208 R p denotes the p-dimensional features for the i-th sample, and bi \u2208 R is its response (numeric for regression, and categorical for classification). In addition, we are given a feature structure, e.g., a group structure, a tree structure, or a graph structure, as part of the input data. We focus on a linear model h :\nis the vector of model parameters. To fit the model with the training samples, we learn the model parameter vector x by solving the following optimization problem:\nwhere L(x) is a loss function, \u2126(x) is a regularization term encoding the prior knowledge on the input features, and \u03bb > 0 is the regularization parameter controlling the tradeoff between the loss L(\u00b7) and the penalty \u2126(\u00b7). The formulation in (1) can be applied for regression, classification, and longitudinal data analysis:\n\u2022 Regression: The outcome b is a continuous value, e.g., the hippocampus volume or the minimental state examination (MMSE) score of a subject in the study of Alzheimer's disease. The least squares loss is commonly used for regression.\n\u2022 Classification: The outcome b is a discrete value, e.g., disease status, including normal controls and disease patients. The logistic loss is commonly used for classification.\n\u2022 Longitudinal Data Analysis: The outcome b is the observed failure/censoring time. If an event occurs at time t, then the subject has a failure time t. If a patient drops from the study at time t, we consider he/she is censored at time t. The Cox model is a popular approach for longitudinal data analysis, in which the negative log-likelihood function of the proportional hazard is used as the loss function [21] .\nThe regularization term \u2126(x) in (1) is commonly employed to penalize the complexity of a learning model and alleviate overfitting, e.g., the 2-norm regularization used in ridge regression. However, the commonly used 2-norm regularization leads to a dense model, i.e., almost all model parameters in x are non-zero. To enhance the interpretability of the model, a sparse model is desired. One popular sparse model, known as the Lasso, is based on the 1-norm penalty:\nThe Lasso has been applied widely in many biomedical applications [91; 94; 107; 111; 123] . In many applications, the features exhibit certain intrinsic structures, e.g., spatial or temporal smoothness, graphs, trees, and disjoint/overlapping groups. The a priori structure information may significantly improve the classification/regression performance and help to identify the important features."}, {"section_title": "Group Lasso and Sparse Group Lasso", "text": "In many applications, the features form a natural group structure. For example, the voxels of the positron emission tomography (PET) images in the Alzheimer's Disease study can be divided into a set of non-overlapping groups according to the brain regions [43] ; in the multi-factor ANOVA problem, each factor may have several levels and can be represented using a group of dummy variables [117] . The selection of group structures has recently received increasing attention in the literature [3; 44; 45; 64; 78; 117; 120] . The pioneer work [117] focused on the non-overlapping group Lasso, i.e., the groups are disjoint. Assume the features are partitioned into k disjoint groups {G1, \u00b7 \u00b7 \u00b7 , G k }. The group Lasso formulation uses the q,1-norm penalty on the model parameters:\nwhere \u00b7 q is the q -norm with q > 1 (most existing work focus on q = 2 or \u221e) [68] , and wi is the weight for the i-th group. The group selection distinguishes the group Lasso from the Lasso which does not take group information into account and does not support group selection. The group Lasso has been applied for regression [55; 80; 117] , classification [78] , joint covariate selection for grouped classification [85] , and multi-task learning [2; 62; 89] . The group Lasso does not perform feature selection within each feature group. For certain applications, it is desirable to perform simultaneous group selection and feature selection. The sparse group Lasso (sgLasso) incorporates the strengthens from both Lasso and group Lasso, and it yields a solution with simultaneous between-and within-group sparsity [30; 87] . The sparse group Lasso penalty is based on a composition of the q,1-norm and the 1-norm:\nwhere \u03b1 \u2208 [0, 1], the first term controls the sparsity in the feature level, and the second term controls the sparsity in the group level. The sparse group Lasso has been applied to analyze multiple types of high dimensional genomic data for biomarker discovery [87] . Figure 1 illustrates Lasso, group Lasso, and sparse group Lasso; we use four types of data sources including Proteomics, GWAS (genome-wide association study), MRI (magnetic resonance imaging), and PET from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database 1 . We construct four feature groups, one for each each data source. As shown in the figure, the Lasso does not consider the group (source) information and selects a subset of features from all four groups; the group Lasso selects a subset of the groups 1 http://www.adni-info.org/ (3 in this example) and all features from these 3 groups are selected; the sparse group Lasso simultaneously selects a subset of the groups and a subset of the features within each selected group."}, {"section_title": "Overlapping Group Lasso and Tree Lasso", "text": "In group Lasso [117] , the groups are disjoint. Some recent work [44; 45; 46; 51; 69; 120] studied the more general case where the groups may overlap. One motivating example is the use of biologically meaningful gene/protein sets (groups). The proteins/genes in the same groups are related if they either appear in the same pathway, or are semantically related in terms of Gene Ontology (GO) hierarchy, or are related from gene set enrichment analysis (GSEA) [97] . The canonical pathway in MSigDB, for example, has provided 639 groups of genes [97] . It has been shown that the group (of proteins/genes) markers are more reproducible than individual protein/gene markers and the use of such group information improves classification performance [19] . Groups may overlap -one protein/gene may belong to multiple groups -and the group Lasso formulation is not applicable. For the general overlapping group patterns, we can make use of the following overlapping group Lasso penalty [120] :\nwhere \u03b1 \u2208 [0, 1], wi > 0 (i = 1, 2, . . . , k), and Gi consists of the indices from the i-th group of features. The k groups of features are pre-specified, and they may overlap. A different overlapping group Lasso formulation was proposed in [44] . In some applications, the features follow a tree structure. For example, an image can be represented using a tree structure where each leaf node corresponds to a feature (pixel) and each internal node corresponds to a group of features (pixels) based on the spatial locality [69] . In such a case, we can make use of the tree structured group Lasso penalty [46; 51; 69; 120] :\nwhere w i j > 0 is a constant weight, and G i j , a node at the depth i, consists of all features in the subtree. Note that any parent node is a superset of its children. Thus, if a specific node is not selected (i.e., its corresponding model coefficient is zero), then all its children will not be selected. It is clear that the tree structured group Lasso is a special case of the overlapping group Lasso with a specific tree structure. (8), the fused Lasso signal approximator."}, {"section_title": "Fused Lasso", "text": "In many applications, the features enjoy certain smoothness properties. For example, the adjacent features in the arrayCGH data are close to each other along the genome. Therefore, it is desirable to enforce the model parameters in x to have the structure of smoothness. Such a structure can be induced by the fused Lasso penalty [28; 99] :\nwhere \u03b1 \u2208 [0, 1]. The fused Lasso penalty in (7) shall induce a solution that xi tends to be close or identical to xi+1 for i = 1, \u00b7 \u00b7 \u00b7 , p \u2212 1. The smoothness structure can also be revealed from the fused Lasso signal approximator [28] : Figure 2 illustrates the fused Lasso signal approximator (8) under different values of \u03bb with \u03b1 = 0.5. We can observe that the solution is piecewise constant."}, {"section_title": "Graph Lasso", "text": "In certain applications, the features form an undirected graph structure, in which two features connected by an edge in the graph are more likely to be selected together. As an example, many biological studies have suggested that genes tend to work in groups according to their biological functions, and there are some regulatory relationships between genes [60] . This biological knowledge can be represented as a graph, where the nodes represent the genes, and the edges imply the regulatory relationships between genes. Figure 3 shows a subgraph consisting of 80 nodes (genes) of the network described in [19] . Several recent studies have shown that the estimation accuracy can be improved using dependency information encoded as a graph. Let (N, E) be a given graph, where N = {1, 2, \u00b7 \u00b7 \u00b7 , p} is a set of nodes, and E is a set of edges. Node i corresponds to the i-th feature. If nodes i and j are connected by an edge in E, then the i-th feature and the j-th feature tend to be grouped. The fussed Lasso penalty in (7) can be extended to a general graph structure; we call it the 1 graph Lasso:\nwhere the second regularization term penalizes a large deviation between two model parameters whose corresponding nodes are connected in the graph. Intuitively, if two genes/proteins are connected in a network, their model parameters are likely to be close to each other, satisfying the so-called smoothness property on a graph. The 1 graph Lasso formulation is computationally expensive to solve. The 2 graph Lasso, or the Laplacian Lasso, is an efficient alternative, which uses the following penalty:\nwhere L is the Laplacian matrix [7; 20] constructed from the graph. It is known that the Laplacian matrix is positive semi-definite, and captures the underlying local geometric structure of the data. When L is an identity matrix, (10) reduces to the elastic net penalty [126] . Existing efficient algorithms for solving the Lasso can be applied to solve the 2 graph Lasso by grouping the loss term L(x) and the Laplacian regularization \u03bb(1 \u2212 \u03b1)x T Lx together, as the latter is both convex and differentiable. Both 1 and 2 graph Lasso encourage positive correlation between the values of coefficients for the features connected by an edge in the graph. However, in certain applications, two features connected may be negatively correlated. To overcome this limitation, GFlasso employs a different 1 reg-ularization over a graph:\nwhere rij is the sample correlation between two features [50] . The penalty in (11) encourages the coefficients xi, xj for features i, j connected by an edge in the graph to be similar when rij > 0, but dissimilar when rij < 0. GFlasso would introduce additional estimation bias due to possible graph misspecification. For example, additional bias may occur when the sign of rij is inaccurate. Another alternative is the so-called graph OSCAR (GOSCAR) penalty given by [110] :\nwhere a pairwise \u221e regularizer is used to encourage the coefficients to be equal [9] , but the grouping constraints are imposed on the nodes connected over the given graph. The\nThe GOSCAR formulation is closely related to OSCAR [9] . The penalty of OSCAR is\nThe 1 regularizer leads to a sparse solution, and the \u221e regularizer encourages the coefficients to be equal. OSCAR can be efficiently solved by accelerated gradient methods, whose key projection can be solved by a simple iterative group merging algorithm [121] . However, OSCAR assumes each node is connected to all the other nodes, which is not sufficient for many applications. Note that OSCAR is a special case of GOSCAR when the graph is complete. GOSCAR, incorporating an arbitrary undirected graph, is much more challenging to solve [110] . The penalty in GOSCAR overcomes the limitation of the Laplacian Lasso that the different signs of coefficients can introduce additional penalty. However, under the \u221e regularizer, even if |xi| and |xj| are close to each other, the penalty on this pair may still be large due to the property of the max operator, resulting in the coefficient xi or xj being over penalized. The additional penalty would result in biased estimation, especially for large coefficients, as in the Lasso case [98] . In GFlasso, when the pairwise sample correlation wrongly estimates the sign between xi and xj, an additional penalty on xi and xj would occur, introducing estimation bias. This motivates the following non-convex feature grouping and selection penalty:\nwhich shrinks only small differences in absolutes values [110; 125] . As a result, estimation bias is reduced as compared to those convex grouping penalties. Note that the non-convex penalty does not assume the sign of an edge is given; it only relies on the graph structure."}, {"section_title": "SPARSE UNDIRECTED GAUSSIAN GRAPHICAL MODELS", "text": "Undirected graphical models are commonly used to describe and explain the relationships among a set of variables based on a collection of observations. In the Gaussian case, the graphical Lasso [29] is a popular approach for learning the structure in an undirected Gaussian graphical model [5] . The basic model for continuous data assumes that the observations have a multivariate Gaussian distribution with mean \u00b5 and covariance matrix \u03a3. If the ijth entry of \u0398 = \u03a3 \u22121 is zero, then variables i and j are conditionally independent, given the other variables. Here, \u0398 is called the precision matrix. Thus, the problem of identifying the structure of the undirected Gaussian graphical model is equivalent to finding the nonzero entries of \u0398. In [5] , the 1 penalty is imposed on the precision matrix to increase its sparsity. The sparse undirected graphical model has been applied to construct biological networks [5] and brain networks [43] ."}, {"section_title": "Graphical Lasso", "text": "Suppose we have n samples independently drawn from a multivariate Gaussian distribution, and these samples are denoted as y1, \u00b7 \u00b7 \u00b7 , yn \u223c N (\u00b5, \u03a3), where yi is a p dimensional vector, \u00b5 \u2208 R p is the mean, and \u03a3 \u2208 R p\u00d7p is the covariance matrix. Let \u0398 = \u03a3 \u22121 be the inverse covariance matrix. The empirical mean is denoted as\u03bc = 1 n n i=1 yi, and the empirical covariance is denoted as S:\nIt can be shown that under a multivariate Gaussian model, the maximum likelihood estimate of \u0398 = \u03a3 \u22121 can be obtained by solving the following maximization problem:\nwhere tr (S\u0398) is the trace of S\u0398, given by the summation of the diagonal entries of S\u0398. Assume that S is nonsingular.\nThe maximum likelihood estimate of the inverse covariance \u0398 is \u0398 = S \u22121 . If the dimensionality is larger than the sample size, i.e., p > n, S is singular. In such a case, regularization is commonly applied, and we estimate \u0398 = \u03a3 \u22121 by maximizing the following objective function:\nwhere J(\u0398) is a penalty function. The graphical Lasso employs the 1 penalty and solves the following optimization problem [5] :\nIt is known that a larger value of \u03bb leads to a sparser \u0398 that fits the data less well, while a smaller value of \u03bb leads to a less sparse \u0398 that fits the data well. Thus, the choice of \u03bb is an important issue in practical application of the graphical Lasso [63; 79] . Banerjee et al. [5] employed the interior point method to solve the optimization problem in (17) . Friedman et al. [29] developed the graphical Lasso (GLasso) which applied the blockwise coordinate descent method to solve (17) . The GLasso fails to converge with warm-starts. To resolve this issue, Mazumder and Hastie [76] proposed a new algorithm called DP-GLasso, each step of which is a box-constrained QP problem. The main challenge of estimating a sparse precision matrix is its high computational complexity. Witten et al. [106] and Mazumder and Hastie [75] independently derived a screening rule, which dramatically reduced the computational cost especially for large regularization parameter values."}, {"section_title": "The Monotone Property", "text": "Huang et al. [43] derived the monotone property of the graphical Lasso. We first introduce the following definition.\nDefinition 1. In the graphical representation of the inverse covariance, if node i is connected to node j by an arc, then node i is called a \"neighbor\" of node j. If node i is connected to node k though some chain of arcs, then node i is called a \"connectivity component\" of node k.\nIntuitively, two nodes are neighbors if they are directly connected, whereas two nodes belong to the same connectivity component if they are indirectly connected, i.e., the connection is mediated through other nodes. In other words, if two nodes do not belong to the same connectivity component (i.e., two nodes completely separated in the graph), then they are completely independent of each other. Huang et al. [43] showed that the connectivity components have the following monotone property: Proposition 1. Let C k (\u03bb1) and C k (\u03bb2) be the sets of all the connectivity components of node k with \u03bb = \u03bb1 and \u03bb = \u03bb2, respectively. If \u03bb1 < \u03bb2, then\nIntuitively, if two nodes are connected (either directly or indirectly) at one level of sparseness, they will be connected at all lower levels of sparseness. This monotone property can be used to identify how strongly connected each node k is to its connectivity components [43] ."}, {"section_title": "Simultaneous Estimation of Multiple Graphs", "text": "In some applications, we need to estimate multiple related precision matrices. A motivating example is the modeling of brain networks for Alzheimer's disease using neuroimaging data such as PET, in which, we want to estimate graphical models for three groups: normal controls (NC), patients of mild cognitive impairment (MCI), and Alzheimer's patients (AD). These graphs are expected to share some common connections, but they are not identical. Furthermore, the graphs are expected to evolve over time, in the order of disease severity from NC to MCI to AD. Estimating the graphical models separately fails to exploit the common structures among them. It is thus beneficial to jointly estimate the three graphs, especially when the number of subjects in each group is small. There is some recent work on the estimation of multiple precision matrices. Guo et al. [36] proposed to jointly estimate multiple graphical models using a hierarchical penalty. The time-varying graphical models were studied by Zhu et al. [122] , and Kolar et al. [53; 54] . Danaher et al. [22] estimated multiple precision matrices simultaneously using a pairwise fused penalty and grouping penalty. Assume we are given K data sets, X (k) \u2208 R n k \u00d7p , k = 1, \u00b7 \u00b7 \u00b7 , K with K \u2265 2, where n k is the number of samples of the ith dataset, and p is the number of features. The p features are common for all K data sets, and all samples are independent. Furthermore, the samples within each data set X (k) are identically distributed with a p-variate Gaussian distribution with zero mean and covariance matrix \u03a3 (k) .\nWe assume that there are many conditionally independent pairs of features, i.e., the precision matrix \u0398 (k) = (\u03a3 (k) ) \u22121 is sparse. Denote the sample covariance matrix for each data set X (k) as S (k) and \u0398 = {\u0398 (1) , . . . , \u0398 (K) }. We can learn multiple precision matrices together by solving the following optimization problem [22; 109] :\nwhere\nand \u03bb1 and \u03bb2 are nonnegative regularization parameters. The 1 regularization leads to a sparse solution, and the fused penalty encourages \u0398 (k) to be similar to its neighbors. The optimization in (18) is computationally expensive to solve. Danaher et al. [22] developed a screening rule for the two graph case to speed up the computation. The screening rule was recently extended to the more general case with more than two graphs in [109] . Specifically, Yang et al. [109] considered the problem of estimating multiple graphical models by maximizing a penalized log likelihood with 1 and fused regularization as in [22] . The 1 regularization yields a sparse solution, and the fused regularization encourages adjacent graphs to be similar. The blockwise coordinate descent method was employed to solve the fused multiple graphical Lasso (FMGL), where each step was solved by the accelerated gradient method [83] . In addition, a screening rule was developed which enabled the efficient estimation of multiple large precision matrices. Specifically, a set of necessary conditions were derived for the solution of FMGL to be block diagonal. These conditions were shown to be sufficient when K \u2264 3. Yang et al. also performed extensive simulation studies; results indicate that these conditions are likely sufficient for any K > 3 as well."}, {"section_title": "PARALLEL MAGNETIC RESONANCE IMAGING", "text": "Magnetic resonance imaging (MRI) [39; 105] is a medical imaging technique used in radiology to visualize internal structures of the body in detail. As a non-invasive imaging technique, MRI makes use of the property of nuclear magnetic resonance to image nuclei of atoms inside the body. MRI has been applied to image the brain, muscles, the heart, cancers, etc."}, {"section_title": "Undersampled k-space", "text": "The acquired raw data by an MR scanner are the Fourier coefficients, or the so-called k-space data (see Figure 4 (a) for illustration). The k-space data are typically acquired by a series of phase encodings (each phase encoding covers a given amount of k-space data that are related to the trajectory, e.g., Cartesian sampling, radial sampling). 12.8 seconds to acquire the full k-space data of one 256\u00d7256 image with the Cartesian trajectory. With the same TR, it takes about 15.4 minutes to acquire the full k-space of a 256 \u00d7 256 \u00d7 72 volume. With higher spatial resolution, the time for acquiring the full k-space can be even longer. In addition, in dynamic cine imaging, we are interested in the study of the motion of the object (heart, blood, etc) over time. This leads to an increased number of phase encodings and increased acquisition time, and one usually has to compromise between spatial resolution and temporal resolution. To save the acquisition time, one has to undersample the kspace, i.e., reducing the number of acquired phase encodings. For example, if the k-space data are acquired every other line, as shown in Figure 4 (c), half of the acquisition time can be saved. The relationship between the acquired k-space data and the image to be reconstructed can be written as\nwhere Fu is a given undersampled Fourier transform operator, f denotes the MR image, y is the acquired k-space data, and n depicts the noise introduced in the acquisition. Unlike the full k-space scenario, one cannot directly apply the inverse Fourier transform to the undersampled data acquired in Figure 4 (c), since otherwise an aliased image shown in Figure 4 (d) will be obtained."}, {"section_title": "Parallel MR Imaging", "text": "Parallel imaging [34; 47; 88; 95] has been proven effective for reducing the acquisition time. It exploits the difference in sensitivities between individual coil elements in a receive array to reduce the number of gradient encodings required for imaging. Figure 5 illustrates parallel imaging with 8 coils. Specifically, the first two rows show the coil images seen by the individual coil/channel, and the last two rows show the coil profiles of these 8 coils. It can be observed that the 8 coils have different sensitivities. Parallel imaging tries to reconstruct the target image with the undersampled k-space data. Based on how the coil sensitivities are used, parallel imaging can be roughly divided into the following two main categories: 1) the approaches that implicitly make use of the coil sensitivities, represented by GRAPPA [34] , and 2) the approaches that explicitly make use of the coil sensitivities, represented by SENSE [88] . In the GRAPPA type approaches, one usually estimates the missing phase encoding lines with the kernels that are estimated by implicitly using the coil sensitivities. In the SENSE type approach, one models the relationship between the target image and the acquired k-space data as:\nwhere yi is the acquired undersampled k-space data by the i-th coil, and Si is the coil sensitivity maps (see the last two rows of Figure 5 ). The relationships between GRAPPA and SENSE have been studied in the literature [8; 35; 47] , and several recent work [57; 58; 72; 73] have shown that GRAPPA and SENSE can be combined to give improved reconstruction performance."}, {"section_title": "Coil Profile Estimation", "text": "The most common way to determine the sensitivity maps is to obtain low-resolution pre-scans. However, when the object is not static, the sensitivity functions are different between pre-scan and under-sampled scans, and this could lead to reconstruction errors. To compensate for this, joint estimation approaches [103; 113] have been proposed. How- Figure 6 : Illustration of the gradient of the phantom (shown in Figure 4 ) along the vertical direction (left) and horizonal direction (right), respectively.\never, these approaches usually have high computation cost and are restricted to the SENSE type reconstruction. The eigen-vector approach proposed in [72] is a very promising approach for sensitivity maps estimation. It tried to build a connection between GRAPPA and SENSE-type approaches, by showing that the Coil Profile used in SENSE can be computed with the GRAPPA-type calibration. Such idea was also used in [57; 58] . It was shown in [72] that the coil sensitivities can be computed as the eigen-vector of a given matrix in the image space corresponding to eigenvalues \"1\"s."}, {"section_title": "Sampling Pattern and Fourier Transform", "text": "Cartesian sampling is the most natural scheme which undersamples the k-space by skipping some lines. In cardiac MR imaging, TSENSE [37; 48] is a well-known approach that is based on time interleaving of k-space lines in sequential images, and there are studies that makes use of variable density to optimize the sampling scheme, e.g., [12] . The Fourier transform associated with the Cartesian sampling can be efficiently computed. Spiral and projection (radial) are the most widely used nonCartesian sampling patterns, among many others. It was observed in several works (e.g., [40] ) that the radial sampling exhibits advantages over Cartesian Sampling. The Fourier transform in the non-Cartesian case is much more challenging than the Cartesian one, and gridding is usually employed for performing Non-Uniform FFT [33] ."}, {"section_title": "Incorporating Prior Knowledge and Optimization", "text": "To recover f from (19) , it is important to note that our target f has certain structures, with which we can better reconstruct f from the undersampled data y. This is where sparse learning can play a role. Typically, we are interested in computing f by solving the following problem\nwhere loss(y, Fuf ) depicts the data fidelity, and \u03c6(f ) incorporates our prior knowledge about the image to be reconstructed.\nFor the data fidelity term, a commonly used one is the squared distance between the acquired data and the prediction: loss(y, Fuf ) = For \u03c6(f ), one needs to take advantage of the structure in the target image f . Figure 6 shows the gradient of the phantom, and it is easy to observe that such gradient is sparse. Cand\u00e8s et al. [14] proposed to set \u03c6(f ) = f T V , showed the effectiveness of the sparsity promoting penalty in the scenario of single coil, and proved the exact recovery under the so-called Robust Uncertainty Principles (RIP). Later on, compressed sensing was used widely in the reconstruction of MR images, e.g., [1; 57; 61; 71; 112] . When applying sparse learning to parallel MR imaging, one key task is to develop a suitable \u03c6(\u00b7) that adapts the structure of the image(s) to be reconstructed. Group sparsity [117] has been used for accelerating dynamic MRI [104] , and total variation and wavelet transformation have also been used for parallel MR imaging [14; 66; 67; 90; 103; 112] . An important and hot research topic is to develop better sparsity promoting penalties that adapt to the images to be reconstructed. The efficient optimization of problem (21) is crucial for parallel imaging. Several popular approaches include conjugate gradient [40] , Newton-type methods [103] , Nesterovtype approaches [81; 82; 66; 49] , and the alternating direction method of multipliers [1; 10; 31; 112]."}, {"section_title": "CONCLUSIONS", "text": "In this paper, we review sparse methods for biomedical data in three specific applications. Sparse methods have also been applied to many other applications, e.g., incomplete multisource data fusion [114] and biological image annotation and retrieval [115] . As with many other data mining and machine learning techniques, the selection of the appropriate sparse method and proper tuning of the associated parameters are critical for finding meaningful and useful results. To this end, one needs to understand the data in a domain specific context and understand the strengths and weaknesses of various sparse methods. Most existing work on sparse learning focus on prediction, parameter estimation, and variable selection. Very few work address the problem of assigning statistical significance or confidence [11; 118] . However, such significance or confidence measures are crucial in biomedical applications where interpretation of parameters and variables is very important [11] . Most sparse methods in the literature are based on a convex regularizer. Sparse methods based on a nonconvex regularizer have recently been proposed and efficient methods based on the difference of convex functions (DC) have been developed [92; 119] . However, their theoretical properties have not been well understood yet, although some recent work demonstrate the advantage of non-convex methods over their convex counterparts [92; 108; 119] . Finally, missing data is ubiquitous in biomedical applications. One important issue that has not been well addressed is how to adapt sparse methods to deal with missing data [70; 96] ."}, {"section_title": "ACKNOWLEDGEMENTS", "text": "This work was supported in part by NSF (IIS-0953662, MCB-1026710, CCF-1025177) and NIH (R01LM010730)."}, {"section_title": "REFERENCES", "text": "[1] M. Afonso, J. Bioucas-Dias, and M. Figueiredo. An augmented lagrangian approach to the constrained optimization formulation of imaging inverse problems."}]