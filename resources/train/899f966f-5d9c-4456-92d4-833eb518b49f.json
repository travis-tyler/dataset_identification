[{"section_title": "Abstract", "text": "Alternating Direction Method of Multipliers (ADMM) has become a widely used optimization method for convex problems, particularly in the context of data mining in which large optimization problems are often encountered. ADMM has several desirable properties, including the ability to decompose large problems into smaller tractable sub-problems and ease of parallelization, that are essential in these scenarios. The most common form of ADMM is the two-block, in which two sets of primal variables are updated alternatingly. Recent years have seen advances in multi-block ADMM, which update more than two blocks of primal variables sequentially. In this paper, we study the empirical question: Is two-block ADMM always comparable with sequential multi-block ADMM solving an equivalent problem? In the context of optimization problems arising in multi-task learning, through a comprehensive set of experiments we surprisingly show that multi-block ADMM consistently outperformed two-block ADMM on optimization performance, and as a consequence on prediction performance, across all datasets and for the entire range of dual step sizes. Our results have an important practical implication: rather than simply using the popular two-block ADMM, one may considerably benefit from experimenting with multi-block ADMM applied to an equivalent problem."}, {"section_title": "Introduction", "text": "The past decade has seen wide ranging applications of the alternating direction method of multipliers (ADMM) for different learning problems [1, 2, 3, 4] . The success of ADMM in the context of data mining can be attributed to its ability to decouple large problems into smaller tractable sub-problems, simplicity of the updates (sometimes in closed form) for these individual sub-problems, ease of parallelization, among others. ADMM has also been widely applied to unconstrained non-smooth problems, such as the lasso and fused lasso, by introducing additional variables (lifting) thereby decoupling smooth and non-smooth terms [5, 4] .\nThe most popular version of ADMM is the two-block ADMM which considers the following problem: min\nwhere x \u2208 R n , z \u2208 R m , A \u2208 R d\u00d7n , B \u2208 R d\u00d7m , c \u2208 R d [6, 7, 8, 9] . The formulation is called two-block because of the two sets of variables (x, z) which are updated alternatingly in the primal updates of ADMM. Recent years have seen the use of multi-block ADMM in some settings [6, 10, 11, 12] , which considers more than two sets of variables which are sequentially updated in the primal. The literature has also looked at potential challenges in doing naive multi-block ADMM, and several approaches to doing it with theoretical guarantees of convergence have been developed [13, 14, 12] . Two-block ADMM is by far the most widely used variant of ADMM. In practice, often two-block ADMM is used without much thought regarding alternative multi-block ADMM options. For convex problems, they are supposed to give the same solution after all, with suitable choices of (dual) step sizes. Typically, a multi-block problem can be solved as a two-block problem with suitable variable and constraint grouping, and similarly two-block problems can be converted into multi-block by suitable lifting [6, 10, 15] . Our work is motivated by a desire to understand why two-block ADMM is preferred. A part of the reason may be the seemingly negative theoretical results associated with convergence of multi-block ADMM [11] . In many cases, such negative theoretical results focused on a specific problem or operating regimes where multi-block ADMM diverges. In practice, most optimization methods have such weak spots in terms of struggling in a type of problem or operating regime, e.g., the simplex algorithm for linear programs [16] , stochastic gradient descent (SGD) for deep networks [17] , etc. In many cases, such algorithms are empirically successful and it takes a while to improve our theoretical understanding of its behavior, e.g., smoothed analysis of the simplex algorithm [18] , convergence of SGD on non-convex problems [19, 20, 21, 22] , etc. Such perspective motivates our empirical work on two-block vs. multi-block ADMM comparisons.\nIn particular, in this paper we study the empirical question: Is two-block ADMM always comparable with multi-block ADMM solving an equivalent problem? For both two-block and multi-block ADMM, we consider linearized ADMM [2] which has considerably computational benefits and keep the (dual) step size as the only hyper-parameter. Also, for multiblock ADMM, we use the direct extension from two-block ADMM which sequentially updates the primal variables [1] . We perform experiments using optimization problems arising in multi-task learning (MTL), with application focus on modeling Alzheimer's disease progression, Parkinson's disease assessment, and air quality prediction. For each problem, we conduct a direct comparison between two-block and multi-block ADMM by varying one hyper-parameter-the dual step-size in ADMM (see Section 4 and 5 for details)-up to 5 orders of magnitude, running each method for 1,000 iterations, and for 10,000 iterations in some cases, and repeating each experiment 10 times to account for any inherent variability. The approaches are evaluated based on different evaluation measures including primal residual convergence and prediction performance on validation set (see Section 5). 2 Our results are surprising: multi-block ADMM empirically outperformed two-block ADMM on optimization performance, and as a consequence on prediction performance, across all datasets and for the entire range of dual step sizes. For optimization performance, multi-block ADMM achieves primal residuals which are a few orders of magnitude smaller than that achieved by two-block ADMM, implying a better convergence to the feasible set. Note that primal iterates in any form of ADMM are always outside the feasible set, so having a really small primal residual is important to ensure good convergence, and multi-block ADMM illustrates desirable performance here. Multi-block ADMM also achieves smaller primal objective, which is especially interesting when primal residuals are small. Finally, the models estimated using multi-block ADMM consistently outperforms the corresponding two-block versions on prediction performance evaluated on validation sets.\nThe take-away from the results reported is the following: if a situation calls for ADMM based optimization, rather than using the popular two-block version by default, one needs to try out equivalent multi-block versions and compare their performance. The results reported do not imply that multi-block ADMM will outperform two-block ADMM on every problem, but that one needs to at least consider multi-block as a serious alternative rather than defaulting to the two-block version due to its popularity. Thus, the contribution of the work is empirical, hoping to change the way our community uses ADMM based algorithms.\nThe rest of the paper is organized as follows. We review related work in Section 2. We present the MTL model in Section 3 and discuss two-block and multi-block ADMM for learning the model in Section 4. We present detailed results comparing the algorithms in Section 5 and conclude in Section 6."}, {"section_title": "Related Work", "text": "The success of ADMM with two blocks has encouraged the natural extension of ADMM to solve problems with multiple blocks [15, 10, 23, 24, 6] . Theoretical results [11] have shown that without additional conditions, the direct extension of ADMM with more than two blocks generally fails to converge. But for certain specific classes of problems and/or conditions, researchers have been able to show convergence guarantees for variants of multi-block ADMM. The existing variants of multi-block ADMM can be mainly grouped into two categories: Gauss-Seidel ADMM and Jacobi ADMM.\nThe Gauss-Seidel ADMM (GSADMM) [24, 23] seeks to solve the primal updates in a cyclic block coordinate fashion. In [7] , a back substitution step was added, which facilitated the convergence proof for ADMM with multiple blocks. In [23] , a block successive upper bound minimization method of multipliers (BSUMM) is proposed to solve the problem. In [25] , the authors proved that multi-block ADMM can achieve -optimal solutions within O(1/ 2 ) iterations by solving a modified version of the original problem, and whitin O(1/ ) iterations when applied to certain sharing problems conditioned on properties of the augmented Lagrangian function. Theoretical convergence of some of these methods need fairly strict conditions, e.g., certain local error bounds need to hold and/or the step size needs to be sufficiently small or decreasing.\nAnother broad approach to multi-block ADMM is Jacobi ADMM [15, 6, 26] , which solves the problem in a parallel block coordinate fashion. In [15, 26] , the problem is solved by using two-block ADMM with splitting variables (sADMM). [6] considers a proximal Jacobian ADMM (PJADMM) by adding proximal terms. A randomized block coordinate variant of ADMM named RBSUMM was proposed in [23] . Recently, [27] demonstrated that ADMM is convergent regardless of the number of variable blocks for certain nonconvex consensus and sharing problems.\nAlthough general theoretical aspects of multi-block ADMM are still not well understood, it has been observed that modified versions of ADMM, including the two-block instance, though with convergence guarantee, often perform slower than the multi-block ADMM with no convergent guarantee [25] . It is thus of great interest to further study the practical aspects of convergence of multi-block ADMM. Aiming to address the lack of numerical experiments, we systematically investigate the convergence and performance of two-block and multi-block ADMM in the context of multitask learning, which has significantly benefited from advances in ADMM-type of methods."}, {"section_title": "Temporally Smooth Multi-task Learning (TS-MTL)", "text": "Consider a multi-task learning problem over T tasks, where each task corresponds to a time point t = 1, . . . , T . For each time point t, we consider a regression task based on data (y t , X t ), where X t \u2208 R n\u00d7p denotes the matrix of covariates, p is the number of covariates and n is the number of samples, shared across all the tasks, and y t \u2208 R n is the matrix of responses. Let \u0398 \u2208 R p\u00d7T denote the regression parameter matrix over all tasks, so that column \u03b8 t \u2208 R p corresponds to the parameters for the task in time step t. This multitask learning formulation is based on our recent work [28] .\nWe pose the multi-task learning problem for \u0398 such that two goals are accomplished: each \u03b8 t accomplishes low regression error for each task t, and \"nearby\" \u03b8 t are coupled to be similar, since the \"nearby\" tasks are temporarily related. The notion of \"nearby\" needs to be suitably defined. In this paper, we adopt an approach inspired by nonparametric regression [29] , which has been recently shown to be effective for TS-MTL problems. In particular, we model a local approximation to \u03b8 t in terms of the other \u03b8 , = t a\u015d\nwhere \u03c3 \u2265 0 is a constant bandwidth parameter. Based on such an approximation, our TS-MTL formulation focus on encouraging sparsity of the residual\nThe TS-MTL problem can now be posed as the following unconstrained problem:\nwhere R \u03bb1 \u03bb2 (\u0398) is the combination of lasso and group lasso penalties, also known as the sparse group Lasso penalty, which allows simultaneous joint feature selection for all tasks and selection of a specific set of features for each task [30] . In particular,\nwhere \u0398 1 is the Lasso penalty and \u0398 2,1 = p j=1 \u03b8 j , \u03b8 j \u2208 R T is the group Lasso penalty considering groups across time for each feature j, encouraging the regression models at different time points to share a common set of features. In the formulation, \u03bb 1 , \u03bb 2 , \u03bb 3 > 0 are the regularization parameters which are fixed, and will be chosen using cross validation."}, {"section_title": "ADMM for Learning TS-MTL Models", "text": "The unconstrained optimization problem in (3) can be difficult to optimize directly due to the non-smooth terms. In this section, we consider two formulations, respectively solved as a two-block and multi-block ADMM, which introduce additional variables and associated linear constraints. While the use of two-block or multi-block ADMM for solving the problem is not especially novel, through extensive experiments in Section 5 we will show that the two formulations and associated ADMM algorithms behave surprisingly differently in practice.\nWe start by noting that the optimization problem in (3) can be formulated as the following linearly constrained optimization problem:\nNote that there are four variables \u0398, \u0393, Q, \u03a0 \u2208 R p\u00d7T in the formulation. The problem can also be formulated as\nThe only difference between (5) and (6) is the form of the constraint associated with temporal smoothness residuals:\nbut the constraints are equivalent since \u0398 = Q. (5) is given by:\nwhere z t = w |t\u2212 | q \u2212 \u03b3 t , and S, U, V represent Lagrange multipliers. The optimization problem can be solved as a two-block ADMM with two sets of primal variables: Z 1 = (\u0398, \u0393), since \u0398 and \u0393 can be updated in parallel, and Z 2 = (Q, \u03a0) since Q and \u03a0 can be updated independently.\nWhile the original objective function does not have terms with interactions between \u03b8 t and \u03b8 , = t, the augmented Lagrangian do have such terms, and such interactions are captured in h(\u0398). In order to decouple the \u03b8 t updates, we will perform the ADMM updates by suitably linearizing h(\u0398) around the current iterate\ndenote the gradient with respect to \u03b8 t . Recent work on Bregman ADMM [2] and related work on inexact ADMM [5, 1] have shown that ADMM updates with such linearization continue to work with the same rates of convergence.\nUpdate \u03b8 k+1 t : The update involves solving the following quadratic objective, which can be done efficiently using Cholesky decomposition as discussed in [1] :\n, the update for \u03b3 t needs to solve:\nUpdate Q: The update for Q is based on linearization and we need to compute the proximal operator for R \u03bb2 \u03bb1 (\u00b7):\nwhere\n, and \u03c1 1 > 0 is a suitably chosen constant. In particular, since h(Q) is smooth and has Lipschitz continuous gradients say with constant \u03bd under 2-norm, it suffices to have \u03c1 1 \u2265 2\u03bd [2] . The expression can be simplified to get in the form of a proximal operator computation for R \u03bb2 \u03bb1 (Q). Update \u03a0: For the update \u03a0, we need to compute the proximal operator for L 1 -norm:\nDual Updates S, U, V : Following standard ADMM dual updates, the updates for the dual variables for our setting are as follows: (6) is given by:\nThe optimization problem can be solved as a multi-block ADMM with 3 sets of primal variables: Z 1 = \u0398, Z 2 = \u0393, and Z 3 = (Q, \u03a0). Unlike the two-block case earlier, \u0398 and \u0393 here are coupled, and hence the updates need to be sequential, i.e., updates to \u0398, followed by updates to \u0393, followed by updates to (Q, \u03a0) which can be done in parallel. Note that one can stack (\u0398, \u0393) into a long vector and update them jointly by solving a high-dimensional quadratic objective. We do not consider such a two-block formulation because we have already considered a two-block formulation which is arguably simpler, since the quadratic objective there only involves \u0398. As before, we work with a linearized version of h(\u0398) for the updates.\nUpdate \u03b8 k+1 t\n: The update involves solving the following unconstrained quadratic objective:\nwhere \u03c1 1 > 0 is a suitably chosen constant. In particular, since h(\u0398) is smooth and has Lipschitz continuous gradients with constant \u03bd under 2-norm, it suffices to have \u03c1 1 \u2265 2\u03bd [2] .\n, the update for \u03b3 t needs to be solve:\nUpdate Q: The update for Q effectively needs to compute the proximal operator for R \u03bb2 \u03bb1 (\u00b7) [31, 32] :\nUpdate \u03a0: The update for \u03a0 needs to solve:\nDual Updates for S, U, V : The updates for the dual variables are as follows:"}, {"section_title": "Two-vs. multi-block ADMM", "text": "From the presented two-block and multi-block ADMMs definition, we notice that the only difference is the fact that multi-block ADMM has an additional block due to the coupling of \u0398 and \u0393 variables. . From this perspective, the multi-block ADMM performs a block coordinate descent on the primal variables \u0398 and \u0393, while the two-block carry out a block update. Table 1 : Side-by-side comparison between the two-block and multi-block ADMM. Two variables in the same block means that they can be updated independently."}, {"section_title": "Two-block", "text": "Z 1 = (\u0398, \u0393) \u0398 k+1 = Eq. (8) \u0393 k+1 = Eq. (9) Z 2 = (Q, \u03a0) Q k+1 = Eq. (10) \u03a0 k+1 = Eq. (11) Multi-block Z 1 = (\u0398) \u0398 k+1 = Eq. (15) Z 2 = (\u0393) \u0393 k+1 = Eq. (16) Z 3 = (Q, \u03a0) Q k+1 = Eq. (17) \u03a0 k+1 = Eq. (18)"}, {"section_title": "Experimental Results", "text": "In this section, we present experimental analysis to evaluate convergence and performance aspects of multi-block ADMM and two-block ADMM in solving TS-MTL formulation for three multitask learning problems: (i) Alzheimer's disease progression, (ii) Parkinson's disease assessment, and (iii) Air quality prediction. Before presenting the datasets, we discuss the experimental setup and evaluation metrics used for all three datasets.\nExperimental methodology: We randomly split the data into training and test sets using a proportion of 70% of the data for training and 30% to evaluate the models. From the training set, 20% of the data is used as validation set.\nTen independent executions were perform to account for variability in the data. For hyper-parameter selection we consider a grid of regularization parameter values, where each regularization parameter varied from 10 \u22121 to 10 3 in log scale. For this selection, the dual step size \u03c1 was set to 1, which is a commonly used value [1] . The data was z-scored before applying regression methods. As the dual step size parameter \u03c1 plays an important role at the convergence of ADMM-type of methods, we considered a wide range of values in our experiments, once the methods regularization parameters are defined.\nEvaluation metrics: For the quantitative performance evaluation, we computed the Root Mean Squared Error (rMSE) between the predicted and target clinical scores for each time point (task). For aggregated performance over all time points, the normalized mean squared error (nMSE) [33, 34] is used. The rMSE and nMSE are defined as follows:\n2 ) 1/2 , and nMSE(Y,\u0176 ) = (\n, where Y and\u0176 are the ground truth cognitive scores and the predicted cognitive scores, respectively, and n t is the number of samples of the t-th time point. Smaller values for nMSE and rMSE represent better regression performance. The average (avg) and standard deviation (std) of performance measures across multiple runs on different splits of data are shown as avg \u00b1 std for the predictive performance experiment.\nWe show nMSE performance of the methods on the validation set over time in order to correlate the reduction in the primal residuals with the actual generalization performance of the multitask regressors. We report the primal residual as the sum of the squares of the three primal residuals defined by the constraints in Eqs. (5) and (6) . It is an aggregated measure of how much the current solution violates the constraints."}, {"section_title": "Alzheimer's disease progression", "text": "We used the dataset from ADNI 3 , which is a multi-site study that aimed to improve clinical trials for the prevention and treatment of Alzheimer disease (AD). The study gathered and analyzed thousands of structural MRI brain scans, genetic profiles, and biomarkers in blood and cerebrospinal fluid that are used to measure the progress of disease or the effects of treatment. For this study, we used data from the first phase of ADNI project, called ADNI-1. Subjects perform an initial screening, referred to as baseline (BL), in which demographics information and an initial brain scan are collected. After approval, the subject is scheduled to attend a calendar of follow-up visits that are denoted by the duration starting from the baseline. We use the notation, for example, Month 6 (M6) to denote the time point half year after the baseline. Currently, ADNI has up to Month 48 follow-up data available for some patients. Table 2 presents the number of patients at each time step (follow-up visits) used in this study. Using the MRI brain scans from each visit, the multi-task learning problem involves accurately predicting a given cognitive score over multiple time steps, i.e., each task focuses on modeling a given cognitive score at a given time step, and different tasks focus on different time steps for the same cognitive score. Instead of using raw structural MRI images, we used the features from cortical reconstruction and volumetric segmentations. Details of the feature extraction procedure are available at: http://adni.loni.usc.edu/methods/mri-tool/. Note that modeling different cognitive scores are treated as separate multitask learning problems, and the tasks correspond to the time points for a given cognitive score.\nConvergence results on ADNI Figure 1 shows the convergence of the two-block and multi-block ADMMs for the ADAS cognitive score. Due to space limitations we do not present the curves for MMSE and RAVLT-TOTAL, as they showed similar behaviors. As we can see, the multi-block ADMM has a smooth and steady convergence than the two-block version for all the dual step sizes tested. On the other hand, the two-block starts with a convergence rate similar to multi-block, but the primal residual increases (\u03c1 = 0.1 and \u03c1 = 1) or get stuck and oscillate in a region of the optimization space (\u03c1 = 10). The superior convergence of multi-block ADMM reflects the better generalization of the trained multitask learning regressors, measured by their performance on the validation set. In all the step sizes investigated, notably, the multi-block ADMM achieved solutions with higher generalization capacity. Figure 1 : Primal residual, validation nMSE performance, and primal objective computed on ADAS cognitive scores from the ADNI dataset considering several values for the dual step size \u03c1. Plot shows the average curves for 10 independent runs. In the Multi-block ADMM, the primal residual has a smoother behavior and mostly monotonically decreases over time, while two-block ADMM shows a more unstable convergence. Better convergence of multi-block ADMM leads to a lower nMSE in the validation set.\nIt is worth noting that not all variation in the primal residual directly affect the validation performance. This can be clearly observed for \u03c1 = 0.1 and \u03c1 = 1, where increments in the primal residual after a period of smooth decreasing do not drop the nMSE performance, which in fact continued to decrease.\nA condensed visualization of the convergence and validation performance of the methods is presented in Figure 2 . For each step size \u03c1, we computed and compared the the average of primal residual and validation nMSE over the last 100 iterations out of the 1,000 iterations used for training the model. The deviation bars show the variation over 10 independent runs. We extended the range of values for dual step size \u03c1 \u2208 [0.001, 0.01, 0.1, 1, 10, 20, 30] . For ADNI, the maximum step size was set to 30, as beyond this value we have empirically observed that the two-block ADMM diverges for this particular problem. For all cases, the multi-block outperforms the two-block ADMM.\nADMM with longer training time: A closer observation of Figure 1 , particularly the method's convergence for smaller dual step sizes, \u03c1 \u2264 0.01, leads to the question whether the two-block ADMM would achieve similar performance to multi-block ADMM if we had continued the optimization process for a longer period of time. To answer this question, we extended the number of iterations for the ADMMs to 10,000 and generated the primal residual curves, as shown in Figure 3 . We observe that even for relatively small step sizes (\u03c1 = 0.001) the two-block ADMM still presents the same unstable behavior for MMSE and RAVLT-TOTAL cognitive scores. For ADAS, the two-block Prediction Performance: The final quantitative performance of the multitask learning regressors is measured by their performance on the held-out (test) data set. Table 3 reports the rMSE per task (time point) and nMSE for the three cognitive scores investigated on ADNI dataset. To compute the prediction performance, we used the step size that led to the lowest nMSE on the validation set. Reflecting the performance on the validation set, the multitask learning formulation optimized by the multi-block ADMM produced lower error on the test set. The improvement is noticed for all cognitive scores and time points (tasks), but is more pronounced in the last time points (M36 and M48), which contains the smallest amount of samples. These results reaffirm the superior performance of the multi-block over the two-block ADMM in our multitask learning formulation for the Alzheimer's Disease progression problem."}, {"section_title": "Parkinson's disease assessment", "text": "For this experiment, we used data from the Parkinson's Progression Marker Initiative (PPMI) 4 repository. The dataset includes information from healthy individuals and patients diagnosed with Parkinson's disease. Biospecimen and demographics information are collected from all patients at the beginning of the study, which is referred to baseline (BL), and cognitive assessments are performed in each scheduled visit. Subject's cognitive function is measured by the MDS-UPDRS-Total score. Not all patients attended all visits, causing some time steps to have a smaller amount of patients. Table 4 shows the number of patients which had their cognitive state assessed at each visit. Table 4 : Parkinson's dataset: Number of patients (samples) at each time step (task). MX correspond to the number of months X after the baseline screening was taken."}, {"section_title": "Baseline", "text": "Months after baseline M3 M6 M9 M12 M18 M24 M30 M36 M42 M48 M54 M60 M72 M84  185  99  99  90  199  97  195  92  209  92  216  80  214  121  82 A set of biological specimens are used to predict the cognitive state of the patient at a certain time in the future (visits). These variables have been identified in the literature as potential biomarkers for Parkinson's disease progression [37] and [38] . The variables used are listed below.\nCerebrospinal Fluid: ABeta 1-42, pTau, tTau, CSF Alpha-synuclein, p-tau/t-tau, p-tau/Abeta 1-42, and t-tau/Abeta 1-42.\nPlasma: Total Cholesterol, HDL, Triglycerides, Apolipoprotein A1, LDL, and EGF ELISA.\nSerum: Serum IGF-1.\nRNA: UBC, DJ-1, SNCA-3UTR-2, RPL13, FBXO7-001, SNCA-3UTR-1, DHPR, GLT25D1, FBXO7-007, MON1B, SNCA-E4E6, SNCA-007, FBXO7-005, SRCAP, FBXO7-010, ZNF746, GUSB, SNCA-E3E4, and FBXO7-008.\nBesides the biospecimen variables, we include demographics information which may be relevant for the prediction task. Four variables are incorporated: race, family history, gender, and age. Family history is a binary variable informing whether any family member has been diagnosed with PD.\nBased on the patients information at the BL time step, the goal is to predict patient's cognitive function at the following time-steps (visits). Therefore, a predictive model with sparsity structures will be able to identify the variables that most explain the evolution of patient's cognitive functions. From the multitask learning perspective, predicting the MDS-UPDRS-Total score for each time step (visit) is considered as a task.\nWe compared the performance of TS-MTL with two-block and multi-block ADMM for several values for the dual step size \u03c1 and the results are showed in Figure 4 . It is worth mentioning that the two-block ADMM diverged for \u03c1 > 1, while multi-block ADMM converged for larger values of \u03c1. We notice that multi-block ADMM achieved consistently lower validation nMSE for the entire range for the dual step size \u03c1. Multi-block primal residuals are significantly smaller for \u03c1 that led to best validation nMSEs (\u03c1 = 0.5 and \u03c1 = 1).\n0.0001 0.001 0.01 0.1 0.5 1.0\nStep size Convergence curves for both ADMM methods on PD dataset are shown in Figure 5 . Notably, multi-block ADMM achieved lower primal residuals and validation nMSE. For \u03c1 = 0.5 and \u03c1 = 1, two-block ADMM stagnates and could not longer reduce primal residual and validation nMSE. It is highly probable that even if more iterations were performed, the two-block ADMM would not obtain results comparable to the multi-block version. Validation nMSE"}, {"section_title": "MDS-UPDRS-Total", "text": "Two-Block ADMM Multi-Block ADMM Figure 5 : Average curves of objective function, primal residuals, and validation nMSE over 10 executions on the Parkinson's disease dataset. Many values for learning rate \u03c1 were investigated. Multi-block shows a better convergence than two-block ADMM."}, {"section_title": "Air quality prediction", "text": "For the Air quality dataset 5 the goal is to predict CO concentration for a certain hour of the day given measurements from 5 distinct sensors + temperature and humidity information. In this setup, we train one regression model for each hour of the day. Although the 24-th hour is connected to 1st hour of the next day, we are not considering this circular dependence for this experiment.\nSimilar to the other datasets, we conducted experiments with TS-MTL using both two-block and multi-block ADMM for several values of the dual step size \u03c1, and the results are presented in Figures 6 and 7 . Again, multi-block shows a steady reduction of primal residuals during optimization, along with a more stable validation performance curves. Although multi-block does not present significant lower nMSE on the validation set in the range of values for \u03c1 \u2208 [1e \u22124 , 1e \u22122 ], it is clearly better for larger values (Figure 6 bottom) . "}, {"section_title": "Conclusions", "text": "In this paper, we reported an experimental study on the convergence aspects of two variants of Alternating Direction Method of Multipliers (ADMM): two-block and multi-block. For many optimization problems associated with data mining and machine learning models, for example, both versions can be used. Given the fact that two-block ADMM has been widely used in the last decade, it has continued to be the method of choice without much thought regarding the use of alternative multi-block ADMM options. For convex problems, they are supposed to give the same solution after all, with suitable choices of (dual) step sizes. However, a question that arises is whether in practice the two-block ADMM is comparable/competitive with multi-block ADMM solving an equivalent problem.\nWe studied this question in the context of optimization problems arising from multitask learning (MTL) with focus on modeling Alzheimer's disease (AD) progression, Parkinson's disease assessment, and air quality prediction. For all three problems, multi-block ADMM showed superior performance when compared to the two-block counterpart, both in terms of convergence stability and prediction power.\nAlthough multi-block ADMM requires further theoretical investigations, we surprisingly showed that in practice it is indeed a candidate to be considered when solving large convex optimization problems if using ADMM. Therefore, we suggest that researchers and practitioners to try out the equivalent multi-block versions of the two-block ADMM and then compare their performance.\nAdditional, both theoretical and empirical, results for multi-block ADMM are still necessary to precisely characterize the convergence and performance of multi-block ADMM, and these are the future research steps."}]