[{"section_title": "Introduction", "text": "The Last Glacial Maximum (LGM, 19 000-23 000 years before present; Mix et al., 2001) was the latest peak of cold climate with a global ice volume and an atmospheric carbon dioxide (CO 2 ) concentration distinctly different from the present-day values. Although the climatic forcing factors are relatively well known [Solomon et al., 2007], some aspects of the climate system during the LGM are still open to discussion. For example, it is uncertain whether the Atlantic meridional overturning circulation (AMOC) was weaker or stronger in the LGM than in the modern day climate [e.g., McCave et al., 1995;Yu et al., 1996;McManus et al., 2004;Curry and Oppo, 2005;Rutberg and Peacock, 2006;Otto-Bliesner et al., 2007;Negre et al., 2010;Muglia and Schmittner, 2015]. Typically, such a problem has been discussed from the viewpoint of paleoceanographic proxy records, numerical modeling, or both of them. Paul and Sch\u00e4fer-Neth [2003] incorporated the seasurface temperature reconstruction by the Glacial Atlantic Ocean Mapping (GLAMAP) project [Sarnthein et al., 2003] into the forcing fields for their ocean general circulation model (OGCM) and suggested that the AMOC during the LGM had a similar strength and depth to those of the modern counterpart. An accurate reconstruction of the LGM ocean circulation is fundamentally important to understand the LGM climate, not only because the ocean is an important component of the climate system due to its large storage and transport of heat, but also because a different ventilation of the deep ocean presumably led to a reorganization of the marine carbon cycle, thereby affecting the global climate through changes in the concentration of CO 2 in the atmosphere [e.g., Sigman and Boyle, 2000;Marchitto et al., 2007;Toggweiler, 2008;Kwon et al., 2011;Tschumi et al., 2011;Ritz et al., 2013;Schmittner and Somes, 2016]. The main goal of this study is an estimate of the circulation and water-mass distribution in the ocean state during the LGM that is statistically consistent, within errors, with observations. As a by-product, the ocean state estimate provides an un-biased, that is, based on model physics, interpolation (or mapping) of the tracer distribution, which is otherwise based on very few and scattered proxy-observations. We obtained the state estimate for the LGM ocean by synthesizing a state-of-the-art physical ocean model and several recent paleoceanographic proxy data compilations with a data assimilation technique called the adjoint method [e.g., Wunsch, 1996;Errico, 1997]. We also found a state estimate for the modern ocean with the same method, which served as a first guess for the LGM state estimation with regard to chemical tracers and as a reference state for the resulting LGM state estimate. Most of the previous data assimilation studies used simplified ocean models to reconstruct the paleo-circulation or water-mass distribution during the LGM [e.g., Legrand and Wunsch, 1995;Gebbie and Huybers, 2006;Huybers et al., 2007;Burke et al., 2011;Gebbie, 2014] and only a few state estimates for the LGM ocean were based on a general circulation model and the adjoint method [Winguth et al., 2000;Dail, 2012;Dail and Wunsch, 2014]. Compared to previous studies, our study constitutes a significant extension in terms of the total number of observations [cf. Winguth et al., 2000], the model domain [cf. Dail, 2012;Dail and Wunsch, 2014], and the length of model integrations [cf. Winguth et al., 2000;Dail, 2012;Dail and Wunsch, 2014]. 2 Data sets Table 1 and Figure 1 summarize the data sets we used in this study. In the following, we describe the data sets in further detail."}, {"section_title": "Modern data", "text": "In the context of this study, \"the modern ocean\" refers to the pre-industrial ocean state, because we want to discuss natural climate variability in terms of differences between glacial and interglacial periods. Although the anthropogenic effects should be excluded for that purpose, the further one goes back into the past, the fewer data are available. For the temperature and salinity data, therefore, we adopted monthly climatological data for the entire ocean from 1951 to 1980 from the World Ocean Database (WOD) [Locarnini et al., 2010;Antonov et al., 2010]. However, we excluded several regions from the domain of model-data comparison for our state estimation for the modern ocean (see Section 4.2). To reinforce the constraints by the temperature and salinity observations and to provide a basis for the first guess of the LGM state estimation, data for \u03b4 18 O water [Schmidt et al., 1999] and \u03b4 13 C DIC [Schmittner et al., 2013] were also incorporated. Following Dail [2012], near surface data of \u03b4 18 O water above 150 m depth were not included, because these data are affected by the seasonal cycle, which is not included in the oxygen isotope module of our circulation model. Similarly, the \u03b4 13 C DIC data above a depth of 1000 m were not used, because this depth range was excluded from the domain of the carbon isotope model (see Section 3). The \u03b4 13 C DIC data were based on the Global Data Analysis Project (GLODAP) and the Carbon Dioxide in the Atlantic Ocean (CARINA) data synthesis project [Schmittner et al., 2013]. Note that the estimated anthropogenic \u03b4 13 C contribution is substantially smaller than the prescribed data uncertainties of this study (see Section 4.2) in most parts of deep ocean below the depth of 1000 m [Schmittner et al., 2013]."}, {"section_title": "LGM data", "text": "The temperature of seawater not only is a good indicator of the past ocean state, but it also contributes to driving the ocean circulation through density differences. To date, the most comprehensive compilation of sea-surface temperature (SST) estimates for the LGM ocean including their uncertainties are provided by the MARGO project [MARGO Project Members et al., 2009]. Because the SST during the LGM cannot be measured directly, it was obtained from paleoceanographic proxy evidence like the assemblages of planktonic foraminifera. It is important to utilize information from the deep ocean as well, especially when we rely on sparse paleoceanographic proxy data [Kurahashi-Nakamura et al., 2014]. We used the isotopic compositions of seawater for oxygen and carbon as proxy data for the LGM deep ocean. The oxygen isotopic composition of seawater, often expressed as \u03b4 18 O water , is an inert passive tracer that reflects the transport by the ocean circulation once set at the surface. For the LGM, the oxygen isotopic composition of the shells of benthic foraminifera (\u03b4 18 O calcite ) is a proxy for \u03b4 18 O water , although it is influenced by the temperature of ambient seawater at the time of calcification. The most comprehensive published compilation of \u03b4 18 O calcite for the LGM consists of 180 records for the Atlantic Ocean [Marchal and Curry, 2008]. The carbon isotopic composition of the dissolved inorganic carbon of seawater (\u03b4 13 C DIC ) is a similarly useful indicator of ocean circulation, but it is also affected by a biogeochemical process (i.e., the remineralization of organic soft tissue). As in the case of oxygen isotopes, \u03b4 13 C DIC for the LGM is recorded in the shells of benthic foraminifera as \u03b4 13 C calcite . We adopted the database compiled by Hesse et al. [2011]. We used the recent compilations of isotopic composition data for the Indian and Pacific Oceans by Oliver et al. [2010] and Peterson et al. [2014] as an independent source of information to assess our state estimate (Section 8.1). Although these data sets would complement the Atlantic data [Marchal and Curry, 2008;Hesse et al., 2011], we did not include them in the adjoint-based state estimate of this study, because their coverage is very low in the Pacific and Indian Oceans (0.15% for \u03b4 18 O and 0.62% for \u03b4 13 C of the combined Indian and Pacific Oceans) and because they also would not substantially increase the data coverage in the Atlantic Ocean (from 2.3 to 3.2% for \u03b4 18 O and 3.2 to 4.0% for \u03b4 13 C). Very sparse data can lead to artefacts around singular data points in the solution, even though the adjoint method provides a dynamical state estimate with the aid of model physics. Further, we designed our experiments as an extension of Dail [2012] and Gebbie [2014] who both used the data sets by Marchal and Curry [2008] and Hesse et al. [2011] by different methods. Our choice to use the data of Oliver et al. [2010] and Peterson et al. [2014] only for independent observations maximizes the comparability with Dail [2012] and Gebbie [2014]."}, {"section_title": "Ocean and tracer models", "text": "We employed the Massachusetts Institute of Technology general circulation model (MITgcm). It was configured to solve the Boussinesq, hydrostatic Navier-Stokes equations [Marshall et al., 1997]. Subgrid-scale mixing was parameterized [Gent and McWilliams, 1990]. A dynamic-thermodynamic sea-ice model was coupled to the ocean model [Losch et al., 2010]. We used a cubed-sphere grid system that avoided converging grid lines and pole singularities [Adcroft et al., 2004] and had six faces with 32 \u00d7 32 horizontal grid cells and 15 vertical layers, respectively. The MITgcm was particularly designed for ocean state estimation projects [e.g., Stammer et al., 2002;Wunsch and Heimbach, 2006;K\u00f6hl and Stammer, 2008;Forget et al., 2015;K\u00f6hl, 2015]. For that purpose, the computer code can be differentiated by automatic differentiation (AD) using the source-to-source compiler TAF [Giering and Kaminski, 1998;Heimbach et al., 2005] to generate exact and efficient \"adjoint\" model code. To simulate \u03b4 18 O water and \u03b4 13 C DIC , we adopted highly simplified models similar to previous data assimilation studies [Marchal and Curry, 2008;Dail, 2012]. The oxygen isotopic composition of seawater \u03b4 18 O water was treated as a passive and conservative tracer with a fixed boundary condition at a certain depth level (i.e., by prescribing a Dirichlet boundary condition at a depth of 150 m). The surface ocean shallower than 150 m is not part of the model domain for \u03b4 18 O water (also see Section 2.1). The carbon isotopic composition of the dissolved inorganic carbon \u03b4 13 C DIC was treated in a similar way, but there were two differences due to the additional process of remineralization of organic carbon. First, the decomposed, isotopically \"lighter\" organic carbon affects the isotopic composition of DIC. Accordingly, we added a source term to the conservation equation for \u03b4 13 C DIC : where \u03b1 is a factor to control the magnitude of the remineralization effect, \u03bb is a standard relaxation timescale, \u03b4 13 C org is the \u03b4 13 C value of the organic carbon, and \u03b4 13 C DIC is the in-situ \u03b4 13 C value of DIC. Assuming an uniform amount of decomposed organic carbon below a depth of 1000 m, the reference value of \u03bb was determined from the modern amount of total remineralization in the global ocean deeper than 1000 m [del Giorgio and Duarte, 2002] and the volume of the corresponding water body, so that 1/\u03bb = 0.6 \u00d7 10 \u22124 yr \u22121 [Marchal and Curry, 2008]. That value corresponded to the central value of the original estimate with an uncertainty of \u00b110% [del Giorgio and Duarte, 2002]. The value of \u03b4 13 C org was assumed to be \u221220% [Goericke and Fry, 1994]. The second difference from the treatment of \u03b4 18 O water was that we excluded the depths shallower than 1000 m from the model domain. Accordingly, a Dirichlet boundary condition was imposed at a depth of 1000 m. The remineralization rate is much higher and highly depth-dependent at these depths [e.g., Yamanaka and Tajika, 1996], hence the model was not suitable for this depth range."}, {"section_title": "State estimation procedures", "text": ""}, {"section_title": "Common framework", "text": "The quality of the model optimization or state estimation was quantified by introducing an objective function J: where J data was the model-data misfit, J SSH was a penalty term for the drift of the globalmean sea-surface height (SSH), and J ctrl imposed penalties for the deviations of control variables from their first-guess values and hence represents a regularization of the inverse problem. Here, control variables mean variables that are adjusted to improve the modeldata misfit. The first term of Equation 2 was formulated as follows: where X was the index showing the category of data, X model was the model results for that category, X obs was the corresponding observation, and W X was the weight matrix that consisted of the inverse of the error covariances. For simplicity and because estimating actual covariances is difficult, we followed a common practice in previous applications of the adjoint method [Stammer et al., 2002;K\u00f6hl et al., 2007;Dail and Wunsch, 2014] and assumed that all errors are uncorrelated. The second term of Equation 2 became where W ssh was a weight factor and SSH 1 and SSH 0 were the final and initial values of the global-mean sea-surface height, respectively. The term J SSH is an implicit constraint on the global-mean salinity, because it regulates the total volume of sea water. In particular, for the LGM state estimation that did not have explicit salinity data, it was the only constraint on salinity. The weight factor W ssh was the inverse of the square of an assumed allowed deviation (0.1 m). However, in order to ensure a sufficiently small SSH drift, a 10 4 -times greater weight (i.e., stricter constraint) was given to J SSH if the absolute value of the drift exceeded two standard deviations. The last term of Equation 2 was taken as: where N data was the number of model-data pairs, N ctrl the number of control variables, T 0 the initial temperature field, S 0 the initial salinity field, F the atmospheric forcings, i indices indicating the kind of forcings, K the vertical diffusion coefficient, O 0 the initial \u03b4 18 O water field, C 0 the initial \u03b4 13 C DIC field, W the weight matrices for each quantity, and W \u03b1 was the weight factor for the penalty for \u03b1. The superscript \"adj\" meant adjusted values (i.e., values in the current iteration), and \"1st\" meant first-guess values (i.e., values in the first iteration). The weight matrices were the inverse of the error covariances. The prior uncertainties assumed for the 200-year or longer state estimations are summarized in Table 2. For W \u03b1 , however, a greater penalty was given for deviations larger than two standard deviations as in the W ssh case, because we were not able to evaluate the consistency between \u03b1 adj and biogeochemical processes that were not explicitly included in our simplified model. The factor N data /N ctrl was only used for the LGM state estimation to balance J ctrl and J data , because N ctrl was much larger than N data for the LGM. Without this factor, the model did not move away from the first guess during optimization. It should be noted that J ctrl = 0 (i.e., no adjustment) at the beginning of the state estimation procedure and that it generally increases with the progress of optimization. The balancing factor N data /N ctrl served to compensate for the increase in J ctrl with a reduction in J data of a comparable magnitude, which allowed for a decrease of the total cost (J). We sought a model ocean that corresponded to the minimum value of the objective function, and we assumed that such an optimized model ocean provided the best estimate for the ocean state. The adjoint method was used to calculate the gradient of the objective function with respect to the selected control variables (i.e., model inputs and parameters that determine the model state), hence, the direction to its minimum. With this information one can iteratively approach the optimized state by incrementally improving the control variables; in our case this was accomplished with a quasi-Newton algorithm [Gilbert and Lemar\u00e9chal, 1989]. For the state estimation with the adjoint method, we needed to prepare a starting point of the iterative search (first guess). It is desirable to have as good a first guess as possible for a successful estimation process. This should be emphasized especially when a long state estimation (i.e., a state estimation with longer forward simulations) is made, because in such a case the model state can show a large drift that may cause great difficulties with the adjoint method [Dail, 2012]. Therefore, although we aimed at a state estimation of 200 years or longer, we started out from a 20-year preparatory estimation only for the physical ocean state, followed by a gradual extension of the estimation length with the \"carry-over\" technique suggested by Dail [2012]. The problem was further preconditioned and regularized by normalizing every control variable according to the characteristic scale of each variable, which typically has a scale similar to the uncertainties, so that we could adjust all control variables in a bal-anced manner. The corresponding fields were smoothed with a 9-point spatial smoothing filter."}, {"section_title": "Modern state estimation", "text": "The first guess fields of the physical ocean state (i.e., for the preparatory phase) was the reference state of Kurahashi-Nakamura et al. [2014], which was simulated by the MITgcm driven by external atmospheric forcing fields based on the protocol of the Coordinated Ocean-ice Reference Experiments (COREs) project [Griffies et al., 2009]. To calculate J data for the modern state estimation, we took the difference between the model results that were averaged over the last 10 years of a model integration and the observations (Equation 3). While the time-averaged model results for \u03b4 18 O and \u03b4 13 C did not take into account seasonal changes, for temperature and salinity we took the difference between the monthly-mean model results and the corresponding monthly-mean data. We determined the prior uncertainties of the temperature and salinity data from the standard error of the monthly mean, although 1 K and 0.1 psu were added considering uncertainties due to data representativeness and model errors. For example, the observations are scattered with respect to time and do not cover the time window evenly, the pattern of the scatter is different from grid cell to grid cell and the spatial resolution of the model would be too coarse to accurately capture the observations that reflected smaller-scale processes. Moreover, the climatological data, which resulted from interannually varying atmospheric forcing, could not be reproduced accurately, because our model was driven by purely periodic atmospheric forcing. Because it was beyond the scope of this study to obtain precise values for these uncertainties, they were determined in an ad-hoc way. However, the overall uncertainties had a magnitude similar to those used in another data assimilation study [Gebbie, 2014] that was based on the World Ocean Circulation Experiment (WOCE) climatology [Gouretski and Koltermann, 2004]. In this study's framework of data assimilation, the adjustment of atmospheric forcings was controlled to avoid too large deviation from the first-guess values. The deviation, however, was only assessed by a single scalar number in the penalty terms (Eq. 5) of the objective function, hence it does not necessarily guarantee that the adjusted atmospheric forcings are within a reasonable range in every spatial and temporal location. Actually, in a preliminary state estimation for the modern ocean assimilating all available data from the WOD (see Appendix), the adjusted atmospheric forcings assumed physically unreasonable values (e.g., negative precipitation, negative specific humidity, too low air temperature, and so on) in several regions, although the lumped penalty terms had acceptable values. We took three measures against this problem of the preceding run. First, to remove quasi-isolated grid cells along coasts that did not have sufficient communication with the open ocean, we slightly modified the model bathymetry; otherwise, such grid cells can have unreasonable tracer concentration (e.g., too high salinity). Second, we excluded regions having unreasonable atmospheric forcings (see Appendix for the definition) from the domain of model-data comparison with regard to the temperature and salinity. Practically, we gave zero weights for J data to any temperature and salinity data located in such regions, which implicitly assumed that the prior uncertainties were actually much larger than the prescribed values in those regions. As a result, the data coverage of the modern temperature and salinity data was reduced by 21% from the original one (also see Appendix). Third, because the unreasonable atmospheric forcings were not eliminated completely even with the reduced data sets, we set an upper and lower bound for air temperature, specific humidity, and precipitation, and forcibly replaced any remaining unreasonable forcings with those values for each forward model run in the iterative searching. These alterations enabled us to obtain a more plausible ocean state consistent with reasonable inputs from the atmosphere. For the geochemical tracers, the first-guess initial and boundary conditions of \u03b4 18 O water and \u03b4 13 C DIC for the 200-year estimation were the optimized states given by the preceding state estimation with the original (i.e., unreduced) WOD data sets. The uncertainties for \u03b4 18 O water and \u03b4 13 C DIC data were assumed to be 0.2% [Dail, 2012;Gebbie, 2014]. They also implicitly contained various sources of uncertainties such as sampling error, time variation of the tracers and errors due to the highly-simplified geochemical tracer model [Dail, 2012].\nThe improvement of model-data misfits for the modern state estimations are summarized in Table 3. For the physical tracers (i.e., temperature and salinity), a substantial amount of the cost decrease was achieved in the preparatory phase. With the help of the improved first guess, a further decrease led to acceptable normalized costs (1.5 for the temperature and 2.4 for the salinity) in MOD200. Without the preparatory phase, 200year state estimations were hardly able to generate a decrease of costs (not shown). The indices FW2\u03c3 and FW1\u03c3 for temperature after the optimization were 94% and 72%, respectively. Combined with the normalized cost, these values indicate that the model was successfully fitted to the temperature data. The results for salinity (87% and 65%) were somewhat worse than for temperature, although the optimized value of FW1\u03c3 nearly met the requirement. The synthesis of the geochemical tracers in MOD200 was as good as for the physical tracers. For the \u03b4 18 O component, although the optimized value of J \u2032 O18 was somewhat larger than one, the indices FW2\u03c3 and FW1\u03c3 met the requirements, suggesting that the synthesis of the \u03b4 18 O data was largely successful. For \u03b4 13 C, all the three measures were improved through the state estimation scheme, closely approaching the required values. Lastly, the SSH drift after the optimization was 7.4 \u00d7 10 \u22122 m/200 yr to give J \u2032 SSH = 0.55, which was well below the allowed magnitude, which indicated that the penalty term in the objective function was actually effective in minimizing the SSH changes. Similarly, J \u2032 ctrl was smaller than one, which showed that the magnitude of the control-variable adjustments was acceptable. We also needed to ensure that the total adjustment of control variables including the preparatory phase was not excessively large. To assess the deviation from the original first-guess values (i.e., the values before the preparatory runs), we calculated the normalized total adjustments assuming the total uncertainties for the air temperature, the downward shortwave radiation and the wind velocity components to be 10 K, 10 W/m 2 , and 1.0 m/s, respectively. It yielded the normalized adjustment of 0.1, which showed that the adjustments were in a reasonable range. Using the carry-over technique again, we extended the 200-year state estimation to a 400-year estimation (MOD400). Although a dramatic further improvement of the modeldata misfit did not occur for the physical tracers, at least we could keep almost as good results as those in MOD200, so that we were able to show that our results were robust with regard to the increased length of the state estimation. The model-data misfits for \u03b4 18 O was improved not only compared to the first guess, but also compared to MOD200, indicating a successful synthesis of those data. On the other hand, the model-data misfits for \u03b4 13 C were slightly worse than in MOD200 at the expense of the longer run ensuring more equilibrated tracer distribution. On the whole, the optimized normalized-cost in MOD400 was somewhat higher than one for all our measures of model-data misfit. However, it is known that the optimization with the adjoint method does not always yield a normalized cost for the model-data misfit as small as one [e.g., K\u00f6hl et al., 2007]. Furthermore, J \u2032 SSH and J \u2032 ctrl were readily within the allowed ranges."}, {"section_title": "LGM state estimation", "text": "Similarly to the modern case, the model results were averaged over the last 10 years and compared with the reconstructions by the MARGO project to calculate the modeldata misfits for the SST. We used prior data errors derived from the uncertainty estimated for each individual data point by the MARGO project [MARGO Project Members et al., 2009]. MARGO uncertainty estimates are conservative and meant to give an upper bound. For the model-data comparison of the geochemical tracers, we calculated \u03b4 18 O calcite from \u03b4 18 O water as a function of seawater temperature (T) [Marchal and Curry, 2008]: \u03b4 13 C calcite was obtained for \u03b4 13 C model-data comparison from the following conversion [Marchal and Curry, 2008]: For both \u03b4 18 O and \u03b4 13 C, a prior error of 0.2% was assumed for the data uncertainty according to Marchal and Curry [2008] and Dail [2012], that is, the same value as for the modern ocean case. The original first guess of the physical ocean state for the LGM was made by spinning up the model forward in time for 2000 model years with climatological monthlymean atmospheric forcing fields derived from an LGM simulation with the comprehensive Community Climate System Model Version 3 (CCSM3) [Merkel et al., 2010]. The LGM bathymetry was created by remapping of the ICE-5G topography [Peltier, 2004] onto our model grid. The initial temperature and salinity states were taken from modern observations [Levitus, 1982], although a 1.1% offset was added to the salinity to take into account the mean salinity change due to the lower sea level during the LGM [Adkins and Schrag, 2001]. This original first guess was improved by a sequence of preparatory state estimations of up to 100 years. We used the improved first guess to initialize a 400-year state estimation that was the major achievement of this study. The first-guess initial and boundary conditions of \u03b4 18 O water for the 200-year state estimation were separately prepared by adding a constant offset of 1.1% [Duplessy et al., 2002] to the modern distribution given by the experiment shown in the Appendix. Similarly, the first-guess initial and boundary conditions of \u03b4 13 C DIC were constructed by adding 0.15% uniformly for depths shallower than 1500 m and by subtracting 0.6% for depths deeper than 2000 m according to the average difference between the late Holocene and the LGM [Hesse et al., 2011].\nThe improvement of the model-data misfits for the LGM state estimations are summarized in Table 4. Both in LGM200 and LGM400, all three indicators for SST show that the optimized model was statistically consistent with the SST data. In fact, the model was already in accordance with the data at the initial state of LGM200 as a result of the preparatory estimation. Similarly to the modern case, the preparatory phase greatly helped to carry out the 200-year state estimation without severe problems. For the \u03b4 18 O component, the FW2\u03c3 and FW1\u03c3 indices were well improved and perfectly met the require-ments, although the optimized cost was somewhat larger than one. It is suggested that the synthesis of the \u03b4 18 O data was basically successful, but that the spread of model results around the corresponding data deviated from a perfect normal distribution. On the other hand, the fit to the \u03b4 13 C calcite data was somewhat worse than for the SST and \u03b4 18 O. Although J \u2032 C13 was substantially reduced, it was still almost twice as large as required. Contrary to the \u03b4 18 O case, the optimized FW2\u03c3 and FW1\u03c3 indices were also insufficient. Those three indicators became even worse during the 1st-guess run for LGM400, because a 200-year forward run was too short to reach a steady state and the remineralization factor (\u03b1) optimized for the 200-year run was not suitable for a 400-year run. During LGM400, however, the model-data misfits greatly improved. The optimized J \u2032 C13 of 2.1 corresponded to 0.29% in terms of a root-mean-square (RMS) of model-data discrepancies. Actually, if we took \u03c3 = 0.29% to calculate FW2\u03c3 and FW1\u03c3, they became 98% and 72%, respectively. The J \u2032 SSH value was dramatically reduced from its first-guess in LGM200, but, although the first-guess J \u2032 SSH was below one in LGM400, the optimized value became larger than one in exchange for the improvement for \u03b4 13 C. However, it was still within the 2\u03c3 range. J \u2032 ctrl was well below one, showing that the magnitude of the control-variable adjustments was acceptable. Similarly to the modern case, to assure that the total adjustment of the control variables including the preparatory phase was not too large, we assessed the deviation from the original first-guess values taken from Merkel et al. [2010] (i.e., the values of F(i) before the preparatory runs). The normalized total adjustment was equal to 0.008, which implied that the model required much smaller control-variable adjustments to meet the much scarcer data coverage (i.e., much weaker constraint) as compared to the modern state estimation."}, {"section_title": "Experimental design", "text": "The main part of this paper describes four state estimations: two for the modern day, and two for the LGM. In both cases, a 200-year state estimation was carried over to a 400-year state estimation (i.e., a 400-year model run for each iteration). The 200-year state estimation for the modern day and for the LGM are called MOD200 and LGM200, and the 400-year estimation MOD400 and LGM400, respectively. Considering the equilibrium time scales for tracer distributions in the global ocean [Wunsch and Heimbach, 2008], 200 years would be too short to reach consistency between the surface boundary conditions and the physical and geochemical tracer distributions in the deep ocean, and thus the thermohaline circulation. This motivated us to conduct longer state estimations (i.e., state estimations with longer forward simulations). However, longer state estimations cause greater computational costs and can be difficult to achieve because of the potentially unreasonably large drift (see Section 4.1). Therefore, we adopted 400 years as the length of the extended state estimations. Although it was still not long enough compared to the equilibrium time scale especially for the very deep ocean in the South Atlantic and Southern Ocean [Wunsch and Heimbach, 2008], it led to the longest adjoint-based ocean state estimation for the LGM that has been achieved to date. The 400-year state estimations (\"MOD400\" for the modern day and \"LGM400\" for the LGM) were initialized from the optimized state obtained by MOD200 and LGM200, respectively. For the LGM, we assumed \u221a 10-times smaller uncertainties of the control variables in the penalty terms than those for LGM200, which corresponded to 10-times larger weights, because the control variables had been already improved in LGM200. That was also helpful to stabilize the longer adjoint run. For the modern case, however, we kept the same uncertainties for MOD400, because we judged that the improvement in MOD200 was not enough especially for the temperature and salinity distributions and that considerable adjustments of the control variables was still necessary. As a general behavior of our state estimation, a rapid reduction of the total cost (J) occurred at an early stage of the iterative procedure, followed by a phase of a very slow reduction seemingly approaching a non-zero limit value. Because it was then highly timeconsuming to obtain a substantial further decrease, the iterative search was stopped when J was reduced by less than 1% during the last ten simulations. 5 Evaluation of model-data misfit"}, {"section_title": "Measures of model-data misfit", "text": "To quantify and evaluate how well the model was optimized in terms of the agreement with observation, several measures of fit were introduced and used throughout this study. The most straightforward way is to follow the development of the objective function. In particular, it is practical to observe the normalized total cost (J \u2032 ), namely, the normalized root-mean-square misfit [e.g., K\u00f6hl et al., 2007;Dail, 2012;Kurahashi-Nakamura et al., 2014]. This idea is based on the principles of the \u03c7-squared test. When a proper model-data agreement is achieved, the normalized cost (i.e., objective function divided by the number of model-data comparisons) should be of the order of one, which implies that the model is statistically consistent with the data within the prior uncertainty. A similar idea was applicable to the two penalty costs (J SSH and J ctrl ). The normalization made no difference for J SSH , because it only had one comparison term. As to J ctrl , the normalized cost was calculated by dividing the original J ctrl by the total number of control variables and also by N data /N ctrl (cf. Equation (5)). The magnitude of the normalized control-variable adjustments should be smaller than one if the adjustments stay within the assumed allowable deviations from the first-guess values. Further indices that indicate the quality of the model-data match would be a \"fraction within 2\u03c3 and 1\u03c3\" (hereafter, FW2\u03c3 and FW1\u03c3). For grid cells with any data points, the ratio of model results that are within the range of \u00b12\u03c3 (\u00b11\u03c3) of the corresponding data should be around 95% (68%), if the model results were considered to be statistically consistent with the data within random noise."}, {"section_title": "Reconstructed tracer fields", "text": "The globally-averaged difference of SST (LGM400 minus MOD400) was \u22122.2 K. As suggested by the normalized cost, the estimated global SST field in LGM400 was generally in good agreement with the MARGO paleo-data (Figure 2a). Relatively larger differences were found in the upwelling regions along the west coasts of the South American and African continents probably caused by the poor representation of the coastal upwelling phenomenon in our coarse-resolution model. Compared to the reconstructed modern SST field in MOD400, the LGM SST was lower in most regions. In particular, it was colder by up to 10 degrees in the mid-latitude (30 \u2022 -60 \u2022 ) bands of both hemispheres (Figure 2b). In some regions, however, the LGM SST was higher than the modern one. Most of the positive temperature anomalies along the latitude of 30 \u2022 S are caused by the positive anomalies in the data themselves [MARGO Project Members et al., 2009;Annan and Hargreaves, 2013]. Also in the North Atlantic, there was one region with positive anomaly near data points from an alkenone-based SST reconstruction. The RMS of model-data discrepancies for the modern \u03b4 18 O water was 0.24% , and 0.25% for the LGM \u03b4 18 O calcite . The reconstructed modern distribution of \u03b4 18 O water (Figure 3a) showed a water mass with positive \u03b4 18 O water values that can be identified with North Atlantic Deep Water (NADW) spread southward up to 45 \u2022 S at a depth of 2000-3000 m, while another water mass corresponding to Antarctic Intermediate Water penetrated northward to the equator at a depth of about 1000 m. The optimized LGM \u03b4 18 O water distribution ( Figure 3b) was obtained from the fitting the model to the \u03b4 18 O calcite data (Figure 3c). For a direct comparison to Figure 3a, the color scale was adjusted by taking the 1.1% offset into account (see Section 4.3). In LGM400, the core of northern-source deep water was shallower by \u223c1000 m than in MOD400. The northward penetration of southern-source water, on the other hand, was weaker both for the bottom water and for the intermediate water, suggesting stronger stratification of the Southern Ocean. In MOD400, the estimated \u03b4 13 C DIC field had an RMS of model-data discrepancies of 0.27% , whereas that for \u03b4 13 C calcite was 0.29% in LGM400. The optimized LGM \u03b4 13 C DIC field ( Figure 4b) was obtained from the fitting to the \u03b4 13 C calcite data (Figure 4c). The main deviations from the modern distribution included the southern-source water having a very low end-member value, a larger vertical gradient in the depth range of 1000-5000 m, and a less pronounced tongue of northern-source water. A prominent discrepancy between the modern estimate and the data was the too low \u03b4 13 C DIC in the bottom water of the estimate. A likely reason for this underestimation is the highly-simplified remineralization model of Equation 1 that assumed a homogeneous amount of remineralization. In reality, the amount of remineralization decreases with depth [e.g., Yamanaka and Tajika, 1996;Gebbie, 2014]. Because the remineralization releases isotopically depleted \u03b4 13 C to the sea water, the assumption of homogeneous remineralization would indeed correspond to supplying too much low \u03b4 13 C to the bottom water, hence causing too low \u03b4 13 C DIC at such depths. In contrast to MOD400, the reconstructed LGM \u03b4 13 C DIC for the bottom water were not too light as compared to the reconstructions in spite of the same simplified remineralization model. One possible reason was that, for the LGM, there were only a few data points that constrained the shallower part (1000-2000 m) of the water column in the tropical region and the Southern Hemisphere, and hence the model adjustment focused on the data in the bottom water."}, {"section_title": "Reconstructed circulation and water-mass distribution", "text": "The volume transport of the AMOC in LGM400 as indicated by the maximum of the streamfunction ( Figure 5) was 21.3 Sv, which is 32% stronger than in MOD400. The rate of southward deep water export at the equator or at 30 \u2022 S was also substantially larger in the LGM than in the modern case. To visualize the distribution of water masses more clearly, a passive dye tracer was released at the sea surface in the high-latitude Northern Atlantic (cf. Gebbie [2014]) in additional forward model runs with the optimized atmospheric forcings and parameters obtained in MOD400 or LGM400. The concentration of the \"dye\" was fixed at 1 for every surface grid cell from 50 \u2022 N to 80 \u2022 N in the northern North Atlantic. To ensure a quasisteady state of the tracer distribution, the model was run for 2000 model years by repeating the 400-year forcings five times. The dye concentration showed that the northern-source deep water occupied shallower depths in the LGM than in the modern case ( Figure 6). In the modern case, the core of the northern-source deep water was located at a depth of \u223c2500 m, while in the LGM case it was at a depth between 1500-2000 m. Since the maximum of \u03b4 13 C DIC was found in the same depth range (Figure 4b), this water mass may be identified with the 13 C-rich intermediate water that was postulated by Duplessy et al. [1988] and Curry and Oppo [2005] and called Glacial North Atlantic Intermediate Water (GNAIW). Focusing on the 0.5 contour lines, the dye concentration would be larger than 0.5 between approximately 1000 and 4500 m in the modern case and between 1000 and 3500 m in the LGM case. The shoaling of the northern-source water also affected the upper ocean (depths shallower than 1000 m). For example, while a dye concentration of 0.2 was observed at a depth of 700-1000 m in the modern case, it occurred at a depth of 500 m or even shallower in the LGM case. This feature would be consistent with a substantially weaker southern-source intermediate water in the LGM case as indicated in Figures 3b and 4b. Although the AMOC streamfunction showed a thicker and much stronger cell with anti-clockwise circulation in the deepest part of the Atlantic (Figure 5), the dye concentration in the LGM bottom water was as high as in the modern bottom water (0.3-0.4), which suggests that one can not easily infer the relative contributions of northern-and southern-source water from the zonally integrated streamfunction only."}, {"section_title": "Discussion", "text": ""}, {"section_title": "Comparison with previous studies", "text": ""}, {"section_title": "Sea surface temperature", "text": "The globally-averaged SST difference between LGM400 and MOD400 was also consistent with the anomaly estimate by the CLIMAP project [Climap Project Members, 1976], although nowadays the CLIMAP anomaly is considered to be too small [e.g., Crowley, 2000]. By averaging over all 5 \u2022 \u00d7 5 \u2022 grid cells that contain data, MARGO Project Members et al. [2009] estimated the globally-averaged SST anomaly between the LGM and modern ocean as (\u22121.9\u00b11.8) K. The estimates based on our study (LGM400 minus MOD400) agreed with those by MARGO Project Members et al. [2009] within the error bars, for the global as well as the regional anomalies (Table 5). However, there seems to be a systematic difference in that our estimates tend to imply a slightly larger glacial cooling. This tendency was most pronounced in the tropics, where our estimate for the Pacific was statistically distinguishable from the MARGO estimate. In our case, the data assimilation scheme using an ocean general circulation model compensated for the sparseness of the data and provided for a physics-based method of interpolation and extrapolation. In the case of MARGO Project Members et al. [2009], extrapolation into areas void of data was avoided on purpose, but as a consequence their estimates of the global and regional SST anomalies suffer from the low grid coverage (which is \u223c20% for the global ocean). The global mean anomaly of surface air temperature was \u22125.8 K (\u22124.2 K over the ocean) in our estimates, which was somewhat larger than the recent estimate of (\u22124.0 \u00b1 0.8) K by Annan and Hargreaves [2013]. The corresponding anomaly for the first-guess forcing fields between the modern and LGM state estimation was already \u22125.8 K. This implies that the state estimation did not require large adjustments of surface air temperature in extensive areas to fit the model to the sparse ocean data."}, {"section_title": "Isotopic composition fields", "text": "The reconstructed modern distribution of \u03b4 18 O water ( Figure 3a) showed a good agreement with a gridded data set based on regional \u03b4 18 O-salinity relationships and an objective interpolation method [ Figure 2 in LeGrande and Schmidt, 2006]. Because through the adjoint method our reconstruction was based on the physics of ocean dynamics, it did not suffer from the sharp discontinuities along regional boundaries that are unavoidable in the method devised by LeGrande and Schmidt [2006]. Instead, the constraint by model physics made it more difficult for the model to fit the data. As a result, the RMS of model-data discrepancies in our estimates was somewhat larger than in the study by LeGrande and Schmidt [2006]. The reconstructed LGM \u03b4 18 O water could be directly compared with the oxygenisotope composition of seawater restored from pore water samples from sea-floor sediments [Adkins et al., 2002], although the \u03b4 18 O water may not be determined uniquely from the pore water [Wunsch, 2016]. Our result for the Southern Hemisphere (ODP site 1093) agrees with the pore-water value within the errors, whereas our estimates for the Northern Hemisphere were considerably higher than the observations (Table 6). In the framework of this study, the assimilation of \u03b4 18 O information was done in terms of \u03b4 18 O calcite . Because \u03b4 18 O calcite depends on both \u03b4 18 O water and temperature of the ambient sea water, the \u03b4 18 O calcite information alone does not uniquely determine \u03b4 18 O water . The comparison with the pore-water observation implied that in LGM400 the model adjusted \u03b4 18 O water too much instead of adjusting sea-water temperature, although that was the best estimate with respect to the chosen objective function. Another potential problem could be the limited length of the forward runs (400 year) that is still not long enough to advect the Northern Hemisphere information to the Southern Hemisphere [Wunsch and Heimbach, 2008]. Thus, the short integration time compared to the advective time scales of \u03b4 18 O water could have been compensated by unreasonably large adjustment of \u03b4 18 O water values in the Northern Hemisphere. The reconstructed modern \u03b4 13 C DIC field was consistent with previous studies [Curry and Oppo, 2005;Schmittner et al., 2013;Gebbie, 2014] in the Atlantic (Figure 4a), at least with respect to the contour lines of 0.8% or 1.0% reaching up to \u223c40 \u2022 S at a depth of 2000-3000 m. The LGM \u03b4 13 C DIC distribution also agreed in general with previous studies' reconstruction [Curry and Oppo, 2005;Brovkin et al., 2007;Gebbie, 2014]. We note again that our estimate was based on both the physical ocean dynamics and the available proxy data. Although the RMS of model-data discrepancies of 0.29% was somewhat larger than the assumed data uncertainty (0.2% ), it was clearly smaller than the 0.68% reported by Hesse et al. [2011], which demonstrates the beneficial effect of the data assimilation in the current study. The reconstructed isotopic composition fields for the LGM were also compared with independent data compilations for the global ocean including the Pacific and Indian Oceans [Oliver et al., 2010;Peterson et al., 2014] (Figure 7). The \u03b4 18 O data from the data sets are associated with large uncertainties in the dating up to 10 kyr [Oliver et al., 2010]. Taking the Pacific and Indian data into account for the LGM400 results leads to J \u2032 O18 = 2.4 and J \u2032 C13 = 6.3 , which correspond to the RMS values of model-data differences of 0.31% and 0.50% , respectively. These cost-function values suggest that the reconstructed isotopic composition fields do not fit the observations within prior errors, even though the model-data misfits in the Pacific and Indian Oceans were slightly improved in the optimization that used only the Atlantic data. More precisely, on the one hand, the LGM400 \u03b4 13 C field fit the measured \u03b4 13 C values in the Indian Ocean reasonably well, because of a good first guess and because the Atlantic data lead to improvement in the Indian Ocean. On the other hand, the LGM400 \u03b4 18 O values in the Pacific are systematically lower than the measured \u03b4 18 O values of Oliver et al. [2010]. Our estimated \u03b4 13 C values in LGM400 are also substantially lower (as much as 1% ) at 3000 m in the Pacific Basin than the measured data. A 400 year (forward) simulation is not long enough for signals from the Atlantic Ocean to reach all parts of the global ocean. This hypothesis was tested by running the model for 3000 extra years forward in time with the optimized forcings and parameters of LGM400. On the one hand, the longer integration clearly improved the negative bias for \u03b4 18 O (Figure 7c, g), calling for an even longer adjoint-based state estimate. On the other hand, the 3000-year model run did not reduce the \u03b4 13 C bias in most parts of the Pacific and Indian Oceans (Figure 7d, h) to suggest that, even though signals from the Atlantic are allowed to propagate into the Pacific, the deep ocean circulation and remineralization processes in the Pacific can not entirely be constrained by processes in the Atlantic. During the LGM, the water-mass structure and circulation in the deep Pacific Ocean may have been different from today because of deep-water formation in the North Pacific Ocean [e.g., Matsumoto et al., 2002;Herguera et al., 2010;Rae et al., 2014]. Such processes in the North Pacific Ocean would have been independent from the Atlantic Ocean and can only be constrained with a sufficient amount of local data. In the worst case, there is a considerable impact of North Pacific deep water on the global circulation, in which case our global state estimate would be very inaccurate in the absence of data to constrain the North Pacific Ocean locally. In future estimates, more Pacific data would be desirable and a remineralization model that depends on the oceanographic provinces should be included to improve the model-data fit."}, {"section_title": "Atlantic meridional overturning circulation", "text": "The stronger AMOC in LGM400 is in line with independent evidence from paleodata that were not assimilated in this study: 231 Pa/ 230 Th isotope ratios [Yu et al., 1996;Lippold et al., 2012]; grain-size analysis of ocean sediments [McCave et al., 1995;Manighetti and McCave, 1995;McCave and Hall, 2006]; and combined Cd/Ca and \u03b4 13 C measurements [Curry and Oppo, 2005]. In particular, the shoaled but more active overturning cell during the LGM is supported by combined proxies of the 231 Pa/ 230 Th ratio and Nd isotopes [Lippold et al., 2016]. But, there is also evidence that suggests that the LGM AMOC was weaker than today [e.g., Lynch-Stieglitz et al., 1999;Piotrowski et al., 2005;McManus et al., 2004;Negre et al., 2010]. Previous adjoint-based state estimations show a 30% weaker AMOC [Winguth et al., 2000] or a strength similar to the modern value [Dail, 2012]. Apart from the remaining ambiguity of the AMOC strength, the three adjointbased studies agree on the change in depth of the overturning circulation cell. For example, in this study, the shoaling of the AMOC from \u223c3500 m in MOD400 to \u223c2500 m in LGM400 was observed. Such a shoaling of the AMOC is also observed in the results of Winguth et al. [2000] and Dail [2012]. Stammer et al. [2016] show time-mean AMOC stream functions from 1960 to 2007 by six different data assimilation projects. Although the time window is not identical to that of our modern state estimate, four out of the six reconstructions clearly have a deeper NADW cell than our LGM reconstruction, and another four of them have a significantly weaker strength of NADW transport. The difference in maximum AMOC strength between MOD400 and LGM400 can be linked to the difference in average densities of two latitudinal strips (50\u221255 \u2022 N, and 35\u221240 \u2022 S) at mid-depth (750 m). The hemispheric density difference was 0.63 kg/m 3 in LGM400, while 0.45 kg/m 3 in MOD400, supporting the positive correlation between the AMOC strength and the meridional density gradient across the Atlantic as suggested by Rahmstorf [1996]. The larger north-south density gradient in LGM400 is due to the salinity difference as seen in Figure 8a. There are (at least) two mechanisms for that. First, the gyre circulation is stronger in LGM400 due to stronger wind stress. As a consequence, more salt is transported northward with the western boundary current in the North Atlantic, contributing the denser water in the convection regions. This is consistent with Muglia and Schmittner [2015] who suggest that strengthened wind-driven northward salt transport into the North Atlantic contributes to the increase of surface-water density at high latitudes, leading to the stronger and deeper AMOC in LGM simulations with the Paleoclimate Model Intercomparison Project Phase 3 (PMIP3) models. Second, in the reconstructed LGM ocean, a distinct positive anomaly of evaporation was found in the highlatitude North Atlantic because of a lower specific humidity and a slightly higher SST. It also contributes to the increase in the density of the surface water. Another potential mechanism leading to stronger AMOC during the LGM is the intensification of the overturning by increased tidal mixing caused by the sea-level drop [Wunsch, 2003;Egbert et al., 2004;Green et al., 2009;Schmittner et al., 2015]. Tidal energy, which at present is dissipated by friction on the shallow continental shelves, would during the LGM instead be dissipated in the deep ocean, because a substantial area of the continental shelves were exposed. For example, Schmittner et al. [2015] estimated that the global mean vertical diffusivity (used as input parameters to a climate model) during the LGM was more than 3 times larger than at present day. The prior vertical diffusivity was 3 \u00d7 10 \u22125 m 2 /s for all the experiments in our study, and rather small uncertainties were prescribed (Table 2) mostly because it aided stabilizing the searches for the optimum solution. The adjustment of the diffusivity was substantially smaller than the given uncertainty both for the modern and LGM state estimates, which suggested that the difference in ocean circulation resulted mostly from the different surface forcing fields. This result, however, does not necessarily lead to rejecting the hypothesis that a more vigorous vertical mixing affected the LGM ocean circulation. Instead, it suggests that a vertical dif-fusivity similar to the modern one is consistent with the LGM data and their uncertainty used in this study. We did another 200-yr LGM state estimate with larger prior vertical diffusivities: 3 \u00d7 10 \u22125 m 2 /s for the depths shallower than 1500 m and 3 \u00d7 10 \u22124 m 2 /s for deeper depths. We obtained as good a cost reduction as in LGM200, with a maximum AMOC strength of 19.8 Sv and a similar depth to that for LGM400. The fit to the LGM data was equally good, suggesting that the vertical diffusivity cannot be constrained better with our method."}, {"section_title": "Implication for the atmospheric pCO 2 level", "text": "The optimized remineralization factor \u03b1 was 0.823 in MOD400 and 1.08 in LGM400, so 30% larger for the LGM. Using process-based biogeochemical models for the LGM, Bopp et al. [2003]; Tagliabue et al. [2009]; Oka et al. [2011]; Schmittner and Somes [2016] suggest a slightly (from several % up to 10%) lower export production during the LGM. Although those estimates are apparently in contradiction with ours, it should be noted that \u03b1 meant the amount of remineralization in the ocean deeper than 1000 m, which depends on the decomposition efficiency of organic matter as well as on the export production. Considering that the lower sea-water temperature during the LGM would slow down the decomposition of organic matter [Matsumoto et al., 2007], the lower export production would be counteracted by the slower decomposition in the deep water. On the other hand, the reconstructed ocean state in LGM400 was more stratified in salinity and density (Figure 8). Several lines of independent evidence support this result [Adkins et al., 2002;Insua et al., 2014], although a recent study suggests that salinity amplification in the abyss during the LGM is not necessarily constrained by the data [Wunsch, 2016]. A more stratified ocean would be consistent with a larger carbon storage in the deep ocean, which would contribute to a lower pCO 2 in the atmosphere [e.g., Sigman and Boyle, 2000;Marchitto et al., 2007], in particular in conjunction with a larger volume of Antarctic Bottom Water (AABW) as shown in Brovkin et al. [2007]. Because our model did not include tracers such as O 2 and 14 C, we were not able to directly infer the ventilation rate of the deep ocean. However, the reconstructed LGM \u03b4 13 C DIC having a larger vertical gradient of concentration especially in the Southern Ocean (Figure 4) suggested a more isolated very deep or bottom water mass. This reduced ventilation combined with the increased remineralization could contribute to the lower atmospheric pCO 2 during the LGM. However, the higher AABW production rates estimated for the LGM that are implied by the maximum circulation rate ( Figure 5) may not support a pCO 2 drawdown hypothesis [De Boer and Hogg, 2014]. In fact, a reduction of the AABW production rate during the LGM was suggested to be able to account for the drawdown of pCO 2 [Toggweiler et al., 2006;de Boer et al., 2010]. Burke and Robinson [2012] argue that the observed depletion of radiocarbon in the Southern Ocean is consistent with a reduced deepocean ventilation during the LGM via the Southern Ocean, and suggest that carbon in the deep ocean was more isolated from the atmosphere than in modern days. On the other hand, the glacial water mass geometry reconstructed from geochemical tracers indicates that the southern source water mass occupied a larger volume fraction of deep water than today [e.g., Duplessy et al., 1988;Curry and Oppo, 2005]. Therefore, larger deep-water volumes of southern origin produced at a slower rate would be required for a consistent pCO 2 drawdown scenario, which implies that reduced vertical mixing between AABW and the northern source water mass would be required [De Boer and Hogg, 2014]. Reduced vertical mixing is also suggested by Lund et al. [2011] from the viewpoint of \u03b4 18 O distribution. Moreover, if the diapycnal mixing between NADW and AABW was smaller during the LGM, then the CO 2 may be favorably sequestered in the abyssal ocean [cf. Stephens and Keeling, 2000;Ferrari et al., 2014]. On the other hand, more tidal energy input to the deep ocean would contribute to an overall increase in vertical mixing as dis-cussed in Section 8.1.3 [Wunsch, 2003;Egbert et al., 2004;Green et al., 2009;Schmittner et al., 2015]. The actual magnitude of vertical mixing and its effect on the deep-and bottom-water ventilation are expected to depend on the distribution of water masses and their relative position to the bottom topography [Lund et al., 2011;Ferrari et al., 2014]. Therefore, in order to better contribute to the question of ventilation rates, one needs to determine the changes in spatial patterns of vertical mixing during the LGM by estimating the three-dimensional distribution of vertical diffusivity. Our method appears to be very well suited to address this question, but as mentioned in Section 8.1.3, the vertical diffusivity was not well constrained by the available data."}, {"section_title": "Uncertainty of the estimates", "text": "The adjoint-based state estimate provides a solution that is physically plausible and consistent with the assumed cost function and probability distribution. In the case of the LGM state estimate, the number of data was much smaller than the number of control variables of the model. Such mathematically underdetermined problems are ill-posed and do not have a unique solution, but our regularization term J ctrl resolves the issue at the cost of introducing a bias towards the first guess. The reconstructed LGM ocean in LGM400 was similar to the first guess by CCSM3 [Merkel et al., 2010] with regard to the depth of NADW cell and the strength of AABW cell, but was distinct with regard to the maximum strength of NADW cell (21.3 Sv in LGM400 vs. 12 Sv for CCSM3). To determine the degree of dependency on the choice of first guess, we would need to carry out a series of state estimates using different first-guess fields. However, we can at least conclude that the LGM ocean reconstructed from the CCSM3 first guess is consistent with the data sets in this study. Here, we are interested in four other aspects of the uncertainty of the estimated ocean circulation and water-mass distribution: 1. variability or sensitivity around a single local minimum of the objective function, 2. uniqueness of the minimum, that is to say, the possible existence of a different, global minimum, 3. the possibility of a different shape of the objective function near the minimum that would result from perturbed data, and 4. systematic model errors. Item 4 is beyond the scope of this paper, as it would require at least one more adjoint OGCM. For items 2 or 3, we would need a large number of additional adjoint simulations, which would be computationally too expensive. However, we were able to infer the uncertainty caused by item 1 as follows. We conducted several forward runs with random noise added to the optimized atmospheric forcings from LGM400. The noise was normally distributed with the following standard deviation: 1% for the air temperature (\u223c3 K) and 5% for other forcing fields. We prepared five sets of perturbed forcing fields to carry out five runs. The maximum AMOC strength in those runs was 21.3 Sv, 20.0 Sv, 20.4 Sv, 21.3 Sv and 21.0 Sv, and in all runs the depth of the GNAIW circulation cell was as shallow as in LGM400. The model-data misfits for the perturbed runs were naturally somewhat worse (several % larger) than the best estimate. These experiments give us some confidence that the estimated ocean state in LGM400 was sufficiently robust from the viewpoint of the sensitivity around the local minimum of the objective function."}, {"section_title": "Conclusions", "text": "Aiming at a physically plausible and reliable reconstruction of the LGM ocean state, an adjoint-based state estimation framework was developed based on state-of-the-art numerical models and proxy data. The framework enabled us to make maximal use of the available knowledge and data in an objective way and carry out the longest LGM adjoint simulations to date. The model-data misfit as formulated in terms of the objective function was successfully minimized in order to provide an LGM ocean state supported both by ocean dynamics and observations. This suggests that the various proxy data of differ-ent origin were compatible with each other within their uncertainties in the sense that they could be tied together by the physical and biogeochemical processes in the model. Compared to the modern ocean state estimated with the same method, the reconstructed LGM ocean state was characterized by a larger rate of the AMOC, a northern-source intermediate water mass GNAIW shallower than the present-day NADW by 500-1000 m and a stronger stratification with more saline deep water. It is noted that the shallower GNAIW did not imply a weaker influence of northern-source deep water on the bottom water of the Atlantic. The state estimation also provided a continuous global mapping of the sea surface temperature based on model physics. The main problem of any LGM state estimation to date is the vast imbalance between the number of observations and the number of control variables, and thus the very large number of degrees of freedom. Naturally, increasing the number of independent observations as much as possible would be the most straightforward way to mitigate this difficulty. Otherwise, more prior knowledge would need to be added by, for example, re-arranging the control variable space or adding more model physics. In a feasible next step, the cost function could be extended to include seasonal surface temperature fields [cf. Paul and Losch, 2012] to take full advantage of seasonal SST reconstructions provided by the MARGO project as well as by other studies [Benz et al., 2016]. Another desirable and potentially very important next step is obtaining sufficient data in the Pacific Ocean to include them in the state estimation framework. These data constrain the Pacific Ocean state better and would help to evaluate hypotheses about ocean circulation patterns in the Pacific Ocean during the LGM, which would in turn shed light on the role of the Pacific Ocean in large-scale climate variability. project PalMod (www.palmod.de; FKZ: 01LP1505D) within the framework of Research for Sustainable Development (FONA, http://fona.de) by the German Federal Ministry for Education and Research (BMBF). The adjoint model was generated using TAF [Giering and Kaminski, 1998]. Table 1. Data sets that were used for state estimation in this study. SST stands for sea-surface temperature. The data coverage refers to the surface area (for annual-mean SST data) or the volume (other data) of the ocean when mapped onto the model grid.  Schmidt et al. [1999] 4.6% only for deeper than 150 m \u03b4 13 C DIC Schmittner et al. [2013] 7.2% only for deeper than 1000 m Table 2. Assumed prior uncertainties of the control variables for the 200-year estimates. The weight of the penalty terms is given by the inverse of the square of prior errors, that is, for example, a \u221a 10-times larger prior error corresponds to a 10-times smaller weight."}, {"section_title": "Data", "text": ""}, {"section_title": "Variables Uncertainty units", "text": "Initial temperature 3.2 \u00d7 10 0 K Initial salinity 3.2 \u00d7 10 \u22121 psu Surface air temperature 1.0 \u00d7 10 0 K Specific humidity 1.0 \u00d7 10 \u22123 kg/kg Precipitation 1.0 \u00d7 10 \u22128 m/s Downward shortwave radiation 1.0 \u00d7 10 0 W/m 2 Wind velocities 3.2 \u00d7 10 \u22121 m/s Vertical diffusion coefficient 3.2 \u00d7 10 \u22126 m 2 /s Initial \u03b4 18 O water 3.2 \u00d7 10 \u22121 % Initial \u03b4 13 C DIC 3.2 \u00d7 10 \u22121 % Remineralization factor 1.0 \u00d7 10 \u22121 - Table 3. Terms of the objective function (the normalized costs J \u2032 ), the fraction within 2\u03c3 (FW2\u03c3) and within 1\u03c3 (FW1\u03c3) for the modern state estimates. For MOD200 and MOD400, the upper row shows the first-guess values and the lower row shows the optimized values, respectively. The top row shows the original first-guess values (i.e., before the preparatory runs) for temperature and salinity.  Table 4. Terms of the objective function (the normalized costs J \u2032 ), the fraction within 2\u03c3 (FW2\u03c3) and within 1\u03c3 (FW1\u03c3) for the LGM state estimates. For LGM200 and LGM400, the upper row shows the firstguess values and the lower row shows the optimized values, respectively. The top row shows the original first-guess values (i.e., before the preparatory runs) for the SST.        LGM400. The dots indicate observations including data in the Indian Ocean and Pacific. Distributions after 3000-year model integration are also shown in (c) and (d). The differences between the reconstructions and observations are shown in (e)-(h). Grid cells with differences smaller than 0.2 % in magnitude (i.e., the assumed uncertainty) are depicted as gray in color. Figure 8. Atlantic zonal-mean differences (LGM400 \u2212 MOD400) for (a) salinity and (b) potential density. For both quantities, the global mean values were subtracted to remove the effects of a systematic difference and reveal the patterns of the difference. A: State estimation for the modern ocean with the original data sets Prior to the state estimations for the modern day described in the main text (i.e., MOD200 and MOD400), we had done another series of modern state estimation without the three countermeasures to avoid unreasonable atmospheric forcing fields in the optimized states (Section 4.2). In addition to that, there were a few alterations; the first-guess initial and boundary conditions of \u03b4 18 O water for the 200-year estimation were taken from LeGrande and Schmidt [2006]. For \u03b4 13 C DIC , they were prepared by interpolating the discrete observations collected by [Schmittner et al., 2013] using Data-Interpolating Variational Analysis (DIVA) [Troupin et al., 2012]. Otherwise, we used the same configurations as used for MOD200 and MOD400. The results of the preceding state estimations (hereafter, called MOD200org and MOD400org) are summarized in Table A.1. Judging from J \u2032 , FW2\u03c3, and FW1\u03c3, no excessive differences were observed between the optimized ocean state in MOD400org and that in MOD400 (Table 3). The reconstructed ocean circulation had 16.4 Sv of the maximum AMOC strength and \u22122.5 Sv of the bottom circulation with AABW, which were also similar to those in MOD400. The reconstructed ocean had a stronger stratification with more saline deep water, too. The resultant modified atmospheric forcings, however, showed remarkable discrepancies. In MOD400org, the bulk assessment of the deviation of modified atmospheric forcings from the first guess (J \u2032 ctrl ) showed the acceptable magnitudes of modification to them; besides, the normalized total adjustments that assess the deviation from the original firstguess values were 1.4, which indicated that the adjustments were in a reasonable range. Nevertheless, the adjusted atmospheric forcing fields were found physically unreasonable in some regions. If we defined the regions of unreasonable adjustments as grid cells that had any of the following: air-temperature adjustment larger than 40 K, negative precipitation, negative specific humidity, or negative downward shortwave radiation, typically they are coast areas including the upwelling regions along the west coasts of continents, \"tongues\" in the tropical Pacific and Atlantic affected by the equatorial upwelling processes, the Arctic regions, and comparatively closed (i.e., insufficient communication with the open oceans) seas. It implied that the model has significant bias in such regions caused by, for example, the poor representation of the coastal upwelling processes due to the coarse resolution of model, so that, to compensate for the model deficit, the model input needed to be modified to realize a good match to the observation. From this point of view, MOD400org was not consistent with plausible reconstructed atmospheric conditions, although it succeeded in providing continuous tracer distributions that are consistent with as much data as available. Table A.1. Development of the objective function (the normalized costs J \u2032 ), the fraction within 2\u03c3 (FW2\u03c3), and that within 1\u03c3 (FW1\u03c3) for the modern state estimates with the original data sets. Figure A.1. Locations of monthly SST data for the modern state estimate: (a) the original data sets, and (b) the reduced data sets. The value shows the number of months that have data."}]