[{"section_title": "Abstract", "text": "The genomics era has led to an increase in the dimensionality of data collected in the investigation of biological questions. In this context, dimension-reduction techniques can be used to summarise high-dimensional signals into low-dimensional ones, to further test for association with one or more covariates of interest. This paper revisits one such approach, previously known as principal component of heritability and renamed here as principal component of explained variance (PCEV). As its name suggests, the PCEV seeks a linear combination of outcomes in an optimal manner, by maximising the proportion of variance explained by one or several covariates of interest. By construction, this method optimises power; however, due to its computational complexity, it has unfortunately received little attention in the past. Here, we propose a general analytical PCEV framework that builds on the assets of the original method, i.e. conceptually simple and free of tuning parameters. Moreover, our framework extends the range of applications of the original procedure by providing a computationally simple strategy for high-dimensional outcomes, along with exact and asymptotic testing procedures that drastically reduce its computational cost. We investigate the merits of the PCEV using an extensive set of simulations. Furthermore, the use of the PCEV approach is illustrated using three examples taken from the fields of epigenetics and brain imaging."}, {"section_title": "Introduction", "text": "In the omics era, a considerable amount of data is now routinely collected to investigate the relationships between a set of covariates of interest (X) and genetic, molecular, or clinical outcomes (Y). As such, a substantial proportion of the methodological research developed in the last decade has focused on the numerous statistical challenges and computational issues highlighted by the joint analysis of such high-dimensional correlated data. If we imagine a spectrum of models indexed by dimensionality, at one end, we have a model that attempts to accommodate all variables at once. This may lead to results that are difficult to interpret. At the other extreme of this spectrum, we have univariate models which treat each variable separately while ignoring the others. This approach may miss subtle signals arising from complex biological interactions and correlations. An intermediate approach would therefore either identify a priori relevant groups of variables Y on which to perform the analysis or reduce the dimensionality of the problem by summarising the data into meaningful components. A popular example of dimension reduction is principal components analysis (PCA). This method seeks linear combinations of the original data that explain the maximum amount of variance.\nIn contrast to studies with high-dimensional covariates, here we focus on studies where the goal is to investigate the association between a set of (possibly high dimensional) correlated outcomes and one or more covariates of interest. In this context, several methods integrating both data reduction and association analysis simultaneously have been developed. For example, in a variant of principal component regression (PCR), a PCA analysis is performed on the set of outcomes Y, producing a smaller number of components. These components are then used in an association test with the covariates of interest X. Although being widely used in practice, this method has very poor power in cases where the outcomes showing the greatest variability -hence those captured by the first principal components -are not the ones associated with the covariates of interest. Since outcomes strongly influenced by the environment could easily have much larger variability than outcomes influenced by a single nucleotide polymorphism (SNP), this is of particular concern in genetic studies. Therefore, PCA in this context is likely to find components where environmentally driven traits have the highest weights (since they are highly variable), whereas the weights associated with genetically-controlled traits may be very small if they are less variable across individuals. If an association analysis is then performed between such components\u1ef8 and genotype data X, it is likely that no association will be detected. Since the data reduction step in PCR is performed independently of the covariates, the low power in such situations is not surprising; as a consequence, other methods have been developed to jointly perform both data reduction and association analysis. For example, partial least square (PLS) regression, 1 canonical correlation analysis (CCA), 2 and linear discriminant analysis (LDA) 3 are widely used component-based methods that can be employed to find, simultaneously, the ''best'' components describing two sets of variables (e.g. outcomes and covariates). This is achieved by maximising the association between them. The optimisation criterion for association differs between these methods: while CCA is based on maximising correlations, PLS optimises covariances. Methods of this type are very powerful by construction and have been extended in several ways to accommodate high-dimensional variables using regularisation techniques or sparsity measures. However, such extensions heavily depend on tuning parameters, and formal testing procedures are currently lacking.\nAlthough PLS and CCA have been gaining popularity in genomics, these methods were originally developed in other fields and mainly for prediction purposes. Ott and Rabinowitz 4 developed a closely related method implementing similar ideas to PLS and CCA, specifically for a genetic context. Using family data and with the aim of performing linkage analysis between multiple correlated outcomes Y and SNP genotypes X, the idea was to find the best linear combination of outcomes (i.e. a component) maximising the heritability at the SNP tested. This method, termed principal component of heritability (PCH), has unfortunately received little attention although it was later extended [5] [6] [7] to the more general setting of population-based studies and high-dimensional outcomes based on regularisation or sparsity techniques. Since heritability is simply defined in PCH as the proportion of variance in the outcomes Y explained by the SNP covariate, it can be seen as a method belonging to the family of PLS and CCA based on another criterion, i.e. the proportion of variance in the outcome variables explained by the covariates. Since the term ''heritability'' can be misleading in the context of population-based studies, and since the concept underlying PCH can also be used in a general setting outside genetics, we have renamed this approach principal component of explained variance (PCEV). We note that this method is also closely related to dual-scaling, 8 originally introduced in the psychometrics literature. In this paper, we present a completely new analytical framework based on the PCEV concept that copes smoothly with data of very high dimension at the outcome level, includes valid hypothesis tests, leads to interpretable results, and yet is computationally efficient. This approach is implemented in an R package, pcev, available on the Comprehensive R Archive Network (CRAN). Specifically, we first show that unlike the competing, similar approaches discussed earlier, PCEV has mathematical properties that allow inclusion of very high-dimensional outcomes without the need to rely on variable selection strategies that require selecting tuning parameters. Secondly, we show that PCEV can be extremely computationally efficient, unlike its competitors, by developing an exact testing procedure that does not rely on permutations. Therefore, this approach is entirely feasible for use in genome-wide studies (or studies with very high-dimensional response variables), such as we frequently encounter today.\nWe then investigate the merits of the PCEV using an extensive set of simulations. In the large n-small p setting, we focus our discussion on methods that do not require tuning parameters and those for which a testing framework is fully developed. For this reason, we discarded from our comparison methods such as sparse PLS (sPLS), 9 regularized CCA (rCCA), 10, 11 and the projection regression model proposed by Lin et al. 12 On the other hand, in the high-dimensional setting (i.e. small n-large p setting), we compare our extension of PCEV to commonly used approaches, such as lasso 13 and sPLS; these two methods do involve tuning parameters. We show that PCEV outperforms all these methods, while also being much more computationally efficient. Finally, the use of the PCEV approach is illustrated in three settings: (i) using DNA methylation data derived from whole genome bisulfite sequencing (Y), we test if a region near the BLK gene on chromosome 8 is differentially methylated in B-cells compared to T-cells or monocytes; (ii) using DNA methylation microarray data (Y), we perform a genome-wide gene-based association analysis of methylation with respect to cigarette smoking status; and (iii) using brain imaging traits derived from [18F]Florbetapir positron emission tomography (PET) scans (Y), we perform an association test between amyloid-b (Ab) accumulation and two sets of covariates: Alzheimer's disease (AD) status and a set of SNPs located near the APOE gene."}, {"section_title": "Method", "text": "The general methodological framework aims to simultaneously test a (possibly large) set of phenotypes or outcomes Y, against a set of covariates X. The method is evaluated through an extensive simulation study and then applied to three independent datasets."}, {"section_title": "General theoretical PCEV framework", "text": "We consider the following setting: let Y be a multivariate phenotype of dimension p (e.g. methylation values at p CpG dinucleotides, or brain imaging measures at p locations in the brain), let X be a q-dimensional vector of covariates of interest (e.g. smoking, cell type or SNPs) and let C be an r-dimensional vector of confounders (e.g. age or sex). We assume that the relationship between Y and X can be represented via a linear model\nwhere B and ! are p \u00c2 q and p \u00c2 r matrices of regression coefficients for the covariates of interest and confounders, respectively, and E $ N p \u00f00, V R \u00de is a vector of residual errors. This model assumption allows us to decompose the total variance of Y, conditional on C, as follows: \nThe original PCH, as presented by Ott and Rabinowicz, 4 relied on the same variance decomposition and the same optimization problem described above. However, genetic data from families were used to estimate the genetic variance component V M . In contrast, Klei et al. 6 and Lin et al. 12 extended the idea to population data using a linear model assumption.\nIt has been shown that w PCEV is the solution to the generalised eigenvector problem V M w \u00bc lV R w, 4 and therefore standard linear algebraic results can be used to get a closed form solution w PCEV . Although there are some similarities between PCA and PCEV since both methods seek a linear combination of outcomes optimising a given criterion, we recall that principal component analysis (PCA) reduces the dimension of Y by looking for a linear combination of its components with maximal variance, independently of the covariates X. Furthermore, an important difference between PCA and PCEV is in the number of components one can extract. While the number of components to select in PCA is usually left to the user, and the maximum number of extracted components is bounded above by p, the maximum number of components that can be extracted for PCEV is bounded above by the number of covariates q. Therefore, if we are only interested in one covariate, only one PCEV can be extracted; this follows from considering the rank of the matrix V \u00c01 R V M ."}, {"section_title": "PCEV with high-dimensional data", "text": "When the number of response variables p is larger than the sample size n, a na\u0131\u00a8ve implementation of PCEV will fail. To ensure the uniqueness of the solution to the maximisation process, the invertibility of the residual matrix V R is required; therefore, an accurate estimation of the matrix V R requires n > p. This limitation has led to the introduction of regularisation techniques for the estimation of w, reminiscent of ridge regression 14 and lasso. 13 We stress once again that these methods require parameters that are computationally expensive to compute. For this reason, we propose a novel alternative, namely a block approach to the estimation of PCEV. Assume we can partition Y into blocks (or clusters) such that the number of components in a given block is small enough, i.e. smaller than n. We can then perform PCEV and get a linear combination\u1ef8 j of the traits belonging to the jth block, for each block j \u00bc 1, . . . , b. We then obtain a new multivariate pseudo-phenotype\u1ef8 \u00bc \u00f0\u1ef8 1 , . . . ,\u1ef8 b \u00de, where each\u1ef8 j is of dimension one. We can then perform PCEV again on\u1ef8. Since the result is a linear combination of linear combinations, it is itself a linear combination of the original traits Y. Although one might think that this stepwise approach is an ad-hoc extension of the original PCEV approach, it has nonetheless a very appealing and relevant mathematical property, described in the following result:\nTheorem 1: Assume one can partition the outcomes Y into blocks in such a way that blocks are uncorrelated (i.e. outcomes lying in different blocks are uncorrelated). Then the linear combination (PCEV) obtained from the traditional approach and that obtained from the stepwise block approach described above are equal.\nThe proof of this result can be found in the Supplementary material. Of course, if such a partition does not exist, there will generally be a difference between the two estimation procedures. However, through a series of simulations and analyses of data sets, we will show that the discrepancy is small. Therefore, comparable conclusions can be achieved even in the presence of some dependence between the blocks."}, {"section_title": "Test of significance", "text": "The PCEV methodology was first presented in the context of family-based studies, and the lack of a proper significance testing framework may have hindered its adoption in population-based studies. Klei et al. 6 discussed such a framework, but their approach relies on computationally intensive sample splitting and resampling. In fact, here we are able to show that there is an analytic test of the null hypothesis H 0 : h 2 \u00f0w\u00de \u00bc 0 that requires no resampling. This test is based on the largest eigenvalue l of the matrix V \u00c01 R V M , a test statistic used in multivariate analysis of variance. 15 When X consists of a single covariate, it is also known as the Wilks statistics; it can be shown that under the null hypothesis\nwhere 16 The asymptotic distribution of a suitable transformation of l was derived by Johnstone. 17 More details are given in the Supplementary material.\nWe note that the Wilks and Roy's largest root test statistics both rely on the assumption of normality of the outcomes, and the former also requires n \u00c0 p \u00c0 1 4 0 (corresponding to the second degree of freedom of the F test). If these assumptions are not satisfied, permutation tests provide an adequate control of the type I error and can be used as an alternative."}, {"section_title": "Variable importance", "text": "The PCEV framework described above allows reduction of the multiple testing burden by performing only a single test for a set of outcomes. If the test is significant, it is of great interest to identify the set of outcomes contributing the most to the global association detected. Here, we use the variable importance on projection (VIP) defined for outcome j as\nThis VIP measure can be signed (as a correlation) or unsigned (in absolute value). In the case of PCEV-block, the VIP measure is defined in the same way, i.e. correlation between the original outcomes and the final PCEV component. We note that the VIP should be used as a means of ranking the contributions of each individual outcome to the overall association; as such, the actual values obtained are not necessarily interpretable. We provide examples of how to use the VIP values in sections 4 and 5."}, {"section_title": "Defining blocks", "text": "Theorem 1 allows us to extend PCEV to the setting where there are more response variables than observations (p ) n), through the use of blocks. In practice, the researcher needs to define these blocks prior to analysis, and correlation-based clustering methods (e.g. hierarchical and k-means clustering) can be used to obtain blocks that are minimally correlated with one another. However, in some settings, prior knowledge about the response variables can be leveraged in defining blocks. For example, DNA methylation at neighbouring CpG dinucleotides is known to be highly correlated. 18 Therefore, in studies of DNA methylation, CpG nucleotides could be grouped based on distance. In the data analyses below, we give examples of both approaches."}, {"section_title": "Simulation study", "text": "Power and type I error of the PCEV approach are evaluated through extensive simulations assuming a sample size n \u00bc 500. In all simulations, a single continuous covariate X $ N\u00f00, 1\u00de is simulated and no confounder variables are included in the analysis. The multivariate outcomes Y are simulated under model (1) using a multivariate normal distribution of the residuals with variances equal to 1 and a block correlation structure made of five blocks of equal size. This correlation structure in V R is governed by parameters w , defined as the within-block correlation and b , defined as the between-block correlation. The regression coefficients B k (k \u00bc 1, . . . , p), corresponding to the kth outcome, are chosen to match the values of h 2 for each outcome, i.e. the proportion of variance explained by the covariate X, according to the relationship\nWhen multiple outcomes are associated with the variable X, we assume that each outcome explains the same proportion of variance h 2 . We compare the PCEV approach with a PCR analysis and a PCEV-block approach. The PCR approach was selected for comparison because it does not involve any calibration (e.g. sparse methods typically require a tuning parameter) and because it provides a convenient framework for significance testing. In the PCEV approach, p values are computed using the asymptotic test described above. In PCR, the first principal component of Y is extracted and then tested for association with the covariate. Finally, the PCEV-block approach is applied by using the same blocking structure as in the simulated data (i.e. five blocks). This analytical strategy is chosen to evaluate the performance of the PCEV-block approach in the situation where the mathematical property of independence between blocks is verified. Furthermore, we chose scenarios where the PCEV-block approach is not necessary (p ( n), allowing us to compare the performance of the PCEV-block with respect to the original PCEV.\nWe investigate the performance of the PCEV approach by varying several simulation parameters, namely p (the number of outcomes), w , b , and h 2 under five main scenarios described in Table 1 . For each scenario, 500 simulated datasets were generated, and power was computed for p \u00bc 20, 50, 100, 200, 300, 400, w \u00bc 0, 0:5, 0:7 and b \u00bc 0, 0:5, 0:7 with w ! b . Scenario 0 is designed to investigate the type I error rate of each method; therefore, none of the outcomes are associated with X. Scenarios 1 \u00c0 3 are designed to examine power under various settings. Finally, scenario 4 evaluates the robustness of the PCEV-block approach with respect to the independence of blocks assumption. In this case, PCEV-block is applied using three strategies to define the blocks: Finally, we also compare PCEV-block to other, traditional high-dimensional methods, namely lasso (as implemented in the R package glmnet 19 ) and sPLS. 9 rCCA is excluded due to its prohibitively high computational time (see Table 2 ). The simulation scenario here is slightly different than described above: the sample size is fixed at n \u00bc 100 and the number of response variables takes values p \u00bc 100, 200, 300, 400, and 500. The correlation structure between these variables varies in the same way as the other scenarios above. However, the association structure resembles that of Scenario 3: the response variables are partitioned into 10 blocks, and 25% of the variables in each block are truly associated with X with h 2 \u00bc 1%. Both lasso and sPLS require a tuning parameter. This parameter is selected using 10-fold cross-validation and with a grid of length 100. Moreover, both methods require a ''reversed'' approach, i.e. the multivariate response vector Y needs to be treated as the covariate vector, and X needs to be treated as the response variable. Finally, to compute power and perform hypothesis testing, we use the correlation between X and the predicted values T Y as our test statistic for sPLS. For lasso, we compute a likelihood ratio test (LRT) statistic comparing the selected model to the null model (i.e. including only an intercept). The null distribution of these test statistics is estimated using 500 permutations. The p value for both PCEV-block approaches is also computed using 500 permutations.\nIn this high-dimensional simulation scenario, we also investigate the impact of block choice on the analysis. To this end, we compare two PCEV-block approaches, one where the 10 true blocks are known a priori, and another where the response variables are randomly assigned to one of 10 blocks.\nAs one can see in Figure 1 , type I error for PCEV is well controlled and is not influenced either by the number of variables or by the correlation between outcomes. Here we have used the Wilks' test for the classical PCEV, and performance is as expected, even when the number of responses is as large as 400. The same figure, with 95% confidence intervals included, appears as Supplementary Figure S1 , showing that the observed variation can be explained by Monte Carlo errors. Supplementary Table S1 shows that the type I error for Wilks' test is also well controlled at significance levels as low as 10 \u00c04 .\nFigures 2, 3 and 4 illustrate the power of the PCEV, PCEV-block, and PCR approaches in the next three simulated scenarios. In all cases, we observe that power of PCR is very low compared to the PCEV approaches. This is as expected, since PCR is not optimal with respect to identifying components maximally related to the covariate X. In all figures, power can also be seen to increase with correlation between the outcomes. Such behaviour is also expected, since we can think of correlation as spreading out the signal across multiple outcomes. Therefore, this signal is easier to detect with the PCEV methods.\nHowever, the relationships between power and patterns of associated responses are more nuanced. Increasing the number of outcomes, i.e. increasing p, has a detrimental effect on power when the number of outcomes associated with X is low (scenario 1, Figure 2 ), and this is true for any of the methods that we explored. However, when the signal to noise ratio is kept constant as p increases (scenarios 2-3-4), power tends to increase with p up to a point, and then decrease afterwards. This inflection point seems to be where the number of outcome variables is large enough that the residual variance matrices become ill-conditioned. Once such a state is reached, the power of the PCEV approach decreases quickly (Figures 3 and 4) with larger values of p. Crucially, however, the PCEV-block approach is not affected by this ill-conditioning phenomenon, since the residuals are estimated within each block. Therefore, each residual variance matrix is estimated from a much smaller number of outcomes. We also note that varying the between-block correlation B in the simulation has a small impact on power, as it increases correlation between variables but does not spread the signal across outcomes. On the contrary, increasing the within-block correlation W increases power, as expected. Figure 5 illustrates the sensitivity of the PCEV-block approach with respect to how the blocks are chosen. As can be seen, power is reduced when the blocks are more correlated with each other. This is as expected, since the PCEV-block approach relies mathematically on the independence between blocks. However, as we will see in the data analysis section, despite the reduction in power, the VIP measures are very robust to misspecifications of the independence assumption.\nFinally, in Figure 6 , we see that PCEV-block has better power than both lasso and sPLS. Moreover, we see that choosing the blocks randomly has little impact on power and we essentially get the same performance as when the blocks are known a priori. Moreover, Table 2 shows that PCEV is more computationally efficient than the other methods, with substantial benefit in some cases."}, {"section_title": "Datasets", "text": "All datasets used in this paper are publicly available or available upon request. The bisulphite sequencing data is included in the R package pcev. The ARCTIC methylation data have been deposited in dbGAP under accession number [phs000779.v1.p1]. Finally, the ADNI data is archived in a secured and encrypted system provided by the LONI Image Data Archive (IDA). Applying for access to the data requires the submission of an online application form. Table 2 . Average running times (in milliseconds) over 100 runs for four high-dimensional methods as a function of the number of outcomes p. The running times for lasso, sparse PLS and regularized CCA include the selection of a tuning parameter using 10-fold cross-validation and a grid of length 100. "}, {"section_title": "Analysis of bisulfite sequencing data", "text": "The dataset contains measurements of DNA methylation levels derived from bisulphite sequencing around the BLK gene, located on chromosome 8. A total of 40 different samples were analysed from three cell types: B cells (8 samples), T cells (19 samples) and monocytes (13 samples). These samples are derived from whole blood collected on a cohort of healthy individuals from Sweden. Data were sequenced on the Illumina HiSeq2000 system. Missing values are imputed using the mean of neighbouring sites and the imputed data is available in the R package pcev. The region analysed contains 24,068 CpG sites, from which we removed the duplicate sites and analysed the 25% most variable sites (which coincide with the sites having the largest depth), for a total of 5986 sites spanning a region of 2.5 Mb. Methylation levels at each CpG sites were measured using the logit of the methylated proportions. To apply the PCEV-block approach, we took advantage of the natural clustering as a function of physical distance between CpG sites on the DNA strand. We clustered CpG sites such that sites within 500 kb were grouped together. In order to obtain clusters with less than 30 sites (to ensure p < n), we subsequently broke large clusters into smaller ones based on genomic distance. We obtained a total of 983 blocks of size ranging from 1 to 30 sites. Since this dataset is too large to use the classical PCEV method, we only used the PCEV-block framework to test the association between methylation levels and cell type (dichotomous variable, testing B-cells versus others).\nThe genomic region we have analysed near the BLK gene is known to be hypomethylated in B-cells, compared to other cell types. 25 Since we have only 40 samples and over 5986 sites in our region of interest, it is impossible to perform the regular PCEV test. Therefore, we use the PCEV-block framework, and we expect to capture the region based association using only a single statistical test. Note that the methylation levels in this dataset show only a mild level of correlation: 50% of the CpG sites pairs have a correlation smaller than 0.15 and 99% of the pairs have a correlation smaller than 0.50. As a result, the assumption of independence between blocks is not strongly violated in this example.\nThe p value obtained with PCEV-block for all 5986 sites simultaneously was 6 \u00c2 10 \u00c05 ; this p value was computed using 100,000 permutations. This result can be considered highly significant since only one test was performed. Furthermore, the results obtained are in excellent agreement with other approaches to analysis. For instance, Figure 7 (a) shows the results using local linear regression to obtain a smoothed curve for each cell type in the region studied. As one can see, there is a region (delimited by vertical bars) comprising the BLK gene and a small upstream region where B-cells are differentially methylated compared to the two other cell types. Univariate regression analyses also confirmed this hypothesis (Supplementary Figure S3) . Using the VIP measure, we are also able to identify which CpG sites contribute most to the association obtained (Figure 7b ). Note that the region delimited in red in Figure 7 (b) is identical to the region delimited by vertical bars in Figure 7 (a). Furthermore, there is a strong relationship between VIP measures and univariate p values (Supplementary Figure S4, left panel) . Signed VIP measures can also be used to detect the direction of the association, and we see in Supplementary Figure S4 (right panel) that such measures are highly correlated with the univariate slope regression coefficients."}, {"section_title": "Gene-based analysis of 450 K illumina methylation data", "text": "This analysis focuses on a sample of 1035 individuals who served as controls for the Assessment of Risk for Colorectal Tumors in Canada (ARCTIC) study. 20, 21 Methylation at 485,512 CpG sites was measured in stored lymphocyte samples using the Infinium Human Methylation450 BeadChip. The CpG sites were allocated to 20,041 genes using the UCSC gene annotation; we also considered sites located 2kB upstream from the transcription starting site and downstream of the 3' end. With this definition, the genes under consideration contained anywhere between 2 and 288 CpGs, allowing us to perform PCEV using both the block and the classical approach since the number of CpGs per gene was substantially smaller than the number of individuals. Methylation data was normalised using functional normalisation 22 and further adjusted to account for cell type mixture using the reference-based method proposed by Houseman et al. 23 After normalisation, correction and exclusion of the X and Y chromosomes, we were left with 169,239 CpGs, located in 18,969 genes. In this analysis, we are interested in the association between methylation at the gene level and cigarette smoking status (which is dichotomous). By performing a gene-based analysis, the epigenome-wide significance threshold was lowered. Therefore, more associated genes were expected to be detected.\nAnalysis of the ARCTIC data was undertaken gene by gene, after selecting probes in or near each gene. In Figure 8 , we show a scatter plot comparing the results of PCEV for each gene (without using the block version), and a gene level summary of the univariate p values, where both are given on the negative log scale. The gene-level summary of the univariate p values is defined as the minimum p value among the CpGs tested in the gene, corrected for the number of independent CpG sites in the gene, i.e. the minimum p value is multiplied by the estimated number of independent CpGs. The effective number of independent tests was estimated using the method proposed by Gao et al. 26 By examining the smallest p values, or the top of this scatter plot, Figure 8 suggests that PCEV tends to enhance power. This is particularly true for the genes with the strongest association. In Figure 9 , we compare the VIP values obtained from both the classical PCEV and the block approach, for four different genes known to be associated with cigarette smoking: F2RL3, AHRR, RARA and GNG12. 27, 28 The PCEV-block approach proceeded by defining three blocks in each of these genes, and by allocating CpG sites to blocks in a linear fashion (for example, if a gene contained 30 sites, the first 10 were assigned to one block, the next 10 to a second block and so on). We purposely chose a non-optimal block definition so that we could evaluate the robustness of the block method to a poor choice of block (here, blocks were not chosen to be independent). As we can see, the VIP values provide information that is very similar to the univariate p values across these four genes."}, {"section_title": "Analysis of brain imaging data", "text": "Data used in the preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). The ADNI was launched in 2003 as a public-private partnership, led by Principal Investigator Michael W Weiner, MD. The primary goal of ADNI has been to test whether serial magnetic resonance imaging (MRI), PET, other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment (MCI) and early AD.\nThe analysis performed was based on data acquired from 340 participants from ADNI GO/2, for whom both genetic and PET data were available. [18F]Florbetapir PET imaging was employed to assess brain Ab protein load using PET standardised uptake value ratios (SUVR) in 96 brain regions. These 96 Ab levels represent the phenotypes of interest in our analysis. Genotype data was derived from the Illumina OmniQuad array. 24 Association of Ab levels with both diagnosis (AD versus others) and with 20 SNPs located in the PVRL2-TOMM40-APOE region on chromosome 19 was investigated. Phenotypes were adjusted for gender, age and education level directly within the PCEV framework.\nThis dataset illustrates an example where p ( n; therefore, no block strategy was necessary to perform a PCEV analysis. However, we performed both analyses (with and without the blocks) for the sake of comparison. In this data, there is a very high level of correlation between all brain regions, as shown in Supplementary Figure  S5 . Such high and extensive correlation makes clustering regions into blocks extremely challenging, and we did not succeed in identifying well-separated clusters with any reasonable level of confidence. Nonetheless, we present the results of the PCEV-block approach using 10 blocks obtained using a hierarchical clustering technique. Moreover, we note that this dataset has several interesting features allowing us to (i) compare results between PCEV and PCEV-block, and (ii) evaluate the performance of the PCEV-block strategy, using the traditional PCEV approach as a gold standard, in a situation where clusters are very poorly chosen and highly correlated with each other. Analysis of the Ab accumulation against disease status (AD versus others) revealed a significant association regardless of the testing procedure chosen (see Table 3 ). Furthermore, Figure 10 illustrates the good agreement between variable importance measures computed using the classical PCEV approach and the block approach. We also note the monotonic relationship between univariate p values (obtained from a simple regression analysis between Ab levels in each brain region and disease status) and VIPs.\nTo further assess robustness to the choice of blocks, we defined 500 random partitions of the 96 brain regions into 10 blocks of similar size. PCEV-block was then performed using each of these random partitions. In all cases, the p values obtained (using 100,000 permutations) were less than 10 \u00c05 . Moreover, the VIP measures were highly consistent. For each of the random partitions, we looked at the brain regions receiving 1 of the 10 largest VIP values. We then compared this list with that coming from the original analysis (i.e. the original PCEV approach, without any block). All such lists coming from the random partitions shared at least seven brain regions with the original analysis, and 95% shared at least eight brain regions. Moreover, the same brain region (i.e. brain stem) consistently received the highest VIP value. Finally, we also tested the global association between the Ab levels and 20 SNPs in a region around the APOE gene. The p value obtained was 0.0117; since only one test was performed, this is significant at level \u00bc 0:05. To measure the contribution of each SNP to the overall association, we looked at two different measures. First, we computed the Pearson correlation between each SNP and the estimated PCEV. Second, we computed a p value using a LRT. We note that these two methods provide distinct and complementary information: the correlation is a marginal measure of association, whereas the LRT p value measures the incremental value of a SNP to the model containing all SNPs but the current one. These values, along with the name of the gene containing each SNP, appear in Table 4 ."}, {"section_title": "Results", "text": ""}, {"section_title": "Data analysis", "text": ""}, {"section_title": "Discussion", "text": "In this article, we have revisited a dimension-reduction approach, PCEV, which has unfortunately received little attention in statistical genetics or biostatistics in general. We showed how PCEV is well-suited for multivariate association studies. This is due to its optimality with respect to capturing the association between a multidimensional phenotype and a set of covariates.\nWe also presented a hypothesis-test framework which relies on an asymptotic result but no resampling. In particular, we presented two analytic tests that can be used with the traditional single-block PCEV: Wilks' and Roy's tests. The former should only be used when there is a single covariate and when n 4 p \u00fe 1, while the latter can be used in much more general settings (even when p ) n). Note also that Wilks' test is exact, whereas Roy's uses an asymptotic result. Hence, the latter test is based on an approximation to the null distribution when the sample size is large enough. These asymptotic results mean that PCEV can be computationally extremely fast, which makes it feasible for use in large scale pipelines of multivariate analyses. For the block approach to PCEV, the independence of blocks assumption violates the distributional assumption necessary for Johnstone's approximation 17 to be valid; for this reason, we have opted for a permutation procedure in order to test for association. Finally, although our discussion has focused on testing the first PCEV component (which is the only component computable when X is a single covariate), this framework can also be applied to several PCEV components independently, when multiple covariates are analysed simultaneously.\nOur framework also allows for the direct inclusion of confounding variables. In the presence of confounding, this feature leads to unbiased estimates of the matrix B of regression coefficients and therefore to correct inference about the association between the outcomes and the covariates. However, we note that covariates (which we denoted X in our model) and confounders (which we denoted Z) play a very different role in the analysis: the covariates should be of scientific interest to the analysis. For example, in our analysis of Ab accumulation and AD, we were not interested in the effect of age, gender and education level on Ab accumulation in the brain. However, since these three variables are known to be associated with both Ab levels and with AD, we included them as confounders in our PCEV analysis.\nWe also introduced a useful metric, the VIP measures, to help researchers decompose the global signal with respect to each phenotype. This metric measures the marginal importance of each variable, in contrast to their incremental importance with respect to the other variables. This fact is highlighted in Figures 9 and 10 , where we show that the VIP measures are a monotone function of the univariate p values. For this reason, we recommend Given that there are likely to be many possible block partitions, understanding the impact of poor block choice is of interest. In general, for a fixed number of blocks of constant size, minimizing the correlation between the blocks will lead to maximal power. On the other hand, when blocks are not chosen in an optimal way, simulation results and data analyses above show that the loss in power is minimal.\nWe compared the performance of PCEV to PCR, which is a very popular approach for region-based analyses. [29] [30] [31] [32] [33] [34] [35] As expected, our simulation results show there is no guarantee that the first principal component is at all associated with the covariates. PCR had very low power to detect association, especially when compared to both the classical and block approaches to PCEV. Hence, in general, we discourage the use of PCR for regionbased analyses of multidimensional phenotypes.\nIn a truly high-dimensional simulation scenario (p ) n), we compared PCEV-block to both lasso and sPLS. As expected, the computational time is much faster for PCEV than the other competing methods -in fact, PCEV is 10 times faster than lasso, its fastest competitor. Moreover, we also showed that power is higher for PCEV in almost all scenarios, with only one exception when there is no correlation between any of the variables. In this context, all four methods give similar results. The lower power of lasso and sPLS seen with other correlation structures is likely due to small effect sizes between the covariate and individual response variables. The lower power of lasso could also be a consequence of its poor performance in the presence of correlated features. 36 As can be seen in Supplementary Figure S2 , neither lasso nor sPLS were successful in selecting the truly associated variables. On the other hand, PCEV was able to capture the global signal even when individual associations were small. We note that PCEV has many other advantages over lasso and sPLS: (i) it does not require a ''reverse'' analysis (where the role of Y and X are reversed); (ii) confounders can be directly included in the analytical framework; and (iii) it can easily handle more than one covariate of any type (i.e. binary, categorical, or continuous).\nFor comparison purposes, we decided to use lasso and sPLS instead of similar procedures that incorporate more structured penalties, such as sparse group lasso 37 or sparse group PLS. 38 These methods make use of external information to define groups of similar variables that are more likely more likely to have similar regression coefficients. The penalty then uses this information to shrink the coefficients toward a common mean. However, this new information needs to be incorporated via the addition of a second tuning parameter. While these methods would likely perform better than lasso and sPLS in terms of variable selection, it is not clear that this would lead to better power. In any case, the simulation shows that when PCEV ignores the block information and the blocks are selected randomly (PCEV-rand), the performance is similar to when the block information is incorporated in the estimation.\nIn all our simulation scenarios, we included only one covariate, and we did not include any confounders. However, the effects of these factors on power and Type I error are well documented; this follows from our assumption of a linear model. For example, including more covariates will decrease power of all methods, while the effect of adding confounders to the analysis will depend on the strength of the correlation between the covariates and the confounders. We therefore decided not to present simulation scenario re-addressing these questions. 39 Our analysis of the bisulfite sequencing data reveals how useful the block approach can be in the presence of high-dimensional data. Indeed, most multivariate methods do not give meaningful results for such a dataset, due to the discrepancy between the sample size and the number of variables. In contrast, the block PCEV approach was able to recapitulate known results about the potential existence of a differentially methylated region (DMR) around the BLK gene. As shown in Figure 7 (b) (right panel), PCEV block captures the association between 5986 variables and the cell-type covariate in a single test, despite the fact that there are only 40 subjects. Moreover, the VIP values were able to accurately pinpoint the source of the signal among the 6000 CpG sites (Figure 7a ). Supplementary Figure S4 With the ARCTIC dataset, our aim was to confirm the results of the association between methylation and cigarette smoking shown in Breitlin et al. 27 Two genes showed particularly strong associations in Figure 8 , F2RL3 (PCEV p value: % 10 \u00c026 ), and AHRR (PCEV p value: % 10 \u00c025 ), and both are previously reported smoking meQTL loci. 27, 28 By pooling all CpG sites at or near each gene, our power to detect these associations was greater than what we could have achieved with an adjusted univariate approach. After using the VIPs to decompose the multivariate signals at these two genes (Figure 9 ), it is evident that for F2RL3, the signal is mostly driven by one CpG site, whereas for AHRR, there seem to be five sites contributing to the overall signal. In the latter case, this ''pooling of forces'' probably leads to the power increase.\nIn the context of the brain imaging study, we were able to investigate the effect of wide-ranging correlations on the block approach to PCEV. We showed that, even though the independence of block assumption was clearly violated, the p value obtained using permutations was comparable to that obtained from a classical approach to PCEV (i.e. without blocks) and an exact test. Furthermore, the VIP factors were similar for both approaches. We also looked at the impact of choosing blocks randomly, and we showed that the results were similar to those obtained from an analysis using the original PCEV framework (i.e. without any block). Therefore, we see that the block approach is quite robust to violations of the assumption contained in Theorem 1.\nWe also provided an example of using PCEV with multiple covariates. We analysed the joint association between Ab accumulation and 20 SNPs in the PVRL2-TOMM40-APOE region on chromosome 19, and we obtained a single, significant p value. In assessing the contribution of each SNP to this global result, we determined that the most important SNP was rs769449, which is located in the APOE gene. This SNP is a well-known risk variant for AD. 40 We also uncovered evidence of linkage disequilibrium in this genomic region: a few SNPs located in the TOMM40 gene show evidence of marginal association (as measured using correlations), but none of them are significant when using a LRT. This is consistent with results in the literature. 41, 42 In summary, we have shown how PCEV is particularly well-suited for finding multivariate signals with widespread association with a set of covariates. There is a fine balance to strike when performing a multivariate association test with a very high-dimensional phenotype: although we want to include multiple correlated response variables and thus borrow strength across phenotypes, at the same time we would also like to retain interpretability of the multivariate signal. Suppose, to give an extreme example, that in the context of a genome-wide methylation association study, an analyst decided to include all the CpG sites available on a microarray as an enormous set of outcomes, and then to test for global association, at the genome level, with a covariate. Rejecting the null hypothesis in such a scenario would be of no help whatsoever in targeting the source of this signal. This example also illustrates the fact that PCEV is not a variable selection method, and our simulations showed that when the signal is very sparse and the signal-to-noise ratio is low, power is greatly reduced. In contrast, if the analyst decides to severely limit the number of outcomes jointly analyzed, then targeting the source of any identified associations will be straightforward, but there will be little benefit over univariate testing. In general, choosing an appropriate set of outcomes for joint analysis, i.e. finding the right balance between interpretability and power, is an important consideration that should consider the context and the biology. We feel that region-based analysis -e.g. where regions could be parts of the brain, genes, pathways or some other external data partitioning -is where this method should be considered and can shine."}, {"section_title": "Software", "text": "An R package called pcev, implementing both the block and the classical approach to PCEV, is currently available on both CRAN (https://cran.r-project.org/) and GitHub (https://github.com/GreenwoodLab/pcev)."}, {"section_title": "Authors' note", "text": "Data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf. Private sector contributions are facilitated by the Foundation for the National Institutes of Health (www.fnih.org). The grantee organization is the Northern California Institute for Research and Education, and the study is coordinated by the Alzheimer's Disease Cooperative Study at the University of California, San Diego. ADNI data are disseminated by the Laboratory for Neuro Imaging at the University of Southern California."}]