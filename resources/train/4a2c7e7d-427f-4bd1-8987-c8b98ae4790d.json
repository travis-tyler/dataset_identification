[{"section_title": "Abstract", "text": "The purpose of mathematics competitions, and in our case the South African Mathematics Olympiad (SAMO), is to promote problem solving skills and strategies, to generate interest and enthusiasm for mathematics and to identify the most talented mathematical minds. SAMO is organised in two divisions -a junior and a senior division -over three rounds. We analysed the results of the junior second round over seven years [2006][2007][2008][2009][2010][2011][2012]. Based on the literature a mathematical content framework was developed, dividing the mathematical content into seven broad content areas. In this paper we investigate the face validity, diagnostic attributes and predictive criterion validity of mathematics olympiad question papers over the period by focussing on the frequency of content area occurrence in the different items. We also look at performance of contestants in the different content areas as a broad diagnosis. Lastly we investigate the item performance, comparing the expected performance by the problem committee of experts setting the question papers and the actual performance of contestants. Topics such as numbers, algebra, patterns and functions, measurement, applications, modelling and logic were used abundantly whereas (quite surprisingly) there were few items on graphs, decimal fractions, spatial logic and vertices and edges of polygons, indicating that the face validity can be improved. Contestants performed best in items on algebra and weakest in items on statistics. The ability of the problem committee to anticipate student item performance varied considerably and was significantly worse in 2012 than in 2006, indicating better predictive criterion validity in 2006."}, {"section_title": "Background", "text": "School mathematics in several countries has moved closer to mechanical calculation or numeracy (Taylor, 2008) . However, creative problem solving skills are crucial to be competitive in the global market of mathematics-based careers (Kenderov, 2006) . Across the world a shortage exists of young people taking up mathematics related careers, often caused by a negative attitude to the subject. According to the literature mathematics competitions may address both issues as they do not consist of a collection of routine tasks to be executed, but purposely emphasise that mathematics is about creative thinking and the development of problem solving methods (Kahane, 2009; Kenderov, 2006) . There is some evidence that participation in mathematics competitions improves performance in school mathematics (Bicknell & Riley, 2012) and attitudes towards mathematics as a subject (Gy\u00f6ngy\u00f6si, 2002; Bicknell & Riley, 2012) . In order for mathematics competitions to also identify ability, the competition instruments need to be valid, i.e. measure ability in creative thinking and problem solving.\nLittle research has been done on the impact and efficiency of mathematics olympiads and various questions arise about mathematics competitions (Somers & Callan, 1999; Gleason, 2008) . Issues that justify investigation include gender issues (boys generally perform better in olympiads than girls), the impact that exposure to competition mathematics has on university preparedness, the diagnostic strengths of olympiad papers and the validity and reliability of the question papers in mathematics competitions.\nIn this study we investigate the validity and diagnostic attributes of mathematics olympiad question papers -do they measure what they are supposed to measure. The investigation uses the South African Mathematics Olympiad as a case study. It focuses on how the spectrum of relevant content areas in the South Africa mathematics curriculum is covered in the olympiad question papers; how performance in the different mathematics content areas compares; and how well the experts setting the question papers predict the level of difficulty of the different items, i.e. at an appropriate level and with appropriate progression of difficulty. Gleason (2008) evaluated mathematics competitions using item response theory. His analysis showed that the multiple-choice format in the mathematics competitions provided sufficient information to discriminate ability levels of contestants, but that the most valid information is provided for discriminating between participants whose ability levels are near the mean. This would imply that this format can best be employed as an initial round of the competition to reduce the number of contestants."}, {"section_title": "The South African Mathematics Olympiad (SAMO)", "text": "The South African Mathematics Olympiad (SAMO), organised by the South African Mathematics Foundation (SAMF) (http://www.samf.ac.za/Default2.aspx) is the biggest mathematics olympiad in the country. The olympiad has been running since 1966. Participation has grown from 5 234 contestants in the first round of the first event to more than 86 000 contestants who participated in the 2016 olympiad. The olympiad involves high school students and consists of a junior division (grades 8-9) and a senior division (grades 10-12).\nA new structure with three rounds came into operation in 1992. A separate second round paper for juniors was introduced in 1994. A third round for juniors was introduced in 2004. The first round is written in March every year and the first round for the junior division consists of separate papers for grades 8 and 9. In this round schools are provided with the solutions -the teachers mark the papers and send the marks to the SAMF office. Consequently no detailed records of the first round answers are available.\nContestants who attain 50% or higher in the first round qualify for the second round. This time the grade 8 and 9 contestants write the same paper. Contestants have two hours to complete twenty multiple-choice questions. Second round answer sheets are sent to the SAMF office where they are marked electronically. In this round comprehensive data about contestants' answers are available. For this reason our study focuses on an analysis of the second round answers specifically for the junior division. The best 100 senior and junior contestants from the second round qualify for the third round. The junior third round paper consists of 15 open ended problems which have to be completed in four hours.\nThe objective of SAMO is not only to find winners. More important is mass participation. The aim of the SAMO is to promote mathematics as a subject and all learners are encouraged to take part. The organisers believe that learners who take part in the olympiad benefit from the exposure to mathematics going beyond the curriculum which assists them to think out-of-the-box."}, {"section_title": "Mathematics competitions worldwide", "text": "The main goal of mathematics competitions is to enrich the study of mathematics. Although inspiring for the better students, tasks can be developed at different levels, also allowing average students to be exposed to the various benefits of competitions (Gy\u00f6ngy\u00f6si, 2002) . Bright students need challenges to keep their minds actively focussed on mathematics and prevent them from moving to endeavours outside mathematics they may find more appealing. Students of every level, background, ability or motivation should be challenged, not only bright students (Gy\u00f6ngy\u00f6si, 2002) . For students with less motivation, challenging mathematical tasks can serve to attract them to mathematics learning rather than to the mastery of algorithms or routine methods. However, it has been found that even the learning of routine material is improved when taking place in a challenging environment (Barbeau & Taylor, 2009 ). Rather than focusing on a small group of winners, broad participation in competitions is more important since by preparing for the competition and trying to solve the problems during the competition itself, all participants increase their knowledge significantly (Kenderov, 2006) .\nCompetitions have been considered as elitist even though there has been a growth in participation in competitions in recent years, e.g. the European Kangaroo (Taylor, 2008) . Gender issues have also been raised by critics -there is evidence that boys are more successful than girls (Niederle & Vesterlund, 2010) . A closer examination reveals that a gender gap in mean scores is so small so as to be of little practical importance, even though the gender gap in the upper tail can be quite large (Niederle & Vesterlund, 2010) . However, even for the high achievers Desjarlais (2009) established that after controlling for ability no statistically significant gender differences in competition performance are evident. There are also arguments that competitions provide unnecessary pressure, stress and feelings of failure from excessive competitiveness (Davis, Rimm & Siegle, 2014) .\nIn spite of these concerns, there is a significant body of committed competition supporters worldwide who highlight many advantages (Gy\u00f6ngy\u00f6si, 2002; Bicknell & Riley, 2012) . Competitions stimulate interest in mathematics. Questions are often set in real world situations to which the students can relate, rather than pure mathematical situations. The impact competitions such as the European Kangaroo with more than 3 million participants, have, is difficult to overestimate (Kenderov, 2006) . Probably the most important advantage of competitions is that they fill a gap in the curriculum, providing an opportunity for students to be exposed to real problem solving and to appreciate some of the aesthetically pleasing parts of mathematics. Taylor (2008) appreciates the variety of mathematical approaches in competitions in comparison with normal classroom assessment tasks in school that are becoming more and more predictable. Kahane (2009) argues strongly for mathematics competitions since they particularly lend themselves to free investigation, imagination and creative activities. Bicknell and Riley (2012) plead for competitions to be acknowledged in school policy as part of the official mathematics programme and that equitable opportunities should be provided for students to participate in mathematics competitions. Kenderov (2006) sees competitions as providing a tool to identify and develop students with higher abilities and talent who do not experience any challenge in the standard curriculum and their mathematical abilities and talent then remain undiscovered and undeveloped.\nPerformance in mathematics competitions does not always correlate with classroom performance (Ridge & Renzulli, 1981) . However, experiences in competitions and related activities improve the preparation of the student for university study. Specifically, Taylor (2008) mentions the fact that many former olympiad participants have become research mathematicians.\nThe social impact of competitions is also mentioned by Bicknell and Riley (2012) and Kenderov (2006) . Mathematics enrichment activities can be viewed as events generating discussions among the students, since competition problems can often be solved in more than one way thus provoking discussions. These informal social interactions might be as important as participation in the competition itself for acquiring new mathematical knowledge. These social interactions can happen in preparation for the competition, working on problems from previous competitions, or sharing after a competition.\nOne of the important benefits of participating in mathematics competitions is the exposure learners get to problem solving. Problems used in problem solving create a challenge for the student, which occurs when there seems to be no standard method of solution. You have to reflect and analyse the situation, possibly bringing together some diverse factors (Barbeau & Taylor, 2009) . Although the ultimate objective is to meet the challenge, i.e. to solve the problem, the process of grappling with its difficulties often results in better understanding, new insights and a sense of personal power:\nThe joy of confronting a novel situation and trying to make sense of it -the joy of banging your head against a mathematical wall, and then discovering that there may be ways of either going around or over that wall (Olkin & Schoenfeld, 1994, p. 43) .\nStudents who can handle unexpected situations and solve new problems are in great demand. Problem solving as part of mathematics has been reported on frequently in literature (e.g. Cai, 2003) . Lester and Cai (2010) define problem solving as \"mathematical tasks that have the potential to provide intellectual challenges for enhancing students' mathematical understanding and development.\" (p. 1).\nWith overfull current mathematics curricula in South Africa, teachers tend to focus on \"technical\" mathematics and recipe-driven manipulations get preference. The syllabus is contracted to manipulation skills only, leaving little time for using these skills in various ways to solve problems in everyday life (Taylor, 2008) . Performance in examinations has become almost too important, causing teachers to spend any available time on examination coaching. This approach again does not promote problem-solving activities. Formulas tend to hide the real content of concepts and to create stereotypes (Gy\u00f6ng\u00f6si, 2002) . A stronger focus on problem solving not only contributes to the development of students' higher-order thinking skills but also improves positive attitudes towards mathematics. (Lester & Cai, 2010) ."}, {"section_title": "Setting olympiad question papers", "text": "SAMO's question papers for mathematics competitions -as with most academic competitions -are set by problem committees of 5-7 people comprising some senior university academics (mathematics professors), some former olympiad contestants and some school teachers. The latter are able to judge whether the cognitive level of the questions is appropriate for the contestants. We will refer to all problem committee members as experts.\nResearch on the way competition papers are set, and the validity and reliability of olympiad papers is almost non-existing -this points to a serious research need. In relating his experience as member of problem solving committee, Miguel (2012) highlights the dilemma of developing problems that are both beautiful and have the right difficulty. To address the face and criterion validity requirements, rather than developing individual problems, a set of problems is developed that are balanced with regard to content areas and difficulty.\nThe working procedure for problem committees is fairly standard (for instance, see Kenderov, 2006) . Committee members would come to committee meetings well prepared. Each member has to contribute a few problems that can serve as possible items for the eventual question paper. New problems are designed by committee members consulting a variety of sources, including question papers of other international mathematics competitions. They would often get ideas from existing problems to develop a \"new\" problem.\nAt the SAMO problem committee meetings (twice a year) members work through the available problems and specifically focus on the following validity issues:\n\uf0b7 Face validity: A representative coverage of the mathematics content. \uf0b7 Predictive criterion validity: Coverage across different levels of difficulty and with progression of difficulty from the beginning to the end of the paper.\nItems in the SAMO test are ranked from 1 to 20 according to the level of difficulty as perceived by the problem committee who sets the papers. Items 1-5 are considered to be easy or accessible to most contestants (attracting 4 marks each); items 6-15 are considered to be moderately difficult (for five marks each) and items 16-20 are perceived to be difficult (for six marks each). In our analysis we weighted items according to their levels of difficulty, using the number of marks contributed to the total. This means that in finding the average performance for a particular content area, the level of difficulty was accounted for, i.e. more difficult questions contributed more to the average than easier items.\nValidity and reliability form an over-arching backdrop during the entire process. After the committee has reached agreement on the final paper, an external moderator would work through the paper to ensure that the question paper complies with the validity issues mentioned."}, {"section_title": "Research questions", "text": "The main purpose of this study is to investigate the validity and the diagnostic attributes of mathematics olympiad question papers. The study focuses on three issues: "}, {"section_title": "Mathematical content framework", "text": "In the USA the NAEP (National Assessment of Education Progress) report is a national data source for achievement by learners (4 th , 8 th and 12 th grades) in mathematics and other subjects (Neidorf, Binkley, Gattis & Nohara, 2006) . Their assessment framework is based on the collaborative input of a wide range of experts from government, education and business. Similarly, the development of the assessment framework for TIMSS (Trends in International Mathematics and Science Study) involves mathematics experts and education professionals from many countries (Mullis, Martin, Ruddock, O'Sullivan & Preuschoff, 2009 ).\nBoth the NAEP and TIMSS mathematics frameworks have five content areas in the content dimension: numbers; measurement; geometry; data; and algebra. Looking at cognitive dimensions, NAEP has three categories (conceptual understanding; procedural knowledge; and problem solving), whereas TIMSS has four categories (knowing facts and procedures; using concepts; solving routine problems; and reasoning).\nIn South Africa the recently introduced CAPS (Curriculum Assessment Policy Statement) mathematics learning areas for grades 8-10 include numbers, operations and relationships; patterns, functions and algebra; space and shape (geometry); measurement; and data handling (statistics). The cognitive distinction (mathematical ability) is between knowledge; routine procedures; complex procedures; and problem solving (DBE, 2011).\nThe content of SAMO over the years has been close to the South African national curriculum but with a somewhat different focus on subtopics. For this study an assessment framework with seven content areas was developed by the authors using the CAPS framework as well as the TIMSS and NAEP frameworks as a basis. We refer to this framework as the MANGSLO classification: M.\nMeasurement, applications, modelling A.\nPatterns, functions and algebra N.\nNumbers, operations and relations G.\nGeometry, space and shape S.\nStatistics, data handling L.\nLogic O.\nOthers\nEssentially the MANGSLO framework is the same as the CAPS, NAEP or TIMMS frameworks, but because of the nature of competition mathematics, we added logic as a separate content area. Each of these main content areas was divided into subtopics in the following classification. We illustrate some of the subtopics with an example from one of the question papers. The actual test papers consisted of multiple-choice questions (MCQ) but for the purpose of this description the MCQ format is dropped. "}, {"section_title": "M: Measurement, applications and modelling", "text": ""}, {"section_title": "Research design", "text": "This study investigates the second round of the junior SAMO test, compiled by a problem committee of experts as described earlier. Contestant responses for seven years (2006) (2007) (2008) (2009) (2010) (2011) (2012) were considered. The number of contestants in each year is given in Table 1 . These numbers depend on how many contestants made it through round 1 of the competition. Contestants need a mark of at least 50% in round 1 to qualify for round 2. All 140 items in these seven question papers were allocated to one or more of the mathematical topics in the MANGSLO classification scheme mentioned in the previous section. The classification was done independently by the researchers, assisted by members of the problem committee, and topic allocations were then discussed. There was a remarkable agreement between different raters, indicating a high inter-rater reliability. Some items fall into more than one content area and such items were considered in all content areas in which they were classified as illustrated by the following item."}, {"section_title": "What is the smallest value of n such that the product n! = 1\u00d72\u00d73\u00d7\u00b7\u00b7\u00b7\u00d7 n ends in at least 10 zeroes?", "text": "This item was considered to belong to three topics, N4 (Properties of numbers), L1 (General logic) and N5 (Powers and exponents).\nTo compare expected with empirical performance, the question papers of two years, 2006 and 2012, were considered as case studies. The anticipated level of difficulty as envisaged by the experts who set the question papers was compared with the actual performance of contestants in the papers."}, {"section_title": "Results", "text": ""}, {"section_title": "Frequency of topics (face validity)", "text": "Since some items were classified into more than one topic, rather than 140 (7 papers of 20 items each) there were 245 item classifications. The frequency distribution of item classifications in the main-and sub-content areas (over the seven years) is given in Table 2 . The most popular sub-topics were L1 (General logic) with 24 item occurrences, M4 (Mass, area, volume) with 19 occurrences and N4 (Properties of numbers) with 17 occurrences. It is surprising that there were no items in two sub-topics, i.e. A6 (Graphs) and N9 (Decimal fractions). The sub-topics M2 (Date and clock arithmetic), L2 (Spatial logic) and G3 (Vertices and edges) were also not popular with only one item occurrence each over the seven years.\nConsidering face validity, these results can be compared to the CAPS weighting of content areas as in Table 3 -the CAPS weighting is the average between the prescribed weightings for grades 8 and 9 in DBE (2011). As is clear from Table 3 , amongst the SAMO items content areas A (Algebra, patterns, functions) and G (Geometry, space and shape) are under-represented as compared to the school curriculum. In contrast, content areas such as M (Measurement, applications, modelling) and N (Numbers, operations and relations) are over-represented. The face validity of the SAMO question papers can at best be described as moderate."}, {"section_title": "Performance in content areas (diagnostic attributes)", "text": "To address the question relating to the diagnostic value of the question papers over the seven years, we compare performance in different content areas over the seven years. As explained earlier, the average performance in the different items was weighted according to the expected level of difficulty. Table 4 shows the weighted performance in the different content areas over the entire period of seven years together with the overall weighted performance over the entire period. The last column in Table 4 shows that contestants performed best in A (algebra, patterns, functions), and performed relatively poorly in S (Statistics and data handling). Table 4 also indicates that student performance in M (Measurement, application, modelling) does not show any real trend over the years, but there was relative poor performance in 2008 and 2012. In contrast, performance in A (Algebra, patterns and functions) was fairly consistent over the seven years, with 2012 showing the highest peak (58%) and 2011 the relatively low weighted average of 29%. In terms of student performance in N (Numbers, operations, relations) Table 4 shows no real trend over the seven years, but there were bad years (2007, 2008 and 2010) where the weighted performance of students was below 30%. Equally, student performance in G (Geometry, space and shape) does not show any real trend over the seven years, with four bad consecutive years (2007 -2010) when the weighted performance of students was below 30%. As already identified from the overall performance, student performance in S (Statistics and data handling) was consistently poor, with 2006 Table 5 compares the expected performance with the empirical performance by the students for the two years 2006 and 2012. The experts considered items 1-5 to be easy or accessible to most contestants, items 6-15 are classified as moderately difficult and items 16-20 as difficult.\nAccording to Table 5 , the actual student performance for 2006 shows that the easy items are (in this order) items 2, 4, 6, 9 and 1. In this category of questions, items 6 and 9 -planned as moderately difficult items -were found easy by students whereas item 5 and 3 were experienced as moderately difficult by the students with item 3 occupying 15 th position. Items 5, 7, 14, 10, 8, 15, 12, 13, 18 and 3 were experienced as moderately difficult by the students. According to the problem committee, beside items 6 and 9 that did not make it in this category, item 11 also is missing, it was perceived difficult by the students and item 18 in this category should be in the last category. Finally, items 17, 19, 16, 11 and 20 were experienced as difficult by the students. Except for Item 11 these agree with how the problem committee planned the paper. So for 2006 the overlap between expected and empirical difficulty is fairly high in all three the groupings of items. The Spearman rank correlation coefficient was calculated between the rankings as made by the experts and the actual performance. For 2006 this correlation coefficient is 0.73, which on the sample of 20 items, is significant on a 0.01 level.\nFor 2012 the picture looks slightly worse. From the students' performance, it can be seen ( Figure 2 , Table 5 ) that items 3, 2, 14, 19, 17, 9, 8, 15, 7 and 11 were experienced as moderately difficult. Items 3 and 2 are supposed to be in the \"easy\" category and item 19 and 17 were planned to be in the \"difficult\" category according to the problem committee. Finally, items 5, 18, 20, 12 and 10 were perceived as difficult by the students. Item 5 is the biggest surprise as it is expected to be accessible to most students (easy items category) and items 12 and 10 were intended to be moderately difficult by the problem committee that set the paper. So for 2012 the overlap between expected and empirical difficulty is poor for all three item groupings. Table 4 the low value of the 2012 coefficient is clear. The expected and actual performance of students vary greatly: easy items according to contestants' performance were: 16, 6, 4, 13 and 1. In this group, item 16 belongs to difficult category and items 6 and 13 to the moderately difficult category as per the problem committee's classification."}, {"section_title": "Discussion and conclusions", "text": "The objective of this study was to investigate the validity and the diagnostic attributes of mathematics olympiad question papers. We focussed on face validity -how the mathematics spectrum is covered regarding the different relevant content areas in mathematics; on diagnostic attributes -how performance in the different mathematics content areas compare; and on predictive criterion validity -how well the experts setting the question papers predict the level of difficulty of the different items, i. e. at an appropriate level and with appropriate progression of difficulty. that is moderate at best. However, the objective with an olympiad is somewhat different from the aims in the standard curriculum. As mentioned earlier, CAPS provides for four different cognitive levels, knowledge, routine procedures, complex procedures and problem solving. In an olympiad the focus is on problem solving. This may result in expecting different weightings for the various content areas compared to the standard curriculum. Content areas such as N (Numbers, operations and relations), A (Algebra, patterns and functions), M (Measurement, applications, modelling) and L (Logic) were well represented in the items over the seven years of the study. On the other hand, it is quite surprising that there were no items on graphs and on decimal fractions at all and that there were almost none on spatial logic and vertices and edges of polygons.\nBy considering the analysis of the frequency and performance of the different content areas in the second round question papers over the seven years, the question immediately arises whether these data can be interpreted as a spontaneous indication of the relative importance of these content areas or whether this indicates that the committees setting the problems should try to be more balanced with regard to addressing different topics in mathematics. We recommend that the problem setting committee consider deciding on a framework for proportional allocation of content areas in question papers, securing a balance between the different content areas to improve the face validity of the question papers.\nOur second research question addresses the diagnostic power of the question papers, comparing performance in the different content areas over the seven years in question. Contestants throughout performed best in A (algebra, patterns and functions) with a weighted average of 39%. This could probably have been expected. School curricula in South Africa are technically driven and procedures get a lot of attention at the cost of creative thinking or real problem solving. Participants in the olympiad are therefore more familiar with algebraic manipulations and working with patterns and functions than in most other topics in mathematics. The fact that contestants performed worst in S (Statistics and data handling) can be explained using the same argument. Statistics is a content area which has been introduced into the curricula in schools fairly recently and many teachers tend to neglect this topic. Our results do not indicate a significant change in performance in specific content areas over the period in question. Using the diagnostic results of the study, educators could consider putting a stronger focus on teaching statistics.\nRegarding the predictive criterion validity of the olympiad papers, the correlation between the actual performance of contestants and the anticipated ranking as set by the problem setting committee was significantly worse in 2012 compared to 2006. This fact indicates that the committee of experts sometimes gets it right but in other instances it is out of touch with what could be expected from contestants. This ability to anticipate contestants' performance could be improved by involving more school teachers in these committees who are working with the learners on ground level."}]