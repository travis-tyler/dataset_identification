[{"section_title": "Abstract", "text": "In many real-world applications, the samples/features acquired are in spatial or temporal order. In such cases, the magnitudes of adjacent samples/features are typically close to each other. Meanwhile, in the high-dimensional scenario, identifying the most relevant samples/features is also desired. In this paper, we consider a regularized model which can simultaneously identify important features and group similar features together. The model is based on a penalty called Absolute Fused Lasso (AFL). The AFL penalty encourages sparsity in the coefficients as well as their successive differences of absolute values-i.e., local constancy of the coefficient components in absolute values. Due to the non-convexity of AFL, it is challenging to develop efficient algorithms to solve the optimization problem. To this end, we employ the Difference of Convex functions (DC) programming to optimize the proposed non-convex problem. At each DC iteration, we adopt the proximal algorithm to solve a convex regularized sub-problem. One of the major contributions of this paper is to develop a highly efficient algorithm to compute the proximal operator. Empirical studies on both synthetic and real-world data sets from Genome-Wide Association Studies demonstrate the efficiency and effectiveness of the proposed approach in simultaneous identifying important features and grouping similar features."}, {"section_title": "INTRODUCTION", "text": "Regularized learning methods have recently attracted increasing attention in various applications. A common scenario that occurs in many studies is that the data sets we Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. investigated are of some natural (e.g., spatial or temporal) order; examples include the comparative genomic hybridization data [18] , prostate cancer data [17] and neuroimaging data [23] . For those classes of studies, it is often the case that the adjacent samples/features are similar and even identical. Moreover, in Genome-Wide Association Studies (GWAS), a causal single-nucleotide polymorphism (SNP) often exhibits high similarity with its nearby SNPs. It is thus desired to group nearby SNPs together. However, due to the ambiguity choice of reference allele during genotype coding [8] , we should group adjacent SNPs if their absolute values are close to each other.\nPrevious works [22, 26, 1, 25, 20] indicate that utilizing the inherent structure information in the data can potentially be beneficial for model construction and interpretation. Thus if the data exhibits some sequential order, we can potentially incorporate such prior knowledge into the model to improve performance. Meanwhile, due to the curse of dimensionality in the high-dimensional scenario, identifying the most relevant features is of crucial importance. In such a case, the traditional Lasso [16] model is insufficient to produce desired results since it tends to select only one of those highly correlated features [29] . There are mainly two approaches in the literature to address the problem. One approach adopts the fused penalty (e.g., fused Lasso), which can yield a sparse solution in both the coefficients and their successive differences [17, 18, 9] . However, it does not consider the case that adjacent features have high similarity but opposite signs. Another approach utilizes the graph structure among features (e.g., OSCAR) during model construction [3, 23, 28] . However, such an approach is too general and does not make full use of the specific structure of the genome data.\nGenerally, a GWA study focuses on investigating associations between genotypes (SNPs) and disease phenotypes. Previous studies [8, 28] have shown that incorporating the linkage disequilibrium (LD) information [13] between adjacent SNPs is beneficial in delineating association SNPs with smoothness and less randomness than individual SNP analysis. The studies in [8] also argue that the fused Lasso is not effective due to the ambiguity choice of coding reference. Thus, it is desired to penalize successive SNPs whose absolute values are close or identical.\nIn this paper, we consider a regularized model which uses a penalty called \"Absolute Fused Lasso\" (AFL) to solve such a problem. The AFL penalty encourages sparsity in the co-efficients as well as their successive differences of absolute values-i.e., local constancy of the coefficient components in absolute value. With AFL, highly similar features can potentially be grouped together even when their signs are different. Since the AFL problems are non-convex, it is challenging to develop efficient optimization algorithms. To this end, we employ the Difference of Convex functions (DC) programming to solve the non-convex problem. At each DC iteration, we adopt the proximal algorithm to efficiently solve the convex subproblem, which iteratively solves a proximal operator problem; we further use the Barzilai-Borwein (BB) rule for line search to accelerate convergence. One of the major contributions of this paper is to show that the proximal operator problem can be solved efficiently. Specifically, by exploiting the special structure of the regularizer, we first convert the computation of such proximal operator to an equivalent optimization problem via a Euclidean projection onto a special polyhedron. We then develop a gradient descent algorithm based on a novel restart technique by utilizing the optimality condition to efficiently solve the projection problem. We have conducted empirical evaluations on both synthetic data and real data. Experimental results demonstrate that the proposed DC-Proximal approach can achieve up to 50x speedup over general DC-ADMM (alternating direction method of multipliers) method-it allows us to perform efficient AFL modeling on large-scale genome data that contains tens of thousands SNPs."}, {"section_title": "THE AFL FORMULATION", "text": "In this paper, we consider the following Absolute Fused Lasso (AFL) regularization model:\nwhere loss(x) is a convex empirical loss function (e.g., the least squares loss or the logistic loss) and the AFL penalty is defined as:\nwhere \u03bb1 and \u03bb2 are non-negative regularization parameters. The second term penalizes differences of successive coefficients' magnitudes and can be considered as a grouping penalty. By imposing both the l1 and the grouping penalties, the AFL model can simultaneously identify important features as well as group similar (identical) features together. Different from the fused Lasso that penalizes the l1-norm on successive differences of coefficients (i.e., \u03bb2 p\u22121 i=1 |xi \u2212 xi+1|), the AFL encourages the smoothness of adjacent coefficients whose absolute values are close or even identical. Thus, strong successive signals can be identified by Eq. (1) even when their signs are different. In general, adopting the AFL penalty is expected to be more effective than the fused Lasso (See an example in Fig. 1 ). Note that in GWAS, the SNPs data we obtain through genotype coding are strongly affected by the choice of reference allele. Thus it is insufficient to just penalize the successive differences without considering the absolute values. In [8] , the authors use the l2-norm on the absolute difference, and apply coordinate descent to solve the proposed formulation. However, due to the use of l2-norm, the fused property, i.e., the absolute values of nearby terms tend to be identical, does not hold any more. In this paper, we propose to adopt the DC programming to solve the AFL problem (1) and apply the proximal algorithm to solve the sub-problem at each DC iteration. One of our main technical contributions is to develop an algorithm to efficiently solve the proximal operator problem by exploiting the special structure of the regularizer, which is a key building block of the proximal algorithm."}, {"section_title": "DC PROGRAMMING FOR SOLVING THE AFL PROBLEM", "text": "The AFL formulation in Eq. (1) is a non-convex optimization problem. We propose to use the Difference of Convex functions (DC) programming [15, 14] to solve it, where a key step is to decompose the objective function in Eq. (1) into the difference of two convex functions. By noting that\nwe decompose the objective function in Eq. (1) into the difference of the following two functions:\nBy linearization of f2(x), the per-iteration subproblem of the DC algorithm can be written as:\nwhere\nand sgn(\u00b7) is the signum function (Detailed derivation is provided in Supplement B). We summarize the DC programming in Algorithm 1. A key building block in this algorithm is how to efficiently solve the subproblem (3) . Next, we show that (3) can be efficiently solved via a proximal algorithm."}, {"section_title": "Algorithm 1 DC algorithm for solving the AFL Problem", "text": "Input: data matrix A \u2208 R n\u00d7p , response vector y \u2208 R n\u00d71 , regularizes \u03bb1, \u03bb2, and tolerance Output: x 1: Initialization:\nUpdate c k according to Eq. (4)."}, {"section_title": "4:", "text": "Update x k+1 according to Eq. (3)."}, {"section_title": "5:", "text": "k \u2190 k + 1. 6: end while"}, {"section_title": "THE PROXIMAL ALGORITHM", "text": "In this paper, we adopt the proximal framework [21] to solve the sub-optimization problem (3) at each DC iteration. Problem (3) is equivalent to\nwhere\nThe proximal algorithm solves problem (3) by generating a sequence {x k } by solving:\nwhere t k > 0 is chosen by some rule introduced below. It is easy to show that (6) is equivalent to the following proximal operator problem:\nwhere\nThus, it can be viewed as the gradient descent along the direction \u2212\u2207l(x k ) with the step size 1/t k plus computing the proximal operator problem (7). The pseudo codes of the algorithm are summarized in Algorithm 2."}, {"section_title": "Algorithm 2 The Proximal Algorithm", "text": "Input: A, y, \u03bb1, \u03bb2 Output: x 1: Choose \u03b7 > 1, tmax > tmin > 0 2: Initialization: x 0 , k = 0 3: while some stopping criterion is not satisfied do 4:\nChoose\nwhile line search criterion is not satisfied do 6:\nUpdate x k+1 according to Eq. (7). 7:\nend while 9:\nk \u2190 k + 1."}, {"section_title": "10: end while", "text": "To guarantee convergence, we use a line search criterion to choose an appropriate step size. Specifically, we accept the step size 1/t k if the following inequality holds:\nwhere \u03c3 \u2208 (0, 1) is a constant. To further accelerate the convergence speed of the proximal algorithm, as suggested by the studies in [21, 5] , we adopt the Barzilai-Borwein (BB) rule to initialize the line search step size as 1/t k,0 , where\n. Notice that a key step in the proximal algorithm is how to efficiently solve the proximal operator problem (7). In the next section, we introduce our efficient approach to solve (7) by exploiting the special structure of the regularizer."}, {"section_title": "EFFICIENT COMPUTATION OF THE PROXIMAL OPERATOR", "text": "For discussion convenience, we absorb t k into the regularization parameters \u03bb1 and \u03bb2, and omit the superscript k in Eq. (7). Then the proximal operator problem (7) can be simplified as follows:\nBy applying the procedure discussed in [4] , we have the following theorem:\nTheorem 1 implies that we can solve problem (8) in two steps: first solve (8) with \u03bb1 = 0 and then applying (9) to obtain the final result. Let \u03bb = 2\u03bb2 and \u03bb1 = 0, Eq. (8) can be rewritten as:\nWe propose to solve problem (10) efficiently by converting the proximal operator to a Euclidean projection onto a special polyhedron. To perform this transformation, we utilize some important properties of (10) as summarized in Lemma 1, where a detailed proof is provided in Supplement C.\nLemma 1. Let x * = \u03c0 \u03bb (u) be the optimal solution to (10). \u2200\u03bb > 0, we have:"}, {"section_title": "Equivalent Euclidean Projection Problem", "text": "Assume u \u2265 0, we define a sparse matrix R \u2208 R (p\u22121)\u00d7p as follows:\nIn addition, we denote a vector w \u2208 R p with the j-th entry defined as:\nWith Lemma 1 and the above definitions of R and w, we next present the following theorem which converts the proximal operator problem to an equivalent Euclidean projection problem.\nTheorem 2. Let u \u2265 0 and \u03bb > 0. Let\nand\nDefine the Euclidean projection of v onto P as:\nWe have\nThe above theorem implies that, the proximal operator in (10) can be solved by solving the Euclidean projection problem in (15) . To further simplify, our next theorem shows that, such a Euclidean projection problem can be solved by a simplified problem without the non-negative constraint.\nand\nWe have\nDetailed proofs of Theorem 2 and Theorem 3 are provided in Supplements D & E. In the next section, we discuss a restart technique to efficiently solve the Euclidean projection problem (18)."}, {"section_title": "The Restart Technique", "text": "Introducing the dual variable z \u2208 R p\u22121 for the inequality constraints in (18), we can obtain the Lagrangian in Supp. (E-48). The dual problem of (18) is equivalent to\nWe propose to solve (18) by simultaneously using the information of the primal and dual problems. The novelty lies in the usage of the so-called restart technique for fast convergence."}, {"section_title": "Optimality Condition and the Support Set", "text": "Our proposed restart technique is built on the introduction of the support set. Specifically, \u2200z \u2265 0 and denote g = \u03c6 (z), we define the support set as follows:\nThe support set S(z) is motivated by the optimality of the problem (20) , and shall be used for defining a nonlinear and discontinuous mapping from z to x. \u2200z * \u2265 0, it is a minimizer of (20) if and only if z \u2212 z * , \u03c6 (z * ) \u2265 0, \u2200z \u2265 0. From the optimality condition, we can build the relationship between the minimizer and its gradient, as summarized in the following lemma: Lemma 2. Let z * be the optimal solution to (20) and\nThe matrix RR T is very special, and it can be shown that its eigenvalues are 2 \u2212 2cos(i\u03c0/p), i = 1, 2, . . . , p \u2212 1, and thus it is positive definite. Note that RR T is the Hessian of \u03c6(z), which implies that the minimizer of (20) is unique."}, {"section_title": "A Nonlinear Mapping \u03c9(\u00b7)", "text": "Let e \u2208 R p be a vector composed of 1's, and eG j and vG j be the j-th group of e and v corresponding to the indices in Gj, respectively. For discussion convenience, assume z0 = zp = 0, then we can define the nonlinear mapping x = \u03c9(z) based on the support set S as:\nWith Lemma 2 and the definition of support set in (21), it is easy to show that the optimal solution to problem (18) can be exactly recovered by the support set S(z * ), as stated in the following theorem.\nTheorem 4. Let z * be the minimizer of the dual problem (20) , and x * be the minimizer of primal problem (18) . Then x * can be recovered by x * = \u03c9(z * )."}, {"section_title": "The Restart Technique and Properties", "text": "By introducing the support set S, Theorem 4 provides an alternative efficient way to computing x * from z * . Specifically, we can exactly obtain x * = \u03c9(z), wherez is an appropriate solution with S(z) = S(z * ) even ifz = z * . The intuition is that, for a given appropriate solutionz = z * , if S(z) is close to S(z * ), x = \u03c9(z) can be a better approximation thanx = v \u2212 R Tz for the primal. We then present a gradient projection algorithm based on the proposed restart technique, as summarized in Algorithm 3. Given an iterative solution z k , we do not perform the gradient projection at the point z = z k . Instead, we first compute x k = \u03c9(z k ). Then, we compute a restart point z k 0 "}, {"section_title": "Algorithm 3 Gradient Projection Algorithm with a Restart Technique", "text": "Input: v, \u03bb, R Output:"}, {"section_title": "Discussion", "text": "To end this section, we summarize our methodology for solving the proximal operator problem (8) as follows. We first show that a minimizer of problem (8) can be obtained by applying a soft-thresholding (9) on the solution of an alternative optimization problem (10) . By applying the properties of (10) introduced in Lemma 1 and two variables R and w defined in (11) and (12), we show that the proximal operator problem (10) can be convert to an equivalent problem (15) . In the sequel, we present to optimize an alternative problem (18) without the non-negative constraint through (19) . To solve problem (18), we develop a novel restart technique by introducing the support set (21) and a nonlinear mapping (22) . We propose to use Algorithm 3 to solve (18) for efficient computation."}, {"section_title": "EXPERIMENTS", "text": "In this section, we evaluate the AFL model (with the leastsquares loss) and the proposed algorithm on both synthetic and real-world data. We first evaluate the efficiency of our proposed algorithm in \u00a76.1.1, and then compare the AFL with the fused Lasso in \u00a76.1.2. Next, we evaluate the prediction performance of the AFL in two GWA studies in \u00a76.2.1 and \u00a76.2.2. Finally, we show the effectiveness of the AFL in identifying genetic risk factors in \u00a76.2.2. For all experiments, we use the following two stopping criteria for our algorithm: 1) the relative difference of function values between two iterations is less than a tolerance of 10 \u22125 , and 2) the algorithm exceeds the maximum iterations (1000 iterations)."}, {"section_title": "Evaluation on Synthetic Data", "text": ""}, {"section_title": "Efficiency of AFL", "text": "We present the empirical studies on the efficiency of our proposed algorithm by comparing our method with the approach that adopts the alternating direction method of multipliers (ADMM) to solve the sub-problem at each DC iteration. The experiments are carried out on a collection of randomly generated data sets A \u2208 R n\u00d7p and outcomes y \u2208 R n\u00d71 . In addition, denote\u03bb = A T y \u221e. We then conduct the evaluations in the following two scenarios: Scenario 1. Varying the number of features p with a fixed sample size and fixed regularization parameters \u03bb 1 and \u03bb 2 . We fix the number of samples n = 500 and vary the number of features p from 1,000 to 20,000. We set the regularizers as \u03bb 1 = \u03bb 2 = 10 \u22123\u03bb .\nScenario 2. Varying regularization parameters \u03bb 1 and \u03bb 2 with a fixed sample/feature size. We fix the n = 500 and p = 10, 000. We choose the values of (\u03bb 1 , \u03bb 2 ) from the following set: {(10 \u22124\u03bb , 10 \u22124\u03bb ), (10 \u22123\u03bb , 10 \u22123\u03bb ), (0.01\u03bb, 0.01\u03bb)}. Figure 2 summarizes the running time (in seconds) and speedup of AFL (proximal algorithm) over ADMM in the above two scenarios. From these figures, we have the following observations: (1) Our proposed algorithm is much more efficient than ADMM in both scenarios. ( 2) The speedup of AFL over ADMM increases as the feature size increases. This indicates that our proposed approach using DC programming and the proximal algorithm is capable of handling large-scale learning problems. (3) The speedup of AFL over ADMM increases as the regularized parameters become larger. Thus, our method is expected to be superior over ADMM in real-world applications, i.e., only a small number of features are relevant-a relatively large regularized parameter value is preferred. "}, {"section_title": "Comparison of AFL and Fused Lasso", "text": "In this study, we compare the AFL model with the fused Lasso. Recall that the AFL is designed to encourage the smoothness of adjacent coefficients whose absolute values are close or even identical. Thus if the adjacent features exhibit different signs in the model, the AFL approach is expected to be more effective than the fused Lasso.\nWe generate the synthetic data via the linear model y = Ax + , where the design matrix A \u2208 R 500\u00d75000 and the noise term \u2208 R n are randomly generated from normal distributions. The ground truthx \u2208 R n contains 10% of the signals, which are evenly partitioned into 5 groups. Specifically, within each group, we first continuously assign the same value for all the signals; and then, we randomly pick {0%, 1%, 2%, 5%, 10%} of the signals and change their signs to the opposite. The regularization parameters \u03bb1 and \u03bb2 are chosen from the interval [10 \u22124\u03bb , 0.9\u03bb] using five-fold crossvalidation for both the AFL and the fused Lasso. We then evaluate the models on a 100 i.i.d. samples testing set. The SLEP package [7, 9] is utilized to solve the fused Lasso problem. We report the averaged prediction performance of 10 replications in Table 1 .\nWe observe from Table 1 that the AFL approach provides better prediction performance than the fused Lasso in most cases. If the ground truthx does not contain too many opposite adjacent signals, both AFL and the fused Lasso can recover the original signal accurately. However, when the number of opposite signals increases, AFL outperforms the fused Lasso significantly. The reason is that, with the AFL penalty, the model tends to select those highly similar adjacent features even if their signs are different. Therefore, the AFL approach is more robust than the fused Lasso in such cases. "}, {"section_title": "Real-world Applications", "text": ""}, {"section_title": "GLT1D1 Data Study", "text": "In this study, we evaluate the AFL approach on a realworld GWAS data called GLT1D1. The data set containing 210 samples and 1,782 SNPs [28] . The major objectives in this study are predicting the gene expression level of GLT1D1 as well as identifying disease risk SNPs. To construct our predictive models, we randomly pick 2/3 of the samples to form the training set and use the same method in \u00a7 6.1.2 to choose the best parameters. We compare the AFL model with the fused Lasso on the remaining 1/3 of the data. The averaged results of 10 replications are summarized in Table 2 . Table 2 shows that the AFL approach achieves better prediction performance in terms of MSE. In addition, our model selects a smaller number of SNPs, which demonstrates the need of considering the absolute values in GWAS due to the ambiguity choice of reference allele during genotype coding. "}, {"section_title": "ADNI WGS Data Study", "text": "In this study, we evaluate the AFL model on the Alzheimer's Disease Neuroimaging Initiative (ADNI) whole genome sequence (WGS) data. Particularly, we investigate imaging genetics associations between imaging phenotypes and SNPs (within the 19th chromosome) using the regression model with the AFL penalty. We follow the procedure in [24] to process the SNP data and the data set contains 717 subjects and 131,670 SNPs. The baseline entorhinal cortex (EC) volume and hippocampal (HIPP) volume are chosen to be the responses, as these are two major brain regions affected by the Alzheimer's disease (AD)."}, {"section_title": "Comparison of Prediction Performance", "text": "We first compare the predictive performance of the AFL model with the fused Lasso. We randomly pick 90% of the samples to form the training set and the remaining 10% of the samples to form the testing set. We perform five-fold cross-validation to choose the best regularization parameters from the interval [10 \u22124\u03bb , 0.9\u03bb]. We report the mean squared error and the number of nonzero coefficients of 10 replications in Table 3 . Table 3 shows that both approaches achieve similar predictive performance in terms of MSE. Specifically, in EC task, AFL achieves a slightly lower MSE by selecting a smaller number of features. In HIPP task, AFL selects more features than the fused Lasso. We take a careful look at the SNPs identified by AFL. The AFL detects several SNPs located in three gene including PVRL2 (rs12972156, rs12972970, rs34342646), APOE (rs769449, rs769450, rs429358) and APOC1 (rs12721051, rs56131196, rs4420638) and assign them into correct groups. Note that the sign of the model coefficient of SNP rs769450 is different from its two adjacent SNPs (i.e., rs769449 and rs429358); the fused Lasso approach fails to group those three SNPs appropriately. Figure 3 shows the study results of EC and HIPP. In the experiment, we can observe that the AFL model can successfully capture AD risk genes including PVRL2 [10] , TOMM40 [12, 6, 11] , APOE [10, 12, 11, 19] and APOC1 [27, 19] . Moreover, the AFL is capable of performing automatic feature grouping even when the signs are different, e.g., rs769449, rs769450 and rs429358 in APOE exhibit high similarity in absolute values. However, the fused Lasso fails to correctly group SNPs like rs769450 since their signals are different. In Table 4 , we further present some statistical scores of SNPs selected by the AFL model, including the p-value 1 (P) and odds ratio (OR) association score. We can observe that most of the selected SNPs achieve high statistical significance."}, {"section_title": "CONCLUSIONS", "text": "In this paper, we study a regularized learning model based on absolute fused Lasso penalty. The AFL penalty encourages sparsity in the coefficients as well as penalizes differences of successive coefficients' magnitudes. Due to the nonconvexity of the proposed model, we propose to use the DC programming to solve it. At each DC iteration, we solve a convex regularized sub-problem via the proximal algorithm. The proximal algorithm iteratively solves a proximal operator problem and adopts the Barzilai-Borwein rule for line search. One of our main technical contributions is to develop a highly efficient algorithm to solve the proximal operator problem via a Euclidean projection based on a novel restart technique. Experimental results on both synthetic and realworld data demonstrate the effectiveness and efficiency of the proposed algorithm."}, {"section_title": "ACKNOWLEDGMENTS", "text": "This work was supported in part by research grants from NIH (R01 LM010730 and RF1 AG051710) and NSF (IIS-0953662 and III-1421057). RS8105340  RS3112439  RS3112440  \u2026  RS421812  RS3865427  \u2026  RS71352237  RS34224078  RS35879138  \u2026  RS12972156  RS12972970  RS34342646  RS283810  RS283811  \u2026  RS283815  RS6857  RS71352238  RS184017  RS157580  RS2075649  RS2075650  RS157581  RS34095326  RS34404554  RS11556505  RS157582  RS59007384  \u2026  UNKNOWN  RS1160985  RS760136  UNKNOWN  RS1038025  RS1038026  RS1305062  RS10119  RS7259620  \u2026  RS769449  RS769450  RS429358  RS7412  RS1081105  RS80125357  \u2026  RS10414043  RS7256200  RS483082  RS59325138  RS438811  \u2026  RS12721046  RS12721056  RS12721051  RS56131196  RS4420638  RS78959900  RS73052341  RS4803770   Coefficients   PVRL2  TOMM40  APOE APOC1 near "}, {"section_title": "B. DC PROGRAMMING FOR SOLVING AFL", "text": "The AFL formulation in Eq. (1) is a non-convex optimization problem. We propose to use the DC programming to solve it. By noting that ||xi| \u2212 |xi+1|| = |xi + xi+1| + |xi \u2212 xi+1| \u2212 (|xi| + |xi+1|),\nwe decompose the objective function in Eq. (1) into the difference of the following two functions:\nDenote the affine minorization of f2(x) as f\n, where \u00b7, \u00b7 refers to the inner product. Then the DC programming solves problem (1) by iteratively solving:\nSince x k , \u2202f2(x k ) is a constant, problem (24) is equivalent to:\nand let c k = \u2202f2(x k ), problem (24) can be rewritten as:\n(|xi + xi+1| + |xi \u2212 xi+1|). (26) Note that\nwhere d1 = dp = 1, di = 2, 2 \u2264 i \u2264 p \u2212 1; sgn(\u00b7) is the signum function. In addition, since max (|xi|, |xi+1|) = "}, {"section_title": "C. PROOF FOR LEMMA 1", "text": "Proof: We prove these properties of the proximal operator problem (10) as follows.\ni) If ui \u2265 0 and x * i < 0, we can constructx * as follows:\nx * i = 0,x * j = x * j , \u2200j = i. It can easily be shown that \u03c6(x * ) < \u03c6(x * ). This contradicts with the fact that x * is the minimizer to (10). If ui \u2265 0 and x * i > ui, we can constructx * as follows:\nx * i = ui,x * j = x * j , \u2200j = i. It can easily be shown that \u03c6(x * ) < \u03c6(x * ). This contradicts with the fact that x * is the minimizer to (10) .\nii) This property can be proved in a similar way as i).\niii) Letx * = \u03c0 \u03bb (|u|). We have Sincex * = \u03c0 \u03bb (|u|) and the minimizer is unique, it follows that sgn(u) x * needs to minimize \u03c6(x).\niv) We only focus on the case ui \u2265 ui+1 \u2265 0 in the proof and the results can be generated to the rest the cases using property iii). With properties i) and ii), we have ui \u2265 x * i \u2265 0 and ui+1 \u2265 x * i+1 \u2265 0. If this property does not hold, we have:\nNext, we show that x * i+1 > x * i leads to a contradiction. With a non-negative and assuming x * i+1 \u2212 > x * i , we constructx * andx * as follows:\nwhere the i + 1 entry of x * is decreased by in constructingx * and the i entry of x * is increased by in constructingx * . Denote\nIf x * i+1 < x * i+2 , we have\nIf x * i+1 \u2212 > x * i+2 , we have\nIf x * i+1 \u2265 x * i+2 \u2265 x * i+1 \u2212 , we have\nIn summary, we have\nSimilarly, we have\nIt is hard to directly prove either g1( ) or g2( ) is negative in the case of (28) . To arrive at the contradiction,"}]