[{"section_title": "Abstract", "text": "Multi-task learning is a type of transfer learning that trains multiple tasks simultaneously and leverages the shared information between related tasks to improve the generalization performance. However, missing features in the input matrix is a much more difficult problem which needs to be carefully addressed. Removing records with missing values can significantly reduce the sample size, which is impractical for datasets with large percentage of missing values. Popular imputation methods often distort the covariance structure of the data, which causes inaccurate inference. In this paper we propose using plug-in covariance matrix estimators to tackle the challenge of missing features. Specifically, we analyze the plug-in estimators under the framework of robust multi-task learning with LASSO and graph regularization, which captures the relatedness between tasks via graph regularization. We use the Alzheimer's disease progression dataset as an example to show how the proposed framework is effective for prediction and model estimation when missing data is present."}, {"section_title": "INTRODUCTION", "text": "The conventional machine learning frameworks consider only one learner seeking to solve a single task. In numerous applications, however, there are multiple tasks labeling the same data instances differently. When the tasks are related, the information learned from each task can be used to enhance the learning of other tasks. It is therefore beneficial to learn relevant tasks together simultaneously, as opposed to learning each task independently. Multi-task learning uses the intrinsic relations between multiple tasks to improve the generalization performance. It benefits all the tasks by leveraging the task relatedness and shared information across relevant tasks."}, {"section_title": "Multi-task learning", "text": "Extensive experimental results have shown advantages of multitask learning compared to single task in problems involving related tasks [1, 5, 9, 20] . Multi-task learning has been effectively used in a wide variety of applications including object location and recognition in image processing [4] , speech classification [17] , data integration from different web directories [18] , identification of handwritten digits [18] , multiple microarray data integration in bioinformatics [21] . In medical applications, multi-task learning has been successfully used to predict disease progression and estimate personalized medical models [8, 16, 22, 24] .\nAssume we have a total number of K different tasks. For each task i = 1, \u00b7 \u00b7 \u00b7 , K, we have n i observations in the observation matrix X i \u2208 R n i \u00d7p . The labels for the observations is denoted by\nR p\u00d7K be the model we want to learn such that y i = X i W i + \u03f5 i . Here \u03f5 i is random noise, and we want to learn W i from the observations X i and outcome y i .\nThe relationship among K tasks is represented by an undirected graph, where each task is a node, and the two nodes are connected if the two tasks are related. To encode the connections, let E be the set of edges in the graph. Define a matrix R \u2208 R K \u00d7| E | , where , and\nWhen no missing data is present, the model W can be efficiently estimated by solving the following optimization problem\nNote that there are other regression-based formulations for multitask learning problems [23] . The difference lies in the various penalization terms used. The proposed framework using plug-in estimators for covariance estimates can be easily adapted to fit the other multi-task learning formulations."}, {"section_title": "Model Alzheimer's disease progression with multi-task learning", "text": "In this paper, we use the Alzheimer's disease data from ADNI as an example of how the plug-in covariance estimators can help improve the performance of multi-task learning for medical and healthcare data. 1 The data X i is derived from the 1.5T MRI data of a total of 756 patients. The MRI images are preprocessed by UCSF using FreeSurfer, resulting in a total of 328 features. The objective is to predict the Alzheimer's Disease Assessment Scale cognitive subscale (ADAS-Cog) from the patients' MRI features. The ADASCog score is the most popular cognitive testing instrument for evaluating the symptoms of Alzheimer's disease [6, 13] . It includes questions on 11 tasks related to the patient's cognitive abilities like memory, language, attention, and praxis. In general, the higher the ADAS-Cog score are, the more significant the symptoms are. The MRI data (X i ) is collected at each patient's first visit (month number 0). The y i includes the patients' ADAS-Cog scores at five visits over a total of two years, each spaced six months apart. We treat each visit as a task, where tasks 1 through 5 are the patients ADAS-Cog score at month number 0, 6, 12, 18, and 24, respectively. The number of patients for each task varies, however the feature vector for each individual patient remains constant for all five tasks.\nThere are various ways to impose the relatedness of the tasks in order to incorporate the insight from task connections when learning all tasks jointly. For disease progression like this Alzheimer's disease case, due to the temporal relationship between the tasks, previous works [8, 16, 24] have assumed a graph model where tasks are connected in temporal orders (as shown in Figure 2 ). However, is this intuitive model accurate? To answer this question, we fit a Weibull distribution to the scores of each task (y i ), and plot the PDF of the five distributions in Figure 1 . As seen in Figure 1 , the score distribution slowly shifts over each visit, where tasks close in time have distributions close to each other 2 . This suggests that the graph model in Figure 2 is suitable. Notice the latter tasks tend to have higher probabilities for high score numbers. This reflects the progression of Alzheimer's disease in the patients. For up-to-date information, see www.adni-info.org. 2 With the exception of Task 3 seems to be closer in distribution to Task 5 than Task 4. This is potentially caused by the sharp drop of patient number in Task 5, which can introduce bias in the data of Task 5. However, since the distributions of Task 4 and Task 5 are extremely close, we keep the graph model in Figure 2 due to its interpretability. The graph model of the five-task problem for Alzheimer's disease progression and experiments in Section 3.1. Each task is related to its previous and subsequent tasks."}, {"section_title": "Handling Incomplete data", "text": "Medical and healthcare data often come with incomplete data. There are plenty of cases in clinical studies where some patients do not answer certain questions or measurements of some biospecimens are partly lost at various stages [10] . In the above mentioned Alzheimer's disease dataset, about 9% of the data is missing. The majority of machine learning approaches are however not developed for incomplete data, and ad hoc methods such as case deletion and single imputation are typically used to complete the data and then proceed with the learning algorithm [19] . Case deletion approach simply removes the feature vectors with missing values. For carefully collected clinical data that has only a small percentage of missing values, case removal can be effective [8, 16, 22, 24] . However, excluding information from the data can introduce bias to the data, and is not efficient for datasets with limited observations. Moreover, even if the case deletion method performs well in training, missing values in the test set are still unavoidable. We cannot remove testing data completely for the reason that some parts of features are missing.\nSingle imputation replaces missing data with estimations. The most common single imputation frameworks use mean imputation by averaging the observed data or alternatively the last observations carried forward in longitudinal studies or streaming. Low rank matrix completion [3] is another popular choice to impute missing values.Matrix completion methods are effective for recovering large portions of missing data. However, imputation methods neglect the uncertainty of missing values by replacing them with fixed instances, induce bias and underrate data variability [7, 11] .\nIt is important to note that in tackling the issue of incomplete data, the objective in this paper is making accurate inference and estimation of the underlying models, not to retrieve missing values. Imputation of missing data does not always positively affect inference. For instance, replacing missing samples with mean of the observations can artificially inflate correlation between data, which is undesirable for inference. Accordingly, incomplete data cannot be properly addressed separately from the modeling, learning and scoring procedures. We therefore focus on handling missing features within the learning process."}, {"section_title": "Multi-task learning with missing data", "text": "In this section we propose using plug-in covariance estimators when missing data exists in the observations. Expand Eq. (2) and replace the empirical covariance matrices X \u22a4 i X i /n i and X \u22a4 i y i /n i with their estimators \u0393 i and \u03b3 i , we have the following optimization problem\nThis step of replacement is similar to [14] , where the authors proposed using unbiased covariance estimators for the single-task LASSO problem (with no graph penalization). [14] shows that such plug-in estimators results in robust model estimation for single-task LASSO when the observations are corrupted with missing data. We extend the idea to the multi-task scenario, and propose in Section 2 a new plug-in estimators which show better performance at high levels of missing data."}, {"section_title": "PLUG-IN COVARIANCE ESTIMATIONS", "text": "In this section, we present three plug-in estimators for \u0393 i and \u03b3 i . The performance of the three estimators is evaluated in Section 3."}, {"section_title": "R-LGR", "text": "The R-LGR plug-in estimators are first proposed in [14] , where the \u0393 i and \u03b3 i are estimated using unbiased estimators, where\nwhere S d is the set of d\u00d7d positive-semidefinite symmetric matrices. The solutions are\nwhere\n, and \u03c1 i is the probability of data missing in X i ."}, {"section_title": "R-LGR1", "text": "R-LGR works well for small percentage of missing data. However, when large percentage of missing data is present (e.g., over 20%), \u0393\ni tend to over-estimate the autocovariance matrix value, and cause large bias in the estimation. To compensate for the bias, we introduce an \u2113 1 penalization in the autocovariance estimation problem, where\nThis solution can be obtained by soft-thresholding the eigenvalues i , we can soft-threshold d i (1) and compute\n, where the thresholding function T (\u00b7) is defined as\ni is the same as \u03b3 (0) i . This matrix LASSO formulation has previously been used for high-dimensional covariance matrix estimation [15] . However, [15] does not analyze the estimator for prediction or model estimation, and no experimental validation is provided."}, {"section_title": "Remarks", "text": "i when \u03b4 = 0 in Eq. (6) . The matrix \u2113 1 penalization term is introduced to reduce the intrinsic dimension of the covariance estimate, which is especially useful when p is close to or larger than n i . In this paper, we do not restrict p \u2265 n i . However, as shown in Section 3, the \u2113 1 penalization offers better estimation stability when the proportion of missing data (\u03c1 i ) is large.\nThe computation of \u0393\ni is the simpler and less computationally expensive than \u0393 (1) i , since \u0393 (1) i computes the eigendecomposition of p \u00d7 p matrices. However, since we only need to compute the eigendecomposition once for each data matrix X i , the cost is usually not prohibitive. with the development and use of efficient distributed algorithms for eigendecomposition, the added computational cost would be manageable.\nThe \u2113 1 penalization used for \u0393\ni can drive the eigenvalues of the autocovariance matrix to zero, resulting in a sparse covariance matrix estimation. This is useful when the data is low-rank, or when we know the data correlation has low-dimensional structures."}, {"section_title": "EXPERIMENTS", "text": "In this section, we show the results of the robust LASSO with graph regularization (R-LGR and R-LGR1) algorithms with both synthetic and real datasets. We compare the proposed algorithm with two benchmark methods: (a) Mean-Impute: fill in missing entries with variable mean, and (b) MF-LGR: fill in missing entries using matrix factorization [12] . For both (a) and (b), after the imputation, we estimate W with standard LASSO with graph penalization (LGR) as shown in Eq. (2) The experimental results are obtained using the projected gradient descent algorithm [2] ."}, {"section_title": "Synthetic Experiments", "text": "In the experiment, we evaluate the performance of the estimators on model and autocovariance estimation. Because there is no ground truth model or autocovariance matrix for real data with missing values, we perform these experiments with randomly generated data. Specifically, we generate five tasks with a graph structure illustrated in Figure 2 . The ambient dimension is p = 50, and we randomly generate n i = 100 observations for i = 1, \u00b7 \u00b7 \u00b7 , 5. For each task, W i is 7-sparse (i.e., W is 35-sparse). We randomly remove 5% to 40% of entries in the data. Each point in the plots is averaged over one hundred random realizations. All algorithms are tuned for minimum NMSE. Figure 3(a) shows the NMSE of W . Figure 3(b) shows the NMSE of the autocovariance estimator \u0393. For mean-impute and matrix factorization, we compute \u0393 = X \u22a4 X after the imputation. In both experiments, R-LGR1 outperform mean-impute and matrix factorization methods for all missing data percentages. R-LGR works as well as R-LGR1 for small percentages of missing data. However, when the percentage of missing data exceeds 20%, the R-LGR introduces more bias into the model estimator, and the NMSE for W grows larger than other competing algorithms, even though the error on \u0393 of R-LGR stays similar to other methods. "}, {"section_title": "Alzheimer's Disease Experiment", "text": "In this experiment, we evaluate the prediction accuracy of the proposed estimators on the Alzheimer's disease dataset. We randomly divide the data into 60% of training and 40% of testing sets. The original dataset has about 9% of missing entries. To access the plug-in estimators' stability with higher percentage of missing data, we also run experiments with randomly added 10% to 40% extra missing entries. We normalize all the features by \u2113 2 norm. The prediction NMSE (averaged over twenty random realizations) is summarized in Table 1 . The first column shows results on the original data, while the second through the fifth column show results with added missing entries. All algorithms are tuned for minimum prediction NMSE ( K i=1 \u2225\u0177 i \u2212y * i \u2225 2 \u2225y * i \u2225 2 ). R-LGR1 outperforms all other competing methods for all levels of missing data tested."}, {"section_title": "CONCLUSION", "text": "In this study, a novel method for handling missing data with multitask learning is proposed. Graph regularization is used as a general powerful tool to capture relatedness between connected tasks. To avoid bias and inaccurate inferences, the missing values are not handled separately from the modeling as performed in imputation and matrix completion approaches. We exploit plug-in estimators for the covariance matrices to handle missing features within the learning process. We also demonstrate that the proposed plug-in estimators provide better accuracy in model estimation and prediction accuracy when compared to other state-of-the-art imputation methods. Experimental results using synthetic and the Alzheimer's dataset show promising results for the R-LGR1 estimator.\nIn the future we intend to pursue research in two directions. First, we will expand the framework of plug-in covariance estimators for more multi-task learning methods with different objective function.\nSecond, we will investigate data-driven methods to effectively generate the R matrix for the graph model, especially in the presence of missing data."}]