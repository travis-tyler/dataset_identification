[{"section_title": "Abstract", "text": "This paper describes a method for building efficient representations of large sets of brain images. Our hypothesis is that the space spanned by a set of brain images can be captured, to a close approximation, by a low-dimensional, nonlinear manifold. This paper presents a method to learn such a low-dimensional manifold from a given data set. The manifold model is generative-brain images can be constructed from a relatively small set of parameters, and new brain images can be projected onto the manifold. This allows to quantify the geometric accuracy of the manifold approximation in terms of projection distance. The manifold coordinates induce a Euclidean coordinate system on the population data that can be used to perform statistical analysis of the population. We evaluate the proposed method on the OASIS and ADNI brain databases of head MR images in two ways. First, the geometric fit of the method is qualitatively and quantitatively evaluated. Second, the ability of the brain manifold model to explain clinical measures is analyzed by linear regression in the manifold coordinate space. The regression models show that the manifold model is a statistically significant descriptor of clinical parameters."}, {"section_title": "Introduction", "text": "Many neuroimaging applications require a summary or representation of a population of brain images. The conventional approach is to build a single template, or atlas, that represents a population (Lorenzen et al., 2005; Joshi et al., 2004; Avants and Gee, 2004) . Recent work introduced clustering-based approaches, which compute multiple templates in a data driven fashion (Blezek and Miller, 2007; Sabuncu et al., 2008) . Each template represents a part of the population. In a different direction, researchers proposed kernel-based regression of brain images with respect to an underlying parameter (Hill et al., 2002; Davis et al., 2007; Ericsson et al., 2008) . This yields a continuous curve in the space of brain images that estimates the conditional expectation of a brain image given the parameter value. A natural question that arises based on these investigations is whether the space spanned by a set of brain images can be approximated by a low-dimensional manifold. Our hypothesis is that a low-dimensional, nonlinear model can effectively represent variability in brain anatomy. This paper describes a method to learn a manifold model from a given population and evaluate its geometric fit and statistical properties.\nRecent work on statistical analysis of brain images with clinical data shows that shape is a statistically significant predictor for various clinical parameters (Chou et al., 2008; Davatzikos et al., 2008; Apostolova and Thompson, 2007; Baron et al., 2001; Busatto et al., 2003; Callen et al., 2001; Chung et al., 2001; Davatzikos et al., 2001 ). As such, we are interested in finding a manifold representation that captures shape variability across large sets of images. Such a manifold model is interesting in several ways. The manifold parametrization gives rise to a coordinate system for the data that indicates its position on the manifold. If the manifold is constructed to reflect shape differences, then brain shape can be quantified in this new coordinate system. In this way, the manifold coordinates can be used as a proxy for statistical analysis of brain populations. The manifold coordinates of a particular image (e.g. patient in a clinical setting) could be used to compare against examples in a database with similar brain shape and known clinical history and to predict the likelihood of specific pathologies. Projections of images onto the manifold of brain images could also be used to construct priors or atlases that would aid in automatic processing or clinical studies.\nThe approach in this paper builds on existing manifold learning techniques to obtain a manifold model from a set of brain MR images. Manifold learning is a specific approach to nonlinear dimensionality reduction based on the assumption that data points are sampled from a low-dimensional manifold embedded in a high-dimensional ambient space. The aim is to uncover the lowdimensional manifold structure from the samples in the highdimensional ambient space. Many methods for manifold learning have been proposed in the machine learning literature. Much of the recent work (Roweis and Saul, 2000; Belkin and Niyogi, 2003; Sch\u00f6lkopf et al., 1998; Tenenbaum et al., 2000) has focused around global or spectral methods. These methods have a closed form solution based on the spectral decomposition of a matrix that is compiled based on local properties of the input data. The bulk of manifold learning research has focused on low-dimensional manifolds embedded in high-dimensional spaces with Euclidean metrics. The space of brain images, however, does not fit directly into this setting. Research in shape analysis has shown that a Euclidean metric is not suitable for capturing shape differences in images (Twining et al., 2008) . In computational anatomy, researchers commonly resort to metrics based on coordinate transformations that align the images (Blezek and Miller, 2007; Sabuncu et al., 2008) . Therefore, the low-dimensional manifold we aim to learn is embedded in the space of images with a metric induced by coordinate transformations on the image-domain, as illustrated Fig. 1 . Note that the structure induced by image metrics based on transformations is often described as a manifold-in this paper we reserve this term to refer to the manifold of brain images as described by the data.\nIn many neuroimaging applications, it is necessary to map from ambient space to manifold coordinates or to construct brain images given manifold coordinates, which requires a generative model. These are the capabilities provided for linear models by principal component analysis (PCA), for instance, that allow one to compute loadings (manifold coordinates), to project data on the linear subspace, and to compute the residual or approximation error associated with these projections. Manifold learning methods thus far are mostly concerned with finding a low-dimensional parametrization of the data, but do not generally provide the tools to project or construct new data points. We propose to use the manifold coordinates obtained by one of the global manifold learning approaches to construct a generative manifold model that explicitly describes the manifold as a parametric surface in the ambient space. The parametric surface is formulated as a kernel regression over the manifold coordinates, that is, an estimate of the mean of the distribution of the manifold given the manifold coordinate. The reverse, a coordinate mapping from ambient space to manifold coordinates, is achieved in a similar manner. Weighted averaging of the manifold coordinates of the initial data set, again by kernel regression, yields a continuous mapping in the ambient space. The proposed methodology for the generative manifold model is motivated by the statistical representation of scattered data using principal surfaces (Hastie and Stuetzle, 1989) . In previous work (Gerber et al., in press), we describe a formulation by which generative manifold models via kernel regression give rise to principal surfaces, a nonlinear extension to PCA, as formally defined by Hastie and Stuetzle (1989) . Thus, the proposed framework for brain image analysis represents a statistically sound, higher-order representation of the space of brains.\nIn ) we showed that it is possible to approximate the space of brain images by a low-dimensional manifold. In this paper we present the method in greater depth. Additionally we show that the learned manifold can be used for statistical analysis on brain image populations. We apply the proposed approach to the OASIS and ADNI brain database, and show that the learned manifold provides a good fit in terms of projection distance and as a proxy for statistical analysis. Linear regression of the learned manifold coordinates with several clinical parameters provides strong evidence that the proposed manifold representation of brain image data sets captures important clinical trends."}, {"section_title": "Related work", "text": "An important aspect of our work is the ability to measure image differences in a way that captures shape. It is known that the L 2 metric does not adequately capture shape differences (Twining et al., 2008) . There are a variety of alternatives, most of which consider coordinate transformations instead of-or in addition tointensity differences. A large body of work (Younes et al., 2009; Christensen et al., 1996; Dupuis and Grenander, 1998; Beg et al., 2005) has examined distances between images using high-dimensional image warps that are constrained to be diffeomorphisms. Thus, distances between images are measured by computing a minimal diffeomorphic map, according to an appropriate metric on the transformation, between two images. This metric defines an infinite-dimensional manifold consisting of all shapes that are equivalent under a diffeomorphism. In a similar fashion, we define a metric based on elastic deformations. Our hypothesis, however, is that the space of brains is essentially (or approximately to within"}, {"section_title": "Space of Smooth Images", "text": "Manifold induced by diffeomorphic image metric some level of noise) of significantly lower dimension. That is, not all images reachable by a diffeomorphic deformation are likely to be brains.\nAn alternative metric for image differences is proposed by Pless (2003) and Zhang et al. (2006) , specifically for the task of manifold learning for sets of time-series data. They use a set of local features (phase maps produced from Gabor filters), which are designed to capture the local offsets of edges and other features of interest. Although this is a computationally efficient way to capture local shape differences, this approach does not lend itself to kernel-based construction of images, which is essential to the work in this paper.\nSeveral authors (Hill et al., 2002; Davis et al., 2007; Ericsson et al., 2008) have proposed kernel-based regression of brain images. The main distinction of the work described in this paper is that the underlying parametrization is learned from the image data. Our interest is to uncover potentially interesting structures from the image data and sets of parameters that can be compared against clinical variables. Kernel regression is used to explicitly represent the learned manifold in the ambient space.\nThe very limited work on learned manifolds for medical image analysis considers only manifolds that are generated from known variables with individual patients. For instance, Zhang et al. (2006) use manifold learning specifically to improve segmentation in cardiac MR images. In that application the breathing and heartbeat form a 2D manifold. In this paper, we propose to discover an unknown manifold structure from a large database of brain images in a way that explicitly learns the structure of inter subject variability. In LEAP (Wolz et al., 2010) , the Laplacian eigenmaps algorithm (Belkin and Niyogi, 2003) is employed to build an information diffusion based segmentation approach. The embedding quality is tested based on a clustering measure and leads to highly accurate segmentation results and indicates that the manifold assumption is valid. This work proposes a generative approach to examine the underlying manifold hypothesis."}, {"section_title": "Methodology", "text": "The approach proposed in this paper is best described as a two step process. First we learn a parametrization that captures the manifold structure in the data. Second we build a generative model, based on the learned parametrization, that explicitly represents the manifold as a parametric surface in the ambient space. The model provides the tools to construct data points on the manifold given parameters and to infer parameters given data points in the ambient space. We refer to the parameter space of the surface as the manifold coordinates C & R d where d is the intrinsic dimensionality of the manifold, the ambient space as A and the manifold as M & A.\nFor the first step, we use isomap (Tenenbaum et al., 2000) , a manifold learning algorithm that finds a low-dimensional Euclidean configuration of points that approximately preserves geodesic distances. Isomap computes a piecewise linear approximation to the geodesic distance through the construction of a nearest neighbor graph using the metric in the ambient space. The assumption is that the geodesic distances between nearby samples can be accurately approximated by the distance in the ambient space. This corresponds exactly to the definition of a manifold-the metric is locally linear. Metric multidimensional scaling (Cox and Cox, 1994) on the pairwise approximate geodesic distances yields a distance preserving d-dimensional Euclidean configuration of points. Thus, applying isomap yields a discrete mapping z i \u00bcf \u00f0y i \u00de defined only on the original input samples, the set of images S y \u00bc fy 1 ; . . . ; y n g 2 A.\nThe second step is the construction of the generative manifold model. The brain images S y are samples from a random variable Y with an arbitrary density p(y) defined on A. Assume we are given a coordinate mapping f :\nis a direct and explicative way to model a d-dimensional surface in A. Each point on the manifold represents the average over all brain images with the same parameter f(Y). For example, if f maps to age, g(x) is the average brain image at each age x. This is illustrated in (Davis et al., 2007) using manifold kernel regression as an unbiased estimate of the conditional expectation. However, the mappingf from isomap is only defined on the input data points, furthermore it is only an approximation to the manifold coordinates. Hence we model the coordinate mapping f on the complete ambient space as a kernel regression on the discrete z i \u00bcf \u00f0y i \u00de.\nIn summary the methodology consists of the following steps:\n1. Compute S z = {z 1 ,. . .,z n } by isomap on S y . 2. Build the generative manifold model with the coordinate mapping f by kernel regression over S z and the explicit manifold representation g(\nestimated by manifold kernel regression.\nAn important observation is that for both steps we do not need an explicit representation of the data in the ambient space. Isomap as well as the kernel regressions only rely on distances. Thus an adaption to an image space only needs an appropriate metric on A that captures shape differences. Furthermore both isomap and kernel regression only require distance to be accurate between nearby samples, large distances are not directly used but approximated using the nearest neighbors graph. We introduce a distance measure that captures shape differences between images based on small deformations using an elastically regularized coordinate transformation and show how to build the generative manifold model in this setting. The proposed distance measure is an approximation to the diffeomorphic metric for small deformations, justifying the notion of learning a submanifold of the space of diffeomorphisms."}, {"section_title": "Diffeomorphic space and approximation with elastic deformations", "text": "For sets of brain images, we restrict our attention to the set of square-integrable functions on the domain X & R 3 , i.e. the infinite-dimensional space L 2 (X). In shape analysis and computational anatomy tasks (Lorenzen et al., 2005; Davis et al., 2007; Joshi et al., 2004) diffeomorphic mappings are often used to assess shape differences. A diffeomorphic coordinate transformation between two images can be written as /(r, 1), where /\u00f0r; t\u00de \u00bc r \u00fe (2) is only well defined for images within the orbit of the diffeomorphic transformations for a given template image. In this setting, the ambient space A is a manifold in L 2 (X) induced by the Riemannian manifold of diffeomorphic transformations acting on a template image.\nTaking advantage of the fact that the proposed method relies only on distances between nearby samples, we propose to compute local approximations to the diffeomorphic metric. For two images that are very similar, the diffeomorphism is close to the identity and v is small. Because the velocities of the geodesics are smooth in time (Younes et al., 2009) , we can approximate the integrals for the coordinate transform and geodesic distance by a single vector field /\u00f0r; 1\u00de % v\u00f0r; 0\u00de \u00bc u\u00f0r\u00de;\nThus we can locally approximate the diffeomorphic distances based on an elastically-regularized deformation u\nwith k\u00c1k Q as in the diffeomorphic setting and allows for noise in the images. Therefore the proposed approach effectively learns a submanifold, estimated from the data, within the ambient space A of diffeomorphisms. For symmetry we define the distance as\nFor this work we choose Q = ar + (1 \u00c0 a)I, i.e. ku\u00f0r\u00dek In all our experiments we set a = 0.9. For a = 1 the magnitude is disregarded and a simple translation would yield a zero distance. In our experiments we found that a penalty on magnitude adds important information, such as for example the magnitude of the change in ventricle size. Note that it is an open problem to show that (5) satisfies the triangle inequality and hence is a metric or a semimetric.\nTo compute the elastic deformation distance for a pair of discrete images, we use a gradient descent scheme. For the L 2 constraint R X ky i \u00f0r \u00fe u\u00f0r\u00de\u00de \u00c0 y j \u00f0r\u00dek 2 2 dr 6 , the squared image intensity differences we introduce a Lagrange multiplier k\nTaking the first variation of (6) with respect to u results in the partial differential equation:\nand the derivative of L with respect to the Lagrange multiplier k is @L\u00f0u;\nWe alternate between solving the PDE in (7) with finite forward differences for a fixed k and update k based on (8) until the constraint is satisfied. We use a multiresolution, coarse to fine, optimization strategy to avoid local minima. Thus to register two images we have the steps 1. Initialize u with the identity transform u(x) = 0 2. Set k = 10. This initial value depends on the range of the image intensities with a trade-off between fast convergence for large k and accuracy using a smaller k. 3. For each scale from coarse to fine:\n(a) Solve the PDE in Eq. (7). (b) If the mean squared error between target and moving image is larger than increase k and repeat (a).\nImplementation details for the numerical solution of PDE's resulting from the minimization of transformations based on regularized vector fields are described in detail in (Modersiztki, 2004) ."}, {"section_title": "Generative manifold model", "text": "The generative manifold model is based on a coordinate mapping f : A ! C and formulating the d-dimensional manifold M as the conditional expectation g(\nThe composition of the two mappings gf provides a projection operator onto the manifold.\nWe compute the conditional expectation g(\nof a finite set of samples using a Nadaraya-Watson kernel regression, which converges to the expectation asymptotically. For the coordinate mapping f, we can use the point samples from S z = {z i , z 2 , . . ., z n }, the output of isomap on the pairwise elastic distances, but they must be extended to A by some suitable scattered data interpolation. In practice, we extend the data through an approximation that smooths out the noisy manifold coordinates obtained by isomap on the sample data. Thus, we also formulate f using Nadaraya-Watson kernel regression.\nThe level sets of the function f define equivalence classes of brain images that have the same manifold coordinates. The conditional expectation g(x) = E[Y-f(y) = x] represents the distribution of a given class of brains (i.e. choice of manifold parameters) by its average. This formulation reflects the fact that brain images will generally not lie exactly on a low-dimensional manifold. The assumption is that a collection of brain images can be captured by a probability density that includes two components: the location on the manifold and a noise term which is transverse to the manifold. In this way the proposed manifold resembles a nonlinear generalization of PCA, such as the method of principal surfaces. Informally, principal surfaces, as defined by Hastie and Stuetzle (1989) , pass through the middle of a distribution and minimize residual variance. In recent work (Gerber et al., in press), we show the formal relationships between the generative manifold model based on kernel regression mappings and principal surfaces. However, that work requires an optimization over the parameters S z of f and is beyond the scope of this paper.\nNote that the ambient space A is not Euclidean and thus the independent variables of f and the dependent variables of g are not in Euclidean space but in a space endowed with a metric based on coordinate transformations. For the coordinate mapping f this amounts to f \u00f0y\u00de \u00bc X n i\u00bc1 K y \u00f0d\u00f0y; y i \u00de\u00dez i P n j\u00bc1 K y \u00f0d\u00f0y; y j \u00de\u00de :\nRecall that d here is the elastic deformation metric as defined in Section 3.1. For the mapping g we adopt the manifold kernel regression formulation in (Davis et al., 2007) based on computing a weighted Fr\u00e9chet mean. The Fr\u00e9chet mean is defined as\nusing the kernel regression weights based on the manifold coordinates. Thus, the kernel regression g with distances based on elastic deformations yields g\u00f0x\u00de \u00bc arg min\nThe minimization in Eq. (11) is computed by the iterative method described in (Davis et al., 2007) . This is a greedy algorithm that alternates between computing transformations v i that minimize d(y, y i ) for a fixed target image y and updating the target image by y\u00f0x\u00de \u00bc The image in Fig. 1b summarizes an image data set consisting of 100 images of dark spiral segments with varying length and location. Fig. 2 shows images constructed by the generative manifold model learned from the 100 images. Thus, the images in Fig. 2 depict samples on the manifold embedded in the ambient space. Fig. 1 also shows the Fr\u00e9chet mean of the data in the diffeomorphic space and the Fr\u00e9chet mean on the learned manifold. The Fr\u00e9chet mean of the learned manifold suggest that it can be beneficial and necessary to model image data sets by a low-dimensional manifold."}, {"section_title": "Discussion of the manifold model", "text": "The distance measure fundamentally defines which features are modeled. The metric used in this work is a trade-off between measuring length of the vector field and smoothness. Spatial normalization removes the effects of scale, orientation and translation prior to computing the distance measures. If this pre-processing step is omitted the manifold estimation will incorporate scale, orientation and translation into the resulting model and, depending on sample size, can result in masking out more subtle changes.\nNote that the proposed approach is not restricted to work within the diffeomorphic framework. Other distance measures can readily be incorporated into the general manifold estimation framework. This provides opportunities for adaption to different image modalities or a more supervised approach. For example, the metric can be tailored to a specific region of interest or information theoretic measures such as Bregman divergences can be employed. For a supervised approach clinical information could be integrated into the metric, leading to a manifold estimation with a trade-off between geometric, image information based, fit and prediction of clinical information. However, this would require a careful cross-validation strategy to avoid overfitting.\nThe manifold model provides additional statistical and geometrical information. For example, a density estimation in the coordinate space, as shown in Fig. 11 , can be employed to detect subjects with low probabilities. For a geometric notion of an outlier, the projection distance can be used, as illustrated in Fig. 5 . The notion of projection distance and density on manifold could be combined to build a full density estimate in the ambient space, based on density on the manifold and variance orthogonal to the manifold.\nThe use of isomap for building the coordinate mapping f ensures that increasing the dimensionality of the manifold model will not change the lower-dimensional coordinates. However, the kernel regression representation of the manifold does not result in a perfect reconstruction with increasing dimension for finite sample sizes.\nKernel regression converges asymptotically to the true conditional expectation with increasing sample size and decreasing kernel bandwidth. The bandwidth in the kernel regression plays an important role, a too small bandwidth results in an overfit of the data while a too large bandwidth results in an overly smooth estimate of the conditional expectation. For example, the limit case of a zero bandwidth results in direct reconstruction of the original data points. On the other hand, for infinite bandwidth the regression results in a point estimate, the sample mean. The selection of an optimal kernel bandwidth is only in theory possible, based on knowledge of the underlying density. In this work we select the kernel bandwidth based on the average distance of the k = 15 nearest neighbor. This is a heuristic that yields an estimate such that, on average, 15 points are within the 60th percentile of the Gaussian kernel and thus have a relatively large influence on the estimate. Changing k to 10 or 20 does not affect the outcomes significantly. Note that both dimensionality and kernel bandwidth affect the quality of the manifold fit. Thus it is possible to have a decreasing fit with increasing manifold dimension depending on the kernel bandwidth.\nAdditionally the kernel regression forces the estimated manifold to lie within the convex hull of the given data set. Thus, the projection of a brain image, with a pathology not present in the original data set, will result in a brain image that resembles an image from the original data set, as illustrated in Fig. 5 ."}, {"section_title": "Results", "text": "The performance of the manifold model is evaluated in two ways. First, the geometric manifold fit is examined qualitatively and quantitatively in terms of projection distances. Second, the statistical explanatory power of the learned manifold parametrization with respect to clinical data is explored. The statistical significance is analyzed with linear regression of the manifold coordinates with the clinical parameters.\nThe evaluations were tested on 156 images of the ADNI database and the full OASIS cross-sectional database."}, {"section_title": "OASIS data set", "text": "The OASIS brain database 1 consists of T1 weighted MRI of 416 subjects aged 18-96. One hundred of the subjects over the age of 60 are diagnosed with mild to moderate dementia. The images are provided skull stripped, gain field corrected and registered to the atlas space of Talairach and Tournoux (1988) with a 12-parameter affine transform. Associated with the data are several clinical parameters. For the statistical analysis we restrict our attention to age, mini mental state examination (MMSE) and clinical dementia rating (CDR). Fig. 3 shows the population characteristics in terms of age, MMSE and CDR."}, {"section_title": "ADNI data set", "text": "We used a subset of T1 weighted MRI of 156 subjects aged 57-88 of the ADNI database.\n2 The images are provided geometry distortion corrected, intensity nonuniformity corrected and histogram peak sharpend. We additionally processed the images with a histogram equalization by a piecewise linear fit to the three histogram peaks of CSF, gray matter and white matter and an affine registration to an unbiased atlas. The population contains subjects diagnosed normal (38 subjects), with mild cognitive impairment (MCI, 84 subjects) and early Alzheimer's disease (AD, 34 subjects).\nFor the statistical analysis we used age, MMSE and the diagnosis. Fig. 4 shows the population characteristics in terms of age and MMSE. The 156 subjects were selected to represent a large portion of subjects diagnosed with MCI, with the aim to span the development from healthy to early AD."}, {"section_title": "Manifold fit evaluation", "text": "We compare the manifold fit against principal component analysis in Euclidean space by projection onto the d dominant (largest eigenvalue) eigenvectors. A nonlinear manifold representation in Euclidean space did not improve over PCA.\nFor PCA, we left one image out during the computation of the principal components and projected this image onto the principal components. This procedure was repeated to leave each image out once. For the generative manifold model, this involved computing isomap without the left out image y i , i.e. removing the row and column of the corresponding image from the pairwise distance matrix, and then compute the projection distance g(f(y i )) with g and f based on the reduced data set. Fig. 5 shows three images from the ADNI data set and reconstructions based on a 2D manifold model.\nOASIS data Fig. 6 shows axial slice 80 on 2D manifold coordinates obtained by the proposed method. A visual inspection reveals that the learned manifold detects the change in ventricle size as the most dominant parameter (x 1 ). It is unclear if the second dimension (x 2 ) captures a global trend. fold is closer than the nearest neighbor. The projection distances for PCA are scaled by average nearest neighbor in Euclidean space. Note that the measurements are in different metric spaces, capturing different properties, and as such only provide a qualitative comparison in terms of behavior with increasing dimension. PCA shows very little reduction in projection distance with increased dimension, barely one percent in the first three dimension. While the improvements for the manifold model are not drastic either, roughly ten percent in the first three dimensions, they are an order of magnitude larger than for PCA. The PCA projection distances do not indicate a linear low-dimensional subspace, rather the almost linear decrease suggests a isotropic normal distribution of the data. The change in gradient at dimension three for the manifold model suggests that the available data can be approximately described with a three-dimensional manifold. Note that the manifold based approach, due to the kernel regression, will not lead to a 0 projection distance with increasing dimension. Beyond dimension 3 the reduction in projection distances is marginal for the proposed approach. This indicates that a three-dimensional manifold is close to achieving the minimal projection distance possible for the given amount of data and the chosen kernel bandwidth. The projection distances for the manifold model are smaller than the average nearest neighbor distance, an indication that the learned manifold accurately captures the data. We do not postulate that the space of brains is captured by a 3D manifold. The approach learns a manifold from the available data and thus it is likely that given more samples we can learn a higher-dimensional manifold for the space of brains. and D e , the pairwise distance of the data points in the d-dimensional Euclidean space. That is, how well do the distorted distances D e explain the original distances D. The isomap residual roughly agree with the manifold projection distances and suggest also a two-to three-dimensional manifold. Fig. 8 shows axial slices of brain images generated with the proposed method by applying the mapping g to regularly sampled grid locations on the 2D manifold coordinates shown in Fig. 6 , i.e. a sampling of the learned brain manifold. The first dimension (x 1 ) shows the change in ventricle size. The second dimension (x 2 ) is less obvious. A slight general trend observable from the axial slices is less gray and white matter as well as a change in frontal horn ventricle shape from elongated to more circular.\nADNI data: The results from the ADNI data show similar behavior as the results from the OASIS data set, although less pronounced. Fig. 9 shows the projection distances as a function of the number of dimensions for the manifold model and PCA in Euclidean space as well as the isomap residuals. Again, PCA does not show any indication of a low-dimensional linear subspace. The projection distance for the manifold model are less telling than for OASIS data set. This might be explained by the differences in the two populations. Compared to the OASIS data set the ADNI data set spans a much smaller age range with stronger pathologies. It might be expected that the ADNI data set has larger variations in different directions due to the differences in anatomy caused by disease progressions. This is in contrast to the OASIS data set where a large portion of the variation is likely based on age. This explanation is supported by the statistical findings for the OASIS and ADNI data set in Section 4.4, where a description of MMSE or diagnosis in the ADNI data set requires additional dimensions beyond those that best describe age. In contrast, MMSE and CDR are contained within the dimensions that best describe age for the OASIS database. Fig. 10 shows the parametrization along the first and second dimension and along the first and sixth dimension. The first and sixth dimension are the dimensions that best explain diagnosis. Again ventricle size is the most significant change observable in both cases along dimension one. In Fig. 13 slices of reconstructed images along dimension one and 6 are shown, indicating some additional differences in ventricle shape."}, {"section_title": "Statistical analysis", "text": "The coordinate space of the estimated manifold can be analyzed with traditional statistical approaches. Fig. 11 shows a kernel density estimate of the distribution of brain images in the coordinate space of the 2D manifold estimated from the OASIS database.\nTo evaluate the statistical predictive power of the manifold coordinates, we fit multiple linear regression models of clinical data versus manifold coordinates. This includes regression of age, MMSE and CDR for the OASIS data set and age, MMSE and diagnosis for the ADNI data set. We compare the regression models on manifold coordinates to regression models on PCA coordinates. As for the geometric fit, regression models on a nonlinear manifold model (i.e. isomap) in Euclidean space did not improve over PCA.\nFor further comparison we also regressed CDR (OASIS data set), diagnosis (ADNI data set), and MMSE with age.\nThe regression of PCA and manifold coordinates involves a model selection process to select the independent variables that best describe the clinical parameters. We use the Schwarz's Bayesian information criterion (BIC) (Schwarz, 1978) with an exhaustive search to select the optimal model in each case. The BIC is \u00c02lnL(M) + kln(n) with n the number of samples, k the number of free parameters of model M and L(M) the maximum likelihood of model M. The BIC is derived based on the asymptotics of a Bayes solution to the model selection problem. The BIC penalizes models with a large number of parameters k, an application of the principal of Occam's razor. More precisely, the maximum likelihood of a model with a parameters needs to improve over a model with b parameters, with a P b, n (a\u00c0b) times to achieve a smaller or equal BIC. In addition to the coordinates, we included age as an independent variable in the models involving MMSE and CDR/diagnosis in the model selection process. This essentially controls for age; independent variables (PCA or manifold coordinates) that do not improve the models over using age as an independent variable are ignored. In all cases ages did not show up in the best fitting models for PCA and the manifold coordinates. Thus, adding age information does not lead to an improved fit under the BIC criterion. We denote the manifold coordinates with x = (x 1 ,. . .,x m ) and the PCA coordinates with l = (l 1 ,. . .,l m ). Thus the notation\ndescribes a multiple linear regression model of age on the first three manifold coordinates. OASIS data Table 1 shows the optimal linear regression results for the OASIS data set. In this data set only 235 out of the 416 subjects have MMSE scores. The regression for MMSE is thus only performed on this subset of the population. For MMSE and CDR the manifold model is the statistically most significant (largest F), and it has the largest R 2 statistic with the smallest residual but the differences between the PCA and manifold model are very small. For age, the best model is achieved with a 5D PCA, while the manifold representation achieves the best fit with the first three coordinates. This nicely supports the 3D manifold suggested by the reconstruction errors in Fig. 7 . Note that the manifold model achieves similar power with three parameters as does the PCA model with five parameters. The residual for MMSE and CDR is fairly large and it is not clear if the statistical significance is due to the relatively large sample size and large amount of healthy young subjects. Table 2 shows optimal linear models, as before, selected based on BIC and exhaustive search, for subjects aged 60-80. The results from this subset are comparable to the results on the full data. This suggests that the manifold is not only capturing the variation induced by the large differences in age among the subjects but also important clinical trends.\nADNI data: Table 3 shows the optimal linear regression results for the ADNI data set. For MMSE and diagnosis the manifold model is the statistically most significant with the smallest residual. For age the best model is achieved with three dimensions of the PCA coordinates, while the manifold representation achieves the best fit with the first two coordinates. Note that the three PCA coordinates that best explain age are not the first three (dominant) principal components which capture the most variation.\nFor explanation of diagnosis note that the manifold model has a significantly lower p-value than either age or the PCA coordinates. Of further interest is the inclusion of the sixth dimension of the manifold model for explanation of diagnosis, which supports the claims made in Section 4.3. Fig. 12 shows residuals from the linear models based on manifold coordinates plotted against the residuals from the PCA regression. The regression residuals show roughly the same behavior, i.e. large residuals in PCA correspond to large residuals in the manifold model of the same sign. However for the regression on diagnosis the residuals for the manifold model show a larger variation for subjects where PCA predicts the diagnosis fairly accurately. The manifold regression model yields a model with a more significant slope and thus some of the subject diagnosed with MCI tend more Table 1 Optimal linear regression models from the OASIS data set. The PCA coordinates are denoted with l i and the manifold coordinates with x i . A < entry denotes smaller than machine precision. Table 3 Optimal linear regression models from the ADNI data set. towards normal or AD, while the PCA model is closer to a constant model and is more likely to predict MCI. Fig. 13 shows axial slices of constructed images on a grid sampled along dimension one and six of the manifold coordinates to illustrate the differences occurring in these dimensions. The shading represent the slope of the linear model of row 8 in Table 3 . Dimension one captures the differences in ventricle size, while dimension six indicates an increase in the left lateral ventricle from top to bottom."}, {"section_title": "Computational considerations", "text": "The distance computation between two images requires an elastic registration which takes with our multiresolution implementation about 1 min on a 128 \u00c2 128 \u00c2 80 volume. The cost for learning the manifold from the 416 images of the OASIS database is high and requires about a week using a cluster of two 50 GHz processors.\nThe implementation of the elastic registration, as well as an isomap implementation, is available at http://www.cs.utah.edu/sgerber/software."}, {"section_title": "Conclusions", "text": "This paper presents a novel approach to represent the space of brain images by a manifold model. The linear regression results show that the generative manifold model is able to uncover, using the raw image data only, statistically significant important relations to clinical data.\nAn open question is whether the manifolds shown here represent the inherent amount of information about shape variability in the data or whether they reflect particular choices in the proposed approach. In particular, implementation specific enhancements on image metric, reconstruction, and manifold kernel regression could lead to refined results.\nThere are various directions for improvements on the proposed method. The diffeomorphic metric is restricted to images that are within the orbit of a given template under the transformation. Recent work on metamorphosis provides a elegant framework that yields a metric over the complete image space. The generative manifold model is directly applicable to an image space endowed with a metric based on metamorphosis. Another area of future work is to extend the approach to other image modalities. A metric that captures different properties or works with different image modalities can easily be integrated into the system.\nAn interesting question is whether the learned submanifold is truly nonlinear or if an approach such as principal geodesic analy- Manifold regression residual PCA regression residual (2) (1) (3) Fig. 12 . Residuals from the manifold regression model against the PCA regression model for: (a) MMSE and (b) diagnosis. The bottom row shows three selected images with large residuals. The first image has a very low MMSE score that is not well represented in the image set. The second image shows a subject with large ventricle scores but a perfect MMSE scores. Both approaches relate ventricle size to lower MMSE scores and thus predict a lower MMSE score. The third subject shows the inverse, a subject with small ventricles but a relatively low MMSE score. Fig. 13 . Slice number 80 of the reconstructed images along the statistically significant manifold coordinates (first and sixth dimension) for explaining the diagnosis. The shading indicates the gradient of the corresponding linear model on diagnosis (row 8 in Table 3 ).\nsis (PGA) (Fletcher et al., 2004) , a generalization of PCA to Riemannian manifolds, leads to similar results. Furthermore, PGA could be used to do analysis on the learned manifold."}]