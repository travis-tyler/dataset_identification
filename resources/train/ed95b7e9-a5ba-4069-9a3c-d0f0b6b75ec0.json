[{"section_title": "Abstract", "text": "Research into modeling the progression of Alzheimer's disease (AD) has made recent progress in identifying plasma proteomic biomarkers to identify the disease at the pre-clinical stage. In contrast with cerebral spinal fluid (CSF) biomarkers and PET imaging, plasma biomarker diagnoses have the advantage of being cost-effective and minimally invasive, thereby improving our understanding of AD and hopefully leading to early interventions as research into this subject advances. The Alzheimer's Disease Neuroimaging Initiative* (ADNI) has collected data on 190 plasma analytes from individuals diagnosed with AD as well subjects with mild cognitive impairment and cognitively normal (CN) controls. We propose an approach to classify subjects as AD or CN via an ensemble of classifiers trained and validated on ADNI data.\nClassifier performance is enhanced by an augmentation of a selective biomarker feature space with principal components obtained from the entire set of biomarkers. This procedure yields accuracy of 89% and area under the ROC curve of 94%."}, {"section_title": "INTRODUCTION", "text": "In the working model for Alzheimer's Disease (AD) progression, a cascade of events starts with the buildup of amyloid plaque, followed by tau-mediated neuronal injury, and then by memory loss and finally clinical diagnosis of AD [1] . Recently, Prestia et al. [2] have provided clinical evidence that the core biomarker patterns are c 1 onsistent with this model. Specifically, the model predicts that tracer retention on amyloid PET imaging and low A -42 concentration in the cerebral spinal fluid (CSF) should become abnormal earlier in the disease course, followed by cortical hypometabolism on F18-FDG-PET, and finally brain atrophy in structural MRI. Although biomarkers obtained through invasive collection of CSF and expensive PET imaging are the most consistent and reliable, predictive biomarkers that can be collected cost-effectively and in a minimally invasive manner would be preferred [3] .\nA number of investigators have reported progress in identifying plasma-based proteomic biomarkers and their effectiveness in predicting AD and mild cognitive impairment (MCI). In 2007, Ray et al. [4] identified 18 signaling proteins in blood plasma that can be used to classify blinded samples from MCI subjects who progressed to AD two to six years later. This study incorporated both unsupervised and supervised machine learning methodology. Ravetti and Moscato [5] re-analyzed the dataset of Ray et [6] using ADNI data. Among these, changes in APOE, BNP, CRP, and pancreatic polypeptide levels were also associated with AD diagnosis and CSF AD biomarkers. APOE has been identified as the most predictive biomarker by Johnstone et al. [7] , who also identified a limited set of paired biomarkers via univariate entropy filtering and the --k feature selection process, achieving accuracy in excess of 85%.\nOther investigators have modeled the longitudinal progression of clinical AD assessments. Doody et al. [8] performed mixed effects regression modeling to predict longitudinal performance on standard clinical measures of AD. A sigmoidal model of the longitudinal changes in AD assessment cognitive sub-scale (ADAScog) was developed by Samtani et al. [9] . Yet, the main contributors in their predictive model were demographic factors and clinical assessment. To our knowledge, there are no studies that incorporate the full set of AD biomarker data in a comprehensive model.\nComplex processes associated with AD are mediated by interactions of functionally related proteins [10] . Since these interactions between the plasma biomarkers are not fully understood, a model that incorporates as many of the biomarker data as practical could be valuable. In this paper, our goal is to build a predictor of clinical assessments from plasma protein biomarker data, which takes advantage of the full set of available ADNI biomarker data, and improves prediction accuracy, compared to previous investigations into plasma biomarkers prediction."}, {"section_title": "DATASET AND EVALUATION \u2020", "text": "Data used in our study were obtained from the ADNI database (adni.loni.ucla.edu). The set of biomarkers and the experimental procedures used to obtain them are described in [11] .\nParticipants received a diagnosis at their first or baseline visit to one of the consortium clinics of cognitively normal (CN), mild cognitive impairment (MCI) or AD based on clinical and neuropsychological testing in accordance with the guidelines in [12] . Twelve months after the baseline assessment, plasma samples were collected again but only from a subset of these participants. In our study, we analyzed data from CN and AD participants, whose plasma samples were collected at both baseline and 12 months. Hence, we used a selected subset of the plasma samples, including 39 CN and 65 AD patients.\nA rigorous quality-control procedure was observed on each of the set of 190 analytes as described in [11] . Analytes with more than 10% of samples below the assay detection limits were excluded by the ADNI Analysis Team. As such, 44 of the 190 analytes were excluded from our consideration, leaving 146 analytes for the feature selection and augmentation process.\nAs is customary, a log transformation was applied prior to feature selection because concentrations for these analytes are generally not normally distributed."}, {"section_title": "Evaluations", "text": "Previous studies on the AD plasma biomarkers investigated the predictive power of individual biomarkers as well as group of biomarkers [4] [5] [6] [7] . In selecting biomarkers for a feature set, entropy heuristic was first applied to filter out non-informative analytes. A common method for feature selection is the ( -)-k feature selection process [14] . Our work is an extension of these explorations, attempting to increase the predictive power by enhancing the feature space. We first re-evaluated three sets of biomarkers identified in [7] , as summarized in Table 1 . We then evaluated augmentations of these biomarker sets informed by clustering and dimensionality-reduction methods.\nClassifiers were trained on various enhancements to the biomarker feature space using the provided AD and CN labels. Due to the limited number of available samples, we used a 13-fold cross validation procedure for each classifier to evaluate performance. Based on the predicted output, we calculated various measures of the classifier's performance, including accuracy, specificity, sensitivity and area under the ROC curve. Note that as the dataset is imbalanced with more AD than CN subjects, a single measure cannot represent overall performance. "}, {"section_title": "METHODS", "text": "A na\u00efve selection of all 146 analytes without feature selection would lead to an overfit to the noisy and uninformative features, leading to high variance in the classification. The feature selection process treats this problem of variance but may result in too few features and hence an underfit model that exhibits classification bias. The bias problem is confounded by the relatively small (65 positive, 39 negative) training and crossvalidation set. Previous studies have concentrated on reduction of variance via feature selection but have ignored the unintended effect on bias that the smaller feature sets can yield. Our goal is to improve the trade-off between over-fit and under-fit by augmenting the feature space comprising selected features identified by previous literature [7] with additional features obtained from clustering and dimensionality reduction. Although the sample size of our data is not large, it is large enough to allow feature augmenting without incurring over-fitting problem.\nIn this work, we assume that the unselected features contain useful information yet are too noisy in their raw form to present individually. The literature is full of examples of improved classifier and regression performance from enhancement of feature sets with clusters of the original data. In [14] for example, an analysis of DNA microarray data was enhanced with Gene Set Enrichment Analysis, which improved the statistical significance of diabetic versus normal predictions. In our study, we have implemented and evaluated a number of clustering and dimensionality reduction methods to enhance the feature spaces of select plasma analytes. We have also investigated methods for improving classification performance via an ensemble of different classification algorithms. We evaluated the efficacy of various feature augmentations on the various classifier topology schemes."}, {"section_title": "Classifier Ensemble", "text": "Ensembles of classifiers reduce the potential for over-fitting that exists with high dimensional data and limited number of samples [15, 16] . As a result, such ensembles have been successfully applied to many bioinformatics applications, e.g. classification in microarray and proteomic data. An ensemble was constructed consisting of five conventional classification algorithms: libSVM [17] with linear kernel, binary decision tree, na\u00efve Bayes, logistic regression and perceptron. The latter four methods were provided by Matlab (The Math Works, Natick, MA). All classifiers were trained on and performed prediction on the same sets of data. The topology of the ensemble includes an aggregating libSVM classifier, as depicted in Figure 1 . The feature space of the aggregating classifier consists of the votes of the five first-layer classifiers. The aggregating classifier was trained on the same labels as the first-later classifier. We performed testing on the individual classifiers as well the ensemble result. "}, {"section_title": "Feature Clustering Methods", "text": "As described in Section 2.1, a number of clustering and dimensionality reduction methods were implemented and tested against the task of cross-validating the individual classifiers in Section 3.1 as well as the ensemble. These clustering methods are described below."}, {"section_title": "Latent Process Decomposition (LPD).", "text": "LPD is an adaptive version of Latent Dirichlet Allocation (LDA) [18] . In LDA, the dependency between features is explained by the unobserved topics [19] . Instead of imposing a multinomial distribution on each feature, the LPD assumes the observations of each feature follow a Gaussian distribution, which is more suitable for proteomic data. First, the hyper-parameters for latent processes were estimated with the mean and variance of each feature with an iterative method with training data. Second, the probability of an observation conditioned on each latent processes was estimated. The vector of conditional probabilities was used as the feature vector in the classifiers."}, {"section_title": "Mixture of Gaussian Model Clustering (GMM).", "text": "GMM clustering is similar to that \"soft K-means\" method, in that it considers clusters as Gaussian distributions centered on their means [20] . The algorithm maximizes the conditional probability of data given the center of clusters. The vector of conditional probability was used as the feature vector in the classifiers, similar to the manner in which LPD uses conditional probabilities."}, {"section_title": "Self-Organizing Feature Map (SOFM).", "text": "SOFM involves training a neural network by using unsupervised learning to produce a low-dimensional representation of the input space of the training samples [21] ."}, {"section_title": "Principal Component Analysis (PCA).", "text": "PCA decomposes of the covariance matrix of features into principal components, or eigenvectors ordered by decreasing eigenvalue load [22] . Only the most significant components are retained, thereby reducing the dimensionality of the representation."}, {"section_title": "Feature Augmentation", "text": "In constructing an augmented feature space, we evaluated two different methods to combine feature sets, as described in [23] . The first method, known as 'early fusion,' uses a single classifier trained on all the feature sets, i.e., the feature vector itself is augmented prior to training the model. This method has the advantage of simplicity and can potentially capture interactions among different features. However, features from different sources might require different preprocessing or scaling or scaling procedures, and may be suitable for different kernels (in the case of SVM). The second method, known as 'late fusion,' combines the outputs of autonomous classifiers trained on each feature type separately. When we tested late fusion with the ensemble, we implemented ten first-layer classifiers, two for each algorithm corresponding to the two feature spaces, namely the select biomarkers, and the clustering representations, respectively."}, {"section_title": "RESULTS", "text": "The first experiment illustrates the effect of the ensemble on the select biomarkers, without augmentation. We used three sets of biomarkers identified by Johnstone et al. [7] , as described in Section 2.1. We found that combining the three feature sets described in Table 1 into a single feature space yields better results than the individual feature spaces. We refer to this combined feature space comprising 11 single-features, 8 metafeatures and 8 longitudinal-features identified, as the selective feature set. Table 2 provides cross-validation results by libSVM against the three sub-spaces in the selective feature set. Table 3 provides the cross-validation results of the various classifiers in the ensemble, on the combined selective feature set. The effect of combining the three sub-spaces of the selective feature set can be appreciated by comparing Table 2 to the first row in Table 3 . The last row of Table 3 provides the cross-validation performance of the ensemble, which is clearly preferable to the individual classifiers on these data. The ensemble's improvement in accuracy and the area under the ROC curve (AUC), when compared with libSVM along, is largely due to improved specificity. The accuracy that we have obtained, 86% for the ensemble, is similar to the 85% reported in [7] , despite the fact that we have used less training data than Johnstone et al., who used 112 AD and 58 CN subjects. By subsampling our labeled data, we discovered that performance measures decrease by approximately 5% as the training data as cut in half. It appears that effect of combining the individual select feature sub-spaces coupled with the effect of the ensemble, compensates for the effect of a smaller training set. "}, {"section_title": "Feature Clustering.", "text": "We applied the 5 different clustering methods described in Section 3.2 on the 146 analytes available in ADNI dataset. Selection of the reduced dimension size s (in LDA-the number of topics; GMM Clustering-the number of clusters; SOFM -number of nodes in the network; PCA-number of principal components) is critical to the desired effect of improving the classification result. As an example, Figure 2 shows the effect on classifier performance on the number of principal components from PCA selected for the augmented feature space used in classification. An inspection of Figure 2 suggests that, for PCA, the value s=20 yields the best result. Although 21 or more features can be selected without compromising performance, these additional features would require unnecessary computational resources.\nUsing similar procedures as those represented in Figure 2 , we determined the best value of s for each clustering method on these data. Table 4 lists these method-specific s values, as well as the corresponding classification results on the clustered features alone. Note that the first row of Table 4 presents the classification result when all 146 analytes are included in the feature space by \"brute force.\" As we have discovered that the 20 PCA components yield the best classification results, we have concluded that PCA is the more effective of the clustering methods. Subsequent tests are performed on only the 20 PCA features.\nWe tested both single SVM (linear kernel) and the ensemble on the 20 PCA features alone as shown in Table 5 . In this result, we find that linear SVM actually outperforms the ensemble. "}, {"section_title": "Feature Augmentation.", "text": "We compared the early fusion and late fusion methods of combining the selective feature set with the PCA features as described in Section 3.3. We assert that since the PCA features were computed from the entire set of 146 analytes, that the PCA feature space contains information not present in the selective feature set. For each fusion method, both single linear SVM and the ensemble classifier are implemented. For early fusion, we constructed a feature vector by concatenating the respective feature vectors from the selective feature-set and the 20 PCA feature-set. For late fusion, selective feature-set and PCA featureset were used parallel as inputs for two classifiers, e.g. two independent SVMs. Then, the outputs of two SVMs are aggregated for final decision using an aggregating SVM. The result of combining the feature sets with early fusion and late fusion is shown in Table 6 . Early fusion on a linear SVM appears to yield the best result, an accuracy of 89% and an area under the ROC curve of 94%. "}, {"section_title": "CONCLUSION AND FUTURE WORK", "text": "Feature augmentation by PCA improved classification performance by 8% for accuracy, 9% for sensitivity, 13% for specificity and 3% for AUC by 3%, compared to using the full selected feature set without augmentation. Our overall best result of 89% accuracy compares favorably with the 85% accuracy reported by Johnstone et al. on a larger sample of the same ADNI dataset.\nAs such, we believe that the PCA augmentation approach proposed here represents a clear improvement in predicting AD assessment from plasma biomarker data.\nWhile we are encouraged by the effect of augmenting a biomarker feature space with features such as PCA features that were derived directly from the original data, there may be alternative clusters of the data based on other sources of evidence such as the literature. As such, we intend to investigate additional feature-space augmentations using latent semantic indexing [24] along with the augmentations proposed here.\nWe also intend to explore dynamic state models to exploit the additional longitudinal data available in ADNI. As we have discovered that a linear SVM yields the best result in an augmented feature space, we are encouraged that a linear model may be appropriate to describe the observations. The Principal Investigator of ADNI is Michael W. Weiner, MD, VA Medical Center and University of California -San Francisco. ADNI is the result of efforts of many co-investigators from a broad range of academic institutions and private corporations, and subjects have been recruited from over 50 sites across the U.S. and Canada. The initial goal of ADNI was to recruit 800 subjects but ADNI has been followed by ADNI-GO and ADNI-2. To date these three protocols have recruited over 1500 adults, ages 55 to 90, to participate in the research, consisting of cognitively normal older individuals, people with early or late MCI, and people with early AD. The follow up duration of each group is specified in the protocols for ADNI-1, ADNI-2 and ADNI-GO. Subjects originally recruited for ADNI-1 and ADNI-GO had the option to be followed in ADNI-2. For up-to-date information, see www.adni-info.org."}, {"section_title": "ACKNOWLEDGMENTS", "text": ""}]